[
  {
    "id": "http://arxiv.org/abs/2012.15274v2",
    "title": "Provably Training Overparameterized Neural Network Classifiers with Non-convex Constraints",
    "authors": [
      "You-Lin Chen",
      "Zhaoran Wang",
      "Mladen Kolar"
    ],
    "author_ids": [],
    "abstract": "Training a classifier under non-convex constraints has gotten increasing\nattention in the machine learning community thanks to its wide range of\napplications such as algorithmic fairness and class-imbalanced classification.\nHowever, several recent works addressing non-convex constraints have only\nfocused on simple models such as logistic regression or support vector\nmachines. Neural networks, one of the most popular models for classification\nnowadays, are precluded and lack theoretical guarantees. In this work, we show\nthat overparameterized neural networks could achieve a near-optimal and\nnear-feasible solution of non-convex constrained optimization problems via the\nproject stochastic gradient descent. Our key ingredient is the no-regret\nanalysis of online learning for neural networks in the overparameterization\nregime, which may be of independent interest in online learning applications.",
    "published_date": "2020-12-30T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15274v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.15259v1",
    "title": "A Maximal Correlation Approach to Imposing Fairness in Machine Learning",
    "authors": [
      "Joshua Lee",
      "Yuheng Bu",
      "Prasanna Sattigeri",
      "Rameswar Panda",
      "Gregory Wornell",
      "Leonid Karlinsky",
      "Rogerio Feris"
    ],
    "author_ids": [],
    "abstract": "As machine learning algorithms grow in popularity and diversify to many\nindustries, ethical and legal concerns regarding their fairness have become\nincreasingly relevant. We explore the problem of algorithmic fairness, taking\nan information-theoretic view. The maximal correlation framework is introduced\nfor expressing fairness constraints and shown to be capable of being used to\nderive regularizers that enforce independence and separation-based fairness\ncriteria, which admit optimization algorithms for both discrete and continuous\nvariables which are more computationally efficient than existing algorithms. We\nshow that these algorithms provide smooth performance-fairness tradeoff curves\nand perform competitively with state-of-the-art methods on both discrete\ndatasets (COMPAS, Adult) and continuous datasets (Communities and Crimes).",
    "published_date": "2020-12-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15259v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.15234v3",
    "title": "Artificial Intelligence Development Races in Heterogeneous Settings",
    "authors": [
      "Theodor Cimpeanu",
      "Francisco C. Santos",
      "Luis Moniz Pereira",
      "Tom Lenaerts",
      "The Anh Han"
    ],
    "author_ids": [],
    "abstract": "Regulation of advanced technologies such as Artificial Intelligence (AI) has\nbecome increasingly important, given the associated risks and apparent ethical\nissues. With the great benefits promised from being able to first supply such\ntechnologies, safety precautions and societal consequences might be ignored or\nshortchanged in exchange for speeding up the development, therefore engendering\na racing narrative among the developers. Starting from a game-theoretical model\ndescribing an idealised technology race in a fully connected world of players,\nhere we investigate how different interaction structures among race\nparticipants can alter collective choices and requirements for regulatory\nactions. Our findings indicate that, when participants portray a strong\ndiversity in terms of connections and peer-influence (e.g., when scale-free\nnetworks shape interactions among parties), the conflicts that exist in\nhomogeneous settings are significantly reduced, thereby lessening the need for\nregulatory actions. Furthermore, our results suggest that technology governance\nand regulation may profit from the world's patent heterogeneity and inequality\namong firms and nations, so as to enable the design and implementation of\nmeticulous interventions on a minority of participants, which is capable of\ninfluencing an entire population towards an ethical and sustainable use of\nadvanced technologies.",
    "published_date": "2020-12-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15234v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.15211v1",
    "title": "The Challenges of Crowd Workers in Rural and Urban America",
    "authors": [
      "Claudia Flores-Saviaga",
      "Yuwen Li",
      "Benjamin V. Hanrahan",
      "Jeffrey Bigham",
      "Saiph Savage"
    ],
    "author_ids": [],
    "abstract": "Crowd work has the potential of helping the financial recovery of regions\ntraditionally plagued by a lack of economic opportunities, e.g., rural areas.\nHowever, we currently have limited information about the challenges facing\ncrowd work-ers from rural and super rural areas as they struggle to make a\nliving through crowd work sites. This paper examines the challenges and\nadvantages of rural and super rural AmazonMechanical Turk (MTurk) crowd workers\nand contrasts them with those of workers from urban areas. Based on a survey\nof421 crowd workers from differing geographic regions in theU.S., we identified\nhow across regions, people struggled with being onboarded into crowd work. We\nuncovered that despite the inequalities and barriers, rural workers tended to\nbe striving more in micro-tasking than their urban counterparts. We also\nidentified cultural traits, relating to time dimension and individualism, that\noffer us an insight into crowd workers and the necessary qualities for them to\nsucceed on gig platforms. We finish by providing design implications based on\nour findings to create more inclusive crowd work platforms and tools",
    "published_date": "2020-12-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15211v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.15081v14",
    "title": "Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using Multi-Agent Reinforcement Learning",
    "authors": [
      "Mingqi Yuan",
      "Qi Cao",
      "Man-on Pun",
      "Yi Chen"
    ],
    "author_ids": [],
    "abstract": "In this work, we develop practical user scheduling algorithms for downlink\nbursty traffic with emphasis on user fairness. In contrast to the conventional\nscheduling algorithms that either equally divides the transmission time slots\namong users or maximizing some ratios without physcial meanings, we propose to\nuse the 5%-tile user data rate (5TUDR) as the metric to evaluate user fairness.\nSince it is difficult to directly optimize 5TUDR, we first cast the problem\ninto the stochastic game framework and subsequently propose a Multi-Agent\nReinforcement Learning (MARL)-based algorithm to perform distributed\noptimization on the resource block group (RBG) allocation. Furthermore, each\nMARL agent is designed to take information measured by network counters from\nmultiple network layers (e.g. Channel Quality Indicator, Buffer size) as the\ninput states while the RBG allocation as action with a proposed reward function\ndesigned to maximize 5TUDR. Extensive simulation is performed to show that the\nproposed MARL-based scheduler can achieve fair scheduling while maintaining\ngood average network throughput as compared to conventional schedulers.",
    "published_date": "2020-12-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.OS",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15081v14",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.15054v2",
    "title": "Bidirectional Mapping Coupled GAN for Generalized Zero-Shot Learning",
    "authors": [
      "Tasfia Shermin",
      "Shyh Wei Teng",
      "Ferdous Sohel",
      "Manzur Murshed",
      "Guojun Lu"
    ],
    "author_ids": [],
    "abstract": "Bidirectional mapping-based generalized zero-shot learning (GZSL) methods\nrely on the quality of synthesized features to recognize seen and unseen data.\nTherefore, learning a joint distribution of seen-unseen domains and preserving\ndomain distinction is crucial for these methods. However, existing methods only\nlearn the underlying distribution of seen data, although unseen class semantics\nare available in the GZSL problem setting. Most methods neglect retaining\ndomain distinction and use the learned distribution to recognize seen and\nunseen data. Consequently, they do not perform well. In this work, we utilize\nthe available unseen class semantics alongside seen class semantics and learn\njoint distribution through a strong visual-semantic coupling. We propose a\nbidirectional mapping coupled generative adversarial network (BMCoGAN) by\nextending the coupled generative adversarial network into a dual-domain\nlearning bidirectional mapping model. We further integrate a Wasserstein\ngenerative adversarial optimization to supervise the joint distribution\nlearning. We design a loss optimization for retaining domain distinctive\ninformation in the synthesized features and reducing bias towards seen classes,\nwhich pushes synthesized seen features towards real seen features and pulls\nsynthesized unseen features away from real seen features. We evaluate BMCoGAN\non benchmark datasets and demonstrate its superior performance against\ncontemporary methods.",
    "published_date": "2020-12-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15054v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14961v1",
    "title": "Towards Fair Deep Anomaly Detection",
    "authors": [
      "Hongjing Zhang",
      "Ian Davidson"
    ],
    "author_ids": [],
    "abstract": "Anomaly detection aims to find instances that are considered unusual and is a\nfundamental problem of data science. Recently, deep anomaly detection methods\nwere shown to achieve superior results particularly in complex data such as\nimages. Our work focuses on deep one-class classification for anomaly detection\nwhich learns a mapping only from the normal samples. However, the non-linear\ntransformation performed by deep learning can potentially find patterns\nassociated with social bias. The challenge with adding fairness to deep anomaly\ndetection is to ensure both making fair and correct anomaly predictions\nsimultaneously. In this paper, we propose a new architecture for the fair\nanomaly detection approach (Deep Fair SVDD) and train it using an adversarial\nnetwork to de-correlate the relationships between the sensitive attributes and\nthe learned representations. This differs from how fairness is typically added\nnamely as a regularizer or a constraint. Further, we propose two effective\nfairness measures and empirically demonstrate that existing deep anomaly\ndetection methods are unfair. We show that our proposed approach can remove the\nunfairness largely with minimal loss on the anomaly detection performance.\nLastly, we conduct an in-depth analysis to show the strength and limitations of\nour proposed model, including parameter analysis, feature visualization, and\nrun-time analysis.",
    "published_date": "2020-12-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14961v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14633v1",
    "title": "Supermodularity and valid inequalities for quadratic optimization with indicators",
    "authors": [
      "Alper Atamturk",
      "Andres Gomez"
    ],
    "author_ids": [],
    "abstract": "We study the minimization of a rank-one quadratic with indicators and show\nthat the underlying set function obtained by projecting out the continuous\nvariables is supermodular. Although supermodular minimization is, in general,\ndifficult, the specific set function for the rank-one quadratic can be\nminimized in linear time. We show that the convex hull of the epigraph of the\nquadratic can be obtaining from inequalities for the underlying supermodular\nset function by lifting them into nonlinear inequalities in the original space\nof variables. Explicit forms of the convex-hull description are given, both in\nthe original space of variables and in an extended formulation via conic\nquadratic-representable inequalities, along with a polynomial separation\nalgorithm. Computational experiments indicate that the lifted supermodular\ninequalities in conic quadratic form are quite effective in reducing the\nintegrality gap for quadratic optimization with indicators.",
    "published_date": "2020-12-29T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14633v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.00004v1",
    "title": "Deep Unsupervised Identification of Selected SNPs between Adapted Populations on Pool-seq Data",
    "authors": [
      "Julia Siekiera",
      "Stefan Kramer"
    ],
    "author_ids": [],
    "abstract": "The exploration of selected single nucleotide polymorphisms (SNPs) to\nidentify genetic diversity between different sequencing population pools\n(Pool-seq) is a fundamental task in genetic research. As underlying sequence\nreads and their alignment are error-prone and univariate statistical solutions\nonly take individual positions of the genome into account, the identification\nof selected SNPs remains a challenging process. Deep learning models like\nconvolutional neural networks (CNNs) are able to consider large input areas in\ntheir decisions. We suggest an unsupervised pipeline to be independent of a\nrarely known ground truth. We train a supervised discriminator CNN to\ndistinguish alignments from different populations and utilize the model for\nunsupervised SNP calling by applying explainable artificial intelligence\nmethods. Our proposed multivariate method is based on two main assumptions: We\nassume (i) that instances having a high predictive certainty of being\ndistinguishable are likely to contain genetic variants, and (ii) that selected\nSNPs are located at regions with input features having the highest influence on\nthe model's decision process. We directly compare our method with statistical\nresults on two different Pool-seq datasets and show that our solution is able\nto extend statistical results.",
    "published_date": "2020-12-28T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.GN",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00004v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14406v2",
    "title": "dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python",
    "authors": [
      "Hubert Baniecki",
      "Wojciech Kretowicz",
      "Piotr Piatyszek",
      "Jakub Wisniewski",
      "Przemyslaw Biecek"
    ],
    "author_ids": [],
    "abstract": "The increasing amount of available data, computing power, and the constant\npursuit for higher performance results in the growing complexity of predictive\nmodels. Their black-box nature leads to opaqueness debt phenomenon inflicting\nincreased risks of discrimination, lack of reproducibility, and deflated\nperformance due to data drift. To manage these risks, good MLOps practices ask\nfor better validation of model performance and fairness, higher explainability,\nand continuous monitoring. The necessity of deeper model transparency appears\nnot only from scientific and social domains, but also emerging laws and\nregulations on artificial intelligence. To facilitate the development of\nresponsible machine learning models, we showcase dalex, a Python package which\nimplements the model-agnostic interface for interactive model exploration. It\nadopts the design crafted through the development of various tools for\nresponsible machine learning; thus, it aims at the unification of the existing\nsolutions. This library's source code and documentation are available under\nopen license at https://python.drwhy.ai/.",
    "published_date": "2020-12-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.HC",
      "cs.SE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14406v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14402v1",
    "title": "Deep Neural Models for color discrimination and color constancy",
    "authors": [
      "Alban Flachot",
      "Arash Akbarinia",
      "Heiko H. Schütt",
      "Roland W. Fleming",
      "Felix A. Wichmann",
      "Karl R. Gegenfurtner"
    ],
    "author_ids": [],
    "abstract": "Color constancy is our ability to perceive constant colors across varying\nilluminations. Here, we trained deep neural networks to be color constant and\nevaluated their performance with varying cues. Inputs to the networks consisted\nof the cone excitations in 3D-rendered images of 2115 different 3D-shapes, with\nspectral reflectances of 1600 different Munsell chips, illuminated under 278\ndifferent natural illuminations. The models were trained to classify the\nreflectance of the objects. One network, Deep65, was trained under a fixed\ndaylight D65 illumination, while DeepCC was trained under varying\nilluminations. Testing was done with 4 new illuminations with equally spaced\nCIEL*a*b* chromaticities, 2 along the daylight locus and 2 orthogonal to it. We\nfound a high degree of color constancy for DeepCC, and constancy was higher\nalong the daylight locus. When gradually removing cues from the scene,\nconstancy decreased. High levels of color constancy were achieved with\ndifferent DNN architectures. Both ResNets and classical ConvNets of varying\ndegrees of complexity performed well. However, DeepCC, a convolutional network,\nrepresented colors along the 3 color dimensions of human color vision, while\nResNets showed a more complex representation.",
    "published_date": "2020-12-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14402v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14400v1",
    "title": "Interactions of Linguistic and Domain Overhypotheses in Category Learning",
    "authors": [
      "Luann C. Jung",
      "Haiyan Wang"
    ],
    "author_ids": [],
    "abstract": "For humans learning to categorize and distinguish parts of the world, the set\nof assumptions (overhypotheses) they hold about potential category structures\nis directly related to their learning process. In this work we examine the\neffects of two overhypotheses for category learning: 1) the bias introduced by\nthe presence of linguistic labels for objects; 2) the conceptual 'domain'\nbiases inherent in the learner about which features are most indicative of\ncategory structure. These two biases work in tandem to impose priors on the\nlearning process; and we model and detail their interaction and effects. This\npaper entails an adaptation and expansion of prior experimental work that\naddressed label bias effects but did not fully explore conceptual domain\nbiases. Our results highlight the importance of both the domain and label\nbiases in facilitating or hindering category learning.",
    "published_date": "2020-12-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14400v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.14261v3",
    "title": "A Survey on Neural Network Interpretability",
    "authors": [
      "Yu Zhang",
      "Peter Tiňo",
      "Aleš Leonardis",
      "Ke Tang"
    ],
    "author_ids": [],
    "abstract": "Along with the great success of deep neural networks, there is also growing\nconcern about their black-box nature. The interpretability issue affects\npeople's trust on deep learning systems. It is also related to many ethical\nproblems, e.g., algorithmic discrimination. Moreover, interpretability is a\ndesired property for deep networks to become powerful tools in other research\nfields, e.g., drug discovery and genomics. In this survey, we conduct a\ncomprehensive review of the neural network interpretability research. We first\nclarify the definition of interpretability as it has been used in many\ndifferent contexts. Then we elaborate on the importance of interpretability and\npropose a novel taxonomy organized along three dimensions: type of engagement\n(passive vs. active interpretation approaches), the type of explanation, and\nthe focus (from local to global interpretability). This taxonomy provides a\nmeaningful 3D view of distribution of papers from the relevant literature as\ntwo of the dimensions are not simply categorical but allow ordinal\nsubcategories. Finally, we summarize the existing interpretability evaluation\nmethods and suggest possible research directions inspired by our new taxonomy.",
    "published_date": "2020-12-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14261v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14173v3",
    "title": "Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques",
    "authors": [
      "David Morales",
      "Estefania Talavera",
      "Beatriz Remeseiro"
    ],
    "author_ids": [],
    "abstract": "The field of deep learning is evolving in different directions, with still\nthe need for more efficient training strategies. In this work, we present a\nnovel and robust training scheme that integrates visual explanation techniques\nin the learning process. Unlike the attention mechanisms that focus on the\nrelevant parts of images, we aim to improve the robustness of the model by\nmaking it pay attention to other regions as well. Broadly speaking, the idea is\nto distract the classifier in the learning process to force it to focus not\nonly on relevant regions but also on those that, a priori, are not so\ninformative for the discrimination of the class. We tested the proposed\napproach by embedding it into the learning process of a convolutional neural\nnetwork for the analysis and classification of two well-known datasets, namely\nStanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a\nreal-case scenario for the classification of egocentric images, allowing us to\nobtain relevant information about peoples' lifestyles. In particular, we work\non the challenging EgoFoodPlaces dataset, achieving state-of-the-art results\nwith a lower level of complexity. The obtained results indicate the suitability\nof our proposed training scheme for image classification, improving the\nrobustness of the final model.",
    "published_date": "2020-12-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14173v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.13891v3",
    "title": "Federated Unlearning",
    "authors": [
      "Gaoyang Liu",
      "Xiaoqiang Ma",
      "Yang Yang",
      "Chen Wang",
      "Jiangchuan Liu"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) has recently emerged as a promising distributed\nmachine learning (ML) paradigm. Practical needs of the \"right to be forgotten\"\nand countering data poisoning attacks call for efficient techniques that can\nremove, or unlearn, specific training data from the trained FL model. Existing\nunlearning techniques in the context of ML, however, are no longer in effect\nfor FL, mainly due to the inherent distinction in the way how FL and ML learn\nfrom data. Therefore, how to enable efficient data removal from FL models\nremains largely under-explored. In this paper, we take the first step to fill\nthis gap by presenting FedEraser, the first federated unlearning methodology\nthat can eliminate the influence of a federated client's data on the global FL\nmodel while significantly reducing the time used for constructing the unlearned\nFL model.The basic idea of FedEraser is to trade the central server's storage\nfor unlearned model's construction time, where FedEraser reconstructs the\nunlearned model by leveraging the historical parameter updates of federated\nclients that have been retained at the central server during the training\nprocess of FL. A novel calibration method is further developed to calibrate the\nretained updates, which are further used to promptly construct the unlearned\nmodel, yielding a significant speed-up to the reconstruction of the unlearned\nmodel while maintaining the model efficacy. Experiments on four realistic\ndatasets demonstrate the effectiveness of FedEraser, with an expected speed-up\nof $4\\times$ compared with retraining from the scratch. We envision our work as\nan early step in FL towards compliance with legal and ethical criteria in a\nfair and transparent manner.",
    "published_date": "2020-12-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.13891v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.02008v1",
    "title": "An Ecosystem Approach to Ethical AI and Data Use: Experimental Reflections",
    "authors": [
      "Mark Findlay",
      "Josephine Seah"
    ],
    "author_ids": [],
    "abstract": "While we have witnessed a rapid growth of ethics documents meant to guide AI\ndevelopment, the promotion of AI ethics has nonetheless proceeded with little\ninput from AI practitioners themselves. Given the proliferation of AI for\nSocial Good initiatives, this is an emerging gap that needs to be addressed in\norder to develop more meaningful ethical approaches to AI use and development.\nThis paper offers a methodology, a shared fairness approach, aimed at\nidentifying the needs of AI practitioners when it comes to confronting and\nresolving ethical challenges and to find a third space where their operational\nlanguage can be married with that of the more abstract principles that\npresently remain at the periphery of their work experiences. We offer a\ngrassroots approach to operational ethics based on dialog and mutualised\nresponsibility. This methodology is centred around conversations intended to\nelicit practitioners perceived ethical attribution and distribution over key\nvalue laden operational decisions, to identify when these decisions arise and\nwhat ethical challenges they confront, and to engage in a language of ethics\nand responsibility which enables practitioners to internalise ethical\nresponsibility. The methodology bridges responsibility imbalances that rest in\nstructural decision making power and elite technical knowledge, by commencing\nwith personal, facilitated conversations, returning the ethical discourse to\nthose meant to give it meaning at the sharp end of the ecosystem. Our primary\ncontribution is to add to the recent literature seeking to bring AI\npractitioners' experiences to the fore by offering a methodology for\nunderstanding how ethics manifests as a relational and interdependent\nsociotechnical practice in their work.",
    "published_date": "2020-12-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02008v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.13831v3",
    "title": "Spatial Contrastive Learning for Few-Shot Classification",
    "authors": [
      "Yassine Ouali",
      "Céline Hudelot",
      "Myriam Tami"
    ],
    "author_ids": [],
    "abstract": "In this paper, we explore contrastive learning for few-shot classification,\nin which we propose to use it as an additional auxiliary training objective\nacting as a data-dependent regularizer to promote more general and transferable\nfeatures. In particular, we present a novel attention-based spatial contrastive\nobjective to learn locally discriminative and class-agnostic features. As a\nresult, our approach overcomes some of the limitations of the cross-entropy\nloss, such as its excessive discrimination towards seen classes, which reduces\nthe transferability of features to unseen classes. With extensive experiments,\nwe show that the proposed method outperforms state-of-the-art approaches,\nconfirming the importance of learning good and transferable embeddings for\nfew-shot learning.",
    "published_date": "2020-12-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.13831v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.13498v1",
    "title": "1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive Pedestrian Re-identification",
    "authors": [
      "Jianyang Gu",
      "Hao Luo",
      "Weihua Chen",
      "Yiqi Jiang",
      "Yuqi Zhang",
      "Shuting He",
      "Fan Wang",
      "Hao Li",
      "Wei Jiang"
    ],
    "author_ids": [],
    "abstract": "This paper presents our proposed methods for domain adaptive pedestrian\nre-identification (Re-ID) task in Visual Domain Adaptation Challenge\n(VisDA-2020). Considering the large gap between the source domain and target\ndomain, we focused on solving two biases that influenced the performance on\ndomain adaptive pedestrian Re-ID and proposed a two-stage training procedure.\nAt the first stage, a baseline model is trained with images transferred from\nsource domain to target domain and from single camera to multiple camera\nstyles. Then we introduced a domain adaptation framework to train the model on\nsource data and target data simultaneously. Different pseudo label generation\nstrategies are adopted to continuously improve the discriminative ability of\nthe model. Finally, with multiple models ensembled and additional post\nprocessing approaches adopted, our methods achieve 76.56% mAP and 84.25% rank-1\non the test set. Codes are available at\nhttps://github.com/vimar-gu/Bias-Eliminate-DA-ReID",
    "published_date": "2020-12-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.13498v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.13380v1",
    "title": "A Regret bound for Non-stationary Multi-Armed Bandits with Fairness Constraints",
    "authors": [
      "Shaarad A. R",
      "Ambedkar Dukkipati"
    ],
    "author_ids": [],
    "abstract": "The multi-armed bandits' framework is the most common platform to study\nstrategies for sequential decision-making problems. Recently, the notion of\nfairness has attracted a lot of attention in the machine learning community.\nOne can impose the fairness condition that at any given point of time, even\nduring the learning phase, a poorly performing candidate should not be\npreferred over a better candidate. This fairness constraint is known to be one\nof the most stringent and has been studied in the stochastic multi-armed\nbandits' framework in a stationary setting for which regret bounds have been\nestablished. The main aim of this paper is to study this problem in a\nnon-stationary setting. We present a new algorithm called Fair Upper Confidence\nBound with Exploration Fair-UCBe algorithm for solving a slowly varying\nstochastic $k$-armed bandit problem. With this we present two results: (i)\nFair-UCBe indeed satisfies the above mentioned fairness condition, and (ii) it\nachieves a regret bound of $O\\left(k^{\\frac{3}{2}} T^{1 - \\frac{\\alpha}{2}}\n\\sqrt{\\log T}\\right)$, for some suitable $\\alpha \\in (0, 1)$, where $T$ is the\ntime horizon. This is the first fair algorithm with a sublinear regret bound\napplicable to non-stationary bandits to the best of our knowledge. We show that\nthe performance of our algorithm in the non-stationary case approaches that of\nits stationary counterpart as the variation in the environment tends to zero.",
    "published_date": "2020-12-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.13380v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.13176v1",
    "title": "Gender Bias in Multilingual Neural Machine Translation: The Architecture Matters",
    "authors": [
      "Marta R. Costa-jussà",
      "Carlos Escolano",
      "Christine Basta",
      "Javier Ferrando",
      "Roser Batlle",
      "Ksenia Kharitonova"
    ],
    "author_ids": [],
    "abstract": "Multilingual Neural Machine Translation architectures mainly differ in the\namount of sharing modules and parameters among languages. In this paper, and\nfrom an algorithmic perspective, we explore if the chosen architecture, when\ntrained with the same data, influences the gender bias accuracy. Experiments in\nfour language pairs show that Language-Specific encoders-decoders exhibit less\nbias than the Shared encoder-decoder architecture. Further interpretability\nanalysis of source embeddings and the attention shows that, in the\nLanguage-Specific case, the embeddings encode more gender information, and its\nattention is more diverted. Both behaviors help in mitigating gender bias.",
    "published_date": "2020-12-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.13176v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14296v2",
    "title": "Network Design for Social Welfare",
    "authors": [
      "Abhishek Shende",
      "Deepanshu Vasal",
      "Sriram Vishwanath"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider the problem of network design on network games. We\nstudy the conditions on the adjacency matrix of the underlying network to\ndesign a game such that the Nash equilibrium coincides with the social optimum.\nWe provide the examples for linear quadratic games that satisfy this condition.\nFurthermore, we identify conditions on properties of adjacency matrix that\nprovide a unique solution using variational inequality formulation, and verify\nthe robustness and continuity of the social cost under perturbations of the\nnetwork. Finally we comment on individual rationality and extension of our\nresults to large random networked games.",
    "published_date": "2020-12-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14296v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.12895v1",
    "title": "A Modern Analysis of Hutchinson's Trace Estimator",
    "authors": [
      "Maciej Skorski"
    ],
    "author_ids": [],
    "abstract": "The paper establishes the new state-of-art in the accuracy analysis of\nHutchinson's trace estimator. Leveraging tools that have not been previously\nused in this context, particularly hypercontractive inequalities and\nconcentration properties of sub-gamma distributions, we offer an elegant and\nmodular analysis, as well as numerically superior bounds. Besides these\nimprovements, this work aims to better popularize the aforementioned techniques\nwithin the CS community.",
    "published_date": "2020-12-23T00:00:00",
    "year": 2020,
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.AP",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12895v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.12821v3",
    "title": "Focal Frequency Loss for Image Reconstruction and Synthesis",
    "authors": [
      "Liming Jiang",
      "Bo Dai",
      "Wayne Wu",
      "Chen Change Loy"
    ],
    "author_ids": [],
    "abstract": "Image reconstruction and synthesis have witnessed remarkable progress thanks\nto the development of generative models. Nonetheless, gaps could still exist\nbetween the real and generated images, especially in the frequency domain. In\nthis study, we show that narrowing gaps in the frequency domain can ameliorate\nimage reconstruction and synthesis quality further. We propose a novel focal\nfrequency loss, which allows a model to adaptively focus on frequency\ncomponents that are hard to synthesize by down-weighting the easy ones. This\nobjective function is complementary to existing spatial losses, offering great\nimpedance against the loss of important frequency information due to the\ninherent bias of neural networks. We demonstrate the versatility and\neffectiveness of focal frequency loss to improve popular models, such as VAE,\npix2pix, and SPADE, in both perceptual quality and quantitative performance. We\nfurther show its potential on StyleGAN2.",
    "published_date": "2020-12-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12821v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.12710v2",
    "title": "Existence and Computation of Maximin Fair Allocations Under Matroid-Rank Valuations",
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ],
    "author_ids": [],
    "abstract": "We study fair and economically efficient allocation of indivisible goods\namong agents whose valuations are rank functions of matroids. Such valuations\nconstitute a well-studied class of submodular functions (i.e., they exhibit a\ndiminishing returns property) and model preferences in several\nresource-allocation settings. We prove that, for matroid-rank valuations, a\nsocial welfare-maximizing allocation that gives each agent her maximin share\nalways exists. Furthermore, such an allocation can be computed in polynomial\ntime. We establish similar existential and algorithmic results for the pairwise\nmaximin share guarantee as well.\n  To complement these results, we show that if the agents have binary XOS\nvaluations or weighted-rank valuations, then maximin fair allocations are not\nguaranteed to exist. Both of these valuation classes are immediate\ngeneralizations of matroid-rank functions.",
    "published_date": "2020-12-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12710v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.12556v6",
    "title": "A Survey on Visual Transformer",
    "authors": [
      "Kai Han",
      "Yunhe Wang",
      "Hanting Chen",
      "Xinghao Chen",
      "Jianyuan Guo",
      "Zhenhua Liu",
      "Yehui Tang",
      "An Xiao",
      "Chunjing Xu",
      "Yixing Xu",
      "Zhaohui Yang",
      "Yiman Zhang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Transformer, first applied to the field of natural language processing, is a\ntype of deep neural network mainly based on the self-attention mechanism.\nThanks to its strong representation capabilities, researchers are looking at\nways to apply transformer to computer vision tasks. In a variety of visual\nbenchmarks, transformer-based models perform similar to or better than other\ntypes of networks such as convolutional and recurrent neural networks. Given\nits high performance and less need for vision-specific inductive bias,\ntransformer is receiving more and more attention from the computer vision\ncommunity. In this paper, we review these vision transformer models by\ncategorizing them in different tasks and analyzing their advantages and\ndisadvantages. The main categories we explore include the backbone network,\nhigh/mid-level vision, low-level vision, and video processing. We also include\nefficient transformer methods for pushing transformer into real device-based\napplications. Furthermore, we also take a brief look at the self-attention\nmechanism in computer vision, as it is the base component in transformer.\nToward the end of this paper, we discuss the challenges and provide several\nfurther research directions for vision transformers.",
    "published_date": "2020-12-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12556v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.12537v1",
    "title": "BENN: Bias Estimation Using Deep Neural Network",
    "authors": [
      "Amit Giloni",
      "Edita Grolman",
      "Tanja Hagemann",
      "Ronald Fromm",
      "Sebastian Fischer",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "author_ids": [],
    "abstract": "The need to detect bias in machine learning (ML) models has led to the\ndevelopment of multiple bias detection methods, yet utilizing them is\nchallenging since each method: i) explores a different ethical aspect of bias,\nwhich may result in contradictory output among the different methods, ii)\nprovides an output of a different range/scale and therefore, can't be compared\nwith other methods, and iii) requires different input, and therefore a human\nexpert needs to be involved to adjust each method according to the examined\nmodel. In this paper, we present BENN -- a novel bias estimation method that\nuses a pretrained unsupervised deep neural network. Given a ML model and data\nsamples, BENN provides a bias estimation for every feature based on the model's\npredictions. We evaluated BENN using three benchmark datasets and one\nproprietary churn prediction model used by a European Telco and compared it\nwith an ensemble of 21 existing bias estimation methods. Evaluation results\nhighlight the significant advantages of BENN over the ensemble, as it is\ngeneric (i.e., can be applied to any ML model) and there is no need for a\ndomain expert, yet it provides bias estimations that are aligned with those of\nthe ensemble.",
    "published_date": "2020-12-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12537v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.12415v3",
    "title": "What Makes People Install a COVID-19 Contact-Tracing App? Understanding the Influence of App Design and Individual Difference on Contact-Tracing App Adoption Intention",
    "authors": [
      "Tianshi Li",
      "Camille Cobb",
      "Jackie",
      "Yang",
      "Sagar Baviskar",
      "Yuvraj Agarwal",
      "Beibei Li",
      "Lujo Bauer",
      "Jason I. Hong"
    ],
    "author_ids": [],
    "abstract": "Smartphone-based contact-tracing apps are a promising solution to help scale\nup the conventional contact-tracing process. However, low adoption rates have\nbecome a major issue that prevents these apps from achieving their full\npotential. In this paper, we present a national-scale survey experiment ($N =\n1963$) in the U.S. to investigate the effects of app design choices and\nindividual differences on COVID-19 contact-tracing app adoption intentions. We\nfound that individual differences such as prosocialness, COVID-19 risk\nperceptions, general privacy concerns, technology readiness, and demographic\nfactors played a more important role than app design choices such as\ndecentralized design vs. centralized design, location use, app providers, and\nthe presentation of security risks. Certain app designs could exacerbate the\ndifferent preferences in different sub-populations which may lead to an\ninequality of acceptance to certain app design choices (e.g., developed by\nstate health authorities vs. a large tech company) among different groups of\npeople (e.g., people living in rural areas vs. people living in urban areas).\nOur mediation analysis showed that one's perception of the public health\nbenefits offered by the app and the adoption willingness of other people had a\nlarger effect in explaining the observed effects of app design choices and\nindividual differences than one's perception of the app's security and privacy\nrisks. With these findings, we discuss practical implications on the design,\nmarketing, and deployment of COVID-19 contact-tracing apps in the U.S.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12415v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.12363v1",
    "title": "The Circlet Inequalities: A New, Circulant-Based Facet-Defining Inequality for the TSP",
    "authors": [
      "Samuel C. Gutekunst",
      "David P. Williamson"
    ],
    "author_ids": [],
    "abstract": "Facet-defining inequalities of the symmetric Traveling Salesman Problem (TSP)\npolytope play a prominent role in both polyhedral TSP research and\nstate-of-the-art TSP solvers. In this paper, we introduce a new class of\nfacet-defining inequalities, the \\emph{circlet inequalities}. These\ninequalities were first conjectured in Gutekunst and Williamson \\cite{Gut19b}\nwhen studying Circulant TSP, and they provide a bridge between polyhedral TSP\nresearch and number-theoretic investigations of Hamiltonian cycles stemming\nfrom a conjecture due to Marco Buratti in 2017. The circlet inequalities\nexhibit circulant symmetry by placing the same weight on all edges of a given\nlength; our main proof exploits this symmetry to prove the validity of the\ncirclet inequalities. We then show that the circlet inequalities are\nfacet-defining and compute their strength following Goemans \\cite{Goe95b}; they\nachieve the same worst-case strength as the similarly circulant crown\ninequalities of Naddef and Rinaldi \\cite{Nad92}, but are generally stronger.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "math.CO",
      "cs.DM",
      "math.NT",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12363v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.12356v2",
    "title": "Unbiased Subdata Selection for Fair Classification: A Unified Framework and Scalable Algorithms",
    "authors": [
      "Qing Ye",
      "Weijun Xie"
    ],
    "author_ids": [],
    "abstract": "As an important problem in modern data analytics, classification has\nwitnessed varieties of applications from different domains. Different from\nconventional classification approaches, fair classification concerns the issues\nof unintentional biases against the sensitive features (e.g., gender, race).\nDue to high nonconvexity of fairness measures, existing methods are often\nunable to model exact fairness, which can cause inferior fair classification\noutcomes. This paper fills the gap by developing a novel unified framework to\njointly optimize accuracy and fairness. The proposed framework is versatile and\ncan incorporate different fairness measures studied in literature precisely as\nwell as can be applicable to many classifiers including deep classification\nmodels. Specifically, in this paper, we first prove Fisher consistency of the\nproposed framework. We then show that many classification models within this\nframework can be recast as mixed-integer convex programs, which can be solved\neffectively by off-the-shelf solvers when the instance sizes are moderate and\ncan be used as benchmarks to compare the efficiency of approximation\nalgorithms. We prove that in the proposed framework, when the classification\noutcomes are known, the resulting problem, termed \"unbiased subdata selection,\"\nis strongly polynomial-solvable and can be used to enhance the classification\nfairness by selecting more representative data points. This motivates us to\ndevelop an iterative refining strategy (IRS) to solve the large-scale\ninstances, where we improve the classification accuracy and conduct the\nunbiased subdata selection in an alternating fashion. We study the convergence\nproperty of IRS and derive its approximation bound. More broadly, this\nframework can be leveraged to improve classification models with unbalanced\ndata by taking F1 score into consideration.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12356v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.12324v1",
    "title": "Do We Need Improved Code Quality Metrics?",
    "authors": [
      "Tushar Sharma",
      "Diomidis Spinellis"
    ],
    "author_ids": [],
    "abstract": "The software development community has been using code quality metrics for\nthe last five decades. Despite their wide adoption, code quality metrics have\nattracted a fair share of criticism. In this paper, first, we carry out a\nqualitative exploration by surveying software developers to gauge their\nopinions about current practices and potential gaps with the present set of\nmetrics. We identify deficiencies including lack of soundness, i.e., the\nability of a metric to capture a notion accurately as promised by the metric,\nlack of support for assessing software architecture quality, and insufficient\nsupport for assessing software testing and infrastructure. In the second part\nof the paper, we focus on one specific code quality metric-LCOM as a case study\nto explore opportunities towards improved metrics. We evaluate existing LCOM\nalgorithms qualitatively and quantitatively to observe how closely they\nrepresent the concept of cohesion. In this pursuit, we first create eight\ndiverse cases that any LCOM algorithm must cover and obtain their cohesion\nlevels by a set of experienced developers and consider them as a ground truth.\nWe show that the present set of LCOM algorithms do poorly w.r.t. these cases.\nTo bridge the identified gap, we propose a new approach to compute LCOM and\nevaluate the new approach with the ground truth. We also show, using a\nquantitative analysis using more than 90 thousand types belonging to 261\nhigh-quality Java repositories, the present set of methods paint a very\ninaccurate and misleading picture of class cohesion. We conclude that the\ncurrent code quality metrics in use suffer from various deficiencies,\npresenting ample opportunities for the research community to address the gaps.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12324v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.12305v2",
    "title": "Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective",
    "authors": [
      "Svetlana Kiritchenko",
      "Isar Nejadgholi",
      "Kathleen C. Fraser"
    ],
    "author_ids": [],
    "abstract": "The pervasiveness of abusive content on the internet can lead to severe\npsychological and physical harm. Significant effort in Natural Language\nProcessing (NLP) research has been devoted to addressing this problem through\nabusive content detection and related sub-areas, such as the detection of hate\nspeech, toxicity, cyberbullying, etc. Although current technologies achieve\nhigh classification performance in research studies, it has been observed that\nthe real-life application of this technology can cause unintended harms, such\nas the silencing of under-represented groups. We review a large body of NLP\nresearch on automatic abuse detection with a new focus on ethical challenges,\norganized around eight established ethical principles: privacy, accountability,\nsafety and security, transparency and explainability, fairness and\nnon-discrimination, human control of technology, professional responsibility,\nand promotion of human values. In many cases, these principles relate not only\nto situational ethical codes, which may be context-dependent, but are in fact\nconnected to universal human rights, such as the right to privacy, freedom from\ndiscrimination, and freedom of expression. We highlight the need to examine the\nbroad social impacts of this technology, and to bring ethical and human rights\nconsiderations to every stage of the application life-cycle, from task\nformulation and dataset design, to model training and evaluation, to\napplication deployment. Guided by these principles, we identify several\nopportunities for rights-respecting, socio-technical solutions to detect and\nconfront online abuse, including `nudging', `quarantining', value sensitive\ndesign, counter-narratives, style transfer, and AI-driven public education\napplications.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12305v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.12230v1",
    "title": "A generalization of Costa's Entropy Power Inequality",
    "authors": [
      "Luca Tamanini"
    ],
    "author_ids": [],
    "abstract": "Aim of this short note is to study Shannon's entropy power along entropic\ninterpolations, thus generalizing Costa's concavity theorem. We shall provide\ntwo proofs of independent interest: the former by $\\Gamma$-calculus, hence\napplicable to more abstract frameworks; the latter with an explicit remainder\nterm, reminiscent of [20], allowing us to characterize the case of equality.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.FA",
      "math.IT",
      "94A15, 39B62",
      "E.4; H.1.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12230v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.12216v1",
    "title": "Quantitative Correlation Inequalities via Semigroup Interpolation",
    "authors": [
      "Anindya De",
      "Shivam Nadimpalli",
      "Rocco A. Servedio"
    ],
    "author_ids": [],
    "abstract": "Most correlation inequalities for high-dimensional functions in the\nliterature, such as the Fortuin-Kasteleyn-Ginibre (FKG) inequality and the\ncelebrated Gaussian Correlation Inequality of Royen, are qualitative statements\nwhich establish that any two functions of a certain type have non-negative\ncorrelation. In this work we give a general approach that can be used to\nbootstrap many qualitative correlation inequalities for functions over product\nspaces into quantitative statements. The approach combines a new extremal\nresult about power series, proved using complex analysis, with harmonic\nanalysis of functions over product spaces. We instantiate this general approach\nin several different concrete settings to obtain a range of new and\nnear-optimal quantitative correlation inequalities, including:\n  $\\bullet$ A quantitative version of Royen's celebrated Gaussian Correlation\nInequality. Royen (2014) confirmed a conjecture, open for 40 years, stating\nthat any two symmetric, convex sets must be non-negatively correlated under any\ncentered Gaussian distribution. We give a lower bound on the correlation in\nterms of the vector of degree-2 Hermite coefficients of the two convex sets,\nanalogous to the correlation bound for monotone Boolean functions over\n$\\{0,1\\}^n$ obtained by Talagrand (1996).\n  $\\bullet$ A quantitative version of the well-known FKG inequality for\nmonotone functions over any finite product probability space, generalizing the\nquantitative correlation bound for monotone Boolean functions over $\\{0,1\\}^n$\nobtained by Talagrand (1996). The only prior generalization of which we are\naware is due to Keller (2008, 2009, 2012), which extended Talagrand's result to\nproduct distributions over $\\{0,1\\}^n$. We also give two different quantitative\nversions of the FKG inequality for monotone functions over the continuous\ndomain $[0,1]^n$, answering a question of Keller (2009).",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "math.PR",
      "cs.CC",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.12216v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.14325v1",
    "title": "Digital me ontology and ethics",
    "authors": [
      "Ljupco Kocarev",
      "Jasna Koteska"
    ],
    "author_ids": [],
    "abstract": "This paper addresses ontology and ethics of an AI agent called digital me. We\ndefine digital me as autonomous, decision-making, and learning agent,\nrepresenting an individual and having practically immortal own life. It is\nassumed that digital me is equipped with the big-five personality model,\nensuring that it provides a model of some aspects of a strong AI:\nconsciousness, free will, and intentionality. As computer-based personality\njudgments are more accurate than those made by humans, digital me can judge the\npersonality of the individual represented by the digital me, other individuals'\npersonalities, and other digital me-s. We describe seven ontological qualities\nof digital me: a) double-layer status of Digital Being versus digital me, b)\ndigital me versus real me, c) mind-digital me and body-digital me, d) digital\nme versus doppelganger (shadow digital me), e) non-human time concept, f)\nsocial quality, g) practical immortality. We argue that with the advancement of\nAI's sciences and technologies, there exist two digital me thresholds. The\nfirst threshold defines digital me having some (rudimentarily) form of\nconsciousness, free will, and intentionality. The second threshold assumes that\ndigital me is equipped with moral learning capabilities, implying that, in\nprinciple, digital me could develop their own ethics which significantly\ndiffers from human's understanding of ethics. Finally we discuss the\nimplications of digital me metaethics, normative and applied ethics, the\nimplementation of the Golden Rule in digital me-s, and we suggest two sets of\nnormative principles for digital me: consequentialist and duty based digital me\nprinciples.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14325v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11810v3",
    "title": "Progressive One-shot Human Parsing",
    "authors": [
      "Haoyu He",
      "Jing Zhang",
      "Bhavani Thuraisingham",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Prior human parsing models are limited to parsing humans into classes\npre-defined in the training data, which is not flexible to generalize to unseen\nclasses, e.g., new clothing in fashion analysis. In this paper, we propose a\nnew problem named one-shot human parsing (OSHP) that requires to parse human\ninto an open set of reference classes defined by any single reference example.\nDuring training, only base classes defined in the training set are exposed,\nwhich can overlap with part of reference classes. In this paper, we devise a\nnovel Progressive One-shot Parsing network (POPNet) to address two critical\nchallenges , i.e., testing bias and small sizes. POPNet consists of two\ncollaborative metric learning modules named Attention Guidance Module and\nNearest Centroid Module, which can learn representative prototypes for base\nclasses and quickly transfer the ability to unseen classes during testing,\nthereby reducing testing bias. Moreover, POPNet adopts a progressive human\nparsing framework that can incorporate the learned knowledge of parent classes\nat the coarse granularity to help recognize the descendant classes at the fine\ngranularity, thereby handling the small sizes issue. Experiments on the ATR-OS\nbenchmark tailored for OSHP demonstrate POPNet outperforms other representative\none-shot segmentation models by large margins and establishes a strong\nbaseline. Source code can be found at\nhttps://github.com/Charleshhy/One-shot-Human-Parsing.",
    "published_date": "2020-12-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11810v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11705v1",
    "title": "Taking Principles Seriously: A Hybrid Approach to Value Alignment",
    "authors": [
      "Tae Wan Kim",
      "John Hooker",
      "Thomas Donaldson"
    ],
    "author_ids": [],
    "abstract": "An important step in the development of value alignment (VA) systems in AI is\nunderstanding how VA can reflect valid ethical principles. We propose that\ndesigners of VA systems incorporate ethics by utilizing a hybrid approach in\nwhich both ethical reasoning and empirical observation play a role. This, we\nargue, avoids committing the \"naturalistic fallacy,\" which is an attempt to\nderive \"ought\" from \"is,\" and it provides a more adequate form of ethical\nreasoning when the fallacy is not committed. Using quantified model logic, we\nprecisely formulate principles derived from deontological ethics and show how\nthey imply particular \"test propositions\" for any given action plan in an AI\nrule base. The action plan is ethical only if the test proposition is\nempirically true, a judgment that is made on the basis of empirical VA. This\npermits empirical VA to integrate seamlessly with independently justified\nethical principles.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11619v2",
    "title": "Defence against adversarial attacks using classical and quantum-enhanced Boltzmann machines",
    "authors": [
      "Aidan Kehoe",
      "Peter Wittek",
      "Yanbo Xue",
      "Alejandro Pozas-Kerstjens"
    ],
    "author_ids": [],
    "abstract": "We provide a robust defence to adversarial attacks on discriminative\nalgorithms. Neural networks are naturally vulnerable to small, tailored\nperturbations in the input data that lead to wrong predictions. On the\ncontrary, generative models attempt to learn the distribution underlying a\ndataset, making them inherently more robust to small perturbations. We use\nBoltzmann machines for discrimination purposes as attack-resistant classifiers,\nand compare them against standard state-of-the-art adversarial defences. We\nfind improvements ranging from 5% to 72% against attacks with Boltzmann\nmachines on the MNIST dataset. We furthermore complement the training with\nquantum-enhanced sampling from the D-Wave 2000Q annealer, finding results\ncomparable with classical techniques and with marginal improvements in some\ncases. These results underline the relevance of probabilistic methods in\nconstructing neural networks and highlight a novel scenario of practical\nrelevance where quantum computers, even with limited hardware capabilites,\ncould provide advantages over classical computers. This work is dedicated to\nthe memory of Peter Wittek.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11619v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11448v1",
    "title": "The Importance of Modeling Data Missingness in Algorithmic Fairness: A Causal Perspective",
    "authors": [
      "Naman Goel",
      "Alfonso Amayuelas",
      "Amit Deshpande",
      "Amit Sharma"
    ],
    "author_ids": [],
    "abstract": "Training datasets for machine learning often have some form of missingness.\nFor example, to learn a model for deciding whom to give a loan, the available\ntraining data includes individuals who were given a loan in the past, but not\nthose who were not. This missingness, if ignored, nullifies any fairness\nguarantee of the training procedure when the model is deployed. Using causal\ngraphs, we characterize the missingness mechanisms in different real-world\nscenarios. We show conditions under which various distributions, used in\npopular fairness algorithms, can or can not be recovered from the training\ndata. Our theoretical results imply that many of these algorithms can not\nguarantee fairness in practice. Modeling missingness also helps to identify\ncorrect design principles for fair algorithms. For example, in multi-stage\nsettings where decisions are made in multiple screening rounds, we use our\nframework to derive the minimal distributions required to design a fair\nalgorithm. Our proposed algorithm decentralizes the decision-making process and\nstill achieves similar performance to the optimal algorithm that requires\ncentralization and non-recoverable distributions.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11448v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11403v2",
    "title": "CAMTA: Causal Attention Model for Multi-touch Attribution",
    "authors": [
      "Sachin Kumar",
      "Garima Gupta",
      "Ranjitha Prasad",
      "Arnab Chatterjee",
      "Lovekesh Vig",
      "Gautam Shroff"
    ],
    "author_ids": [],
    "abstract": "Advertising channels have evolved from conventional print media, billboards\nand radio advertising to online digital advertising (ad), where the users are\nexposed to a sequence of ad campaigns via social networks, display ads, search\netc. While advertisers revisit the design of ad campaigns to concurrently serve\nthe requirements emerging out of new ad channels, it is also critical for\nadvertisers to estimate the contribution from touch-points (view, clicks,\nconverts) on different channels, based on the sequence of customer actions.\nThis process of contribution measurement is often referred to as multi-touch\nattribution (MTA). In this work, we propose CAMTA, a novel deep recurrent\nneural network architecture which is a casual attribution mechanism for\nuser-personalised MTA in the context of observational data. CAMTA minimizes the\nselection bias in channel assignment across time-steps and touchpoints.\nFurthermore, it utilizes the users' pre-conversion actions in a principled way\nin order to predict pre-channel attribution. To quantitatively benchmark the\nproposed MTA model, we employ the real world Criteo dataset and demonstrate the\nsuperior performance of CAMTA with respect to prediction accuracy as compared\nto several baselines. In addition, we provide results for budget allocation and\nuser-behaviour modelling on the predicted channel attribution.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11403v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11399v2",
    "title": "The COVID-19 pandemic: socioeconomic and health disparities",
    "authors": [
      "Behzad Javaheri"
    ],
    "author_ids": [],
    "abstract": "Disadvantaged groups around the world have suffered and endured higher\nmortality during the current COVID-19 pandemic. This contrast disparity\nsuggests that socioeconomic and health-related factors may drive inequality in\ndisease outcome. To identify these factors correlated with COVID-19 outcome,\ncountry aggregate data provided by the Lancet COVID-19 Commission subjected to\ncorrelation analysis. Socioeconomic and health-related variables were used to\npredict mortality in the top 5 most affected countries using ridge regression\nand extreme gradient boosting (XGBoost) models. Our data reveal that predictors\nrelated to demographics and social disadvantage correlate with COVID-19\nmortality per million and that XGBoost performed better than ridge regression.\nTaken together, our findings suggest that the health consequence of the current\npandemic is not just confined to indiscriminate impact of a viral infection but\nthat these preventable effects are amplified based on pre-existing health and\nsocioeconomic inequalities.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11399v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11134v1",
    "title": "Learning content and context with language bias for Visual Question Answering",
    "authors": [
      "Chao Yang",
      "Su Feng",
      "Dongsheng Li",
      "Huawei Shen",
      "Guoqing Wang",
      "Bin Jiang"
    ],
    "author_ids": [],
    "abstract": "Visual Question Answering (VQA) is a challenging multimodal task to answer\nquestions about an image. Many works concentrate on how to reduce language bias\nwhich makes models answer questions ignoring visual content and language\ncontext. However, reducing language bias also weakens the ability of VQA models\nto learn context prior. To address this issue, we propose a novel learning\nstrategy named CCB, which forces VQA models to answer questions relying on\nContent and Context with language Bias. Specifically, CCB establishes Content\nand Context branches on top of a base VQA model and forces them to focus on\nlocal key content and global effective context respectively. Moreover, a joint\nloss function is proposed to reduce the importance of biased samples and retain\ntheir beneficial influence on answering questions. Experiments show that CCB\noutperforms the state-of-the-art methods in terms of accuracy on VQA-CP v2.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11134v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.11066v2",
    "title": "Fairness, Welfare, and Equity in Personalized Pricing",
    "authors": [
      "Nathan Kallus",
      "Angela Zhou"
    ],
    "author_ids": [],
    "abstract": "We study the interplay of fairness, welfare, and equity considerations in\npersonalized pricing based on customer features. Sellers are increasingly able\nto conduct price personalization based on predictive modeling of demand\nconditional on covariates: setting customized interest rates, targeted\ndiscounts of consumer goods, and personalized subsidies of scarce resources\nwith positive externalities like vaccines and bed nets. These different\napplication areas may lead to different concerns around fairness, welfare, and\nequity on different objectives: price burdens on consumers, price envy, firm\nrevenue, access to a good, equal access, and distributional consequences when\nthe good in question further impacts downstream outcomes of interest. We\nconduct a comprehensive literature review in order to disentangle these\ndifferent normative considerations and propose a taxonomy of different\nobjectives with mathematical definitions. We focus on observational metrics\nthat do not assume access to an underlying valuation distribution which is\neither unobserved due to binary feedback or ill-defined due to overriding\nbehavioral concerns regarding interpreting revealed preferences. In the setting\nof personalized pricing for the provision of goods with positive benefits, we\ndiscuss how price optimization may provide unambiguous benefit by achieving a\n\"triple bottom line\": personalized pricing enables expanding access, which in\nturn may lead to gains in welfare due to heterogeneous utility, and improve\nrevenue or budget utilization. We empirically demonstrate the potential\nbenefits of personalized pricing in two settings: pricing subsidies for an\nelective vaccine, and the effects of personalized interest rates on downstream\noutcomes in microcredit.",
    "published_date": "2020-12-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.11066v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10986v1",
    "title": "Biased Models Have Biased Explanations",
    "authors": [
      "Aditya Jain",
      "Manish Ravula",
      "Joydeep Ghosh"
    ],
    "author_ids": [],
    "abstract": "We study fairness in Machine Learning (FairML) through the lens of\nattribute-based explanations generated for machine learning models. Our\nhypothesis is: Biased Models have Biased Explanations. To establish that, we\nfirst translate existing statistical notions of group fairness and define these\nnotions in terms of explanations given by the model. Then, we propose a novel\nway of detecting (un)fairness for any black box model. We further look at\npost-processing techniques for fairness and reason how explanations can be used\nto make a bias mitigation technique more individually fair. We also introduce a\nnovel post-processing mitigation technique which increases individual fairness\nin recourse while maintaining group level fairness.",
    "published_date": "2020-12-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10986v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10931v2",
    "title": "Recent advances in deep learning theory",
    "authors": [
      "Fengxiang He",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Deep learning is usually described as an experiment-driven field under\ncontinuous criticizes of lacking theoretical foundations. This problem has been\npartially fixed by a large volume of literature which has so far not been well\norganized. This paper reviews and organizes the recent advances in deep\nlearning theory. The literature is categorized in six groups: (1) complexity\nand capacity-based approaches for analyzing the generalizability of deep\nlearning; (2) stochastic differential equations and their dynamic systems for\nmodelling stochastic gradient descent and its variants, which characterize the\noptimization and generalization of deep learning, partially inspired by\nBayesian inference; (3) the geometrical structures of the loss landscape that\ndrives the trajectories of the dynamic systems; (4) the roles of\nover-parameterization of deep neural networks from both positive and negative\nperspectives; (5) theoretical foundations of several special structures in\nnetwork architectures; and (6) the increasingly intensive concerns in ethics\nand security and their relationships with generalizability.",
    "published_date": "2020-12-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10931v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10790v1",
    "title": "Achieving Reliable Causal Inference with Data-Mined Variables: A Random Forest Approach to the Measurement Error Problem",
    "authors": [
      "Mochen Yang",
      "Edward McFowland III",
      "Gordon Burtch",
      "Gediminas Adomavicius"
    ],
    "author_ids": [],
    "abstract": "Combining machine learning with econometric analysis is becoming increasingly\nprevalent in both research and practice. A common empirical strategy involves\nthe application of predictive modeling techniques to 'mine' variables of\ninterest from available data, followed by the inclusion of those variables into\nan econometric framework, with the objective of estimating causal effects.\nRecent work highlights that, because the predictions from machine learning\nmodels are inevitably imperfect, econometric analyses based on the predicted\nvariables are likely to suffer from bias due to measurement error. We propose a\nnovel approach to mitigate these biases, leveraging the ensemble learning\ntechnique known as the random forest. We propose employing random forest not\njust for prediction, but also for generating instrumental variables to address\nthe measurement error embedded in the prediction. The random forest algorithm\nperforms best when comprised of a set of trees that are individually accurate\nin their predictions, yet which also make 'different' mistakes, i.e., have\nweakly correlated prediction errors. A key observation is that these properties\nare closely related to the relevance and exclusion requirements of valid\ninstrumental variables. We design a data-driven procedure to select tuples of\nindividual trees from a random forest, in which one tree serves as the\nendogenous covariate and the other trees serve as its instruments. Simulation\nexperiments demonstrate the efficacy of the proposed approach in mitigating\nestimation biases and its superior performance over three alternative methods\nfor bias correction.",
    "published_date": "2020-12-19T00:00:00",
    "year": 2020,
    "categories": [
      "econ.EM",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10790v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10674v2",
    "title": "Camera-aware Proxies for Unsupervised Person Re-Identification",
    "authors": [
      "Menglin Wang",
      "Baisheng Lai",
      "Jianqiang Huang",
      "Xiaojin Gong",
      "Xian-Sheng Hua"
    ],
    "author_ids": [],
    "abstract": "This paper tackles the purely unsupervised person re-identification (Re-ID)\nproblem that requires no annotations. Some previous methods adopt clustering\ntechniques to generate pseudo labels and use the produced labels to train Re-ID\nmodels progressively. These methods are relatively simple but effective.\nHowever, most clustering-based methods take each cluster as a pseudo identity\nclass, neglecting the large intra-ID variance caused mainly by the change of\ncamera views. To address this issue, we propose to split each single cluster\ninto multiple proxies and each proxy represents the instances coming from the\nsame camera. These camera-aware proxies enable us to deal with large intra-ID\nvariance and generate more reliable pseudo labels for learning. Based on the\ncamera-aware proxies, we design both intra- and inter-camera contrastive\nlearning components for our Re-ID model to effectively learn the ID\ndiscrimination ability within and across cameras. Meanwhile, a proxy-balanced\nsampling strategy is also designed, which facilitates our learning further.\nExtensive experiments on three large-scale Re-ID datasets show that our\nproposed approach outperforms most unsupervised methods by a significant\nmargin. Especially, on the challenging MSMT17 dataset, we gain $14.3\\%$ Rank-1\nand $10.2\\%$ mAP improvements when compared to the second place. Code is\navailable at: \\texttt{https://github.com/Terminator8758/CAP-master}.",
    "published_date": "2020-12-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10674v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.14285v1",
    "title": "Affirmative Algorithms: The Legal Grounds for Fairness as Awareness",
    "authors": [
      "Daniel E. Ho",
      "Alice Xiang"
    ],
    "author_ids": [],
    "abstract": "While there has been a flurry of research in algorithmic fairness, what is\nless recognized is that modern antidiscrimination law may prohibit the adoption\nof such techniques. We make three contributions. First, we discuss how such\napproaches will likely be deemed \"algorithmic affirmative action,\" posing\nserious legal risks of violating equal protection, particularly under the\nhigher education jurisprudence. Such cases have increasingly turned toward\nanticlassification, demanding \"individualized consideration\" and barring\nformal, quantitative weights for race regardless of purpose. This case law is\nhence fundamentally incompatible with fairness in machine learning. Second, we\nargue that the government-contracting cases offer an alternative grounding for\nalgorithmic fairness, as these cases permit explicit and quantitative\nrace-based remedies based on historical discrimination by the actor. Third,\nwhile limited, this doctrinal approach also guides the future of algorithmic\nfairness, mandating that adjustments be calibrated to the entity's\nresponsibility for historical discrimination causing present-day disparities.\nThe contractor cases provide a legally viable path for algorithmic fairness\nunder current constitutional doctrine but call for more research at the\nintersection of algorithmic fairness and causal inference to ensure that bias\nmitigation is tailored to specific causes and mechanisms of bias.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14285v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10424v2",
    "title": "Separation and Concentration in Deep Networks",
    "authors": [
      "John Zarka",
      "Florentin Guth",
      "Stéphane Mallat"
    ],
    "author_ids": [],
    "abstract": "Numerical experiments demonstrate that deep neural network classifiers\nprogressively separate class distributions around their mean, achieving linear\nseparability on the training set, and increasing the Fisher discriminant ratio.\nWe explain this mechanism with two types of operators. We prove that a\nrectifier without biases applied to sign-invariant tight frames can separate\nclass means and increase Fisher ratios. On the opposite, a soft-thresholding on\ntight frames can reduce within-class variabilities while preserving class\nmeans. Variance reduction bounds are proved for Gaussian mixture models. For\nimage classification, we show that separation of class means can be achieved\nwith rectified wavelet tight frames that are not learned. It defines a\nscattering transform. Learning $1 \\times 1$ convolutional tight frames along\nscattering channels and applying a soft-thresholding reduces within-class\nvariabilities. The resulting scattering network reaches the classification\naccuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no\nlearned biases.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10424v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10348v1",
    "title": "Small Business Classification By Name: Addressing Gender and Geographic Origin Biases",
    "authors": [
      "Daniel Shapiro"
    ],
    "author_ids": [],
    "abstract": "Small business classification is a difficult and important task within many\napplications, including customer segmentation. Training on small business names\nintroduces gender and geographic origin biases. A model for predicting one of\n66 business types based only upon the business name was developed in this work\n(top-1 f1-score = 60.2%). Two approaches to removing the bias from this model\nare explored: replacing given names with a placeholder token, and augmenting\nthe training data with gender-swapped examples. The results for these\napproaches is reported, and the bias in the model was reduced by hiding given\nnames from the model. However, bias reduction was accomplished at the expense\nof classification performance (top-1 f1-score = 56.6%). Augmentation of the\ntraining data with gender-swapping samples proved less effective at bias\nreduction than the name hiding approach on the evaluated dataset.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10348v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10311v1",
    "title": "Multi-characteristic Subject Selection from Biased Datasets",
    "authors": [
      "Tahereh Arabghalizi",
      "Alexandros Labrinidis"
    ],
    "author_ids": [],
    "abstract": "Subject selection plays a critical role in experimental studies, especially\nones with human subjects. Anecdotal evidence suggests that many such studies,\ndone at or near university campus settings suffer from selection bias, i.e.,\nthe too-many-college-kids-as-subjects problem. Unfortunately, traditional\nsampling techniques, when applied over biased data, will typically return\nbiased results. In this paper, we tackle the problem of multi-characteristic\nsubject selection from biased datasets. We present a constrained\noptimization-based method that finds the best possible sampling fractions for\nthe different population subgroups, based on the desired sampling fractions\nprovided by the researcher running the subject selection.We perform an\nextensive experimental study, using a variety of real datasets. Our results\nshow that our proposed method outperforms the baselines for all problem\nvariations by up to 90%.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10311v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10289v2",
    "title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection",
    "authors": [
      "Binny Mathew",
      "Punyajoy Saha",
      "Seid Muhie Yimam",
      "Chris Biemann",
      "Pawan Goyal",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "Hate speech is a challenging issue plaguing the online social media. While\nbetter models for hate speech detection are continuously being developed, there\nis little research on the bias and interpretability aspects of hate speech. In\nthis paper, we introduce HateXplain, the first benchmark hate speech dataset\ncovering multiple aspects of the issue. Each post in our dataset is annotated\nfrom three different perspectives: the basic, commonly used 3-class\nclassification (i.e., hate, offensive or normal), the target community (i.e.,\nthe community that has been the victim of hate speech/offensive speech in the\npost), and the rationales, i.e., the portions of the post on which their\nlabelling decision (as hate, offensive or normal) is based. We utilize existing\nstate-of-the-art models and observe that even models that perform very well in\nclassification do not score high on explainability metrics like model\nplausibility and faithfulness. We also observe that models, which utilize the\nhuman rationales for training, perform better in reducing unintended bias\ntowards target communities. We have made our code and dataset public at\nhttps://github.com/punyajoy/HateXplain",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10289v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10216v4",
    "title": "Fair for All: Best-effort Fairness Guarantees for Classification",
    "authors": [
      "Anilesh K. Krishnaswamy",
      "Zhihao Jiang",
      "Kangning Wang",
      "Yu Cheng",
      "Kamesh Munagala"
    ],
    "author_ids": [],
    "abstract": "Standard approaches to group-based notions of fairness, such as \\emph{parity}\nand \\emph{equalized odds}, try to equalize absolute measures of performance\nacross known groups (based on race, gender, etc.). Consequently, a group that\nis inherently harder to classify may hold back the performance on other groups;\nand no guarantees can be provided for unforeseen groups. Instead, we propose a\nfairness notion whose guarantee, on each group $g$ in a class $\\mathcal{G}$, is\nrelative to the performance of the best classifier on $g$. We apply this notion\nto broad classes of groups, in particular, where (a) $\\mathcal{G}$ consists of\nall possible groups (subsets) in the data, and (b) $\\mathcal{G}$ is more\nstreamlined.\n  For the first setting, which is akin to groups being completely unknown, we\ndevise the {\\sc PF} (Proportional Fairness) classifier, which guarantees, on\nany possible group $g$, an accuracy that is proportional to that of the optimal\nclassifier for $g$, scaled by the relative size of $g$ in the data set. Due to\nincluding all possible groups, some of which could be too complex to be\nrelevant, the worst-case theoretical guarantees here have to be proportionally\nweaker for smaller subsets.\n  For the second setting, we devise the {\\sc BeFair} (Best-effort Fair)\nframework which seeks an accuracy, on every $g \\in \\mathcal{G}$, which\napproximates that of the optimal classifier on $g$, independent of the size of\n$g$. Aiming for such a guarantee results in a non-convex problem, and we design\nnovel techniques to get around this difficulty when $\\mathcal{G}$ is the set of\nlinear hypotheses. We test our algorithms on real-world data sets, and present\ninteresting comparative insights on their performance.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10216v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10210v1",
    "title": "On Modality Bias in the TVQA Dataset",
    "authors": [
      "Thomas Winterbottom",
      "Sarah Xiao",
      "Alistair McLean",
      "Noura Al Moubayed"
    ],
    "author_ids": [],
    "abstract": "TVQA is a large scale video question answering (video-QA) dataset based on\npopular TV shows. The questions were specifically designed to require \"both\nvision and language understanding to answer\". In this work, we demonstrate an\ninherent bias in the dataset towards the textual subtitle modality. We infer\nsaid bias both directly and indirectly, notably finding that models trained\nwith subtitles learn, on-average, to suppress video feature contribution. Our\nresults demonstrate that models trained on only the visual information can\nanswer ~45% of the questions, while using only the subtitles achieves ~68%. We\nfind that a bilinear pooling based joint representation of modalities damages\nmodel performance by 9% implying a reliance on modality specific information.\nWe also show that TVQA fails to benefit from the RUBi modality bias reduction\ntechnique popularised in VQA. By simply improving text processing using BERT\nembeddings with the simple model first proposed for TVQA, we achieve\nstate-of-the-art results (72.13%) compared to the highly complex STAGE model\n(70.50%). We recommend a multimodal evaluation framework that can highlight\nbiases in models and isolate visual and textual reliant subsets of data. Using\nthis framework we propose subsets of TVQA that respond exclusively to either or\nboth modalities in order to facilitate multimodal modelling as TVQA originally\nintended.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "68T99",
      "I.2.10; I.2.7; I.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10210v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10095v1",
    "title": "A First Look at Human Values-Violation in App Reviews",
    "authors": [
      "Humphrey O. Obie",
      "Waqar Hussain",
      "Xin Xia",
      "John Grundy",
      "Li Li",
      "Burak Turhan",
      "Jon Whittle",
      "Mojtaba Shahin"
    ],
    "author_ids": [],
    "abstract": "Ubiquitous technologies such as mobile software applications (mobile apps)\nhave a tremendous influence on the evolution of the social, cultural, economic,\nand political facets of life in society. Mobile apps fulfil many practical\npurposes for users including entertainment, transportation, financial\nmanagement, etc. Given the ubiquity of mobile apps in the lives of individuals\nand the consequent effect of these technologies on society, it is essential to\nconsider the relationship between human values and the development and\ndeployment of mobile apps. The many negative consequences of violating human\nvalues such as privacy, fairness or social justice by technology have been\ndocumented in recent times. If we can detect these violations in a timely\nmanner, developers can look to better address them. To understand the violation\nof human values in a range of common mobile apps, we analysed 22,119 app\nreviews from Google Play Store using natural language processing techniques. We\nbase our values violation detection approach on a widely accepted model of\nhuman values; the Schwartz theory of basic human values. The results of our\nanalysis show that 26.5% of the reviews contained text indicating user\nperceived violations of human values. We found that benevolence and\nself-direction were the most violated value categories, and conformity and\ntradition were the least violated categories. Our results also highlight the\nneed for a proactive approach to the alignment of values amongst stakeholders\nand the use of app reviews as a valuable additional source for mining values\nrequirements.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10069v1",
    "title": "Fairness and Accuracy in Federated Learning",
    "authors": [
      "Wei Huang",
      "Tianrui Li",
      "Dexian Wang",
      "Shengdong Du",
      "Junbo Zhang"
    ],
    "author_ids": [],
    "abstract": "In the federated learning setting, multiple clients jointly train a model\nunder the coordination of the central server, while the training data is kept\non the client to ensure privacy. Normally, inconsistent distribution of data\nacross different devices in a federated network and limited communication\nbandwidth between end devices impose both statistical heterogeneity and\nexpensive communication as major challenges for federated learning. This paper\nproposes an algorithm to achieve more fairness and accuracy in federated\nlearning (FedFa). It introduces an optimization scheme that employs a double\nmomentum gradient, thereby accelerating the convergence rate of the model. An\nappropriate weight selection algorithm that combines the information quantity\nof training accuracy and training frequency to measure the weights is proposed.\nThis procedure assists in addressing the issue of unfairness in federated\nlearning due to preferences for certain clients. Our results show that the\nproposed FedFa algorithm outperforms the baseline algorithm in terms of\naccuracy and fairness.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10069v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09995v2",
    "title": "Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies",
    "authors": [
      "Nicholas Vincent",
      "Hanlin Li",
      "Nicole Tilly",
      "Stevie Chancellor",
      "Brent Hecht"
    ],
    "author_ids": [],
    "abstract": "Many powerful computing technologies rely on implicit and explicit data\ncontributions from the public. This dependency suggests a potential source of\nleverage for the public in its relationship with technology companies: by\nreducing, stopping, redirecting, or otherwise manipulating data contributions,\nthe public can reduce the effectiveness of many lucrative technologies. In this\npaper, we synthesize emerging research that seeks to better understand and help\npeople action this \\textit{data leverage}. Drawing on prior work in areas\nincluding machine learning, human-computer interaction, and fairness and\naccountability in computing, we present a framework for understanding data\nleverage that highlights new opportunities to change technology company\nbehavior related to privacy, economic inequality, content moderation and other\nareas of societal concern. Our framework also points towards ways that\npolicymakers can bolster data leverage as a means of changing the balance of\npower between the public and tech companies.",
    "published_date": "2020-12-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09995v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09951v1",
    "title": "Fairkit, Fairkit, on the Wall, Who's the Fairest of Them All? Supporting Data Scientists in Training Fair Models",
    "authors": [
      "Brittany Johnson",
      "Jesse Bartola",
      "Rico Angell",
      "Katherine Keith",
      "Sam Witty",
      "Stephen J. Giguere",
      "Yuriy Brun"
    ],
    "author_ids": [],
    "abstract": "Modern software relies heavily on data and machine learning, and affects\ndecisions that shape our world. Unfortunately, recent studies have shown that\nbecause of biases in data, software systems frequently inject bias into their\ndecisions, from producing better closed caption transcriptions of men's voices\nthan of women's voices to overcharging people of color for financial loans. To\naddress bias in machine learning, data scientists need tools that help them\nunderstand the trade-offs between model quality and fairness in their specific\ndata domains. Toward that end, we present fairkit-learn, a toolkit for helping\ndata scientists reason about and understand fairness. Fairkit-learn works with\nstate-of-the-art machine learning tools and uses the same interfaces to ease\nadoption. It can evaluate thousands of models produced by multiple machine\nlearning algorithms, hyperparameters, and data permutations, and compute and\nvisualize a small Pareto-optimal set of models that describe the optimal\ntrade-offs between fairness and quality. We evaluate fairkit-learn via a user\nstudy with 54 students, showing that students using fairkit-learn produce\nmodels that provide a better balance between fairness and quality than students\nusing scikit-learn and IBM AI Fairness 360 toolkits. With fairkit-learn, users\ncan select models that are up to 67% more fair and 10% more accurate than the\nmodels they are likely to train with scikit-learn.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09951v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09935v3",
    "title": "Increasing the efficiency of randomized trial estimates via linear adjustment for a prognostic score",
    "authors": [
      "Alejandro Schuler",
      "David Walsh",
      "Diana Hall",
      "Jon Walsh",
      "Charles Fisher"
    ],
    "author_ids": [],
    "abstract": "Estimating causal effects from randomized experiments is central to clinical\nresearch. Reducing the statistical uncertainty in these analyses is an\nimportant objective for statisticians. Registries, prior trials, and health\nrecords constitute a growing compendium of historical data on patients under\nstandard-of-care that may be exploitable to this end. However, most methods for\nhistorical borrowing achieve reductions in variance by sacrificing strict\ntype-I error rate control. Here, we propose a use of historical data that\nexploits linear covariate adjustment to improve the efficiency of trial\nanalyses without incurring bias. Specifically, we train a prognostic model on\nthe historical data, then estimate the treatment effect using a linear\nregression while adjusting for the trial subjects' predicted outcomes (their\nprognostic scores). We prove that, under certain conditions, this prognostic\ncovariate adjustment procedure attains the minimum variance possible among a\nlarge class of estimators. When those conditions are not met, prognostic\ncovariate adjustment is still more efficient than raw covariate adjustment and\nthe gain in efficiency is proportional to a measure of the predictive accuracy\nof the prognostic model above and beyond the linear relationship with the raw\ncovariates. We demonstrate the approach using simulations and a reanalysis of\nan Alzheimer's Disease clinical trial and observe meaningful reductions in\nmean-squared error and the estimated variance. Lastly, we provide a simplified\nformula for asymptotic variance that enables power calculations that account\nfor these gains. Sample size reductions between 10% and 30% are attainable when\nusing prognostic models that explain a clinically realistic percentage of the\noutcome variance.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09935v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09841v3",
    "title": "Taming Transformers for High-Resolution Image Synthesis",
    "authors": [
      "Patrick Esser",
      "Robin Rombach",
      "Björn Ommer"
    ],
    "author_ids": [],
    "abstract": "Designed to learn long-range interactions on sequential data, transformers\ncontinue to show state-of-the-art results on a wide variety of tasks. In\ncontrast to CNNs, they contain no inductive bias that prioritizes local\ninteractions. This makes them expressive, but also computationally infeasible\nfor long sequences, such as high-resolution images. We demonstrate how\ncombining the effectiveness of the inductive bias of CNNs with the expressivity\nof transformers enables them to model and thereby synthesize high-resolution\nimages. We show how to (i) use CNNs to learn a context-rich vocabulary of image\nconstituents, and in turn (ii) utilize transformers to efficiently model their\ncomposition within high-resolution images. Our approach is readily applied to\nconditional synthesis tasks, where both non-spatial information, such as object\nclasses, and spatial information, such as segmentations, can control the\ngenerated image. In particular, we present the first results on\nsemantically-guided synthesis of megapixel images with transformers and obtain\nthe state of the art among autoregressive models on class-conditional ImageNet.\nCode and pretrained models can be found at\nhttps://github.com/CompVis/taming-transformers .",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09841v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09839v2",
    "title": "Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning",
    "authors": [
      "Zhiyuan Li",
      "Yuping Luo",
      "Kaifeng Lyu"
    ],
    "author_ids": [],
    "abstract": "Matrix factorization is a simple and natural test-bed to investigate the\nimplicit regularization of gradient descent. Gunasekar et al. (2017)\nconjectured that Gradient Flow with infinitesimal initialization converges to\nthe solution that minimizes the nuclear norm, but a series of recent papers\nargued that the language of norm minimization is not sufficient to give a full\ncharacterization for the implicit regularization. In this work, we provide\ntheoretical and empirical evidence that for depth-2 matrix factorization,\ngradient flow with infinitesimal initialization is mathematically equivalent to\na simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under\nsome reasonable assumptions. This generalizes the rank minimization view from\nprevious works to a much broader setting and enables us to construct\ncounter-examples to refute the conjecture from Gunasekar et al. (2017). We also\nextend the results to the case where depth $\\ge 3$, and we show that the\nbenefit of being deeper is that the above convergence has a much weaker\ndependence over initialization magnitude so that this rank minimization is more\nlikely to take effect for initialization with practical scale.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09839v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09699v2",
    "title": "A Generalization of Transformer Networks to Graphs",
    "authors": [
      "Vijay Prakash Dwivedi",
      "Xavier Bresson"
    ],
    "author_ids": [],
    "abstract": "We propose a generalization of transformer neural network architecture for\narbitrary graphs. The original transformer was designed for Natural Language\nProcessing (NLP), which operates on fully connected graphs representing all\nconnections between the words in a sequence. Such architecture does not\nleverage the graph connectivity inductive bias, and can perform poorly when the\ngraph topology is important and has not been encoded into the node features. We\nintroduce a graph transformer with four new properties compared to the standard\nmodel. First, the attention mechanism is a function of the neighborhood\nconnectivity for each node in the graph. Second, the positional encoding is\nrepresented by the Laplacian eigenvectors, which naturally generalize the\nsinusoidal positional encodings often used in NLP. Third, the layer\nnormalization is replaced by a batch normalization layer, which provides faster\ntraining and better generalization performance. Finally, the architecture is\nextended to edge feature representation, which can be critical to tasks s.a.\nchemistry (bond type) or link prediction (entity relationship in knowledge\ngraphs). Numerical experiments on a graph benchmark demonstrate the performance\nof the proposed graph transformer architecture. This work closes the gap\nbetween the original transformer, which was designed for the limited case of\nline graphs, and graph neural networks, that can work with arbitrary graphs. As\nour architecture is simple and generic, we believe it can be used as a black\nbox for future applications that wish to consider transformer and graphs.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09699v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09689v1",
    "title": "Trajectory Planning Under Stochastic and Bounded Sensing Uncertainties Using Reachability Analysis",
    "authors": [
      "Akshay Shetty",
      "Grace Xingxin Gao"
    ],
    "author_ids": [],
    "abstract": "Trajectory planning under uncertainty is an active research topic. Previous\nworks predict state and state estimation uncertainties along trajectories to\ncheck for collision safety. They assume either stochastic or bounded sensing\nuncertainties. However, GNSS pseudoranges are typically modeled to contain\nstochastic uncertainties with additional biases in urban environments. Thus,\ngiven bounds for the bias, the planner needs to account for both stochastic and\nbounded sensing uncertainties. In our prior work we presented a reachability\nanalysis to predict state and state estimation uncertainties under stochastic\nand bounded uncertainties. However, we ignored the correlation between these\nuncertainties, leading to an imperfect approximation of the state uncertainty.\nIn this paper we improve our reachability analysis by predicting state\nuncertainty as a function of independent quantities. We design a metric for the\npredicted uncertainty to compare candidate trajectories during planning.\nFinally, we validate the planner for GNSS-based urban navigation of fixed-wing\nUAS.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09689v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.09496v1",
    "title": "Exploiting Learnable Joint Groups for Hand Pose Estimation",
    "authors": [
      "Moran Li",
      "Yuan Gao",
      "Nong Sang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose to estimate 3D hand pose by recovering the 3D\ncoordinates of joints in a group-wise manner, where less-related joints are\nautomatically categorized into different groups and exhibit different features.\nThis is different from the previous methods where all the joints are considered\nholistically and share the same feature. The benefits of our method are\nillustrated by the principle of multi-task learning (MTL), i.e., by separating\nless-related joints into different groups (as different tasks), our method\nlearns different features for each of them, therefore efficiently avoids the\nnegative transfer (among less related tasks/groups of joints). The key of our\nmethod is a novel binary selector that automatically selects related joints\ninto the same group. We implement such a selector with binary values\nstochastically sampled from a Concrete distribution, which is constructed using\nGumbel softmax on trainable parameters. This enables us to preserve the\ndifferentiable property of the whole network. We further exploit features from\nthose less-related groups by carrying out an additional feature fusing scheme\namong them, to learn more discriminative features. This is realized by\nimplementing multiple 1x1 convolutions on the concatenated features, where each\njoint group contains a unique 1x1 convolution for feature fusion. The detailed\nablation analysis and the extensive experiments on several benchmark datasets\ndemonstrate the promising performance of the proposed method over the\nstate-of-the-art (SOTA) methods. Besides, our method achieves top-1 among all\nthe methods that do not exploit the dense 3D shape labels on the most recently\nreleased FreiHAND competition at the submission date. The source code and\nmodels are available at https://github.com/ moranli-aca/LearnableGroups-Hand.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09496v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.10231v5",
    "title": "Controlling conditional expectations by zero-determinant strategies",
    "authors": [
      "Masahiko Ueda"
    ],
    "author_ids": [],
    "abstract": "Zero-determinant strategies are memory-one strategies in repeated games which\nunilaterally enforce linear relations between expected payoffs of players.\nRecently, the concept of zero-determinant strategies was extended to the class\nof memory-$n$ strategies with $n\\geq 1$, which enables more complicated control\nof payoffs by one player. However, what we can do by memory-$n$\nzero-determinant strategies is still not clear. Here, we show that memory-$n$\nzero-determinant strategies in repeated games can be used to control\nconditional expectations of payoffs. Equivalently, they can be used to control\nexpected payoffs in biased ensembles, where a history of action profiles with\nlarge value of bias function is more weighted. Controlling conditional\nexpectations of payoffs is useful for strengthening zero-determinant\nstrategies, because players can choose conditions in such a way that only\nunfavorable action profiles to one player are contained in the conditions. We\nprovide several examples of memory-$n$ zero-determinant strategies in the\nrepeated prisoner's dilemma game. We also explain that a deformed version of\nzero-determinant strategies is easily extended to the memory-$n$ case.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.GT",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.10231v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.09421v4",
    "title": "Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Matthieu Zimmer",
      "Claire Glanois",
      "Umer Siddique",
      "Paul Weng"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of learning fair policies in (deep) cooperative\nmulti-agent reinforcement learning (MARL). We formalize it in a principled way\nas the problem of optimizing a welfare function that explicitly encodes two\nimportant aspects of fairness: efficiency and equity. As a solution method, we\npropose a novel neural network architecture, which is composed of two\nsub-networks specifically designed for taking into account the two aspects of\nfairness. In experiments, we demonstrate the importance of the two sub-networks\nfor fair optimization. Our overall approach is general as it can accommodate\nany (sub)differentiable welfare function. Therefore, it is compatible with\nvarious notions of fairness that have been proposed in the literature (e.g.,\nlexicographic maximin, generalized Gini social welfare function, proportional\nfairness). Our solution method is generic and can be implemented in various\nMARL settings: centralized training and decentralized execution, or fully\ndecentralized. Finally, we experimentally validate our approach in various\ndomains and show that it can perform much better than previous methods.",
    "published_date": "2020-12-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09421v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09056v1",
    "title": "Beyond kinetic harm and towards a dynamic conceptualization of cyberterrorism",
    "authors": [
      "Vince J. Straub"
    ],
    "author_ids": [],
    "abstract": "After more than two decades of discussion, the concept of cyberterrorism\nremains plagued by confusion. This article presents the result of an\nintegrative review which maps the development of the term and situates the\nepistemic communities that have shaped the debate. After critically assessing\nexisting accounts and highlighting the key ethical, social, and legal\ndimensions at stake in preventing cyberterrorist attacks, it calls for a more\ndynamic conceptualization that views cyberterrorism as more abstract, difficult\nto predict, and hard to isolate; and which embraces a different conception of\nsufficient harm. In concluding it proposes a novel definition of\ncyberterrorism, intended to catalyse a new research programme, and sketches a\nroadmap for further research.",
    "published_date": "2020-12-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09056v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.09014v1",
    "title": "I3DOL: Incremental 3D Object Learning without Catastrophic Forgetting",
    "authors": [
      "Jiahua Dong",
      "Yang Cong",
      "Gan Sun",
      "Bingtao Ma",
      "Lichen Wang"
    ],
    "author_ids": [],
    "abstract": "3D object classification has attracted appealing attentions in academic\nresearches and industrial applications. However, most existing methods need to\naccess the training data of past 3D object classes when facing the common\nreal-world scenario: new classes of 3D objects arrive in a sequence. Moreover,\nthe performance of advanced approaches degrades dramatically for past learned\nclasses (i.e., catastrophic forgetting), due to the irregular and redundant\ngeometric structures of 3D point cloud data. To address these challenges, we\npropose a new Incremental 3D Object Learning (i.e., I3DOL) model, which is the\nfirst exploration to learn new classes of 3D object continually. Specifically,\nan adaptive-geometric centroid module is designed to construct discriminative\nlocal geometric structures, which can better characterize the irregular point\ncloud representation for 3D object. Afterwards, to prevent the catastrophic\nforgetting brought by redundant geometric information, a geometric-aware\nattention mechanism is developed to quantify the contributions of local\ngeometric structures, and explore unique 3D geometric characteristics with high\ncontributions for classes incremental learning. Meanwhile, a score fairness\ncompensation strategy is proposed to further alleviate the catastrophic\nforgetting caused by unbalanced data between past and new classes of 3D object,\nby compensating biased prediction for new classes in the validation phase.\nExperiments on 3D representative datasets validate the superiority of our I3DOL\nframework.",
    "published_date": "2020-12-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.08895v1",
    "title": "ReINTEL: A Multimodal Data Challenge for Responsible Information Identification on Social Network Sites",
    "authors": [
      "Duc-Trong Le",
      "Xuan-Son Vu",
      "Nhu-Dung To",
      "Huu-Quang Nguyen",
      "Thuy-Trinh Nguyen",
      "Linh Le",
      "Anh-Tuan Nguyen",
      "Minh-Duc Hoang",
      "Nghia Le",
      "Huyen Nguyen",
      "Hoang D. Nguyen"
    ],
    "author_ids": [],
    "abstract": "This paper reports on the ReINTEL Shared Task for Responsible Information\nIdentification on social network sites, which is hosted at the seventh annual\nworkshop on Vietnamese Language and Speech Processing (VLSP 2020). Given a\npiece of news with respective textual, visual content and metadata,\nparticipants are required to classify whether the news is `reliable' or\n`unreliable'. In order to generate a fair benchmark, we introduce a novel\nhuman-annotated dataset of over 10,000 news collected from a social network in\nVietnam. All models will be evaluated in terms of AUC-ROC score, a typical\nevaluation metric for classification. The competition was run on the Codalab\nplatform. Within two months, the challenge has attracted over 60 participants\nand recorded nearly 1,000 submission entries.",
    "published_date": "2020-12-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08895v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2101.03126v1",
    "title": "piSAAC: Extended notion of SAAC feature selection novel method for discrimination of Enzymes model using different machine learning algorithm",
    "authors": [
      "Zaheer Ullah Khan",
      "Dechang Pi",
      "Izhar Ahmed Khan",
      "Asif Nawaz",
      "Jamil Ahmad",
      "Mushtaq Hussain"
    ],
    "author_ids": [],
    "abstract": "Enzymes and proteins are live driven biochemicals, which has a dramatic\nimpact over the environment, in which it is active. So, therefore, it is highly\nlooked-for to build such a robust and highly accurate automatic and\ncomputational model to accurately predict enzymes nature. In this study, a\nnovel split amino acid composition model named piSAAC is proposed. In this\nmodel, protein sequence is discretized in equal and balanced terminus to fully\nevaluate the intrinsic correlation properties of the sequence. Several\nstate-of-the-art algorithms have been employed to evaluate the proposed model.\nA 10-folds cross-validation evaluation is used for finding out the authenticity\nand robust-ness of the model using different statistical measures e.g.\nAccuracy, sensitivity, specificity, F-measure and area un-der ROC curve. The\nexperimental results show that, probabilistic neural network algorithm with\npiSAAC feature extraction yields an accuracy of 98.01%, sensitivity of 97.12%,\nspecificity of 95.87%, f-measure of 0.9812and AUC 0.95812, over dataset S1,\naccuracy of 97.85%, sensitivity of 97.54%, specificity of 96.24%, f-measure of\n0.9774 and AUC 0.9803 over dataset S2. Evident from these excellent empirical\nresults, the proposed model would be a very useful tool for academic research\nand drug designing related application areas.",
    "published_date": "2020-12-16T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.03126v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.08723v1",
    "title": "Exacerbating Algorithmic Bias through Fairness Attacks",
    "authors": [
      "Ninareh Mehrabi",
      "Muhammad Naveed",
      "Fred Morstatter",
      "Aram Galstyan"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has attracted significant attention in recent years,\nwith many quantitative measures suggested for characterizing the fairness of\ndifferent machine learning algorithms. Despite this interest, the robustness of\nthose fairness measures with respect to an intentional adversarial attack has\nnot been properly addressed. Indeed, most adversarial machine learning has\nfocused on the impact of malicious attacks on the accuracy of the system,\nwithout any regard to the system's fairness. We propose new types of data\npoisoning attacks where an adversary intentionally targets the fairness of a\nsystem. Specifically, we propose two families of attacks that target fairness\nmeasures. In the anchoring attack, we skew the decision boundary by placing\npoisoned points near specific target points to bias the outcome. In the\ninfluence attack on fairness, we aim to maximize the covariance between the\nsensitive attributes and the decision outcome and affect the fairness of the\nmodel. We conduct extensive experiments that indicate the effectiveness of our\nproposed attacks.",
    "published_date": "2020-12-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.08668v3",
    "title": "Mitigating Bias in Calibration Error Estimation",
    "authors": [
      "Rebecca Roelofs",
      "Nicholas Cain",
      "Jonathon Shlens",
      "Michael C. Mozer"
    ],
    "author_ids": [],
    "abstract": "For an AI system to be reliable, the confidence it expresses in its decisions\nmust match its accuracy. To assess the degree of match, examples are typically\nbinned by confidence and the per-bin mean confidence and accuracy are compared.\nMost research in calibration focuses on techniques to reduce this empirical\nmeasure of calibration error, ECE_bin. We instead focus on assessing\nstatistical bias in this empirical measure, and we identify better estimators.\nWe propose a framework through which we can compute the bias of a particular\nestimator for an evaluation data set of a given size. The framework involves\nsynthesizing model outputs that have the same statistics as common neural\narchitectures on popular data sets. We find that binning-based estimators with\nbins of equal mass (number of instances) have lower bias than estimators with\nbins of equal width. Our results indicate two reliable calibration-error\nestimators: the debiased estimator (Brocker, 2012; Ferro and Fricker, 2012) and\na method we propose, ECE_sweep, which uses equal-mass bins and chooses the\nnumber of bins to be as large as possible while preserving monotonicity in the\ncalibration function. With these estimators, we observe improvements in the\neffectiveness of recalibration methods and in the detection of model\nmiscalibration.",
    "published_date": "2020-12-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08668v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.08648v1",
    "title": "Online Learning Demands in Max-min Fairness",
    "authors": [
      "Kirthevasan Kandasamy",
      "Gur-Eyal Sela",
      "Joseph E Gonzalez",
      "Michael I Jordan",
      "Ion Stoica"
    ],
    "author_ids": [],
    "abstract": "We describe mechanisms for the allocation of a scarce resource among multiple\nusers in a way that is efficient, fair, and strategy-proof, but when users do\nnot know their resource requirements. The mechanism is repeated for multiple\nrounds and a user's requirements can change on each round. At the end of each\nround, users provide feedback about the allocation they received, enabling the\nmechanism to learn user preferences over time. Such situations are common in\nthe shared usage of a compute cluster among many users in an organisation,\nwhere all teams may not precisely know the amount of resources needed to\nexecute their jobs. By understating their requirements, users will receive less\nthan they need and consequently not achieve their goals. By overstating them,\nthey may siphon away precious resources that could be useful to others in the\norganisation. We formalise this task of online learning in fair division via\nnotions of efficiency, fairness, and strategy-proofness applicable to this\nsetting, and study this problem under three types of feedback: when the users'\nobservations are deterministic, when they are stochastic and follow a\nparametric model, and when they are stochastic and nonparametric. We derive\nmechanisms inspired by the classical max-min fairness procedure that achieve\nthese requisites, and quantify the extent to which they are achieved via\nasymptotic rates. We corroborate these insights with an experimental evaluation\non synthetic problems and a web-serving task.",
    "published_date": "2020-12-15T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08648v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.08571v1",
    "title": "Modernizing Data Control: Making Personal Digital Data Mutually Beneficial for Citizens and Industry",
    "authors": [
      "Sujata Banerjee",
      "Yiling Chen",
      "Kobbi Nissim",
      "David Parkes",
      "Katie Siek",
      "Lauren Wilcox"
    ],
    "author_ids": [],
    "abstract": "We are entering a new \"data everywhere-anytime\" era that pivots us from being\ntracked online to continuous tracking as we move through our everyday lives. We\nhave smart devices in our homes, on our bodies, and around our communities that\ncollect data that is used to guide decisions that have a major impact on our\nlives - from loans to job interviews and judicial rulings to health care\ninterventions. We create a lot of data, but who owns that data? How is it\nshared? How will it be used? While the average person does not have a good\nunderstanding of how the data is being used, they know that it carries risks\nfor them and society.\n  Although some people may believe they own their data, in reality, the problem\nof understanding the myriad ways in which data is collected, shared, and used,\nand the consequences of these uses is so complex that only a few people want to\nmanage their data themselves. Furthermore, much of the value in the data cannot\nbe extracted by individuals alone, as it lies in the connections and insights\ngarnered from (1) one's own personal data (is your fitness improving? Is your\nhome more energy efficient than the average home of this size?) and (2) one's\nrelationship with larger groups (demographic group voting blocks; friend\nnetwork influence on purchasing). But sometimes these insights have unintended\nconsequences for the person generating the data, especially in terms of loss of\nprivacy, unfairness, inappropriate inferences, information bias, manipulation,\nand discrimination. There are also societal impacts, such as effects on speech\nfreedoms, political manipulation, and amplified harms to weakened and\nunderrepresented communities. To this end, we look at major questions that\npolicymakers should ask and things to consider when addressing these data\nownership concerns.",
    "published_date": "2020-12-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08571v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.09109v1",
    "title": "Big Data",
    "authors": [
      "Andreas L Opdahl",
      "Vimala Nunavath"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things, crowdsourcing, social media, public authorities, and\nother sources generate bigger and bigger data sets. Big and open data offers\nmany benefits for emergency management, but also pose new challenges. This\nchapter will review the sources of big data and their characteristics. We then\ndiscuss potential benefits of big data for emergency management along with the\ntechnological and societal challenges it poses. We review central technologies\nfor big-data storage and processing in general, before presenting the Spark\nbig-data engine in more detail. Finally, we review ethical and societal threats\nthat big data pose.",
    "published_date": "2020-12-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09109v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.08313v1",
    "title": "On Fairness in Voting Consensus Protocols",
    "authors": [
      "Sebastian Müller",
      "Andreas Penzkofer",
      "Darcy Camargo",
      "Olivia Saa"
    ],
    "author_ids": [],
    "abstract": "Voting algorithms have been widely used as consensus protocols in the\nrealization of fault-tolerant systems. These algorithms are best suited for\ndistributed systems of nodes with low computational power or heterogeneous\nnetworks, where different nodes may have different levels of reputation or\nweight. Our main contribution is the construction of a fair voting protocol in\nthe sense that the influence of the eventual outcome of a given participant is\nlinear in its weight. Specifically, the fairness property guarantees that any\nnode can actively participate in the consensus finding even with low resources\nor weight. We investigate effects that may arise from weighted voting, such as\nloss of anonymity, centralization, scalability, and discuss their relevance to\nprotocol design and implementation.",
    "published_date": "2020-12-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08313v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.08255v1",
    "title": "Robust Multi-Agent Reinforcement Learning with Social Empowerment for Coordination and Communication",
    "authors": [
      "T. van der Heiden",
      "C. Salge",
      "E. Gavves",
      "H. van Hoof"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of robust multi-agent reinforcement learning (MARL)\nfor cooperative communication and coordination tasks. MARL agents, mainly those\ntrained in a centralized way, can be brittle because they can adopt policies\nthat act under the expectation that other agents will act a certain way rather\nthan react to their actions. Our objective is to bias the learning process\ntowards finding strategies that remain reactive towards others' behavior.\nSocial empowerment measures the potential influence between agents' actions. We\npropose it as an additional reward term, so agents better adapt to other\nagents' actions. We show that the proposed method results in obtaining higher\nrewards faster and a higher success rate in three cooperative communication and\ncoordination tasks.",
    "published_date": "2020-12-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.08255v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07978v1",
    "title": "Model Choices Influence Attributive Word Associations: A Semi-supervised Analysis of Static Word Embeddings",
    "authors": [
      "Geetanjali Bihani",
      "Julia Taylor Rayz"
    ],
    "author_ids": [],
    "abstract": "Static word embeddings encode word associations, extensively utilized in\ndownstream NLP tasks. Although prior studies have discussed the nature of such\nword associations in terms of biases and lexical regularities captured, the\nvariation in word associations based on the embedding training procedure\nremains in obscurity. This work aims to address this gap by assessing\nattributive word associations across five different static word embedding\narchitectures, analyzing the impact of the choice of the model architecture,\ncontext learning flavor and training corpora. Our approach utilizes a\nsemi-supervised clustering method to cluster annotated proper nouns and\nadjectives, based on their word embedding features, revealing underlying\nattributive word associations formed in the embedding space, without\nintroducing any confirmation bias. Our results reveal that the choice of the\ncontext learning flavor during embedding training (CBOW vs skip-gram) impacts\nthe word association distinguishability and word embeddings' sensitivity to\ndeviations in the training corpora. Moreover, it is empirically shown that even\nwhen trained over the same corpora, there is significant inter-model disparity\nand intra-model similarity in the encoded word associations across different\nword embedding models, portraying specific patterns in the way the embedding\nspace is created for each embedding architecture.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07978v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07936v1",
    "title": "Minimum Robust Multi-Submodular Cover for Fairness",
    "authors": [
      "Lan N. Nguyen",
      "My T. Thai"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study a novel problem, Minimum Robust Multi-Submodular\nCover for Fairness (MinRF), as follows: given a ground set $V$; $m$ monotone\nsubmodular functions $f_1,...,f_m$; $m$ thresholds $T_1,...,T_m$ and a\nnon-negative integer $r$, MinRF asks for the smallest set $S$ such that for all\n$i \\in [m]$, $\\min_{|X| \\leq r} f_i(S \\setminus X) \\geq T_i$. We prove that\nMinRF is inapproximable within $(1-\\epsilon)\\ln m$; and no algorithm, taking\nfewer than exponential number of queries in term of $r$, is able to output a\nfeasible set to MinRF with high certainty. Three bicriteria approximation\nalgorithms with performance guarantees are proposed: one for $r=0$, one for\n$r=1$, and one for general $r$. We further investigate our algorithms'\nperformance in two applications of MinRF, Information Propagation for Multiple\nGroups and Movie Recommendation for Multiple Users. Our algorithms have shown\nto outperform baseline heuristics in both solution quality and the number of\nqueries in most cases.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07936v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.09110v4",
    "title": "Developing Future Human-Centered Smart Cities: Critical Analysis of Smart City Security, Interpretability, and Ethical Challenges",
    "authors": [
      "Kashif Ahmad",
      "Majdi Maabreh",
      "Mohamed Ghaly",
      "Khalil Khan",
      "Junaid Qadir",
      "Ala Al-Fuqaha"
    ],
    "author_ids": [],
    "abstract": "As the globally increasing population drives rapid urbanisation in various\nparts of the world, there is a great need to deliberate on the future of the\ncities worth living. In particular, as modern smart cities embrace more and\nmore data-driven artificial intelligence services, it is worth remembering that\ntechnology can facilitate prosperity, wellbeing, urban livability, or social\njustice, but only when it has the right analog complements (such as\nwell-thought out policies, mature institutions, responsible governance); and\nthe ultimate objective of these smart cities is to facilitate and enhance human\nwelfare and social flourishing. Researchers have shown that various\ntechnological business models and features can in fact contribute to social\nproblems such as extremism, polarization, misinformation, and Internet\naddiction. In the light of these observations, addressing the philosophical and\nethical questions involved in ensuring the security, safety, and\ninterpretability of such AI algorithms that will form the technological bedrock\nof future cities assumes paramount importance. Globally there are calls for\ntechnology to be made more humane and human-centered. In this paper, we analyze\nand explore key challenges including security, robustness, interpretability,\nand ethical (data and algorithmic) challenges to a successful deployment of AI\nin human-centric applications, with a particular emphasis on the convergence of\nthese concepts/challenges. We provide a detailed review of existing literature\non these key challenges and analyze how one of these challenges may lead to\nothers or help in solving other challenges. The paper also advises on the\ncurrent limitations, pitfalls, and future directions of research in these\ndomains, and how it can fill the current gaps and lead to better solutions. We\nbelieve such rigorous analysis will provide a baseline for future research in\nthe domain.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09110v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07785v3",
    "title": "Noisy Linear Convergence of Stochastic Gradient Descent for CV@R Statistical Learning under Polyak-Łojasiewicz Conditions",
    "authors": [
      "Dionysios S. Kalogerias"
    ],
    "author_ids": [],
    "abstract": "Conditional Value-at-Risk ($\\mathrm{CV@R}$) is one of the most popular\nmeasures of risk, which has been recently considered as a performance criterion\nin supervised statistical learning, as it is related to desirable operational\nfeatures in modern applications, such as safety, fairness, distributional\nrobustness, and prediction error stability. However, due to its variational\ndefinition, $\\mathrm{CV@R}$ is commonly believed to result in difficult\noptimization problems, even for smooth and strongly convex loss functions. We\ndisprove this statement by establishing noisy (i.e., fixed-accuracy) linear\nconvergence of stochastic gradient descent for sequential $\\mathrm{CV@R}$\nlearning, for a large class of not necessarily strongly-convex (or even convex)\nloss functions satisfying a set-restricted Polyak-Lojasiewicz inequality. This\nclass contains all smooth and strongly convex losses, confirming that classical\nproblems, such as linear least squares regression, can be solved efficiently\nunder the $\\mathrm{CV@R}$ criterion, just as their risk-neutral versions. Our\nresults are illustrated numerically on such a risk-aware ridge regression task,\nalso verifying their validity in practice.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SP",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07785v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07680v1",
    "title": "Fair and Efficient Allocations under Lexicographic Preferences",
    "authors": [
      "Hadi Hosseini",
      "Sujoy Sikdar",
      "Rohit Vaish",
      "Lirong Xia"
    ],
    "author_ids": [],
    "abstract": "Envy-freeness up to any good (EFX) provides a strong and intuitive guarantee\nof fairness in the allocation of indivisible goods. But whether such\nallocations always exist or whether they can be efficiently computed remains an\nimportant open question. We study the existence and computation of EFX in\nconjunction with various other economic properties under lexicographic\npreferences--a well-studied preference model in artificial intelligence and\neconomics. In sharp contrast to the known results for additive valuations, we\nnot only prove the existence of EFX and Pareto optimal allocations, but in fact\nprovide an algorithmic characterization of these two properties. We also\ncharacterize the mechanisms that are, in addition, strategyproof, non-bossy,\nand neutral. When the efficiency notion is strengthened to rank-maximality, we\nobtain non-existence and computational hardness results, and show that\ntractability can be restored when EFX is relaxed to another well-studied\nfairness notion called maximin share guarantee (MMS).",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07680v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07555v1",
    "title": "Successive Projection for Solving Systems of Nonlinear Equations/Inequalities",
    "authors": [
      "Wen-Jun Zeng",
      "Jieping Ye"
    ],
    "author_ids": [],
    "abstract": "Solving large-scale systems of nonlinear equations/inequalities is a\nfundamental problem in computing and optimization. In this paper, we propose a\ngeneric successive projection (SP) framework for this problem. The SP\nsequentially projects the current iterate onto the constraint set corresponding\nto each nonlinear (in)equality. It extends von Neumann's alternating projection\nfor finding a point in the intersection of two linear subspaces, Bregman's\nmethod for finding a common point of convex sets and the Kaczmarz method for\nsolving systems of linear equations to the more general case of multiple\nnonlinear and nonconvex sets. The existing convergence analyses on randomized\nKaczmarz are merely applicable to linear case. There are no theoretical\nconvergence results of the SP for solving nonlinear equations. This paper\npresents the first proof that the SP locally converges to a solution of\nnonlinear equations/inequalities at a linear rate. Our work establishes the\nconvergence theory of the SP for the case of multiple nonlinear and nonconvex\nsets. Besides cyclic and randomized projections, we devise two new greedy\nprojection approaches that significantly accelerate the convergence.\nFurthermore, the theoretical bounds of the convergence rates are derived. We\nreveal that the convergence rates are related to the Hoffman constants of the\nJacobian matrix of the nonlinear functions at the solution. Applying the SP to\nsolve the graph realization problem, which attracts much attention in\ntheoretical computer science, is discussed.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07555v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.07513v2",
    "title": "A Single Iterative Step for Anytime Causal Discovery",
    "authors": [
      "Raanan Y. Rohekar",
      "Yaniv Gurwicz",
      "Shami Nisimov",
      "Gal Novik"
    ],
    "author_ids": [],
    "abstract": "We present a sound and complete algorithm for recovering causal graphs from\nobserved, non-interventional data, in the possible presence of latent\nconfounders and selection bias. We rely on the causal Markov and faithfulness\nassumptions and recover the equivalence class of the underlying causal graph by\nperforming a series of conditional independence (CI) tests between observed\nvariables. We propose a single step that is applied iteratively, such that the\nindependence and causal relations entailed from the resulting graph, after any\niteration, is correct and becomes more informative with successive iteration.\nEssentially, we tie the size of the CI condition set to its distance from the\ntested nodes on the resulting graph. Each iteration refines the skeleton and\norientation by performing CI tests having condition sets that are larger than\nin the preceding iteration. In an iteration, condition sets of CI tests are\nconstructed from nodes that are within a specified search distance, and the\nsizes of these condition sets is equal to this search distance. The algorithm\nthen iteratively increases the search distance along with the condition set\nsizes. Thus, each iteration refines a graph, that was recovered by previous\niterations having smaller condition sets -- having a higher statistical power.\nWe demonstrate that our algorithm requires significantly fewer CI tests and\nsmaller condition sets compared to the FCI algorithm. This is evident for both\nrecovering the true underlying graph using a perfect CI oracle, and accurately\nestimating the graph using limited observed data.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07513v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07480v1",
    "title": "Synchronous LoRa Communication by Exploiting Large-Area Out-of-Band Synchronization",
    "authors": [
      "Luca Beltramelli",
      "Aamir Mahmood",
      "Paolo Ferrari",
      "Patrik Österberg",
      "Mikael Gidlund",
      "Emiliano Sisinni"
    ],
    "author_ids": [],
    "abstract": "Many new narrowband low-power wide-area networks (LPWANs) (e.g., LoRaWAN,\nSigfox) have opted to use pure ALOHA-like access for its reduced control\noverhead and asynchronous transmissions. Although asynchronous access reduces\nthe energy consumption of IoT devices, the network performance suffers from\nhigh intra-network interference in dense deployments. Contrarily, synchronous\naccess can improve throughput and fairness, but it requires time\nsynchronization. Unfortunately, maintaining synchronization over the narrowband\nLPWANs wastes channel time and transmission opportunities. In this paper, we\npropose the use of out-of-band time-dissemination to relatively synchronize\nLoRa devices and thereby facilitate resource-efficient slotted uplink\ncommunication. To this end, we conceptualize and analyze a co-designed\nsynchronization and random access mechanism that can effectively exploit\ntechnologies providing limited time accuracy, such as FM radio data system\n(FM-RDS). While considering the LoRa-specific parameters, we derive the\nthroughput of the proposed mechanism, compare it to a generic synchronous\nrandom access using in-band synchronization, and design the communication\nparameters under time uncertainty. We scrutinize the transmission time\nuncertainty of a device by introducing a clock error model that accounts for\nthe errors in the synchronization source, local clock, propagation delay, and\ntransceiver's transmission time uncertainty. We characterize the time\nuncertainty of FM-RDS with hardware measurements and perform simulations to\nevaluate the proposed solution. The results, presented in terms of success\nprobability, throughput, and fairness for a single-cell scenario, suggest that\nFM-RDS, despite its poor absolute synchronization, can be used effectively to\nrealize slotted LoRa communication with performance similar to that of more\naccurate time-dissemination technologies.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07480v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.07315v2",
    "title": "Morphology on categorical distributions",
    "authors": [
      "Silas Nyboe Ørting",
      "Hans Jacob Teglbjærg Stephensen",
      "Jon Sporring"
    ],
    "author_ids": [],
    "abstract": "The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.",
    "published_date": "2020-12-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07315v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07119v2",
    "title": "Demystifying Deep Neural Networks Through Interpretation: A Survey",
    "authors": [
      "Giang Dao",
      "Minwoo Lee"
    ],
    "author_ids": [],
    "abstract": "Modern deep learning algorithms tend to optimize an objective metric, such as\nminimize a cross entropy loss on a training dataset, to be able to learn. The\nproblem is that the single metric is an incomplete description of the real\nworld tasks. The single metric cannot explain why the algorithm learn. When an\nerroneous happens, the lack of interpretability causes a hardness of\nunderstanding and fixing the error. Recently, there are works done to tackle\nthe problem of interpretability to provide insights into neural networks\nbehavior and thought process. The works are important to identify potential\nbias and to ensure algorithm fairness as well as expected performance.",
    "published_date": "2020-12-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07119v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06995v1",
    "title": "Bi-Classifier Determinacy Maximization for Unsupervised Domain Adaptation",
    "authors": [
      "Shuang Li",
      "Fangrui Lv",
      "Binhui Xie",
      "Chi Harold Liu",
      "Jian Liang",
      "Chen Qin"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptation challenges the problem of transferring\nknowledge from a well-labelled source domain to an unlabelled target domain.\nRecently,adversarial learning with bi-classifier has been proven effective in\npushing cross-domain distributions close. Prior approaches typically leverage\nthe disagreement between bi-classifier to learn transferable representations,\nhowever, they often neglect the classifier determinacy in the target domain,\nwhich could result in a lack of feature discriminability. In this paper, we\npresent a simple yet effective method, namely Bi-Classifier Determinacy\nMaximization(BCDM), to tackle this problem. Motivated by the observation that\ntarget samples cannot always be separated distinctly by the decision boundary,\nhere in the proposed BCDM, we design a novel classifier determinacy disparity\n(CDD) metric, which formulates classifier discrepancy as the class relevance of\ndistinct target predictions and implicitly introduces constraint on the target\nfeature discriminability. To this end, the BCDM can generate discriminative\nrepresentations by encouraging target predictive outputs to be consistent and\ndetermined, meanwhile, preserve the diversity of predictions in an adversarial\nmanner. Furthermore, the properties of CDD as well as the theoretical\nguarantees of BCDM's generalization bound are both elaborated. Extensive\nexperiments show that BCDM compares favorably against the existing\nstate-of-the-art domain adaptation methods.",
    "published_date": "2020-12-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06995v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06950v2",
    "title": "Double Free-Layer Magnetic Tunnel Junctions for Probabilistic Bits",
    "authors": [
      "Kerem Y. Camsari",
      "Mustafa Mert Torunbalci",
      "William A. Borders",
      "Hideo Ohno",
      "Shunsuke Fukami"
    ],
    "author_ids": [],
    "abstract": "Naturally random devices that exploit ambient thermal noise have recently\nattracted attention as hardware primitives for accelerating probabilistic\ncomputing applications. One such approach is to use a low barrier nanomagnet as\nthe free layer of a magnetic tunnel junction (MTJ) whose magnetic fluctuations\nare converted to resistance fluctuations in the presence of a stable fixed\nlayer. Here, we propose and theoretically analyze a magnetic tunnel junction\nwith no fixed layers but two free layers that are circularly shaped disk\nmagnets. We use an experimentally benchmarked model that accounts for finite\ntemperature magnetization dynamics, bias-dependent charge and spin-polarized\ncurrents as well as the dipolar coupling between the free layers. We obtain\nanalytical results for statistical averages of fluctuations that are in good\nagreement with the numerical model. We find that the free layers with low\ndiameters fluctuate to randomize the resistance of the MTJ in an approximately\nbias-independent manner. We show how such MTJs can be used to build a binary\nstochastic neuron (or a p-bit) in hardware. Unlike earlier stochastic MTJs that\nneed to operate at a specific bias point to produce random fluctuations, the\nproposed design can be random for a wide range of bias values, independent of\nspin-transfer-torque pinning. Moreover, in the absence of a carefully optimized\nstabled fixed layer, the symmetric double-free layer stack can be manufactured\nusing present day Magnetoresistive Random Access Memory (MRAM) technology by\nminimal changes to the fabrication process. Such devices can be used as\nhardware accelerators in energy-efficient computing schemes that require a\nlarge throughput of tunably random bits.",
    "published_date": "2020-12-13T00:00:00",
    "year": 2020,
    "categories": [
      "cond-mat.mes-hall",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06950v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.06850v1",
    "title": "Trading the System Efficiency for the Income Equality of Drivers in Rideshare",
    "authors": [
      "Yifan Xu",
      "Pan Xu"
    ],
    "author_ids": [],
    "abstract": "Several scientific studies have reported the existence of the income gap\namong rideshare drivers based on demographic factors such as gender, age, race,\netc. In this paper, we study the income inequality among rideshare drivers due\nto discriminative cancellations from riders, and the tradeoff between the\nincome inequality (called fairness objective) with the system efficiency\n(called profit objective). We proposed an online bipartite-matching model where\nriders are assumed to arrive sequentially following a distribution known in\nadvance. The highlight of our model is the concept of acceptance rate between\nany pair of driver-rider types, where types are defined based on demographic\nfactors. Specially, we assume each rider can accept or cancel the driver\nassigned to her, each occurs with a certain probability which reflects the\nacceptance degree from the rider type towards the driver type. We construct a\nbi-objective linear program as a valid benchmark and propose two LP-based\nparameterized online algorithms. Rigorous online competitive ratio analysis is\noffered to demonstrate the flexibility and efficiency of our online algorithms\nin balancing the two conflicting goals, promotions of fairness and profit.\nExperimental results on a real-world dataset are provided as well, which\nconfirm our theoretical predictions.",
    "published_date": "2020-12-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06850v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06723v1",
    "title": "On Duality Gap as a Measure for Monitoring GAN Training",
    "authors": [
      "Sahil Sidheekh",
      "Aroof Aimen",
      "Vineet Madan",
      "Narayanan C. Krishnan"
    ],
    "author_ids": [],
    "abstract": "Generative adversarial network (GAN) is among the most popular deep learning\nmodels for learning complex data distributions. However, training a GAN is\nknown to be a challenging task. This is often attributed to the lack of\ncorrelation between the training progress and the trajectory of the generator\nand discriminator losses and the need for the GAN's subjective evaluation. A\nrecently proposed measure inspired by game theory - the duality gap, aims to\nbridge this gap. However, as we demonstrate, the duality gap's capability\nremains constrained due to limitations posed by its estimation process. This\npaper presents a theoretical understanding of this limitation and proposes a\nmore dependable estimation process for the duality gap. At the crux of our\napproach is the idea that local perturbations can help agents in a zero-sum\ngame escape non-Nash saddle points efficiently. Through exhaustive\nexperimentation across GAN models and datasets, we establish the efficacy of\nour approach in capturing the GAN training progress with minimal increase to\nthe computational complexity. Further, we show that our estimate, with its\nability to identify model convergence/divergence, is a potential performance\nmeasure that can be used to tune the hyperparameters of a GAN.",
    "published_date": "2020-12-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06682v3",
    "title": "Mind the Gap: Cake Cutting With Separation",
    "authors": [
      "Edith Elkind",
      "Erel Segal-Halevi",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We study the problem of fairly allocating a divisible resource, also known as\ncake cutting, with an additional requirement that the shares that different\nagents receive should be sufficiently separated from one another. This\ncaptures, for example, constraints arising from social distancing guidelines.\nWhile it is sometimes impossible to allocate a proportional share to every\nagent under the separation requirement, we show that the well-known criterion\nof maximin share fairness can always be attained. We then provide algorithmic\nanalysis of maximin share fairness in this setting -- for instance, the maximin\nshare of an agent cannot be computed exactly by any finite algorithm, but can\nbe approximated with an arbitrarily small error. In addition, we consider the\ndivision of a pie (i.e., a circular cake) and show that an ordinal relaxation\nof maximin share fairness can be achieved. We also prove that an envy-free or\nequitable allocation that allocates the maximum amount of resource exists under\nseparation.",
    "published_date": "2020-12-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06682v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.06466v1",
    "title": "Digital Contact Tracing: Technologies, Shortcomings, and the Path Forward",
    "authors": [
      "Amee Trivedi",
      "Deepak Vasisht"
    ],
    "author_ids": [],
    "abstract": "Since the start of the COVID-19 pandemic, technology enthusiasts have pushed\nfor digital contact tracing as a critical tool for breaking the COVID-19\ntransmission chains. Motivated by this push, many countries and companies have\ncreated apps that enable digital contact tracing with the goal to identify the\nchain of transmission from an infected individual to others and enable early\nquarantine. Digital contact tracing applications like AarogyaSetu in India,\nTraceTogether in Singapore, SwissCovid in Switzerland, and others have been\ndownloaded hundreds of millions of times. Yet, this technology hasn't seen the\nimpact that we envisioned at the start of the pandemic. Some countries have\nrolled back their apps, while others have seen low adoption.\n  Therefore, it is prudent to ask what the technology landscape of\ncontact-tracing looks like and what are the missing pieces. We attempt to\nundertake this task in this paper. We present a high-level review of\ntechnologies underlying digital contact tracing, a set of metrics that are\nimportant while evaluating different contact tracing technologies, and evaluate\nwhere the different technologies stand today on this set of metrics. Our hope\nis two-fold: (a) Future designers of contact tracing applications can use this\nreview paper to understand the technology landscape, and (b) Researchers can\nidentify and solve the missing pieces of this puzzle so that we are ready to\nface the rest of the COVID-19 pandemic and any future pandemics. A majority of\nthis discussion is focused on the ability to identify contact between\nindividuals. The questions of ethics, privacy, and security of such contact\ntracing are briefly mentioned but not discussed in detail.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06466v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.06387v4",
    "title": "TARA: Training and Representation Alteration for AI Fairness and Domain Generalization",
    "authors": [
      "William Paul",
      "Armin Hadzic",
      "Neil Joshi",
      "Fady Alajaji",
      "Phil Burlina"
    ],
    "author_ids": [],
    "abstract": "We propose a novel method for enforcing AI fairness with respect to protected\nor sensitive factors. This method uses a dual strategy performing training and\nrepresentation alteration (TARA) for the mitigation of prominent causes of AI\nbias by including: a) the use of representation learning alteration via\nadversarial independence to suppress the bias-inducing dependence of the data\nrepresentation from protected factors; and b) training set alteration via\nintelligent augmentation to address bias-causing data imbalance, by using\ngenerative models that allow the fine control of sensitive factors related to\nunderrepresented populations via domain adaptation and latent space\nmanipulation. When testing our methods on image analytics, experiments\ndemonstrate that TARA significantly or fully debiases baseline models while\noutperforming competing debiasing methods that have the same amount of\ninformation, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.\nthe baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.\n(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in\ncurrent metrics used for assessing debiasing performance, we propose novel\nconjunctive debiasing metrics. Our experiments also demonstrate the ability of\nthese novel metrics in assessing the Pareto efficiency of the proposed methods.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06387v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06244v4",
    "title": "The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks",
    "authors": [
      "Bohan Wang",
      "Qi Meng",
      "Wei Chen",
      "Tie-Yan Liu"
    ],
    "author_ids": [],
    "abstract": "Despite their overwhelming capacity to overfit, deep neural networks trained\nby specific optimization algorithms tend to generalize well to unseen data.\nRecently, researchers explained it by investigating the implicit regularization\neffect of optimization algorithms. A remarkable progress is the work (Lyu&Li,\n2019), which proves gradient descent (GD) maximizes the margin of homogeneous\ndeep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp\nand Adam are popular owing to their rapid training process. However,\ntheoretical guarantee for the generalization of adaptive optimization\nalgorithms is still lacking. In this paper, we study the implicit\nregularization of adaptive optimization algorithms when they are optimizing the\nlogistic loss on homogeneous deep neural networks. We prove that adaptive\nalgorithms that adopt exponential moving average strategy in conditioner (such\nas Adam and RMSProp) can maximize the margin of the neural network, while\nAdaGrad that directly sums historical squared gradients in conditioner can not.\nIt indicates superiority on generalization of exponential moving average\nstrategy in the design of the conditioner. Technically, we provide a unified\nframework to analyze convergent direction of adaptive optimization algorithms\nby constructing novel adaptive gradient flow and surrogate margin. Our\nexperiments can well support the theoretical findings on convergent direction\nof adaptive optimization algorithms.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06244v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06158v2",
    "title": "Reduced-Order Nonlinear Observers via Contraction Analysis and Convex Optimization",
    "authors": [
      "Bowen Yi",
      "Ruigang Wang",
      "Ian R. Manchester"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a new approach to design globally convergent\nreduced-order observers for nonlinear control systems via contraction analysis\nand convex optimization. Despite the fact that contraction is a concept\nnaturally suitable for state estimation, the existing solutions are either\nlocal or relatively conservative when applying to physical systems. To address\nthis, we show that this problem can be translated into an off-line search for a\ncoordinate transformation after which the dynamics is (transversely)\ncontracting. The obtained sufficient condition consists of some easily\nverifiable differential inequalities, which, on one hand, identify a very\ngeneral class of \"detectable\" nonlinear systems, and on the other hand, can be\nexpressed as computationally efficient convex optimization, making the design\nprocedure more systematic. Connections with some well-established approaches\nand concepts are also clarified in the paper. Finally, we illustrate the\nproposed method with several numerical and physical examples, including\npolynomial, mechanical, electromechanical and biochemical systems.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06158v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.06157v3",
    "title": "Fairness in Rating Prediction by Awareness of Verbal and Gesture Quality of Public Speeches",
    "authors": [
      "Ankani Chattoraj",
      "Rupam Acharyya",
      "Shouman Das",
      "Md. Iftekhar Tanveer",
      "Ehsan Hoque"
    ],
    "author_ids": [],
    "abstract": "The role of verbal and non-verbal cues towards great public speaking has been\na topic of exploration for many decades. We identify a commonality across\npresent theories, the element of \"variety or heterogeneity\" in channels or\nmodes of communication (e.g. resorting to stories, scientific facts, emotional\nconnections, facial expressions etc.) which is essential for effectively\ncommunicating information. We use this observation to formalize a novel\nHEterogeneity Metric, HEM, that quantifies the quality of a talk both in the\nverbal and non-verbal domain (transcript and facial gestures). We use TED talks\nas an input repository of public speeches because it consists of speakers from\na diverse community besides having a wide outreach. We show that there is an\ninteresting relationship between HEM and the ratings of TED talks given to\nspeakers by viewers. It emphasizes that HEM inherently and successfully\nrepresents the quality of a talk based on \"variety or heterogeneity\". Further,\nwe also discover that HEM successfully captures the prevalent bias in ratings\nwith respect to race and gender, that we call sensitive attributes (because\nprediction based on these might result in unfair outcome). We incorporate the\nHEM metric into the loss function of a neural network with the goal to reduce\nunfairness in rating predictions with respect to race and gender. Our results\nshow that the modified loss function improves fairness in prediction without\nconsiderably affecting prediction accuracy of the neural network. Our work ties\ntogether a novel metric for public speeches in both verbal and non-verbal\ndomain with the computational power of a neural network to design a fair\nprediction system for speakers.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06157v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06103v4",
    "title": "Fairness-Oriented Multiple RISs-Aided MmWave Transmission: Stochastic Optimization Methods",
    "authors": [
      "Gui Zhou",
      "Cunhua Pan",
      "Hong Ren",
      "Kezhi Wang",
      "Marco Di Renzo"
    ],
    "author_ids": [],
    "abstract": "In millimeter wave (mmWave) systems, it is challenging to ensure the reliable\nconnectivity of communications due to its sensitivity to the presence of\nblockages. In order to improve the robustness of the mmWave system under the\npresence of the random blockages, multiple reconfigurable intelligent surfaces\n(RISs) are deployed to enhance the spatial diversity gain, and robust\nbeamforming is then designed based on a stochastic optimization for minimizing\nthe maximum outage probability among multiple users to ensure the fairness.\nUnder the stochastic optimization framework, we adopt the stochastic\nmajorization--minimization (SMM) method and the stochastic successive convex\napproximation (SSCA) method to construct deterministic surrogate problems at\neach iteration for new channel realizations, and obtain the closed-form\nsolutions of the precoding matrix at the base station (BS) and the passive\nbeamforming vectors at the RISs. Both stochastic optimization methods have been\nproved to converge to the set of stationary points of the original stochastic\nproblems. Finally, simulation results show that the proposed robust beamforming\nin the RIS-aided system can effectively compensate for the performance loss\ncaused by the presence of the random blockages, especially at high blockage\nprobability, compared with the benchmark solutions.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06103v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.06058v1",
    "title": "Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable",
    "authors": [
      "Odest Chadwicke Jenkins",
      "Daniel Lopresti",
      "Melanie Mitchell"
    ],
    "author_ids": [],
    "abstract": "The history of AI has included several \"waves\" of ideas. The first wave, from\nthe mid-1950s to the 1980s, focused on logic and symbolic hand-encoded\nrepresentations of knowledge, the foundations of so-called \"expert systems\".\nThe second wave, starting in the 1990s, focused on statistics and machine\nlearning, in which, instead of hand-programming rules for behavior, programmers\nconstructed \"statistical learning algorithms\" that could be trained on large\ndatasets. In the most recent wave research in AI has largely focused on deep\n(i.e., many-layered) neural networks, which are loosely inspired by the brain\nand trained by \"deep learning\" methods. However, while deep neural networks\nhave led to many successes and new capabilities in computer vision, speech\nrecognition, language processing, game-playing, and robotics, their potential\nfor broad application remains limited by several factors.\n  A concerning limitation is that even the most successful of today's AI\nsystems suffer from brittleness-they can fail in unexpected ways when faced\nwith situations that differ sufficiently from ones they have been trained on.\nThis lack of robustness also appears in the vulnerability of AI systems to\nadversarial attacks, in which an adversary can subtly manipulate data in a way\nto guarantee a specific wrong answer or action from an AI system. AI systems\nalso can absorb biases-based on gender, race, or other factors-from their\ntraining data and further magnify these biases in their subsequent\ndecision-making. Taken together, these various limitations have prevented AI\nsystems such as automatic medical diagnosis or autonomous vehicles from being\nsufficiently trustworthy for wide deployment. The massive proliferation of AI\nacross society will require radically new ideas to yield technology that will\nnot sacrifice our productivity, our quality of life, or our values.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06058v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.06057v1",
    "title": "Interdisciplinary Approaches to Understanding Artificial Intelligence's Impact on Society",
    "authors": [
      "Suresh Venkatasubramanian",
      "Nadya Bliss",
      "Helen Nissenbaum",
      "Melanie Moses"
    ],
    "author_ids": [],
    "abstract": "Innovations in AI have focused primarily on the questions of \"what\" and\n\"how\"-algorithms for finding patterns in web searches, for instance-without\nadequate attention to the possible harms (such as privacy, bias, or\nmanipulation) and without adequate consideration of the societal context in\nwhich these systems operate. In part, this is driven by incentives and forces\nin the tech industry, where a more product-driven focus tends to drown out\nbroader reflective concerns about potential harms and misframings. But this\nfocus on what and how is largely a reflection of the engineering and\nmathematics-focused training in computer science, which emphasizes the building\nof tools and development of computational concepts.\n  As a result of this tight technical focus, and the rapid, worldwide explosion\nin its use, AI has come with a storm of unanticipated socio-technical problems,\nranging from algorithms that act in racially or gender-biased ways, get caught\nin feedback loops that perpetuate inequalities, or enable unprecedented\nbehavioral monitoring surveillance that challenges the fundamental values of\nfree, democratic societies.\n  Given that AI is no longer solely the domain of technologists but rather of\nsociety as a whole, we need tighter coupling of computer science and those\ndisciplines that study society and societal values.",
    "published_date": "2020-12-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.06057v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05796v2",
    "title": "Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?",
    "authors": [
      "Andrea Simonelli",
      "Samuel Rota Bulò",
      "Lorenzo Porzi",
      "Peter Kontschieder",
      "Elisa Ricci"
    ],
    "author_ids": [],
    "abstract": "Pseudo-LiDAR-based methods for monocular 3D object detection have received\nconsiderable attention in the community due to the performance gains exhibited\non the KITTI3D benchmark, in particular on the commonly reported validation\nsplit. This generated a distorted impression about the superiority of\nPseudo-LiDAR-based (PL-based) approaches over methods working with RGB images\nonly. Our first contribution consists in rectifying this view by pointing out\nand showing experimentally that the validation results published by PL-based\nmethods are substantially biased. The source of the bias resides in an overlap\nbetween the KITTI3D object detection validation set and the training/validation\nsets used to train depth predictors feeding PL-based methods. Surprisingly, the\nbias remains also after geographically removing the overlap. This leaves the\ntest set as the only reliable set for comparison, where published PL-based\nmethods do not excel. Our second contribution brings PL-based methods back up\nin the ranking with the design of a novel deep architecture which introduces a\n3D confidence prediction module. We show that 3D confidence estimation\ntechniques derived from RGB-only 3D detection approaches can be successfully\nintegrated into our framework and, more importantly, that improved performance\ncan be obtained with a newly designed 3D confidence measure, leading to\nstate-of-the-art performance on the KITTI3D benchmark.",
    "published_date": "2020-12-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05796v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05533v3",
    "title": "Data-Efficient Framework for Real-world Multiple Sound Source 2D Localization",
    "authors": [
      "Guillaume Le Moing",
      "Phongtharin Vinayavekhin",
      "Don Joven Agravante",
      "Tadanobu Inoue",
      "Jayakorn Vongkulbhisal",
      "Asim Munawar",
      "Ryuki Tachibana"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks have recently led to promising results for the task of\nmultiple sound source localization. Yet, they require a lot of training data to\ncover a variety of acoustic conditions and microphone array layouts. One can\nleverage acoustic simulators to inexpensively generate labeled training data.\nHowever, models trained on synthetic data tend to perform poorly with\nreal-world recordings due to the domain mismatch. Moreover, learning for\ndifferent microphone array layouts makes the task more complicated due to the\ninfinite number of possible layouts. We propose to use adversarial learning\nmethods to close the gap between synthetic and real domains. Our novel\nensemble-discrimination method significantly improves the localization\nperformance without requiring any label from the real data. Furthermore, we\npropose a novel explicit transformation layer to be embedded in the\nlocalization architecture. It enables the model to be trained with data from\nspecific microphone array layouts while generalizing well to unseen layouts\nduring inference.",
    "published_date": "2020-12-10T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05533v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05463v1",
    "title": "Investigating Bias in Image Classification using Model Explanations",
    "authors": [
      "Schrasing Tong",
      "Lalana Kagal"
    ],
    "author_ids": [],
    "abstract": "We evaluated whether model explanations could efficiently detect bias in\nimage classification by highlighting discriminating features, thereby removing\nthe reliance on sensitive attributes for fairness calculations. To this end, we\nformulated important characteristics for bias detection and observed how\nexplanations change as the degree of bias in models change. The paper\nidentifies strengths and best practices for detecting bias using explanations,\nas well as three main weaknesses: explanations poorly estimate the degree of\nbias, could potentially introduce additional bias into the analysis, and are\nsometimes inefficient in terms of human effort involved.",
    "published_date": "2020-12-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05463v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05447v3",
    "title": "Deep Mining Generation of Lung Cancer Malignancy Models from Chest X-ray Images",
    "authors": [
      "Michael J. Horry",
      "Subrata Chakraborty",
      "Biswajeet Pradhan",
      "Manoranjan Paul",
      "Douglas P. S. Gomes",
      "Anwaar Ul-Haq"
    ],
    "author_ids": [],
    "abstract": "Lung cancer is the leading cause of cancer death and morbidity worldwide.\nMany studies have shown machine learning models to be effective at detecting\nlung nodules from chest X-ray images. However, these techniques have yet to be\nembraced by the medical community due to several practical, ethical, and\nregulatory constraints stemming from the black-box nature of deep learning\nmodels. Additionally, most lung nodules visible on chest X-ray are benign;\ntherefore, the narrow task of computer vision-based lung nodule detection\ncannot be equated to automated lung cancer detection. Addressing both concerns,\nthis study introduces a novel hybrid deep learning and decision tree-based\ncomputer vision model which presents lung cancer malignancy predictions as\ninterpretable decision trees. The deep learning component of this process is\ntrained using a large publicly available dataset on pathological biomarkers\nassociated with lung cancer. These models are then used to inference biomarker\nscores for chest X-ray images from two, independent data sets for which\nmalignancy metadata is available. We mine multi-variate predictive models by\nfitting shallow decision trees to the malignancy stratified datasets and\ninterrogate a range of metrics to determine the best model. Our best decision\ntree model achieves sensitivity and specificity of 86.7% and 80.0% respectively\nwith a positive predictive value of 92.9%. Decision trees mined using this\nmethod may be considered as a starting point for refinement into clinically\nuseful multi-variate lung cancer malignancy models for implementation as a\nworkflow augmentation tool to improve the efficiency of human radiologists.",
    "published_date": "2020-12-10T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05447v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05225v2",
    "title": "MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias",
    "authors": [
      "Nataniel Ruiz",
      "Barry-John Theobald",
      "Anurag Ranjan",
      "Ahmed Hussein Abdelaziz",
      "Nicholas Apostoloff"
    ],
    "author_ids": [],
    "abstract": "To detect bias in face recognition networks, it can be useful to probe a\nnetwork under test using samples in which only specific attributes vary in some\ncontrolled way. However, capturing a sufficiently large dataset with specific\ncontrol over the attributes of interest is difficult. In this work, we describe\na simulator that applies specific head pose and facial expression adjustments\nto images of previously unseen people. The simulator first fits a 3D morphable\nmodel to a provided image, applies the desired head pose and facial expression\ncontrols, then renders the model into an image. Next, a conditional Generative\nAdversarial Network (GAN) conditioned on the original image and the rendered\nmorphable model is used to produce the image of the original person with the\nnew facial expression and head pose. We call this conditional GAN -- MorphGAN.\nImages generated using MorphGAN conserve the identity of the person in the\noriginal image, and the provided control over head pose and facial expression\nallows test sets to be created to identify robustness issues of a facial\nrecognition deep network with respect to pose and expression. Images generated\nby MorphGAN can also serve as data augmentation when training data are scarce.\nWe show that by augmenting small datasets of faces with new poses and\nexpressions improves the recognition performance by up to 9% depending on the\naugmentation and data scarcity.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05225v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05217v1",
    "title": "Positional Encoding as Spatial Inductive Bias in GANs",
    "authors": [
      "Rui Xu",
      "Xintao Wang",
      "Kai Chen",
      "Bolei Zhou",
      "Chen Change Loy"
    ],
    "author_ids": [],
    "abstract": "SinGAN shows impressive capability in learning internal patch distribution\ndespite its limited effective receptive field. We are interested in knowing how\nsuch a translation-invariant convolutional generator could capture the global\nstructure with just a spatially i.i.d. input. In this work, taking SinGAN and\nStyleGAN2 as examples, we show that such capability, to a large extent, is\nbrought by the implicit positional encoding when using zero padding in the\ngenerators. Such positional encoding is indispensable for generating images\nwith high fidelity. The same phenomenon is observed in other generative\narchitectures such as DCGAN and PGGAN. We further show that zero padding leads\nto an unbalanced spatial bias with a vague relation between locations. To offer\na better spatial inductive bias, we investigate alternative positional\nencodings and analyze their effects. Based on a more flexible positional\nencoding explicitly, we propose a new multi-scale training strategy and\ndemonstrate its effectiveness in the state-of-the-art unconditional generator\nStyleGAN2. Besides, the explicit spatial inductive bias substantially improve\nSinGAN for more versatile image manipulation.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05217v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05010v2",
    "title": "Strong but Simple Baseline with Dual-Granularity Triplet Loss for Visible-Thermal Person Re-Identification",
    "authors": [
      "Haijun Liu",
      "Yanxia Chai",
      "Xiaoheng Tan",
      "Dong Li",
      "Xichuan Zhou"
    ],
    "author_ids": [],
    "abstract": "In this letter, we propose a conceptually simple and effective\ndual-granularity triplet loss for visible-thermal person re-identification\n(VT-ReID). In general, ReID models are always trained with the sample-based\ntriplet loss and identification loss from the fine granularity level. It is\npossible when a center-based loss is introduced to encourage the intra-class\ncompactness and inter-class discrimination from the coarse granularity level.\nOur proposed dual-granularity triplet loss well organizes the sample-based\ntriplet loss and center-based triplet loss in a hierarchical fine to coarse\ngranularity manner, just with some simple configurations of typical operations,\nsuch as pooling and batch normalization. Experiments on RegDB and SYSU-MM01\ndatasets show that with only the global features our dual-granularity triplet\nloss can improve the VT-ReID performance by a significant margin. It can be a\nstrong VT-ReID baseline to boost future research with high quality.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05010v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04955v1",
    "title": "Breeding Gender-aware Direct Speech Translation Systems",
    "authors": [
      "Marco Gaido",
      "Beatrice Savoldi",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "author_ids": [],
    "abstract": "In automatic speech translation (ST), traditional cascade approaches\ninvolving separate transcription and translation steps are giving ground to\nincreasingly competitive and more robust direct solutions. In particular, by\ntranslating speech audio data without intermediate transcription, direct ST\nmodels are able to leverage and preserve essential information present in the\ninput (e.g. speaker's vocal characteristics) that is otherwise lost in the\ncascade framework. Although such ability proved to be useful for gender\ntranslation, direct ST is nonetheless affected by gender bias just like its\ncascade counterpart, as well as machine translation and numerous other natural\nlanguage processing applications. Moreover, direct ST systems that exclusively\nrely on vocal biometric features as a gender cue can be unsuitable and\npotentially harmful for certain users. Going beyond speech signals, in this\npaper we compare different approaches to inform direct ST models about the\nspeaker's gender and test their ability to handle gender translation from\nEnglish into Italian and French. To this aim, we manually annotated large\ndatasets with speakers' gender information and used them for experiments\nreflecting different possible real-world scenarios. Our results show that\ngender-aware direct ST solutions can significantly outperform strong - but\ngender-unaware - direct ST models. In particular, the translation of\ngender-marked words can increase up to 30 points in accuracy while preserving\noverall translation quality.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04955v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04937v1",
    "title": "Removing Class Imbalance using Polarity-GAN: An Uncertainty Sampling Approach",
    "authors": [
      "Kumari Deepshikha",
      "Anugunj Naman"
    ],
    "author_ids": [],
    "abstract": "Class imbalance is a challenging issue in practical classification problems\nfor deep learning models as well as for traditional models. Traditionally\nsuccessful countermeasures such as synthetic over-sampling have had limited\nsuccess with complex, structured data handled by deep learning models. In this\nwork, we propose to use a Generative Adversarial Network (GAN) equipped with a\ngenerator network G, a discriminator network D and a classifier network C to\nremove the class-imbalance in visual data sets. The generator network is\ninitialized with auto-encoder to make it stable. The discriminator D ensures\nthat G adheres to class distribution of imbalanced class. In conventional\nmethods, where Generator G competes with discriminator D in a min-max game, we\npropose to further add an additional classifier network to the original\nnetwork. Now, the generator network tries to compete in a min-max game with\nDiscriminator as well as the new classifier that we have introduced. An\nadditional condition is enforced on generator network G to produce points in\nthe convex hull of desired imbalanced class. Further the contention of\nadversarial game with classifier C, pushes conditional distribution learned by\nG towards the periphery of the respective class, compensating the problem of\nclass imbalance. Experimental evidence shows that this initialization results\nin stable training of the network. We achieve state of the art performance on\nextreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark,\nMVTec Anomaly Detection dataset, Chest X-Ray dataset and others.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04937v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04842v2",
    "title": "Improving the Fairness of Deep Generative Models without Retraining",
    "authors": [
      "Shuhan Tan",
      "Yujun Shen",
      "Bolei Zhou"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GANs) advance face synthesis through\nlearning the underlying distribution of observed data. Despite the high-quality\ngenerated faces, some minority groups can be rarely generated from the trained\nmodels due to a biased image generation process. To study the issue, we first\nconduct an empirical study on a pre-trained face synthesis model. We observe\nthat after training the GAN model not only carries the biases in the training\ndata but also amplifies them to some degree in the image generation process. To\nfurther improve the fairness of image generation, we propose an interpretable\nbaseline method to balance the output facial attributes without retraining. The\nproposed method shifts the interpretable semantic distribution in the latent\nspace for a more balanced image generation while preserving the sample\ndiversity. Besides producing more balanced data regarding a particular\nattribute (e.g., race, gender, etc.), our method is generalizable to handle\nmore than one attribute at a time and synthesize samples of fine-grained\nsubgroups. We further show the positive applicability of the balanced data\nsampled from GANs to quantify the biases in other face recognition systems,\nlike commercial face attribute classifiers and face super-resolution\nalgorithms.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04842v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04800v1",
    "title": "A Statistical Test for Probabilistic Fairness",
    "authors": [
      "Bahar Taskesen",
      "Jose Blanchet",
      "Daniel Kuhn",
      "Viet Anh Nguyen"
    ],
    "author_ids": [],
    "abstract": "Algorithms are now routinely used to make consequential decisions that affect\nhuman lives. Examples include college admissions, medical interventions or law\nenforcement. While algorithms empower us to harness all information hidden in\nvast amounts of data, they may inadvertently amplify existing biases in the\navailable datasets. This concern has sparked increasing interest in fair\nmachine learning, which aims to quantify and mitigate algorithmic\ndiscrimination. Indeed, machine learning models should undergo intensive tests\nto detect algorithmic biases before being deployed at scale. In this paper, we\nuse ideas from the theory of optimal transport to propose a statistical\nhypothesis test for detecting unfair classifiers. Leveraging the geometry of\nthe feature space, the test statistic quantifies the distance of the empirical\ndistribution supported on the test samples to the manifold of distributions\nthat render a pre-trained classifier fair. We develop a rigorous hypothesis\ntesting mechanism for assessing the probabilistic fairness of any pre-trained\nlogistic classifier, and we show both theoretically as well as empirically that\nthe proposed test is asymptotically correct. In addition, the proposed\nframework offers interpretability by identifying the most favorable\nperturbation of the data so that the given classifier becomes fair.",
    "published_date": "2020-12-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04800v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04750v1",
    "title": "Mitigating the Impact of Adversarial Attacks in Very Deep Networks",
    "authors": [
      "Mohammed Hassanin",
      "Ibrahim Radwan",
      "Nour Moustafa",
      "Murat Tahtali",
      "Neeraj Kumar"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Network (DNN) models have vulnerabilities related to security\nconcerns, with attackers usually employing complex hacking techniques to expose\ntheir structures. Data poisoning-enabled perturbation attacks are complex\nadversarial ones that inject false data into models. They negatively impact the\nlearning process, with no benefit to deeper networks, as they degrade a model's\naccuracy and convergence rates. In this paper, we propose an\nattack-agnostic-based defense method for mitigating their influence. In it, a\nDefensive Feature Layer (DFL) is integrated with a well-known DNN architecture\nwhich assists in neutralizing the effects of illegitimate perturbation samples\nin the feature space. To boost the robustness and trustworthiness of this\nmethod for correctly classifying attacked input samples, we regularize the\nhidden space of a trained model with a discriminative loss function called\nPolarized Contrastive Loss (PCL). It improves discrimination among samples in\ndifferent classes and maintains the resemblance of those in the same class.\nAlso, we integrate a DFL and PCL in a compact model for defending against data\npoisoning attacks. This method is trained and tested using the CIFAR-10 and\nMNIST datasets with data poisoning-enabled perturbation attacks, with the\nexperimental results revealing its excellent performance compared with those of\nrecent peer techniques.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04750v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04698v2",
    "title": "Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text",
    "authors": [
      "Nishtha Madaan",
      "Inkit Padhi",
      "Naveen Panwar",
      "Diptikalyan Saha"
    ],
    "author_ids": [],
    "abstract": "Machine Learning has seen tremendous growth recently, which has led to larger\nadoption of ML systems for educational assessments, credit risk, healthcare,\nemployment, criminal justice, to name a few. The trustworthiness of ML and NLP\nsystems is a crucial aspect and requires a guarantee that the decisions they\nmake are fair and robust. Aligned with this, we propose a framework GYC, to\ngenerate a set of counterfactual text samples, which are crucial for testing\nthese ML systems. Our main contributions include a) We introduce GYC, a\nframework to generate counterfactual samples such that the generation is\nplausible, diverse, goal-oriented, and effective, b) We generate counterfactual\nsamples, that can direct the generation towards a corresponding condition such\nas named-entity tag, semantic role label, or sentiment. Our experimental\nresults on various domains show that GYC generates counterfactual text samples\nexhibiting the above four properties. GYC generates counterfactuals that can\nact as test cases to evaluate a model and any text debiasing algorithm.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04698v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04511v1",
    "title": "Emotive Response to a Hybrid-Face Robot and Translation to Consumer Social Robots",
    "authors": [
      "Maitreyee Wairagkar",
      "Maria R Lima",
      "Daniel Bazo",
      "Richard Craig",
      "Hugo Weissbart",
      "Appolinaire C Etoundi",
      "Tobias Reichenbach",
      "Prashant Iyenger",
      "Sneh Vaswani",
      "Christopher James",
      "Payam Barnaghi",
      "Chris Melhuish",
      "Ravi Vaidyanathan"
    ],
    "author_ids": [],
    "abstract": "We introduce the conceptual formulation, design, fabrication, control and\ncommercial translation with IoT connection of a hybrid-face social robot and\nvalidation of human emotional response to its affective interactions. The\nhybrid-face robot integrates a 3D printed faceplate and a digital display to\nsimplify conveyance of complex facial movements while providing the impression\nof three-dimensional depth for natural interaction. We map the space of\npotential emotions of the robot to specific facial feature parameters and\ncharacterise the recognisability of the humanoid hybrid-face robot's archetypal\nfacial expressions. We introduce pupil dilation as an additional degree of\nfreedom for conveyance of emotive states. Human interaction experiments\ndemonstrate the ability to effectively convey emotion from the hybrid-robot\nface to human observers by mapping their neurophysiological\nelectroencephalography (EEG) response to perceived emotional information and\nthrough interviews. Results show main hybrid-face robotic expressions can be\ndiscriminated with recognition rates above 80% and invoke human emotive\nresponse similar to that of actual human faces as measured by the face-specific\nN170 event-related potentials in EEG. The hybrid-face robot concept has been\nmodified, implemented, and released in the commercial IoT robotic platform Miko\n(My Companion), an affective robot with facial and conversational features\ncurrently in use for human-robot interaction in children by Emotix Inc. We\ndemonstrate that human EEG responses to Miko emotions are comparative to\nneurophysiological responses for actual human facial recognition. Finally,\ninterviews show above 90% expression recognition rates in our commercial robot.\nWe conclude that simplified hybrid-face abstraction conveys emotions\neffectively and enhances human-robot interaction.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04511v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04510v2",
    "title": "Graph-based open-ended survey on concerns related to COVID-19",
    "authors": [
      "Tatsuro Kawamoto",
      "Takaaki Aoki",
      "Michiko Ueda"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic is an unprecedented public health crisis with broad\nsocial and economic consequences. We conducted four surveys between April and\nAugust 2020 using the graph-based open-ended survey (GOS) framework, and\ninvestigated the most pressing concerns and issues for the general public in\nJapan. The GOS framework is a hybrid of the two traditional survey frameworks\nthat allows respondents to post their opinions in a free-format style, which\ncan subsequently serve as one of the choice items for other respondents, just\nas in a multiple-choice survey. As a result, this framework generates an\nopinion graph that relates opinions and respondents. We can also construct\nannotated opinion graphs to achieve a higher resolution. By clustering the\nannotated opinion graphs, we revealed the characteristic evolution of the\nresponse patterns as well as the interconnectedness and multi-faceted nature of\nopinions. Substantively, our notable finding is that \"social pressure,\" not\n\"infection risk,\" was one of the major concerns of our respondents. Social\npressure refers to criticism and discrimination that they anticipate receiving\nfrom others should they contract COVID-19. It is possible that the collectivist\nnature of Japanese culture coupled with the government's policy of relying on\npersonal responsibility to combat COVID-19 explains some of the above findings,\nas the latter has led to the emergence of vigilantes. The presence of mutual\nsurveillance can contribute to growing skepticism toward others as well as fear\nof ostracism, which may have negative consequences at both the societal and\nindividual levels.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04510v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.04405v1",
    "title": "Cyber Autonomy: Automating the Hacker- Self-healing, self-adaptive, automatic cyber defense systems and their impact to the industry, society and national security",
    "authors": [
      "Ryan K L Ko"
    ],
    "author_ids": [],
    "abstract": "This paper sets the context for the urgency for cyber autonomy, and the\ncurrent gaps of the cyber security industry. A novel framework proposing four\nphases of maturity for full cyber autonomy will be discussed. The paper also\nreviews new and emerging cyber security automation techniques and tools, and\ndiscusses their impact on society, the perceived cyber security skills\ngap/shortage and national security. We will also be discussing the delicate\nbalance between national security, human rights and ethics, and the potential\ndemise of the manual penetration testing industry in the face of automation.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.SE",
      "I.2.2; I.2.m; K.4.0; K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04405v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.05073v2",
    "title": "COVID-19 Detection in Chest X-Ray Images using a New Channel Boosted CNN",
    "authors": [
      "Saddam Hussain Khan",
      "Anabia Sohail",
      "Asifullah Khan"
    ],
    "author_ids": [],
    "abstract": "COVID-19 is a highly contagious respiratory infection that has affected a\nlarge population across the world and continues with its devastating\nconsequences. It is imperative to detect COVID-19 at the earliest to limit the\nspan of infection. In this work, a new classification technique CB-STM-RENet\nbased on deep Convolutional Neural Network (CNN) and Channel Boosting is\nproposed for the screening of COVID-19 in chest X-Rays. In this connection, to\nlearn the COVID-19 specific radiographic patterns, a new convolution block\nbased on split-transform-merge (STM) is developed. This new block\nsystematically incorporates region and edge-based operations at each branch to\ncapture the diverse set of features at various levels, especially those related\nto region homogeneity, textural variations, and boundaries of the infected\nregion. The learning and discrimination capability of the proposed CNN\narchitecture is enhanced by exploiting the Channel Boosting idea that\nconcatenates the auxiliary channels along with the original channels. The\nauxiliary channels are generated from the pre-trained CNNs using Transfer\nLearning. The effectiveness of the proposed technique CB-STM-RENet is evaluated\non three different datasets of chest X-Rays namely CoV-Healthy-6k,\nCoV-NonCoV-10k, and CoV-NonCoV-15k. The performance comparison of the proposed\nCB-STM-RENet with the existing techniques exhibits high performance both in\ndiscriminating COVID-19 chest infections from Healthy, as well as, other types\nof chest infections. CB-STM-RENet provides the highest performance on all these\nthree datasets; especially on the stringent CoV-NonCoV-15k dataset. The good\ndetection rate (97%), and high precision (93%) of the proposed technique\nsuggest that it can be adapted for the diagnosis of COVID-19 infected patients.\nThe test code is available at\nhttps://github.com/PRLAB21/COVID-19-Detection-System-using-Chest-X-Ray-Images.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.05073v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04221v3",
    "title": "Ditto: Fair and Robust Federated Learning Through Personalization",
    "authors": [
      "Tian Li",
      "Shengyuan Hu",
      "Ahmad Beirami",
      "Virginia Smith"
    ],
    "author_ids": [],
    "abstract": "Fairness and robustness are two important concerns for federated learning\nsystems. In this work, we identify that robustness to data and model poisoning\nattacks and fairness, measured as the uniformity of performance across devices,\nare competing constraints in statistically heterogeneous networks. To address\nthese constraints, we propose employing a simple, general framework for\npersonalized federated learning, Ditto, that can inherently provide fairness\nand robustness benefits, and develop a scalable solver for it. Theoretically,\nwe analyze the ability of Ditto to achieve fairness and robustness\nsimultaneously on a class of linear problems. Empirically, across a suite of\nfederated datasets, we show that Ditto not only achieves competitive\nperformance relative to recent personalization methods, but also enables more\naccurate, robust, and fair models relative to state-of-the-art fair or robust\nbaselines.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04221v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04216v1",
    "title": "Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives",
    "authors": [
      "Angie Peng",
      "Jeff Naecker",
      "Ben Hutchinson",
      "Andrew Smart",
      "Nyalleng Moorosi"
    ],
    "author_ids": [],
    "abstract": "How should we decide which fairness criteria or definitions to adopt in\nmachine learning systems? To answer this question, we must study the fairness\npreferences of actual users of machine learning systems. Stringent parity\nconstraints on treatment or impact can come with trade-offs, and may not even\nbe preferred by the social groups in question (Zafar et al., 2017). Thus it\nmight be beneficial to elicit what the group's preferences are, rather than\nrely on a priori defined mathematical fairness constraints. Simply asking for\nself-reported rankings of users is challenging because research has shown that\nthere are often gaps between people's stated and actual preferences(Bernheim et\nal., 2013).\n  This paper outlines a research program and experimental designs for\ninvestigating these questions. Participants in the experiments are invited to\nperform a set of tasks in exchange for a base payment--they are told upfront\nthat they may receive a bonus later on, and the bonus could depend on some\ncombination of output quantity and quality. The same group of workers then\nvotes on a bonus payment structure, to elicit preferences. The voting is\nhypothetical (not tied to an outcome) for half the group and actual (tied to\nthe actual payment outcome) for the other half, so that we can understand the\nrelation between a group's actual preferences and hypothetical (stated)\npreferences. Connections and lessons from fairness in machine learning are\nexplored.",
    "published_date": "2020-12-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04216v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.04092v3",
    "title": "Conditional independence structures over four discrete random variables revisited: conditional Ingleton inequalities",
    "authors": [
      "Milan Studeny"
    ],
    "author_ids": [],
    "abstract": "The paper deals with conditional linear information inequalities valid for\nentropy functions induced by discrete random variables. Specifically, the\nso-called conditional Ingleton inequalities are in the center of interest:\nthese are valid under conditional independence assumptions on the inducing\nrandom variables. We discuss five inequalities of this particular type, four of\nwhich has appeared earlier in the literature. Besides the proof of the new\nfifth inequality, simpler proofs of (some of) former inequalities are\npresented. These five information inequalities are used to characterize all\nconditional independence structures induced by four discrete random variables.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.CO",
      "math.IT",
      "94A17 68T37 52B40"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04092v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03979v3",
    "title": "Computing Welfare-Maximizing Fair Allocations of Indivisible Goods",
    "authors": [
      "Haris Aziz",
      "Xin Huang",
      "Nicholas Mattei",
      "Erel Segal-Halevi"
    ],
    "author_ids": [],
    "abstract": "We analyze the run-time complexity of computing allocations that are both\nfair and maximize the utilitarian social welfare, defined as the sum of agents'\nutilities. We focus on two tractable fairness concepts: envy-freeness up to one\nitem (EF1) and proportionality up to one item (PROP1). We consider two\ncomputational problems: (1) Among the utilitarian-maximal allocations, decide\nwhether there exists one that is also fair; (2) among the fair allocations,\ncompute one that maximizes the utilitarian welfare. We show that both problems\nare strongly NP-hard when the number of agents is variable, and remain NP-hard\nfor a fixed number of agents greater than two. For the special case of two\nagents, we find that problem (1) is polynomial-time solvable, while problem (2)\nremains NP-hard. Finally, with a fixed number of agents, we design\npseudopolynomial-time algorithms for both problems. We extend our results to\nthe stronger fairness notions envy-freeness up to any item (EFx) and\nproportionality up to any item (PROPx).",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03979v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03812v1",
    "title": "Improving Fairness and Privacy in Selection Problems",
    "authors": [
      "Mohammad Mahdi Khalili",
      "Xueru Zhang",
      "Mahed Abroshan",
      "Somayeh Sojoudi"
    ],
    "author_ids": [],
    "abstract": "Supervised learning models have been increasingly used for making decisions\nabout individuals in applications such as hiring, lending, and college\nadmission. These models may inherit pre-existing biases from training datasets\nand discriminate against protected attributes (e.g., race or gender). In\naddition to unfairness, privacy concerns also arise when the use of models\nreveals sensitive personal information. Among various privacy notions,\ndifferential privacy has become popular in recent years. In this work, we study\nthe possibility of using a differentially private exponential mechanism as a\npost-processing step to improve both fairness and privacy of supervised\nlearning models. Unlike many existing works, we consider a scenario where a\nsupervised model is used to select a limited number of applicants as the number\nof available positions is limited. This assumption is well-suited for various\nscenarios, such as job application and college admission. We use ``equal\nopportunity'' as the fairness notion and show that the exponential mechanisms\ncan make the decision-making process perfectly fair. Moreover, the experiments\non real-world datasets show that the exponential mechanism can improve both\nprivacy and fairness, with a slight decrease in accuracy compared to the model\nwithout post-processing.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03812v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03766v1",
    "title": "Budget-feasible Maximum Nash Social Welfare Allocation is Almost Envy-free",
    "authors": [
      "Xiaowei Wu",
      "Bo Li",
      "Jiarui Gan"
    ],
    "author_ids": [],
    "abstract": "The Nash social welfare (NSW) is a well-known social welfare measurement that\nbalances individual utilities and the overall efficiency. In the context of\nfair allocation of indivisible goods, it has been shown by Caragiannis et al.\n(EC 2016 and TEAC 2019) that an allocation maximizing the NSW is envy-free up\nto one good (EF1). In this paper, we are interested in the fairness of the NSW\nin a budget-feasible allocation problem, in which each item has a cost that\nwill be incurred to the agent it is allocated to, and each agent has a budget\nconstraint on the total cost of items she receives. We show that a\nbudget-feasible allocation that maximizes the NSW achieves a 1/4-approximation\nof EF1 and the approximation ratio is tight. The approximation ratio improves\ngracefully when the items have small costs compared with the agents' budgets;\nit converges to 1/2 when the budget-cost ratio approaches infinity.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03766v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.04457v3",
    "title": "Codimensional Incremental Potential Contact",
    "authors": [
      "Minchen Li",
      "Danny M. Kaufman",
      "Chenfanfu Jiang"
    ],
    "author_ids": [],
    "abstract": "We extend the incremental potential contact (IPC) model for contacting\nelastodynamics to resolve systems composed of codimensional DOFs in arbitrary\ncombination. This enables a unified, interpenetration-free, robust, and stable\nsimulation framework that couples codimension-0,1,2, and 3 geometries\nseamlessly with frictional contact. Extending IPC to thin structures poses new\nchallenges in computing strain, modeling thickness and determining collisions.\nTo address these challenges we propose three corresponding contributions.\nFirst, we introduce a C2 constitutive barrier model that directly enforces\nstrain limiting as an energy potential while preserving rest state. This\nprovides energetically-consistent strain limiting models (both isotropic and\nanisotropic) for cloth that enable strict satisfaction of strain-limit\ninequalities with direct coupling to both elastodynamics and contact via\nminimization of the incremental potential. Second, to capture the geometric\nthickness of codimensional domains we extend the IPC model to directly enforce\ndistance offsets. Our treatment imposes a strict guarantee that mid-surfaces\n(resp. mid-lines) of shells (resp. rods) will not move closer than applied\nthickness values. This enables us to account for thickness in the contact\nbehavior of codimensional structures and so robustly capture challenging\ncontacting geometries; a number of which, to our knowledge, have not been\nsimulated before. Third, codimensional models, especially with modeled\nthickness, mandate strict accuracy requirements that pose a severe challenge to\nall existing continuous collision detection (CCD) methods. To address these\nlimitations we develop a new, efficient, simple-to-implement additive CCD\n(ACCD) method that applies conservative advancement to iteratively refine a\nlower bound for deforming primitives, converging to time of impact.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04457v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03676v1",
    "title": "Consensus Control of Linear Multi-Agent Systems with Non-uniform Time-varying Communication Delays",
    "authors": [
      "Rajnish Bhusal",
      "Kamesh Subbarao"
    ],
    "author_ids": [],
    "abstract": "This paper is concerned with the consensus problem for multi-agent systems\nsubject to communication delays between the neighboring agents. We consider a\nscenario where each agent is characterized by a general high-order linear\nsystem and the communication delays between the agents are non-uniform and\ntime-varying. We design a distributed control protocol for the agents and\nprovide an equivalent stability problem to be solved that guarantees the state\nconsensus in the group of agents. Moreover, a delay-dependent stability\ncriterion is provided by combining the Lyapunov-Krasovskii method with the\nlinear matrix inequality approach.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03676v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03488v3",
    "title": "Multi-agent Policy Optimization with Approximatively Synchronous Advantage Estimation",
    "authors": [
      "Lipeng Wan",
      "Xuwei Song",
      "Xuguang Lan",
      "Nanning Zheng"
    ],
    "author_ids": [],
    "abstract": "Cooperative multi-agent tasks require agents to deduce their own\ncontributions with shared global rewards, known as the challenge of credit\nassignment. General methods for policy based multi-agent reinforcement learning\nto solve the challenge introduce differentiate value functions or advantage\nfunctions for individual agents. In multi-agent system, polices of different\nagents need to be evaluated jointly. In order to update polices synchronously,\nsuch value functions or advantage functions also need synchronous evaluation.\nHowever, in current methods, value functions or advantage functions use\ncounter-factual joint actions which are evaluated asynchronously, thus suffer\nfrom natural estimation bias. In this work, we propose the approximatively\nsynchronous advantage estimation. We first derive the marginal advantage\nfunction, an expansion from single-agent advantage function to multi-agent\nsystem. Further more, we introduce a policy approximation for synchronous\nadvantage estimation, and break down the multi-agent policy optimization\nproblem into multiple sub-problems of single-agent policy optimization. Our\nmethod is compared with baseline algorithms on StarCraft multi-agent\nchallenges, and shows the best performance on most of the tasks.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03488v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03434v2",
    "title": "Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations",
    "authors": [
      "Woo-Jeoung Nam",
      "Jaesik Choi",
      "Seong-Whan Lee"
    ],
    "author_ids": [],
    "abstract": "The clear transparency of Deep Neural Networks (DNNs) is hampered by complex\ninternal structures and nonlinear transformations along deep hierarchies. In\nthis paper, we propose a new attribution method, Relative Sectional Propagation\n(RSP), for fully decomposing the output predictions with the characteristics of\nclass-discriminative attributions and clear objectness. We carefully revisit\nsome shortcomings of backpropagation-based attribution methods, which are\ntrade-off relations in decomposing DNNs. We define hostile factor as an element\nthat interferes with finding the attributions of the target and propagate it in\na distinguishable way to overcome the non-suppressed nature of activated\nneurons. As a result, it is possible to assign the bi-polar relevance scores of\nthe target (positive) and hostile (negative) attributions while maintaining\neach attribution aligned with the importance. We also present the purging\ntechniques to prevent the decrement of the gap between the relevance scores of\nthe target and hostile attributions during backward propagation by eliminating\nthe conflicting units to channel attribution map. Therefore, our method makes\nit possible to decompose the predictions of DNNs with clearer\nclass-discriminativeness and detailed elucidations of activation neurons\ncompared to the conventional attribution methods. In a verified experimental\nenvironment, we report the results of the assessments: (i) Pointing Game, (ii)\nmIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and\nImageNet datasets. The results demonstrate that our method outperforms existing\nbackward decomposition methods, including distinctive and intuitive\nvisualizations.",
    "published_date": "2020-12-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03434v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03293v1",
    "title": "DiffPerf: Towards Performance Differentiation and Optimization with SDN Implementation",
    "authors": [
      "Walid Aljoby",
      "Xin Wang",
      "Dinil Mon Divakaran",
      "Tom Z. J. Fu",
      "Richard T. B. Ma"
    ],
    "author_ids": [],
    "abstract": "Continuing the current trend, Internet traffic is expected to grow\nsignificantly over the coming years, with video traffic consuming the biggest\nshare. On the one hand, this growth poses challenges to access providers (APs),\nwho have to upgrade their infrastructure to meet the growing traffic demands as\nwell as find new ways to monetize their network resources. On the other hand,\ndespite numerous optimizations of the underlying transport protocol, a user's\nutilization of network bandwidth and is thus the user's perceived quality still\nbeing largely affected by network latency and buffer size. To address both\nconcerns, we propose DiffPerf, a class-based differentiation framework, that,\nat a macroscopic level dynamically allocates bandwidth to service classes\npre-defined by the APs, and at a microscopic level statistically differentiates\nand isolates user flows to help them achieve better performance. We implement\nDiffPerf on OpenDaylight SDN controller and programmable Barefoot Tofino switch\nand evaluate it from an application perspective for MPEG-DASH video streaming.\nOur evaluations demonstrate the practicality and flexibility that DiffPerf\nprovides APs with capabilities through which a spectrum of qualities are\nprovisioned at multiple classes. Meanwhile, it assists in achieving better\nfairness and improving overall user's perceived quality within the same class.",
    "published_date": "2020-12-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03293v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03272v2",
    "title": "Bayesian Persuasion under Ex Ante and Ex Post Constraints",
    "authors": [
      "Yakov Babichenko",
      "Inbal Talgam-Cohen",
      "Konstantin Zabarnyi"
    ],
    "author_ids": [],
    "abstract": "Bayesian persuasion is the study of information sharing policies among\nstrategic agents. A prime example is signaling in online ad auctions: what\ninformation should a platform signal to an advertiser regarding a user when\nselling the opportunity to advertise to her? Practical considerations such as\npreventing discrimination, protecting privacy or acknowledging limited\nattention of the information receiver impose constraints on information\nsharing. In this work, we propose and analyze a simple way to mathematically\nmodel such constraints as restrictions on Receiver's admissible posterior\nbeliefs.\n  We consider two families of constraints - ex ante and ex post, where the\nlatter limits each instance of Sender-Receiver communication, while the former\nmore general family can also pose restrictions in expectation. For the ex ante\nfamily, Doval and Skreta establish the existence of an optimal signaling scheme\nwith a small number of signals - at most the number of constraints plus the\nnumber of states of nature; we show this result is tight and provide an\nalternative proof for it. For the ex post family, we tighten a bound of\nV{\\o}lund, showing that the required number of signals is at most the number of\nstates of nature, as in the original Kamenica-Gentzkow setting. As our main\nalgorithmic result, we provide an additive bi-criteria FPTAS for an optimal\nconstrained signaling scheme assuming a constant number of states; we improve\nthe approximation to single-criteria under a Slater-like regularity condition.\nThe FPTAS holds under standard assumptions; relaxed assumptions yield a PTAS.\nFinally, we bound the ratio between Sender's optimal utility under convex ex\nante constraints and the corresponding ex post constraints. This bound applies\nto finding an approximately welfare-maximizing constrained signaling scheme in\nad auctions.",
    "published_date": "2020-12-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03272v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03170v3",
    "title": "Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis",
    "authors": [
      "Joshua Ball"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks (CNNs) have been successful in representing the\nfully-connected inferencing ability perceived to be seen in the human brain:\nthey take full advantage of the hierarchy-style patterns commonly seen in\ncomplex data and develop more patterns using simple features. Countless\nimplementations of CNNs have shown how strong their ability is to learn these\ncomplex patterns, particularly in the realm of image classification. However,\nthe cost of getting a high performance CNN to a so-called \"state of the art\"\nlevel is computationally costly. Even when using transfer learning, which\nutilize the very deep layers from models such as MobileNetV2, CNNs still take a\ngreat amount of time and resources. Linear discriminant analysis (LDA), a\ngeneralization of Fisher's linear discriminant, can be implemented in a\nmulti-class classification method to increase separability of class features\nwhile not needing a high performance system to do so for image classification.\nSimilarly, we also believe LDA has great promise in performing well. In this\npaper, we discuss our process of developing a robust CNN for food\nclassification as well as our effective implementation of multi-class LDA and\nprove that (1) CNN is superior to LDA for image classification and (2) why LDA\nshould not be left out of the races for image classification, particularly for\nbinary cases.",
    "published_date": "2020-12-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03170v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03107v3",
    "title": "When Do Curricula Work?",
    "authors": [
      "Xiaoxia Wu",
      "Ethan Dyer",
      "Behnam Neyshabur"
    ],
    "author_ids": [],
    "abstract": "Inspired by human learning, researchers have proposed ordering examples\nduring training based on their difficulty. Both curriculum learning, exposing a\nnetwork to easier examples early in training, and anti-curriculum learning,\nshowing the most difficult examples first, have been suggested as improvements\nto the standard i.i.d. training. In this work, we set out to investigate the\nrelative benefits of ordered learning. We first investigate the \\emph{implicit\ncurricula} resulting from architectural and optimization bias and find that\nsamples are learned in a highly consistent order. Next, to quantify the benefit\nof \\emph{explicit curricula}, we conduct extensive experiments over thousands\nof orderings spanning three kinds of learning: curriculum, anti-curriculum, and\nrandom-curriculum -- in which the size of the training dataset is dynamically\nincreased over time, but the examples are randomly ordered. We find that for\nstandard benchmark datasets, curricula have only marginal benefits, and that\nrandomly ordered samples perform as well or better than curricula and\nanti-curricula, suggesting that any benefit is entirely due to the dynamic\ntraining set size. Inspired by common use cases of curriculum learning in\npractice, we investigate the role of limited training time budget and noisy\ndata in the success of curriculum learning. Our experiments demonstrate that\ncurriculum, but not anti-curriculum can indeed improve the performance either\nwith limited training time budget or in existence of noisy data.",
    "published_date": "2020-12-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03107v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03075v2",
    "title": "Social System Inference from Noisy Observations",
    "authors": [
      "Yanbing Mao",
      "Naira Hovakimyan",
      "Tarek Abdelzaher",
      "Evangelos Theodorou"
    ],
    "author_ids": [],
    "abstract": "This paper studies social system inference from a single trajectory of public\nevolving opinions, wherein observation noise leads to the statistical\ndependence of samples on time and coordinates. We first propose a cyber-social\nsystem that comprises individuals in a social network and a set of information\nsources in a cyber layer, whose opinion dynamics explicitly takes confirmation\nbias, novelty bias and process noise into account. Based on the proposed social\nmodel, we then study the sample complexity of least-square auto-regressive\nmodel estimation, which governs the number of observations that are sufficient\nfor the identified model to achieve the prescribed levels of accuracy and\nconfidence. Building on the identified social model, we then investigate social\ninference, with particular focus on the weighted network topology, the\nsubconscious bias and the model parameters of confirmation bias and novelty\nbias. Finally, the theoretical results and the effectiveness of the proposed\nsocial model and inference algorithm are validated by the US Senate Member\nIdeology data.",
    "published_date": "2020-12-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03075v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03063v2",
    "title": "FairOD: Fairness-aware Outlier Detection",
    "authors": [
      "Shubhranshu Shekhar",
      "Neil Shah",
      "Leman Akoglu"
    ],
    "author_ids": [],
    "abstract": "Fairness and Outlier Detection (OD) are closely related, as it is exactly the\ngoal of OD to spot rare, minority samples in a given population. However, when\nbeing a minority (as defined by protected variables, such as\nrace/ethnicity/sex/age) does not reflect positive-class membership (such as\ncriminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD\nhas been almost untouched in prior work, as fair machine learning literature\nmainly focuses on supervised settings. Our work aims to bridge this gap.\nSpecifically, we develop desiderata capturing well-motivated fairness criteria\nfor OD, and systematically formalize the fair OD problem. Further, guided by\nour desiderata, we propose FairOD, a fairness-aware outlier detector that has\nthe following desirable properties: FairOD (1) exhibits treatment parity at\ntest time, (2) aims to flag equal proportions of samples from all groups (i.e.\nobtain group fairness, via statistical parity), and (3) strives to flag truly\nhigh-risk samples within each group. Extensive experiments on a diverse set of\nsynthetic and real world datasets show that FairOD produces outcomes that are\nfair with respect to protected variables, while performing comparable to (and\nin some cases, even better than) fairness-agnostic detectors in terms of\ndetection performance.",
    "published_date": "2020-12-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03063v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02992v1",
    "title": "Spatially-Adaptive Pixelwise Networks for Fast Image Translation",
    "authors": [
      "Tamar Rott Shaham",
      "Michael Gharbi",
      "Richard Zhang",
      "Eli Shechtman",
      "Tomer Michaeli"
    ],
    "author_ids": [],
    "abstract": "We introduce a new generator architecture, aimed at fast and efficient\nhigh-resolution image-to-image translation. We design the generator to be an\nextremely lightweight function of the full-resolution image. In fact, we use\npixel-wise networks; that is, each pixel is processed independently of others,\nthrough a composition of simple affine transformations and nonlinearities. We\ntake three important steps to equip such a seemingly simple function with\nadequate expressivity. First, the parameters of the pixel-wise networks are\nspatially varying so they can represent a broader function class than simple\n1x1 convolutions. Second, these parameters are predicted by a fast\nconvolutional network that processes an aggressively low-resolution\nrepresentation of the input; Third, we augment the input image with a\nsinusoidal encoding of spatial coordinates, which provides an effective\ninductive bias for generating realistic novel high-frequency image content. As\na result, our model is up to 18x faster than state-of-the-art baselines. We\nachieve this speedup while generating comparable visual quality across\ndifferent image resolutions and translation domains.",
    "published_date": "2020-12-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02992v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02972v3",
    "title": "Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy",
    "authors": [
      "Kit T. Rodolfa",
      "Hemank Lamba",
      "Rayid Ghani"
    ],
    "author_ids": [],
    "abstract": "Growing use of machine learning in policy and social impact settings have\nraised concerns for fairness implications, especially for racial minorities.\nThese concerns have generated considerable interest among machine learning and\nartificial intelligence researchers, who have developed new methods and\nestablished theoretical bounds for improving fairness, focusing on the source\ndata, regularization and model training, or post-hoc adjustments to model\nscores. However, little work has studied the practical trade-offs between\nfairness and accuracy in real-world settings to understand how these bounds and\nmethods translate into policy choices and impact on society. Our empirical\nstudy fills this gap by investigating the impact of mitigating disparities on\naccuracy, focusing on the common context of using machine learning to inform\nbenefit allocation in resource-constrained programs across education, mental\nhealth, criminal justice, and housing safety. Here we describe applied work in\nwhich we find fairness-accuracy trade-offs to be negligible in practice. In\neach setting studied, explicitly focusing on achieving equity and using our\nproposed post-hoc disparity mitigation methods, fairness was substantially\nimproved without sacrificing accuracy. This observation was robust across\npolicy contexts studied, scale of resources available for intervention, time,\nand relative size of the protected groups. These empirical results challenge a\ncommonly held assumption that reducing disparities either requires accepting an\nappreciable drop in accuracy or the development of novel, complex methods,\nmaking reducing disparities in these applications more practical.",
    "published_date": "2020-12-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02972v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02885v1",
    "title": "Verifiable Proof of Health using Public Key Cryptography",
    "authors": [
      "Abhishek Singh",
      "Ramesh Raskar"
    ],
    "author_ids": [],
    "abstract": "In the current pandemic, testing continues to be the most important tool for\nmonitoring and curbing the disease spread and early identification of the\ndisease to perform health-related interventions like quarantine, contact\ntracing and etc. Therefore, the ability to verify the testing status is\npertinent as public places prepare to safely open. Recent advances in\ncryptographic tools have made it possible to build a secure and resilient\ndigital-id system. In this work, we propose to build an end to end COVID-19\nresults verification protocol that takes privacy, computation, and other\npractical concerns into account for designing an inter-operable layer of\ntesting results verification system that could potentially enable less\nstringent and more selective lockdowns. We also discuss various concerns\nencompassing the security, privacy, ethics and equity aspect of the proposed\nsystem.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02885v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.02875v1",
    "title": "Inductive Bias and Language Expressivity in Emergent Communication",
    "authors": [
      "Shangmin Guo",
      "Yi Ren",
      "Agnieszka Słowik",
      "Kory Mathewson"
    ],
    "author_ids": [],
    "abstract": "Referential games and reconstruction games are the most common game types for\nstudying emergent languages. We investigate how the type of the language game\naffects the emergent language in terms of: i) language compositionality and ii)\ntransfer of an emergent language to a task different from its origin, which we\nrefer to as language expressivity. With empirical experiments on a handcrafted\nsymbolic dataset, we show that languages emerged from different games have\ndifferent compositionality and further different expressivity.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02875v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02845v4",
    "title": "Experimental Evaluation of Algorithm-Assisted Human Decision-Making: Application to Pretrial Public Safety Assessment",
    "authors": [
      "Kosuke Imai",
      "Zhichao Jiang",
      "James Greiner",
      "Ryan Halen",
      "Sooahn Shin"
    ],
    "author_ids": [],
    "abstract": "Despite an increasing reliance on fully-automated algorithmic decision-making\nin our day-to-day lives, human beings still make highly consequential\ndecisions. As frequently seen in business, healthcare, and public policy,\nrecommendations produced by algorithms are provided to human decision-makers to\nguide their decisions. While there exists a fast-growing literature evaluating\nthe bias and fairness of such algorithmic recommendations, an overlooked\nquestion is whether they help humans make better decisions. We develop a\nstatistical methodology for experimentally evaluating the causal impacts of\nalgorithmic recommendations on human decisions. We also show how to examine\nwhether algorithmic recommendations improve the fairness of human decisions and\nderive the optimal decision rules under various settings. We apply the proposed\nmethodology to preliminary data from the first-ever randomized controlled trial\nthat evaluates the pretrial Public Safety Assessment (PSA) in the criminal\njustice system. A goal of the PSA is to help judges decide which arrested\nindividuals should be released. On the basis of the preliminary data available,\nwe find that providing the PSA to the judge has little overall impact on the\njudge's decisions and subsequent arrestee behavior. However, our analysis\nyields some potentially suggestive evidence that the PSA may help avoid\nunnecessarily harsh decisions for female arrestees regardless of their risk\nlevels while it encourages the judge to make stricter decisions for male\narrestees who are deemed to be risky. In terms of fairness, the PSA appears to\nincrease the gender bias against males while having little effect on any\nexisting racial differences in judges' decision. Finally, we find that the\nPSA's recommendations might be unnecessarily severe unless the cost of a new\ncrime is sufficiently high.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02845v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.02733v2",
    "title": "Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation Learning",
    "authors": [
      "Haohang Xu",
      "Xiaopeng Zhang",
      "Hao Li",
      "Lingxi Xie",
      "Hongkai Xiong",
      "Qi Tian"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning based on instance discrimination has shown\nremarkable progress. In particular, contrastive learning, which regards each\nimage as well as its augmentations as an individual class and tries to\ndistinguish them from all other images, has been verified effective for\nrepresentation learning. However, pushing away two images that are de facto\nsimilar is suboptimal for general representation. In this paper, we propose a\nhierarchical semantic alignment strategy via expanding the views generated by a\nsingle image to \\textbf{Cross-samples and Multi-level} representation, and\nmodels the invariance to semantically similar images in a hierarchical way.\nThis is achieved by extending the contrastive loss to allow for multiple\npositives per anchor, and explicitly pulling semantically similar\nimages/patches together at different layers of the network. Our method, termed\nas CsMl, has the ability to integrate multi-level visual representations across\nsamples in a robust way. CsMl is applicable to current contrastive learning\nbased methods and consistently improves the performance. Notably, using the\nmoco as an instantiation, CsMl achieves a \\textbf{76.6\\% }top-1 accuracy with\nlinear evaluation using ResNet-50 as backbone, and \\textbf{66.7\\%} and\n\\textbf{75.1\\%} top-1 accuracy with only 1\\% and 10\\% labels, respectively.\n\\textbf{All these numbers set the new state-of-the-art.}",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02733v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02456v4",
    "title": "Characterization of Excess Risk for Locally Strongly Convex Population Risk",
    "authors": [
      "Mingyang Yi",
      "Ruoyu Wang",
      "Zhi-Ming Ma"
    ],
    "author_ids": [],
    "abstract": "We establish upper bounds for the expected excess risk of models trained by\nproper iterative algorithms which approximate the local minima. Unlike the\nresults built upon the strong globally strongly convexity or global growth\nconditions e.g., PL-inequality, we only require the population risk to be\n\\emph{locally} strongly convex around its local minima. Concretely, our bound\nunder convex problems is of order $\\tilde{\\cO}(1/n)$. For non-convex problems\nwith $d$ model parameters such that $d/n$ is smaller than a threshold\nindependent of $n$, the order of $\\tilde{\\cO}(1/n)$ can be maintained if the\nempirical risk has no spurious local minima with high probability. Moreover,\nthe bound for non-convex problem becomes $\\tilde{\\cO}(1/\\sqrt{n})$ without such\nassumption. Our results are derived via algorithmic stability and\ncharacterization of the empirical risk's landscape. Compared with the existing\nalgorithmic stability based results, our bounds are dimensional insensitive and\nwithout restrictions on the algorithm's implementation, learning rate, and the\nnumber of iterations. Our bounds underscore that with locally strongly convex\npopulation risk, the models trained by any proper iterative algorithm can\ngeneralize well, even for non-convex problems, and $d$ is large.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02456v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02447v1",
    "title": "Mitigating Bias in Federated Learning",
    "authors": [
      "Annie Abay",
      "Yi Zhou",
      "Nathalie Baracaldo",
      "Shashank Rajamoni",
      "Ebube Chuba",
      "Heiko Ludwig"
    ],
    "author_ids": [],
    "abstract": "As methods to create discrimination-aware models develop, they focus on\ncentralized ML, leaving federated learning (FL) unexplored. FL is a rising\napproach for collaborative ML, in which an aggregator orchestrates multiple\nparties to train a global model without sharing their training data. In this\npaper, we discuss causes of bias in FL and propose three pre-processing and\nin-processing methods to mitigate bias, without compromising data privacy, a\nkey FL requirement. As data heterogeneity among parties is one of the\nchallenging characteristics of FL, we conduct experiments over several data\ndistributions to analyze their effects on model performance, fairness metrics,\nand bias learning patterns. We conduct a comprehensive analysis of our proposed\ntechniques, the results demonstrating that these methods are effective even\nwhen parties have skewed data distributions or as little as 20% of parties\nemploy the methods.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02447v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02394v1",
    "title": "Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics",
    "authors": [
      "Bo Cowgill",
      "Fabrizio Dell'Acqua",
      "Samuel Deng",
      "Daniel Hsu",
      "Nakul Verma",
      "Augustin Chaintreau"
    ],
    "author_ids": [],
    "abstract": "Why do biased predictions arise? What interventions can prevent them? We\nevaluate 8.2 million algorithmic predictions of math performance from\n$\\approx$400 AI engineers, each of whom developed an algorithm under a randomly\nassigned experimental condition. Our treatment arms modified programmers'\nincentives, training data, awareness, and/or technical knowledge of AI ethics.\nWe then assess out-of-sample predictions from their algorithms using randomized\naudit manipulations of algorithm inputs and ground-truth math performance for\n20K subjects. We find that biased predictions are mostly caused by biased\ntraining data. However, one-third of the benefit of better training data comes\nthrough a novel economic mechanism: Engineers exert greater effort and are more\nresponsive to incentives when given better training data. We also assess how\nperformance varies with programmers' demographic characteristics, and their\nperformance on a psychological test of implicit bias (IAT) concerning gender\nand careers. We find no evidence that female, minority and low-IAT engineers\nexhibit lower bias or discrimination in their code. However, we do find that\nprediction errors are correlated within demographic groups, which creates\nperformance improvements through cross-demographic averaging. Finally, we\nquantify the benefits and tradeoffs of practical managerial or policy\ninterventions such as technical advice, simple reminders, and improved\nincentives for decreasing algorithmic bias.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02394v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02393v1",
    "title": "The Managerial Effects of Algorithmic Fairness Activism",
    "authors": [
      "Bo Cowgill",
      "Fabrizio Dell'Acqua",
      "Sandra Matz"
    ],
    "author_ids": [],
    "abstract": "How do ethical arguments affect AI adoption in business? We randomly expose\nbusiness decision-makers to arguments used in AI fairness activism. Arguments\nemphasizing the inescapability of algorithmic bias lead managers to abandon AI\nfor manual review by humans and report greater expectations about lawsuits and\nnegative PR. These effects persist even when AI lowers gender and racial\ndisparities and when engineering investments to address AI fairness are\nfeasible. Emphasis on status quo comparisons yields opposite effects. We also\nmeasure the effects of \"scientific veneer\" in AI ethics arguments. Scientific\nveneer changes managerial behavior but does not asymmetrically benefit\nfavorable (versus critical) AI activism.",
    "published_date": "2020-12-04T00:00:00",
    "year": 2020,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02393v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03659v2",
    "title": "Non-portability of Algorithmic Fairness in India",
    "authors": [
      "Nithya Sambasivan",
      "Erin Arnesen",
      "Ben Hutchinson",
      "Vinodkumar Prabhakaran"
    ],
    "author_ids": [],
    "abstract": "Conventional algorithmic fairness is Western in its sub-groups, values, and\noptimizations. In this paper, we ask how portable the assumptions of this\nlargely Western take on algorithmic fairness are to a different geo-cultural\ncontext such as India. Based on 36 expert interviews with Indian scholars, and\nan analysis of emerging algorithmic deployments in India, we identify three\nclusters of challenges that engulf the large distance between machine learning\nmodels and oppressed communities in India. We argue that a mere translation of\ntechnical fairness work to Indian subgroups may serve only as a window\ndressing, and instead, call for a collective re-imagining of Fair-ML, by\nre-contextualising data and models, empowering oppressed communities, and more\nimportantly, enabling ecosystems.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03659v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02166v1",
    "title": "Visualization of Supervised and Self-Supervised Neural Networks via Attribution Guided Factorization",
    "authors": [
      "Shir Gur",
      "Ameen Ali",
      "Lior Wolf"
    ],
    "author_ids": [],
    "abstract": "Neural network visualization techniques mark image locations by their\nrelevancy to the network's classification. Existing methods are effective in\nhighlighting the regions that affect the resulting classification the most.\nHowever, as we show, these methods are limited in their ability to identify the\nsupport for alternative classifications, an effect we name {\\em the saliency\nbias} hypothesis. In this work, we integrate two lines of research:\ngradient-based methods and attribution-based methods, and develop an algorithm\nthat provides per-class explainability. The algorithm back-projects the per\npixel local influence, in a manner that is guided by the local attributions,\nwhile correcting for salient features that would otherwise bias the\nexplanation. In an extensive battery of experiments, we demonstrate the ability\nof our methods to class-specific visualization, and not just the predicted\nlabel. Remarkably, the method obtains state of the art results in benchmarks\nthat are commonly applied to gradient-based methods as well as in those that\nare employed mostly for evaluating attribution methods. Using a new\nunsupervised procedure, our method is also successful in demonstrating that\nself-supervised methods learn semantic information.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02166v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02048v1",
    "title": "Ethical Testing in the Real World: Evaluating Physical Testing of Adversarial Machine Learning",
    "authors": [
      "Kendra Albert",
      "Maggie Delano",
      "Jonathon Penney",
      "Afsaneh Rigot",
      "Ram Shankar Siva Kumar"
    ],
    "author_ids": [],
    "abstract": "This paper critically assesses the adequacy and representativeness of\nphysical domain testing for various adversarial machine learning (ML) attacks\nagainst computer vision systems involving human subjects. Many papers that\ndeploy such attacks characterize themselves as \"real world.\" Despite this\nframing, however, we found the physical or real-world testing conducted was\nminimal, provided few details about testing subjects and was often conducted as\nan afterthought or demonstration. Adversarial ML research without\nrepresentative trials or testing is an ethical, scientific, and health/safety\nissue that can cause real harms. We introduce the problem and our methodology,\nand then critique the physical domain testing methodologies employed by papers\nin the field. We then explore various barriers to more inclusive physical\ntesting in adversarial ML and offer recommendations to improve such testing\nnotwithstanding these challenges.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02048v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02015v1",
    "title": "Context in Informational Bias Detection",
    "authors": [
      "Esther van den Berg",
      "Katja Markert"
    ],
    "author_ids": [],
    "abstract": "Informational bias is bias conveyed through sentences or clauses that provide\ntangential, speculative or background information that can sway readers'\nopinions towards entities. By nature, informational bias is context-dependent,\nbut previous work on informational bias detection has not explored the role of\ncontext beyond the sentence. In this paper, we explore four kinds of context\nfor informational bias in English news articles: neighboring sentences, the\nfull article, articles on the same event from other news publishers, and\narticles from the same domain (but potentially different events). We find that\nintegrating event context improves classification performance over a very\nstrong baseline. In addition, we perform the first error analysis of models on\nthis task. We find that the best-performing context-inclusive model outperforms\nthe baseline on longer sentences, and sentences from politically centrist\narticles.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02015v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2106.15255v1",
    "title": "Gender issues in fundamental physics: Strumia's bibliometric analysis fails to account for key confounders and confuses correlation with causation",
    "authors": [
      "Philip Ball",
      "T. Benjamin Britton",
      "Erin Hengel",
      "Philip Moriarty",
      "Rachel A. Oliver",
      "Gina Rippon",
      "Angela Saini",
      "Jessica Wade"
    ],
    "author_ids": [],
    "abstract": "Alessandro Strumia recently published a survey of gender differences in\npublications and citations in high-energy physics (HEP). In addition to\nproviding full access to the data, code, and methodology, Strumia (2020)\nsystematically describes and accounts for gender differences in HEP citation\nnetworks. His analysis points both to ongoing difficulties in attracting women\nto high-energy physics and an encouraging-though slow-trend in improvement.\nUnfortunately, however, the time and effort Strumia (2020) devoted to collating\nand quantifying the data are not matched by a similar rigour in interpreting\nthe results. To support his conclusions, he selectively cites available\nliterature and fails to adequately adjust for a range of confounding factors.\nFor example, his analyses do not consider how unobserved factors -- e.g., a\ntendency to overcite well-known authors -- drive a wedge between quality and\ncitations and correlate with author gender. He also fails to take into account\nmany structural and non-structural factors -- including, but not limited to,\ndirect discrimination and the expectations women form (and actions they take)\nin response to it -- that undoubtedly lead to gender differences in\nproductivity. We therefore believe that a number of Strumia's conclusions are\nnot supported by his analysis. Indeed, we re-analyse a subsample of\nsolo-authored papers from his data, adjusting for year and journal of\npublication, authors' research age and their lifetime \"fame\". Our re-analysis\nsuggests that female-authored papers are actually cited more than male-authored\npapers. This finding is inconsistent with the \"greater male variability\"\nhypothesis Strumia (2020) proposes to explain many of his results.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.DL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.15255v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.01696v2",
    "title": "FairBatch: Batch Selection for Model Fairness",
    "authors": [
      "Yuji Roh",
      "Kangwook Lee",
      "Steven Euijong Whang",
      "Changho Suh"
    ],
    "author_ids": [],
    "abstract": "Training a fair machine learning model is essential to prevent demographic\ndisparity. Existing techniques for improving model fairness require broad\nchanges in either data preprocessing or model training, rendering themselves\ndifficult-to-adopt for potentially already complex machine learning systems. We\naddress this problem via the lens of bilevel optimization. While keeping the\nstandard training algorithm as an inner optimizer, we incorporate an outer\noptimizer so as to equip the inner problem with an additional functionality:\nAdaptively selecting minibatch sizes for the purpose of improving model\nfairness. Our batch selection algorithm, which we call FairBatch, implements\nthis optimization and supports prominent fairness measures: equal opportunity,\nequalized odds, and demographic parity. FairBatch comes with a significant\nimplementation benefit -- it does not require any modification to data\npreprocessing or model training. For instance, a single-line change of PyTorch\ncode for replacing batch selection part of model training suffices to employ\nFairBatch. Our experiments conducted both on synthetic and benchmark real data\ndemonstrate that FairBatch can provide such functionalities while achieving\ncomparable (or even greater) performances against the state of the arts.\nFurthermore, FairBatch can readily improve fairness of any pre-trained model\nsimply via fine-tuning. It is also compatible with existing batch selection\ntechniques intended for different purposes, such as faster convergence, thus\ngracefully achieving multiple purposes.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01696v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01634v1",
    "title": "Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D",
    "authors": [
      "Ankit Goyal",
      "Kaiyu Yang",
      "Dawei Yang",
      "Jia Deng"
    ],
    "author_ids": [],
    "abstract": "Understanding spatial relations (e.g., \"laptop on table\") in visual input is\nimportant for both humans and robots. Existing datasets are insufficient as\nthey lack large-scale, high-quality 3D ground truth information, which is\ncritical for learning spatial relations. In this paper, we fill this gap by\nconstructing Rel3D: the first large-scale, human-annotated dataset for\ngrounding spatial relations in 3D. Rel3D enables quantifying the effectiveness\nof 3D information in predicting spatial relations on large-scale human data.\nMoreover, we propose minimally contrastive data collection -- a novel\ncrowdsourcing method for reducing dataset bias. The 3D scenes in our dataset\ncome in minimally contrastive pairs: two scenes in a pair are almost identical,\nbut a spatial relation holds in one and fails in the other. We empirically\nvalidate that minimally contrastive examples can diagnose issues with current\nrelation detection models as well as lead to sample-efficient training. Code\nand data are available at https://github.com/princeton-vl/Rel3D.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01634v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01604v2",
    "title": "Going Beyond Classification Accuracy Metrics in Model Compression",
    "authors": [
      "Vinu Joseph",
      "Shoaib Ahmed Siddiqui",
      "Aditya Bhaskara",
      "Ganesh Gopalakrishnan",
      "Saurav Muralidharan",
      "Michael Garland",
      "Sheraz Ahmed",
      "Andreas Dengel"
    ],
    "author_ids": [],
    "abstract": "With the rise in edge-computing devices, there has been an increasing demand\nto deploy energy and resource-efficient models. A large body of research has\nbeen devoted to developing methods that can reduce the size of the model\nconsiderably without affecting the standard metrics such as top-1 accuracy.\nHowever, these pruning approaches tend to result in a significant mismatch in\nother metrics such as fairness across classes and explainability. To combat\nsuch misalignment, we propose a novel multi-part loss function inspired by the\nknowledge-distillation literature. Through extensive experiments, we\ndemonstrate the effectiveness of our approach across different compression\nalgorithms, architectures, tasks as well as datasets. In particular, we obtain\nup to $4.1\\times$ reduction in the number of prediction mismatches between the\ncompressed and reference models, and up to $5.7\\times$ in cases where the\nreference model makes the correct prediction; all while making no changes to\nthe compression algorithm, and minor modifications to the loss function.\nFurthermore, we demonstrate how inducing simple alignment between the\npredictions of the models naturally improves the alignment on other metrics\nincluding fairness and attributions. Our framework can thus serve as a simple\nplug-and-play component for compression algorithms in the future.",
    "published_date": "2020-12-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01604v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01469v3",
    "title": "Fair Attribute Classification through Latent Space De-biasing",
    "authors": [
      "Vikram V. Ramaswamy",
      "Sunnie S. Y. Kim",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Fairness in visual recognition is becoming a prominent and critical topic of\ndiscussion as recognition systems are deployed at scale in the real world.\nModels trained from data in which target labels are correlated with protected\nattributes (e.g., gender, race) are known to learn and exploit those\ncorrelations. In this work, we introduce a method for training accurate target\nclassifiers while mitigating biases that stem from these correlations. We use\nGANs to generate realistic-looking images, and perturb these images in the\nunderlying latent space to generate training data that is balanced for each\nprotected attribute. We augment the original dataset with this perturbed\ngenerated data, and empirically demonstrate that target classifiers trained on\nthe augmented dataset exhibit a number of both quantitative and qualitative\nbenefits. We conduct a thorough evaluation across multiple target labels and\nprotected attributes in the CelebA dataset, and provide an in-depth analysis\nand comparison to existing literature in the space.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01469v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01300v1",
    "title": "Learning from others' mistakes: Avoiding dataset biases without modeling them",
    "authors": [
      "Victor Sanh",
      "Thomas Wolf",
      "Yonatan Belinkov",
      "Alexander M. Rush"
    ],
    "author_ids": [],
    "abstract": "State-of-the-art natural language processing (NLP) models often learn to\nmodel dataset biases and surface form correlations instead of features that\ntarget the intended underlying task. Previous work has demonstrated effective\nmethods to circumvent these issues when knowledge of the bias is available. We\nconsider cases where the bias issues may not be explicitly identified, and show\na method for training models that learn to ignore these problematic\ncorrelations. Our approach relies on the observation that models with limited\ncapacity primarily learn to exploit biases in the dataset. We can leverage the\nerrors of such limited capacity models to train a more robust model in a\nproduct of experts, thus bypassing the need to hand-craft a biased model. We\nshow the effectiveness of this method to retain improvements in\nout-of-distribution settings even if no particular bias is targeted by the\nbiased model.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01300v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01273v1",
    "title": "Regularization and False Alarms Quantification: Two Sides of the Explainability Coin",
    "authors": [
      "Nima Safaei",
      "Pooria Assadi"
    ],
    "author_ids": [],
    "abstract": "Regularization is a well-established technique in machine learning (ML) to\nachieve an optimal bias-variance trade-off which in turn reduces model\ncomplexity and enhances explainability. To this end, some hyper-parameters must\nbe tuned, enabling the ML model to accurately fit the unseen data as well as\nthe seen data. In this article, the authors argue that the regularization of\nhyper-parameters and quantification of costs and risks of false alarms are in\nreality two sides of the same coin, explainability. Incorrect or non-existent\nestimation of either quantities undermines the measurability of the economic\nvalue of using ML, to the extent that might make it practically useless.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01273v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01271v1",
    "title": "Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing",
    "authors": [
      "Taewook Kim",
      "Yonghyun Kim"
    ],
    "author_ids": [],
    "abstract": "Face anti-spoofing aims to prevent false authentications of face recognition\nsystems by distinguishing whether an image is originated from a human face or a\nspoof medium. We propose a novel method called Doubly Adversarial Suppression\nNetwork (DASN) for domain-agnostic face anti-spoofing; DASN improves the\ngeneralization ability to unseen domains by learning to effectively suppress\nspoof-irrelevant factors (SiFs) (e.g., camera sensors, illuminations). To\nachieve our goal, we introduce two types of adversarial learning schemes. In\nthe first adversarial learning scheme, multiple SiFs are suppressed by\ndeploying multiple discrimination heads that are trained against an encoder. In\nthe second adversarial learning scheme, each of the discrimination heads is\nalso adversarially trained to suppress a spoof factor, and the group of the\nsecondary spoof classifier and the encoder aims to intensify the spoof factor\nby overcoming the suppression. We evaluate the proposed method on four public\nbenchmark datasets, and achieve remarkable evaluation results. The results\ndemonstrate the effectiveness of the proposed method.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01271v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.07749v1",
    "title": "Towards Fairness in Classifying Medical Conversations into SOAP Sections",
    "authors": [
      "Elisa Ferracane",
      "Sandeep Konam"
    ],
    "author_ids": [],
    "abstract": "As machine learning algorithms are more widely deployed in healthcare, the\nquestion of algorithmic fairness becomes more critical to examine. Our work\nseeks to identify and understand disparities in a deployed model that\nclassifies doctor-patient conversations into sections of a medical SOAP note.\nWe employ several metrics to measure disparities in the classifier performance,\nand find small differences in a portion of the disadvantaged groups. A deeper\nanalysis of the language in these conversations and further stratifying the\ngroups suggests these differences are related to and often attributable to the\ntype of medical appointment (e.g., psychiatric vs. internist). Our findings\nstress the importance of understanding the disparities that may exist in the\ndata itself and how that affects a model's ability to equally distribute\nbenefits.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07749v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02292v1",
    "title": "FAST: A Fairness Assured Service Recommendation Strategy Considering Service Capacity Constraint",
    "authors": [
      "Yao Wu",
      "Jian Cao",
      "Guandong Xu"
    ],
    "author_ids": [],
    "abstract": "An excessive number of customers often leads to a degradation in service\nquality. However, the capacity constraints of services are ignored by\nrecommender systems, which may lead to unsatisfactory recommendation. This\nproblem can be solved by limiting the number of users who receive the\nrecommendation for a service, but this may be viewed as unfair. In this paper,\nwe propose a novel metric Top-N Fairness to measure the individual fairness of\nmulti-round recommendations of services with capacity constraints. By\nconsidering the fact that users are often only affected by top-ranked items in\na recommendation, Top-N Fairness only considers a sub-list consisting of top N\nservices. Based on the metric, we design FAST, a Fairness Assured service\nrecommendation STrategy. FAST adjusts the original recommendation list to\nprovide users with recommendation results that guarantee the long-term fairness\nof multi-round recommendations. We prove the convergence property of the\nvariance of Top-N Fairness of FAST theoretically. FAST is tested on the Yelp\ndataset and synthetic datasets. The experimental results show that FAST\nachieves better recommendation fairness while still maintaining high\nrecommendation quality.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02292v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01067v2",
    "title": "Making Weak Memory Models Fair",
    "authors": [
      "Ori Lahav",
      "Egor Namakonov",
      "Jonas Oberhauser",
      "Anton Podkopaev",
      "Viktor Vafeiadis"
    ],
    "author_ids": [],
    "abstract": "Liveness properties, such as termination, of even the simplest shared-memory\nconcurrent programs under sequential consistency typically require some\nfairness assumptions about the scheduler. Under weak memory models, we observe\nthat the standard notions of thread fairness are insufficient, and an\nadditional fairness property, which we call memory fairness, is needed. In this\npaper, we propose a uniform definition for memory fairness that can be\nintegrated into any declarative memory model enforcing acyclicity of the union\nof the program order and the reads-from relation. For the well-known models,\nSC, x86-TSO, RA, and StrongCOH, that have equivalent operational and\ndeclarative presentations, we show that our declarative memory fairness\ncondition is equivalent to an intuitive model-specific operational notion of\nmemory fairness, which requires the memory system to fairly execute its\ninternal propagation steps. Our fairness condition preserves the correctness of\nlocal transformations and the compilation scheme from RC11 to x86-TSO, and also\nenables the first formal proofs of termination of mutual exclusion lock\nimplementations under declarative weak memory models.",
    "published_date": "2020-12-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.PL",
      "D.3.1; F.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01067v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.00617v1",
    "title": "Overcoming the limitations of patch-based learning to detect cancer in whole slide images",
    "authors": [
      "Ozan Ciga",
      "Tony Xu",
      "Sharon Nofech-Mozes",
      "Shawna Noy",
      "Fang-I Lu",
      "Anne L. Martel"
    ],
    "author_ids": [],
    "abstract": "Whole slide images (WSIs) pose unique challenges when training deep learning\nmodels. They are very large which makes it necessary to break each image down\ninto smaller patches for analysis, image features have to be extracted at\nmultiple scales in order to capture both detail and context, and extreme class\nimbalances may exist. Significant progress has been made in the analysis of\nthese images, thanks largely due to the availability of public annotated\ndatasets. We postulate, however, that even if a method scores well on a\nchallenge task, this success may not translate to good performance in a more\nclinically relevant workflow. Many datasets consist of image patches which may\nsuffer from data curation bias; other datasets are only labelled at the whole\nslide level and the lack of annotations across an image may mask erroneous\nlocal predictions so long as the final decision is correct. In this paper, we\noutline the differences between patch or slide-level classification versus\nmethods that need to localize or segment cancer accurately across the whole\nslide, and we experimentally verify that best practices differ in both cases.\nWe apply a binary cancer detection network on post neoadjuvant therapy breast\ncancer WSIs to find the tumor bed outlining the extent of cancer, a task which\nrequires sensitivity and precision across the whole slide. We extensively study\nmultiple design choices and their effects on the outcome, including\narchitectures and augmentations. Furthermore, we propose a negative data\nsampling strategy, which drastically reduces the false positive rate (7% on\nslide level) and improves each metric pertinent to our problem, with a 15%\nreduction in the error of tumor extent.",
    "published_date": "2020-12-01T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00617v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.00423v2",
    "title": "Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring",
    "authors": [
      "Tom Sühr",
      "Sophie Hilgard",
      "Himabindu Lakkaraju"
    ],
    "author_ids": [],
    "abstract": "Ranking algorithms are being widely employed in various online hiring\nplatforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has\ndemonstrated that ranking algorithms employed by these platforms are prone to a\nvariety of undesirable biases, leading to the proposal of fair ranking\nalgorithms (e.g., Det-Greedy) which increase exposure of underrepresented\ncandidates. However, there is little to no work that explores whether fair\nranking algorithms actually improve real world outcomes (e.g., hiring\ndecisions) for underrepresented groups. Furthermore, there is no clear\nunderstanding as to how other factors (e.g., job context, inherent biases of\nthe employers) may impact the efficacy of fair ranking in practice. In this\nwork, we analyze various sources of gender biases in online hiring platforms,\nincluding the job context and inherent biases of employers and establish how\nthese factors interact with ranking algorithms to affect hiring decisions. To\nthe best of our knowledge, this work makes the first attempt at studying the\ninterplay between the aforementioned factors in the context of online hiring.\nWe carry out a largescale user study simulating online hiring scenarios with\ndata from TaskRabbit, a popular online freelancing site. Our results\ndemonstrate that while fair ranking algorithms generally improve the selection\nrates of underrepresented minorities, their effectiveness relies heavily on the\njob contexts and candidate profiles.",
    "published_date": "2020-12-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00423v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.00360v1",
    "title": "Symbolic AI for XAI: Evaluating LFIT Inductive Programming for Fair and Explainable Automatic Recruitment",
    "authors": [
      "Alfonso Ortega",
      "Julian Fierrez",
      "Aythami Morales",
      "Zilong Wang",
      "Tony Ribeiro"
    ],
    "author_ids": [],
    "abstract": "Machine learning methods are growing in relevance for biometrics and personal\ninformation processing in domains such as forensics, e-health, recruitment, and\ne-learning. In these domains, white-box (human-readable) explanations of\nsystems built on machine learning methods can become crucial. Inductive Logic\nProgramming (ILP) is a subfield of symbolic AI aimed to automatically learn\ndeclarative theories about the process of data. Learning from Interpretation\nTransition (LFIT) is an ILP technique that can learn a propositional logic\ntheory equivalent to a given black-box system (under certain conditions). The\npresent work takes a first step to a general methodology to incorporate\naccurate declarative explanations to classic machine learning by checking the\nviability of LFIT in a specific AI application scenario: fair recruitment based\non an automatic tool generated with machine learning methods for ranking\nCurricula Vitae that incorporates soft biometric information (gender and\nethnicity). We show the expressiveness of LFIT for this specific problem and\npropose a scheme that can be applicable to other domains.",
    "published_date": "2020-12-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.00282v2",
    "title": "FairFaceGAN: Fairness-aware Facial Image-to-Image Translation",
    "authors": [
      "Sunhee Hwang",
      "Sungho Park",
      "Dohyung Kim",
      "Mirae Do",
      "Hyeran Byun"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce FairFaceGAN, a fairness-aware facial\nImage-to-Image translation model, mitigating the problem of unwanted\ntranslation in protected attributes (e.g., gender, age, race) during facial\nattributes editing. Unlike existing models, FairFaceGAN learns fair\nrepresentations with two separate latents - one related to the target\nattributes to translate, and the other unrelated to them. This strategy enables\nFairFaceGAN to separate the information about protected attributes and that of\ntarget attributes. It also prevents unwanted translation in protected\nattributes while target attributes editing. To evaluate the degree of fairness,\nwe perform two types of experiments on CelebA dataset. First, we compare the\nfairness-aware classification performances when augmenting data by existing\nimage translation methods and FairFaceGAN respectively. Moreover, we propose a\nnew fairness metric, namely Frechet Protected Attribute Distance (FPAD), which\nmeasures how well protected attributes are preserved. Experimental results\ndemonstrate that FairFaceGAN shows consistent improvements in terms of fairness\nover the existing image translation models. Further, we also evaluate image\ntranslation performances, where FairFaceGAN shows competitive results, compared\nto those of existing methods.",
    "published_date": "2020-12-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00282v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.00136v1",
    "title": "A Critique of Immunity Passports and W3C Decentralized Identifiers",
    "authors": [
      "Harry Halpin"
    ],
    "author_ids": [],
    "abstract": "Due to the widespread COVID-19 pandemic, there has been a push for `immunity\npassports' and even technical proposals. Although the debate about the medical\nand ethical problems of immunity passports has been widespread, there has been\nless inspection of the technical foundations of immunity passport schemes.\nThese schemes are envisaged to be used for sharing COVID-19 test and\nvaccination results in general. The most prominent immunity passport schemes\nhave involved a stack of little-known standards, such as Decentralized\nIdentifiers (DIDs) and Verifiable Credentials (VCs) from the World Wide Web\nConsortium (W3C). Our analysis shows that this group of technical identity\nstandards are based on under-specified and often non-standardized documents\nthat have substantial security and privacy issues, due in part to the\nquestionable use of blockchain technology. One concrete proposal for immunity\npassports is even susceptible to dictionary attacks. The use of `cryptography\ntheater' in efforts like immunity passports, where cryptography is used to\nallay the privacy concerns of users, should be discouraged in standardization.\nDeployment of these W3C standards for `self-sovereign identity' in use-cases\nlike immunity passports could just as well lead to a dangerous form identity\ntotalitarianism.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00136v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.00106v1",
    "title": "Towards Auditability for Fairness in Deep Learning",
    "authors": [
      "Ivoline C. Ngong",
      "Krystal Maughan",
      "Joseph P. Near"
    ],
    "author_ids": [],
    "abstract": "Group fairness metrics can detect when a deep learning model behaves\ndifferently for advantaged and disadvantaged groups, but even models that score\nwell on these metrics can make blatantly unfair predictions. We present smooth\nprediction sensitivity, an efficiently computed measure of individual fairness\nfor deep learning models that is inspired by ideas from interpretability in\ndeep learning. smooth prediction sensitivity allows individual predictions to\nbe audited for fairness. We present preliminary experimental results suggesting\nthat smooth prediction sensitivity can help distinguish between fair and unfair\npredictions, and that it may be helpful in detecting blatantly unfair\npredictions from \"group-fair\" models.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00106v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.15093v3",
    "title": "Reducing Textural Bias Improves Robustness of Deep Segmentation Models",
    "authors": [
      "Seoin Chai",
      "Daniel Rueckert",
      "Ahmed E. Fetit"
    ],
    "author_ids": [],
    "abstract": "Despite advances in deep learning, robustness under domain shift remains a\nmajor bottleneck in medical imaging settings. Findings on natural images\nsuggest that deep neural models can show a strong textural bias when carrying\nout image classification tasks. In this thorough empirical study, we draw\ninspiration from findings on natural images and investigate ways in which\naddressing the textural bias phenomenon could bring up the robustness of deep\nsegmentation models when applied to three-dimensional (3D) medical data. To\nachieve this, publicly available MRI scans from the Developing Human Connectome\nProject are used to study ways in which simulating textural noise can help\ntrain robust models in a complex semantic segmentation task. We contribute an\nextensive empirical investigation consisting of 176 experiments and illustrate\nhow applying specific types of simulated textural noise prior to training can\nlead to texture invariant models, resulting in improved robustness when\nsegmenting scans corrupted by previously unseen noise types and levels.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.15093v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14966v1",
    "title": "Depression Status Estimation by Deep Learning based Hybrid Multi-Modal Fusion Model",
    "authors": [
      "Hrithwik Shalu",
      "Harikrishnan P",
      "Hari Sankar CN",
      "Akash Das",
      "Saptarshi Majumder",
      "Arnhav Datar",
      "Subin Mathew MS",
      "Anugyan Das",
      "Juned Kadiwala"
    ],
    "author_ids": [],
    "abstract": "Preliminary detection of mild depression could immensely help in effective\ntreatment of the common mental health disorder. Due to the lack of proper\nawareness and the ample mix of stigmas and misconceptions present within the\nsociety, mental health status estimation has become a truly difficult task. Due\nto the immense variations in character level traits from person to person,\ntraditional deep learning methods fail to generalize in a real world setting.\nIn our study we aim to create a human allied AI workflow which could\nefficiently adapt to specific users and effectively perform in real world\nscenarios. We propose a Hybrid deep learning approach that combines the essence\nof one shot learning, classical supervised deep learning methods and human\nallied interactions for adaptation. In order to capture maximum information and\nmake efficient diagnosis video, audio, and text modalities are utilized. Our\nHybrid Fusion model achieved a high accuracy of 96.3% on the Dataset; and\nattained an AUC of 0.9682 which proves its robustness in discriminating classes\nin complex real-world scenarios making sure that no cases of mild depression\nare missed during diagnosis. The proposed method is deployed in a cloud-based\nsmartphone application for robust testing. With user-specific adaptations and\nstate of the art methodologies, we present a state-of-the-art model with user\nfriendly experience.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14966v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14906v1",
    "title": "Person Perception Biases Exposed: Revisiting the First Impressions Dataset",
    "authors": [
      "Julio C. S. Jacques Junior",
      "Agata Lapedriza",
      "Cristina Palmero",
      "Xavier Baró",
      "Sergio Escalera"
    ],
    "author_ids": [],
    "abstract": "This work revisits the ChaLearn First Impressions database, annotated for\npersonality perception using pairwise comparisons via crowdsourcing. We analyse\nfor the first time the original pairwise annotations, and reveal existing\nperson perception biases associated to perceived attributes like gender,\nethnicity, age and face attractiveness. We show how person perception bias can\ninfluence data labelling of a subjective task, which has received little\nattention from the computer vision and machine learning communities by now. We\nfurther show that the mechanism used to convert pairwise annotations to\ncontinuous values may magnify the biases if no special treatment is considered.\nThe findings of this study are relevant for the computer vision community that\nis still creating new datasets on subjective tasks, and using them for\npractical applications, ignoring these perceptual biases.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14906v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14871v1",
    "title": "ViDi: Descriptive Visual Data Clustering as Radiologist Assistant in COVID-19 Streamline Diagnostic",
    "authors": [
      "Sahithya Ravi",
      "Samaneh Khoshrou",
      "Mykola Pechenizkiy"
    ],
    "author_ids": [],
    "abstract": "In the light of the COVID-19 pandemic, deep learning methods have been widely\ninvestigated in detecting COVID-19 from chest X-rays. However, a more pragmatic\napproach to applying AI methods to a medical diagnosis is designing a framework\nthat facilitates human-machine interaction and expert decision making. Studies\nhave shown that categorization can play an essential rule in accelerating\nreal-world decision making. Inspired by descriptive document clustering, we\npropose a domain-independent explanatory clustering framework to group\ncontextually related instances and support radiologists' decision making. While\nmost descriptive clustering approaches employ domain-specific characteristics\nto form meaningful clusters, we focus on model-level explanation as a more\ngeneral-purpose element of every learning process to achieve cluster\nhomogeneity. We employ DeepSHAP to generate homogeneous clusters in terms of\ndisease severity and describe the clusters using favorable and unfavorable\nsaliency maps, which visualize the class discriminating regions of an image.\nThese human-interpretable maps complement radiologist knowledge to investigate\nthe whole cluster at once. Besides, as part of this study, we evaluate a model\nbased on VGG-19, which can identify COVID and pneumonia cases with a positive\npredictive value of 95% and 97%, respectively, comparable to the recent\nexplainable approaches for COVID diagnosis.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14871v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14646v1",
    "title": "Prior and Prejudice: The Novice Reviewers' Bias against Resubmissions in Conference Peer Review",
    "authors": [
      "Ivan Stelmakh",
      "Nihar B. Shah",
      "Aarti Singh",
      "Hal Daumé III"
    ],
    "author_ids": [],
    "abstract": "Modern machine learning and computer science conferences are experiencing a\nsurge in the number of submissions that challenges the quality of peer review\nas the number of competent reviewers is growing at a much slower rate. To curb\nthis trend and reduce the burden on reviewers, several conferences have started\nencouraging or even requiring authors to declare the previous submission\nhistory of their papers. Such initiatives have been met with skepticism among\nauthors, who raise the concern about a potential bias in reviewers'\nrecommendations induced by this information. In this work, we investigate\nwhether reviewers exhibit a bias caused by the knowledge that the submission\nunder review was previously rejected at a similar venue, focusing on a\npopulation of novice reviewers who constitute a large fraction of the reviewer\npool in leading machine learning and computer science conferences. We design\nand conduct a randomized controlled trial closely replicating the relevant\ncomponents of the peer-review pipeline with $133$ reviewers (master's, junior\nPhD students, and recent graduates of top US universities) writing reviews for\n$19$ papers. The analysis reveals that reviewers indeed become negatively\nbiased when they receive a signal about paper being a resubmission, giving\nalmost 1 point lower overall score on a 10-point Likert item ($\\Delta = -0.78,\n\\ 95\\% \\ \\text{CI} = [-1.30, -0.24]$) than reviewers who do not receive such a\nsignal. Looking at specific criteria scores (originality, quality, clarity and\nsignificance), we observe that novice reviewers tend to underrate quality the\nmost.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14646v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01930v1",
    "title": "Learning Explainable Interventions to Mitigate HIV Transmission in Sex Workers Across Five States in India",
    "authors": [
      "Raghav Awasthi",
      "Prachi Patel",
      "Vineet Joshi",
      "Shama Karkal",
      "Tavpritesh Sethi"
    ],
    "author_ids": [],
    "abstract": "Female sex workers(FSWs) are one of the most vulnerable and stigmatized\ngroups in society. As a result, they often suffer from a lack of quality access\nto care. Grassroot organizations engaged in improving health services are often\nfaced with the challenge of improving the effectiveness of interventions due to\ncomplex influences. This work combines structure learning, discriminative\nmodeling, and grass-root level expertise of designing interventions across five\ndifferent Indian states to discover the influence of non-obvious factors for\nimproving safe-sex practices in FSWs. A bootstrapped, ensemble-averaged\nBayesian Network structure was learned to quantify the factors that could\nmaximize condom usage as revealed from the model. A discriminative model was\nthen constructed using XgBoost and random forest in order to predict condom use\nbehavior The best model achieved 83% sensitivity, 99% specificity, and 99% area\nunder the precision-recall curve for the prediction. Both generative and\ndiscriminative modeling approaches revealed that financial literacy training\nwas the primary influence and predictor of condom use in FSWs. These insights\nhave led to a currently ongoing field trial for assessing the real-world\nutility of this approach. Our work highlights the potential of explainable\nmodels for transparent discovery and prioritization of anti-HIV interventions\nin female sex workers in a resource-limited setting.",
    "published_date": "2020-11-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01930v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.03769v3",
    "title": "Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation",
    "authors": [
      "August DuMont Schütte",
      "Jürgen Hetzel",
      "Sergios Gatidis",
      "Tobias Hepp",
      "Benedikt Dietz",
      "Stefan Bauer",
      "Patrick Schwab"
    ],
    "author_ids": [],
    "abstract": "Privacy concerns around sharing personally identifiable information are a\nmajor practical barrier to data sharing in medical research. However, in many\ncases, researchers have no interest in a particular individual's information\nbut rather aim to derive insights at the level of cohorts. Here, we utilize\nGenerative Adversarial Networks (GANs) to create derived medical imaging\ndatasets consisting entirely of synthetic patient data. The synthetic images\nideally have, in aggregate, similar statistical properties to those of a source\ndataset but do not contain sensitive personal information. We assess the\nquality of synthetic data generated by two GAN models for chest radiographs\nwith 14 different radiology findings and brain computed tomography (CT) scans\nwith six types of intracranial hemorrhages. We measure the synthetic image\nquality by the performance difference of predictive models trained on either\nthe synthetic or the real dataset. We find that synthetic data performance\ndisproportionately benefits from a reduced number of unique label combinations.\nOur open-source benchmark also indicates that at low number of samples per\nclass, label overfitting effects start to dominate GAN training. We\nadditionally conducted a reader study in which trained radiologists do not\nperform better than random on discriminating between synthetic and real medical\nimages for intermediate levels of resolutions. In accordance with our benchmark\nresults, the classification accuracy of radiologists increases at higher\nspatial resolution levels. Our study offers valuable guidelines and outlines\npractical conditions under which insights derived from synthetic medical images\nare similar to those that would have been derived from real imaging data. Our\nresults indicate that synthetic data sharing may be an attractive and\nprivacy-preserving alternative to sharing real patient-level data in the right\nsettings.",
    "published_date": "2020-11-29T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03769v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14317v3",
    "title": "FROCC: Fast Random projection-based One-Class Classification",
    "authors": [
      "Arindam Bhattacharya",
      "Sumanth Varambally",
      "Amitabha Bagchi",
      "Srikanta Bedathur"
    ],
    "author_ids": [],
    "abstract": "We present Fast Random projection-based One-Class Classification (FROCC), an\nextremely efficient method for one-class classification. Our method is based on\na simple idea of transforming the training data by projecting it onto a set of\nrandom unit vectors that are chosen uniformly and independently from the unit\nsphere, and bounding the regions based on separation of the data. FROCC can be\nnaturally extended with kernels. We theoretically prove that FROCC generalizes\nwell in the sense that it is stable and has low bias. FROCC achieves up to 3.1\npercent points better ROC, with 1.2--67.8x speedup in training and test times\nover a range of state-of-the-art benchmarks including the SVM and the deep\nlearning based models for the OCC task.",
    "published_date": "2020-11-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14317v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14293v1",
    "title": "Inflating Topic Relevance with Ideology: A Case Study of Political Ideology Bias in Social Topic Detection Models",
    "authors": [
      "Meiqi Guo",
      "Rebecca Hwa",
      "Yu-Ru Lin",
      "Wen-Ting Chung"
    ],
    "author_ids": [],
    "abstract": "We investigate the impact of political ideology biases in training data.\nThrough a set of comparison studies, we examine the propagation of biases in\nseveral widely-used NLP models and its effect on the overall retrieval\naccuracy. Our work highlights the susceptibility of large, complex models to\npropagating the biases from human-selected input, which may lead to a\ndeterioration of retrieval accuracy, and the importance of controlling for\nthese biases. Finally, as a way to mitigate the bias, we propose to learn a\ntext representation that is invariant to political ideology while still judging\ntopic relevance.",
    "published_date": "2020-11-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14293v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14269v4",
    "title": "Generalization and Memorization: The Bias Potential Model",
    "authors": [
      "Hongkang Yang",
      "Weinan E"
    ],
    "author_ids": [],
    "abstract": "Models for learning probability distributions such as generative models and\ndensity estimators behave quite differently from models for learning functions.\nOne example is found in the memorization phenomenon, namely the ultimate\nconvergence to the empirical distribution, that occurs in generative\nadversarial networks (GANs). For this reason, the issue of generalization is\nmore subtle than that for supervised learning. For the bias potential model, we\nshow that dimension-independent generalization accuracy is achievable if early\nstopping is adopted, despite that in the long term, the model either memorizes\nthe samples or diverges.",
    "published_date": "2020-11-29T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "68T07, 60-08"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14269v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14126v5",
    "title": "Risk-Monotonicity in Statistical Learning",
    "authors": [
      "Zakaria Mhammedi"
    ],
    "author_ids": [],
    "abstract": "Acquisition of data is a difficult task in many applications of machine\nlearning, and it is only natural that one hopes and expects the population risk\nto decrease (better performance) monotonically with increasing data points. It\nturns out, somewhat surprisingly, that this is not the case even for the most\nstandard algorithms that minimize the empirical risk. Non-monotonic behavior of\nthe risk and instability in training have manifested and appeared in the\npopular deep learning paradigm under the description of double descent. These\nproblems highlight the current lack of understanding of learning algorithms and\ngeneralization. It is, therefore, crucial to pursue this concern and provide a\ncharacterization of such behavior. In this paper, we derive the first\nconsistent and risk-monotonic (in high probability) algorithms for a general\nstatistical learning setting under weak assumptions, consequently answering\nsome questions posed by Viering et al. 2019 on how to avoid non-monotonic\nbehavior of risk curves. We further show that risk monotonicity need not\nnecessarily come at the price of worse excess risk rates. To achieve this, we\nderive new empirical Bernstein-like concentration inequalities of independent\ninterest that hold for certain non-i.i.d.~processes such as Martingale\nDifference Sequences.",
    "published_date": "2020-11-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14126v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.14075v1",
    "title": "Feedback Effects in Repeat-Use Criminal Risk Assessments",
    "authors": [
      "Benjamin Laufer"
    ],
    "author_ids": [],
    "abstract": "In the criminal legal context, risk assessment algorithms are touted as\ndata-driven, well-tested tools. Studies known as validation tests are typically\ncited by practitioners to show that a particular risk assessment algorithm has\npredictive accuracy, establishes legitimate differences between risk groups,\nand maintains some measure of group fairness in treatment. To establish these\nimportant goals, most tests use a one-shot, single-point measurement. Using a\nPolya Urn model, we explore the implication of feedback effects in sequential\nscoring-decision processes. We show through simulation that risk can propagate\nover sequential decisions in ways that are not captured by one-shot tests. For\nexample, even a very small or undetectable level of bias in risk allocation can\namplify over sequential risk-based decisions, leading to observable group\ndifferences after a number of decision iterations. Risk assessment tools\noperate in a highly complex and path-dependent process, fraught with historical\ninequity. We conclude from this study that these tools do not properly account\nfor compounding effects, and require new approaches to development and\nauditing.",
    "published_date": "2020-11-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.DS",
      "cs.LG",
      "cs.SI",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14075v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.01193v1",
    "title": "Black Loans Matter: Distributionally Robust Fairness for Fighting Subgroup Discrimination",
    "authors": [
      "Mark Weber",
      "Mikhail Yurochkin",
      "Sherif Botros",
      "Vanio Markov"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness in lending today relies on group fairness metrics for\nmonitoring statistical parity across protected groups. This approach is\nvulnerable to subgroup discrimination by proxy, carrying significant risks of\nlegal and reputational damage for lenders and blatantly unfair outcomes for\nborrowers. Practical challenges arise from the many possible combinations and\nsubsets of protected groups. We motivate this problem against the backdrop of\nhistorical and residual racism in the United States polluting all available\ntraining data and raising public sensitivity to algorithimic bias. We review\nthe current regulatory compliance protocols for fairness in lending and discuss\ntheir limitations relative to the contributions state-of-the-art fairness\nmethods may afford. We propose a solution for addressing subgroup\ndiscrimination, while adhering to existing group fairness requirements, from\nrecent developments in individual fairness methods and corresponding fair\nmetric learning algorithms.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.01193v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.13988v2",
    "title": "Reducing Discrimination in Learning Algorithms for Social Good in Sociotechnical Systems",
    "authors": [
      "Katelyn Morrison"
    ],
    "author_ids": [],
    "abstract": "Sociotechnical systems within cities are now equipped with machine learning\nalgorithms in hopes to increase efficiency and functionality by modeling and\npredicting trends. Machine learning algorithms have been applied in these\ndomains to address challenges such as balancing the distribution of bikes\nthroughout a city and identifying demand hotspots for ride sharing drivers.\nHowever, these algorithms applied to challenges in sociotechnical systems have\nexacerbated social inequalities due to previous bias in data sets or the lack\nof data from marginalized communities. In this paper, I will address how smart\nmobility initiatives in cities use machine learning algorithms to address\nchallenges. I will also address how these algorithms unintentionally\ndiscriminate against features such as socioeconomic status to motivate the\nimportance of algorithmic fairness. Using the bike sharing program in\nPittsburgh, PA, I will present a position on how discrimination can be\neliminated from the pipeline using Bayesian Optimization.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13988v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.02104v2",
    "title": "Discriminatory Expressions to Produce Interpretable Models in Short Documents",
    "authors": [
      "Manuel Francisco",
      "Juan Luis Castro"
    ],
    "author_ids": [],
    "abstract": "Social Networking Sites (SNS) are one of the most important ways of\ncommunication. In particular, microblogging sites are being used as analysis\navenues due to their peculiarities (promptness, short texts...). There are\ncountless researches that use SNS in novel manners, but machine learning has\nfocused mainly in classification performance rather than interpretability\nand/or other goodness metrics. Thus, state-of-the-art models are black boxes\nthat should not be used to solve problems that may have a social impact. When\nthe problem requires transparency, it is necessary to build interpretable\npipelines. Although the classifier may be interpretable, resulting models are\ntoo complex to be considered comprehensible, making it impossible for humans to\nunderstand the actual decisions. This paper presents a feature selection\nmechanism that is able to improve comprehensibility by using less but more\nmeaningful features while achieving good performance in microblogging contexts\nwhere interpretability is mandatory. Moreover, we present a ranking method to\nevaluate features in terms of statistical relevance and bias. We conducted\nexhaustive tests with five different datasets in order to evaluate\nclassification performance, generalisation capacity and complexity of the\nmodel. Results show that our proposal is better and the most stable one in\nterms of accuracy, generalisation and comprehensibility.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2; I.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.02104v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13908v3",
    "title": "Group-level Fairness Maximization in Online Bipartite Matching",
    "authors": [
      "Will Ma",
      "Pan Xu",
      "Yifan Xu"
    ],
    "author_ids": [],
    "abstract": "We consider the allocation of limited resources to heterogeneous customers\nwho arrive in an online fashion. We would like to allocate the resources\n\"fairly\", so that no group of customers is marginalized in terms of their\noverall service rate. We study whether this is possible to do so in an online\nfashion, and if so, what a good online allocation policy is.\n  We model this problem using online bipartite matching under stationary\narrivals, a fundamental model in the literature typically studied under the\nobjective of maximizing the total number of customers served. We instead study\nthe objective of maximizing the minimum service rate across all groups, and\npropose two notions of fairness: long-run and short-run.\n  For these fairness objectives, we analyze how competitive online algorithms\ncan be, in comparison to offline algorithms which know the sequence of demands\nin advance. For long-run fairness, we propose two online heuristics (Sampling\nand Pooling) which establish asymptotic optimality in different regimes (no\nspecialized supplies, no rare demand types, or imbalanced supply/demand). By\ncontrast, outside all of these regimes, we show that the competitive ratio of\nonline algorithms is between 0.632 and 0.732. For short-run fairness, we show\nfor complete bipartite graphs that the competitive ratio of online algorithms\nis between 0.863 and 0.942; we also derive a probabilistic rejection algorithm\nwhich is asymptotically optimal in the total demand.\n  Depending on the overall scarcity of resources, either our Sampling or\nPooling heuristics could be desirable. The most difficult situation for online\nallocation occurs when the total supply is just enough to serve the total\ndemand.\n  We simulate our algorithms on a public ride-hailing dataset, which both\ndemonstrates the efficacy of our heuristics and validates our managerial\ninsights.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13908v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.14929v1",
    "title": "Windowed Prophet Inequalities",
    "authors": [
      "William Marshall",
      "Nolan Miranda",
      "Albert Zuo"
    ],
    "author_ids": [],
    "abstract": "The prophet inequalities problem has received significant study over the past\ndecades and has several applications such as to online auctions. In this paper,\nwe study two variants of the i.i.d. prophet inequalities problem, namely the\nwindowed prophet inequalities problem and the batched prophet inequalities\nproblem. For the windowed prophet inequalities problem, we show that for window\nsize $o(n)$, the optimal competitive ratio is $\\alpha \\approx 0.745$, the same\nas in the non-windowed case. In the case where the window size is $n/k$ for\nsome constant $k$, we show that $\\alpha_k < WIN_{n/k} \\le \\alpha_k + o_k(1)$\nwhere $WIN_{n/k}$ is the optimal competitive ratio for the window size $n/k$\nprophet inequalities problem and $\\alpha_k$ is the optimal competitive ratio\nfor the $k$ sample i.i.d. prophet inequalities problem. Finally, we prove an\nequivalence between the batched prophet inequalities problem and the i.i.d.\nprophet inequalities problem.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.14929v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.13772v5",
    "title": "Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank",
    "authors": [
      "Hung-Hsu Chou",
      "Carsten Gieshoff",
      "Johannes Maly",
      "Holger Rauhut"
    ],
    "author_ids": [],
    "abstract": "In deep learning, it is common to use more network parameters than training\npoints. In such scenarioof over-parameterization, there are usually multiple\nnetworks that achieve zero training error so that thetraining algorithm induces\nan implicit bias on the computed solution. In practice, (stochastic)\ngradientdescent tends to prefer solutions which generalize well, which provides\na possible explanation of thesuccess of deep learning. In this paper we analyze\nthe dynamics of gradient descent in the simplifiedsetting of linear networks\nand of an estimation problem. Although we are not in an\noverparameterizedscenario, our analysis nevertheless provides insights into the\nphenomenon of implicit bias. In fact, wederive a rigorous analysis of the\ndynamics of vanilla gradient descent, and characterize the dynamicalconvergence\nof the spectrum. We are able to accurately locate time intervals where the\neffective rankof the iterates is close to the effective rank of a low-rank\nprojection of the ground-truth matrix. Inpractice, those intervals can be used\nas criteria for early stopping if a certain regularity is desired. Wealso\nprovide empirical evidence for implicit bias in more general scenarios, such as\nmatrix sensing andrandom initialization. This suggests that deep learning\nprefers trajectories whose complexity (measuredin terms of effective rank) is\nmonotonically increasing, which we believe is a fundamental concept for\nthetheoretical understanding of deep learning.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13772v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13583v1",
    "title": "An Ethical Highlighter for People-Centric Dataset Creation",
    "authors": [
      "Margot Hanley",
      "Apoorv Khandelwal",
      "Hadar Averbuch-Elor",
      "Noah Snavely",
      "Helen Nissenbaum"
    ],
    "author_ids": [],
    "abstract": "Important ethical concerns arising from computer vision datasets of people\nhave been receiving significant attention, and a number of datasets have been\nwithdrawn as a result. To meet the academic need for people-centric datasets,\nwe propose an analytical framework to guide ethical evaluation of existing\ndatasets and to serve future dataset creators in avoiding missteps. Our work is\ninformed by a review and analysis of prior works and highlights where such\nethical challenges arise.",
    "published_date": "2020-11-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13583v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13477v1",
    "title": "Decoding and Diversity in Machine Translation",
    "authors": [
      "Nicholas Roberts",
      "Davis Liang",
      "Graham Neubig",
      "Zachary C. Lipton"
    ],
    "author_ids": [],
    "abstract": "Neural Machine Translation (NMT) systems are typically evaluated using\nautomated metrics that assess the agreement between generated translations and\nground truth candidates. To improve systems with respect to these metrics, NLP\nresearchers employ a variety of heuristic techniques, including searching for\nthe conditional mode (vs. sampling) and incorporating various training\nheuristics (e.g., label smoothing). While search strategies significantly\nimprove BLEU score, they yield deterministic outputs that lack the diversity of\nhuman translations. Moreover, search tends to bias the distribution of\ntranslated gender pronouns. This makes human-level BLEU a misleading benchmark\nin that modern MT systems cannot approach human-level BLEU while simultaneously\nmaintaining human-level translation diversity. In this paper, we characterize\ndistributional differences between generated and real translations, examining\nthe cost in diversity paid for the BLEU scores enjoyed by NMT. Moreover, our\nstudy implicates search as a salient source of known bias when translating\ngender pronouns.",
    "published_date": "2020-11-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13477v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13416v3",
    "title": "Overcoming Failures of Imagination in AI Infused System Development and Deployment",
    "authors": [
      "Margarita Boyarskaya",
      "Alexandra Olteanu",
      "Kate Crawford"
    ],
    "author_ids": [],
    "abstract": "NeurIPS 2020 requested that research paper submissions include impact\nstatements on \"potential nefarious uses and the consequences of failure.\"\nHowever, as researchers, practitioners and system designers, a key challenge to\nanticipating risks is overcoming what Clarke (1962) called 'failures of\nimagination.' The growing research on bias, fairness, and transparency in\ncomputational systems aims to illuminate and mitigate harms, and could thus\nhelp inform reflections on possible negative impacts of particular pieces of\ntechnical work. The prevalent notion of computational harms -- narrowly\nconstrued as either allocational or representational harms -- does not fully\ncapture the open, context dependent, and unobservable nature of harms across\nthe wide range of AI infused systems.The current literature focuses on a small\nrange of examples of harms to motivate algorithmic fixes, overlooking the wider\nscope of probable harms and the way these harms might affect different\nstakeholders. The system affordances may also exacerbate harms in unpredictable\nways, as they determine stakeholders' control(including of non-users) over how\nthey use and interact with a system output. To effectively assist in\nanticipating harmful uses, we argue that frameworks of harms must be\ncontext-aware and consider a wider range of potential stakeholders, system\naffordances, as well as viable proxies for assessing harms in the widest sense.",
    "published_date": "2020-11-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13416v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2012.09115v1",
    "title": "Towards robust and speculation-reduction real estate pricing models based on a data-driven strategy",
    "authors": [
      "Vladimir Vargas-Calderón",
      "Jorge E. Camargo"
    ],
    "author_ids": [],
    "abstract": "In many countries, real estate appraisal is based on conventional methods\nthat rely on appraisers' abilities to collect data, interpret it and model the\nprice of a real estate property. With the increasing use of real estate online\nplatforms and the large amount of information found therein, there exists the\npossibility of overcoming many drawbacks of conventional pricing models such as\nsubjectivity, cost, unfairness, among others. In this paper we propose a\ndata-driven real estate pricing model based on machine learning methods to\nestimate prices reducing human bias. We test the model with 178,865 flats\nlistings from Bogot\\'a, collected from 2016 to 2020. Results show that the\nproposed state-of-the-art model is robust and accurate in estimating real\nestate prices. This case study serves as an incentive for local governments\nfrom developing countries to discuss and build real estate pricing models based\non large data sets that increases fairness for all the real estate market\nstakeholders and reduces price speculation.",
    "published_date": "2020-11-26T00:00:00",
    "year": 2020,
    "categories": [
      "econ.GN",
      "cs.LG",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09115v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13925v1",
    "title": "Investigation on Research Ethics and Building a Benchmark",
    "authors": [
      "Shun Inagaki",
      "Robert Ramirez",
      "Masaki Shimaoka",
      "Kenichi Magata"
    ],
    "author_ids": [],
    "abstract": "When dealing with leading edge cyber security research, especially when\noperating from the perspective of an attacker or a red team, it becomes\nnecessary for one to at times consider how ethics comes into play. There are\ncurrently no cyber security-specific ethics standards, which in particular is\none reason more adversarial cyber security research lags behind in Japan. In\nthis research, using machine learning and manual methods we extracted best\npractices for research ethics from past top conference papers. Using this\nknowledge we constructed an ethics knowledge base for cyber security research.\nSuch a knowledge base can be used to properly distinguish grey-area research so\nthat it is not wrongly forbidden. Using a decision tree-style user interface\nthat we created for our knowledge base, researchers may be able to efficiently\nidentify which aspects of their research require ethical consideration. In this\nwork, as a preliminary step we focused on only a portion of the areas of\nresearch covered by cyber security conferences, but our results are applicable\nto any area of research.",
    "published_date": "2020-11-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "68T50, 68M25",
      "I.7.5; K.4.1; H.3.1; H.3.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13925v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13170v1",
    "title": "Anticipatory Ethics and the Role of Uncertainty",
    "authors": [
      "Priyanka Nanayakkara",
      "Nicholas Diakopoulos",
      "Jessica Hullman"
    ],
    "author_ids": [],
    "abstract": "Making conjectures about future consequences of a technology is an exercise\nin trying to reduce various forms of uncertainty. Both to produce and reason\nabout these conjectures requires understanding their potential limitations. In\nother words, we need systematic ways of considering uncertainty associated with\ngiven conjectures for downstream consequences. In this work, we frame the task\nof considering future consequences as an anticipatory ethics problem, where the\ngoal is to develop scenarios that reflect plausible outcomes and their ethical\nimplications following a technology's introduction into society. In order to\nshed light on how various forms of uncertainty might inform how we reason about\na resulting scenario, we provide a characterization of the types of uncertainty\nthat arise in a potential scenario-building process.",
    "published_date": "2020-11-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13170v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2012.03849v1",
    "title": "Correct block-design experiments mitigate temporal correlation bias in EEG classification",
    "authors": [
      "Simone Palazzo",
      "Concetto Spampinato",
      "Joseph Schmidt",
      "Isaak Kavasidis",
      "Daniela Giordano",
      "Mubarak Shah"
    ],
    "author_ids": [],
    "abstract": "It is argued in [1] that [2] was able to classify EEG responses to visual\nstimuli solely because of the temporal correlation that exists in all EEG data\nand the use of a block design. We here show that the main claim in [1] is\ndrastically overstated and their other analyses are seriously flawed by wrong\nmethodological choices. To validate our counter-claims, we evaluate the\nperformance of state-of-the-art methods on the dataset in [2] reaching about\n50% classification accuracy over 40 classes, lower than in [2], but still\nsignificant. We then investigate the influence of EEG temporal correlation on\nclassification accuracy by testing the same models in two additional\nexperimental settings: one that replicates [1]'s rapid-design experiment, and\nanother one that examines the data between blocks while subjects are shown a\nblank screen. In both cases, classification accuracy is at or near chance, in\ncontrast to what [1] reports, indicating a negligible contribution of temporal\ncorrelation to classification accuracy. We, instead, are able to replicate the\nresults in [1] only when intentionally contaminating our data by inducing a\ntemporal correlation. This suggests that what Li et al. [1] demonstrate is that\ntheir data are strongly contaminated by temporal correlation and low\nsignal-to-noise ratio. We argue that the reason why Li et al. [1] observe such\nhigh correlation in EEG data is their unconventional experimental design and\nsettings that violate the basic cognitive neuroscience design recommendations,\nfirst and foremost the one of limiting the experiments' duration, as instead\ndone in [2]. Our analyses in this paper refute the claims of the \"perils and\npitfalls of block-design\" in [1]. Finally, we conclude the paper by examining a\nnumber of other oversimplistic statements, inconsistencies, misinterpretation\nof machine learning concepts, speculations and misleading claims in [1].",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.03849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.13032v1",
    "title": "Like a Researcher Stating Broader Impact For the Very First Time",
    "authors": [
      "Grace Abuhamad",
      "Claudel Rheault"
    ],
    "author_ids": [],
    "abstract": "In requiring that a statement of broader impact accompany all submissions for\nthis year's conference, the NeurIPS program chairs made ethics part of the\nstake in groundbreaking AI research. While there is precedent from other fields\nand increasing awareness within the NeurIPS community, this paper seeks to\nanswer the question of how individual researchers reacted to the new\nrequirement, including not just their views, but also their experience in\ndrafting and their reflections after paper acceptances. We present survey\nresults and considerations to inform the next iteration of the broader impact\nrequirement should it remain a requirement for future NeurIPS conferences.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.13032v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12983v1",
    "title": "Best response dynamics on random graphs",
    "authors": [
      "Jordan Chellig",
      "Calina Durbac",
      "Nikolaos Fountoulakis"
    ],
    "author_ids": [],
    "abstract": "We consider evolutionary games on a population whose underlying topology of\ninteractions is determined by a binomial random graph $G(n,p)$. Our focus is on\n2-player symmetric games with 2 strategies played between the incident members\nof such a population. Players update their strategies synchronously. At each\nround, each player selects the strategy that is the best response to the\ncurrent set of strategies its neighbours play. We show that such a system\nreduces to generalised majority and minority dynamics. We show rapid\nconvergence to unanimity for $p$ in a range that depends on a certain\ncharacteristic of the payoff matrix. In the presence of a bias among the pure\nNash equilibria of the game, we determine a sharp threshold on $p$ above which\nthe largest connected component reaches unanimity with high probability. For\n$p$ below this critical value, where this does not happen, we identify those\nsubstructures inside the largest component that remain discordant throughout\nthe evolution of the system.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "math.CO",
      "cs.GT",
      "math.PR",
      "05C80, 91A22, 91A05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12983v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.12960v1",
    "title": "Deep Convolutional Neural Networks: A survey of the foundations, selected improvements, and some current applications",
    "authors": [
      "Lars Lien Ankile",
      "Morgan Feet Heggland",
      "Kjartan Krange"
    ],
    "author_ids": [],
    "abstract": "Within the world of machine learning there exists a wide range of different\nmethods with respective advantages and applications. This paper seeks to\npresent and discuss one such method, namely Convolutional Neural Networks\n(CNNs). CNNs are deep neural networks that use a special linear operation\ncalled convolution. This operation represents a key and distinctive element of\nCNNs, and will therefore be the focus of this method paper. The discussion\nstarts with the theoretical foundations that underlie convolutions and CNNs.\nThen, the discussion proceeds to discuss some improvements and augmentations\nthat can be made to adapt the method to estimate a wider set of function\nclasses. The paper mainly investigates two ways of improving the method: by\nusing locally connected layers, which can make the network less invariant to\ntranslation, and tiled convolution, which allows for the learning of more\ncomplex invariances than standard convolution. Furthermore, the use of the Fast\nFourier Transform can improve the computational efficiency of convolution.\nSubsequently, this paper discusses two applications of convolution that have\nproven to be very effective in practice. First, the YOLO architecture is a\nstate of the art neural network for image object classification, which\naccurately predicts bounding boxes around objects in images. Second, tumor\ndetection in mammography may be performed using CNNs, accomplishing 7.2% higher\nspecificity than actual doctors with only .3% less sensitivity. Finally, the\ninvention of technology that outperforms humans in different fields also raises\ncertain ethical and regulatory questions that are briefly discussed.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12960v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12750v2",
    "title": "AI virtues -- The missing link in putting AI ethics into practice",
    "authors": [
      "Thilo Hagendorff"
    ],
    "author_ids": [],
    "abstract": "Several seminal ethics initiatives have stipulated sets of principles and\nstandards for good technology development in the AI sector. However, widespread\ncriticism has pointed out a lack of practical realization of these principles.\nFollowing that, AI ethics underwent a practical turn, but without deviating\nfrom the principled approach and the many shortcomings associated with it. This\npaper proposes a different approach. It defines four basic AI virtues, namely\njustice, honesty, responsibility and care, all of which represent specific\nmotivational settings that constitute the very precondition for ethical\ndecision making in the AI field. Moreover, it defines two second-order AI\nvirtues, prudence and fortitude, that bolster achieving the basic virtues by\nhelping with overcoming bounded ethicality or the many hidden psychological\nforces that impair ethical decision making and that are hitherto disregarded in\nAI ethics. Lastly, the paper describes measures for successfully cultivating\nthe mentioned virtues in organizations dealing with AI research and\ndevelopment.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12750v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12616v1",
    "title": "Unsupervised Domain Adaptation in Semantic Segmentation via Orthogonal and Clustered Embeddings",
    "authors": [
      "Marco Toldo",
      "Umberto Michieli",
      "Pietro Zanuttigh"
    ],
    "author_ids": [],
    "abstract": "Deep learning frameworks allowed for a remarkable advancement in semantic\nsegmentation, but the data hungry nature of convolutional networks has rapidly\nraised the demand for adaptation techniques able to transfer learned knowledge\nfrom label-abundant domains to unlabeled ones. In this paper we propose an\neffective Unsupervised Domain Adaptation (UDA) strategy, based on a feature\nclustering method that captures the different semantic modes of the feature\ndistribution and groups features of the same class into tight and\nwell-separated clusters. Furthermore, we introduce two novel learning\nobjectives to enhance the discriminative clustering performance: an\northogonality loss forces spaced out individual representations to be\northogonal, while a sparsity loss reduces class-wise the number of active\nfeature channels. The joint effect of these modules is to regularize the\nstructure of the feature space. Extensive evaluations in the synthetic-to-real\nscenario show that we achieve state-of-the-art performance.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12616v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12547v2",
    "title": "Implicit bias of deep linear networks in the large learning rate phase",
    "authors": [
      "Wei Huang",
      "Weitao Du",
      "Richard Yi Da Xu",
      "Chunrui Liu"
    ],
    "author_ids": [],
    "abstract": "Most theoretical studies explaining the regularization effect in deep\nlearning have only focused on gradient descent with a sufficient small learning\nrate or even gradient flow (infinitesimal learning rate). Such researches,\nhowever, have neglected a reasonably large learning rate applied in most\npractical applications. In this work, we characterize the implicit bias effect\nof deep linear networks for binary classification using the logistic loss in\nthe large learning rate regime, inspired by the seminal work by Lewkowycz et\nal. [26] in a regression setting with squared loss. They found a learning rate\nregime with a large stepsize named the catapult phase, where the loss grows at\nthe early stage of training and eventually converges to a minimum that is\nflatter than those found in the small learning rate regime. We claim that\ndepending on the separation conditions of data, the gradient descent iterates\nwill converge to a flatter minimum in the catapult phase. We rigorously prove\nthis claim under the assumption of degenerate data by overcoming the difficulty\nof the non-constant Hessian of logistic loss and further characterize the\nbehavior of loss and Hessian for non-separable data. Finally, we demonstrate\nthat flatter minima in the space spanned by non-separable data along with the\nlearning rate in the catapult phase can lead to better generalization\nempirically.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12547v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12486v2",
    "title": "CircleGAN: Generative Adversarial Learning across Spherical Circles",
    "authors": [
      "Woohyeon Shim",
      "Minsu Cho"
    ],
    "author_ids": [],
    "abstract": "We present a novel discriminator for GANs that improves realness and\ndiversity of generated samples by learning a structured hypersphere embedding\nspace using spherical circles. The proposed discriminator learns to populate\nrealistic samples around the longest spherical circle, i.e., a great circle,\nwhile pushing unrealistic samples toward the poles perpendicular to the great\ncircle. Since longer circles occupy larger area on the hypersphere, they\nencourage more diversity in representation learning, and vice versa.\nDiscriminating samples based on their corresponding spherical circles can thus\nnaturally induce diversity to generated samples. We also extend the proposed\nmethod for conditional settings with class labels by creating a hypersphere for\neach category and performing class-wise discrimination and update. In\nexperiments, we validate the effectiveness for both unconditional and\nconditional generation on standard benchmarks, achieving the state of the art.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12486v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12465v1",
    "title": "The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability",
    "authors": [
      "Sunipa Dev"
    ],
    "author_ids": [],
    "abstract": "High-dimensional representations for words, text, images, knowledge graphs\nand other structured data are commonly used in different paradigms of machine\nlearning and data mining. These representations have different degrees of\ninterpretability, with efficient distributed representations coming at the cost\nof the loss of feature to dimension mapping. This implies that there is\nobfuscation in the way concepts are captured in these embedding spaces. Its\neffects are seen in many representations and tasks, one particularly\nproblematic one being in language representations where the societal biases,\nlearned from underlying data, are captured and occluded in unknown dimensions\nand subspaces. As a result, invalid associations (such as different races and\ntheir association with a polar notion of good versus bad) are made and\npropagated by the representations, leading to unfair outcomes in different\ntasks where they are used. This work addresses some of these problems\npertaining to the transparency and interpretability of such representations. A\nprimary focus is the detection, quantification, and mitigation of socially\nbiased associations in language representation.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12465v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12461v4",
    "title": "Deep Discriminative Feature Learning for Accent Recognition",
    "authors": [
      "Wei Wang",
      "Chao Zhang",
      "Xiaopei Wu"
    ],
    "author_ids": [],
    "abstract": "Accent recognition with deep learning framework is a similar work to deep\nspeaker identification, they're both expected to give the input speech an\nidentifiable representation.\n  Compared with the individual-level features learned by speaker identification\nnetwork, the deep accent recognition work throws a more challenging point that\nforging group-level accent features for speakers.\n  In this paper, we borrow and improve the deep speaker identification\nframework to recognize accents, in detail, we adopt Convolutional Recurrent\nNeural Network as front-end encoder and integrate local features using\nRecurrent Neural Network to make an utterance-level accent representation.\n  Novelly, to address overfitting, we simply add Connectionist Temporal\nClassification based speech recognition auxiliary task during training, and for\nambiguous accent discrimination, we introduce some powerful discriminative loss\nfunctions in face recognition works to enhance the discriminative power of\naccent features.\n  We show that our proposed network with discriminative training method\n(without data-augment) is significantly ahead of the baseline system on the\naccent classification track in the Accented English Speech Recognition\nChallenge 2020, where the loss function Circle-Loss has achieved the best\ndiscriminative optimization for accent representation.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.AI",
      "62-08",
      "I.2.10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12461v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12454v4",
    "title": "Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer",
    "authors": [
      "Zidi Xiu",
      "Junya Chen",
      "Ricardo Henao",
      "Benjamin Goldstein",
      "Lawrence Carin",
      "Chenyang Tao"
    ],
    "author_ids": [],
    "abstract": "Dealing with severe class imbalance poses a major challenge for real-world\napplications, especially when the accurate classification and generalization of\nminority classes is of primary interest. In computer vision, learning from long\ntailed datasets is a recurring theme, especially for natural image datasets.\nWhile existing solutions mostly appeal to sampling or weighting adjustments to\nalleviate the pathological imbalance, or imposing inductive bias to prioritize\nnon-spurious associations, we take novel perspectives to promote sample\nefficiency and model generalization based on the invariance principles of\ncausality. Our proposal posits a meta-distributional scenario, where the data\ngenerating mechanism is invariant across the label-conditional feature\ndistributions. Such causal assumption enables efficient knowledge transfer from\nthe dominant classes to their under-represented counterparts, even if the\nrespective feature distributions show apparent disparities. This allows us to\nleverage a causal data inflation procedure to enlarge the representation of\nminority classes. Our development is orthogonal to the existing extreme\nclassification techniques thus can be seamlessly integrated. The utility of our\nproposal is validated with an extensive set of synthetic and real-world\ncomputer vision tasks against SOTA solutions.",
    "published_date": "2020-11-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12454v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12199v1",
    "title": "A Priori Error Analysis for an Optimal Control Problem Governed by a Variational Inequality of the Second Kind",
    "authors": [
      "Christian Meyer",
      "Monika Weymuth"
    ],
    "author_ids": [],
    "abstract": "We consider an optimal control problem governed by an elliptic variational\ninequality of the second kind. The problem is discretized by linear finite\nelements for the state and a variational discrete approach for the control.\nBased on a quadratic growth condition we derive nearly optimal a priori error\nestimates. Moreover, we establish second order sufficient optimality conditions\nthat ensure a quadratic growth condition. These conditions are rather\nrestrictive, but allow us to construct a one-dimensional locally optimal\nsolution with reduced regularity, which serves as an exact solution for\nnumerical experiments.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC",
      "49M25, 65G99, 65K15, 65N30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12199v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.12183v1",
    "title": "Generating Intelligible Plumitifs Descriptions: Use Case Application with Ethical Considerations",
    "authors": [
      "David Beauchemin",
      "Nicolas Garneau",
      "Eve Gaumond",
      "Pierre-Luc Déziel",
      "Richard Khoury",
      "Luc Lamontagne"
    ],
    "author_ids": [],
    "abstract": "Plumitifs (dockets) were initially a tool for law clerks. Nowadays, they are\nused as summaries presenting all the steps of a judicial case. Information\nconcerning parties' identity, jurisdiction in charge of administering the case,\nand some information relating to the nature and the course of the preceding are\navailable through plumitifs. They are publicly accessible but barely\nunderstandable; they are written using abbreviations and referring to\nprovisions from the Criminal Code of Canada, which makes them hard to reason\nabout. In this paper, we propose a simple yet efficient multi-source language\ngeneration architecture that leverages both the plumitif and the Criminal\nCode's content to generate intelligible plumitifs descriptions. It goes without\nsaying that ethical considerations rise with these sensitive documents made\nreadable and available at scale, legitimate concerns that we address in this\npaper.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12183v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12919v2",
    "title": "Analyzing the Machine Learning Conference Review Process",
    "authors": [
      "David Tran",
      "Alex Valtchanov",
      "Keshav Ganapathy",
      "Raymond Feng",
      "Eric Slud",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "author_ids": [],
    "abstract": "Mainstream machine learning conferences have seen a dramatic increase in the\nnumber of participants, along with a growing range of perspectives, in recent\nyears. Members of the machine learning community are likely to overhear\nallegations ranging from randomness of acceptance decisions to institutional\nbias. In this work, we critically analyze the review process through a\ncomprehensive study of papers submitted to ICLR between 2017 and 2020. We\nquantify reproducibility/randomness in review scores and acceptance decisions,\nand examine whether scores correlate with paper impact. Our findings suggest\nstrong institutional bias in accept/reject decisions, even after controlling\nfor paper quality. Furthermore, we find evidence for a gender gap, with female\nauthors receiving lower scores, lower acceptance rates, and fewer citations per\npaper than their male counterparts. We conclude our work with recommendations\nfor future conference organizers.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12919v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12096v1",
    "title": "Gender bias in magazines oriented to men and women: a computational approach",
    "authors": [
      "Diego Kozlowski",
      "Gabriela Lozano",
      "Carla M. Felcher",
      "Fernando Gonzalez",
      "Edgar Altszyler"
    ],
    "author_ids": [],
    "abstract": "Cultural products are a source to acquire individual values and behaviours.\nTherefore, the differences in the content of the magazines aimed specifically\nat women or men are a means to create and reproduce gender stereotypes. In this\nstudy, we compare the content of a women-oriented magazine with that of a\nmen-oriented one, both produced by the same editorial group, over a decade\n(2008-2018). With Topic Modelling techniques we identify the main themes\ndiscussed in the magazines and quantify how much the presence of these topics\ndiffers between magazines over time. Then, we performed a word-frequency\nanalysis to validate this methodology and extend the analysis to other subjects\nthat did not emerge automatically. Our results show that the frequency of\nappearance of the topics Family, Business and Women as sex objects, present an\ninitial bias that tends to disappear over time. Conversely, in Fashion and\nScience topics, the initial differences between both magazines are maintained.\nBesides, we show that in 2012, the content associated with horoscope increased\nin the women-oriented magazine, generating a new gap that remained open over\ntime. Also, we show a strong increase in the use of words associated with\nfeminism since 2015 and specifically the word abortion in 2018. Overall, these\ncomputational tools allowed us to analyse more than 24,000 articles. Up to our\nknowledge, this is the first study to compare magazines in such a large\ndataset, a task that would have been prohibitive using manual content analysis\nmethodologies.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12096v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12086v1",
    "title": "Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis",
    "authors": [
      "Michael A. Lepori"
    ],
    "author_ids": [],
    "abstract": "We present a new approach for detecting human-like social biases in word\nembeddings using representational similarity analysis. Specifically, we probe\ncontextualized and non-contextualized embeddings for evidence of intersectional\nbiases against Black women. We show that these embeddings represent Black women\nas simultaneously less feminine than White women, and less Black than Black\nmen. This finding aligns with intersectionality theory, which argues that\nmultiple identity categories (such as race or sex) layer on top of each other\nin order to create unique modes of discrimination that are not shared by any\nindividual category.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12086v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.12014v1",
    "title": "Argument from Old Man's View: Assessing Social Bias in Argumentation",
    "authors": [
      "Maximilian Spliethöver",
      "Henning Wachsmuth"
    ],
    "author_ids": [],
    "abstract": "Social bias in language - towards genders, ethnicities, ages, and other\nsocial groups - poses a problem with ethical impact for many NLP applications.\nRecent research has shown that machine learning models trained on respective\ndata may not only adopt, but even amplify the bias. So far, however, little\nattention has been paid to bias in computational argumentation. In this paper,\nwe study the existence of social biases in large English debate portals. In\nparticular, we train word embedding models on portal-specific corpora and\nsystematically evaluate their bias using WEAT, an existing metric to measure\nbias in word embeddings. In a word co-occurrence analysis, we then investigate\ncauses of bias. The results suggest that all tested debate corpora contain\nunbalanced and biased data, mostly in favor of male people with\nEuropean-American names. Our empirical insights contribute towards an\nunderstanding of bias in argumentative data sources.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.12014v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11953v3",
    "title": "DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations",
    "authors": [
      "Wenhao Wang",
      "Shengcai Liao",
      "Fang Zhao",
      "Cuicui Kang",
      "Ling Shao"
    ],
    "author_ids": [],
    "abstract": "Existing person re-identification models often have low generalizability,\nwhich is mostly due to limited availability of large-scale labeled data in\ntraining. However, labeling large-scale training data is very expensive and\ntime-consuming, while large-scale synthetic dataset shows promising value in\nlearning generalizable person re-identification models. Therefore, in this\npaper a novel and practical person re-identification task is proposed,i.e. how\nto use labeled synthetic dataset and unlabeled real-world dataset to train a\nuniversal model. In this way, human annotations are no longer required, and it\nis scalable to large and diverse real-world datasets. To address the task, we\nintroduce a framework with high generalizability, namely DomainMix.\nSpecifically, the proposed method firstly clusters the unlabeled real-world\nimages and selects the reliable clusters. During training, to address the large\ndomain gap between two domains, a domain-invariant feature learning method is\nproposed, which introduces a new loss,i.e. domain balance loss, to conduct an\nadversarial learning between domain-invariant feature learning and domain\ndiscrimination, and meanwhile learns a discriminative feature for person\nre-identification. This way, the domain gap between synthetic and real-world\ndata is much reduced, and the learned feature is generalizable thanks to the\nlarge-scale and diverse training data. Experimental results show that the\nproposed annotation-free method is more or less comparable to the counterpart\ntrained with full human annotations, which is quite promising. In addition, it\nachieves the current state of the art on several person re-identification\ndatasets under direct cross-dataset evaluation.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11953v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11878v2",
    "title": "Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder",
    "authors": [
      "Hyemi Kim",
      "Seungjae Shin",
      "JoonHo Jang",
      "Kyungwoo Song",
      "Weonyoung Joo",
      "Wanmo Kang",
      "Il-Chul Moon"
    ],
    "author_ids": [],
    "abstract": "The problem of fair classification can be mollified if we develop a method to\nremove the embedded sensitive information from the classification features.\nThis line of separating the sensitive information is developed through the\ncausal inference, and the causal inference enables the counterfactual\ngenerations to contrast the what-if case of the opposite sensitive attribute.\nAlong with this separation with the causality, a frequent assumption in the\ndeep latent causal model defines a single latent variable to absorb the entire\nexogenous uncertainty of the causal graph. However, we claim that such\nstructure cannot distinguish the 1) information caused by the intervention\n(i.e., sensitive variable) and 2) information correlated with the intervention\nfrom the data. Therefore, this paper proposes Disentangled Causal Effect\nVariational Autoencoder (DCEVAE) to resolve this limitation by disentangling\nthe exogenous uncertainty into two latent variables: either 1) independent to\ninterventions or 2) correlated to interventions without causality.\nParticularly, our disentangling approach preserves the latent variable\ncorrelated to interventions in generating counterfactual examples. We show that\nour method estimates the total effect and the counterfactual effect without a\ncomplete causal graph. By adding a fairness regularization, DCEVAE generates a\ncounterfactual fair dataset while losing less original information. Also,\nDCEVAE generates natural counterfactual images by only flipping sensitive\ninformation. Additionally, we theoretically show the differences in the\ncovariance structures of DCEVAE and prior works from the perspective of the\nlatent disentanglement.",
    "published_date": "2020-11-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11878v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11611v1",
    "title": "FERN: Fair Team Formation for Mutually Beneficial Collaborative Learning",
    "authors": [
      "Maria Kalantzi",
      "Agoritsa Polyzou",
      "George Karypis"
    ],
    "author_ids": [],
    "abstract": "Automated Team Formation is becoming increasingly important for a plethora of\napplications in open source community projects, remote working platforms, as\nwell as online educational systems. The latter case, in particular, poses\nsignificant challenges that are specific to the educational domain. Indeed,\nteaming students aims to accomplish far more than the successful completion of\na specific task. It needs to ensure that all members in the team benefit from\nthe collaborative work, while also ensuring that the participants are not\ndiscriminated with respect to their protected attributes, such as race and\ngender. Towards achieving these goals, this work introduces FERN, a fair team\nformation approach that promotes mutually beneficial peer learning, dictated by\nprotected group fairness as equality of opportunity in collaborative learning.\nWe formulate the problem as a multi-objective discrete optimization problem. We\nshow this problem to be NP-hard and propose a heuristic hill-climbing\nalgorithm. Extensive experiments on both synthetic and real-world datasets\nagainst well-known team formation techniques show the effectiveness of the\nproposed method.",
    "published_date": "2020-11-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11611v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.11311v2",
    "title": "Uncovering the Bias in Facial Expressions",
    "authors": [
      "Jessica Deuschel",
      "Bettina Finzel",
      "Ines Rieger"
    ],
    "author_ids": [],
    "abstract": "Over the past decades the machine and deep learning community has celebrated\ngreat achievements in challenging tasks such as image classification. The deep\narchitecture of artificial neural networks together with the plenitude of\navailable data makes it possible to describe highly complex relations. Yet, it\nis still impossible to fully capture what the deep learning model has learned\nand to verify that it operates fairly and without creating bias, especially in\ncritical tasks, for instance those arising in the medical field. One example\nfor such a task is the detection of distinct facial expressions, called Action\nUnits, in facial images. Considering this specific task, our research aims to\nprovide transparency regarding bias, specifically in relation to gender and\nskin color. We train a neural network for Action Unit classification and\nanalyze its performance quantitatively based on its accuracy and qualitatively\nbased on heatmaps. A structured review of our results indicates that we are\nable to detect bias. Even though we cannot conclude from our results that lower\nclassification performance emerged solely from gender and skin color bias,\nthese biases must be addressed, which is why we end by giving suggestions on\nhow the detected bias can be avoided.",
    "published_date": "2020-11-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11311v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11203v1",
    "title": "Geometry-Aware Universal Mirror-Prox",
    "authors": [
      "Reza Babanezhad",
      "Simon Lacoste-Julien"
    ],
    "author_ids": [],
    "abstract": "Mirror-prox (MP) is a well-known algorithm to solve variational inequality\n(VI) problems. VI with a monotone operator covers a large group of settings\nsuch as convex minimization, min-max or saddle point problems. To get a\nconvergent algorithm, the step-size of the classic MP algorithm relies heavily\non the problem dependent knowledge of the operator such as its smoothness\nparameter which is hard to estimate. Recently, a universal variant of MP for\nsmooth/bounded operators has been introduced that depends only on the norm of\nupdates in MP. In this work, we relax the dependence to evaluating the norm of\nupdates to Bregman divergence between updates. This relaxation allows us to\nextends the analysis of universal MP to the settings where the operator is not\nsmooth or bounded. Furthermore, we analyse the VI problem with a stochastic\nmonotone operator in different settings and obtain an optimal rate up to a\nlogarithmic factor.",
    "published_date": "2020-11-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11203v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11199v1",
    "title": "Balance Regularized Neural Network Models for Causal Effect Estimation",
    "authors": [
      "Mehrdad Farajtabar",
      "Andrew Lee",
      "Yuanjian Feng",
      "Vishal Gupta",
      "Peter Dolan",
      "Harish Chandran",
      "Martin Szummer"
    ],
    "author_ids": [],
    "abstract": "Estimating individual and average treatment effects from observational data\nis an important problem in many domains such as healthcare and e-commerce. In\nthis paper, we advocate balance regularization of multi-head neural network\narchitectures. Our work is motivated by representation learning techniques to\nreduce differences between treated and untreated distributions that potentially\narise due to confounding factors. We further regularize the model by\nencouraging it to predict control outcomes for individuals in the treatment\ngroup that are similar to control outcomes in the control group. We empirically\nstudy the bias-variance trade-off between different weightings of the\nregularizers, as well as between inductive and transductive inference.",
    "published_date": "2020-11-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11199v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11001v1",
    "title": "Fairness-guided SMT-based Rectification of Decision Trees and Random Forests",
    "authors": [
      "Jiang Zhang",
      "Ivan Beschastnikh",
      "Sergey Mechtaev",
      "Abhik Roychoudhury"
    ],
    "author_ids": [],
    "abstract": "Data-driven decision making is gaining prominence with the popularity of\nvarious machine learning models. Unfortunately, real-life data used in machine\nlearning training may capture human biases, and as a result the learned models\nmay lead to unfair decision making. In this paper, we provide a solution to\nthis problem for decision trees and random forests. Our approach converts any\ndecision tree or random forest into a fair one with respect to a specific data\nset, fairness criteria, and sensitive attributes. The \\emph{FairRepair} tool,\nbuilt based on our approach, is inspired by automated program repair techniques\nfor traditional programs. It uses an SMT solver to decide which paths in the\ndecision tree could have their outcomes flipped to improve the fairness of the\nmodel. Our experiments on the well-known adult dataset from UC Irvine\ndemonstrate that FairRepair scales to realistic decision trees and random\nforests. Furthermore, FairRepair provides formal guarantees about soundness and\ncompleteness of finding a repair. Since our fairness-guided repair technique\nrepairs decision trees and random forests obtained from a given (unfair)\ndata-set, it can help to identify and rectify biases in decision-making in an\norganisation.",
    "published_date": "2020-11-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11001v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10998v1",
    "title": "Using ontology embeddings for structural inductive bias in gene expression data analysis",
    "authors": [
      "Maja Trębacz",
      "Zohreh Shams",
      "Mateja Jamnik",
      "Paul Scherer",
      "Nikola Simidjievski",
      "Helena Andres Terre",
      "Pietro Liò"
    ],
    "author_ids": [],
    "abstract": "Stratifying cancer patients based on their gene expression levels allows\nimproving diagnosis, survival analysis and treatment planning. However, such\ndata is extremely highly dimensional as it contains expression values for over\n20000 genes per patient, and the number of samples in the datasets is low. To\ndeal with such settings, we propose to incorporate prior biological knowledge\nabout genes from ontologies into the machine learning system for the task of\npatient classification given their gene expression data. We use ontology\nembeddings that capture the semantic similarities between the genes to direct a\nGraph Convolutional Network, and therefore sparsify the network connections. We\nshow this approach provides an advantage for predicting clinical targets from\nhigh-dimensional low-sample data.",
    "published_date": "2020-11-22T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.GN",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10998v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10754v1",
    "title": "Human computation requires and enables a new approach to ethical review",
    "authors": [
      "Libuše Hannah Vepřek",
      "Patricia Seymour",
      "Pietro Michelucci"
    ],
    "author_ids": [],
    "abstract": "With humans increasingly serving as computational elements in distributed\ninformation processing systems and in consideration of the profit-driven\nmotives and potential inequities that might accompany the emerging thinking\neconomy[1], we recognize the need for establishing a set of related ethics to\nensure the fair treatment and wellbeing of online cognitive laborers and the\nconscientious use of the capabilities to which they contribute. Toward this\nend, we first describe human-in-the-loop computing in context of the new\nconcerns it raises that are not addressed by traditional ethical research\nstandards. We then describe shortcomings in the traditional approach to ethical\nreview and introduce a dynamic approach for sustaining an ethical framework\nthat can continue to evolve within the rapidly shifting context of disruptive\nnew technologies.",
    "published_date": "2020-11-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "68T01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10695v2",
    "title": "Sparse sketches with small inversion bias",
    "authors": [
      "Michał Dereziński",
      "Zhenyu Liao",
      "Edgar Dobriban",
      "Michael W. Mahoney"
    ],
    "author_ids": [],
    "abstract": "For a tall $n\\times d$ matrix $A$ and a random $m\\times n$ sketching matrix\n$S$, the sketched estimate of the inverse covariance matrix $(A^\\top A)^{-1}$\nis typically biased: $E[(\\tilde A^\\top\\tilde A)^{-1}]\\ne(A^\\top A)^{-1}$, where\n$\\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in\nstatistics and distributed optimization, when averaging multiple independently\nconstructed estimates of quantities that depend on the inverse covariance. We\ndevelop a framework for analyzing inversion bias, based on our proposed concept\nof an $(\\epsilon,\\delta)$-unbiased estimator for random matrices. We show that\nwhen the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,\nthen after simple rescaling, the estimator $(\\frac m{m-d}\\tilde A^\\top\\tilde\nA)^{-1}$ is $(\\epsilon,\\delta)$-unbiased for $(A^\\top A)^{-1}$ with a sketch of\nsize $m=O(d+\\sqrt d/\\epsilon)$. This implies that for $m=O(d)$, the inversion\nbias of this estimator is $O(1/\\sqrt d)$, which is much smaller than the\n$\\Theta(1)$ approximation error obtained as a consequence of the subspace\nembedding guarantee for sub-gaussian sketches. We then propose a new sketching\ntechnique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas\nfrom both data-oblivious sparse embeddings as well as data-aware leverage-based\nrow sampling methods, to get $\\epsilon$ inversion bias for sketch size\n$m=O(d\\log d+\\sqrt d/\\epsilon)$ in time $O(\\text{nnz}(A)\\log n+md^2)$, where\nnnz is the number of non-zeros. The key techniques enabling our analysis\ninclude an extension of a classical inequality of Bai and Silverstein for\nrandom quadratic forms, which we call the Restricted Bai-Silverstein\ninequality; and anti-concentration of the Binomial distribution via the\nPaley-Zygmund inequality, which we use to prove a lower bound showing that\nleverage score sampling sketches generally do not achieve small inversion bias.",
    "published_date": "2020-11-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10695v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10672v2",
    "title": "AI Governance for Businesses",
    "authors": [
      "Johannes Schneider",
      "Rene Abraham",
      "Christian Meske",
      "Jan vom Brocke"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) governance regulates the exercise of authority\nand control over the management of AI. It aims at leveraging AI through\neffective use of data and minimization of AI-related cost and risk. While\ntopics such as AI governance and AI ethics are thoroughly discussed on a\ntheoretical, philosophical, societal and regulatory level, there is limited\nwork on AI governance targeted to companies and corporations. This work views\nAI products as systems, where key functionality is delivered by machine\nlearning (ML) models leveraging (training) data. We derive a conceptual\nframework by synthesizing literature on AI and related fields such as ML. Our\nframework decomposes AI governance into governance of data, (ML) models and\n(AI) systems along four dimensions. It relates to existing IT and data\ngovernance frameworks and practices. It can be adopted by practitioners and\nacademics alike. For practitioners the synthesis of mainly research papers, but\nalso practitioner publications and publications of regulatory bodies provides a\nvaluable starting point to implement AI governance, while for academics the\npaper highlights a number of areas of AI governance that deserve more\nattention.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10672v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10635v3",
    "title": "Landmark and IMU Data Fusion: Systematic Convergence Geometric Nonlinear Observer for SLAM and Velocity Bias",
    "authors": [
      "Hashim A. Hashim",
      "Abdelrahman E. E. Eltoukhy"
    ],
    "author_ids": [],
    "abstract": "Navigation solutions suitable for cases when both autonomous robot's pose\n(\\textit{i.e}., attitude and position) and its environment are unknown are in\ngreat demand. Simultaneous Localization and Mapping (SLAM) fulfills this need\nby concurrently mapping the environment and observing robot's pose with respect\nto the map. This work proposes a nonlinear observer for SLAM posed on the\nmanifold of the Lie group of $\\mathbb{SLAM}_{n}\\left(3\\right)$, characterized\nby systematic convergence, and designed to mimic the nonlinear motion dynamics\nof the true SLAM problem. The system error is constrained to start within a\nknown large set and decay systematically to settle within a known small set.\nThe proposed estimator is guaranteed to achieve predefined transient and\nsteady-state performance and eliminate the unknown bias inevitably present in\nvelocity measurements by directly using measurements of angular and\ntranslational velocity, landmarks, and information collected by an inertial\nmeasurement unit (IMU). Experimental results obtained by testing the proposed\nsolution on a real-world dataset collected by a quadrotor demonstrate the\nobserver's ability to estimate the six-degrees-of-freedom (6 DoF) robot pose\nand to position unknown landmarks in three-dimensional (3D) space. Keywords:\nSimultaneous Localization and Mapping, Nonlinear filter for SLAM, Nonlinear\nfilter for SLAM on Matrix Lie group, pose, asymptotic stability, prescribed\nperformance, adaptive estimate, feature, inertial measurement unit, inertial\nvision unit, IMU, SE(3), SO(3), noise.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10635v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.10487v3",
    "title": "Normalization effects on shallow neural networks and related asymptotic expansions",
    "authors": [
      "Jiahui Yu",
      "Konstantinos Spiliopoulos"
    ],
    "author_ids": [],
    "abstract": "We consider shallow (single hidden layer) neural networks and characterize\ntheir performance when trained with stochastic gradient descent as the number\nof hidden units $N$ and gradient descent steps grow to infinity. In particular,\nwe investigate the effect of different scaling schemes, which lead to different\nnormalizations of the neural network, on the network's statistical output,\nclosing the gap between the $1/\\sqrt{N}$ and the mean-field $1/N$\nnormalization. We develop an asymptotic expansion for the neural network's\nstatistical output pointwise with respect to the scaling parameter as the\nnumber of hidden units grows to infinity. Based on this expansion, we\ndemonstrate mathematically that to leading order in $N$, there is no\nbias-variance trade off, in that both bias and variance (both explicitly\ncharacterized) decrease as the number of hidden units increases and time grows.\nIn addition, we show that to leading order in $N$, the variance of the neural\nnetwork's statistical output decays as the implied normalization by the scaling\nparameter approaches the mean field normalization. Numerical studies on the\nMNIST and CIFAR10 datasets show that test and train accuracy monotonically\nimprove as the neural network's normalization gets closer to the mean field\nnormalization.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "60F05, 68T01, 60G99"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10487v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10464v2",
    "title": "A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning",
    "authors": [
      "Xinyi Xu",
      "Lingjuan Lyu"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) is an emerging practical framework for effective and\nscalable machine learning among multiple participants, such as end users,\norganizations and companies. However, most existing FL or distributed learning\nframeworks have not well addressed two important issues together: collaborative\nfairness and adversarial robustness (e.g. free-riders and malicious\nparticipants). In conventional FL, all participants receive the global model\n(equal rewards), which might be unfair to the high-contributing participants.\nFurthermore, due to the lack of a safeguard mechanism, free-riders or malicious\nadversaries could game the system to access the global model for free or to\nsabotage it. In this paper, we propose a novel Robust and Fair Federated\nLearning (RFFL) framework to achieve collaborative fairness and adversarial\nrobustness simultaneously via a reputation mechanism. RFFL maintains a\nreputation for each participant by examining their contributions via their\nuploaded gradients (using vector similarity) and thus identifies\nnon-contributing or malicious participants to be removed. Our approach\ndifferentiates itself by not requiring any auxiliary/validation dataset.\nExtensive experiments on benchmark datasets show that RFFL can achieve high\nfairness and is very robust to different types of adversaries while achieving\ncompetitive predictive accuracy.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10464v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11393v1",
    "title": "Training Ethically Responsible AI Researchers: a Case Study",
    "authors": [
      "Hang Yuan",
      "Claudia Vanea",
      "Federica Lucivero",
      "Nina Hallowell"
    ],
    "author_ids": [],
    "abstract": "Ethical oversight of AI research is beset by a number of problems. There are\nnumerous ways to tackle these problems, however, they leave full responsibility\nfor ethical reflection in the hands of review boards and committees. In this\npaper, we propose an alternative solution: the training of ethically\nresponsible AI researchers. We showcase this solution through a case study of a\ncentre for doctoral training and outline how ethics training is structured in\nthe program. We go on to present two second-year students' reflections on their\ntraining which demonstrates some of their newly found capabilities as ethically\nresponsible researchers.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11393v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10280v1",
    "title": "Are Chess Discussions Racist? An Adversarial Hate Speech Data Set",
    "authors": [
      "Rupak Sarkar",
      "Ashiqur R. KhudaBukhsh"
    ],
    "author_ids": [],
    "abstract": "On June 28, 2020, while presenting a chess podcast on Grandmaster Hikaru\nNakamura, Antonio Radi\\'c's YouTube handle got blocked because it contained\n\"harmful and dangerous\" content. YouTube did not give further specific reason,\nand the channel got reinstated within 24 hours. However, Radi\\'c speculated\nthat given the current political situation, a referral to \"black against\nwhite\", albeit in the context of chess, earned him this temporary ban. In this\npaper, via a substantial corpus of 681,995 comments, on 8,818 YouTube videos\nhosted by five highly popular chess-focused YouTube channels, we ask the\nfollowing research question: \\emph{how robust are off-the-shelf hate-speech\nclassifiers to out-of-domain adversarial examples?} We release a data set of\n1,000 annotated comments where existing hate speech classifiers misclassified\nbenign chess discussions as hate speech. We conclude with an intriguing analogy\nresult on racial bias with our findings pointing out to the broader challenge\nof color polysemy.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10280v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10216v2",
    "title": "Sequential Targeting: an incremental learning approach for data imbalance in text classification",
    "authors": [
      "Joel Jang",
      "Yoonjeon Kim",
      "Kyoungho Choi",
      "Sungho Suh"
    ],
    "author_ids": [],
    "abstract": "Classification tasks require a balanced distribution of data to ensure the\nlearner to be trained to generalize over all classes. In real-world datasets,\nhowever, the number of instances vary substantially among classes. This\ntypically leads to a learner that promotes bias towards the majority group due\nto its dominating property. Therefore, methods to handle imbalanced datasets\nare crucial for alleviating distributional skews and fully utilizing the\nunder-represented data, especially in text classification. While addressing the\nimbalance in text data, most methods utilize sampling methods on the numerical\nrepresentation of the data, which limits its efficiency on how effective the\nrepresentation is. We propose a novel training method, Sequential\nTargeting(ST), independent of the effectiveness of the representation method,\nwhich enforces an incremental learning setting by splitting the data into\nmutually exclusive subsets and training the learner adaptively. To address\nproblems that arise within incremental learning, we apply elastic weight\nconsolidation. We demonstrate the effectiveness of our method through\nexperiments on simulated benchmark datasets (IMDB) and data collected from\nNAVER.",
    "published_date": "2020-11-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10216v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10023v1",
    "title": "Online decentralized decision making with inequality constraints: an ADMM approach",
    "authors": [
      "Yuxiao Chen",
      "Mario Santillo",
      "Mrdjan Jankovic",
      "Aaron D. Ames"
    ],
    "author_ids": [],
    "abstract": "We discuss an online decentralized decision making problem where the agents\nare coupled with affine inequality constraints. Alternating Direction Method of\nMultipliers (ADMM) is used as the computation engine and we discuss the\nconvergence of the algorithm in an online setting. To be specific, when\ndecisions have to be made sequentially with a fixed time step, there might not\nbe enough time for the ADMM to converge before the scenario changes and the\ndecision needs to be updated. In this case, a suboptimal solution is employed\nand we analyze the optimality gap given the convergence condition. Moreover, in\nmany cases, the decision making problem changes gradually over time. We propose\na warm-start scheme to accelerate the convergence of ADMM and analyze the\nbenefit of the warm-start. The proposed method is demonstrated in a\ndecentralized multiagent control barrier function problem with simulation.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10023v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.09988v1",
    "title": "Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics",
    "authors": [
      "Adam Coscia",
      "Duen Horng Chau",
      "Alex Endert"
    ],
    "author_ids": [],
    "abstract": "Mixed-initiative visual analytics systems incorporate well-established design\nprinciples that improve users' abilities to solve problems. As these systems\nconsider whether to take initiative towards achieving user goals, many current\nsystems address the potential for cognitive bias in human initiatives\nstatically, relying on fixed initiatives they can take instead of identifying,\ncommunicating and addressing the bias as it occurs. We argue that\nmixed-initiative design principles can and should incorporate cognitive bias\nmitigation strategies directly through development of mitigation techniques\nembedded in the system to address cognitive biases in situ. We identify domain\nexperts in machine learning adopting visual analytics techniques and systems\nthat incorporate existing mixed-initiative principles and examine their\npotential to support bias mitigation strategies. This examination considers the\nunique perspective these experts bring to visual analytics and is situated in\nexisting user-centered systems that make exemplary use of design principles\ninformed by cognitive theory. We then suggest informed opportunities for domain\nexperts to take initiative toward addressing cognitive biases in light of their\nexisting contributions to the field. Finally, we contribute open questions and\nresearch directions for designers seeking to adopt visual analytics techniques\nthat incorporate bias-aware initiatives in future systems.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09988v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09801v1",
    "title": "Novel Classification of Ischemic Heart Disease Using Artificial Neural Network",
    "authors": [
      "Giulia Silveri",
      "Marco Merlo",
      "Luca Restivo",
      "Gianfranco Sinagra",
      "Agostino Accardo"
    ],
    "author_ids": [],
    "abstract": "Ischemic heart disease (IHD), particularly in its chronic stable form, is a\nsubtle pathology due to its silent behavior before developing in unstable\nangina, myocardial infarction or sudden cardiac death. Machine learning\ntechniques applied to parameters extracted form heart rate variability (HRV)\nsignal seem to be a valuable support in the early diagnosis of some cardiac\ndiseases. However, so far, IHD patients were identified using Artificial Neural\nNetworks (ANNs) applied to a limited number of HRV parameters and only to very\nfew subjects. In this study, we used several linear and non-linear HRV\nparameters applied to ANNs, in order to confirm these results on a large cohort\nof 965 sample of subjects and to identify which features could discriminate IHD\npatients with high accuracy. By using principal component analysis and stepwise\nregression, we reduced the original 17 parameters to five, used as inputs, for\na series of ANNs. The highest accuracy of 82% was achieved using meanRR, LFn,\nSD1, gender and age parameters and two hidden neurons.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09801v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09766v1",
    "title": "Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery",
    "authors": [
      "Zhuo Zheng",
      "Yanfei Zhong",
      "Junjue Wang",
      "Ailong Ma"
    ],
    "author_ids": [],
    "abstract": "Geospatial object segmentation, as a particular semantic segmentation task,\nalways faces with larger-scale variation, larger intra-class variance of\nbackground, and foreground-background imbalance in the high spatial resolution\n(HSR) remote sensing imagery. However, general semantic segmentation methods\nmainly focus on scale variation in the natural scene, with inadequate\nconsideration of the other two problems that usually happen in the large area\nearth observation scene. In this paper, we argue that the problems lie on the\nlack of foreground modeling and propose a foreground-aware relation network\n(FarSeg) from the perspectives of relation-based and optimization-based\nforeground modeling, to alleviate the above two problems. From perspective of\nrelation, FarSeg enhances the discrimination of foreground features via\nforeground-correlated contexts associated by learning foreground-scene\nrelation. Meanwhile, from perspective of optimization, a foreground-aware\noptimization is proposed to focus on foreground examples and hard examples of\nbackground during training for a balanced optimization. The experimental\nresults obtained using a large scale dataset suggest that the proposed method\nis superior to the state-of-the-art general semantic segmentation methods and\nachieves a better trade-off between speed and accuracy. Code has been made\navailable at: \\url{https://github.com/Z-Zheng/FarSeg}.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09766v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.11486v1",
    "title": "Latent Adversarial Debiasing: Mitigating Collider Bias in Deep Neural Networks",
    "authors": [
      "Luke Darlow",
      "Stanisław Jastrzębski",
      "Amos Storkey"
    ],
    "author_ids": [],
    "abstract": "Collider bias is a harmful form of sample selection bias that neural networks\nare ill-equipped to handle. This bias manifests itself when the underlying\ncausal signal is strongly correlated with other confounding signals due to the\ntraining data collection procedure. In the situation where the confounding\nsignal is easy-to-learn, deep neural networks will latch onto this and the\nresulting model will generalise poorly to in-the-wild test scenarios. We argue\nherein that the cause of failure is a combination of the deep structure of\nneural networks and the greedy gradient-driven learning process used - one that\nprefers easy-to-compute signals when available. We show it is possible to\nmitigate against this by generating bias-decoupled training data using latent\nadversarial debiasing (LAD), even when the confounding signal is present in\n100% of the training data. By training neural networks on these adversarial\nexamples,we can improve their generalisation in collider bias settings.\nExperiments show state-of-the-art performance of LAD in label-free debiasing\nwith gains of 76.12% on background coloured MNIST, 35.47% on fore-ground\ncoloured MNIST, and 8.27% on corrupted CIFAR-10.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11486v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09641v2",
    "title": "On the geometry of symmetry breaking inequalities",
    "authors": [
      "José Verschae",
      "Matías Villagra",
      "Léonard von Niederhäusern"
    ],
    "author_ids": [],
    "abstract": "Breaking symmetries is a popular way of speeding up the branch-and-bound\nmethod for symmetric integer programs. We study fundamental domains, which are\nminimal and closed symmetry breaking polyhedra. Our long-term goal is to\nunderstand the relationship between the complexity of such polyhedra and their\nsymmetry breaking capability.\n  Borrowing ideas from geometric group theory, we provide structural properties\nthat relate the action of the group with the geometry of the facets of\nfundamental domains. Inspired by these insights, we provide a new generalized\nconstruction for fundamental domains, which we call generalized Dirichlet\ndomain (GDD). Our construction is recursive and exploits the coset\ndecomposition of the subgroups that fix given vectors in $\\mathbb{R}^n$. We use\nthis construction to analyze a recently introduced set of symmetry breaking\ninequalities by Salvagnin (2018) and Liberti and Ostrowski (2014), called\nSchreier-Sims inequalities. In particular, this shows that every permutation\ngroup admits a fundamental domain with less than $n$ facets. We also show that\nthis bound is tight.\n  Finally, we prove that the Schreier-Sims inequalities can contain an\nexponential number of isomorphic binary vectors for a given permutation group\n$G$, which provides evidence of the lack of symmetry breaking effectiveness of\nthis fundamental domain. Conversely, a suitably constructed GDD for this $G$\nhas linearly many inequalities and contains unique representatives for\nisomorphic binary vectors.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DM",
      "math.OC",
      "52B15 (Primary), 90C10 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09641v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.11483v4",
    "title": "Rethinking recidivism through a causal lens",
    "authors": [
      "Vik Shirvaikar",
      "Choudur Lakshminarayan"
    ],
    "author_ids": [],
    "abstract": "Predictive modeling of criminal recidivism, or whether people will re-offend\nin the future, has a long and contentious history. Modern causal inference\nmethods allow us to move beyond prediction and target the \"treatment effect\" of\na specific intervention on an outcome in an observational dataset. In this\npaper, we look specifically at the effect of incarceration (prison time) on\nrecidivism, using a well-known dataset from North Carolina. Two popular causal\nmethods for addressing confounding bias are explained and demonstrated:\ndirected acyclic graph (DAG) adjustment and double machine learning (DML),\nincluding a sensitivity analysis for unobserved confounders. We find that\nincarceration has a detrimental effect on recidivism, i.e., longer prison\nsentences make it more likely that individuals will re-offend after release,\nalthough this conclusion should not be generalized beyond the scope of our\ndata. We hope that this case study can inform future applications of causal\ninference to criminal justice analysis.",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.11483v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09625v2",
    "title": "Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP",
    "authors": [
      "John Chen",
      "Ian Berlot-Attwell",
      "Safwan Hossain",
      "Xindi Wang",
      "Frank Rudzicz"
    ],
    "author_ids": [],
    "abstract": "Clinical machine learning is increasingly multimodal, collected in both\nstructured tabular formats and unstructured forms such as freetext. We propose\na novel task of exploring fairness on a multimodal clinical dataset, adopting\nequalized odds for the downstream medical prediction tasks. To this end, we\ninvestigate a modality-agnostic fairness algorithm - equalized odds post\nprocessing - and compare it to a text-specific fairness algorithm: debiased\nclinical word embeddings. Despite the fact that debiased word embeddings do not\nexplicitly address equalized odds of protected groups, we show that a\ntext-specific approach to fairness may simultaneously achieve a good balance of\nperformance and classical notions of fairness. We hope that our paper inspires\nfuture contributions at the critical intersection of clinical NLP and fairness.\nThe full source code is available here:\nhttps://github.com/johntiger1/multimodal_fairness",
    "published_date": "2020-11-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09625v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09526v1",
    "title": "Contextual Fusion For Adversarial Robustness",
    "authors": [
      "Aiswarya Akumalla",
      "Seth Haney",
      "Maksim Bazhenov"
    ],
    "author_ids": [],
    "abstract": "Mammalian brains handle complex reasoning tasks in a gestalt manner by\nintegrating information from regions of the brain that are specialised to\nindividual sensory modalities. This allows for improved robustness and better\ngeneralisation ability. In contrast, deep neural networks are usually designed\nto process one particular information stream and susceptible to various types\nof adversarial perturbations. While many methods exist for detecting and\ndefending against adversarial attacks, they do not generalise across a range of\nattacks and negatively affect performance on clean, unperturbed data. We\ndeveloped a fusion model using a combination of background and foreground\nfeatures extracted in parallel from Places-CNN and Imagenet-CNN. We tested the\nbenefits of the fusion approach on preserving adversarial robustness for human\nperceivable (e.g., Gaussian blur) and network perceivable (e.g.,\ngradient-based) attacks for CIFAR-10 and MS COCO data sets. For gradient based\nattacks, our results show that fusion allows for significant improvements in\nclassification without decreasing performance on unperturbed data and without\nneed to perform adversarial retraining. Our fused model revealed improvements\nfor Gaussian blur type perturbations as well. The increase in performance from\nfusion approach depended on the variability of the image contexts; larger\nincreases were seen for classes of images with larger differences in their\ncontexts. We also demonstrate the effect of regularization to bias the\nclassifier decision in the presence of a known adversary. We propose that this\nbiologically inspired approach to integrate information across multiple\nmodalities provides a new way to improve adversarial robustness that can be\ncomplementary to current state of the art approaches.",
    "published_date": "2020-11-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09526v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.10472v2",
    "title": "GenderRobustness: Robustness of Gender Detection in Facial Recognition Systems with variation in Image Properties",
    "authors": [
      "Sharadha Srinivasan",
      "Madan Musuvathi"
    ],
    "author_ids": [],
    "abstract": "In recent times, there have been increasing accusations on artificial\nintelligence systems and algorithms of computer vision of possessing implicit\nbiases. Even though these conversations are more prevalent now and systems are\nimproving by performing extensive testing and broadening their horizon, biases\nstill do exist. One such class of systems where bias is said to exist is facial\nrecognition systems, where bias has been observed on the basis of gender,\nethnicity, skin tone and other facial attributes. This is even more disturbing,\ngiven the fact that these systems are used in practically every sector of the\nindustries today. From as critical as criminal identification to as simple as\ngetting your attendance registered, these systems have gained a huge market,\nespecially in recent years. That in itself is a good enough reason for\ndevelopers of these systems to ensure that the bias is kept to a bare minimum\nor ideally non-existent, to avoid major issues like favoring a particular\ngender, race, or class of people or rather making a class of people susceptible\nto false accusations due to inability of these systems to correctly recognize\nthose people.",
    "published_date": "2020-11-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.10472v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09406v1",
    "title": "Non-Adaptive Matroid Prophet Inequalities",
    "authors": [
      "Shuchi Chawla",
      "Kira Goldner",
      "Anna R. Karlin",
      "J. Benjamin Miller"
    ],
    "author_ids": [],
    "abstract": "We investigate non-adaptive algorithms for matroid prophet inequalities.\nMatroid prophet inequalities have been considered resolved since 2012 when\n[KW12] introduced thresholds that guarantee a tight 2-approximation to the\nprophet; however, this algorithm is adaptive. Other approaches of [CHMS10] and\n[FSZ16] have used non-adaptive thresholds with a feasibility restriction;\nhowever, this translates to adaptively changing an item's threshold to infinity\nwhen it cannot be taken with respect to the additional feasibility constraint,\nhence the algorithm is not truly non-adaptive. A major application of prophet\ninequalities is in auction design, where non-adaptive prices possess a\nsignificant advantage: they convert to order-oblivious posted pricings, and are\nessential for translating a prophet inequality into a truthful mechanism for\nmulti-dimensional buyers. The existing matroid prophet inequalities do not\nsuffice for this application. We present the first non-adaptive constant-factor\nprophet inequality for graphic matroids.",
    "published_date": "2020-11-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09406v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.09135v3",
    "title": "A Polyhedral Study for the Cubic Formulation of the Unconstrained Traveling Tournament Problem",
    "authors": [
      "Marije Siemann",
      "Matthias Walter"
    ],
    "author_ids": [],
    "abstract": "We consider the unconstrained traveling tournament problem, a sports\ntimetabling problem that minimizes traveling of teams. Since its introduction\nabout 20 years ago, most research was devoted to modeling and reformulation\napproaches. In this paper we carry out a polyhedral study for the cubic integer\nprogramming formulation by establishing the dimension of the integer hull as\nwell as of faces induced by model inequalities. Moreover, we introduce a new\nclass of inequalities and show that they are facet-defining. Finally, we\nevaluate the impact of these inequalities on the linear programming bounds.",
    "published_date": "2020-11-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DM",
      "math.OC",
      "90C57",
      "G.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09135v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.08946v3",
    "title": "A Unified Seeding Framework",
    "authors": [
      "Ya-Wen Teng",
      "Hsi-Wen Chen",
      "De-Nian Yang",
      "Yvonne-Anne Pignolet",
      "Ting-Wei Li",
      "Lydia Chen"
    ],
    "author_ids": [],
    "abstract": "Online social networks have become a crucial medium to disseminate the latest\npolitical, commercial, and social information. Users with high visibility are\noften selected as seeds to spread information and affect their adoption in\ntarget groups. We study how gender differences and similarities can impact the\ninformation spreading process. Using a large-scale Instagram dataset and a\nsmall-scale Facebook dataset, we first conduct a multi-faceted analysis taking\nthe interaction type, directionality and frequency into account. To this end,\nwe explore a variety of existing and new single and multihop centrality\nmeasures. Our analysis unveils that males and females interact differently\ndepending on the interaction types, e.g., likes or comments, and they feature\ndifferent support and promotion patterns. We complement prior work showing that\nfemales do not reach top visibility (often referred to as the glass ceiling\neffect) jointly factoring in the connectivity and interaction intensity, both\nof which were previously mainly discussed independently.\n  Inspired by these observations, we propose a novel seeding framework, called\nDisparity Seeding, which aims at maximizing spread while reaching a target user\ngroup, e.g., a certain percentage of females -- promoting the influence of\nunder-represented groups. Disparity Seeding ranks influential users with two\ngender-aware measures, the Target HI-index and the Embedding index. Extensive\nsimulations comparing Disparity Seeding with target-agnostic algorithms show\nthat Disparity Seeding meets the target percentage while effectively maximizing\nthe spread. Disparity Seeding can be generalized to counter different types of\ninequality, e.g., race, and proactively promote minorities in the society.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08946v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08898v1",
    "title": "Probing Fairness of Mobile Ocular Biometrics Methods Across Gender on VISOB 2.0 Dataset",
    "authors": [
      "Anoop Krishnan",
      "Ali Almadan",
      "Ajita Rattani"
    ],
    "author_ids": [],
    "abstract": "Recent research has questioned the fairness of face-based recognition and\nattribute classification methods (such as gender and race) for dark-skinned\npeople and women. Ocular biometrics in the visible spectrum is an alternate\nsolution over face biometrics, thanks to its accuracy, security, robustness\nagainst facial expression, and ease of use in mobile devices. With the recent\nCOVID-19 crisis, ocular biometrics has a further advantage over face biometrics\nin the presence of a mask. However, fairness of ocular biometrics has not been\nstudied till now. This first study aims to explore the fairness of ocular-based\nauthentication and gender classification methods across males and females. To\nthis aim, VISOB $2.0$ dataset, along with its gender annotations, is used for\nthe fairness analysis of ocular biometrics methods based on ResNet-50,\nMobileNet-V2 and lightCNN-29 models. Experimental results suggest the\nequivalent performance of males and females for ocular-based mobile\nuser-authentication in terms of genuine match rate (GMR) at lower false match\nrates (FMRs) and an overall Area Under Curve (AUC). For instance, an AUC of\n0.96 for females and 0.95 for males was obtained for lightCNN-29 on an average.\nHowever, males significantly outperformed females in deep learning based gender\nclassification models based on ocular-region.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08898v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08756v3",
    "title": "Stochastic Client Selection for Federated Learning with Volatile Clients",
    "authors": [
      "Tiansheng Huang",
      "Weiwei Lin",
      "Li Shen",
      "Keqin Li",
      "Albert Y. Zomaya"
    ],
    "author_ids": [],
    "abstract": "Federated Learning (FL), arising as a privacy-preserving machine learning\nparadigm, has received notable attention from the public. In each round of\nsynchronous FL training, only a fraction of available clients are chosen to\nparticipate, and the selection decision might have a significant effect on the\ntraining efficiency, as well as the final model performance. In this paper, we\ninvestigate the client selection problem under a volatile context, in which the\nlocal training of heterogeneous clients is likely to fail due to various kinds\nof reasons and in different levels of frequency. {\\color{black}Intuitively, too\nmuch training failure might potentially reduce the training efficiency, while\ntoo much selection on clients with greater stability might introduce bias,\nthereby resulting in degradation of the training effectiveness. To tackle this\ntradeoff, we in this paper formulate the client selection problem under joint\nconsideration of effective participation and fairness.} Further, we propose\nE3CS, a stochastic client selection scheme to solve the problem, and we\ncorroborate its effectiveness by conducting real data-based experiments.\nAccording to our experimental results, the proposed selection scheme is able to\nachieve up to 2x faster convergence to a fixed model accuracy while maintaining\nthe same level of final model accuracy, compared with the state-of-the-art\nselection schemes.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08756v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08731v1",
    "title": "A convergent structure-preserving finite-volume scheme for the Shigesada-Kawasaki-Teramoto population system",
    "authors": [
      "Antoine Zurek",
      "Ansgar Jüngel"
    ],
    "author_ids": [],
    "abstract": "An implicit Euler finite-volume scheme for an $n$-species population\ncross-diffusion system of Shigesada--Kawasaki--Teramoto-type in a bounded\ndomain with no-flux boundary conditions is proposed and analyzed. The scheme\npreserves the formal gradient-flow or entropy structure and preserves the\nnonnegativity of the population densities. The key idea is to consider a\nsuitable mean of the mobilities in such a way that a discrete chain rule is\nfulfilled and a discrete analog of the entropy inequality holds. The existence\nof finite-volume solutions, the convergence of the scheme, and the large-time\nasymptotics to the constant steady state are proven. Furthermore, numerical\nexperiments in one and two space dimensiona for two and three species are\npresented. The results are valid for a more general class of cross-diffusion\nsystems satisfying some structural conditions.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "65M08, 65M12, 35K51, 35Q92, 92D25"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08731v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.08678v2",
    "title": "Curriculum CycleGAN for Textual Sentiment Domain Adaptation with Multiple Sources",
    "authors": [
      "Sicheng Zhao",
      "Yang Xiao",
      "Jiang Guo",
      "Xiangyu Yue",
      "Jufeng Yang",
      "Ravi Krishna",
      "Pengfei Xu",
      "Kurt Keutzer"
    ],
    "author_ids": [],
    "abstract": "Sentiment analysis of user-generated reviews or comments on products and\nservices in social networks can help enterprises to analyze the feedback from\ncustomers and take corresponding actions for improvement. To mitigate\nlarge-scale annotations on the target domain, domain adaptation (DA) provides\nan alternate solution by learning a transferable model from other labeled\nsource domains. Existing multi-source domain adaptation (MDA) methods either\nfail to extract some discriminative features in the target domain that are\nrelated to sentiment, neglect the correlations of different sources and the\ndistribution difference among different sub-domains even in the same source, or\ncannot reflect the varying optimal weighting during different training stages.\nIn this paper, we propose a novel instance-level MDA framework, named\ncurriculum cycle-consistent generative adversarial network (C-CycleGAN), to\naddress the above issues. Specifically, C-CycleGAN consists of three\ncomponents: (1) pre-trained text encoder which encodes textual input from\ndifferent domains into a continuous representation space, (2) intermediate\ndomain generator with curriculum instance-level adaptation which bridges the\ngap across source and target domains, and (3) task classifier trained on the\nintermediate domain for final sentiment classification. C-CycleGAN transfers\nsource samples at instance-level to an intermediate domain that is closer to\nthe target domain with sentiment semantics preserved and without losing\ndiscriminative features. Further, our dynamic instance-level weighting\nmechanisms can assign the optimal weights to different source samples in each\ntraining stage. We conduct extensive experiments on three benchmark datasets\nand achieve substantial gains over state-of-the-art DA approaches. Our source\ncode is released at: https://github.com/WArushrush/Curriculum-CycleGAN.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08678v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08558v3",
    "title": "On the Transferability of Adversarial Attacksagainst Neural Text Classifier",
    "authors": [
      "Liping Yuan",
      "Xiaoqing Zheng",
      "Yi Zhou",
      "Cho-Jui Hsieh",
      "Kai-wei Chang"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks are vulnerable to adversarial attacks, where a small\nperturbation to an input alters the model prediction. In many cases, malicious\ninputs intentionally crafted for one model can fool another model. In this\npaper, we present the first study to systematically investigate the\ntransferability of adversarial examples for text classification models and\nexplore how various factors, including network architecture, tokenization\nscheme, word embedding, and model capacity, affect the transferability of\nadversarial examples. Based on these studies, we propose a genetic algorithm to\nfind an ensemble of models that can be used to induce adversarial examples to\nfool almost all existing models. Such adversarial examples reflect the defects\nof the learning process and the data bias in the training set. Finally, we\nderive word replacement rules that can be used for model diagnostics from these\nadversarial examples.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08558v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08398v1",
    "title": "Augmented Fairness: An Interpretable Model Augmenting Decision-Makers' Fairness",
    "authors": [
      "Tong Wang",
      "Maytal Saar-Tsechansky"
    ],
    "author_ids": [],
    "abstract": "We propose a model-agnostic approach for mitigating the prediction bias of a\nblack-box decision-maker, and in particular, a human decision-maker. Our method\ndetects in the feature space where the black-box decision-maker is biased and\nreplaces it with a few short decision rules, acting as a \"fair surrogate\". The\nrule-based surrogate model is trained under two objectives, predictive\nperformance and fairness. Our model focuses on a setting that is common in\npractice but distinct from other literature on fairness. We only have black-box\naccess to the model, and only a limited set of true labels can be queried under\na budget constraint. We formulate a multi-objective optimization for building a\nsurrogate model, where we simultaneously optimize for both predictive\nperformance and bias. To train the model, we propose a novel training algorithm\nthat combines a nondominated sorting genetic algorithm with active learning. We\ntest our model on public datasets where we simulate various biased \"black-box\"\nclassifiers (decision-makers) and apply our approach for interpretable\naugmented fairness.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08398v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08366v2",
    "title": "Uniform Bipartition in the Population Protocol Model with Arbitrary Communication Graphs",
    "authors": [
      "Hiroto Yasumi",
      "Fukuhito Ooshita",
      "Michiko Inoue",
      "Sébastien Tixeuil"
    ],
    "author_ids": [],
    "abstract": "In this paper, we focus on the uniform bipartition problem in the population\nprotocol model. This problem aims to divide a population into two groups of\nequal size. In particular, we consider the problem in the context of\n\\emph{arbitrary} communication graphs. As a result, we clarify the solvability\nof the uniform bipartition problem with arbitrary communication graphs when\nagents in the population have designated initial states, under various\nassumptions such as the existence of a base station, symmetry of the protocol,\nand fairness of the execution. When the problem is solvable, we present\nprotocols for uniform bipartition. When global fairness is assumed, the space\ncomplexity of our solutions is tight.",
    "published_date": "2020-11-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08366v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.08324v1",
    "title": "Examining the Feasibility of Off-the-Shelf Algorithms for Masking Directly Identifiable Information in Social Media Data",
    "authors": [
      "Rachel Dorn",
      "Alicia L. Nobles",
      "Masoud Rouhizadeh",
      "Mark Dredze"
    ],
    "author_ids": [],
    "abstract": "The identification and removal/replacement of protected information from\nsocial media data is an understudied problem, despite being desirable from an\nethical and legal perspective. This paper identifies types of potentially\ndirectly identifiable information (inspired by protected health information in\nclinical texts) contained in tweets that may be readily removed using\noff-the-shelf algorithms, introduces an English dataset of tweets annotated for\nidentifiable information, and compiles these off-the-shelf algorithms into a\ntool (Nightjar) to evaluate the feasibility of using Nightjar to remove\ndirectly identifiable information from the tweets. Nightjar as well as the\nannotated data can be retrieved from https://bitbucket.org/mdredze/nightjar.",
    "published_date": "2020-11-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08324v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.08278v1",
    "title": "A bibliometric methodology to unveil territorial inequities in the scientific wealth to combat COVID-19",
    "authors": [
      "Giovanni Abramo",
      "Ciriaco Andrea D'Angelo"
    ],
    "author_ids": [],
    "abstract": "In this paper we develop a methodology to assess the scientific wealth of\nterritories at field level. Our methodology uses a bibliometric approach based\non the observation of academic research performance and overall scientific\nproduction in each territory. We apply it to assess disparities in the Italian\nterritories in the medical specialties at the front line of the COVID-19\nemergency. Italy has been the first among western countries to be severely\naffected by the onset of the COVID-19 pandemic. The study reveals remarkable\ninequities across territories, with scientific weaknesses concentrated in the\nsouth. Policies for rebalancing the north-south divide should also consider, in\naddition to tangible assets, the gap in production and availability of quality\nmedical knowledge.",
    "published_date": "2020-11-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08278v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.08141v1",
    "title": "A Model of Polarization on Social Media Caused by Empathy and Repulsion",
    "authors": [
      "Naoki Hirakura",
      "Masaki Aida",
      "Konosuke Kawashima"
    ],
    "author_ids": [],
    "abstract": "In recent years, the ease with which social media can be accessed has led to\nthe unexpected problem of a shrinkage in information sources. This phenomenon\nis caused by a system that facilitates the connection of people with similar\nideas and recommendation systems. Bias in the selection of information sources\npromotes polarization that divides people into multiple groups with opposing\nviews and creates conflicts between opposing groups. This paper elucidates the\nmechanism of polarization by proposing a model of opinion formation in social\nmedia that considers users' reactions of empathy and repulsion. Based on the\nidea that opinion neutrality is only relative, this model offers a novel\ntechnology for dealing with polarization.",
    "published_date": "2020-11-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.PF",
      "physics.soc-ph",
      "94-06, 94-10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08141v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07887v4",
    "title": "A Survey of Requirements for COVID-19 Mitigation Strategies. Part I: Newspaper Clips",
    "authors": [
      "Wojciech Jamroga",
      "David Mestel",
      "Peter B. Roenne",
      "Peter Y. A. Ryan",
      "Marjan Skrobot"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic has influenced virtually all aspects of our lives.\nAcross the world, countries have applied various mitigation strategies for the\nepidemic, based on social, political, and technological instruments. We\npostulate that one should {identify the relevant requirements} before\ncommitting to a particular mitigation strategy. One way to achieve it is\nthrough an overview of what is considered relevant by the general public, and\nreferred to in the media. To this end, we have collected a number of news clips\nthat mention the possible goals and requirements for a mitigation strategy. The\nsnippets are sorted thematically into several categories, such as\nhealth-related goals, social and political impact, civil rights, ethical\nrequirements, and so on.\n  In a forthcoming companion paper, we will present a digest of the\nrequirements, derived from the news clips, and a preliminary take on their\nformal specification.",
    "published_date": "2020-11-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07887v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07734v2",
    "title": "SamWalker++: recommendation with informative sampling strategy",
    "authors": [
      "Can Wang",
      "Jiawei Chen",
      "Sheng Zhou",
      "Qihao Shi",
      "Yan Feng",
      "Chun Chen"
    ],
    "author_ids": [],
    "abstract": "Recommendation from implicit feedback is a highly challenging task due to the\nlack of reliable negative feedback data. Existing methods address this\nchallenge by treating all the un-observed data as negative (dislike) but\ndownweight the confidence of these data. However, this treatment causes two\nproblems: (1) Confidence weights of the unobserved data are usually assigned\nmanually, which lack flexibility and may create empirical bias on evaluating\nuser's preference. (2) To handle massive volume of the unobserved feedback\ndata, most of the existing methods rely on stochastic inference and data\nsampling strategies. However, since a user is only aware of a very small\nfraction of items in a large dataset, it is difficult for existing samplers to\nselect informative training instances in which the user really dislikes the\nitem rather than does not know it.\n  To address the above two problems, we propose two novel recommendation\nmethods SamWalker and SamWalker++ that support both adaptive confidence\nassignment and efficient model learning. SamWalker models data confidence with\na social network-aware function, which can adaptively specify different weights\nto different data according to users' social contexts. However, the social\nnetwork information may not be available in many recommender systems, which\nhinders application of SamWalker. Thus, we further propose SamWalker++, which\ndoes not require any side information and models data confidence with a\nconstructed pseudo-social network. We also develop fast random-walk-based\nsampling strategies for our SamWalker and SamWalker++ to adaptively draw\ninformative training instances, which can speed up gradient estimation and\nreduce sampling variance. Extensive experiments on five real-world datasets\ndemonstrate the superiority of the proposed SamWalker and SamWalker++.",
    "published_date": "2020-11-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07734v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07647v1",
    "title": "Good proctor or \"Big Brother\"? AI Ethics and Online Exam Supervision Technologies",
    "authors": [
      "Simon Coghlan",
      "Tim Miller",
      "Jeannie Paterson"
    ],
    "author_ids": [],
    "abstract": "This article philosophically analyzes online exam supervision technologies,\nwhich have been thrust into the public spotlight due to campus lockdowns during\nthe COVID-19 pandemic and the growing demand for online courses. Online exam\nproctoring technologies purport to provide effective oversight of students\nsitting online exams, using artificial intelligence (AI) systems and human\ninvigilators to supplement and review those systems. Such technologies have\nalarmed some students who see them as `Big Brother-like', yet some universities\ndefend their judicious use. Critical ethical appraisal of online proctoring\ntechnologies is overdue. This article philosophically analyzes these\ntechnologies, focusing on the ethical concepts of academic integrity, fairness,\nnon-maleficence, transparency, privacy, respect for autonomy, liberty, and\ntrust. Most of these concepts are prominent in the new field of AI ethics and\nall are relevant to the education context. The essay provides ethical\nconsiderations that educational institutions will need to carefully review\nbefore electing to deploy and govern specific online proctoring technologies.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07647v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.07626v1",
    "title": "Stability Analysis of Complementarity Systems with Neural Network Controllers",
    "authors": [
      "Alp Aydinoglu",
      "Mahyar Fazlyab",
      "Manfred Morari",
      "Michael Posa"
    ],
    "author_ids": [],
    "abstract": "Complementarity problems, a class of mathematical optimization problems with\northogonality constraints, are widely used in many robotics tasks, such as\nlocomotion and manipulation, due to their ability to model non-smooth phenomena\n(e.g., contact dynamics). In this paper, we propose a method to analyze the\nstability of complementarity systems with neural network controllers. First, we\nintroduce a method to represent neural networks with rectified linear unit\n(ReLU) activations as the solution to a linear complementarity problem. Then,\nwe show that systems with ReLU network controllers have an equivalent linear\ncomplementarity system (LCS) description. Using the LCS representation, we turn\nthe stability verification problem into a linear matrix inequality (LMI)\nfeasibility problem. We demonstrate the approach on several examples, including\nmulti-contact problems and friction models with non-unique solutions.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07626v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08181v5",
    "title": "A Random Matrix Theory Approach to Damping in Deep Learning",
    "authors": [
      "Diego Granziol",
      "Nicholas Baskerville"
    ],
    "author_ids": [],
    "abstract": "We conjecture that the inherent difference in generalisation between adaptive\nand non-adaptive gradient methods in deep learning stems from the increased\nestimation noise in the flattest directions of the true loss surface. We\ndemonstrate that typical schedules used for adaptive methods (with low\nnumerical stability or damping constants) serve to bias relative movement\ntowards flat directions relative to sharp directions, effectively amplifying\nthe noise-to-signal ratio and harming generalisation. We further demonstrate\nthat the numerical damping constant used in these methods can be decomposed\ninto a learning rate reduction and linear shrinkage of the estimated curvature\nmatrix. We then demonstrate significant generalisation improvements by\nincreasing the shrinkage coefficient, closing the generalisation gap entirely\nin both logistic regression and several deep neural network experiments.\nExtending this line further, we develop a novel random matrix theory based\ndamping learner for second order optimiser inspired by linear shrinkage\nestimation. We experimentally demonstrate our learner to be very insensitive to\nthe initialised value and to allow for extremely fast convergence in\nconjunction with continued stable training and competitive generalisation.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08181v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.07542v3",
    "title": "Automatic and perceptual discrimination between dysarthria, apraxia of speech, and neurotypical speech",
    "authors": [
      "I. Kodrasi",
      "M. Pernon",
      "M. Laganaro",
      "H. Bourlard"
    ],
    "author_ids": [],
    "abstract": "Automatic techniques in the context of motor speech disorders (MSDs) are\ntypically two-class techniques aiming to discriminate between dysarthria and\nneurotypical speech or between dysarthria and apraxia of speech (AoS). Further,\nalthough such techniques are proposed to support the perceptual assessment of\nclinicians, the automatic and perceptual classification accuracy has never been\ncompared. In this paper, we investigate a three-class automatic technique and a\nset of handcrafted features for the discrimination of dysarthria, AoS and\nneurotypical speech. Instead of following the commonly used One-versus-One or\nOne-versus-Rest approaches for multi-class classification, a hierarchical\napproach is proposed. Further, a perceptual study is conducted where speech and\nlanguage pathologists are asked to listen to recordings of dysarthria, AoS, and\nneurotypical speech and decide which class the recordings belong to. The\nproposed automatic technique is evaluated on the same recordings and the\nautomatic and perceptual classification performance are compared. The presented\nresults show that the hierarchical classification approach yields a higher\nclassification accuracy than baseline One-versus-One and One-versus-Rest\napproaches. Further, the presented results show that the automatic approach\nyields a higher classification accuracy than the perceptual assessment of\nspeech and language pathologists, demonstrating the potential advantages of\nintegrating automatic tools in clinical practice.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07542v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07495v1",
    "title": "FAIR: Fair Adversarial Instance Re-weighting",
    "authors": [
      "Andrija Petrović",
      "Mladen Nikolić",
      "Sandro Radovanović",
      "Boris Delibašić",
      "Miloš Jovanović"
    ],
    "author_ids": [],
    "abstract": "With growing awareness of societal impact of artificial intelligence,\nfairness has become an important aspect of machine learning algorithms. The\nissue is that human biases towards certain groups of population, defined by\nsensitive features like race and gender, are introduced to the training data\nthrough data collection and labeling. Two important directions of fairness\nensuring research have focused on (i) instance weighting in order to decrease\nthe impact of more biased instances and (ii) adversarial training in order to\nconstruct data representations informative of the target variable, but\nuninformative of the sensitive attributes. In this paper we propose a Fair\nAdversarial Instance Re-weighting (FAIR) method, which uses adversarial\ntraining to learn instance weighting function that ensures fair predictions.\nMerging the two paradigms, it inherits desirable properties from both --\ninterpretability of reweighting and end-to-end trainability of adversarial\ntraining. We propose four different variants of the method and, among other\nthings, demonstrate how the method can be cast in a fully probabilistic\nframework. Additionally, theoretical analysis of FAIR models' properties have\nbeen studied extensively. We compare FAIR models to 7 other related and\nstate-of-the-art models and demonstrate that FAIR is able to achieve a better\ntrade-off between accuracy and unfairness. To the best of our knowledge, this\nis the first model that merges reweighting and adversarial approaches by means\nof a weighting function that can provide interpretable information about\nfairness of individual instances.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07495v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.07453v1",
    "title": "Debiasing Convolutional Neural Networks via Meta Orthogonalization",
    "authors": [
      "Kurtis Evan David",
      "Qiang Liu",
      "Ruth Fong"
    ],
    "author_ids": [],
    "abstract": "While deep learning models often achieve strong task performance, their\nsuccesses are hampered by their inability to disentangle spurious correlations\nfrom causative factors, such as when they use protected attributes (e.g., race,\ngender, etc.) to make decisions. In this work, we tackle the problem of\ndebiasing convolutional neural networks (CNNs) in such instances. Building off\nof existing work on debiasing word embeddings and model interpretability, our\nMeta Orthogonalization method encourages the CNN representations of different\nconcepts (e.g., gender and class labels) to be orthogonal to one another in\nactivation space while maintaining strong downstream task performance. Through\na variety of experiments, we systematically test our method and demonstrate\nthat it significantly mitigates model bias and is competitive against current\nadversarial debiasing methods.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08434v4",
    "title": "Simple and optimal methods for stochastic variational inequalities, II: Markovian noise and policy evaluation in reinforcement learning",
    "authors": [
      "Georgios Kotsalis",
      "Guanghui Lan",
      "Tianjiao Li"
    ],
    "author_ids": [],
    "abstract": "The focus of this paper is on stochastic variational inequalities (VI) under\nMarkovian noise. A prominent application of our algorithmic developments is the\nstochastic policy evaluation problem in reinforcement learning. Prior\ninvestigations in the literature focused on temporal difference (TD) learning\nby employing nonsmooth finite time analysis motivated by stochastic subgradient\ndescent leading to certain limitations. These encompass the requirement of\nanalyzing a modified TD algorithm that involves projection to an a-priori\ndefined Euclidean ball, achieving a non-optimal convergence rate and no clear\nway of deriving the beneficial effects of parallel implementation. Our approach\nremedies these shortcomings in the broader context of stochastic VIs and in\nparticular when it comes to stochastic policy evaluation. We developed a\nvariety of simple TD learning type algorithms motivated by its original version\nthat maintain its simplicity, while offering distinct advantages from a\nnon-asymptotic analysis point of view. We first provide an improved analysis of\nthe standard TD algorithm that can benefit from parallel implementation. Then\nwe present versions of a conditional TD algorithm (CTD), that involves periodic\nupdates of the stochastic iterates, which reduce the bias and therefore exhibit\nimproved iteration complexity. This brings us to the fast TD (FTD) algorithm\nwhich combines elements of CTD and the stochastic operator extrapolation method\nof the companion paper. For a novel index resetting policy FTD exhibits the\nbest known convergence rate. We also devised a robust version of the algorithm\nthat is particularly suitable for discounting factors close to 1.",
    "published_date": "2020-11-15T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "90C25, 90C15, 62L20, 68Q25"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08434v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.07359v1",
    "title": "Analyzing 'Near Me' Services: Potential for Exposure Bias in Location-based Retrieval",
    "authors": [
      "Ashmi Banerjee",
      "Gourab K Patro",
      "Linus W. Dietz",
      "Abhijnan Chakraborty"
    ],
    "author_ids": [],
    "abstract": "The proliferation of smartphones has led to the increased popularity of\nlocation-based search and recommendation systems. Online platforms like Google\nand Yelp allow location-based search in the form of nearby feature to query for\nhotels or restaurants in the vicinity. Moreover, hotel booking platforms like\nBooking[dot]com, Expedia, or Trivago allow travelers searching for\naccommodations using either their desired location as a search query or near a\nparticular landmark. Since the popularity of different locations in a city\nvaries, certain locations may get more queries than other locations. Thus, the\nexposure received by different establishments at these locations may be very\ndifferent from their intrinsic quality as captured in their ratings.\n  Today, many small businesses (shops, hotels, or restaurants) rely on such\nonline platforms for attracting customers. Thus, receiving less exposure than\nthat is expected can be unfavorable for businesses. It could have a negative\nimpact on their revenue and potentially lead to economic starvation or even\nshutdown. By gathering and analyzing data from three popular platforms, we\nobserve that many top-rated hotels and restaurants get less exposure vis-a-vis\ntheir quality, which could be detrimental for them. Following a meritocratic\nnotion, we define and quantify such exposure disparity due to location-based\nsearches on these platforms. We attribute this exposure disparity mainly to two\nkinds of biases -- Popularity Bias and Position Bias. Our experimental\nevaluation on multiple datasets reveals that although the platforms are doing\nwell in delivering distance-based results, exposure disparity exists for\nindividual businesses and needs to be reduced for business sustainability.",
    "published_date": "2020-11-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07359v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07312v1",
    "title": "Shortcomings of Counterfactual Fairness and a Proposed Modification",
    "authors": [
      "Fabian Beigang"
    ],
    "author_ids": [],
    "abstract": "In this paper, I argue that counterfactual fairness does not constitute a\nnecessary condition for an algorithm to be fair, and subsequently suggest how\nthe constraint can be modified in order to remedy this shortcoming. To this\nend, I discuss a hypothetical scenario in which counterfactual fairness and an\nintuitive judgment of fairness come apart. Then, I turn to the question how the\nconcept of discrimination can be explicated in order to examine the\nshortcomings of counterfactual fairness as a necessary condition of algorithmic\nfairness in more detail. I then incorporate the insights of this analysis into\na novel fairness constraint, causal relevance fairness, which is a modification\nof the counterfactual fairness constraint that seems to circumvent its\nshortcomings.",
    "published_date": "2020-11-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07312v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.07221v3",
    "title": "Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty",
    "authors": [
      "Soufiane Belharbi",
      "Jérôme Rony",
      "Jose Dolz",
      "Ismail Ben Ayed",
      "Luke McCaffrey",
      "Eric Granger"
    ],
    "author_ids": [],
    "abstract": "Weakly-supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations. Given global image labels,\nWSL methods yield pixel-level predictions (segmentations), which enable to\ninterpret class predictions. Despite their recent success, mostly with natural\nimages, such methods can face important challenges when the foreground and\nbackground regions have similar visual cues, yielding high false-positive rates\nin segmentations, as is the case in challenging histology images. WSL training\nis commonly driven by standard classification losses, which implicitly maximize\nmodel confidence, and locate the discriminative regions linked to\nclassification decisions. Therefore, they lack mechanisms for modeling\nexplicitly non-discriminative regions and reducing false-positive rates. We\npropose novel regularization terms, which enable the model to seek both\nnon-discriminative and discriminative regions, while discouraging unbalanced\nsegmentations. We introduce high uncertainty as a criterion to localize\nnon-discriminative regions that do not affect classifier decision, and describe\nit with original Kullback-Leibler (KL) divergence losses evaluating the\ndeviation of posterior predictions from the uniform distribution. Our KL terms\nencourage high uncertainty of the model when the latter inputs the latent\nnon-discriminative regions. Our loss integrates: (i) a cross-entropy seeking a\nforeground, where model confidence about class prediction is high; (ii) a KL\nregularizer seeking a background, where model uncertainty is high; and (iii)\nlog-barrier terms discouraging unbalanced segmentations. Comprehensive\nexperiments and ablation studies over the public GlaS colon cancer data and a\nCamelyon16 patch-based benchmark for breast cancer show substantial\nimprovements over state-of-the-art WSL methods, and confirm the effect of our\nnew regularizers.",
    "published_date": "2020-11-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07221v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.07194v2",
    "title": "Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy",
    "authors": [
      "Amanda Coston",
      "Neel Guha",
      "Derek Ouyang",
      "Lisa Lu",
      "Alexandra Chouldechova",
      "Daniel E. Ho"
    ],
    "author_ids": [],
    "abstract": "Anonymized smartphone-based mobility data has been widely adopted in devising\nand evaluating COVID-19 response strategies such as the targeting of public\nhealth resources. Yet little attention has been paid to measurement validity\nand demographic bias, due in part to the lack of documentation about which\nusers are represented as well as the challenge of obtaining ground truth data\non unique visits and demographics. We illustrate how linking large-scale\nadministrative data can enable auditing mobility data for bias in the absence\nof demographic information and ground truth labels. More precisely, we show\nthat linking voter roll data -- containing individual-level voter turnout for\nspecific voting locations along with race and age -- can facilitate the\nconstruction of rigorous bias and reliability tests. These tests illuminate a\nsampling bias that is particularly noteworthy in the pandemic context: older\nand non-white voters are less likely to be captured by mobility data. We show\nthat allocating public health resources based on such mobility data could\ndisproportionately harm high-risk elderly and minority groups.",
    "published_date": "2020-11-14T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07194v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07161v1",
    "title": "Ambient heat and human sleep",
    "authors": [
      "Kelton Minor",
      "Andreas Bjerre-Nielsen",
      "Sigga Svala Jonasdottir",
      "Sune Lehmann",
      "Nick Obradovich"
    ],
    "author_ids": [],
    "abstract": "Ambient temperatures are rising globally, with the greatest increases\nrecorded at night. Concurrently, the prevalence of insufficient sleep is\nincreasing in many populations, with substantial costs to human health and\nwell-being. Even though nearly a third of the human lifespan is spent asleep,\nit remains unknown whether temperature and weather impact objective measures of\nsleep in real-world settings, globally. Here we link billions of sleep\nmeasurements from wearable devices comprising over 7 million nighttime sleep\nrecords across 68 countries to local daily meteorological data from 2015 to\n2017. Rising nighttime temperatures shorten within-person sleep duration\nprimarily through delayed onset, increasing the probability of insufficient\nsleep. The effect of temperature on sleep loss is substantially larger for\nresidents from lower income countries and older adults, and females are\naffected more than are males. Nighttime temperature increases inflict the\ngreatest sleep loss during summer and fall months, and we do not find evidence\nof short-term acclimatization. Coupling historical behavioral measurements with\noutput from climate models, we project that climate change will further erode\nhuman sleep, producing substantial geographic inequalities. Our findings have\nsignificant implications for adaptation planning and illuminate a pathway\nthrough which rising temperatures may globally impact public health.",
    "published_date": "2020-11-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "econ.GN",
      "q-fin.EC",
      "J.3; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07161v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.07158v1",
    "title": "An example of prediction which complies with Demographic Parity and equalizes group-wise risks in the context of regression",
    "authors": [
      "Evgenii Chzhen",
      "Nicolas Schreuder"
    ],
    "author_ids": [],
    "abstract": "Let $(X, S, Y) \\in \\mathbb{R}^p \\times \\{1, 2\\} \\times \\mathbb{R}$ be a\ntriplet following some joint distribution $\\mathbb{P}$ with feature vector $X$,\nsensitive attribute $S$ , and target variable $Y$. The Bayes optimal prediction\n$f^*$ which does not produce Disparate Treatment is defined as $f^*(x) =\n\\mathbb{E}[Y | X = x]$. We provide a non-trivial example of a prediction $x \\to\nf(x)$ which satisfies two common group-fairness notions: Demographic Parity\n\\begin{align} (f(X) | S = 1) &\\stackrel{d}{=} (f(X) | S = 2) \\end{align} and\nEqual Group-Wise Risks \\begin{align}\n  \\mathbb{E}[(f^*(X) - f(X))^2 | S = 1] = \\mathbb{E}[(f^*(X) - f(X))^2 | S =\n2]. \\end{align} To the best of our knowledge this is the first explicit\nconstruction of a non-constant predictor satisfying the above. We discuss\nseveral implications of this result on better understanding of mathematical\nnotions of algorithmic fairness.",
    "published_date": "2020-11-13T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.08001v3",
    "title": "Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment",
    "authors": [
      "Omid Haji Maghsoudi",
      "Aimilia Gastounioti",
      "Christopher Scott",
      "Lauren Pantalone",
      "Fang-Fang Wu",
      "Eric A. Cohen",
      "Stacey Winham",
      "Emily F. Conant",
      "Celine Vachon",
      "Despina Kontos"
    ],
    "author_ids": [],
    "abstract": "Breast density is an important risk factor for breast cancer that also\naffects the specificity and sensitivity of screening mammography. Current\nfederal legislation mandates reporting of breast density for all women\nundergoing breast screening. Clinically, breast density is assessed visually\nusing the American College of Radiology Breast Imaging Reporting And Data\nSystem (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)\nmethod to estimate breast percentage density (PD) from digital mammograms. Our\nmethod leverages deep learning (DL) using two convolutional neural network\narchitectures to accurately segment the breast area. A machine-learning\nalgorithm combining superpixel generation, texture feature analysis, and\nsupport vector machine is then applied to differentiate dense from non-dense\ntissue regions, from which PD is estimated. Our method has been trained and\nvalidated on a multi-ethnic, multi-institutional dataset of 15,661 images\n(4,437 women), and then tested on an independent dataset of 6,368 digital\nmammograms (1,702 women; cases=414) for both PD estimation and discrimination\nof breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and\nan expert reader were strongly correlated (Spearman correlation coefficient =\n0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination\nperformance (area under the ROC curve, AUC = 0.611 [95% confidence interval\n(CI): 0.583, 0.639]) compared to four other widely-used research and commercial\nPD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong\nagreement of PD estimates between Deep-LIBRA and gold-standard assessment by an\nexpert reader, as well as improved performance in breast cancer risk assessment\nover state-of-the-art open-source and commercial methods.",
    "published_date": "2020-11-13T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.08001v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06738v1",
    "title": "Metric-Free Individual Fairness with Cooperative Contextual Bandits",
    "authors": [
      "Qian Hu",
      "Huzefa Rangwala"
    ],
    "author_ids": [],
    "abstract": "Data mining algorithms are increasingly used in automated decision making\nacross all walks of daily life. Unfortunately, as reported in several studies\nthese algorithms inject bias from data and environment leading to inequitable\nand unfair solutions. To mitigate bias in machine learning, different\nformalizations of fairness have been proposed that can be categorized into\ngroup fairness and individual fairness. Group fairness requires that different\ngroups should be treated similarly which might be unfair to some individuals\nwithin a group. On the other hand, individual fairness requires that similar\nindividuals be treated similarly. However, individual fairness remains\nunderstudied due to its reliance on problem-specific similarity metrics. We\npropose a metric-free individual fairness and a cooperative contextual bandits\n(CCB) algorithm. The CCB algorithm utilizes fairness as a reward and attempts\nto maximize it. The advantage of treating fairness as a reward is that the\nfairness criterion does not need to be differentiable. The proposed algorithm\nis tested on multiple real-world benchmark datasets. The results show the\neffectiveness of the proposed algorithm at mitigating bias and at achieving\nboth individual and group fairness.",
    "published_date": "2020-11-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06738v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06569v3",
    "title": "Usefulness of adaptive strategies in asymptotic quantum channel discrimination",
    "authors": [
      "Farzin Salek",
      "Masahito Hayashi",
      "Andreas Winter"
    ],
    "author_ids": [],
    "abstract": "Adaptiveness is a key principle in information processing including\nstatistics and machine learning. We investigate the usefulness of adaptive\nmethods in the framework of asymptotic binary hypothesis testing, when each\nhypothesis represents asymptotically many independent instances of a quantum\nchannel, and the tests are based on using the unknown channel and observing\noutputs. Unlike the familiar setting of quantum states as hypotheses, there is\na fundamental distinction between adaptive and non-adaptive strategies with\nrespect to the channel uses, and we introduce a number of further variants of\nthe discrimination tasks by imposing different restrictions on the test\nstrategies. The following results are obtained: (1) We prove that for\nclassical-quantum channels, adaptive and non-adaptive strategies lead to the\nsame error exponents both in the symmetric (Chernoff) and asymmetric\n(Hoeffding, Stein) settings. (2) The first separation between adaptive and\nnon-adaptive symmetric hypothesis testing exponents for quantum channels, which\nwe derive from a general lower bound on the error probability for non-adaptive\nstrategies; the concrete example we analyze is a pair of entanglement-breaking\nchannels. (3)We prove, in some sense generalizing the previous statement, that\nfor general channels adaptive strategies restricted to classical feed-forward\nand product state channel inputs are not superior in the asymptotic limit to\nnon-adaptive product state strategies. (4) As an application of our findings,\nwe address the discrimination power of an arbitrary quantum channel and show\nthat adaptive strategies with classical feedback and no quantum memory at the\ninput do not increase the discrimination power of the channel beyond\nnon-adaptive tensor product input strategies.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06569v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06550v4",
    "title": "Implicit bias of any algorithm: bounding bias via margin",
    "authors": [
      "Elvis Dohmatob"
    ],
    "author_ids": [],
    "abstract": "Consider $n$ points $x_1,\\ldots,x_n$ in finite-dimensional euclidean space,\neach having one of two colors. Suppose there exists a separating hyperplane\n(identified with its unit normal vector $w)$ for the points, i.e a hyperplane\nsuch that points of same color lie on the same side of the hyperplane. We\nmeasure the quality of such a hyperplane by its margin $\\gamma(w)$, defined as\nminimum distance between any of the points $x_i$ and the hyperplane. In this\npaper, we prove that the margin function $\\gamma$ satisfies a nonsmooth\nKurdyka-Lojasiewicz inequality with exponent $1/2$. This result has\nfar-reaching consequences. For example, let $\\gamma^{opt}$ be the maximum\npossible margin for the problem and let $w^{opt}$ be the parameter for the\nhyperplane which attains this value. Given any other separating hyperplane with\nparameter $w$, let $d(w):=\\|w-w^{opt}\\|$ be the euclidean distance between $w$\nand $w^{opt}$, also called the bias of $w$. From the previous KL-inequality, we\ndeduce that $(\\gamma^{opt}-\\gamma(w)) / R \\le d(w) \\le\n2\\sqrt{(\\gamma^{opt}-\\gamma(w))/\\gamma^{opt}}$, where $R:=\\max_i \\|x_i\\|$ is\nthe maximum distance of the points $x_i$ from the origin. Consequently, for any\noptimization algorithm (gradient-descent or not), the bias of the iterates\nconverges at least as fast as the square-root of the rate of their convergence\nof the margin. Thus, our work provides a generic tool for analyzing the\nimplicit bias of any algorithm in terms of its margin, in situations where a\nspecialized analysis might not be available: it is sufficient to establish a\ngood rate for converge of the margin, a task which is usually much easier.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06550v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06516v2",
    "title": "Sample-driven optimal stopping: From the secretary problem to the i.i.d. prophet inequality",
    "authors": [
      "José Correa",
      "Andrés Cristi",
      "Boris Epstein",
      "José Soto"
    ],
    "author_ids": [],
    "abstract": "We take a unifying approach to single selection optimal stopping problems\nwith random arrival order and independent sampling of items. In the problem we\nconsider, a decision maker (DM) initially gets to sample each of $N$ items\nindependently with probability $p$, and can observe the relative rankings of\nthese sampled items. Then, the DM faces the remaining items in an online\nfashion, observing the relative rankings of all revealed items. While scanning\nthe sequence the DM makes irrevocable stop/continue decisions and her reward\nfor stopping the sequence facing the item with rank $i$ is $Y_i$. The goal of\nthe DM is to maximize her reward. We start by studying the case in which the\nvalues $Y_i$ are known to the DM, and then move to the case in which these\nvalues are adversarial.\n  For the former case, we write the natural linear program that captures the\nperformance of an algorithm, and take its continuous limit. We prove a\nstructural result about this continuous limit, which allows us to reduce the\nproblem to a relatively simple real optimization problem. We establish that the\noptimal algorithm is given by a sequence of thresholds $t_1\\le t_2\\le\\cdots$\nsuch that the DM should stop if seeing an item with current ranking $i$ after\ntime $t_i$. Additionally we are able to recover several classic results in the\narea such as those for secretary problem and the minimum ranking problem. For\nthe adversarial case, we obtain a similar linear program with an additional\nstochastic dominance constraint. Using the same machinery we are able to pin\ndown the optimal competitive ratios for all values of $p$. Notably, we prove\nthat as $p$ approaches 1, our guarantee converges linearly to 0.745, matching\nthat of the i.i.d.~prophet inequality. Also interesting is the case $p=1/2$,\nwhere our bound evaluates to $0.671$, which improves upon the state of the art.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06516v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.06485v2",
    "title": "Fairness and Robustness in Invariant Learning: A Case Study in Toxicity Classification",
    "authors": [
      "Robert Adragna",
      "Elliot Creager",
      "David Madras",
      "Richard Zemel"
    ],
    "author_ids": [],
    "abstract": "Robustness is of central importance in machine learning and has given rise to\nthe fields of domain generalization and invariant learning, which are concerned\nwith improving performance on a test distribution distinct from but related to\nthe training distribution. In light of recent work suggesting an intimate\nconnection between fairness and robustness, we investigate whether algorithms\nfrom robust ML can be used to improve the fairness of classifiers that are\ntrained on biased data and tested on unbiased data. We apply Invariant Risk\nMinimization (IRM), a domain generalization algorithm that employs a causal\ndiscovery inspired method to find robust predictors, to the task of fairly\npredicting the toxicity of internet comments. We show that IRM achieves better\nout-of-distribution accuracy and fairness than Empirical Risk Minimization\n(ERM) methods, and analyze both the difficulties that arise when applying IRM\nin practice and the conditions under which IRM will likely be effective in this\nscenario. We hope that this work will inspire further studies of how robust\nmachine learning methods relate to algorithmic fairness.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06485v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06445v2",
    "title": "How to Measure Gender Bias in Machine Translation: Optimal Translators, Multiple Reference Points",
    "authors": [
      "Anna Farkas",
      "Renáta Németh"
    ],
    "author_ids": [],
    "abstract": "In this paper, as a case study, we present a systematic study of gender bias\nin machine translation with Google Translate. We translated sentences\ncontaining names of occupations from Hungarian, a language with gender-neutral\npronouns, into English. Our aim was to present a fair measure for bias by\ncomparing the translations to an optimal non-biased translator. When assessing\nbias, we used the following reference points: (1) the distribution of men and\nwomen among occupations in both the source and the target language countries,\nas well as (2) the results of a Hungarian survey that examined if certain jobs\nare generally perceived as feminine or masculine. We also studied how expanding\nsentences with adjectives referring to occupations effect the gender of the\ntranslated pronouns. As a result, we found bias against both genders, but\nbiased results against women are much more frequent. Translations are closer to\nour perception of occupations than to objective occupational statistics.\nFinally, occupations have a greater effect on translation than adjectives.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06445v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06422v1",
    "title": "Pursuing Open-Source Development of Predictive Algorithms: The Case of Criminal Sentencing Algorithms",
    "authors": [
      "Philip D. Waggoner",
      "Alec Macmillen"
    ],
    "author_ids": [],
    "abstract": "Currently, there is uncertainty surrounding the merits of open-source versus\nproprietary algorithm development. Though justification in favor of each\nexists, we argue that open-source algorithm development should be the standard\nin highly consequential contexts that affect people's lives for reasons of\ntransparency and collaboration, which contribute to greater predictive accuracy\nand enjoy the additional advantage of cost-effectiveness. To make this case, we\nfocus on criminal sentencing algorithms, as criminal sentencing is highly\nconsequential, and impacts society and individual people. Further, the\npopularity of this topic has surged in the wake of recent studies uncovering\nracial bias in proprietary sentencing algorithms among other issues of\nover-fitting and model complexity. We suggest these issues are exacerbated by\nthe proprietary and expensive nature of virtually all widely used criminal\nsentencing algorithms. Upon replicating a major algorithm using real criminal\nprofiles, we fit three penalized regressions and demonstrate an increase in\npredictive power of these open-source and relatively computationally\ninexpensive options. The result is a data-driven suggestion that if judges who\nare making sentencing decisions want to craft appropriate sentences based on a\nhigh degree of accuracy and at low costs, then they should be pursuing\nopen-source options.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06422v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.06186v4",
    "title": "Towards Optimal Problem Dependent Generalization Error Bounds in Statistical Learning Theory",
    "authors": [
      "Yunbei Xu",
      "Assaf Zeevi"
    ],
    "author_ids": [],
    "abstract": "We study problem-dependent rates, i.e., generalization errors that scale\nnear-optimally with the variance, the effective loss, or the gradient norms\nevaluated at the \"best hypothesis.\" We introduce a principled framework dubbed\n\"uniform localized convergence,\" and characterize sharp problem-dependent rates\nfor central statistical learning problems. From a methodological viewpoint, our\nframework resolves several fundamental limitations of existing uniform\nconvergence and localization analysis approaches. It also provides improvements\nand some level of unification in the study of localized complexities, one-sided\nuniform inequalities, and sample-based iterative algorithms. In the so-called\n\"slow rate\" regime, we provides the first (moment-penalized) estimator that\nachieves the optimal variance-dependent rate for general \"rich\" classes; we\nalso establish improved loss-dependent rate for standard empirical risk\nminimization. In the \"fast rate\" regime, we establish finite-sample\nproblem-dependent bounds that are comparable to precise asymptotics. In\naddition, we show that iterative algorithms like gradient descent and\nfirst-order Expectation-Maximization can achieve optimal generalization error\nin several representative problems across the areas of non-convex learning,\nstochastic optimization, and learning with missing data.",
    "published_date": "2020-11-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06186v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06100v2",
    "title": "Exploring Gender Disparities in Time to Diagnosis",
    "authors": [
      "Tony Y. Sun",
      "Oliver J. Bear Don't Walk IV",
      "Jennifer L. Chen",
      "Harry Reyes Nieva",
      "Noémie Elhadad"
    ],
    "author_ids": [],
    "abstract": "Sex and gender-based healthcare disparities contribute to differences in\nhealth outcomes. We focus on time to diagnosis (TTD) by conducting two\nlarge-scale, complementary analyses among men and women across 29 phenotypes\nand 195K patients. We first find that women are consistently more likely to\nexperience a longer TTD than men, even when presenting with the same\nconditions. We further explore how TTD disparities affect diagnostic\nperformance between genders, both across and persistent to time, by evaluating\ngender-agnostic disease classifiers across increasing diagnostic information.\nIn both fairness analyses, the diagnostic process favors men over women,\ncontradicting the previous observation that women may demonstrate relevant\nsymptoms earlier than men. These analyses suggest that TTD is an important yet\ncomplex aspect when studying gender disparities, and warrants further\ninvestigation.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06100v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.06049v2",
    "title": "Colorado in Context: Congressional Redistricting and Competing Fairness Criteria in Colorado",
    "authors": [
      "Jeanne Clelland",
      "Haley Colgate",
      "Daryl DeFord",
      "Beth Malmskog",
      "Flavia Sancier-Barbosa"
    ],
    "author_ids": [],
    "abstract": "In this paper, we apply techniques of ensemble analysis to understand the\npolitical baseline for Congressional representation in Colorado. We generate a\nlarge random sample of reasonable redistricting plans and determine the\npartisan balance of each district using returns from state-wide elections in\n2018, and analyze the 2011/2012 enacted districts in this context. Colorado\nrecently adopted a new framework for redistricting, creating an independent\ncommission to draw district boundaries, prohibiting partisan bias and\nincumbency considerations, requiring that political boundaries (such as\ncounties) be preserved as much as possible, and also requiring that mapmakers\nmaximize the number of competitive districts. We investigate the relationships\nbetween partisan outcomes, number of counties which are split, and number of\ncompetitive districts in a plan. This paper also features two novel\nimprovements in methodology--a more rigorous statistical framework for\nunderstanding necessary sample size, and a weighted-graph method for generating\nrandom plans which split approximately as few counties as acceptable\nhuman-drawn maps.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "physics.soc-ph",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.06049v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.05950v2",
    "title": "Joint Management of Compute and Radio Resources in Mobile Edge Computing: a Market Equilibrium Approach",
    "authors": [
      "Eugenio Moro",
      "Ilario Filippini"
    ],
    "author_ids": [],
    "abstract": "Edge computing has been recently introduced as a way to bring computational\ncapabilities closer to end users of modern network-based services, in order to\nsupport existent and future delay-sensitive applications by effectively\naddressing the high propagation delay issue that affects cloud computing.\nHowever, the problem of efficiently and fairly manage the system resources\npresents particular challenges due to the limited capacity of both edge nodes\nand wireless access networks, as well as the heterogeneity of resources and\nservices' requirements. To this end, we propose a techno-economic market where\nservice providers act as buyers, securing both radio and computing resources\nfor the execution of their associated end users' jobs, while being constrained\nby a budget limit. We design an allocation mechanism that employs convex\nprogramming in order to find the unique market equilibrium point that maximizes\nfairness, while making sure that all buyers receive their preferred resource\nbundle. Additionally, we derive theoretical properties that confirm how the\nmarket equilibrium approach strikes a balance between fairness and efficiency.\nWe also propose alternative allocation mechanisms and give a comparison with\nthe market-based mechanism. Finally, we conduct simulations in order to\nnumerically analyze and compare the performance of the mechanisms and confirm\nthe theoretical properties of the market model.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05950v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.05949v4",
    "title": "On contraction coefficients, partial orders and approximation of capacities for quantum channels",
    "authors": [
      "Christoph Hirche",
      "Cambyse Rouzé",
      "Daniel Stilck França"
    ],
    "author_ids": [],
    "abstract": "The data processing inequality is the most basic requirement for any\nmeaningful measure of information. It essentially states that\ndistinguishability measures between states decrease if we apply a quantum\nchannel and is the centerpiece of many results in information theory. Moreover,\nit justifies the operational interpretation of most entropic quantities. In\nthis work, we revisit the notion of contraction coefficients of quantum\nchannels, which provide sharper and specialized versions of the data processing\ninequality. A concept closely related to data processing is partial orders on\nquantum channels. First, we discuss several quantum extensions of the\nwell-known less noisy ordering and relate them to contraction coefficients. We\nfurther define approximate versions of the partial orders and show how they can\ngive strengthened and conceptually simple proofs of several results on\napproximating capacities. Moreover, we investigate the relation to other\npartial orders in the literature and their properties, particularly with regard\nto tensorization. We then examine the relation between contraction coefficients\nwith other properties of quantum channels such as hypercontractivity. Next, we\nextend the framework of contraction coefficients to general f-divergences and\nprove several structural results. Finally, we consider two important classes of\nquantum channels, namely Weyl-covariant and bosonic Gaussian channels. For\nthose, we determine new contraction coefficients and relations for various\npartial orders.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05949v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.05911v1",
    "title": "Situated Data, Situated Systems: A Methodology to Engage with Power Relations in Natural Language Processing Research",
    "authors": [
      "Lucy Havens",
      "Melissa Terras",
      "Benjamin Bach",
      "Beatrice Alex"
    ],
    "author_ids": [],
    "abstract": "We propose a bias-aware methodology to engage with power relations in natural\nlanguage processing (NLP) research. NLP research rarely engages with bias in\nsocial contexts, limiting its ability to mitigate bias. While researchers have\nrecommended actions, technical methods, and documentation practices, no\nmethodology exists to integrate critical reflections on bias with technical NLP\nmethods. In this paper, after an extensive and interdisciplinary literature\nreview, we contribute a bias-aware methodology for NLP research. We also\ncontribute a definition of biased text, a discussion of the implications of\nbiased NLP systems, and a case study demonstrating how we are executing the\nbias-aware methodology in research on archival metadata descriptions.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05911v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.09865v3",
    "title": "An experiment on the mechanisms of racial bias in ML-based credit scoring in Brazil",
    "authors": [
      "Ramon Vilarino",
      "Renato Vicente"
    ],
    "author_ids": [],
    "abstract": "We dissect an experimental credit scoring model developed with real data and\ndemonstrate - without access to protected attributes - how the use of location\ninformation introduces racial bias. We analyze the tree gradient boosting model\nwith the aid of a game-theoretic inspired machine learning explainability\ntechnique, counterfactual experiments and Brazilian census data. By exposing\nalgorithmic racial bias explaining the trained machine learning model inner\nmechanisms, this experiment comprises an interesting artifact to aid the\nendeavor of theoretical understanding of the emergence of racial bias in\nmachine learning systems. Without access to individuals' racial categories, we\nshow how classification parity measures using geographically defined groups\ncould carry information about model racial bias. The experiment testifies to\nthe need for methods and language that do not presuppose access to protected\nattributes when auditing ML models, the importance of considering regional\nspecifics when addressing racial issues, and the central role of census data in\nthe AI research community. To the best of our knowledge, this is the first\ndocumented case of algorithmic racial bias in ML-based credit scoring in\nBrazil, the country with the second largest Black population in the world.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09865v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.05502v1",
    "title": "An Instance-Based Algorithm for Deciding the Bias of a Coin",
    "authors": [
      "Luís Fernando Schultz Xavier da Silveira",
      "Michiel Smid"
    ],
    "author_ids": [],
    "abstract": "Let $q \\in (0,1)$ and $\\delta \\in (0,1)$ be real numbers, and let $C$ be a\ncoin that comes up heads with an unknown probability $p$, such that $p \\neq q$.\nWe present an algorithm that, on input $C$, $q$, and $\\delta$, decides, with\nprobability at least $1-\\delta$, whether $p<q$ or $p>q$. The expected number of\ncoin flips made by this algorithm is $O \\left( \\frac{\\log\\log(1/\\varepsilon) +\n\\log(1/\\delta)}{\\varepsilon^2} \\right)$, where $\\varepsilon = |p-q|$.",
    "published_date": "2020-11-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05502v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.05393v1",
    "title": "Polarization Model of Online Social Networks Based on the Concept of Spontaneous Symmetry Breaking",
    "authors": [
      "Masaki Aida",
      "Ayako Hashizume",
      "Chisa Takano",
      "Masayuki Murata"
    ],
    "author_ids": [],
    "abstract": "The spread of information networks has not only made it easier for people to\naccess a variety of information sources but also greatly enhanced the ability\nof individuals to disseminate information. Unfortunately, however, the problem\nof slander in online social networks shows that the evolving information\nnetwork environment does not necessarily support mutual understanding in\nsociety. Since information with particular bias is distributed only to those\ncommunities that prefer it, the division of society into various opposing\ngroups is strengthened. This phenomenon is called polarization. It is necessary\nto understand the mechanism of polarization to establish technologies that can\ncounter polarization. This paper introduces a fundamental model for\nunderstanding polarization that is based on the concept of spontaneous symmetry\nbreaking; our starting point is the oscillation model that describes user\ndynamics in online social networks.",
    "published_date": "2020-11-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph",
      "quant-ph",
      "94-06"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05393v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.05287v1",
    "title": "Two-Sided Fairness in Non-Personalised Recommendations",
    "authors": [
      "Aadi Swadipto Mondal",
      "Rakesh Bal",
      "Sayan Sinha",
      "Gourab K Patro"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are one of the most widely used services on several\nonline platforms to suggest potential items to the end-users. These services\noften use different machine learning techniques for which fairness is a\nconcerning factor, especially when the downstream services have the ability to\ncause social ramifications. Thus, focusing on the non-personalised (global)\nrecommendations in news media platforms (e.g., top-k trending topics on\nTwitter, top-k news on a news platform, etc.), we discuss on two specific\nfairness concerns together (traditionally studied separately)---user fairness\nand organisational fairness. While user fairness captures the idea of\nrepresenting the choices of all the individual users in the case of global\nrecommendations, organisational fairness tries to ensure\npolitically/ideologically balanced recommendation sets. This makes user\nfairness a user-side requirement and organisational fairness a platform-side\nrequirement. For user fairness, we test with methods from social choice theory,\ni.e., various voting rules known to better represent user choices in their\nresults. Even in our application of voting rules to the recommendation setup,\nwe observe high user satisfaction scores. Now for organisational fairness, we\npropose a bias metric which measures the aggregate ideological bias of a\nrecommended set of items (articles). Analysing the results obtained from voting\nrule-based recommendation, we find that while the well-known voting rules are\nbetter from the user side, they show high bias values and clearly not suitable\nfor organisational requirements of the platforms. Thus, there is a need to\nbuild an encompassing mechanism by cohesively bridging ideas of user fairness\nand organisational fairness. In this abstract paper, we intend to frame the\nelementary ideas along with the clear motivation behind the requirement of such\na mechanism.",
    "published_date": "2020-11-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05287v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.05127v1",
    "title": "A Soft Computing Approach for Selecting and Combining Spectral Bands",
    "authors": [
      "Juan F. H. Albarracín",
      "Rafael S. Oliveira",
      "Marina Hirota",
      "Jefersson A. dos Santos",
      "Ricardo da S. Torres"
    ],
    "author_ids": [],
    "abstract": "We introduce a soft computing approach for automatically selecting and\ncombining indices from remote sensing multispectral images that can be used for\nclassification tasks. The proposed approach is based on a Genetic-Programming\n(GP) framework, a technique successfully used in a wide variety of optimization\nproblems. Through GP, it is possible to learn indices that maximize the\nseparability of samples from two different classes. Once the indices\nspecialized for all the pairs of classes are obtained, they are used in\npixelwise classification tasks. We used the GP-based solution to evaluate\ncomplex classification problems, such as those that are related to the\ndiscrimination of vegetation types within and between tropical biomes. Using\ntime series defined in terms of the learned spectral indices, we show that the\nGP framework leads to superior results than other indices that are used to\ndiscriminate and classify tropical biomes.",
    "published_date": "2020-11-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05127v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.05002v3",
    "title": "Input Bias in Rectified Gradients and Modified Saliency Maps",
    "authors": [
      "Lennart Brocki",
      "Neo Christopher Chung"
    ],
    "author_ids": [],
    "abstract": "Interpretation and improvement of deep neural networks relies on better\nunderstanding of their underlying mechanisms. In particular, gradients of\nclasses or concepts with respect to the input features (e.g., pixels in images)\nare often used as importance scores or estimators, which are visualized in\nsaliency maps. Thus, a family of saliency methods provide an intuitive way to\nidentify input features with substantial influences on classifications or\nlatent concepts. Several modifications to conventional saliency maps, such as\nRectified Gradients and Layer-wise Relevance Propagation (LRP), have been\nintroduced to allegedly denoise and improve interpretability. While visually\ncoherent in certain cases, Rectified Gradients and other modified saliency maps\nintroduce a strong input bias (e.g., brightness in the RGB space) because of\ninappropriate uses of the input features. We demonstrate that dark areas of an\ninput image are not highlighted by a saliency map using Rectified Gradients,\neven if it is relevant for the class or concept. Even in the scaled images, the\ninput bias exists around an artificial point in color spectrum. Our\nmodification, which simply eliminates multiplication with input features,\nremoves this bias. This showcases how a visual criteria may not align with true\nexplainability of deep learning models.",
    "published_date": "2020-11-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.05002v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.04857v1",
    "title": "Competitive Influence Propagation and Fake News Mitigation in the Presence of Strong User Bias",
    "authors": [
      "Akrati Saxena",
      "Harsh Saxena",
      "Ralucca Gera"
    ],
    "author_ids": [],
    "abstract": "Due to the extensive role of social networks in social media, it is easy for\npeople to share the news, and it spreads faster than ever before. These\nplatforms also have been exploited to share the rumor or fake information,\nwhich is a threat to society. One method to reduce the impact of fake\ninformation is making people aware of the correct information based on hard\nproof. In this work, first, we propose a propagation model called Competitive\nIndependent Cascade Model with users' Bias (CICMB) that considers the presence\nof strong user bias towards different opinions, believes, or political parties.\nWe further propose a method, called $k-TruthScore$, to identify an optimal set\nof truth campaigners from a given set of prospective truth campaigners to\nminimize the influence of rumor spreaders on the network. We compare\n$k-TruthScore$ with state of the art methods, and we measure their performances\nas the percentage of the saved nodes (nodes that would have believed in the\nfake news in the absence of the truth campaigners). We present these results on\na few real-world networks, and the results show that $k-TruthScore$ method\noutperforms baseline methods.",
    "published_date": "2020-11-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04857v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.04801v1",
    "title": "Fairness-Oriented User Association in HetNets Using Bargaining Game Theory",
    "authors": [
      "Ehsan Sadeghi",
      "Hamid Behroozi",
      "Stefano Rini"
    ],
    "author_ids": [],
    "abstract": "In this paper, the user association and resource allocation problem is\ninvestigated for a two-tier HetNet consisting of one macro Base Station (BS)\nand a number of pico BSs. The effectiveness of user association to BSs is\nevaluated in terms of fairness and load distribution. In particular, the\nproblem of determining a fair user association is formulated as a bargaining\ngame so that for the Nash Bargaining Solution (NBS) abiding the fairness axioms\nprovides an optimal and fair user association. The NBS also yields in a Pareto\noptimal solution and leads to a proportional fair solution in the proposed\nHetNet model. Additionally, we introduce a novel algorithmic solution in which\na new Coalition Generation Algorithm (CGA), called SINR-based CGA, is\nconsidered in order to simplify the coalition generation phase. Our simulation\nresults show the efficiency of the proposed user association scheme in terms of\nfairness and load distribution among BSs and users. In particular, we compare\nthe performance of the proposed solution with that of the throughput-oriented\nscheme in terms of the max-sum-rate scheme and show that the proposed solution\nyields comparable average data rates and overall sum rate.",
    "published_date": "2020-11-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.GT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04801v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.04760v1",
    "title": "Diamond Message Set Groupcasting: From an Inner Bound for the DM Broadcast Channel to the Capacity Region of the Combination Network",
    "authors": [
      "Mohamed Salman",
      "Mahesh K. Varanasi"
    ],
    "author_ids": [],
    "abstract": "Multiple groupcasting over the broadcast channel (BC) is studied. In\nparticular, an inner bound is obtained for the $K$-receiver discrete memoryless\n(DM) BC for the diamond message set which consists of four groupcast messages:\none desired by all receivers, one by all but two receivers, and two more\ndesired by all but each one of those two receivers. The inner bound is based on\nrate-splitting and superposition coding and is given in explicit form herein as\na union over coding distributions of four-dimensional polytopes. When\nspecialized to the so-called combination network, which is a class of\nthree-layer (two-hop) broadcast networks parameterized by $2^K-1$\nfinite-and-arbitrary-capacity noiseless links from the source node in the first\nlayer to as many nodes of the second layer, our top-down approach from the DM\nBC to the combination network yields an explicit inner bound as a single\npolytope via the identification of a single coding distribution. This inner\nbound consists of inequalities which are then identified to be within the class\nof a plethora of (indeed, infinitely many) generalized cut-set outer bounds\nrecently obtained by Salimi et al for broadcast networks. We hence establish\nthe capacity region of the general $K$-user combination network for the diamond\nmessage set, and do so in explicit form. Such a result implies a certain\nstrength of our inner bound for the DM BC in that it (a) produces a hitherto\nunknown capacity region when specialized to the combination network and (b) may\ncapture many combinatorial aspects of the capacity region of the $K$-receiver\nDM BC itself (for the diamond message set). Moreover, we further extend that\ninner bound by adding binning to it and providing that inner bound also in\nexplicit form as a union over coding distributions of four-dimensional\npolytopes in the message rates.",
    "published_date": "2020-11-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04760v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.04645v2",
    "title": "On the error exponents of binary state discrimination with composite hypotheses",
    "authors": [
      "Milán Mosonyi",
      "Zsombor Szilágyi",
      "Mihály Weiner"
    ],
    "author_ids": [],
    "abstract": "The trade-off between the two types of errors in binary state discrimination\nmay be quantified in the asymptotics by various error exponents. In the case of\nsimple i.i.d. hypotheses, each of these exponents is equal to a divergence\n(pseudo-distance) of the two states. In the case of composite hypotheses,\nrepresented by sets of states $R,S$, one always has the inequality\n$\\mathrm{e}(R\\|S)\\le \\mathrm{E}(R\\|S)$, where $\\mathrm{e}$ is the exponent,\n$\\mathrm{E}$ is the corresponding divergence, and the question is whether\nequality holds. The relation between the composite exponents and the worst\npairwise exponents may be influenced by a number of factors: the type of\nexponents considered; whether the problem is classical or quantum; the\ncardinality and the geometric properties of the sets representing the\nhypotheses; and, on top of the above, possibly whether the underlying Hilbert\nspace is finite- or infinite-dimensional.\n  Our main contribution in this paper is clarifying this landscape\nconsiderably: We exhibit explicit examples for hitherto unstudied cases where\nthe above inequality fails to hold with equality, while we also prove equality\nfor various general classes of state discrimination problems. In particular, we\nshow that equality may fail for any of the error exponents even in the\nclassical case, if the system is allowed to be infinite-dimensional, and the\nalternative hypothesis contains countably infinitely many states. Moreover, we\nshow that in the quantum case strict inequality is the generic behavior in the\nsense that, starting from any pair of non-commuting density operators of any\ndimension, and for any of the exponents, it is possible to construct an example\nwith a simple null-hypothesis and an alternative hypothesis consisting of only\ntwo states, such that strict inequality holds for the given exponent.",
    "published_date": "2020-11-09T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04645v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.04315v2",
    "title": "Coupled regularized sample covariance matrix estimator for multiple classes",
    "authors": [
      "Elias Raninen",
      "Esa Ollila"
    ],
    "author_ids": [],
    "abstract": "The estimation of covariance matrices of multiple classes with limited\ntraining data is a difficult problem. The sample covariance matrix (SCM) is\nknown to perform poorly when the number of variables is large compared to the\navailable number of samples. In order to reduce the mean squared error (MSE) of\nthe SCM, regularized (shrinkage) SCM estimators are often used. In this work,\nwe consider regularized SCM (RSCM) estimators for multiclass problems that\ncouple together two different target matrices for regularization: the pooled\n(average) SCM of the classes and the scaled identity matrix. Regularization\ntoward the pooled SCM is beneficial when the population covariances are\nsimilar, whereas regularization toward the identity matrix guarantees that the\nestimators are positive definite. We derive the MSE optimal tuning parameters\nfor the estimators as well as propose a method for their estimation under the\nassumption that the class populations follow (unspecified) elliptical\ndistributions with finite fourth-order moments. The MSE performance of the\nproposed coupled RSCMs are evaluated with simulations and in a regularized\ndiscriminant analysis (RDA) classification set-up on real data. The results\nbased on three different real data sets indicate comparable performance to\ncross-validation but with a significant speed-up in computation time.",
    "published_date": "2020-11-09T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04315v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.04219v2",
    "title": "Mitigating Bias in Set Selection with Noisy Protected Attributes",
    "authors": [
      "Anay Mehrotra",
      "L. Elisa Celis"
    ],
    "author_ids": [],
    "abstract": "Subset selection algorithms are ubiquitous in AI-driven applications,\nincluding, online recruiting portals and image search engines, so it is\nimperative that these tools are not discriminatory on the basis of protected\nattributes such as gender or race. Currently, fair subset selection algorithms\nassume that the protected attributes are known as part of the dataset. However,\nprotected attributes may be noisy due to errors during data collection or if\nthey are imputed (as is often the case in real-world settings). While a wide\nbody of work addresses the effect of noise on the performance of machine\nlearning algorithms, its effect on fairness remains largely unexamined. We find\nthat in the presence of noisy protected attributes, in attempting to increase\nfairness without considering noise, one can, in fact, decrease the fairness of\nthe result!\n  Towards addressing this, we consider an existing noise model in which there\nis probabilistic information about the protected attributes (e.g., [58, 34, 20,\n46]), and ask is fair selection possible under noisy conditions? We formulate a\n``denoised'' selection problem which functions for a large class of fairness\nmetrics; given the desired fairness goal, the solution to the denoised problem\nviolates the goal by at most a small multiplicative amount with high\nprobability. Although this denoised problem turns out to be NP-hard, we give a\nlinear-programming based approximation algorithm for it. We evaluate this\napproach on both synthetic and real-world datasets. Our empirical results show\nthat this approach can produce subsets which significantly improve the fairness\nmetrics despite the presence of noisy protected attributes, and, compared to\nprior noise-oblivious approaches, has better Pareto-tradeoffs between utility\nand fairness.",
    "published_date": "2020-11-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.DS",
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04219v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.04049v1",
    "title": "FairLens: Auditing Black-box Clinical Decision Support Systems",
    "authors": [
      "Cecilia Panigutti",
      "Alan Perotti",
      "Andrè Panisson",
      "Paolo Bajardi",
      "Dino Pedreschi"
    ],
    "author_ids": [],
    "abstract": "The pervasive application of algorithmic decision-making is raising concerns\non the risk of unintended bias in AI systems deployed in critical settings such\nas healthcare. The detection and mitigation of biased models is a very delicate\ntask which should be tackled with care and involving domain experts in the\nloop. In this paper we introduce FairLens, a methodology for discovering and\nexplaining biases. We show how our tool can be used to audit a fictional\ncommercial black-box model acting as a clinical decision support system. In\nthis scenario, the healthcare facility experts can use FairLens on their own\nhistorical data to discover the model's biases before incorporating it into the\nclinical decision flow. FairLens first stratifies the available patient data\naccording to attributes such as age, ethnicity, gender and insurance; it then\nassesses the model performance on such subgroups of patients identifying those\nin need of expert evaluation. Finally, building on recent state-of-the-art XAI\n(eXplainable Artificial Intelligence) techniques, FairLens explains which\nelements in patients' clinical history drive the model error in the selected\nsubgroup. Therefore, FairLens allows experts to investigate whether to trust\nthe model and to spotlight group-specific biases that might constitute\npotential fairness issues.",
    "published_date": "2020-11-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "68U99, 68T99",
      "J.3; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04049v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03856v1",
    "title": "Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles",
    "authors": [
      "Christopher Clark",
      "Mark Yatskar",
      "Luke Zettlemoyer"
    ],
    "author_ids": [],
    "abstract": "Many datasets have been shown to contain incidental correlations created by\nidiosyncrasies in the data collection process. For example, sentence entailment\ndatasets can have spurious word-class correlations if nearly all contradiction\nsentences contain the word \"not\", and image recognition datasets can have\ntell-tale object-background correlations if dogs are always indoors. In this\npaper, we propose a method that can automatically detect and ignore these kinds\nof dataset-specific patterns, which we call dataset biases. Our method trains a\nlower capacity model in an ensemble with a higher capacity model. During\ntraining, the lower capacity model learns to capture relatively shallow\ncorrelations, which we hypothesize are likely to reflect dataset bias. This\nfrees the higher capacity model to focus on patterns that should generalize\nbetter. We ensure the models learn non-overlapping approaches by introducing a\nnovel method to make them conditionally independent. Importantly, our approach\ndoes not require the bias to be known in advance. We evaluate performance on\nsynthetic datasets, and four datasets built to penalize models that exploit\nknown biases on textual entailment, visual question answering, and image\nrecognition tasks. We show improvement in all settings, including a 10 point\ngain on the visual question answering dataset.",
    "published_date": "2020-11-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03856v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03731v4",
    "title": "On the Privacy Risks of Algorithmic Fairness",
    "authors": [
      "Hongyan Chang",
      "Reza Shokri"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness and privacy are essential pillars of trustworthy machine\nlearning. Fair machine learning aims at minimizing discrimination against\nprotected groups by, for example, imposing a constraint on models to equalize\ntheir behavior across different groups. This can subsequently change the\ninfluence of training data points on the fair model, in a disproportionate way.\nWe study how this can change the information leakage of the model about its\ntraining data. We analyze the privacy risks of group fairness (e.g., equalized\nodds) through the lens of membership inference attacks: inferring whether a\ndata point is used for training a model. We show that fairness comes at the\ncost of privacy, and this cost is not distributed equally: the information\nleakage of fair models increases significantly on the unprivileged subgroups,\nwhich are the ones for whom we need fair learning. We show that the more biased\nthe training data is, the higher the privacy cost of achieving fairness for the\nunprivileged subgroups will be. We provide comprehensive empirical analysis for\ngeneral machine learning algorithms.",
    "published_date": "2020-11-07T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03731v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03698v2",
    "title": "Exploiting User Mobility for WiFi RTT Positioning: A Geometric Approach",
    "authors": [
      "Kyuwon Han",
      "Seung Min Yu",
      "Seong-Lyun Kim",
      "Seung-Woo Ko"
    ],
    "author_ids": [],
    "abstract": "Recently, round-trip time (RTT) measured by a fine-timing measurement\nprotocol has received great attention in the area of WiFi positioning. It\nprovides an acceptable ranging accuracy in favorable environments when a\nline-of-sight (LOS) path exists. Otherwise, a signal is detoured along with\nnon-LOS paths, making the resultant ranging results different from the\nground-truth, called an RTT bias, which is the main reason for poor positioning\nperformance. To address it, we aim at leveraging the user mobility trajectory\ndetected by a smartphone's inertial measurement units, called pedestrian dead\nreckoning (PDR). Specifically, PDR provides the geographic relation among\nadjacent locations, guiding the resultant positioning estimates' sequence not\nto deviate from the user trajectory. To this end, we describe their relations\nas multiple geometric equations, enabling us to render a novel positioning\nalgorithm with acceptable accuracy. Depending on the mobility pattern being\nlinear or arbitrary, we develop different algorithms divided into two phases.\nFirst, we can jointly estimate an RTT bias of each AP and the user's step\nlength by leveraging the geometric relation mentioned above. It enables us to\nconstruct a user's relative trajectory defined on the concerned AP's local\ncoordinate system. Second, we align every AP's relative trajectory into a\nsingle one, called trajectory alignment, equivalent to transformation to the\nglobal coordinate system. As a result, we can estimate the sequence of the\nuser's absolute locations from the aligned trajectory. Various field\nexperiments extensively verify the proposed algorithm's effectiveness that the\naverage positioning error is approximately 0.369 (m) and 1.705 (m) in LOS and\nNLOS environments, respectively.",
    "published_date": "2020-11-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03698v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.03654v4",
    "title": "Fair Machine Learning Under Partial Compliance",
    "authors": [
      "Jessica Dai",
      "Sina Fazelpour",
      "Zachary C. Lipton"
    ],
    "author_ids": [],
    "abstract": "Typically, fair machine learning research focuses on a single decisionmaker\nand assumes that the underlying population is stationary. However, many of the\ncritical domains motivating this work are characterized by competitive\nmarketplaces with many decisionmakers. Realistically, we might expect only a\nsubset of them to adopt any non-compulsory fairness-conscious policy, a\nsituation that political philosophers call partial compliance. This possibility\nraises important questions: how does the strategic behavior of decision\nsubjects in partial compliance settings affect the allocation outcomes? If k%\nof employers were to voluntarily adopt a fairness-promoting intervention,\nshould we expect k% progress (in aggregate) towards the benefits of universal\nadoption, or will the dynamics of partial compliance wash out the hoped-for\nbenefits? How might adopting a global (versus local) perspective impact the\nconclusions of an auditor? In this paper, we propose a simple model of an\nemployment market, leveraging simulation as a tool to explore the impact of\nboth interaction effects and incentive effects on outcomes and auditing\nmetrics. Our key findings are that at equilibrium: (1) partial compliance (k%\nof employers) can result in far less than proportional (k%) progress towards\nthe full compliance outcomes; (2) the gap is more severe when fair employers\nmatch global (vs local) statistics; (3) choices of local vs global statistics\ncan paint dramatically different pictures of the performance vis-a-vis fairness\ndesiderata of compliant versus non-compliant employers; and (4) partial\ncompliance to local parity measures can induce extreme segregation.",
    "published_date": "2020-11-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03654v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03492v2",
    "title": "Practical and Ethical Considerations in the Effective use of Emotion and Sentiment Lexicons",
    "authors": [
      "Saif M. Mohammad"
    ],
    "author_ids": [],
    "abstract": "Lexicons of word-emotion associations are widely used in research and\nreal-world applications. As part of my research, I have created several such\nlexicons (e.g., the NRC Emotion Lexicon). This paper outlines some practical\nand ethical considerations involved in the effective use of these lexical\nresources.",
    "published_date": "2020-11-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03492v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03195v1",
    "title": "Explainable AI meets Healthcare: A Study on Heart Disease Dataset",
    "authors": [
      "Devam Dave",
      "Het Naik",
      "Smiti Singhal",
      "Pankesh Patel"
    ],
    "author_ids": [],
    "abstract": "With the increasing availability of structured and unstructured data and the\nswift progress of analytical techniques, Artificial Intelligence (AI) is\nbringing a revolution to the healthcare industry. With the increasingly\nindispensable role of AI in healthcare, there are growing concerns over the\nlack of transparency and explainability in addition to potential bias\nencountered by predictions of the model. This is where Explainable Artificial\nIntelligence (XAI) comes into the picture. XAI increases the trust placed in an\nAI system by medical practitioners as well as AI researchers, and thus,\neventually, leads to an increasingly widespread deployment of AI in healthcare.\n  In this paper, we present different interpretability techniques. The aim is\nto enlighten practitioners on the understandability and interpretability of\nexplainable AI systems using a variety of techniques available which can be\nvery advantageous in the health-care domain. Medical diagnosis model is\nresponsible for human life and we need to be confident enough to treat a\npatient as instructed by a black-box model. Our paper contains examples based\non the heart disease dataset and elucidates on how the explainability\ntechniques should be preferred to create trustworthiness while using AI systems\nin healthcare.",
    "published_date": "2020-11-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03195v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03176v2",
    "title": "On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method",
    "authors": [
      "Ye He",
      "Krishnakumar Balasubramanian",
      "Murat A. Erdogdu"
    ],
    "author_ids": [],
    "abstract": "The randomized midpoint method, proposed by [SL19], has emerged as an optimal\ndiscretization procedure for simulating the continuous time Langevin\ndiffusions. Focusing on the case of strong-convex and smooth potentials, in\nthis paper, we analyze several probabilistic properties of the randomized\nmidpoint discretization method for both overdamped and underdamped Langevin\ndiffusions. We first characterize the stationary distribution of the discrete\nchain obtained with constant step-size discretization and show that it is\nbiased away from the target distribution. Notably, the step-size needs to go to\nzero to obtain asymptotic unbiasedness. Next, we establish the asymptotic\nnormality for numerical integration using the randomized midpoint method and\nhighlight the relative advantages and disadvantages over other discretizations.\nOur results collectively provide several insights into the behavior of the\nrandomized midpoint discretization method, including obtaining confidence\nintervals for numerical integrations.",
    "published_date": "2020-11-06T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.CO",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03176v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03173v2",
    "title": "Does enforcing fairness mitigate biases caused by subpopulation shift?",
    "authors": [
      "Subha Maity",
      "Debarghya Mukherjee",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "Many instances of algorithmic bias are caused by subpopulation shifts. For\nexample, ML models often perform worse on demographic groups that are\nunderrepresented in the training data. In this paper, we study whether\nenforcing algorithmic fairness during training improves the performance of the\ntrained model in the \\emph{target domain}. On one hand, we conceive scenarios\nin which enforcing fairness does not improve performance in the target domain.\nIn fact, it may even harm performance. On the other hand, we derive necessary\nand sufficient conditions under which enforcing algorithmic fairness leads to\nthe Bayes model in the target domain. We also illustrate the practical\nimplications of our theoretical results in simulations and on real data.",
    "published_date": "2020-11-06T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03173v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03156v5",
    "title": "Wasserstein-based fairness interpretability framework for machine learning models",
    "authors": [
      "Alexey Miroshnikov",
      "Konstandinos Kotsiopoulos",
      "Ryan Franks",
      "Arjun Ravi Kannan"
    ],
    "author_ids": [],
    "abstract": "The objective of this article is to introduce a fairness interpretability\nframework for measuring and explaining the bias in classification and\nregression models at the level of a distribution. In our work, we measure the\nmodel bias across sub-population distributions in the model output using the\nWasserstein metric. To properly quantify the contributions of predictors, we\ntake into account the favorability of both the model and predictors with\nrespect to the non-protected class. The quantification is accomplished by the\nuse of transport theory, which gives rise to the decomposition of the model\nbias and bias explanations to positive and negative contributions. To gain more\ninsight into the role of favorability and allow for additivity of bias\nexplanations, we adapt techniques from cooperative game theory.",
    "published_date": "2020-11-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.PR",
      "49Q22, 91A12, 68T01, 90C08"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03156v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03155v2",
    "title": "Parametric Flatten-T Swish: An Adaptive Non-linear Activation Function For Deep Learning",
    "authors": [
      "Hock Hung Chieng",
      "Noorhaniza Wahid",
      "Pauline Ong"
    ],
    "author_ids": [],
    "abstract": "Activation function is a key component in deep learning that performs\nnon-linear mappings between the inputs and outputs. Rectified Linear Unit\n(ReLU) has been the most popular activation function across the deep learning\ncommunity. However, ReLU contains several shortcomings that can result in\ninefficient training of the deep neural networks, these are: 1) the negative\ncancellation property of ReLU tends to treat negative inputs as unimportant\ninformation for the learning, resulting in a performance degradation; 2) the\ninherent predefined nature of ReLU is unlikely to promote additional\nflexibility, expressivity, and robustness to the networks; 3) the mean\nactivation of ReLU is highly positive and leads to bias shift effect in network\nlayers; and 4) the multilinear structure of ReLU restricts the non-linear\napproximation power of the networks. To tackle these shortcomings, this paper\nintroduced Parametric Flatten-T Swish (PFTS) as an alternative to ReLU. By\ntaking ReLU as a baseline method, the experiments showed that PFTS improved\nclassification accuracy on SVHN dataset by 0.31%, 0.98%, 2.16%, 17.72%, 1.35%,\n0.97%, 39.99%, and 71.83% on DNN-3A, DNN-3B, DNN-4, DNN- 5A, DNN-5B, DNN-5C,\nDNN-6, and DNN-7, respectively. Besides, PFTS also achieved the highest mean\nrank among the comparison methods. The proposed PFTS manifested higher\nnon-linear approximation power during training and thereby improved the\npredictive performance of the networks.",
    "published_date": "2020-11-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.NE",
      "68T07",
      "I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03155v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03110v2",
    "title": "Exploring End-to-End Multi-channel ASR with Bias Information for Meeting Transcription",
    "authors": [
      "Xiaofei Wang",
      "Naoyuki Kanda",
      "Yashesh Gaur",
      "Zhuo Chen",
      "Zhong Meng",
      "Takuya Yoshioka"
    ],
    "author_ids": [],
    "abstract": "Joint optimization of multi-channel front-end and automatic speech\nrecognition (ASR) has attracted much interest. While promising results have\nbeen reported for various tasks, past studies on its meeting transcription\napplication were limited to small scale experiments. It is still unclear\nwhether such a joint framework can be beneficial for a more practical setup\nwhere a massive amount of single channel training data can be leveraged for\nbuilding a strong ASR back-end. In this work, we present our investigation on\nthe joint modeling of a mask-based beamformer and\nAttention-Encoder-Decoder-based ASR in the setting where we have 75k hours of\nsingle-channel data and a relatively small amount of real multi-channel data\nfor model training. We explore effective training procedures, including a\ncomparison of simulated and real multi-channel training data. To guide the\nrecognition towards a target speaker and deal with overlapped speech, we also\nexplore various combinations of bias information, such as direction of arrivals\nand speaker profiles. We propose an effective location bias integration method\ncalled deep concatenation for the beamformer network. In our evaluation on\nvarious meeting recordings, we show that the proposed framework achieves a\nsubstantial word error rate reduction.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03110v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.03108v2",
    "title": "Minimax Group Fairness: Algorithms and Experiments",
    "authors": [
      "Emily Diana",
      "Wesley Gill",
      "Michael Kearns",
      "Krishnaram Kenthapadi",
      "Aaron Roth"
    ],
    "author_ids": [],
    "abstract": "We consider a recently introduced framework in which fairness is measured by\nworst-case outcomes across groups, rather than by the more standard differences\nbetween group outcomes. In this framework we provide provably convergent\noracle-efficient learning algorithms (or equivalently, reductions to non-fair\nlearning) for minimax group fairness. Here the goal is that of minimizing the\nmaximum loss across all groups, rather than equalizing group losses. Our\nalgorithms apply to both regression and classification settings and support\nboth overall error and false positive or false negative rates as the fairness\nmeasure of interest. They also support relaxations of the fairness constraints,\nthus permitting study of the tradeoff between overall accuracy and minimax\nfairness. We compare the experimental behavior and performance of our\nalgorithms across a variety of fairness-sensitive data sets and show empirical\ncases in which minimax fairness is strictly and strongly preferable to equal\noutcome notions.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03108v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03016v1",
    "title": "Ethics in the Software Development Process: From Codes of Conduct to Ethical Deliberation",
    "authors": [
      "Jan Gogoll",
      "Niina Zuber",
      "Severin Kacianka",
      "Timo Greger",
      "Alexander Pretschner",
      "Julian Nida-Rümelin"
    ],
    "author_ids": [],
    "abstract": "Software systems play an ever more important role in our lives and software\nengineers and their companies find themselves in a position where they are held\nresponsible for ethical issues that may arise. In this paper, we try to\ndisentangle ethical considerations that can be performed at the level of the\nsoftware engineer from those that belong in the wider domain of business\nethics. The handling of ethical problems that fall into the responsibility of\nthe engineer have traditionally been addressed by the publication of Codes of\nEthics and Conduct. We argue that these Codes are barely able to provide\nnormative orientation in software development. The main contribution of this\npaper is, thus, to analyze the normative features of Codes of Ethics in\nsoftware engineering and to explicate how their value-based approach might\nprevent their usefulness from a normative perspective. Codes of Conduct cannot\nreplace ethical deliberation because they do not and cannot offer guidance\nbecause of their underdetermined nature. This lack of orientation, we argue,\ntriggers reactive behavior such as \"cherry-picking\", \"risk of indifference\",\n\"ex-post orientation\" and the \"desire to rely on gut feeling\". In the light of\nthis, we propose to implement ethical deliberation within software development\nteams as a way out.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03016v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.02987v5",
    "title": "Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation",
    "authors": [
      "Georgios Kotsalis",
      "Guanghui Lan",
      "Tianjiao Li"
    ],
    "author_ids": [],
    "abstract": "In this paper we first present a novel operator extrapolation (OE) method for\nsolving deterministic variational inequality (VI) problems. Similar to the\ngradient (operator) projection method, OE updates one single search sequence by\nsolving a single projection subproblem in each iteration. We show that OE can\nachieve the optimal rate of convergence for solving a variety of VI problems in\na much simpler way than existing approaches. We then introduce the stochastic\noperator extrapolation (SOE) method and establish its optimal convergence\nbehavior for solving different stochastic VI problems. In particular, SOE\nachieves the optimal complexity for solving a fundamental problem, i.e.,\nstochastic smooth and strongly monotone VI, for the first time in the\nliterature. We also present a stochastic block operator extrapolations (SBOE)\nmethod to further reduce the iteration cost for the OE method applied to\nlarge-scale deterministic VIs with a certain block structure. Numerical\nexperiments have been conducted to demonstrate the potential advantages of the\nproposed algorithms. In fact, all these algorithms are applied to solve\ngeneralized monotone variational inequality (GMVI) problems whose operator is\nnot necessarily monotone. We will also discuss optimal OE-based policy\nevaluation methods for reinforcement learning in a companion paper.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "90C25, 90C15, 62L20, 68Q25"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02987v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02787v1",
    "title": "The State of AI Ethics Report (October 2020)",
    "authors": [
      "Abhishek Gupta",
      "Alexandrine Royer",
      "Victoria Heath",
      "Connor Wright",
      "Camylle Lanteigne",
      "Allison Cohen",
      "Marianna Bergamaschi Ganapini",
      "Muriam Fancy",
      "Erick Galinkin",
      "Ryan Khurana",
      "Mo Akif",
      "Renjie Butalid",
      "Falaah Arif Khan",
      "Masa Sweidan",
      "Audrey Balogh"
    ],
    "author_ids": [],
    "abstract": "The 2nd edition of the Montreal AI Ethics Institute's The State of AI Ethics\ncaptures the most relevant developments in the field of AI Ethics since July\n2020. This report aims to help anyone, from machine learning experts to human\nrights activists and policymakers, quickly digest and understand the\never-changing developments in the field. Through research and article\nsummaries, as well as expert commentary, this report distills the research and\nreporting surrounding various domains related to the ethics of AI, including:\nAI and society, bias and algorithmic justice, disinformation, humans and AI,\nlabor impacts, privacy, risk, and future of AI ethics.\n  In addition, The State of AI Ethics includes exclusive content written by\nworld-class AI Ethics experts from universities, research institutes,\nconsulting firms, and governments. These experts include: Danit Gal (Tech\nAdvisor, United Nations), Amba Kak (Director of Global Policy and Programs,\nNYU's AI Now Institute), Rumman Chowdhury (Global Lead for Responsible AI,\nAccenture), Brent Barron (Director of Strategic Projects and Knowledge\nManagement, CIFAR), Adam Murray (U.S. Diplomat working on tech policy, Chair of\nthe OECD Network on AI), Thomas Kochan (Professor, MIT Sloan School of\nManagement), and Katya Klinova (AI and Economy Program Lead, Partnership on\nAI).\n  This report should be used not only as a point of reference and insight on\nthe latest thinking in the field of AI Ethics, but should also be used as a\ntool for introspection as we aim to foster a more nuanced conversation\nregarding the impacts of AI on the world.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02787v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02675v2",
    "title": "Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for Perturbation Difficulty",
    "authors": [
      "Camilo Pestana",
      "Wei Liu",
      "David Glance",
      "Ajmal Mian"
    ],
    "author_ids": [],
    "abstract": "Dataset bias is a problem in adversarial machine learning, especially in the\nevaluation of defenses. An adversarial attack or defense algorithm may show\nbetter results on the reported dataset than can be replicated on other\ndatasets. Even when two algorithms are compared, their relative performance can\nvary depending on the dataset. Deep learning offers state-of-the-art solutions\nfor image recognition, but deep models are vulnerable even to small\nperturbations. Research in this area focuses primarily on adversarial attacks\nand defense algorithms. In this paper, we report for the first time, a class of\nrobust images that are both resilient to attacks and that recover better than\nrandom images under adversarial attacks using simple defense techniques. Thus,\na test dataset with a high proportion of robust images gives a misleading\nimpression about the performance of an adversarial attack or defense. We\npropose three metrics to determine the proportion of robust images in a dataset\nand provide scoring to determine the dataset bias. We also provide an\nImageNet-R dataset of 15000+ robust images to facilitate further research on\nthis intriguing phenomenon of image strength under attack. Our dataset,\ncombined with the proposed metrics, is valuable for unbiased benchmarking of\nadversarial attack and defense algorithms.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02675v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02661v1",
    "title": "Knowledge-Base Practicality for Cybersecurity Research Ethics Evaluation",
    "authors": [
      "Robert B. Ramirez",
      "Tomohiko Yano",
      "Masaki Shimaoka",
      "Kenichi Magata"
    ],
    "author_ids": [],
    "abstract": "Research ethics in Information and Communications Technology has seen a\nresurgence in popularity in recent years. Although a number of general ethics\nstandards have been issued, cyber security specifically has yet to see one.\nFurthermore, such standards are often abstract, lacking in guidance on specific\npractices. In this paper we compare peer-reviewed ethical analyses of condemned\nresearch papers to analyses derived from a knowledge base (KB) of concrete\ncyber security research ethics best practices. The KB we employ was compiled in\nprior work from a large random survey of research papers. We demonstrate\npreliminary evidence that such a KB can be used to yield comparable or more\nextensive ethical analyses of published cyber security research than expert\napplication of standards like the Menlo Report. We extend the ethical analyses\nof the reviewed manuscripts, and calculate measures of the efficiency with\nwhich the expert versus KB methods yield ethical insights.",
    "published_date": "2020-11-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "68M25",
      "K.7.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02661v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.02578v2",
    "title": "Learning and Evaluating Representations for Deep One-class Classification",
    "authors": [
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Jinsung Yoon",
      "Minho Jin",
      "Tomas Pfister"
    ],
    "author_ids": [],
    "abstract": "We present a two-stage framework for deep one-class classification. We first\nlearn self-supervised representations from one-class data, and then build\none-class classifiers on learned representations. The framework not only allows\nto learn better representations, but also permits building one-class\nclassifiers that are faithful to the target task. We argue that classifiers\ninspired by the statistical perspective in generative or discriminative models\nare more effective than existing approaches, such as a normality score from a\nsurrogate classifier. We thoroughly evaluate different self-supervised\nrepresentation learning algorithms under the proposed framework for one-class\nclassification. Moreover, we present a novel distribution-augmented contrastive\nlearning that extends training distributions via data augmentation to obstruct\nthe uniformity of contrastive representations. In experiments, we demonstrate\nstate-of-the-art performance on visual domain one-class classification\nbenchmarks, including novelty and anomaly detection. Finally, we present visual\nexplanations, confirming that the decision-making process of deep one-class\nclassifiers is intuitive to humans. The code is available at\nhttps://github.com/google-research/deep_representation_one_class.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02578v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.03321v1",
    "title": "Understanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition",
    "authors": [
      "Ben Adlam",
      "Jeffrey Pennington"
    ],
    "author_ids": [],
    "abstract": "Classical learning theory suggests that the optimal generalization\nperformance of a machine learning model should occur at an intermediate model\ncomplexity, with simpler models exhibiting high bias and more complex models\nexhibiting high variance of the predictive function. However, such a simple\ntrade-off does not adequately describe deep learning models that simultaneously\nattain low bias and variance in the heavily overparameterized regime. A primary\nobstacle in explaining this behavior is that deep learning algorithms typically\ninvolve multiple sources of randomness whose individual contributions are not\nvisible in the total variance. To enable fine-grained analysis, we describe an\ninterpretable, symmetric decomposition of the variance into terms associated\nwith the randomness from sampling, initialization, and the labels. Moreover, we\ncompute the high-dimensional asymptotic behavior of this decomposition for\nrandom feature kernel regression, and analyze the strikingly rich phenomenology\nthat arises. We find that the bias decreases monotonically with the network\nwidth, but the variance terms exhibit non-monotonic behavior and can diverge at\nthe interpolation boundary, even in the absence of label noise. The divergence\nis caused by the \\emph{interaction} between sampling and initialization and can\ntherefore be eliminated by marginalizing over samples (i.e. bagging) \\emph{or}\nover the initial parameters (i.e. ensemble learning).",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.03321v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02451v2",
    "title": "Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model",
    "authors": [
      "Zheheng Jiang",
      "Feixiang Zhou",
      "Aite Zhao",
      "Xin Li",
      "Ling Li",
      "Dacheng Tao",
      "Xuelong Li",
      "Huiyu Zhou"
    ],
    "author_ids": [],
    "abstract": "Home-cage social behaviour analysis of mice is an invaluable tool to assess\ntherapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts\nmade within the research community, single-camera video recordings are mainly\nused for such analysis. Because of the potential to create rich descriptions of\nmouse social behaviors, the use of multi-view video recordings for rodent\nobservations is increasingly receiving much attention. However, identifying\nsocial behaviours from various views is still challenging due to the lack of\ncorrespondence across data sources. To address this problem, we here propose a\nnovel multiview latent-attention and dynamic discriminative model that jointly\nlearns view-specific and view-shared sub-structures, where the former captures\nunique dynamics of each view whilst the latter encodes the interaction between\nthe views. Furthermore, a novel multi-view latent-attention variational\nautoencoder model is introduced in learning the acquired features, enabling us\nto learn discriminative features in each view. Experimental results on the\nstandard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB)\ndatasets demonstrate that our model outperforms the other state of the arts\ntechnologies and effectively deals with the imbalanced data problem.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02451v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02417v1",
    "title": "Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization",
    "authors": [
      "Tristan Thrush",
      "Ethan Wilcox",
      "Roger Levy"
    ],
    "author_ids": [],
    "abstract": "Previous studies investigating the syntactic abilities of deep learning\nmodels have not targeted the relationship between the strength of the\ngrammatical generalization and the amount of evidence to which the model is\nexposed during training. We address this issue by deploying a novel\nword-learning paradigm to test BERT's few-shot learning capabilities for two\naspects of English verbs: alternations and classes of selectional preferences.\nFor the former, we fine-tune BERT on a single frame in a verbal-alternation\npair and ask whether the model expects the novel verb to occur in its sister\nframe. For the latter, we fine-tune BERT on an incomplete selectional network\nof verbal objects and ask whether it expects unattested but plausible\nverb/object pairs. We find that BERT makes robust grammatical generalizations\nafter just one or two instances of a novel word in fine-tuning. For the verbal\nalternation tests, we find that the model displays behavior that is consistent\nwith a transitivity bias: verbs seen few times are expected to take direct\nobjects, but verbs seen with direct objects are not expected to occur\nintransitively.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02417v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02407v2",
    "title": "Debiasing classifiers: is reality at variance with expectation?",
    "authors": [
      "Ashrya Agrawal",
      "Florian Pfisterer",
      "Bernd Bischl",
      "Francois Buet-Golfouse",
      "Srijan Sood",
      "Jiahao Chen",
      "Sameena Shah",
      "Sebastian Vollmer"
    ],
    "author_ids": [],
    "abstract": "We present an empirical study of debiasing methods for classifiers, showing\nthat debiasers often fail in practice to generalize out-of-sample, and can in\nfact make fairness worse rather than better. A rigorous evaluation of the\ndebiasing treatment effect requires extensive cross-validation beyond what is\nusually done. We demonstrate that this phenomenon can be explained as a\nconsequence of bias-variance trade-off, with an increase in variance\nnecessitated by imposing a fairness constraint. Follow-up experiments validate\nthe theoretical prediction that the estimation variance depends strongly on the\nbase rates of the protected class. Considering fairness--performance trade-offs\njustifies the counterintuitive notion that partial debiasing can actually yield\nbetter results in practice on out-of-sample data.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "econ.EM",
      "68T01, 68Q32, 68T05",
      "G.4; I.2.0; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02407v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02395v2",
    "title": "Fairness in Biometrics: a figure of merit to assess biometric verification systems",
    "authors": [
      "Tiago de Freitas Pereira",
      "Sébastien Marcel"
    ],
    "author_ids": [],
    "abstract": "Machine learning-based (ML) systems are being largely deployed since the last\ndecade in a myriad of scenarios impacting several instances in our daily lives.\nWith this vast sort of applications, aspects of fairness start to rise in the\nspotlight due to the social impact that this can get in minorities. In this\nwork aspects of fairness in biometrics are addressed. First, we introduce the\nfirst figure of merit that is able to evaluate and compare fairness aspects\nbetween multiple biometric verification systems, the so-called Fairness\nDiscrepancy Rate (FDR). A use case with two synthetic biometric systems is\nintroduced and demonstrates the potential of this figure of merit in extreme\ncases of fair and unfair behavior. Second, a use case using face biometrics is\npresented where several systems are evaluated compared with this new figure of\nmerit using three public datasets exploring gender and race demographics.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02395v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02872v2",
    "title": "Transfer Meta-Learning: Information-Theoretic Bounds and Information Meta-Risk Minimization",
    "authors": [
      "Sharu Theresa Jose",
      "Osvaldo Simeone",
      "Giuseppe Durisi"
    ],
    "author_ids": [],
    "abstract": "Meta-learning automatically infers an inductive bias by observing data from a\nnumber of related tasks. The inductive bias is encoded by hyperparameters that\ndetermine aspects of the model class or training algorithm, such as\ninitialization or learning rate. Meta-learning assumes that the learning tasks\nbelong to a task environment, and that tasks are drawn from the same task\nenvironment both during meta-training and meta-testing. This, however, may not\nhold true in practice. In this paper, we introduce the problem of transfer\nmeta-learning, in which tasks are drawn from a target task environment during\nmeta-testing that may differ from the source task environment observed during\nmeta-training. Novel information-theoretic upper bounds are obtained on the\ntransfer meta-generalization gap, which measures the difference between the\nmeta-training loss, available at the meta-learner, and the average loss on\nmeta-test data from a new, randomly selected, task in the target task\nenvironment. The first bound, on the average transfer meta-generalization gap,\ncaptures the meta-environment shift between source and target task environments\nvia the KL divergence between source and target data distributions. The second,\nPAC-Bayesian bound, and the third, single-draw bound, account for this shift\nvia the log-likelihood ratio between source and target task distributions.\nFurthermore, two transfer meta-learning solutions are introduced. For the\nfirst, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the\naverage optimality gap. The second, referred to as Information Meta-Risk\nMinimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is\nshown via experiments to potentially outperform EMRM.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02872v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02241v1",
    "title": "The Forchheim Image Database for Camera Identification in the Wild",
    "authors": [
      "Benjamin Hadwiger",
      "Christian Riess"
    ],
    "author_ids": [],
    "abstract": "Image provenance can represent crucial knowledge in criminal investigation\nand journalistic fact checking. In the last two decades, numerous algorithms\nhave been proposed for obtaining information on the source camera and\ndistribution history of an image. For a fair ranking of these techniques, it is\nimportant to rigorously assess their performance on practically relevant test\ncases. To this end, a number of datasets have been proposed. However, we argue\nthat there is a gap in existing databases: to our knowledge, there is currently\nno dataset that simultaneously satisfies two goals, namely a) to cleanly\nseparate scene content and forensic traces, and b) to support realistic\npost-processing like social media recompression. In this work, we propose the\nForchheim Image Database (FODB) to close this gap. It consists of more than\n23,000 images of 143 scenes by 27 smartphone cameras, and it allows to cleanly\nseparate image content from forensic artifacts. Each image is provided in 6\ndifferent qualities: the original camera-native version, and five copies from\nsocial networks. We demonstrate the usefulness of FODB in an evaluation of\nmethods for camera identification. We report three findings. First, the\nrecently proposed general-purpose EfficientNet remarkably outperforms several\ndedicated forensic CNNs both on clean and compressed images. Second,\nclassifiers obtain a performance boost even on unknown post-processing after\naugmentation by artificial degradations. Third, FODB's clean separation of\nscene content and forensic traces imposes important, rigorous boundary\nconditions for algorithm benchmarking.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02241v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02079v2",
    "title": "On the Moral Justification of Statistical Parity",
    "authors": [
      "Corinna Hertweck",
      "Christoph Heitz",
      "Michele Loi"
    ],
    "author_ids": [],
    "abstract": "A crucial but often neglected aspect of algorithmic fairness is the question\nof how we justify enforcing a certain fairness metric from a moral perspective.\nWhen fairness metrics are proposed, they are typically argued for by\nhighlighting their mathematical properties. Rarely are the moral assumptions\nbeneath the metric explained. Our aim in this paper is to consider the moral\naspects associated with the statistical fairness criterion of independence\n(statistical parity). To this end, we consider previous work, which discusses\nthe two worldviews \"What You See Is What You Get\" (WYSIWYG) and \"We're All\nEqual\" (WAE) and by doing so provides some guidance for clarifying the possible\nassumptions in the design of algorithms. We present an extension of this work,\nwhich centers on morality. The most natural moral extension is that\nindependence needs to be fulfilled if and only if differences in predictive\nfeatures (e.g. high school grades and standardized test scores are predictive\nof performance at university) between socio-demographic groups are caused by\nunjust social disparities or measurement errors. Through two counterexamples,\nwe demonstrate that this extension is not universally true. This means that the\nquestion of whether independence should be used or not cannot be satisfactorily\nanswered by only considering the justness of differences in the predictive\nfeatures.",
    "published_date": "2020-11-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02079v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02066v2",
    "title": "University of Washington at TREC 2020 Fairness Ranking Track",
    "authors": [
      "Yunhe Feng",
      "Daniel Saelid",
      "Ke Li",
      "Ruoyuan Gao",
      "Chirag Shah"
    ],
    "author_ids": [],
    "abstract": "InfoSeeking Lab's FATE (Fairness Accountability Transparency Ethics) group at\nUniversity of Washington participated in 2020 TREC Fairness Ranking Track. This\nreport describes that track, assigned data and tasks, our group definitions,\nand our results. Our approach to bringing fairness in retrieval and re-ranking\ntasks with Semantic Scholar data was to extract various dimensions of author\nidentity. These dimensions included gender and location. We developed modules\nfor these extractions in a way that allowed us to plug them in for either of\nthe tasks as needed. After trying different combinations of relative weights\nassigned to relevance, gender, and location information, we chose five runs for\nretrieval and five runs for re-ranking tasks. The results showed that our runs\nperformed below par for re-ranking task, but above average for retrieval.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02066v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.02036v1",
    "title": "(Un)fairness in Post-operative Complication Prediction Models",
    "authors": [
      "Sandhya Tripathi",
      "Bradley A. Fritz",
      "Mohamed Abdelhack",
      "Michael S. Avidan",
      "Yixin Chen",
      "Christopher R. King"
    ],
    "author_ids": [],
    "abstract": "With the current ongoing debate about fairness, explainability and\ntransparency of machine learning models, their application in high-impact\nclinical decision-making systems must be scrutinized. We consider a real-life\nexample of risk estimation before surgery and investigate the potential for\nbias or unfairness of a variety of algorithms. Our approach creates transparent\ndocumentation of potential bias so that the users can apply the model\ncarefully. We augment a model-card like analysis using propensity scores with a\ndecision-tree based guide for clinicians that would identify predictable\nshortcomings of the model. In addition to functioning as a guide for users, we\npropose that it can guide the algorithm development and informatics team to\nfocus on data sources and structures that can address these shortcomings.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02036v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01961v1",
    "title": "Insights into Fairness through Trust: Multi-scale Trust Quantification for Financial Deep Learning",
    "authors": [
      "Alexander Wong",
      "Andrew Hryniowski",
      "Xiao Yu Wang"
    ],
    "author_ids": [],
    "abstract": "The success of deep learning in recent years have led to a significant\nincrease in interest and prevalence for its adoption to tackle financial\nservices tasks. One particular question that often arises as a barrier to\nadopting deep learning for financial services is whether the developed\nfinancial deep learning models are fair in their predictions, particularly in\nlight of strong governance and regulatory compliance requirements in the\nfinancial services industry. A fundamental aspect of fairness that has not been\nexplored in financial deep learning is the concept of trust, whose variations\nmay point to an egocentric view of fairness and thus provide insights into the\nfairness of models. In this study we explore the feasibility and utility of a\nmulti-scale trust quantification strategy to gain insights into the fairness of\na financial deep learning model, particularly under different scenarios at\ndifferent scales. More specifically, we conduct multi-scale trust\nquantification on a deep neural network for the purpose of credit card default\nprediction to study: 1) the overall trustworthiness of the model 2) the trust\nlevel under all possible prediction-truth relationships, 3) the trust level\nacross the spectrum of possible predictions, 4) the trust level across\ndifferent demographic groups (e.g., age, gender, and education), and 5)\ndistribution of overall trust for an individual prediction scenario. The\ninsights for this proof-of-concept study demonstrate that such a multi-scale\ntrust quantification strategy may be helpful for data scientists and regulators\nin financial services as part of the verification and certification of\nfinancial deep learning solutions to gain insights into fairness and trust of\nthese solutions.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01961v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01897v2",
    "title": "A Multi-aspect Analysis of Gender Bias on Online Student Evaluations",
    "authors": [
      "Sofia Maria Nikolakaki",
      "Joseph Lai",
      "Evimaria Terzi"
    ],
    "author_ids": [],
    "abstract": "Institutions widely use student evaluations to assess the faculty's teaching\nperformance, but underlying trends and biases can influence their\ninterpretation. Using data from Rate My Professors, we conduct the largest and\nmost recent quantitative data analysis to study questions related to the\nevaluation criteria that students have when they review the performance of\ntheir male and female professors. Our analysis spans data from two decades\n(1999-2019), thus taking into account recent changes on the website and in the\nperception of students, and demonstrates interesting insights related to how\nstudents perceive the teaching style and personality traits of their male and\nfemale professors. We also present the first analysis that investigates how\ngender bias evolves over time and changes over space. We believe that our\nresults are interesting from a sociological viewpoint, as they investigate the\nrole of gender in higher education by disclosing how students perceive and\nevaluate professors of different genders. In addition, we believe that our\nfindings can be useful to educational institutions when considering possible\nbiases that exist in the evaluations of their faculty.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01897v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.01888v1",
    "title": "Unsupervised Attention Based Instance Discriminative Learning for Person Re-Identification",
    "authors": [
      "Kshitij Nikhal",
      "Benjamin S. Riggan"
    ],
    "author_ids": [],
    "abstract": "Recent advances in person re-identification have demonstrated enhanced\ndiscriminability, especially with supervised learning or transfer learning.\nHowever, since the data requirements---including the degree of data\ncurations---are becoming increasingly complex and laborious, there is a\ncritical need for unsupervised methods that are robust to large intra-class\nvariations, such as changes in perspective, illumination, articulated motion,\nresolution, etc. Therefore, we propose an unsupervised framework for person\nre-identification which is trained in an end-to-end manner without any\npre-training. Our proposed framework leverages a new attention mechanism that\ncombines group convolutions to (1) enhance spatial attention at multiple scales\nand (2) reduce the number of trainable parameters by 59.6%. Additionally, our\nframework jointly optimizes the network with agglomerative clustering and\ninstance learning to tackle hard samples. We perform extensive analysis using\nthe Market1501 and DukeMTMC-reID datasets to demonstrate that our method\nconsistently outperforms the state-of-the-art methods (with and without\npre-trained weights).",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01888v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01869v2",
    "title": "Learning unbiased group-wise registration (LUGR) and joint segmentation: evaluation on longitudinal diffusion MRI",
    "authors": [
      "Bo Li",
      "Wiro J. Niessen",
      "Stefan Klein",
      "M. Arfan Ikram",
      "Meike W. Vernooij",
      "Esther E. Bron"
    ],
    "author_ids": [],
    "abstract": "Analysis of longitudinal changes in imaging studies often involves both\nsegmentation of structures of interest and registration of multiple timeframes.\nThe accuracy of such analysis could benefit from a tailored framework that\njointly optimizes both tasks to fully exploit the information available in the\nlongitudinal data. Most learning-based registration algorithms, including joint\noptimization approaches, currently suffer from bias due to selection of a fixed\nreference frame and only support pairwise transformations. We here propose an\nanalytical framework based on an unbiased learning strategy for group-wise\nregistration that simultaneously registers images to the mean space of a group\nto obtain consistent segmentations. We evaluate the proposed method on\nlongitudinal analysis of a white matter tract in a brain MRI dataset with 2-3\ntime-points for 3249 individuals, i.e., 8045 images in total. The\nreproducibility of the method is evaluated on test-retest data from 97\nindividuals. The results confirm that the implicit reference image is an\naverage of the input image. In addition, the proposed framework leads to\nconsistent segmentations and significantly lower processing bias than that of a\npair-wise fixed-reference approach. This processing bias is even smaller than\nthose obtained when translating segmentations by only one voxel, which can be\nattributed to subtle numerical instabilities and interpolation. Therefore, we\npostulate that the proposed mean-space learning strategy could be widely\napplied to learning-based registration tasks. In addition, this group-wise\nframework introduces a novel way for learning-based longitudinal studies by\ndirect construction of an unbiased within-subject template and allowing\nreliable and efficient analysis of spatio-temporal imaging biomarkers.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01869v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01837v3",
    "title": "The Gap on GAP: Tackling the Problem of Differing Data Distributions in Bias-Measuring Datasets",
    "authors": [
      "Vid Kocijan",
      "Oana-Maria Camburu",
      "Thomas Lukasiewicz"
    ],
    "author_ids": [],
    "abstract": "Diagnostic datasets that can detect biased models are an important\nprerequisite for bias reduction within natural language processing. However,\nundesired patterns in the collected data can make such tests incorrect. For\nexample, if the feminine subset of a gender-bias-measuring coreference\nresolution dataset contains sentences with a longer average distance between\nthe pronoun and the correct candidate, an RNN-based model may perform worse on\nthis subset due to long-term dependencies. In this work, we introduce a\ntheoretically grounded method for weighting test samples to cope with such\npatterns in the test data. We demonstrate the method on the GAP dataset for\ncoreference resolution. We annotate GAP with spans of all personal names and\nshow that examples in the female subset contain more personal names and a\nlonger distance between pronouns and their referents, potentially affecting the\nbias score in an undesired way. Using our weighting method, we find the set of\nweights on the test instances that should be used for coping with these\ncorrelations, and we re-evaluate 16 recently released coreference models.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01837v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01821v1",
    "title": "Minimax Pareto Fairness: A Multi Objective Perspective",
    "authors": [
      "Natalia Martinez",
      "Martin Bertran",
      "Guillermo Sapiro"
    ],
    "author_ids": [],
    "abstract": "In this work we formulate and formally characterize group fairness as a\nmulti-objective optimization problem, where each sensitive group risk is a\nseparate objective. We propose a fairness criterion where a classifier achieves\nminimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary\nharm, and can lead to the best zero-gap model if policy dictates so. We provide\na simple optimization algorithm compatible with deep neural networks to satisfy\nthese constraints. Since our method does not require test-time access to\nsensitive attributes, it can be applied to reduce worst-case classification\nerrors between outcomes in unbalanced classification problems. We test the\nproposed methodology on real case-studies of predicting income, ICU patient\nmortality, skin lesions classification, and assessing credit risk,\ndemonstrating how our framework compares favorably to other approaches.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01821v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01712v1",
    "title": "Economic Principles of PoPCoin, a Democratic Time-based Cryptocurrency",
    "authors": [
      "Haoqian Zhang",
      "Cristina Basescu",
      "Bryan Ford"
    ],
    "author_ids": [],
    "abstract": "While democracy is founded on the principle of equal opportunity to manage\nour lives and pursue our fortunes, the forms of money we have inherited from\nmillenia of evolution has brought us to an unsustainable dead-end of exploding\ninequality. PoPCoin proposes to leverage the unique historical opportunities\nthat digital cryptocurrencies present for a \"clean-slate\" redesign of money, in\nparticular around long-term equitability and sustainability, rather than solely\nstability, as our primary goals. We develop and analyze a monetary policy for\nPoPCoin that embodies these equitability goals in two basic rules that maybe\nsummarized as supporting equal opportunity in \"space\" and \"time\": the first by\nregularly distributing new money equally to all participants much like a basic\nincome, the second by holding the aggregate value of these distributions to a\nconstant and non-diminishing portion of total money supply through demurrage.\nThrough preliminary economic analysis, we find that these rules in combination\nyield a unique form of money with numerous intriguing and promising properties,\nsuch as a quantifiable and provable upper bound on monetary inequality, a\nnatural \"early adopter's reward\" that could incentivize rapid growth while\ntapering off as participation saturates, resistance to the risk of deflationary\nspirals, and migration incentives opposite those created by conventional basic\nincomes.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "q-fin.GN"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01712v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.01575v1",
    "title": "AraWEAT: Multidimensional Analysis of Biases in Arabic Word Embeddings",
    "authors": [
      "Anne Lauscher",
      "Rafik Takieddin",
      "Simone Paolo Ponzetto",
      "Goran Glavaš"
    ],
    "author_ids": [],
    "abstract": "Recent work has shown that distributional word vector spaces often encode\nhuman biases like sexism or racism. In this work, we conduct an extensive\nanalysis of biases in Arabic word embeddings by applying a range of recently\nintroduced bias tests on a variety of embedding spaces induced from corpora in\nArabic. We measure the presence of biases across several dimensions, namely:\nembedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of\ntext (encyclopedic text, and news vs. user-generated content), dialects\n(Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses\nover corpora from different time periods). Our analysis yields several\ninteresting findings, e.g., that implicit gender bias in embeddings trained on\nArabic news corpora steadily increases over time (between 2007 and 2017). We\nmake the Arabic bias specifications (AraWEAT) publicly available.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01575v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01516v3",
    "title": "Quadratic Metric Elicitation for Fairness and Beyond",
    "authors": [
      "Gaurush Hiranandani",
      "Jatin Mathur",
      "Harikrishna Narasimhan",
      "Oluwasanmi Koyejo"
    ],
    "author_ids": [],
    "abstract": "Metric elicitation is a recent framework for eliciting classification\nperformance metrics that best reflect implicit user preferences based on the\ntask and context. However, available elicitation strategies have been limited\nto linear (or quasi-linear) functions of predictive rates, which can be\npractically restrictive for many applications including fairness. This paper\ndevelops a strategy for eliciting more flexible multiclass metrics defined by\nquadratic functions of rates, designed to reflect human preferences better. We\nshow its application in eliciting quadratic violation-based group-fair metrics.\nOur strategy requires only relative preference feedback, is robust to noise,\nand achieves near-optimal query complexity. We further extend this strategy to\neliciting polynomial metrics -- thus broadening the use cases for metric\nelicitation.",
    "published_date": "2020-11-03T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01516v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02272v1",
    "title": "Trustworthy AI",
    "authors": [
      "Richa Singh",
      "Mayank Vatsa",
      "Nalini Ratha"
    ],
    "author_ids": [],
    "abstract": "Modern AI systems are reaping the advantage of novel learning methods. With\ntheir increasing usage, we are realizing the limitations and shortfalls of\nthese systems. Brittleness to minor adversarial changes in the input data,\nability to explain the decisions, address the bias in their training data, high\nopacity in terms of revealing the lineage of the system, how they were trained\nand tested, and under which parameters and conditions they can reliably\nguarantee a certain level of performance, are some of the most prominent\nlimitations. Ensuring the privacy and security of the data, assigning\nappropriate credits to data sources, and delivering decent outputs are also\nrequired features of an AI system. We propose the tutorial on Trustworthy AI to\naddress six critical issues in enhancing user and public trust in AI systems,\nnamely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of\nadversarial attacks, (iv) improved privacy and security in model building, (v)\nbeing decent, and (vi) model attribution, including the right level of credit\nassignment to the data sources, model architectures, and transparency in\nlineage.",
    "published_date": "2020-11-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02272v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01139v1",
    "title": "Automated Transcription of Non-Latin Script Periodicals: A Case Study in the Ottoman Turkish Print Archive",
    "authors": [
      "Suphan Kirmizialtin",
      "David Wrisley"
    ],
    "author_ids": [],
    "abstract": "Our study utilizes deep learning methods for the automated transcription of\nlate nineteenth- and early twentieth-century periodicals written in Arabic\nscript Ottoman Turkish (OT) using the Transkribus platform. We discuss the\nhistorical situation of OT text collections and how they were excluded for the\nmost part from the late twentieth century corpora digitization that took place\nin many Latin script languages. This exclusion has two basic reasons: the\ntechnical challenges of OCR for Arabic script languages, and the rapid\nabandonment of that very script in the Turkish historical context. In the\nspecific case of OT, opening periodical collections to digital tools require\ntraining HTR models to generate transcriptions in the Latin writing system of\ncontemporary readers of Turkish, and not, as some may expect, in right-to-left\nArabic script text. In the paper we discuss the challenges of training such\nmodels where one-to-one correspondence between the writing systems do not\nexist, and we report results based on our HTR experiments with two OT\nperiodicals from the early twentieth century. Finally, we reflect on potential\ndomain bias of HTR models in historical languages exhibiting spatio-temporal\nvariance as well as the significance of working between writing systems for\nlanguage communities that have experienced language reform and script change.",
    "published_date": "2020-11-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01139v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00719v2",
    "title": "Optimizing embedding-related quantum annealing parameters for reducing hardware bias",
    "authors": [
      "Aaron Barbosa",
      "Elijah Pelofske",
      "Georg Hahn",
      "Hristo N. Djidjev"
    ],
    "author_ids": [],
    "abstract": "Quantum annealers have been designed to propose near-optimal solutions to\nNP-hard optimization problems. However, the accuracy of current annealers such\nas the ones of D-Wave Systems, Inc., is limited by environmental noise and\nhardware biases. One way to deal with these imperfections and to improve the\nquality of the annealing results is to apply a variety of pre-processing\ntechniques such as spin reversal (SR), anneal offsets (AO), or chain weights\n(CW). Maximizing the effectiveness of these techniques involves performing\noptimizations over a large number of parameters, which would be too costly if\nneeded to be done for each new problem instance. In this work, we show that the\naforementioned parameter optimization can be done for an entire class of\nproblems, given each instance uses a previously chosen fixed embedding.\nSpecifically, in the training phase, we fix an embedding E of a complete graph\nonto the hardware of the annealer, and then run an optimization algorithm to\ntune the following set of parameter values: the set of bits to be flipped for\nSR, the specific qubit offsets for AO, and the distribution of chain weights,\noptimized over a set of training graphs randomly chosen from that class, where\nthe graphs are embedded onto the hardware using E. In the testing phase, we\nestimate how well the parameters computed during the training phase work on a\nrandom selection of other graphs from that class. We investigate graph\ninstances of varying densities for the Maximum Clique, Maximum Cut, and Graph\nPartitioning problems. Our results indicate that, compared to their default\nbehavior, substantial improvements of the annealing results can be achieved by\nusing the optimized parameters for SR, AO, and CW.",
    "published_date": "2020-11-02T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.ET",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00719v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00681v1",
    "title": "Event-Related Bias Removal for Real-time Disaster Events",
    "authors": [
      "Evangelia Spiliopoulou",
      "Salvador Medina Maza",
      "Eduard Hovy",
      "Alexander Hauptmann"
    ],
    "author_ids": [],
    "abstract": "Social media has become an important tool to share information about crisis\nevents such as natural disasters and mass attacks. Detecting actionable posts\nthat contain useful information requires rapid analysis of huge volume of data\nin real-time. This poses a complex problem due to the large amount of posts\nthat do not contain any actionable information. Furthermore, the classification\nof information in real-time systems requires training on out-of-domain data, as\nwe do not have any data from a new emerging crisis. Prior work focuses on\nmodels pre-trained on similar event types. However, those models capture\nunnecessary event-specific biases, like the location of the event, which affect\nthe generalizability and performance of the classifiers on new unseen data from\nan emerging new event. In our work, we train an adversarial neural model to\nremove latent event-specific biases and improve the performance on tweet\nimportance classification.",
    "published_date": "2020-11-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00681v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00641v1",
    "title": "Active Structure Learning of Causal DAGs via Directed Clique Tree",
    "authors": [
      "Chandler Squires",
      "Sara Magliacane",
      "Kristjan Greenewald",
      "Dmitriy Katz",
      "Murat Kocaoglu",
      "Karthikeyan Shanmugam"
    ],
    "author_ids": [],
    "abstract": "A growing body of work has begun to study intervention design for efficient\nstructure learning of causal directed acyclic graphs (DAGs). A typical setting\nis a causally sufficient setting, i.e. a system with no latent confounders,\nselection bias, or feedback, when the essential graph of the observational\nequivalence class (EC) is given as an input and interventions are assumed to be\nnoiseless. Most existing works focus on worst-case or average-case lower bounds\nfor the number of interventions required to orient a DAG. These worst-case\nlower bounds only establish that the largest clique in the essential graph\ncould make it difficult to learn the true DAG. In this work, we develop a\nuniversal lower bound for single-node interventions that establishes that the\nlargest clique is always a fundamental impediment to structure learning.\nSpecifically, we present a decomposition of a DAG into independently orientable\ncomponents through directed clique trees and use it to prove that the number of\nsingle-node interventions necessary to orient any DAG in an EC is at least the\nsum of half the size of the largest cliques in each chain component of the\nessential graph. Moreover, we present a two-phase intervention design algorithm\nthat, under certain conditions on the chordal skeleton, matches the optimal\nnumber of interventions up to a multiplicative logarithmic factor in the number\nof maximal cliques. We show via synthetic experiments that our algorithm can\nscale to much larger graphs than most of the related work and achieves better\nworst-case performance than other scalable approaches. A code base to recreate\nthese results can be found at https://github.com/csquires/dct-policy",
    "published_date": "2020-11-01T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00603v1",
    "title": "Making ML models fairer through explanations: the case of LimeOut",
    "authors": [
      "Guilherme Alves",
      "Vaishnavi Bhargava",
      "Miguel Couceiro",
      "Amedeo Napoli"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decisions are now being used on a daily basis, and based on\nMachine Learning (ML) processes that may be complex and biased. This raises\nseveral concerns given the critical impact that biased decisions may have on\nindividuals or on society as a whole. Not only unfair outcomes affect human\nrights, they also undermine public trust in ML and AI. In this paper we address\nfairness issues of ML models based on decision outcomes, and we show how the\nsimple idea of \"feature dropout\" followed by an \"ensemble approach\" can improve\nmodel fairness. To illustrate, we will revisit the case of \"LimeOut\" that was\nproposed to tackle \"process fairness\", which measures a model's reliance on\nsensitive or discriminatory features. Given a classifier, a dataset and a set\nof sensitive features, LimeOut first assesses whether the classifier is fair by\nchecking its reliance on sensitive features using \"Lime explanations\". If\ndeemed unfair, LimeOut then applies feature dropout to obtain a pool of\nclassifiers. These are then combined into an ensemble classifier that was\nempirically shown to be less dependent on sensitive features without\ncompromising the classifier's accuracy. We present different experiments on\nmultiple datasets and several state of the art classifiers, which show that\nLimeOut's classifiers improve (or at least maintain) not only process fairness\nbut also other fairness metrics such as individual and group fairness, equal\nopportunity, and demographic parity.",
    "published_date": "2020-11-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00603v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.01533v1",
    "title": "Joint Energy Harvest and Information Transfer for Energy Beamforming in Backscatter Multiuser Networks",
    "authors": [
      "Wenyuan Ma",
      "Wei Wang",
      "Tao Jiang"
    ],
    "author_ids": [],
    "abstract": "Wirelessly powered backscatter communication (WPBC) has been identified as a\npromising technology for low-power communication systems, which can reap the\nbenefits of energy beamforming to improve energy transfer efficiency. However,\nexisting studies on energy beamforming fail to simultaneously take energy\nsupply and information transfer in WPBC into account. This paper takes the\nfirst step to fill this gap, by considering the restrictive relationship\nbetween the energy harvesting rate and achievable rate with estimated\nbackscatter channel state information (BS-CSI). To ensure reliable\ncommunication and user fairness, we formulate the energy beamforming design as\na max-min optimization problem by maximizing the minimum achievable rate for\nall backscatter tags subject to the energy constraint. We derive the\nclosed-form expression of the energy harvesting rate, as well as the lower\nbound of the achievable rate for maximum-ratio combining (MRC) and zero-forcing\n(ZF) receivers. Our numerical results indicate that our scheme significantly\noutperforms state-of-the-art energy beamforming schemes. Additionally, the\nachievable rate of our scheme approaches more than $90\\%$ of the rate limit\nachieved via beamforming with perfect CSI for both receivers.",
    "published_date": "2020-11-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.01533v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.00379v2",
    "title": "Fair Classification with Group-Dependent Label Noise",
    "authors": [
      "Jialu Wang",
      "Yang Liu",
      "Caleb Levy"
    ],
    "author_ids": [],
    "abstract": "This work examines how to train fair classifiers in settings where training\nlabels are corrupted with random noise, and where the error rates of corruption\ndepend both on the label class and on the membership function for a protected\nsubgroup. Heterogeneous label noise models systematic biases towards particular\ngroups when generating annotations. We begin by presenting analytical results\nwhich show that naively imposing parity constraints on demographic disparity\nmeasures, without accounting for heterogeneous and group-dependent error rates,\ncan decrease both the accuracy and the fairness of the resulting classifier.\nOur experiments demonstrate these issues arise in practice as well. We address\nthese problems by performing empirical risk minimization with carefully defined\nsurrogate loss functions and surrogate constraints that help avoid the pitfalls\nintroduced by heterogeneous label noise. We provide both theoretical and\nempirical justifications for the efficacy of our methods. We view our results\nas an important example of how imposing fairness on biased data sets without\nproper care can do at least as much harm as it does good.",
    "published_date": "2020-10-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00379v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00364v2",
    "title": "Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization",
    "authors": [
      "Jelena Diakonikolas",
      "Constantinos Daskalakis",
      "Michael I. Jordan"
    ],
    "author_ids": [],
    "abstract": "The use of min-max optimization in adversarial training of deep neural\nnetwork classifiers and training of generative adversarial networks has\nmotivated the study of nonconvex-nonconcave optimization objectives, which\nfrequently arise in these applications. Unfortunately, recent results have\nestablished that even approximate first-order stationary points of such\nobjectives are intractable, even under smoothness conditions, motivating the\nstudy of min-max objectives with additional structure. We introduce a new class\nof structured nonconvex-nonconcave min-max optimization problems, proposing a\ngeneralization of the extragradient algorithm which provably converges to a\nstationary point. The algorithm applies not only to Euclidean spaces, but also\nto general $\\ell_p$-normed finite-dimensional real vector spaces. We also\ndiscuss its stability under stochastic oracles and provide bounds on its sample\ncomplexity. Our iteration complexity and sample complexity bounds either match\nor improve the best known bounds for the same or less general\nnonconvex-nonconcave settings, such as those that satisfy variational coherence\nor in which a weak solution to the associated variational inequality problem is\nassumed to exist.",
    "published_date": "2020-10-31T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00364v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02279v2",
    "title": "Ideal theory in AI ethics",
    "authors": [
      "Daniel Estrada"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the ways AI ethics research operates on an ideology of\nideal theory, in the sense discussed by Mills (2005) and recently applied to AI\nethics by Fazelpour \\& Lipton (2020). I address the structural and\nmethodological conditions that attract AI ethics researchers to ideal\ntheorizing, and the consequences this approach has for the quality and future\nof our research community. Finally, I discuss the possibilities for a nonideal\nfuture in AI ethics.",
    "published_date": "2020-10-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02279v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00256v1",
    "title": "Ostrowski type inequalities and some selected quadrature formulae",
    "authors": [
      "Gradimir V. Milovanovic"
    ],
    "author_ids": [],
    "abstract": "Some selected Ostrowski type inequalities and a connection with numerical\nintegration are studied in this survey paper, which is dedicated to the memory\nof Professor D.S. Mitrinovic, who left us 25 years ago. His significant\ninfluence to the development of the theory of inequalities is briefly given in\nthe first section of this paper. Beside some basic facts on quadrature formulas\nand an approach for estimating the error term using Ostrowski type inequalities\nand Peano kernel techniques, we give several examples of selected quadrature\nformulas and the corresponding inequalities, including the basic Ostrowski's\ninequality (1938), inequality of Milovanovic and Pecaric (1976) and its\nmodifications, inequality of Dragomir, Cerone and Roumeliotis (2000), symmetric\ninequality of Guessab and Schmeisser (2002) and asymmetric inequality of\nFranjic (2009), as well as four point symmetric inequalites by Alomari (2012)\nand a variant with double internal nodes given by Liu and Park (2017).",
    "published_date": "2020-10-31T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "26D15, 41A55, 65D30, 65D32"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00256v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.00244v2",
    "title": "Evaluating Bias In Dutch Word Embeddings",
    "authors": [
      "Rodrigo Alejandro Chávez Mulsa",
      "Gerasimos Spanakis"
    ],
    "author_ids": [],
    "abstract": "Recent research in Natural Language Processing has revealed that word\nembeddings can encode social biases present in the training data which can\naffect minorities in real world applications. This paper explores the gender\nbias implicit in Dutch embeddings while investigating whether English language\nbased approaches can also be used in Dutch. We implement the Word Embeddings\nAssociation Test (WEAT), Clustering and Sentence Embeddings Association Test\n(SEAT) methods to quantify the gender bias in Dutch word embeddings, then we\nproceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods\nand finally we evaluate the performance of the debiased embeddings in\ndownstream tasks. The results suggest that, among others, gender bias is\npresent in traditional and contextualized Dutch word embeddings. We highlight\nhow techniques used to measure and reduce bias created for English can be used\nin Dutch embeddings by adequately translating the data and taking into account\nthe unique characteristics of the language. Furthermore, we analyze the effect\nof the debiasing techniques on downstream tasks which show a negligible impact\non traditional embeddings and a 2% decrease in performance in contextualized\nembeddings. Finally, we release the translated Dutch datasets to the public\nalong with the traditional embeddings with mitigated bias.",
    "published_date": "2020-10-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00244v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.02027v1",
    "title": "Analysis and Reliability of Separable Systems",
    "authors": [
      "Héctor Cancela",
      "Gustavo Guerberoff",
      "Franco Robledo",
      "Pablo Romero"
    ],
    "author_ids": [],
    "abstract": "The operation of a system, such as a vehicle, communication network or\nautomatic process, heavily depends on the correct operation of its components.\nA Stochastic Binary System (SBS) mathematically models the behavior of on-off\nsystems, where the components are subject to probabilistic failures. Our goal\nis to understand the reliability of the global system.\n  The reliability evaluation of an SBS belongs to the class of NP-Hard\nproblems, and the combinatorics of SBS imposes several challenges. In a\nprevious work by the same authors, a special sub-class of SBSs called\n\"separable systems\" was introduced. These systems accept an efficient\nrepresentation by a linear inequality on the binary states of the components.\nHowever, the reliability evaluation of separable systems is still hard.\n  A theoretical contribution in the understanding of separable systems is\ngiven. We fully characterize separable systems under the all-terminal\nreliability model, finding that they admit efficient reliability evaluation in\nthis relevant context.",
    "published_date": "2020-10-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DM",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02027v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.02282v2",
    "title": "\"What We Can't Measure, We Can't Understand\": Challenges to Demographic Data Procurement in the Pursuit of Fairness",
    "authors": [
      "McKane Andrus",
      "Elena Spitzer",
      "Jeffrey Brown",
      "Alice Xiang"
    ],
    "author_ids": [],
    "abstract": "As calls for fair and unbiased algorithmic systems increase, so too does the\nnumber of individuals working on algorithmic fairness in industry. However,\nthese practitioners often do not have access to the demographic data they feel\nthey need to detect bias in practice. Even with the growing variety of toolkits\nand strategies for working towards algorithmic fairness, they almost invariably\nrequire access to demographic attributes or proxies. We investigated this\ndilemma through semi-structured interviews with 38 practitioners and\nprofessionals either working in or adjacent to algorithmic fairness.\nParticipants painted a complex picture of what demographic data availability\nand use look like on the ground, ranging from not having access to personal\ndata of any kind to being legally required to collect and use demographic data\nfor discrimination assessments. In many domains, demographic data collection\nraises a host of difficult questions, including how to balance privacy and\nfairness, how to define relevant social categories, how to ensure meaningful\nconsent, and whether it is appropriate for private companies to infer someone's\ndemographics. Our research suggests challenges that must be considered by\nbusinesses, regulators, researchers, and community groups in order to enable\npractitioners to address algorithmic bias in practice. Critically, we do not\npropose that the overall goal of future work should be to simply lower the\nbarriers to collecting demographic data. Rather, our study surfaces a swath of\nnormative questions about how, when, and whether this data should be procured,\nand, in cases where it is not, what should still be done to mitigate bias.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.02282v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2011.00092v1",
    "title": "Analyzing Gender Bias within Narrative Tropes",
    "authors": [
      "Dhruvil Gala",
      "Mohammad Omar Khursheed",
      "Hannah Lerner",
      "Brendan O'Connor",
      "Mohit Iyyer"
    ],
    "author_ids": [],
    "abstract": "Popular media reflects and reinforces societal biases through the use of\ntropes, which are narrative elements, such as archetypal characters and plot\narcs, that occur frequently across media. In this paper, we specifically\ninvestigate gender bias within a large collection of tropes. To enable our\nstudy, we crawl tvtropes.org, an online user-created repository that contains\n30K tropes associated with 1.9M examples of their occurrences across film,\ntelevision, and literature. We automatically score the \"genderedness\" of each\ntrope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered\ntopics within tropes, (2) the relationship between gender bias and popular\nreception, and (3) how the gender of a work's creator correlates with the types\nof tropes that they use.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00092v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.16409v1",
    "title": "Inherent Trade-offs in the Fair Allocation of Treatments",
    "authors": [
      "Yuzi He",
      "Keith Burghardt",
      "Siyi Guo",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "Explicit and implicit bias clouds human judgement, leading to discriminatory\ntreatment of minority groups. A fundamental goal of algorithmic fairness is to\navoid the pitfalls in human judgement by learning policies that improve the\noverall outcomes while providing fair treatment to protected classes. In this\npaper, we propose a causal framework that learns optimal intervention policies\nfrom data subject to fairness constraints. We define two measures of treatment\nbias and infer best treatment assignment that minimizes the bias while\noptimizing overall outcome. We demonstrate that there is a dilemma of balancing\nfairness and overall benefit; however, allowing preferential treatment to\nprotected classes in certain circumstances (affirmative action) can\ndramatically improve the overall benefit while also preserving fairness. We\napply our framework to data containing student outcomes on standardized tests\nand show how it can be used to design real-world policies that fairly improve\nstudent test scores. Our framework provides a principled way to learn fair\ntreatment policies in real-world settings.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.16409v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.16326v1",
    "title": "All of the Fairness for Edge Prediction with Optimal Transport",
    "authors": [
      "Charlotte Laclau",
      "Ievgen Redko",
      "Manvi Choudhary",
      "Christine Largeron"
    ],
    "author_ids": [],
    "abstract": "Machine learning and data mining algorithms have been increasingly used\nrecently to support decision-making systems in many areas of high societal\nimportance such as healthcare, education, or security. While being very\nefficient in their predictive abilities, the deployed algorithms sometimes tend\nto learn an inductive model with a discriminative bias due to the presence of\nthis latter in the learning sample. This problem gave rise to a new field of\nalgorithmic fairness where the goal is to correct the discriminative bias\nintroduced by a certain attribute in order to decorrelate it from the model's\noutput. In this paper, we study the problem of fairness for the task of edge\nprediction in graphs, a largely underinvestigated scenario compared to a more\npopular setting of fair classification. To this end, we formulate the problem\nof fair edge prediction, analyze it theoretically, and propose an\nembedding-agnostic repairing procedure for the adjacency matrix of an arbitrary\ngraph with a trade-off between the group and individual fairness. We\nexperimentally show the versatility of our approach and its capacity to provide\nexplicit control over different notions of fairness and prediction accuracy.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.16326v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.16309v1",
    "title": "Ethical Decision Making During Automated Vehicle Crashes",
    "authors": [
      "Noah Goodall"
    ],
    "author_ids": [],
    "abstract": "Automated vehicles have received much attention recently, particularly the\nDARPA Urban Challenge vehicles, Google's self-driving cars, and various others\nfrom auto manufacturers. These vehicles have the potential to significantly\nreduce crashes and improve roadway efficiency by automating the\nresponsibilities of the driver. Still, automated vehicles are expected to crash\noccasionally, even when all sensors, vehicle control components, and algorithms\nfunction perfectly. If a human driver is unable to take control in time, a\ncomputer will be responsible for pre-crash behavior. Unlike other automated\nvehicles--such as aircraft, where every collision is catastrophic, and guided\ntrack systems, which can only avoid collisions in one dimension--automated\nroadway vehicles can predict various crash trajectory alternatives and select a\npath with the lowest damage or likelihood of collision. In some situations, the\npreferred path may be ambiguous. This study investigates automated vehicle\ncrashing and concludes the following: (1) automated vehicles will almost\ncertainly crash, (2) an automated vehicle's decisions preceding certain crashes\nwill have a moral component, and (3) there is no obvious way to effectively\nencode complex human morals in software. A three-phase approach to developing\nethical crashing algorithms is presented, consisting of a rational approach, an\nartificial intelligence approach, and a natural language requirement. The\nphases are theoretical and should be implemented as the technology becomes\navailable.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.16309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.16290v2",
    "title": "3XOR Games with Perfect Commuting Operator Strategies Have Perfect Tensor Product Strategies and are Decidable in Polynomial Time",
    "authors": [
      "Adam Bene Watts",
      "J. William Helton"
    ],
    "author_ids": [],
    "abstract": "We consider 3XOR games with perfect commuting operator strategies. Given any\n3XOR game, we show existence of a perfect commuting operator strategy for the\ngame can be decided in polynomial time. Previously this problem was not known\nto be decidable. Our proof leads to a construction, showing a 3XOR game has a\nperfect commuting operator strategy iff it has a perfect tensor product\nstrategy using a 3 qubit (8 dimensional) GHZ state. This shows that for perfect\n3XOR games the advantage of a quantum strategy over a classical strategy\n(defined by the quantum-classical bias ratio) is bounded. This is in contrast\nto the general 3XOR case where the optimal quantum strategies can require high\ndimensional states and there is no bound on the quantum advantage.\n  To prove these results, we first show equivalence between deciding the value\nof an XOR game and solving an instance of the subgroup membership problem on a\nclass of right angled Coxeter groups. We then show, in a proof that consumes\nmost of this paper, that the instances of this problem corresponding to 3XOR\ngames can be solved in polynomial time.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.CC",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.16290v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.16228v2",
    "title": "\"Thy algorithm shalt not bear false witness\": An Evaluation of Multiclass Debiasing Methods on Word Embeddings",
    "authors": [
      "Thalea Schlender",
      "Gerasimos Spanakis"
    ],
    "author_ids": [],
    "abstract": "With the vast development and employment of artificial intelligence\napplications, research into the fairness of these algorithms has been\nincreased. Specifically, in the natural language processing domain, it has been\nshown that social biases persist in word embeddings and are thus in danger of\namplifying these biases when used. As an example of social bias, religious\nbiases are shown to persist in word embeddings and the need for its removal is\nhighlighted. This paper investigates the state-of-the-art multiclass debiasing\ntechniques: Hard debiasing, SoftWEAT debiasing and Conceptor debiasing. It\nevaluates their performance when removing religious bias on a common basis by\nquantifying bias removal via the Word Embedding Association Test (WEAT), Mean\nAverage Cosine Similarity (MAC) and the Relative Negative Sentiment Bias\n(RNSB). By investigating the religious bias removal on three widely used word\nembeddings, namely: Word2Vec, GloVe, and ConceptNet, it is shown that the\npreferred method is ConceptorDebiasing. Specifically, this technique manages to\ndecrease the measured religious bias on average by 82,42%, 96,78% and 54,76%\nfor the three word embedding sets respectively.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.16228v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.16092v1",
    "title": "An Unsupervised Approach towards Varying Human Skin Tone Using Generative Adversarial Networks",
    "authors": [
      "Debapriya Roy",
      "Diganta Mukherjee",
      "Bhabatosh Chanda"
    ],
    "author_ids": [],
    "abstract": "With the increasing popularity of augmented and virtual reality, retailers\nare now focusing more towards customer satisfaction to increase the amount of\nsales. Although augmented reality is not a new concept but it has gained much\nneeded attention over the past few years. Our present work is targeted towards\nthis direction which may be used to enhance user experience in various virtual\nand augmented reality based applications. We propose a model to change skin\ntone of a person. Given any input image of a person or a group of persons with\nsome value indicating the desired change of skin color towards fairness or\ndarkness, this method can change the skin tone of the persons in the image.\nThis is an unsupervised method and also unconstrained in terms of pose,\nillumination, number of persons in the image etc. The goal of this work is to\nreduce the time and effort which is generally required for changing the skin\ntone using existing applications (e.g., Photoshop) by professionals or novice.\nTo establish the efficacy of this method we have compared our result with that\nof some popular photo editor and also with the result of some existing\nbenchmark method related to human attribute manipulation. Rigorous experiments\non different datasets show the effectiveness of this method in terms of\nsynthesizing perceptually convincing outputs.",
    "published_date": "2020-10-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.16092v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15665v1",
    "title": "Machine Ethics and Automated Vehicles",
    "authors": [
      "Noah J. Goodall"
    ],
    "author_ids": [],
    "abstract": "Road vehicle travel at a reasonable speed involves some risk, even when using\ncomputer-controlled driving with failure-free hardware and perfect sensing. A\nfully-automated vehicle must continuously decide how to allocate this risk\nwithout a human driver's oversight. These are ethical decisions, particularly\nin instances where an automated vehicle cannot avoid crashing. In this chapter,\nI introduce the concept of moral behavior for an automated vehicle, argue the\nneed for research in this area through responses to anticipated critiques, and\ndiscuss relevant applications from machine ethics and moral modeling research.",
    "published_date": "2020-10-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15665v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.15455v5",
    "title": "Optimal Sharing and Fair Cost Allocation of Community Energy Storage",
    "authors": [
      "Yu Yang",
      "Guoqiang Hu",
      "Costas J. Spanos"
    ],
    "author_ids": [],
    "abstract": "This paper studies an energy storage (ES) sharing model which is\ncooperatively invested by multiple buildings for harnessing on-site renewable\nutilization and grid price arbitrage. To maximize the economic benefits, we\njointly consider the ES sizing, operation, and cost allocation via a coalition\ngame formulation. Particularly, we study a fair ex-post cost allocation based\non nucleolus which addresses fairness by minimizing the minimal dissatisfaction\nof all the players. To overcome the exponential computation burden caused by\nthe implicit characteristic function, we employ a constraint generation\ntechnique to gradually approach the unique nucleolus by leveraging the sparse\nproblem structure. We demonstrate both the fairness and computational\nefficiency of the method through case studies, which are not provided by the\nexisting Shapley approach or proportional method. Particularly, only a small\nfraction of characteristic function (less than 1% for 20 buildings) is required\nto achieve the cost allocation versus the exponential information required by\nShapley approach. Though there exists a minor increase of computation over the\nproportional method, the proposed method can ensure fairness while the latter\nfails in some cases. Further, we demonstrate both the building-wise and\ncommunity-wise economic benefits are enhanced with the ES sharing model over\nthe individual ES (IES) model. Accordingly, the overall value of ES is\nconsiderably improved (about 1.83 times).",
    "published_date": "2020-10-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15455v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.15363v2",
    "title": "Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System",
    "authors": [
      "Tianxin Wei",
      "Fuli Feng",
      "Jiawei Chen",
      "Ziwei Wu",
      "Jinfeng Yi",
      "Xiangnan He"
    ],
    "author_ids": [],
    "abstract": "The general aim of the recommender system is to provide personalized\nsuggestions to users, which is opposed to suggesting popular items. However,\nthe normal training paradigm, i.e., fitting a recommender model to recover the\nuser behavior data with pointwise or pairwise loss, makes the model biased\ntowards popular items. This results in the terrible Matthew effect, making\npopular items be more frequently recommended and become even more popular.\nExisting work addresses this issue with Inverse Propensity Weighting (IPW),\nwhich decreases the impact of popular items on the training and increases the\nimpact of long-tail items. Although theoretically sound, IPW methods are highly\nsensitive to the weighting strategy, which is notoriously difficult to tune. In\nthis work, we explore the popularity bias issue from a novel and fundamental\nperspective -- cause-effect. We identify that popularity bias lies in the\ndirect effect from the item node to the ranking score, such that an item's\nintrinsic property is the cause of mistakenly assigning it a higher ranking\nscore. To eliminate popularity bias, it is essential to answer the\ncounterfactual question that what the ranking score would be if the model only\nuses item property. To this end, we formulate a causal graph to describe the\nimportant cause-effect relations in the recommendation process. During\ntraining, we perform multi-task learning to achieve the contribution of each\ncause; during testing, we perform counterfactual inference to remove the effect\nof item popularity. Remarkably, our solution amends the learning process of\nrecommendation which is agnostic to a wide range of models -- it can be easily\nimplemented in existing methods. We demonstrate it on Matrix Factorization (MF)\nand LightGCN [20]. Experiments on five real-world datasets demonstrate the\neffectiveness of our method.",
    "published_date": "2020-10-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15363v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.15346v1",
    "title": "Developing Augmented Reality based Gaming Model to Teach Ethical Education in Primary Schools",
    "authors": [
      "Mohammad Ali"
    ],
    "author_ids": [],
    "abstract": "Education sector is adopting new technologies for both teaching and learning\npedagogy. Augmented Reality (AR) is a new technology that can be used in the\neducational pedagogy to enhance the engagement with students. Students interact\nwith AR-based educational material for more visualization and explanation.\nTherefore, the use of AR in education is becoming more popular. However, most\nresearches narrate the use of AR technologies in the field of English, Maths,\nScience, Culture, Arts, and History education but the absence of ethical\neducation is visible. In our paper, we design the system and develop an\nAR-based mobile game model in the field of Ethical education for pre-primary\nstudents. Students from pre-primary require more interactive lessons than\ntheoretical concepts. So, we use AR technology to develop a game which offers\ninteractive procedures where students can learn with fun and engage with the\ncontext. Finally, we develop a prototype that works with our research\nobjective. We conclude our paper with future works.",
    "published_date": "2020-10-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15346v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2011.00159v2",
    "title": "Learning Strategies in Decentralized Matching Markets under Uncertain Preferences",
    "authors": [
      "Xiaowu Dai",
      "Michael I. Jordan"
    ],
    "author_ids": [],
    "abstract": "We study the problem of decision-making in the setting of a scarcity of\nshared resources when the preferences of agents are unknown a priori and must\nbe learned from data. Taking the two-sided matching market as a running\nexample, we focus on the decentralized setting, where agents do not share their\nlearned preferences with a central authority. Our approach is based on the\nrepresentation of preferences in a reproducing kernel Hilbert space, and a\nlearning algorithm for preferences that accounts for uncertainty due to the\ncompetition among the agents in the market. Under regularity conditions, we\nshow that our estimator of preferences converges at a minimax optimal rate.\nGiven this result, we derive optimal strategies that maximize agents' expected\npayoffs and we calibrate the uncertain state by taking opportunity costs into\naccount. We also derive an incentive-compatibility property and show that the\noutcome from the learned strategies has a stability property. Finally, we prove\na fairness property that asserts that there exists no justified envy according\nto the learned strategies.",
    "published_date": "2020-10-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.LG",
      "stat.ME",
      "stat.ML",
      "91B24, 91A10, 62G05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.00159v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15300v1",
    "title": "Uncovering Latent Biases in Text: Method and Application to Peer Review",
    "authors": [
      "Emaad Manzoor",
      "Nihar B. Shah"
    ],
    "author_ids": [],
    "abstract": "Quantifying systematic disparities in numerical quantities such as employment\nrates and wages between population subgroups provides compelling evidence for\nthe existence of societal biases. However, biases in the text written for\nmembers of different subgroups (such as in recommendation letters for male and\nnon-male candidates), though widely reported anecdotally, remain challenging to\nquantify. In this work, we introduce a novel framework to quantify bias in text\ncaused by the visibility of subgroup membership indicators. We develop a\nnonparametric estimation and inference procedure to estimate this bias. We then\nformalize an identification strategy to causally link the estimated bias to the\nvisibility of subgroup membership indicators, provided observations from time\nperiods both before and after an identity-hiding policy change. We identify an\napplication wherein \"ground truth\" bias can be inferred to evaluate our\nframework, instead of relying on synthetic or secondary data. Specifically, we\napply our framework to quantify biases in the text of peer reviews from a\nreputed machine learning conference before and after the conference adopted a\ndouble-blind reviewing policy. We show evidence of biases in the review ratings\nthat serves as \"ground truth\", and show that our proposed framework accurately\ndetects these biases from the review text without having access to the review\nratings.",
    "published_date": "2020-10-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15300v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15277v3",
    "title": "Class-incremental learning: survey and performance evaluation on image classification",
    "authors": [
      "Marc Masana",
      "Xialei Liu",
      "Bartlomiej Twardowski",
      "Mikel Menta",
      "Andrew D. Bagdanov",
      "Joost van de Weijer"
    ],
    "author_ids": [],
    "abstract": "For future learning systems, incremental learning is desirable because it\nallows for: efficient resource usage by eliminating the need to retrain from\nscratch at the arrival of new data; reduced memory usage by preventing or\nlimiting the amount of data required to be stored -- also important when\nprivacy limitations are imposed; and learning that more closely resembles human\nlearning. The main challenge for incremental learning is catastrophic\nforgetting, which refers to the precipitous drop in performance on previously\nlearned tasks after learning a new one. Incremental learning of deep neural\nnetworks has seen explosive growth in recent years. Initial work focused on\ntask-incremental learning, where a task-ID is provided at inference time.\nRecently, we have seen a shift towards class-incremental learning where the\nlearner must discriminate at inference time between all classes seen in\nprevious tasks without recourse to a task-ID. In this paper, we provide a\ncomplete survey of existing class-incremental learning methods for image\nclassification, and in particular, we perform an extensive experimental\nevaluation on thirteen class-incremental methods. We consider several new\nexperimental scenarios, including a comparison of class-incremental methods on\nmultiple large-scale image classification datasets, an investigation into small\nand large domain shifts, and a comparison of various network architectures.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15277v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15052v3",
    "title": "Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases",
    "authors": [
      "Ryan Steed",
      "Aylin Caliskan"
    ],
    "author_ids": [],
    "abstract": "Recent advances in machine learning leverage massive datasets of unlabeled\nimages from the web to learn general-purpose image representations for tasks\nfrom image classification to face recognition. But do unsupervised computer\nvision models automatically learn implicit patterns and embed social biases\nthat could have harmful downstream effects? We develop a novel method for\nquantifying biased associations between representations of social concepts and\nattributes in images. We find that state-of-the-art unsupervised models trained\non ImageNet, a popular benchmark image dataset curated from internet images,\nautomatically learn racial, gender, and intersectional biases. We replicate 8\npreviously documented human biases from social psychology, from the innocuous,\nas with insects and flowers, to the potentially harmful, as with race and\ngender. Our results closely match three hypotheses about intersectional bias\nfrom social psychology. For the first time in unsupervised computer vision, we\nalso quantify implicit human biases about weight, disabilities, and several\nethnicities. When compared with statistical patterns in online image datasets,\nour findings suggest that machine learning models can automatically learn bias\nfrom the way people are stereotypically portrayed on the web.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15052v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15120v3",
    "title": "Gender Bias in Depression Detection Using Audio Features",
    "authors": [
      "Andrew Bailey",
      "Mark D. Plumbley"
    ],
    "author_ids": [],
    "abstract": "Depression is a large-scale mental health problem and a challenging area for\nmachine learning researchers in detection of depression. Datasets such as\nDistress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created\nto aid research in this area. However, on top of the challenges inherent in\naccurately detecting depression, biases in datasets may result in skewed\nclassification performance. In this paper we examine gender bias in the\nDAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an\noverreporting of performance. By different concepts from Fair Machine Learning,\nsuch as data re-distribution, and using raw audio features, we can mitigate\nagainst the harmful effects of bias.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15120v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15012v1",
    "title": "Measurement-based coexistence studies of LAA & Wi-Fi deployments in Chicago",
    "authors": [
      "Vanlin Sathya",
      "Muhammad Iqbal Rochman",
      "Monisha Ghosh"
    ],
    "author_ids": [],
    "abstract": "LTE-Licensed Assisted Access (LAA) networks are beginning to be deployed\nwidely in major metropolitan areas in the US in the unlicensed 5 GHz bands,\nwhich have existing dense deployments of Wi-Fi as well. Various aspects of the\ncoexistence scenarios such deployments give rise to have been considered ina\nvast body of academic and industry research. However, there is very little data\nand research on how these coexisting networks will behave in practice. The\nquestion of fair coexistence between Wi-Fi and LAA has moved from a theoretical\nquestion to reality. The recent roll-out of LAA deployments provides an\nopportunity to collect data on the operation of these networks as well as\nstudying coexistence issues on the ground. In this paper we describe the first\nresults of a measurement campaign conducted over many months, using custom apps\nas well as off-the-shelf tools, in several areas of Chicago where the major\ncarriers have been expanding LAA deployments. The measurements reveal that\ncoexistence between LAA and Wi-Fi in dense, urban environments where both\nsystems aggregate multiple channels, continues to be a challenging problem that\nrequires further research.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.PF",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15012v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.14952v1",
    "title": "Towards Ethics by Design in Online Abusive Content Detection",
    "authors": [
      "Svetlana Kiritchenko",
      "Isar Nejadgholi"
    ],
    "author_ids": [],
    "abstract": "To support safety and inclusion in online communications, significant efforts\nin NLP research have been put towards addressing the problem of abusive content\ndetection, commonly defined as a supervised classification task. The research\neffort has spread out across several closely related sub-areas, such as\ndetection of hate speech, toxicity, cyberbullying, etc. There is a pressing\nneed to consolidate the field under a common framework for task formulation,\ndataset design and performance evaluation. Further, despite current\ntechnologies achieving high classification accuracies, several ethical issues\nhave been revealed. We bring ethical issues to forefront and propose a unified\nframework as a two-step process. First, online content is categorized around\npersonal and identity-related subject matters. Second, severity of abuse is\nidentified through comparative annotation within each category. The novel\nframework is guided by the Ethics by Design principle and is a step towards\nbuilding more accurate and trusted models.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14952v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15675v4",
    "title": "Deep DA for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos",
    "authors": [
      "Gnana Praveen R",
      "Eric Granger",
      "Patrick Cardinal"
    ],
    "author_ids": [],
    "abstract": "Automatic estimation of pain intensity from facial expressions in videos has\nan immense potential in health care applications. However, domain adaptation\n(DA) is needed to alleviate the problem of domain shifts that typically occurs\nbetween video data captured in source and target do-mains. Given the laborious\ntask of collecting and annotating videos, and the subjective bias due to\nambiguity among adjacent intensity levels, weakly-supervised learning (WSL)is\ngaining attention in such applications. Yet, most state-of-the-art WSL models\nare typically formulated as regression problems, and do not leverage the\nordinal relation between intensity levels, nor the temporal coherence of\nmultiple consecutive frames. This paper introduces a new deep learn-ing model\nfor weakly-supervised DA with ordinal regression(WSDA-OR), where videos in\ntarget domain have coarse la-bels provided on a periodic basis. The WSDA-OR\nmodel enforces ordinal relationships among the intensity levels as-signed to\nthe target sequences, and associates multiple relevant frames to sequence-level\nlabels (instead of a single frame). In particular, it learns discriminant and\ndomain-invariant feature representations by integrating multiple in-stance\nlearning with deep adversarial DA, where soft Gaussian labels are used to\nefficiently represent the weak ordinal sequence-level labels from the target\ndomain. The proposed approach was validated on the RECOLA video dataset as\nfully-labeled source domain, and UNBC-McMaster video data as weakly-labeled\ntarget domain. We have also validated WSDA-OR on BIOVID and Fatigue (private)\ndatasets for sequence level estimation. Experimental results indicate that our\napproach can provide a significant improvement over the state-of-the-art\nmodels, allowing to achieve a greater localization accuracy.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15675v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.14718v2",
    "title": "Delegated Stochastic Probing",
    "authors": [
      "Curtis Bechtel",
      "Shaddin Dughmi"
    ],
    "author_ids": [],
    "abstract": "Delegation covers a broad class of problems in which a principal doesn't have\nthe resources or expertise necessary to complete a task by themselves, so they\ndelegate the task to an agent whose interests may not be aligned with their\nown. Stochastic probing describes problems in which we are tasked with\nmaximizing expected utility by \"probing\" known distributions for acceptable\nsolutions subject to certain constraints. In this work, we combine the concepts\nof delegation and stochastic probing into a single mechanism design framework\nwhich we term delegated stochastic probing. We study how much a principal loses\nby delegating a stochastic probing problem, compared to their utility in the\nnon-delegated solution. Our model and results are heavily inspired by the work\nof Kleinberg and Kleinberg in \"Delegated Search Approximates Efficient Search.\"\nBuilding on their work, we show that there exists a connection between\ndelegated stochastic probing and generalized prophet inequalities, which\nprovides us with constant-factor deterministic mechanisms for a large class of\ndelegated stochastic probing problems. We also explore randomized mechanisms in\na simple delegated probing setting, and show that they outperform deterministic\nmechanisms in some instances but not in the worst case.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14718v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.14680v2",
    "title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices",
    "authors": [
      "Arash Tavakoli",
      "Mehdi Fatemi",
      "Petar Kormushev"
    ],
    "author_ids": [],
    "abstract": "Action-value estimation is a critical component of many reinforcement\nlearning (RL) methods whereby sample complexity relies heavily on how fast a\ngood estimator for action value can be learned. By viewing this problem through\nthe lens of representation learning, good representations of both state and\naction can facilitate action-value estimation. While advances in deep learning\nhave seamlessly driven progress in learning state representations, given the\nspecificity of the notion of agency to RL, little attention has been paid to\nlearning action representations. We conjecture that leveraging the\ncombinatorial structure of multi-dimensional action spaces is a key ingredient\nfor learning good representations of action. To test this, we set forth the\naction hypergraph networks framework -- a class of functions for learning\naction representations in multi-dimensional discrete action spaces with a\nstructural inductive bias. Using this framework we realise an agent class based\non a combination with deep Q-networks, which we dub hypergraph Q-networks. We\nshow the effectiveness of our approach on a myriad of domains: illustrative\nprediction problems under minimal confounding effects, Atari 2600 games, and\ndiscretised physical control benchmarks.",
    "published_date": "2020-10-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14680v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.14534v1",
    "title": "Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias",
    "authors": [
      "Marion Bartl",
      "Malvina Nissim",
      "Albert Gatt"
    ],
    "author_ids": [],
    "abstract": "Contextualized word embeddings have been replacing standard embeddings as the\nrepresentational knowledge source of choice in NLP systems. Since a variety of\nbiases have previously been found in standard word embeddings, it is crucial to\nassess biases encoded in their replacements as well. Focusing on BERT (Devlin\net al., 2018), we measure gender bias by studying associations between\ngender-denoting target words and names of professions in English and German,\ncomparing the findings with real-world workforce statistics. We mitigate bias\nby fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying\nCounterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that\nour method of measuring bias is appropriate for languages such as English, but\nnot for languages with a rich morphology and gender-marking, such as German.\nOur results highlight the importance of investigating bias and mitigation\ntechniques cross-linguistically, especially in view of the current emphasis on\nlarge-scale, multilingual language models.",
    "published_date": "2020-10-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.14531v2",
    "title": "Assessing Viewpoint Diversity in Search Results Using Ranking Fairness Metrics",
    "authors": [
      "Tim Draws",
      "Nava Tintarev",
      "Ujwal Gadiraju",
      "Alessandro Bozzon",
      "Benjamin Timmermans"
    ],
    "author_ids": [],
    "abstract": "The way pages are ranked in search results influences whether the users of\nsearch engines are exposed to more homogeneous, or rather to more diverse\nviewpoints. However, this viewpoint diversity is not trivial to assess. In this\npaper we use existing and novel ranking fairness metrics to evaluate viewpoint\ndiversity in search result rankings. We conduct a controlled simulation study\nthat shows how ranking fairness metrics can be used for viewpoint diversity,\nhow their outcome should be interpreted, and which metric is most suitable\ndepending on the situation. This paper lays out important ground work for\nfuture research to measure and assess viewpoint diversity in real search result\nrankings.",
    "published_date": "2020-10-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14531v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.14465v4",
    "title": "Evaluating Gender Bias in Speech Translation",
    "authors": [
      "Marta R. Costa-jussà",
      "Christine Basta",
      "Gerard I. Gállego"
    ],
    "author_ids": [],
    "abstract": "The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n  This paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.",
    "published_date": "2020-10-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14465v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.14445v1",
    "title": "Revolutionizing Medical Data Sharing Using Advanced Privacy Enhancing Technologies: Technical, Legal and Ethical Synthesis",
    "authors": [
      "James Scheibner",
      "Jean Louis Raisaro",
      "Juan Ramón Troncoso-Pastoriza",
      "Marcello Ienca",
      "Jacques Fellay",
      "Effy Vayena",
      "Jean-Pierre Hubaux"
    ],
    "author_ids": [],
    "abstract": "Multisite medical data sharing is critical in modern clinical practice and\nmedical research. The challenge is to conduct data sharing that preserves\nindividual privacy and data usability. The shortcomings of traditional\nprivacy-enhancing technologies mean that institutions rely on bespoke data\nsharing contracts. These contracts increase the inefficiency of data sharing\nand may disincentivize important clinical treatment and medical research. This\npaper provides a synthesis between two novel advanced privacy enhancing\ntechnologies (PETs): Homomorphic Encryption and Secure Multiparty Computation\n(defined together as Multiparty Homomorphic Encryption or MHE). These PETs\nprovide a mathematical guarantee of privacy, with MHE providing a performance\nadvantage over separately using HE or SMC. We argue MHE fulfills legal\nrequirements for medical data sharing under the General Data Protection\nRegulation (GDPR) which has set a global benchmark for data protection.\nSpecifically, the data processed and shared using MHE can be considered\nanonymized data. We explain how MHE can reduce the reliance on customized\ncontractual measures between institutions. The proposed approach can accelerate\nthe pace of medical research whilst offering additional incentives for\nhealthcare and research institutes to employ common data interoperability\nstandards.",
    "published_date": "2020-10-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14445v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.14022v2",
    "title": "ByteCover: Cover Song Identification via Multi-Loss Training",
    "authors": [
      "Xingjian Du",
      "Zhesong Yu",
      "Bilei Zhu",
      "Xiaoou Chen",
      "Zejun Ma"
    ],
    "author_ids": [],
    "abstract": "We present in this paper ByteCover, which is a new feature learning method\nfor cover song identification (CSI). ByteCover is built based on the classical\nResNet model, and two major improvements are designed to further enhance the\ncapability of the model for CSI. In the first improvement, we introduce the\nintegration of instance normalization (IN) and batch normalization (BN) to\nbuild IBN blocks, which are major components of our ResNet-IBN model. With the\nhelp of the IBN blocks, our CSI model can learn features that are invariant to\nthe changes of musical attributes such as key, tempo, timbre and genre, while\npreserving the version information. In the second improvement, we employ the\nBNNeck method to allow a multi-loss training and encourage our method to\njointly optimize a classification loss and a triplet loss, and by this means,\nthe inter-class discrimination and intra-class compactness of cover songs, can\nbe ensured at the same time. A set of experiments demonstrated the\neffectiveness and efficiency of ByteCover on multiple datasets, and in the\nDa-TACOS dataset, ByteCover outperformed the best competitive system by 20.9\\%.",
    "published_date": "2020-10-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14022v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13952v1",
    "title": "An Adversarial Domain Separation Framework for Septic Shock Early Prediction Across EHR Systems",
    "authors": [
      "Farzaneh Khoshnevisan",
      "Min Chi"
    ],
    "author_ids": [],
    "abstract": "Modeling patient disease progression using Electronic Health Records (EHRs)\nis critical to assist clinical decision making. While most of prior work has\nmainly focused on developing effective disease progression models using EHRs\ncollected from an individual medical system, relatively little work has\ninvestigated building robust yet generalizable diagnosis models across\ndifferent systems. In this work, we propose a general domain adaptation (DA)\nframework that tackles two categories of discrepancies in EHRs collected from\ndifferent medical systems: one is caused by heterogeneous patient populations\n(covariate shift) and the other is caused by variations in data collection\nprocedures (systematic bias). Prior research in DA has mainly focused on\naddressing covariate shift but not systematic bias. In this work, we propose an\nadversarial domain separation framework that addresses both categories of\ndiscrepancies by maintaining one globally-shared invariant latent\nrepresentation across all systems} through an adversarial learning process,\nwhile also allocating a domain-specific model for each system to extract local\nlatent representations that cannot and should not be unified across systems.\nMoreover, our proposed framework is based on variational recurrent neural\nnetwork (VRNN) because of its ability to capture complex temporal dependencies\nand handling missing values in time-series data. We evaluate our framework for\nearly diagnosis of an extremely challenging condition, septic shock, using two\nreal-world EHRs from distinct medical systems in the U.S. The results show that\nby separating globally-shared from domain-specific representations, our\nframework significantly improves septic shock early prediction performance in\nboth EHRs and outperforms the current state-of-the-art DA models.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13952v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13949v2",
    "title": "KFC: A Scalable Approximation Algorithm for $k$-center Fair Clustering",
    "authors": [
      "Elfarouk Harb",
      "Ho Shan Lam"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the problem of fair clustering on the $k-$center\nobjective. In fair clustering, the input is $N$ points, each belonging to at\nleast one of $l$ protected groups, e.g. male, female, Asian, Hispanic. The\nobjective is to cluster the $N$ points into $k$ clusters to minimize a\nclassical clustering objective function. However, there is an additional\nconstraint that each cluster needs to be fair, under some notion of fairness.\nThis ensures that no group is either \"over-represented\" or \"under-represented\"\nin any cluster. Our work builds on the work of Chierichetti et al. (NIPS 2017),\nBera et al. (NeurIPS 2019), Ahmadian et al. (KDD 2019), and Bercea et al.\n(APPROX 2019). We obtain a randomized $3-$approximation algorithm for the\n$k-$center objective function, beating the previous state of the art\n($4-$approximation). We test our algorithm on real datasets, and show that our\nalgorithm is effective in finding good clusters without over-representation or\nunder-representation, surpassing the current state of the art in runtime speed,\nclustering cost, while achieving similar fairness violations.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13949v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13933v4",
    "title": "Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models",
    "authors": [
      "Jason W. Rocks",
      "Pankaj Mehta"
    ],
    "author_ids": [],
    "abstract": "The bias-variance trade-off is a central concept in supervised learning. In\nclassical statistics, increasing the complexity of a model (e.g., number of\nparameters) reduces bias but also increases variance. Until recently, it was\ncommonly believed that optimal performance is achieved at intermediate model\ncomplexities which strike a balance between bias and variance. Modern Deep\nLearning methods flout this dogma, achieving state-of-the-art performance using\n\"over-parameterized models\" where the number of fit parameters is large enough\nto perfectly fit the training data. As a result, understanding bias and\nvariance in over-parameterized models has emerged as a fundamental problem in\nmachine learning. Here, we use methods from statistical physics to derive\nanalytic expressions for bias and variance in two minimal models of\nover-parameterization (linear regression and two-layer neural networks with\nnonlinear data distributions), allowing us to disentangle properties stemming\nfrom the model architecture and random sampling of data. In both models,\nincreasing the number of fit parameters leads to a phase transition where the\ntraining error goes to zero and the test error diverges as a result of the\nvariance (while the bias remains finite). Beyond this threshold, the test error\nof the two-layer neural network decreases due to a monotonic decrease in\n\\emph{both} the bias and variance in contrast with the classical bias-variance\ntrade-off. We also show that in contrast with classical intuition,\nover-parameterized models can overfit even in the absence of noise and exhibit\nbias even if the student and teacher models match. We synthesize these results\nto construct a holistic understanding of generalization error and the\nbias-variance trade-off in over-parameterized models and relate our results to\nrandom matrix theory.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13933v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13834v1",
    "title": "End-to-End Learning and Intervention in Games",
    "authors": [
      "Jiayang Li",
      "Jing Yu",
      "Yu Marco Nie",
      "Zhaoran Wang"
    ],
    "author_ids": [],
    "abstract": "In a social system, the self-interest of agents can be detrimental to the\ncollective good, sometimes leading to social dilemmas. To resolve such a\nconflict, a central designer may intervene by either redesigning the system or\nincentivizing the agents to change their behaviors. To be effective, the\ndesigner must anticipate how the agents react to the intervention, which is\ndictated by their often unknown payoff functions. Therefore, learning about the\nagents is a prerequisite for intervention. In this paper, we provide a unified\nframework for learning and intervention in games. We cast the equilibria of\ngames as individual layers and integrate them into an end-to-end optimization\nframework. To enable the backward propagation through the equilibria of games,\nwe propose two approaches, respectively based on explicit and implicit\ndifferentiation. Specifically, we cast the equilibria as the solutions to\nvariational inequalities (VIs). The explicit approach unrolls the projection\nmethod for solving VIs, while the implicit approach exploits the sensitivity of\nthe solutions to VIs. At the core of both approaches is the differentiation\nthrough a projection operator. Moreover, we establish the correctness of both\napproaches and identify the conditions under which one approach is more\ndesirable than the other. The analytical results are validated using several\nreal-world problems.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13834v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13816v1",
    "title": "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction",
    "authors": [
      "Xinyao Ma",
      "Maarten Sap",
      "Hannah Rashkin",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "Unconscious biases continue to be prevalent in modern text and media, calling\nfor algorithms that can assist writers with bias correction. For example, a\nfemale character in a story is often portrayed as passive and powerless (\"She\ndaydreams about being a doctor\") while a man is portrayed as more proactive and\npowerful (\"He pursues his dream of being a doctor\").\n  We formulate *Controllable Debiasing*, a new revision task that aims to\nrewrite a given text to correct the implicit and potentially undesirable bias\nin character portrayals. We then introduce PowerTransformer as an approach that\ndebiases text through the lens of connotation frames (Sap et al., 2017), which\nencode pragmatic knowledge of implied power dynamics with respect to verb\npredicates. One key challenge of our task is the lack of parallel corpora. To\naddress this challenge, we adopt an unsupervised approach using auxiliary\nsupervision with related tasks such as paraphrasing and self-supervision based\non a reconstruction loss, building on pretrained language models.\n  Through comprehensive experiments based on automatic and human evaluations,\nwe demonstrate that our approach outperforms ablations and existing methods\nfrom related tasks. Furthermore, we demonstrate the use of PowerTransformer as\na step toward mitigating the well-documented gender bias in character portrayal\nin movie scripts.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13816v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13787v3",
    "title": "Hierarchical Inference With Bayesian Neural Networks: An Application to Strong Gravitational Lensing",
    "authors": [
      "Sebastian Wagner-Carena",
      "Ji Won Park",
      "Simon Birrer",
      "Philip J. Marshall",
      "Aaron Roodman",
      "Risa H. Wechsler"
    ],
    "author_ids": [],
    "abstract": "In the past few years, approximate Bayesian Neural Networks (BNNs) have\ndemonstrated the ability to produce statistically consistent posteriors on a\nwide range of inference problems at unprecedented speed and scale. However, any\ndisconnect between training sets and the distribution of real-world objects can\nintroduce bias when BNNs are applied to data. This is a common challenge in\nastrophysics and cosmology, where the unknown distribution of objects in our\nUniverse is often the science goal. In this work, we incorporate BNNs with\nflexible posterior parameterizations into a hierarchical inference framework\nthat allows for the reconstruction of population hyperparameters and removes\nthe bias introduced by the training distribution. We focus on the challenge of\nproducing posterior PDFs for strong gravitational lens mass model parameters\ngiven Hubble Space Telescope (HST) quality single-filter, lens-subtracted,\nsynthetic imaging data. We show that the posterior PDFs are sufficiently\naccurate (i.e., statistically consistent with the truth) across a wide variety\nof power-law elliptical lens mass distributions. We then apply our approach to\ntest data sets whose lens parameters are drawn from distributions that are\ndrastically different from the training set. We show that our hierarchical\ninference framework mitigates the bias introduced by an unrepresentative\ntraining set's interim prior. Simultaneously, given a sufficiently broad\ntraining set, we can precisely reconstruct the population hyperparameters\ngoverning our test distributions. Our full pipeline, from training to\nhierarchical inference on thousands of lenses, can be run in a day. The\nframework presented here will allow us to efficiently exploit the full\nconstraining power of future ground- and space-based surveys.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "astro-ph.CO",
      "astro-ph.IM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13787v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13494v2",
    "title": "One-vs.-One Mitigation of Intersectional Bias: A General Method to Extend Fairness-Aware Binary Classification",
    "authors": [
      "Kenji Kobayashi",
      "Yuri Nakao"
    ],
    "author_ids": [],
    "abstract": "With the widespread adoption of machine learning in the real world, the\nimpact of the discriminatory bias has attracted attention. In recent years,\nvarious methods to mitigate the bias have been proposed. However, most of them\nhave not considered intersectional bias, which brings unfair situations where\npeople belonging to specific subgroups of a protected group are treated worse\nwhen multiple sensitive attributes are taken into consideration. To mitigate\nthis bias, in this paper, we propose a method called One-vs.-One Mitigation by\napplying a process of comparison between each pair of subgroups related to\nsensitive attributes to the fairness-aware machine learning for binary\nclassification. We compare our method and the conventional fairness-aware\nbinary classification methods in comprehensive settings using three approaches\n(pre-processing, in-processing, and post-processing), six metrics (the ratio\nand difference of demographic parity, equalized odds, and equal opportunity),\nand two real-world datasets (Adult and COMPAS). As a result, our method\nmitigates the intersectional bias much better than conventional methods in all\nthe settings. With the result, we open up the potential of fairness-aware\nbinary classification for solving more realistic problems occurring when there\nare multiple sensitive attributes.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "I.6.5; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13494v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13365v2",
    "title": "Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy",
    "authors": [
      "Philipp Benz",
      "Chaoning Zhang",
      "Adil Karjauv",
      "In So Kweon"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks (CNNs) have made significant advancement,\nhowever, they are widely known to be vulnerable to adversarial attacks.\nAdversarial training is the most widely used technique for improving\nadversarial robustness to strong white-box attacks. Prior works have been\nevaluating and improving the model average robustness without class-wise\nevaluation. The average evaluation alone might provide a false sense of\nrobustness. For example, the attacker can focus on attacking the vulnerable\nclass, which can be dangerous, especially, when the vulnerable class is a\ncritical one, such as \"human\" in autonomous driving. We propose an empirical\nstudy on the class-wise accuracy and robustness of adversarially trained\nmodels. We find that there exists inter-class discrepancy for accuracy and\nrobustness even when the training dataset has an equal number of samples for\neach class. For example, in CIFAR10, \"cat\" is much more vulnerable than other\nclasses. Moreover, this inter-class discrepancy also exists for normally\ntrained models, while adversarial training tends to further increase the\ndiscrepancy. Our work aims to investigate the following questions: (a) is the\nphenomenon of inter-class discrepancy universal regardless of datasets, model\narchitectures and optimization hyper-parameters? (b) If so, what can be\npossible explanations for the inter-class discrepancy? (c) Can the techniques\nproposed in the long tail classification be readily extended to adversarial\ntraining for addressing the inter-class discrepancy?",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13365v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13782v1",
    "title": "Interpretable Assessment of Fairness During Model Evaluation",
    "authors": [
      "Amir Sepehri",
      "Cyrus DiCiccio"
    ],
    "author_ids": [],
    "abstract": "For companies developing products or algorithms, it is important to\nunderstand the potential effects not only globally, but also on sub-populations\nof users. In particular, it is important to detect if there are certain groups\nof users that are impacted differently compared to others with regard to\nbusiness metrics or for whom a model treats unequally along fairness concerns.\nIn this paper, we introduce a novel hierarchical clustering algorithm to detect\nheterogeneity among users in given sets of sub-populations with respect to any\nspecified notion of group similarity. We prove statistical guarantees about the\noutput and provide interpretable results. We demonstrate the performance of the\nalgorithm on real data from LinkedIn.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13782v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13266v2",
    "title": "Biases in Generative Art -- A Causal Look from the Lens of Art History",
    "authors": [
      "Ramya Srinivasan",
      "Kanji Uchino"
    ],
    "author_ids": [],
    "abstract": "With rapid progress in artificial intelligence (AI), popularity of generative\nart has grown substantially. From creating paintings to generating novel art\nstyles, AI based generative art has showcased a variety of applications.\nHowever, there has been little focus concerning the ethical impacts of AI based\ngenerative art. In this work, we investigate biases in the generative art AI\npipeline right from those that can originate due to improper problem\nformulation to those related to algorithm design. Viewing from the lens of art\nhistory, we discuss the socio-cultural impacts of these biases. Leveraging\ncausal models, we highlight how current methods fall short in modeling the\nprocess of art creation and thus contribute to various types of biases. We\nillustrate the same through case studies, in particular those related to style\ntransfer. To the best of our knowledge, this is the first extensive analysis\nthat investigates biases in the generative art AI pipeline from the perspective\nof art history. We hope our work sparks interdisciplinary discussions related\nto accountability of generative art.",
    "published_date": "2020-10-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13266v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13228v1",
    "title": "Unified Gradient Reweighting for Model Biasing with Applications to Source Separation",
    "authors": [
      "Efthymios Tzinis",
      "Dimitrios Bralios",
      "Paris Smaragdis"
    ],
    "author_ids": [],
    "abstract": "Recent deep learning approaches have shown great improvement in audio source\nseparation tasks. However, the vast majority of such work is focused on\nimproving average separation performance, often neglecting to examine or\ncontrol the distribution of the results. In this paper, we propose a simple,\nunified gradient reweighting scheme, with a lightweight modification to bias\nthe learning process of a model and steer it towards a certain distribution of\nresults. More specifically, we reweight the gradient updates of each batch,\nusing a user-specified probability distribution. We apply this method to\nvarious source separation tasks, in order to shift the operating point of the\nmodels towards different objectives. We demonstrate different parameterizations\nof our unified reweighting scheme can be used towards addressing several\nreal-world problems, such as unreliable separation estimates. Our framework\nenables the user to control a robustness trade-off between worst and average\nperformance. Moreover, we experimentally show that our unified reweighting\nscheme can also be used in order to shift the focus of the model towards being\nmore accurate for user-specified sound classes or even towards easier examples\nin order to enable faster convergence.",
    "published_date": "2020-10-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13228v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13168v1",
    "title": "Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings",
    "authors": [
      "Vaibhav Kumar",
      "Tenzin Singhay Bhotia",
      "Vaibhav Kumar"
    ],
    "author_ids": [],
    "abstract": "Non-contextual word embedding models have been shown to inherit human-like\nstereotypical biases of gender, race and religion from the training corpora. To\ncounter this issue, a large body of research has emerged which aims to mitigate\nthese biases while keeping the syntactic and semantic utility of embeddings\nintact. This paper describes Fair Embedding Engine (FEE), a library for\nanalysing and mitigating gender bias in word embeddings. FEE combines various\nstate of the art techniques for quantifying, visualising and mitigating gender\nbias in word embeddings under a standard abstraction. FEE will aid\npractitioners in fast track analysis of existing debiasing methods on their\nembedding models. Further, it will allow rapid prototyping of new methods by\nevaluating their performance on a suite of standard metrics.",
    "published_date": "2020-10-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.13013v2",
    "title": "Tractable contextual bandits beyond realizability",
    "authors": [
      "Sanath Kumar Krishnamurthy",
      "Vitor Hadad",
      "Susan Athey"
    ],
    "author_ids": [],
    "abstract": "Tractable contextual bandit algorithms often rely on the realizability\nassumption - i.e., that the true expected reward model belongs to a known\nclass, such as linear functions. In this work, we present a tractable bandit\nalgorithm that is not sensitive to the realizability assumption and\ncomputationally reduces to solving a constrained regression problem in every\nepoch. When realizability does not hold, our algorithm ensures the same\nguarantees on regret achieved by realizability-based algorithms under\nrealizability, up to an additive term that accounts for the misspecification\nerror. This extra term is proportional to T times a function of the mean\nsquared error between the best model in the class and the true model, where T\nis the total number of time-steps. Our work sheds light on the bias-variance\ntrade-off for tractable contextual bandits. This trade-off is not captured by\nalgorithms that assume realizability, since under this assumption there exists\nan estimator in the class that attains zero bias.",
    "published_date": "2020-10-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.13013v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12919v5",
    "title": "Causal Effects of Linguistic Properties",
    "authors": [
      "Reid Pryzant",
      "Dallas Card",
      "Dan Jurafsky",
      "Victor Veitch",
      "Dhanya Sridhar"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of using observational data to estimate the causal\neffects of linguistic properties. For example, does writing a complaint\npolitely lead to a faster response time? How much will a positive product\nreview increase sales? This paper addresses two technical challenges related to\nthe problem before developing a practical method. First, we formalize the\ncausal quantity of interest as the effect of a writer's intent, and establish\nthe assumptions necessary to identify this from observational data. Second, in\npractice, we only have access to noisy proxies for the linguistic properties of\ninterest -- e.g., predictions from classifiers and lexicons. We propose an\nestimator for this setting and prove that its bias is bounded when we perform\nan adjustment for the text. Based on these results, we introduce TextCause, an\nalgorithm for estimating causal effects of linguistic properties. The method\nleverages (1) distant supervision to improve the quality of noisy proxies, and\n(2) a pre-trained language model (BERT) to adjust for the text. We show that\nthe proposed method outperforms related approaches when estimating the effect\nof Amazon review sentiment on semi-simulated sales figures. Finally, we present\nan applied case study investigating the effects of complaint politeness on\nbureaucratic response times.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12919v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12913v1",
    "title": "Classifying Eye-Tracking Data Using Saliency Maps",
    "authors": [
      "Shafin Rahman",
      "Sejuti Rahman",
      "Omar Shahid",
      "Md. Tahmeed Abdullah",
      "Jubair Ahmed Sourov"
    ],
    "author_ids": [],
    "abstract": "A plethora of research in the literature shows how human eye fixation pattern\nvaries depending on different factors, including genetics, age, social\nfunctioning, cognitive functioning, and so on. Analysis of these variations in\nvisual attention has already elicited two potential research avenues: 1)\ndetermining the physiological or psychological state of the subject and 2)\npredicting the tasks associated with the act of viewing from the recorded\neye-fixation data. To this end, this paper proposes a visual saliency based\nnovel feature extraction method for automatic and quantitative classification\nof eye-tracking data, which is applicable to both of the research directions.\nInstead of directly extracting features from the fixation data, this method\nemploys several well-known computational models of visual attention to predict\neye fixation locations as saliency maps. Comparing the saliency amplitudes,\nsimilarity and dissimilarity of saliency maps with the corresponding eye\nfixations maps gives an extra dimension of information which is effectively\nutilized to generate discriminative features to classify the eye-tracking data.\nExtensive experimentation using Saliency4ASD, Age Prediction, and Visual\nPerceptual Task dataset show that our saliency-based feature can achieve\nsuperior performance, outperforming the previous state-of-the-art methods by a\nconsiderable margin. Moreover, unlike the existing application-specific\nsolutions, our method demonstrates performance improvement across three\ndistinct problems from the real-life domain: Autism Spectrum Disorder\nscreening, toddler age prediction, and human visual perceptual task\nclassification, providing a general paradigm that utilizes the\nextra-information inherent in saliency maps for a more accurate classification.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12913v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.14624v1",
    "title": "On Fair Virtual Conference Scheduling: Achieving Equitable Participant and Speaker Satisfaction",
    "authors": [
      "Gourab K Patro",
      "Abhijnan Chakraborty",
      "Niloy Ganguly",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "The (COVID-19) pandemic-induced restrictions on travel and social gatherings\nhave prompted most conference organizers to move their events online. However,\nin contrast to physical conferences, virtual conferences face a challenge in\nefficiently scheduling talks, accounting for the availability of participants\nfrom different time-zones as well as their interests in attending different\ntalks. In such settings, a natural objective for the conference organizers\nwould be to maximize some global welfare measure, such as the total expected\naudience participation across all talks. However, we show that optimizing for\nglobal welfare could result in a schedule that is unfair to the stakeholders,\ni.e., the individual utilities for participants and speakers can be highly\nunequal. To address the fairness concerns, we formally define fairness notions\nfor participants and speakers, and subsequently derive suitable fairness\nobjectives for them. We show that the welfare and fairness objectives can be in\nconflict with each other, and there is a need to maintain a balance between\nthese objective while caring for them simultaneously. Thus, we propose a joint\noptimization framework that allows conference organizers to design talk\nschedules that balance (i.e., allow trade-offs) between global welfare,\nparticipant fairness and the speaker fairness objectives. We show that the\noptimization problem can be solved using integer linear programming, and\nempirically evaluate the necessity and benefits of such joint optimization\napproach in virtual conference scheduling.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.14624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12909v3",
    "title": "Inductive Bias of Gradient Descent for Weight Normalized Smooth Homogeneous Neural Nets",
    "authors": [
      "Depen Morwani",
      "Harish G. Ramaswamy"
    ],
    "author_ids": [],
    "abstract": "We analyze the inductive bias of gradient descent for weight normalized\nsmooth homogeneous neural nets, when trained on exponential or cross-entropy\nloss. We analyse both standard weight normalization (SWN) and exponential\nweight normalization (EWN), and show that the gradient flow path with EWN is\nequivalent to gradient flow on standard networks with an adaptive learning\nrate. We extend these results to gradient descent, and establish asymptotic\nrelations between weights and gradients for both SWN and EWN. We also show that\nEWN causes weights to be updated in a way that prefers asymptotic relative\nsparsity. For EWN, we provide a finite-time convergence rate of the loss with\ngradient flow and a tight asymptotic convergence rate with gradient descent. We\ndemonstrate our results for SWN and EWN on synthetic data sets. Experimental\nresults on simple datasets support our claim on sparse EWN solutions, even with\nSGD. This demonstrates its potential applications in learning neural networks\namenable to pruning.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML",
      "I.5.1; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12909v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12888v1",
    "title": "Discriminative feature generation for classification of imbalanced data",
    "authors": [
      "Sungho Suh",
      "Paul Lukowicz",
      "Yong Oh Lee"
    ],
    "author_ids": [],
    "abstract": "The data imbalance problem is a frequent bottleneck in the classification\nperformance of neural networks. In this paper, we propose a novel supervised\ndiscriminative feature generation (DFG) method for a minority class dataset.\nDFG is based on the modified structure of a generative adversarial network\nconsisting of four independent networks: generator, discriminator, feature\nextractor, and classifier. To augment the selected discriminative features of\nthe minority class data by adopting an attention mechanism, the generator for\nthe class-imbalanced target task is trained, and the feature extractor and\nclassifier are regularized using the pre-trained features from a large source\ndata. The experimental results show that the DFG generator enhances the\naugmentation of the label-preserved and diverse features, and the\nclassification results are significantly improved on the target task. The\nfeature generation model can contribute greatly to the development of data\naugmentation methods through discriminative feature generation and supervised\nattention methods.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12888v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12864v2",
    "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning",
    "authors": [
      "Xisen Jin",
      "Francesco Barbieri",
      "Brendan Kennedy",
      "Aida Mostafazadeh Davani",
      "Leonardo Neves",
      "Xiang Ren"
    ],
    "author_ids": [],
    "abstract": "Fine-tuned language models have been shown to exhibit biases against\nprotected groups in a host of modeling tasks such as text classification and\ncoreference resolution. Previous works focus on detecting these biases,\nreducing bias in data representations, and using auxiliary training objectives\nto mitigate bias during fine-tuning. Although these techniques achieve bias\nreduction for the task and domain at hand, the effects of bias mitigation may\nnot directly transfer to new tasks, requiring additional data collection and\ncustomized annotation of sensitive attributes, and re-evaluation of appropriate\nfairness metrics. We explore the feasibility and benefits of upstream bias\nmitigation (UBM) for reducing bias on downstream tasks, by first applying bias\nmitigation to an upstream model through fine-tuning and subsequently using it\nfor downstream fine-tuning. We find, in extensive experiments across hate\nspeech detection, toxicity detection, occupation prediction, and coreference\nresolution tasks over various bias factors, that the effects of UBM are indeed\ntransferable to new downstream tasks or domains via fine-tuning, creating less\nbiased downstream models than directly fine-tuning on the downstream task or\ntransferring from a vanilla upstream model. Though challenges remain, we show\nthat UBM promises more efficient and accessible bias mitigation in LM\nfine-tuning.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12864v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12779v1",
    "title": "Fair Hate Speech Detection through Evaluation of Social Group Counterfactuals",
    "authors": [
      "Aida Mostafazadeh Davani",
      "Ali Omrani",
      "Brendan Kennedy",
      "Mohammad Atari",
      "Xiang Ren",
      "Morteza Dehghani"
    ],
    "author_ids": [],
    "abstract": "Approaches for mitigating bias in supervised models are designed to reduce\nmodels' dependence on specific sensitive features of the input data, e.g.,\nmentioned social groups. However, in the case of hate speech detection, it is\nnot always desirable to equalize the effects of social groups because of their\nessential role in distinguishing outgroup-derogatory hate, such that particular\ntypes of hateful rhetoric carry the intended meaning only when contextualized\naround certain social group tokens. Counterfactual token fairness for a\nmentioned social group evaluates the model's predictions as to whether they are\nthe same for (a) the actual sentence and (b) a counterfactual instance, which\nis generated by changing the mentioned social group in the sentence. Our\napproach assures robust model predictions for counterfactuals that imply\nsimilar meaning as the actual sentence. To quantify the similarity of a\nsentence and its counterfactual, we compare their likelihood score calculated\nby generative language models. By equalizing model behaviors on each sentence\nand its counterfactuals, we mitigate bias in the proposed model while\npreserving the overall classification performance.",
    "published_date": "2020-10-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12779v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12687v1",
    "title": "Robust Correction of Sampling Bias Using Cumulative Distribution Functions",
    "authors": [
      "Bijan Mazaheri",
      "Siddharth Jain",
      "Jehoshua Bruck"
    ],
    "author_ids": [],
    "abstract": "Varying domains and biased datasets can lead to differences between the\ntraining and the target distributions, known as covariate shift. Current\napproaches for alleviating this often rely on estimating the ratio of training\nand target probability density functions. These techniques require parameter\ntuning and can be unstable across different datasets. We present a new method\nfor handling covariate shift using the empirical cumulative distribution\nfunction estimates of the target distribution by a rigorous generalization of a\nrecent idea proposed by Vapnik and Izmailov. Further, we show experimentally\nthat our method is more robust in its predictions, is not reliant on parameter\ntuning and shows similar classification performance compared to the current\nstate-of-the-art techniques on synthetic and real datasets.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12687v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15590v1",
    "title": "Enjeux éthiques de l'IA en santé : une humanisation du parcours de soin par l'intelligence artificielle ?",
    "authors": [
      "Fabrice Muhlenbach"
    ],
    "author_ids": [],
    "abstract": "Considering the use of artificial intelligence for greater personalization of\npatient care and better management of human and material resources may seem\nlike an opportunity not to be missed. In order to offer a better humanization\nof the care pathway, artificial intelligence is a tool that decision-makers in\nthe hospital sector must appropriate by taking care of the new ethical issues\nand conflicts of values that this technology generates.\n  Envisager le recours \\`a l'intelligence artificielle pour une plus grande\npersonnalisation de la prise en charge du patient et une meilleure gestion des\nressources humaines et mat\\'erielles peut sembler une opportunit\\'e \\`a ne pas\nmanquer. Afin de proposer une meilleure humanisation du parcours de soin,\nl'intelligence artificielle est un outil que les d\\'ecideurs du milieu\nhospitalier doivent s'approprier en veillant aux nouveaux enjeux \\'ethiques et\nconflits de valeurs que cette technologie engendre.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "I.2; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15590v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12611v2",
    "title": "Information access representations and social capital in networks",
    "authors": [
      "Ashkan Bashardoust",
      "Hannah C. Beilinson",
      "Sorelle A. Friedler",
      "Jiajie Ma",
      "Jade Rousseau",
      "Carlos E. Scheidegger",
      "Blair D. Sullivan",
      "Nasanbayar Ulzii-Orshikh",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "Social network position confers power and social capital. In the setting of\nonline social networks that have massive reach, creating mathematical\nrepresentations of social capital is an important step towards understanding\nhow network position can differentially confer advantage to different groups\nand how network position can itself be a source of advantage. In this paper, we\nuse well established models for information flow on networks as a base to\npropose a formal descriptor of the network position of a node as represented by\nits information access. Combining these descriptors allows a full\nrepresentation of social capital across the network. Using real-world networks,\nwe demonstrate that this representation allows the identification of\ndifferences between groups based on network specific measures of inequality of\naccess.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12611v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.12510v1",
    "title": "Improving Robustness by Augmenting Training Sentences with Predicate-Argument Structures",
    "authors": [
      "Nafise Sadat Moosavi",
      "Marcel de Boer",
      "Prasetya Ajie Utama",
      "Iryna Gurevych"
    ],
    "author_ids": [],
    "abstract": "Existing NLP datasets contain various biases, and models tend to quickly\nlearn those biases, which in turn limits their robustness. Existing approaches\nto improve robustness against dataset biases mostly focus on changing the\ntraining objective so that models learn less from biased examples. Besides,\nthey mostly focus on addressing a specific bias, and while they improve the\nperformance on adversarial evaluation sets of the targeted bias, they may bias\nthe model in other ways, and therefore, hurt the overall robustness. In this\npaper, we propose to augment the input sentences in the training data with\ntheir corresponding predicate-argument structures, which provide a higher-level\nabstraction over different realizations of the same meaning and help the model\nto recognize important parts of sentences. We show that without targeting a\nspecific bias, our sentence augmentation improves the robustness of transformer\nmodels against multiple biases. In addition, we show that models can still be\nvulnerable to the lexical overlap bias, even when the training data does not\ncontain this bias, and that the sentence augmentation also improves the\nrobustness in this scenario. We will release our adversarial datasets to\nevaluate bias in such a scenario as well as our augmentation scripts at\nhttps://github.com/UKPLab/data-augmentation-for-robustness.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12510v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12473v1",
    "title": "Intrinsic Quality Assessment of Arguments",
    "authors": [
      "Henning Wachsmuth",
      "Till Werner"
    ],
    "author_ids": [],
    "abstract": "Several quality dimensions of natural language arguments have been\ninvestigated. Some are likely to be reflected in linguistic features (e.g., an\nargument's arrangement), whereas others depend on context (e.g., relevance) or\ntopic knowledge (e.g., acceptability). In this paper, we study the intrinsic\ncomputational assessment of 15 dimensions, i.e., only learning from an\nargument's text. In systematic experiments with eight feature types on an\nexisting corpus, we observe moderate but significant learning success for most\ndimensions. Rhetorical quality seems hardest to assess, and subjectivity\nfeatures turn out strong, although length bias in the corpus impedes full\nvalidity. We also find that human assessors differ more clearly to each other\nthan to our approach.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12473v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12337v1",
    "title": "Fusion of Dual Spatial Information for Hyperspectral Image Classification",
    "authors": [
      "Puhong Duan",
      "Pedram Ghamisi",
      "Xudong Kang",
      "Behnood Rasti",
      "Shutao Li",
      "Richard Gloaguen"
    ],
    "author_ids": [],
    "abstract": "The inclusion of spatial information into spectral classifiers for\nfine-resolution hyperspectral imagery has led to significant improvements in\nterms of classification performance. The task of spectral-spatial hyperspectral\nimage classification has remained challenging because of high intraclass\nspectrum variability and low interclass spectral variability. This fact has\nmade the extraction of spatial information highly active. In this work, a novel\nhyperspectral image classification framework using the fusion of dual spatial\ninformation is proposed, in which the dual spatial information is built by both\nexploiting pre-processing feature extraction and post-processing spatial\noptimization. In the feature extraction stage, an adaptive texture smoothing\nmethod is proposed to construct the structural profile (SP), which makes it\npossible to precisely extract discriminative features from hyperspectral\nimages. The SP extraction method is used here for the first time in the remote\nsensing community. Then, the extracted SP is fed into a spectral classifier. In\nthe spatial optimization stage, a pixel-level classifier is used to obtain the\nclass probability followed by an extended random walker-based spatial\noptimization technique. Finally, a decision fusion rule is utilized to fuse the\nclass probabilities obtained by the two different stages. Experiments performed\non three data sets from different scenes illustrate that the proposed method\ncan outperform other state-of-the-art classification techniques. In addition,\nthe proposed feature extraction method, i.e., SP, can effectively improve the\ndiscrimination between different land covers.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.IT",
      "eess.IV",
      "math.IT",
      "68T45",
      "J.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12337v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12326v2",
    "title": "Robust Footstep Planning and LQR Control for Dynamic Quadrupedal Locomotion",
    "authors": [
      "Guiyang Xin",
      "Songyan Xin",
      "Oguzhan Cebe",
      "Mathew Jose Pollayil",
      "Franco Angelini",
      "Manolo Garabini",
      "Sethu Vijayakumar",
      "Michael Mistry"
    ],
    "author_ids": [],
    "abstract": "In this paper, we aim to improve the robustness of dynamic quadrupedal\nlocomotion through two aspects: 1) fast model predictive foothold planning, and\n2) applying LQR to projected inverse dynamic control for robust motion\ntracking. In our proposed planning and control framework, foothold plans are\nupdated at 400 Hz considering the current robot state and an LQR controller\ngenerates optimal feedback gains for motion tracking. The LQR optimal gain\nmatrix with non-zero off-diagonal elements leverages the coupling of dynamics\nto compensate for system underactuation. Meanwhile, the projected inverse\ndynamic control complements the LQR to satisfy inequality constraints. In\naddition to these contributions, we show robustness of our control framework to\nunmodeled adaptive feet. Experiments on the quadruped ANYmal demonstrate the\neffectiveness of the proposed method for robust dynamic locomotion given\nexternal disturbances and environmental uncertainties.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12326v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.12286v1",
    "title": "Unbiased Estimation Equation under $f$-Separable Bregman Distortion Measures",
    "authors": [
      "Masahiro Kobayashi",
      "Kazuho Watanabe"
    ],
    "author_ids": [],
    "abstract": "We discuss unbiased estimation equations in a class of objective function\nusing a monotonically increasing function $f$ and Bregman divergence. The\nchoice of the function $f$ gives desirable properties such as robustness\nagainst outliers. In order to obtain unbiased estimation equations,\nanalytically intractable integrals are generally required as bias correction\nterms. In this study, we clarify the combination of Bregman divergence,\nstatistical model, and function $f$ in which the bias correction term vanishes.\nFocusing on Mahalanobis and Itakura-Saito distances, we provide a\ngeneralization of fundamental existing results and characterize a class of\ndistributions of positive reals with a scale parameter, which includes the\ngamma distribution as a special case. We discuss the possibility of latent bias\nminimization when the proportion of outliers is large, which is induced by the\nextinction of the bias correction term.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12286v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12269v5",
    "title": "Reducing Bias in Modeling Real-world Password Strength via Deep Learning and Dynamic Dictionaries",
    "authors": [
      "Dario Pasquini",
      "Marco Cianfriglia",
      "Giuseppe Ateniese",
      "Massimo Bernaschi"
    ],
    "author_ids": [],
    "abstract": "Password security hinges on an in-depth understanding of the techniques\nadopted by attackers. Unfortunately, real-world adversaries resort to pragmatic\nguessing strategies such as dictionary attacks that are inherently difficult to\nmodel in password security studies. In order to be representative of the actual\nthreat, dictionary attacks must be thoughtfully configured and tuned. However,\nthis process requires a domain-knowledge and expertise that cannot be easily\nreplicated. The consequence of inaccurately calibrating dictionary attacks is\nthe unreliability of password security analyses, impaired by a severe\nmeasurement bias.\n  In the present work, we introduce a new generation of dictionary attacks that\nis consistently more resilient to inadequate configurations. Requiring no\nsupervision or domain-knowledge, this technique automatically approximates the\nadvanced guessing strategies adopted by real-world attackers. To achieve this:\n(1) We use deep neural networks to model the proficiency of adversaries in\nbuilding attack configurations. (2) Then, we introduce dynamic guessing\nstrategies within dictionary attacks. These mimic experts' ability to adapt\ntheir guessing strategies on the fly by incorporating knowledge on their\ntargets.\n  Our techniques enable more robust and sound password strength estimates\nwithin dictionary attacks, eventually reducing overestimation in modeling\nreal-world threats in password security. Code available:\nhttps://github.com/TheAdamProject/adams",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12269v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12184v3",
    "title": "Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation",
    "authors": [
      "Taotao Jing",
      "Bingrong Xu",
      "Jingjing Li",
      "Zhengming Ding"
    ],
    "author_ids": [],
    "abstract": "Domain adaptation (DA) becomes an up-and-coming technique to address the\ninsufficient or no annotation issue by exploiting external source knowledge.\nExisting DA algorithms mainly focus on practical knowledge transfer through\ndomain alignment. Unfortunately, they ignore the fairness issue when the\nauxiliary source is extremely imbalanced across different categories, which\nresults in severe under-presented knowledge adaptation of minority source set.\nTo this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to\nhandle the fairness challenge in imbalanced cross-domain learning.\nSpecifically, a novel cross-domain mixup generation is exploited to augment the\nminority source set with target information to enhance fairness. Moreover, dual\ndistinct classifiers and cross-domain prototype alignment are developed to seek\na more robust classifier boundary and mitigate the domain shift. Such three\nstrategies are formulated into a unified framework to address the fairness\nissue and domain shift challenge. Extensive experiments over two popular\nbenchmarks have verified the effectiveness of our proposed model by comparing\nto existing state-of-the-art DA models, and especially our model significantly\nimproves over 20% on two benchmarks in terms of the overall accuracy.",
    "published_date": "2020-10-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12184v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12102v1",
    "title": "Achieving User-Side Fairness in Contextual Bandits",
    "authors": [
      "Wen Huang",
      "Kevin Labille",
      "Xintao Wu",
      "Dongwon Lee",
      "Neil Heffernan"
    ],
    "author_ids": [],
    "abstract": "Personalized recommendation based on multi-arm bandit (MAB) algorithms has\nshown to lead to high utility and efficiency as it can dynamically adapt the\nrecommendation strategy based on feedback. However, unfairness could incur in\npersonalized recommendation. In this paper, we study how to achieve user-side\nfairness in personalized recommendation. We formulate our fair personalized\nrecommendation as a modified contextual bandit and focus on achieving fairness\non the individual whom is being recommended an item as opposed to achieving\nfairness on the items that are being recommended. We introduce and define a\nmetric that captures the fairness in terms of rewards received for both the\nprivileged and protected groups. We develop a fair contextual bandit algorithm,\nFair-LinUCB, that improves upon the traditional LinUCB algorithm to achieve\ngroup-level fairness of users. Our algorithm detects and monitors unfairness\nwhile it learns to recommend personalized videos to students to achieve high\nefficiency. We provide a theoretical regret analysis and show that our\nalgorithm has a slightly higher regret bound than LinUCB. We conduct numerous\nexperimental evaluations to compare the performances of our fair contextual\nbandit to that of LinUCB and show that our approach achieves group-level\nfairness while maintaining a high utility.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12102v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12089v1",
    "title": "The Pursuit of Algorithmic Fairness: On \"Correcting\" Algorithmic Unfairness in a Child Welfare Reunification Success Classifier",
    "authors": [
      "Jordan Purdy",
      "Brian Glass"
    ],
    "author_ids": [],
    "abstract": "The algorithmic fairness of predictive analytic tools in the public sector\nhas increasingly become a topic of rigorous exploration. While instruments\npertaining to criminal recidivism and academic admissions, for example, have\ngarnered much attention, the predictive instruments of Child Welfare\njurisdictions have received considerably less attention. This is in part\nbecause comparatively few such instruments exist and because even fewer have\nbeen scrutinized through the lens of algorithmic fairness. In this work, we\nseek to address both of these gaps. To this end, a novel classification\nalgorithm for predicting reunification success within Oregon Child Welfare is\npresented, including all of the relevant details associated with building such\nan instrument. The purpose of this tool is to maximize the number of stable\nreunifications and identify potentially unstable reunifications which may\nrequire additional resources and scrutiny. Additionally, because the\nalgorithmic fairness of the resulting tool, if left unaltered, is\nunquestionably lacking, the utilized procedure for mitigating such unfairness\nis presented, along with the rationale behind each difficult and unavoidable\nchoice. This procedure, though similar to other post-processing group-specific\nthresholding methods, is novel in its use of a penalized optimizer and\ncontextually requisite subsampling. These novel methodological components yield\na rich and informative empirical understanding of the trade-off continuum\nbetween fairness and accuracy. As the developed procedure is generalizable\nacross a variety of group-level definitions of algorithmic fairness, as well as\nacross an arbitrary number of protected attribute levels and risk thresholds,\nthe approach is broadly applicable both within and beyond Child Welfare.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12089v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15578v1",
    "title": "Exploring the Nuances of Designing (with/for) Artificial Intelligence",
    "authors": [
      "Niya Stoimenova",
      "Rebecca Price"
    ],
    "author_ids": [],
    "abstract": "Solutions relying on artificial intelligence are devised to predict data\npatterns and answer questions that are clearly defined, involve an enumerable\nset of solutions, clear rules, and inherently binary decision mechanisms. Yet,\nas they become exponentially implemented in our daily activities, they begin to\ntranscend these initial boundaries and to affect the larger sociotechnical\nsystem in which they are situated. In this arrangement, a solution is under\npressure to surpass true or false criteria and move to an ethical evaluation of\nright and wrong. Neither algorithmic solutions, nor purely humanistic ones will\nbe enough to fully mitigate undesirable outcomes in the narrow state of AI or\nits future incarnations. We must take a holistic view. In this paper we explore\nthe construct of infrastructure as a means to simultaneously address\nalgorithmic and societal issues when designing AI.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15578v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.11993v1",
    "title": "Unsupervised deep learning for grading of age-related macular degeneration using retinal fundus images",
    "authors": [
      "Baladitya Yellapragada",
      "Sascha Hornhauer",
      "Kiersten Snyder",
      "Stella Yu",
      "Glenn Yiu"
    ],
    "author_ids": [],
    "abstract": "Many diseases are classified based on human-defined rubrics that are prone to\nbias. Supervised neural networks can automate the grading of retinal fundus\nimages, but require labor-intensive annotations and are restricted to the\nspecific trained task. Here, we employed an unsupervised network with\nNon-Parametric Instance Discrimination (NPID) to grade age-related macular\ndegeneration (AMD) severity using fundus photographs from the Age-Related Eye\nDisease Study (AREDS). Our unsupervised algorithm demonstrated versatility\nacross different AMD classification schemes without retraining, and achieved\nunbalanced accuracies comparable to supervised networks and human\nophthalmologists in classifying advanced or referable AMD, or on the 4-step AMD\nseverity scale. Exploring the networks behavior revealed disease-related fundus\nfeatures that drove predictions and unveiled the susceptibility of more\ngranular human-defined AMD severity schemes to misclassification by both\nophthalmologists and neural networks. Importantly, unsupervised learning\nenabled unbiased, data-driven discovery of AMD features such as geographic\natrophy, as well as other ocular phenotypes of the choroid, vitreous, and lens,\nsuch as visually-impairing cataracts, that were not pre-defined by human\nlabels.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.11993v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.11805v1",
    "title": "Urban Sound Classification : striving towards a fair comparison",
    "authors": [
      "Augustin Arnault",
      "Baptiste Hanssens",
      "Nicolas Riche"
    ],
    "author_ids": [],
    "abstract": "Urban sound classification has been achieving remarkable progress and is\nstill an active research area in audio pattern recognition. In particular, it\nallows to monitor the noise pollution, which becomes a growing concern for\nlarge cities. The contribution of this paper is two-fold. First, we present our\nDCASE 2020 task 5 winning solution which aims at helping the monitoring of\nurban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse\n/ fine classification on validation set. Moreover, it reaches accuracies of\n89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not\neasy to find a fair comparison and to reproduce the performance of existing\nmodels. Sometimes authors copy-pasting the results of the original papers which\nis not helping reproducibility. As a result, we provide a fair comparison by\nusing the same input representation, metrics and optimizer to assess\nperformances. We preserve data augmentation used by the original papers. We\nhope this framework could help evaluate new architectures in this field. For\nbetter reproducibility, the code is available on our GitHub repository.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.11805v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.15581v1",
    "title": "The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research",
    "authors": [
      "Nur Ahmed",
      "Muntasir Wahed"
    ],
    "author_ids": [],
    "abstract": "Increasingly, modern Artificial Intelligence (AI) research has become more\ncomputationally intensive. However, a growing concern is that due to unequal\naccess to computing power, only certain firms and elite universities have\nadvantages in modern AI research. Using a novel dataset of 171394 papers from\n57 prestigious computer science conferences, we document that firms, in\nparticular, large technology firms and elite universities have increased\nparticipation in major AI conferences since deep learning's unanticipated rise\nin 2012. The effect is concentrated among elite universities, which are ranked\n1-50 in the QS World University Rankings. Further, we find two strategies\nthrough which firms increased their presence in AI research: first, they have\nincreased firm-only publications; and second, firms are collaborating primarily\nwith elite universities. Consequently, this increased presence of firms and\nelite universities in AI research has crowded out mid-tier (QS ranked 201-300)\nand lower-tier (QS ranked 301-500) universities. To provide causal evidence\nthat deep learning's unanticipated rise resulted in this divergence, we\nleverage the generalized synthetic control method, a data-driven counterfactual\nestimator. Using machine learning based text analysis methods, we provide\nadditional evidence that the divergence between these two groups - large firms\nand non-elite universities - is driven by access to computing power or compute,\nwhich we term as the \"compute divide\". This compute divide between large firms\nand non-elite universities increases concerns around bias and fairness within\nAI technology, and presents an obstacle towards \"democratizing\" AI. These\nresults suggest that a lack of access to specialized equipment such as compute\ncan de-democratize knowledge production.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.15581v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.11672v1",
    "title": "CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion",
    "authors": [
      "Takuhiro Kaneko",
      "Hirokazu Kameoka",
      "Kou Tanaka",
      "Nobukatsu Hojo"
    ],
    "author_ids": [],
    "abstract": "Non-parallel voice conversion (VC) is a technique for learning mappings\nbetween source and target speeches without using a parallel corpus. Recently,\ncycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2 have shown\npromising results regarding this problem and have been widely used as benchmark\nmethods. However, owing to the ambiguity of the effectiveness of\nCycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically used for\nmel-cepstrum conversion even when comparative methods employ mel-spectrogram as\na conversion target. To address this, we examined the applicability of\nCycleGAN-VC/VC2 to mel-spectrogram conversion. Through initial experiments, we\ndiscovered that their direct applications compromised the time-frequency\nstructure that should be preserved during conversion. To remedy this, we\npropose CycleGAN-VC3, an improvement of CycleGAN-VC2 that incorporates\ntime-frequency adaptive normalization (TFAN). Using TFAN, we can adjust the\nscale and bias of the converted features while reflecting the time-frequency\nstructure of the source mel-spectrogram. We evaluated CycleGAN-VC3 on\ninter-gender and intra-gender non-parallel VC. A subjective evaluation of\nnaturalness and similarity showed that for every VC pair, CycleGAN-VC3\noutperforms or is competitive with the two types of CycleGAN-VC2, one of which\nwas applied to mel-cepstrum and the other to mel-spectrogram. Audio samples are\navailable at\nhttp://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.11672v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.11666v1",
    "title": "Reducing Unintended Identity Bias in Russian Hate Speech Detection",
    "authors": [
      "Nadezhda Zueva",
      "Madina Kabirova",
      "Pavel Kalaidin"
    ],
    "author_ids": [],
    "abstract": "Toxicity has become a grave problem for many online communities and has been\ngrowing across many languages, including Russian. Hate speech creates an\nenvironment of intimidation, discrimination, and may even incite some\nreal-world violence. Both researchers and social platforms have been focused on\ndeveloping models to detect toxicity in online communication for a while now. A\ncommon problem of these models is the presence of bias towards some words (e.g.\nwoman, black, jew) that are not toxic, but serve as triggers for the classifier\ndue to model caveats. In this paper, we describe our efforts towards\nclassifying hate speech in Russian, and propose simple techniques of reducing\nunintended bias, such as generating training data with language models using\nterms and words related to protected identities as context and applying word\ndropout to such words.",
    "published_date": "2020-10-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.11666v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.11300v1",
    "title": "How Do Fair Decisions Fare in Long-term Qualification?",
    "authors": [
      "Xueru Zhang",
      "Ruibo Tu",
      "Yang Liu",
      "Mingyan Liu",
      "Hedvig Kjellström",
      "Kun Zhang",
      "Cheng Zhang"
    ],
    "author_ids": [],
    "abstract": "Although many fairness criteria have been proposed for decision making, their\nlong-term impact on the well-being of a population remains unclear. In this\nwork, we study the dynamics of population qualification and algorithmic\ndecisions under a partially observed Markov decision problem setting. By\ncharacterizing the equilibrium of such dynamics, we analyze the long-term\nimpact of static fairness constraints on the equality and improvement of group\nwell-being. Our results show that static fairness constraints can either\npromote equality or exacerbate disparity depending on the driving factor of\nqualification transitions and the effect of sensitive attributes on feature\ndistributions. We also consider possible interventions that can effectively\nimprove group qualification or promote equality of group qualification. Our\ntheoretical results and experiments on static real-world datasets with\nsimulated dynamics show that our framework can be used to facilitate social\nscience studies.",
    "published_date": "2020-10-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.11300v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10992v1",
    "title": "The Effect of the Rooney Rule on Implicit Bias in the Long Term",
    "authors": [
      "L. Elisa Celis",
      "Chris Hays",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "A robust body of evidence demonstrates the adverse effects of implicit bias\nin various contexts--from hiring to health care. The Rooney Rule is an\nintervention developed to counter implicit bias and has been implemented in the\nprivate and public sectors. The Rooney Rule requires that a selection panel\ninclude at least one candidate from an underrepresented group in their\nshortlist of candidates. Recently, Kleinberg and Raghavan proposed a model of\nimplicit bias and studied the effectiveness of the Rooney Rule when applied to\na single selection decision. However, selection decisions often occur\nrepeatedly over time. Further, it has been observed that, given consistent\ncounterstereotypical feedback, implicit biases against underrepresented\ncandidates can change.\n  We consider a model of how a selection panel's implicit bias changes over\ntime given their hiring decisions either with or without the Rooney Rule in\nplace. Our main result is that, when the panel is constrained by the Rooney\nRule, their implicit bias roughly reduces at a rate that is the inverse of the\nsize of the shortlist--independent of the number of candidates, whereas without\nthe Rooney Rule, the rate is inversely proportional to the number of\ncandidates. Thus, when the number of candidates is much larger than the size of\nthe shortlist, the Rooney Rule enables a faster reduction in implicit bias,\nproviding an additional reason in favor of using it as a strategy to mitigate\nimplicit bias. Towards empirically evaluating the long-term effect of the\nRooney Rule in repeated selection decisions, we conduct an iterative candidate\nselection experiment on Amazon MTurk. We observe that, indeed, decision-makers\nsubject to the Rooney Rule select more minority candidates in addition to those\nrequired by the rule itself than they would if no rule is in effect, and do so\nwithout considerably decreasing the utility of candidates selected.",
    "published_date": "2020-10-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10992v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10969v2",
    "title": "Incorporating Interpretable Output Constraints in Bayesian Neural Networks",
    "authors": [
      "Wanqian Yang",
      "Lars Lorch",
      "Moritz A. Graule",
      "Himabindu Lakkaraju",
      "Finale Doshi-Velez"
    ],
    "author_ids": [],
    "abstract": "Domains where supervised models are deployed often come with task-specific\nconstraints, such as prior expert knowledge on the ground-truth function, or\ndesiderata like safety and fairness. We introduce a novel probabilistic\nframework for reasoning with such constraints and formulate a prior that\nenables us to effectively incorporate them into Bayesian neural networks\n(BNNs), including a variant that can be amortized over tasks. The resulting\nOutput-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework\nfor uncertainty quantification and is amenable to black-box inference. Unlike\ntypical BNN inference in uninterpretable parameter space, OC-BNNs widen the\nrange of functional knowledge that can be incorporated, especially for model\nusers without expertise in machine learning. We demonstrate the efficacy of\nOC-BNNs on real-world datasets, spanning multiple domains such as healthcare,\ncriminal justice, and credit scoring.",
    "published_date": "2020-10-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10969v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10855v3",
    "title": "Ultimate Limits of Thermal Pattern Recognition",
    "authors": [
      "Cillian Harney",
      "Leonardo Banchi",
      "Stefano Pirandola"
    ],
    "author_ids": [],
    "abstract": "Quantum Channel Discrimination (QCD) presents a fundamental task in quantum\ninformation theory, with critical applications in quantum reading,\nillumination, data-readout and more. The extension to multiple quantum channel\ndiscrimination has seen a recent focus to characterise potential quantum\nadvantage associated with quantum enhanced discriminatory protocols. In this\npaper, we study thermal imaging as an environment localisation task, in which\nthermal images are modelled as ensembles of Gaussian phase insensitive channels\nwith identical transmissivity, and pixels possess properties according to\nbackground (cold) or target (warm) thermal channels. Via the teleportation\nstretching of adaptive quantum protocols, we derive ultimate limits on the\nprecision of pattern classification of abstract, binary thermal image spaces,\nand show that quantum enhanced strategies may be used to provide significant\nquantum advantage over known optimal classical strategies. The environmental\nconditions and necessary resources for which advantage may be obtained are\nstudied and discussed. We then numerically investigate the use of quantum\nenhanced statistical classifiers, in which quantum sensors are used in\nconjunction with machine learning image classification methods. Proving\ndefinitive advantage in the low loss regime, this work motivates the use of\nquantum enhanced sources for short-range thermal imaging and detection\ntechniques for future quantum technologies.",
    "published_date": "2020-10-21T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.LG",
      "physics.optics"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10855v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10783v4",
    "title": "Self-supervised Graph Learning for Recommendation",
    "authors": [
      "Jiancan Wu",
      "Xiang Wang",
      "Fuli Feng",
      "Xiangnan He",
      "Liang Chen",
      "Jianxun Lian",
      "Xing Xie"
    ],
    "author_ids": [],
    "abstract": "Representation learning on user-item graph for recommendation has evolved\nfrom using single ID or interaction history to exploiting higher-order\nneighbors. This leads to the success of graph convolution networks (GCNs) for\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\nthat they suffer from two limitations: (1) high-degree nodes exert larger\nimpact on the representation learning, deteriorating the recommendations of\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\ninteractions, as the neighborhood aggregation scheme further enlarges the\nimpact of observed edges.\n  In this work, we explore self-supervised learning on user-item graph, so as\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\nto supplement the classical supervised task of recommendation with an auxiliary\nself-supervised task, which reinforces node representation learning via\nself-discrimination. Specifically, we generate multiple views of a node,\nmaximizing the agreement between different views of the same node compared to\nthat of other nodes. We devise three operators to generate the views -- node\ndropout, edge dropout, and random walk -- that change the graph structure in\ndifferent manners. We term this new learning paradigm as\n\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\nhas the ability of automatically mining hard negatives. Empirical studies on\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\nthe recommendation accuracy, especially on long-tail items, and the robustness\nagainst interaction noises. Our implementations are available at\n\\url{https://github.com/wujcan/SGL}.",
    "published_date": "2020-10-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10783v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12015v1",
    "title": "Artificial Tikkun Olam: AI Can Be Our Best Friend in Building an Open Human-Computer Society",
    "authors": [
      "Simon Kasif"
    ],
    "author_ids": [],
    "abstract": "Technological advances of virtually every kind pose risks to society\nincluding fairness and bias. We review a long-standing wisdom that a widespread\npractical deployment of any technology may produce adverse side effects\nmisusing the knowhow. This includes AI but AI systems are not solely\nresponsible for societal risks. We describe some of the common and AI specific\nrisks in health industries and other sectors and propose both broad and\nspecific solutions. Each technology requires very specialized and informed\ntracking, monitoring and creative solutions. We postulate that AI systems are\nuniquely poised to produce conceptual and methodological solutions to both\nfairness and bias in automated decision-making systems. We propose a simple\nintelligent system quotient that may correspond to their adverse societal\nimpact and outline a multi-tier architecture for producing solutions of\nincreasing complexity to these risks. We also propose that universities may\nconsider forming interdisciplinary Study of Future Technology Centers to\ninvestigate and predict the fuller range of risks posed by technology and seek\nboth common and AI specific solutions using computational, technical,\nconceptual and ethical analysis",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12015v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10652v1",
    "title": "Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity",
    "authors": [
      "Wei-Fan Chen",
      "Khalid Al-Khatib",
      "Henning Wachsmuth",
      "Benno Stein"
    ],
    "author_ids": [],
    "abstract": "Media organizations bear great reponsibility because of their considerable\ninfluence on shaping beliefs and positions of our society. Any form of media\ncan contain overly biased content, e.g., by reporting on political events in a\nselective or incomplete manner. A relevant question hence is whether and how\nsuch form of imbalanced news coverage can be exposed. The research presented in\nthis paper addresses not only the automatic detection of bias but goes one step\nfurther in that it explores how political bias and unfairness are manifested\nlinguistically. In this regard we utilize a new corpus of 6964 news articles\nwith labels derived from adfontesmedia.com and develop a neural model for bias\nassessment. By analyzing this model on article excerpts, we find insightful\nbias patterns at different levels of text granularity, from single words to the\nwhole article discourse.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10652v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10649v1",
    "title": "Detecting Media Bias in News Articles using Gaussian Bias Distributions",
    "authors": [
      "Wei-Fan Chen",
      "Khalid Al-Khatib",
      "Benno Stein",
      "Henning Wachsmuth"
    ],
    "author_ids": [],
    "abstract": "Media plays an important role in shaping public opinion. Biased media can\ninfluence people in undesirable directions and hence should be unmasked as\nsuch. We observe that featurebased and neural text classification approaches\nwhich rely only on the distribution of low-level lexical information fail to\ndetect media bias. This weakness becomes most noticeable for articles on new\nevents, where words appear in new contexts and hence their \"bias\npredictiveness\" is unclear. In this paper, we therefore study how second-order\ninformation about biased statements in an article helps to improve detection\neffectiveness. In particular, we utilize the probability distributions of the\nfrequency, positions, and sequential order of lexical and informational\nsentence-level bias in a Gaussian Mixture Model. On an existing media bias\ndataset, we find that the frequency and positions of biased statements strongly\nimpact article-level bias, whereas their exact sequential order is secondary.\nUsing a standard model for sentence-level bias detection, we provide empirical\nevidence that article-level bias detectors that use second-order information\nclearly outperform those without.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10580v2",
    "title": "Causal Understanding of Fake News Dissemination on Social Media",
    "authors": [
      "Lu Cheng",
      "Ruocheng Guo",
      "Kai Shu",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "Recent years have witnessed remarkable progress towards computational fake\nnews detection. To mitigate its negative impact, we argue that it is critical\nto understand what user attributes potentially cause users to share fake news.\nThe key to this causal-inference problem is to identify confounders --\nvariables that cause spurious associations between treatments (e.g., user\nattributes) and outcome (e.g., user susceptibility). In fake news\ndissemination, confounders can be characterized by fake news sharing behavior\nthat inherently relates to user attributes and online activities. Learning such\nuser behavior is typically subject to selection bias in users who are\nsusceptible to share news on social media. Drawing on causal inference\ntheories, we first propose a principled approach to alleviating selection bias\nin fake news dissemination. We then consider the learned unbiased fake news\nsharing behavior as the surrogate confounder that can fully capture the causal\nlinks between user attributes and user susceptibility. We theoretically and\nempirically characterize the effectiveness of the proposed approach and find\nthat it could be useful in protecting society from the perils of fake news.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10580v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.10409v1",
    "title": "Bias in Conversational Search: The Double-Edged Sword of the Personalized Knowledge Graph",
    "authors": [
      "Emma J. Gerritse",
      "Faegheh Hasibi",
      "Arjen P. de Vries"
    ],
    "author_ids": [],
    "abstract": "Conversational AI systems are being used in personal devices, providing users\nwith highly personalized content. Personalized knowledge graphs (PKGs) are one\nof the recently proposed methods to store users' information in a structured\nform and tailor answers to their liking. Personalization, however, is prone to\namplifying bias and contributing to the echo-chamber phenomenon. In this paper,\nwe discuss different types of biases in conversational search systems, with the\nemphasis on the biases that are related to PKGs. We review existing definitions\nof bias in the literature: people bias, algorithm bias, and a combination of\nthe two, and further propose different strategies for tackling these biases for\nconversational search systems. Finally, we discuss methods for measuring bias\nand evaluating user satisfaction.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10409v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10407v3",
    "title": "Where Is the Normative Proof? Assumptions and Contradictions in ML Fairness Research",
    "authors": [
      "A. Feder Cooper"
    ],
    "author_ids": [],
    "abstract": "Across machine learning (ML) sub-disciplines researchers make mathematical\nassumptions to facilitate proof-writing. While such assumptions are necessary\nfor providing mathematical guarantees for how algorithms behave, they also\nnecessarily limit the applicability of these algorithms to different problem\nsettings. This practice is known--in fact, obvious--and accepted in ML\nresearch. However, similar attention is not paid to the normative assumptions\nthat ground this work. I argue such assumptions are equally as important,\nespecially in areas of ML with clear social impact, such as fairness. This is\nbecause, similar to how mathematical assumptions constrain applicability,\nnormative assumptions also limit algorithm applicability to certain problem\ndomains. I show that, in existing papers published in top venues, once\nnormative assumptions are clarified, it is often possible to get unclear or\ncontradictory results. While the mathematical assumptions and results are\nsound, the implicit normative assumptions and accompanying normative results\ncontraindicate using these methods in practical fairness applications.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10407v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10368v2",
    "title": "A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age Estimation",
    "authors": [
      "Ali Akbari",
      "Muhammad Awais",
      "Zhen-Hua Feng",
      "Ammarah Farooq",
      "Josef Kittler"
    ],
    "author_ids": [],
    "abstract": "The most existing studies in the facial age estimation assume training and\ntest images are captured under similar shooting conditions. However, this is\nrarely valid in real-world applications, where training and test sets usually\nhave different characteristics. In this paper, we advocate a cross-dataset\nprotocol for age estimation benchmarking. In order to improve the cross-dataset\nage estimation performance, we mitigate the inherent bias caused by the\nlearning algorithm itself. To this end, we propose a novel loss function that\nis more effective for neural network training. The relative smoothness of the\nproposed loss function is its advantage with regards to the optimisation\nprocess performed by stochastic gradient descent (SGD). Compared with existing\nloss functions, the lower gradient of the proposed loss function leads to the\nconvergence of SGD to a better optimum point, and consequently a better\ngeneralisation. The cross-dataset experimental results demonstrate the\nsuperiority of the proposed method over the state-of-the-art algorithms in\nterms of accuracy and generalisation capability.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10368v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10338v3",
    "title": "Edge Bias in Federated Learning and its Solution by Buffered Knowledge Distillation",
    "authors": [
      "Sangho Lee",
      "Kiyoon Yoo",
      "Nojun Kwak"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL), which utilizes communication between the server\n(core) and local devices (edges) to indirectly learn from more data, is an\nemerging field in deep learning research. Recently, Knowledge\nDistillation-based FL methods with notable performance and high applicability\nhave been suggested. In this paper, we choose knowledge distillation-based FL\nmethod as our baseline and tackle a challenging problem that ensues from using\nthese methods. Especially, we focus on the problem incurred in the server model\nthat tries to mimic different datasets, each of which is unique to an\nindividual edge device. We dub the problem 'edge bias', which occurs when\nmultiple teacher models trained on different datasets are used individually to\ndistill knowledge. We introduce this nuisance that occurs in certain scenarios\nof FL, and to alleviate it, we propose a simple yet effective distillation\nscheme named 'buffered distillation'. In addition, we also experimentally show\nthat this scheme is effective in mitigating the straggler problem caused by\ndelayed edges.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10338v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10285v1",
    "title": "Leveraging Technology for Healthcare and Retaining Access to Personal Health Data to Enhance Personal Health and Well-being",
    "authors": [
      "Ayan Chatterjee",
      "Ali Shahaab",
      "Martin W. Gerdes",
      "Santiago Martinez",
      "Pankaj Khatiwada"
    ],
    "author_ids": [],
    "abstract": "Health data is a sensitive category of personal data. It might result in a\nhigh risk to individual and health information handling rights and\nopportunities unless there is a palatable defense. Reasonable security\nstandards are needed to protect electronic health records (EHR). All personal\ndata handling needs adequate explanation. Maintaining access to medical data\neven in the developing world would favor health and well-being across the\nworld. Unfortunately, there are still countries that hinder the portability of\nmedical records. Numerous occurrences have shown that it still takes weeks for\nthe medical data to be ported from one general physician (GP) to another. Cross\nborder portability is nearly impossible due to the lack of technical\ninfrastructure and standardization. We demonstrate the difficulty of the\nportability of medical records with some example case studies as a\ncollaborative engagement exercise through a data mapping process to describe\nhow different people and datapoints interact and evaluate EHR portability\ntechniques. We then propose a blockchain-based EHR system that allows secure,\nand cross border sharing of medical data. The ethical and technical challenges\naround having such a system have also been discussed in this study.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10285v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.10221v4",
    "title": "Inequalities for space-bounded Kolmogorov complexity",
    "authors": [
      "Bruno Bauwens",
      "Peter Gács",
      "Andrei Romashchenko",
      "Alexander Shen"
    ],
    "author_ids": [],
    "abstract": "There is a parallelism between Shannon information theory and algorithmic\ninformation theory. In particular, the same linear inequalities are true for\nShannon entropies of tuples of random variables and Kolmogorov complexities of\ntuples of strings (Hammer et al., 1997), as well as for sizes of subgroups and\nprojections of sets (Chan, Yeung, Romashchenko, Shen, Vereshchagin,\n1998--2002). This parallelism started with the Kolmogorov-Levin formula (1968)\nfor the complexity of pairs of strings with logarithmic precision. Longpr\\'e\n(1986) proved a version of this formula for space-bounded complexities.\n  In this paper we prove an improved version of Longpr\\'e's result with a\ntighter space bound, using Sipser's trick (1980). Then, using this space bound,\nwe show that every linear inequality that is true for complexities or\nentropies, is also true for space-bounded Kolmogorov complexities with a\npolynomial space overhead.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.LO",
      "68Q30",
      "H.1.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10221v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.10083v1",
    "title": "Bias-Resistant Social News Aggregator Based on Blockchain",
    "authors": [
      "Amir Ziashahabi",
      "Mohammad Ali Maddah-Ali",
      "Abbas Heydarnoori"
    ],
    "author_ids": [],
    "abstract": "In today's world, social networks have become one of the primary sources for\ncreation and propagation of news. Social news aggregators are one of the actors\nin this area in which users post news items and use positive or negative votes\nto indicate their preference toward a news item. News items will be ordered and\ndisplayed according to their aggregated votes. This approach suffers from\nseveral problems raging from being prone to the dominance of the majority to\ndifficulty in discerning between correct and fake news, and lack of incentive\nfor honest behaviors. In this paper, we propose a graph-based news aggregator\nin which instead of voting on the news items, users submit their votes on the\nrelations between pairs of news items. More precisely, if a user believes two\nnews items support each other, he will submit a positive vote on the link\nbetween the two items, and if he believes that two news items undermine each\nother, he will submit a negative vote on the corresponding link. This approach\nhas mainly two desirable features: (1) mitigating the effect of personal\npreferences on voting, (2) connection of new items to endorsing and disputing\nevidence. This approach helps the newsreaders to understand different aspects\nof a news item better. We also introduce an incentive layer that uses\nblockchain as a distributed transparent manager to encourages users to behave\nhonestly and abstain from adversary behaviors. The incentive layer takes into\naccount that users can have different viewpoints toward news, enabling users\nfrom a wide range of viewpoints to contribute to the network and benefit from\nits rewards. In addition, we introduce a protocol that enables us to prove\nfraud in computations of the incentive layer model on the blockchain.\nUltimately, we will analyze the fraud proof protocol and examine our incentive\nlayer on a wide range of synthesized datasets.",
    "published_date": "2020-10-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10083v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.09851v1",
    "title": "Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference",
    "authors": [
      "Disi Ji",
      "Padhraic Smyth",
      "Mark Steyvers"
    ],
    "author_ids": [],
    "abstract": "We investigate the problem of reliably assessing group fairness when labeled\nexamples are few but unlabeled examples are plentiful. We propose a general\nBayesian framework that can augment labeled data with unlabeled data to produce\nmore accurate and lower-variance estimates compared to methods based on labeled\ndata alone. Our approach estimates calibrated scores for unlabeled examples in\neach group using a hierarchical latent variable model conditioned on labeled\nexamples. This in turn allows for inference of posterior distributions with\nassociated notions of uncertainty for a variety of group fairness metrics. We\ndemonstrate that our approach leads to significant and consistent reductions in\nestimation error across multiple well-known fairness datasets, sensitive\nattributes, and predictive models. The results show the benefits of using both\nunlabeled data and Bayesian inference in terms of assessing whether a\nprediction model is fair or not.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09851v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09768v1",
    "title": "Not Judging a User by Their Cover: Understanding Harm in Multi-Modal Processing within Social Media Research",
    "authors": [
      "Jiachen Jiang",
      "Soroush Vosoughi"
    ],
    "author_ids": [],
    "abstract": "Social media has shaken the foundations of our society, unlikely as it may\nseem. Many of the popular tools used to moderate harmful digital content,\nhowever, have received widespread criticism from both the academic community\nand the public sphere for middling performance and lack of accountability.\nThough social media research is thought to center primarily on natural language\nprocessing, we demonstrate the need for the community to understand multimedia\nprocessing and its unique ethical considerations. Specifically, we identify\nstatistical differences in the performance of Amazon Turk (MTurk) annotators\nwhen different modalities of information are provided and discuss the patterns\nof harm that arise from crowd-sourced human demographic prediction. Finally, we\ndiscuss the consequences of those biases through auditing the performance of a\ntoxicity detector called Perspective API on the language of Twitter users\nacross a variety of demographic categories.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09768v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09705v1",
    "title": "Constrained-Order Prophet Inequalities",
    "authors": [
      "Makis Arsenis",
      "Odysseas Drosis",
      "Robert Kleinberg"
    ],
    "author_ids": [],
    "abstract": "Free order prophet inequalities bound the ratio between the expected value\nobtained by two parties each selecting a value from a set of independent random\nvariables: a \"prophet\" who knows the value of each variable and may select the\nmaximum one, and a \"gambler\" who is free to choose the order in which to\nobserve the values but must select one of them immediately after observing it,\nwithout knowing what values will be sampled for the unobserved variables. It is\nknown that the gambler can always ensure an expected payoff at least\n$0.669\\dots$ times as great as that of the prophet. In fact, there exists a\nthreshold stopping rule which guarantees a gambler-to-prophet ratio of at least\n$1-\\frac1e=0.632\\dots$. In contrast, if the gambler must observe the values in\na predetermined order, the tight bound for the gambler-to-prophet ratio is\n$1/2$.\n  In this work we investigate a model that interpolates between these two\nextremes. We assume there is a predefined set of permutations, and the gambler\nis free to choose the order of observation to be any one of these predefined\npermutations. Surprisingly, we show that even when only two orderings are\nallowed---namely, the forward and reverse orderings---the gambler-to-prophet\nratio improves to $\\varphi^{-1}=0.618\\dots$, the inverse of the golden ratio.\nAs the number of allowed permutations grows beyond 2, a striking \"double\nplateau\" phenomenon emerges: after increasing from $0.5$ to $\\varphi^{-1}$, the\ngambler-to-prophet ratio achievable by threshold stopping rules does not exceed\n$\\varphi^{-1}+o(1)$ until the number of allowed permutations grows to $O(\\log\nn)$. The ratio reaches $1-\\frac1e-\\varepsilon$ for a suitably chosen set of\n$O(\\text{poly}(\\varepsilon^{-1})\\cdot\\log n)$ permutations and does not exceed\n$1-\\frac1e$ even when the full set of $n!$ permutations is allowed.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09705v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.09670v3",
    "title": "RobustBench: a standardized adversarial robustness benchmark",
    "authors": [
      "Francesco Croce",
      "Maksym Andriushchenko",
      "Vikash Sehwag",
      "Edoardo Debenedetti",
      "Nicolas Flammarion",
      "Mung Chiang",
      "Prateek Mittal",
      "Matthias Hein"
    ],
    "author_ids": [],
    "abstract": "As a research community, we are still lacking a systematic understanding of\nthe progress on adversarial robustness which often makes it hard to identify\nthe most promising ideas in training robust models. A key challenge in\nbenchmarking robustness is that its evaluation is often error-prone leading to\nrobustness overestimation. Our goal is to establish a standardized benchmark of\nadversarial robustness, which as accurately as possible reflects the robustness\nof the considered models within a reasonable computational budget. To this end,\nwe start by considering the image classification task and introduce\nrestrictions (possibly loosened in the future) on the allowed models. We\nevaluate adversarial robustness with AutoAttack, an ensemble of white- and\nblack-box attacks, which was recently shown in a large-scale study to improve\nalmost all robustness evaluations compared to the original publications. To\nprevent overadaptation of new defenses to AutoAttack, we welcome external\nevaluations based on adaptive attacks, especially where AutoAttack flags a\npotential overestimation of robustness. Our leaderboard, hosted at\nhttps://robustbench.github.io/, contains evaluations of 120+ models and aims at\nreflecting the current state of the art in image classification on a set of\nwell-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models and on common\ncorruptions, with possible extensions in the future. Additionally, we\nopen-source the library https://github.com/RobustBench/robustbench that\nprovides unified access to 80+ robust models to facilitate their downstream\napplications. Finally, based on the collected models, we analyze the impact of\nrobustness on the performance on distribution shifts, calibration,\nout-of-distribution detection, fairness, privacy leakage, smoothness, and\ntransferability.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09670v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09637v2",
    "title": "Price of Fairness in Budget Division for Egalitarian Social Welfare",
    "authors": [
      "Zhongzheng Tang",
      "Chenhao Wang",
      "Mengqi Zhang"
    ],
    "author_ids": [],
    "abstract": "We study a participatory budgeting problem of aggregating the preferences of\nagents and dividing a budget over the projects. A budget division solution is a\nprobability distribution over the projects. The main purpose of our study\nconcerns the comparison between the system optimum solution and a fair\nsolution. We are interested in assessing the quality of fair solutions, i.e.,\nin measuring the system efficiency loss under a fair allocation compared to the\none that maximizes (egalitarian) social welfare. This indicator is called the\nprice of fairness. We are also interested in the performance of several\naggregation rules. Asymptotically tight bounds are provided both for the price\nof fairness and the efficiency guarantee of aggregation rules.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "91A68 (Primary) 91B03, 68W25 (Secondary)",
      "F.2.2; G.2.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09637v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.09635v1",
    "title": "Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control",
    "authors": [
      "Guangzhi Tang",
      "Neelesh Kumar",
      "Raymond Yoo",
      "Konstantinos P. Michmizos"
    ],
    "author_ids": [],
    "abstract": "The energy-efficient control of mobile robots is crucial as the complexity of\ntheir real-world applications increasingly involves high-dimensional\nobservation and action spaces, which cannot be offset by limited on-board\nresources. An emerging non-Von Neumann model of intelligence, where spiking\nneural networks (SNNs) are run on neuromorphic processors, is regarded as an\nenergy-efficient and robust alternative to the state-of-the-art real-time\nrobotic controllers for low dimensional control tasks. The challenge now for\nthis new computing paradigm is to scale so that it can keep up with real-world\ntasks. To do so, SNNs need to overcome the inherent limitations of their\ntraining, namely the limited ability of their spiking neurons to represent\ninformation and the lack of effective learning algorithms. Here, we propose a\npopulation-coded spiking actor network (PopSAN) trained in conjunction with a\ndeep critic network using deep reinforcement learning (DRL). The population\ncoding scheme dramatically increased the representation capacity of the network\nand the hybrid learning combined the training advantages of deep networks with\nthe energy-efficient inference of spiking networks. To show the general\napplicability of our approach, we integrated it with a spectrum of both\non-policy and off-policy DRL algorithms. We deployed the trained PopSAN on\nIntel's Loihi neuromorphic chip and benchmarked our method against the\nmainstream DRL algorithms for continuous control. To allow for a fair\ncomparison among all methods, we validated them on OpenAI gym tasks. Our\nLoihi-run PopSAN consumed 140 times less energy per inference when compared\nagainst the deep actor network on Jetson TX2, and had the same level of\nperformance. Our results support the efficiency of neuromorphic controllers and\nsuggest our hybrid RL as an alternative to deep learning, when both\nenergy-efficiency and robustness are important.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09635v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10028v2",
    "title": "Towards an Ethical Framework in the Complex Digital Era",
    "authors": [
      "David Pastor-Escuredo",
      "Ricardo Vinuesa"
    ],
    "author_ids": [],
    "abstract": "The digital revolution has brought ethical crossroads of technology, behavior\nand truth. However, the need of a comprehensive and constructive ethical\nframework is emerging as digital platforms have been used to build a global\nchaotic and truth-agnostic system. The unequal structure of the global system\nleads to dynamic changes and systemic problems, which have a more significant\nimpact on those that are most vulnerable. Ethical frameworks based only on the\nindividual level are no longer sufficient as they lack the necessary\narticulation to provide solutions to the new challenges. A new ethical vision\nmust comprise the understanding of the scales and complex interconnections, as\nwell as the causal chains of modern social systems. Many of these systems are\ninternally fragile and very sensitive to external factors and threats, which\nlead to unethical situations that require systemic solutions still centered on\nindividuals. Furthermore, the multi-layered net-like social tissue generates\nclusters of power that prevent certain communities from proper development.\nDigital technology has also impacted at the individual level posing the risk of\na more homogeneous, predictable and ultimately controllable humankind. To\npreserve the core of humanity and the aspiration of common truth, a new ethical\nframework must empower individuals and uniqueness, as well as cultural\nheterogeneity, tackling the negative outcomes of digitalization. Only combining\nhuman-centered and collectiveness-oriented digital development will it be\npossible to construct new social models and interactions that are ethical. This\nvision requires science to enhance ethical frameworks and principles using\ncomputational tools to support truth-grounded actions, so as to transform and\nconfigure properties of the social systems.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10028v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09577v1",
    "title": "GANs for learning from very high class conditional noisy labels",
    "authors": [
      "Sandhya Tripathi",
      "N Hemachandra"
    ],
    "author_ids": [],
    "abstract": "We use Generative Adversarial Networks (GANs) to design a class conditional\nlabel noise (CCN) robust scheme for binary classification. It first generates a\nset of correctly labelled data points from noisy labelled data and 0.1% or 1%\nclean labels such that the generated and true (clean) labelled data\ndistributions are close; generated labelled data is used to learn a good\nclassifier. The mode collapse problem while generating correct feature-label\npairs and the problem of skewed feature-label dimension ratio ($\\sim$ 784:1)\nare avoided by using Wasserstein GAN and a simple data representation change.\nAnother WGAN with information-theoretic flavour on top of the new\nrepresentation is also proposed. The major advantage of both schemes is their\nsignificant improvement over the existing ones in presence of very high CCN\nrates, without either estimating or cross-validating over the noise rates. We\nproved that KL divergence between clean and noisy distribution increases w.r.t.\nnoise rates in symmetric label noise model; can be extended to high CCN rates.\nThis implies that our schemes perform well due to the adversarial nature of\nGANs. Further, use of generative approach (learning clean joint distribution)\nwhile handling noise enables our schemes to perform better than discriminative\napproaches like GLC, LDMI and GCE; even when the classes are highly imbalanced.\nUsing Friedman F test and Nemenyi posthoc test, we showed that on high\ndimensional binary class synthetic, MNIST and Fashion MNIST datasets, our\nschemes outperform the existing methods and demonstrate consistent performance\nacross noise rates.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09577v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09572v2",
    "title": "Teacher-Student Competition for Unsupervised Domain Adaptation",
    "authors": [
      "Ruixin Xiao",
      "Zhilei Liu",
      "Baoyuan Wu"
    ],
    "author_ids": [],
    "abstract": "With the supervision from source domain only in class-level, existing\nunsupervised domain adaptation (UDA) methods mainly learn the domain-invariant\nrepresentations from a shared feature extractor, which causes the source-bias\nproblem. This paper proposes an unsupervised domain adaptation approach with\nTeacher-Student Competition (TSC). In particular, a student network is\nintroduced to learn the target-specific feature space, and we design a novel\ncompetition mechanism to select more credible pseudo-labels for the training of\nstudent network. We introduce a teacher network with the structure of existing\nconventional UDA method, and both teacher and student networks compete to\nprovide target pseudo-labels to constrain every target sample's training in\nstudent network. Extensive experiments demonstrate that our proposed TSC\nframework significantly outperforms the state-of-the-art domain adaptation\nmethods on Office-31 and ImageCLEF-DA benchmarks.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09572v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09553v7",
    "title": "Survey on Causal-based Machine Learning Fairness Notions",
    "authors": [
      "Karima Makhlouf",
      "Sami Zhioua",
      "Catuscia Palamidessi"
    ],
    "author_ids": [],
    "abstract": "Addressing the problem of fairness is crucial to safely use machine learning\nalgorithms to support decisions with a critical impact on people's lives such\nas job hiring, child maltreatment, disease diagnosis, loan granting, etc.\nSeveral notions of fairness have been defined and examined in the past decade,\nsuch as statistical parity and equalized odds. The most recent fairness\nnotions, however, are causal-based and reflect the now widely accepted idea\nthat using causality is necessary to appropriately address the problem of\nfairness. This paper examines an exhaustive list of causal-based fairness\nnotions and study their applicability in real-world scenarios. As the majority\nof causal-based fairness notions are defined in terms of non-observable\nquantities (e.g., interventions and counterfactuals), their deployment in\npractice requires to compute or estimate those quantities using observational\ndata. This paper offers a comprehensive report of the different approaches to\ninfer causal quantities from observational data including identifiability\n(Pearl's SCM framework) and estimation (potential outcome framework). The main\ncontributions of this survey paper are (1) a guideline to help selecting a\nsuitable fairness notion given a specific real-world scenario, and (2) a\nranking of the fairness notions according to Pearl's causation ladder\nindicating how difficult it is to deploy each notion in practice.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09553v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09283v1",
    "title": "Neuralizing Efficient Higher-order Belief Propagation",
    "authors": [
      "Mohammed Haroon Dupty",
      "Wee Sun Lee"
    ],
    "author_ids": [],
    "abstract": "Graph neural network models have been extensively used to learn node\nrepresentations for graph structured data in an end-to-end setting. These\nmodels often rely on localized first order approximations of spectral graph\nconvolutions and hence are unable to capture higher-order relational\ninformation between nodes. Probabilistic Graphical Models form another class of\nmodels that provide rich flexibility in incorporating such relational\ninformation but are limited by inefficient approximate inference algorithms at\nhigher order. In this paper, we propose to combine these approaches to learn\nbetter node and graph representations. First, we derive an efficient\napproximate sum-product loopy belief propagation inference algorithm for\nhigher-order PGMs. We then embed the message passing updates into a neural\nnetwork to provide the inductive bias of the inference algorithm in end-to-end\nlearning. This gives us a model that is flexible enough to accommodate domain\nknowledge while maintaining the computational advantage. We further propose\nmethods for constructing higher-order factors that are conditioned on node and\nedge features and share parameters wherever necessary. Our experimental\nevaluation shows that our model indeed captures higher-order information,\nsubstantially outperforming state-of-the-art $k$-order graph neural networks in\nmolecular datasets.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09283v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.09282v1",
    "title": "Characterizing the First-Arriving Multipath Component in 5G Millimeter Wave Networks: TOA, AOA, and Non-Line-of-Sight Bias",
    "authors": [
      "Christopher E. O'Lone",
      "Harpreet S. Dhillon",
      "R. Michael Buehrer"
    ],
    "author_ids": [],
    "abstract": "This paper presents a stochastic geometry-based analysis of propagation\nstatistics for 5G millimeter wave (mm-wave) cellular. In particular, the\ntime-of-arrival (TOA) and angle-of-arrival (AOA) distributions of the\nfirst-arriving multipath component (MPC) are derived. These statistics find\ntheir utility in many applications such as cellular-based localization, channel\nmodeling, and link establishment for mm-wave initial access (IA). Leveraging\ntools from stochastic geometry, a Boolean model is used to statistically\ncharacterize the random locations, orientations, and sizes of reflectors, e.g.,\nbuildings. Assuming non-line-of-sight (NLOS) propagation is due to first-order\n(i.e., single-bounce) reflections, and that reflectors can either facilitate or\nblock reflections, the distribution of the path length (i.e., absolute time\ndelay) of the first-arriving MPC is derived. This result is then used to obtain\nthe first NLOS bias distribution in the localization literature that is based\non the absolute delay of the first-arriving MPC for outdoor time-of-flight\n(TOF) range measurements. This distribution is shown to match exceptionally\nwell with commonly assumed gamma and exponential NLOS bias models in the\nliterature, which were only attainted previously through heuristic or indirect\nmethods. Continuing under this analytical framework, the AOA distribution of\nthe first-arriving MPC is derived, which gives novel insight into how\nenvironmental obstacles affect the AOA and also represents the first AOA\ndistribution derived under the Boolean Model.",
    "published_date": "2020-10-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09282v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.09141v1",
    "title": "Diverse Data Selection under Fairness Constraints",
    "authors": [
      "Zafeiria Moumoulidou",
      "Andrew McGregor",
      "Alexandra Meliou"
    ],
    "author_ids": [],
    "abstract": "Diversity is an important principle in data selection and summarization,\nfacility location, and recommendation systems. Our work focuses on maximizing\ndiversity in data selection, while offering fairness guarantees. In particular,\nwe offer the first study that augments the Max-Min diversification objective\nwith fairness constraints. More specifically, given a universe $U$ of $n$\nelements that can be partitioned into $m$ disjoint groups, we aim to retrieve a\n$k$-sized subset that maximizes the pairwise minimum distance within the set\n(diversity) and contains a pre-specified $k_i$ number of elements from each\ngroup $i$ (fairness). We show that this problem is NP-complete even in metric\nspaces, and we propose three novel algorithms, linear in $n$, that provide\nstrong theoretical approximation guarantees for different values of $m$ and\n$k$. Finally, we extend our algorithms and analysis to the case where groups\ncan be overlapping.",
    "published_date": "2020-10-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09141v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.09089v1",
    "title": "Training Stronger Baselines for Learning to Optimize",
    "authors": [
      "Tianlong Chen",
      "Weiyi Zhang",
      "Jingyang Zhou",
      "Shiyu Chang",
      "Sijia Liu",
      "Lisa Amini",
      "Zhangyang Wang"
    ],
    "author_ids": [],
    "abstract": "Learning to optimize (L2O) has gained increasing attention since classical\noptimizers require laborious problem-specific design and hyperparameter tuning.\nHowever, there is a gap between the practical demand and the achievable\nperformance of existing L2O models. Specifically, those learned optimizers are\napplicable to only a limited class of problems, and often exhibit instability.\nWith many efforts devoted to designing more sophisticated L2O models, we argue\nfor another orthogonal, under-explored theme: the training techniques for those\nL2O models. We show that even the simplest L2O model could have been trained\nmuch better. We first present a progressive training scheme to gradually\nincrease the optimizer unroll length, to mitigate a well-known L2O dilemma of\ntruncation bias (shorter unrolling) versus gradient explosion (longer\nunrolling). We further leverage off-policy imitation learning to guide the L2O\nlearning, by taking reference to the behavior of analytical optimizers. Our\nimproved training techniques are plugged into a variety of state-of-the-art L2O\nmodels, and immediately boost their performance, without making any change to\ntheir model structures. Especially, by our proposed techniques, an earliest and\nsimplest L2O model can be trained to outperform the latest complicated L2O\nmodels on a number of tasks. Our results demonstrate a greater potential of L2O\nyet to be unleashed, and urge to rethink the recent progress. Our codes are\npublicly available at: https://github.com/VITA-Group/L2O-Training-Techniques.",
    "published_date": "2020-10-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09089v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.08912v1",
    "title": "The Leaky Pipeline in Physics Publishing",
    "authors": [
      "Clara O Ross",
      "Aditya Gupta",
      "Ninareh Mehrabi",
      "Goran Muric",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "Women make up a shrinking portion of physics faculty in senior positions, a\nphenomenon known as a \"leaky pipeline.\" While fixing this problem has been a\npriority in academic institutions, efforts have been stymied by the diverse\nsources of leaks. In this paper we identify a bias potentially contributing to\nthe leaky pipeline. We analyze bibliographic data provided by the American\nPhysical Society (APS), a leading publisher of physics research. By inferring\nthe gender of authors from names, we are able to measure the fraction of women\nauthors over past decades. We show that the more selective, higher impact APS\njournals have lower fractions of women authors compared to other APS journals.\nCorrecting this bias may help more women publish in prestigious APS journals,\nand in turn help improve their academic promotion cases.",
    "published_date": "2020-10-18T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.08912v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.08850v2",
    "title": "Against Scale: Provocations and Resistances to Scale Thinking",
    "authors": [
      "Alex Hanna",
      "Tina M. Park"
    ],
    "author_ids": [],
    "abstract": "At the heart of what drives the bulk of innovation and activity in Silicon\nValley and elsewhere is scalability. This unwavering commitment to scalability\n-- to identify strategies for efficient growth -- is at the heart of what we\nrefer to as \"scale thinking.\" Whether people are aware of it or not, scale\nthinking is all-encompassing. It is not just an attribute of one's product,\nservice, or company, but frames how one thinks about the world (what\nconstitutes it and how it can be observed and measured), its problems (what is\na problem worth solving versus not), and the possible technological fixes for\nthose problems. This paper examines different facets of scale thinking and its\nimplication on how we view technology and collaborative work. We argue that\ntechnological solutions grounded in scale thinking are unlikely to be as\nliberatory or effective at deep, systemic change as their purveyors imagine.\nRather, solutions which resist scale thinking are necessary to undo the social\nstructures which lie at the heart of social inequality. We draw on recent work\non mutual aid networks and propose questions to ask of collaborative work\nsystems as a means to evaluate technological solutions and guide designers in\nidentifying sites of resistance to scale thinking.",
    "published_date": "2020-10-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.08850v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.09714v1",
    "title": "A Convenient Generalization of Schlick's Bias and Gain Functions",
    "authors": [
      "Jonathan T. Barron"
    ],
    "author_ids": [],
    "abstract": "We present a generalization of Schlick's bias and gain functions -- simple\nparametric curve-shaped functions for inputs in [0, 1]. Our single function\nincludes both bias and gain as special cases, and is able to describe other\nsmooth and monotonic curves with variable degrees of asymmetry.",
    "published_date": "2020-10-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.09714v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.08515v2",
    "title": "Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?",
    "authors": [
      "Zhiyuan Li",
      "Yi Zhang",
      "Sanjeev Arora"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks often dominate fully-connected counterparts in\ngeneralization performance, especially on image classification tasks. This is\noften explained in terms of 'better inductive bias'. However, this has not been\nmade mathematically rigorous, and the hurdle is that the fully connected net\ncan always simulate the convolutional net (for a fixed task). Thus the training\nalgorithm plays a role. The current work describes a natural task on which a\nprovable sample complexity gap can be shown, for standard training algorithms.\nWe construct a single natural distribution on $\\mathbb{R}^d\\times\\{\\pm 1\\}$ on\nwhich any orthogonal-invariant algorithm (i.e. fully-connected networks trained\nwith most gradient-based methods from gaussian initialization) requires\n$\\Omega(d^2)$ samples to generalize while $O(1)$ samples suffice for\nconvolutional architectures. Furthermore, we demonstrate a single target\nfunction, learning which on all possible distributions leads to an $O(1)$ vs\n$\\Omega(d^2/\\varepsilon)$ gap. The proof relies on the fact that SGD on\nfully-connected network is orthogonal equivariant. Similar results are achieved\nfor $\\ell_2$ regression and adaptive training algorithms, e.g. Adam and\nAdaGrad, which are only permutation equivariant.",
    "published_date": "2020-10-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.08515v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.08323v1",
    "title": "QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph",
    "authors": [
      "Saeedeh Shekarpour",
      "Abhishek Nadgeri",
      "Kuldeep Singh"
    ],
    "author_ids": [],
    "abstract": "In the era of Big Knowledge Graphs, Question Answering (QA) systems have\nreached a milestone in their performance and feasibility. However, their\napplicability, particularly in specific domains such as the biomedical domain,\nhas not gained wide acceptance due to their \"black box\" nature, which hinders\ntransparency, fairness, and accountability of QA systems. Therefore, users are\nunable to understand how and why particular questions have been answered,\nwhereas some others fail. To address this challenge, in this paper, we develop\nan automatic approach for generating explanations during various stages of a\npipeline-based QA system. Our approach is a supervised and automatic approach\nwhich considers three classes (i.e., success, no answer, and wrong answer) for\nannotating the output of involved QA components. Upon our prediction, a\ntemplate explanation is chosen and integrated into the output of the\ncorresponding component. To measure the effectiveness of the approach, we\nconducted a user survey as to how non-expert users perceive our generated\nexplanations. The results of our study show a significant increase in the four\ndimensions of the human factor from the Human-computer interaction community.",
    "published_date": "2020-10-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.08323v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.08140v1",
    "title": "Monitoring Trust in Human-Machine Interactions for Public Sector Applications",
    "authors": [
      "Farhana Faruqe",
      "Ryan Watkins",
      "Larry Medsker"
    ],
    "author_ids": [],
    "abstract": "The work reported here addresses the capacity of psychophysiological sensors\nand measures using Electroencephalogram (EEG) and Galvanic Skin Response (GSR)\nto detect levels of trust for humans using AI-supported Human-Machine\nInteraction (HMI). Improvements to the analysis of EEG and GSR data may create\nmodels that perform as well, or better than, traditional tools. A challenge to\nanalyzing the EEG and GSR data is the large amount of training data required\ndue to a large number of variables in the measurements. Researchers have\nroutinely used standard machine-learning classifiers like artificial neural\nnetworks (ANN), support vector machines (SVM), and K-nearest neighbors (KNN).\nTraditionally, these have provided few insights into which features of the EEG\nand GSR data facilitate the more and least accurate predictions - thus making\nit harder to improve the HMI and human-machine trust relationship. A key\ningredient to applying trust-sensor research results to practical situations\nand monitoring trust in work environments is the understanding of which key\nfeatures are contributing to trust and then reducing the amount of data needed\nfor practical applications. We used the Local Interpretable Model-agnostic\nExplanations (LIME) model as a process to reduce the volume of data required to\nmonitor and enhance trust in HMI systems - a technology that could be valuable\nfor governmental and public sector applications. Explainable AI can make HMI\nsystems transparent and promote trust. From customer service in government\nagencies and community-level non-profit public service organizations to\nnational military and cybersecurity institutions, many public sector\norganizations are increasingly concerned to have effective and ethical HMI with\nservices that are trustworthy, unbiased, and free of unintended negative\nconsequences.",
    "published_date": "2020-10-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.08140v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07938v2",
    "title": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making",
    "authors": [
      "Charvi Rastogi",
      "Yunfeng Zhang",
      "Dennis Wei",
      "Kush R. Varshney",
      "Amit Dhurandhar",
      "Richard Tomsett"
    ],
    "author_ids": [],
    "abstract": "Several strands of research have aimed to bridge the gap between artificial\nintelligence (AI) and human decision-makers in AI-assisted decision-making,\nwhere humans are the consumers of AI model predictions and the ultimate\ndecision-makers in high-stakes applications. However, people's perception and\nunderstanding are often distorted by their cognitive biases, such as\nconfirmation bias, anchoring bias, availability bias, to name a few. In this\nwork, we use knowledge from the field of cognitive science to account for\ncognitive biases in the human-AI collaborative decision-making setting, and\nmitigate their negative effects on collaborative performance. To this end, we\nmathematically model cognitive biases and provide a general framework through\nwhich researchers and practitioners can understand the interplay between\ncognitive biases and human-AI accuracy. We then focus specifically on anchoring\nbias, a bias commonly encountered in human-AI collaboration. We implement a\ntime-based de-anchoring strategy and conduct our first user experiment that\nvalidates its effectiveness in human-AI collaborative decision-making. With\nthis result, we design a time allocation strategy for a resource-constrained\nsetting that achieves optimal human-AI collaboration under some assumptions.\nWe, then, conduct a second user experiment which shows that our time allocation\nstrategy with explanation can effectively de-anchor the human and improve\ncollaborative performance when the AI model has low confidence and is\nincorrect.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07938v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07881v1",
    "title": "An Empirical Analysis of Visual Features for Multiple Object Tracking in Urban Scenes",
    "authors": [
      "Mehdi Miah",
      "Justine Pepin",
      "Nicolas Saunier",
      "Guillaume-Alexandre Bilodeau"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the problem of selecting appearance features for\nmultiple object tracking (MOT) in urban scenes. Over the years, a large number\nof features has been used for MOT. However, it is not clear whether some of\nthem are better than others. Commonly used features are color histograms,\nhistograms of oriented gradients, deep features from convolutional neural\nnetworks and re-identification (ReID) features. In this study, we assess how\ngood these features are at discriminating objects enclosed by a bounding box in\nurban scene tracking scenarios. Several affinity measures, namely the\n$\\mathrm{L}_1$, $\\mathrm{L}_2$ and the Bhattacharyya distances, Rank-1 counts\nand the cosine similarity, are also assessed for their impact on the\ndiscriminative power of the features. Results on several datasets show that\nfeatures from ReID networks are the best for discriminating instances from one\nanother regardless of the quality of the detector. If a ReID model is not\navailable, color histograms may be selected if the detector has a good recall\nand there are few occlusions; otherwise, deep features are more robust to\ndetectors with lower recall. The project page is\nhttp://www.mehdimiah.com/visual_features.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07881v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07866v2",
    "title": "Double Robust Representation Learning for Counterfactual Prediction",
    "authors": [
      "Shuxi Zeng",
      "Serge Assaad",
      "Chenyang Tao",
      "Shounak Datta",
      "Lawrence Carin",
      "Fan Li"
    ],
    "author_ids": [],
    "abstract": "Causal inference, or counterfactual prediction, is central to decision making\nin healthcare, policy and social sciences. To de-bias causal estimators with\nhigh-dimensional data in observational studies, recent advances suggest the\nimportance of combining machine learning models for both the propensity score\nand the outcome function. We propose a novel scalable method to learn\ndouble-robust representations for counterfactual predictions, leading to\nconsistent causal estimation if the model for either the propensity score or\nthe outcome, but not necessarily both, is correctly specified. Specifically, we\nuse the entropy balancing method to learn the weights that minimize the\nJensen-Shannon divergence of the representation between the treated and control\ngroups, based on which we make robust and efficient counterfactual predictions\nfor both individual and average treatment effects. We provide theoretical\njustifications for the proposed method. The algorithm shows competitive\nperformance with the state-of-the-art on real world and synthetic data.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07866v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07848v1",
    "title": "Towards a Flexible Framework for Algorithmic Fairness",
    "authors": [
      "Philip Hacker",
      "Emil Wiedemann",
      "Meike Zehlike"
    ],
    "author_ids": [],
    "abstract": "Increasingly, scholars seek to integrate legal and technological insights to\ncombat bias in AI systems. In recent years, many different definitions for\nensuring non-discrimination in algorithmic decision systems have been put\nforward. In this paper, we first briefly describe the EU law framework covering\ncases of algorithmic discrimination. Second, we present an algorithm that\nharnesses optimal transport to provide a flexible framework to interpolate\nbetween different fairness definitions. Third, we show that important normative\nand legal challenges remain for the implementation of algorithmic fairness\ninterventions in real-world scenarios. Overall, the paper seeks to contribute\nto the quest for flexible technical frameworks that can be adapted to varying\nlegal and normative fairness constraints.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07848v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07799v3",
    "title": "Adaptive and Universal Algorithms for Variational Inequalities with Optimal Convergence",
    "authors": [
      "Alina Ene",
      "Huy L. Nguyen"
    ],
    "author_ids": [],
    "abstract": "We develop new adaptive algorithms for variational inequalities with monotone\noperators, which capture many problems of interest, notably convex optimization\nand convex-concave saddle point problems. Our algorithms automatically adapt to\nunknown problem parameters such as the smoothness and the norm of the operator,\nand the variance of the stochastic evaluation oracle. We show that our\nalgorithms are universal and simultaneously achieve the optimal convergence\nrates in the non-smooth, smooth, and stochastic settings. The convergence\nguarantees of our algorithms improve over existing adaptive methods by a\n$\\Omega(\\sqrt{\\ln T})$ factor, matching the optimal non-adaptive algorithms.\nAdditionally, prior works require that the optimization domain is bounded. In\nthis work, we remove this restriction and give algorithms for unbounded domains\nthat are adaptive and universal. Our general proof techniques can be used for\nmany variants of the algorithm using one or two operator evaluations per\niteration. The classical methods based on the ExtraGradient/MirrorProx\nalgorithm require two operator evaluations per iteration, which is the dominant\nfactor in the running time in many settings.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07799v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07777v1",
    "title": "A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning",
    "authors": [
      "Arnu Pretorius",
      "Scott Cameron",
      "Elan van Biljon",
      "Tom Makkink",
      "Shahil Mawjee",
      "Jeremy du Plessis",
      "Jonathan Shock",
      "Alexandre Laterre",
      "Karim Beguir"
    ],
    "author_ids": [],
    "abstract": "Multi-agent reinforcement learning has recently shown great promise as an\napproach to networked system control. Arguably, one of the most difficult and\nimportant tasks for which large scale networked system control is applicable is\ncommon-pool resource management. Crucial common-pool resources include arable\nland, fresh water, wetlands, wildlife, fish stock, forests and the atmosphere,\nof which proper management is related to some of society's greatest challenges\nsuch as food security, inequality and climate change. Here we take inspiration\nfrom a recent research program investigating the game-theoretic incentives of\nhumans in social dilemma situations such as the well-known tragedy of the\ncommons. However, instead of focusing on biologically evolved human-like\nagents, our concern is rather to better understand the learning and operating\nbehaviour of engineered networked systems comprising general-purpose\nreinforcement learning agents, subject only to nonbiological constraints such\nas memory, computation and communication bandwidth. Harnessing tools from\nempirical game-theoretic analysis, we analyse the differences in resulting\nsolution concepts that stem from employing different information structures in\nthe design of networked multi-agent systems. These information structures\npertain to the type of information shared between agents as well as the\nemployed communication protocol and network topology. Our analysis contributes\nnew insights into the consequences associated with certain design choices and\nprovides an additional dimension of comparison between systems beyond\nefficiency, robustness, scalability and mean control performance.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07777v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07754v1",
    "title": "EnCoD: Distinguishing Compressed and Encrypted File Fragments",
    "authors": [
      "Fabio De Gaspari",
      "Dorjan Hitaj",
      "Giulio Pagnotta",
      "Lorenzo De Carli",
      "Luigi V. Mancini"
    ],
    "author_ids": [],
    "abstract": "Reliable identification of encrypted file fragments is a requirement for\nseveral security applications, including ransomware detection, digital\nforensics, and traffic analysis. A popular approach consists of estimating high\nentropy as a proxy for randomness. However, many modern content types (e.g.\noffice documents, media files, etc.) are highly compressed for storage and\ntransmission efficiency. Compression algorithms also output high-entropy data,\nthus reducing the accuracy of entropy-based encryption detectors. Over the\nyears, a variety of approaches have been proposed to distinguish encrypted file\nfragments from high-entropy compressed fragments. However, these approaches are\ntypically only evaluated over a few, select data types and fragment sizes,\nwhich makes a fair assessment of their practical applicability impossible. This\npaper aims to close this gap by comparing existing statistical tests on a\nlarge, standardized dataset. Our results show that current approaches cannot\nreliably tell apart encryption and compression, even for large fragment sizes.\nTo address this issue, we design EnCoD, a learning-based classifier which can\nreliably distinguish compressed and encrypted data, starting with fragments as\nsmall as 512 bytes. We evaluate EnCoD against current approaches over a large\ndataset of different data types, showing that it outperforms current\nstate-of-the-art for most considered fragment sizes and data types.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07650v2",
    "title": "Altruist: Argumentative Explanations through Local Interpretations of Predictive Models",
    "authors": [
      "Ioannis Mollas",
      "Nick Bassiliades",
      "Grigorios Tsoumakas"
    ],
    "author_ids": [],
    "abstract": "Explainable AI is an emerging field providing solutions for acquiring\ninsights into automated systems' rationale. It has been put on the AI map by\nsuggesting ways to tackle key ethical and societal issues. Existing explanation\ntechniques are often not comprehensible to the end user. Lack of evaluation and\nselection criteria also makes it difficult for the end user to choose the most\nsuitable technique. In this study, we combine logic-based argumentation with\nInterpretable Machine Learning, introducing a preliminary meta-explanation\nmethodology that identifies the truthful parts of feature importance oriented\ninterpretations. This approach, in addition to being used as a meta-explanation\ntechnique, can be used as an evaluation or selection tool for multiple feature\nimportance techniques. Experimentation strongly indicates that an ensemble of\nmultiple interpretation techniques yields considerably more truthful\nexplanations.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "I.2.0; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07650v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07610v1",
    "title": "A Methodology for Ethics-by-Design AI Systems: Dealing with Human Value Conflicts",
    "authors": [
      "Fabrice Muhlenbach"
    ],
    "author_ids": [],
    "abstract": "The introduction of artificial intelligence into activities traditionally\ncarried out by human beings produces brutal changes. This is not without\nconsequences for human values. This paper is about designing and implementing\nmodels of ethical behaviors in AI-based systems, and more specifically it\npresents a methodology for designing systems that take ethical aspects into\naccount at an early stage while finding an innovative solution to prevent human\nvalues from being affected. Two case studies where AI-based innovations\ncomplement economic and social proposals with this methodology are presented:\none in the field of culture and operated by a private company, the other in the\nfield of scientific research and supported by a state organization.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.HC",
      "I.2.0; H.1.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07610v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07486v2",
    "title": "CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical Imaging",
    "authors": [
      "Lei Mou",
      "Yitian Zhao",
      "Huazhu Fu",
      "Yonghuai Liu",
      "Jun Cheng",
      "Yalin Zheng",
      "Pan Su",
      "Jianlong Yang",
      "Li Chen",
      "Alejandro F Frang",
      "Masahiro Akiba",
      "Jiang Liu"
    ],
    "author_ids": [],
    "abstract": "Automated detection of curvilinear structures, e.g., blood vessels or nerve\nfibres, from medical and biomedical images is a crucial early step in automatic\nimage interpretation associated to the management of many diseases. Precise\nmeasurement of the morphological changes of these curvilinear organ structures\ninforms clinicians for understanding the mechanism, diagnosis, and treatment of\ne.g. cardiovascular, kidney, eye, lung, and neurological conditions. In this\nwork, we propose a generic and unified convolution neural network for the\nsegmentation of curvilinear structures and illustrate in several 2D/3D medical\nimaging modalities. We introduce a new curvilinear structure segmentation\nnetwork (CS2-Net), which includes a self-attention mechanism in the encoder and\ndecoder to learn rich hierarchical representations of curvilinear structures.\nTwo types of attention modules - spatial attention and channel attention - are\nutilized to enhance the inter-class discrimination and intra-class\nresponsiveness, to further integrate local features with their global\ndependencies and normalization, adaptively. Furthermore, to facilitate the\nsegmentation of curvilinear structures in medical images, we employ a 1x3 and a\n3x1 convolutional kernel to capture boundary features. ...",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07486v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.08146v1",
    "title": "Online Decision Trees with Fairness",
    "authors": [
      "Wenbin Zhang",
      "Liang Zhao"
    ],
    "author_ids": [],
    "abstract": "While artificial intelligence (AI)-based decision-making systems are\nincreasingly popular, significant concerns on the potential discrimination\nduring the AI decision-making process have been observed. For example, the\ndistribution of predictions is usually biased and dependents on the sensitive\nattributes (e.g., gender and ethnicity). Numerous approaches have therefore\nbeen proposed to develop decision-making systems that are\ndiscrimination-conscious by-design, which are typically batch-based and require\nthe simultaneous availability of all the training data for model learning.\nHowever, in the real-world, the data streams usually come on the fly which\nrequires the model to process each input data once \"on arrival\" and without the\nneed for storage and reprocessing. In addition, the data streams might also\nevolve over time, which further requires the model to be able to simultaneously\nadapt to non-stationary data distributions and time-evolving bias patterns,\nwith an effective and robust trade-off between accuracy and fairness. In this\npaper, we propose a novel framework of online decision tree with fairness in\nthe data stream with possible distribution drifting. Specifically, first, we\npropose two novel fairness splitting criteria that encode the data as well as\npossible, while simultaneously removing dependence on the sensitive attributes,\nand further adapts to non-stationary distribution with fine-grained control\nwhen needed. Second, we propose two fairness decision tree online growth\nalgorithms that fulfills different online fair decision-making requirements.\nOur experiments show that our algorithms are able to deal with discrimination\nin massive and non-stationary streaming environments, with a better trade-off\nbetween fairness and predictive performance.",
    "published_date": "2020-10-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.08146v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07431v2",
    "title": "Fairness in Streaming Submodular Maximization: Algorithms and Hardness",
    "authors": [
      "Marwa El Halabi",
      "Slobodan Mitrović",
      "Ashkan Norouzi-Fard",
      "Jakab Tardos",
      "Jakub Tarnawski"
    ],
    "author_ids": [],
    "abstract": "Submodular maximization has become established as the method of choice for\nthe task of selecting representative and diverse summaries of data. However, if\ndatapoints have sensitive attributes such as gender or age, such machine\nlearning algorithms, left unchecked, are known to exhibit bias: under- or\nover-representation of particular groups. This has made the design of fair\nmachine learning algorithms increasingly important. In this work we address the\nquestion: Is it possible to create fair summaries for massive datasets? To this\nend, we develop the first streaming approximation algorithms for submodular\nmaximization under fairness constraints, for both monotone and non-monotone\nfunctions. We validate our findings empirically on exemplar-based clustering,\nmovie recommendation, DPP-based summarization, and maximum coverage in social\nnetworks, showing that fairness constraints do not significantly impact\nutility.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07431v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07414v3",
    "title": "On Cross-Dataset Generalization in Automatic Detection of Online Abuse",
    "authors": [
      "Isar Nejadgholi",
      "Svetlana Kiritchenko"
    ],
    "author_ids": [],
    "abstract": "NLP research has attained high performances in abusive language detection as\na supervised classification task. While in research settings, training and test\ndatasets are usually obtained from similar data samples, in practice systems\nare often applied on data that are different from the training set in topic and\nclass distributions. Also, the ambiguity in class definitions inherited in this\ntask aggravates the discrepancies between source and target datasets. We\nexplore the topic bias and the task formulation bias in cross-dataset\ngeneralization. We show that the benign examples in the Wikipedia Detox dataset\nare biased towards platform-specific topics. We identify these examples using\nunsupervised topic modeling and manual inspection of topics' keywords. Removing\nthese topics increases cross-dataset generalization, without reducing in-domain\nclassification performance. For a robust dataset design, we suggest applying\ninexpensive unsupervised methods to inspect the collected data and downsize the\nnon-generalizable content before manually annotating for class labels.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07414v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07343v3",
    "title": "Causal Multi-Level Fairness",
    "authors": [
      "Vishwali Mhasawade",
      "Rumi Chunara"
    ],
    "author_ids": [],
    "abstract": "Algorithmic systems are known to impact marginalized groups severely, and\nmore so, if all sources of bias are not considered. While work in algorithmic\nfairness to-date has primarily focused on addressing discrimination due to\nindividually linked attributes, social science research elucidates how some\nproperties we link to individuals can be conceptualized as having causes at\nmacro (e.g. structural) levels, and it may be important to be fair to\nattributes at multiple levels. For example, instead of simply considering race\nas a causal, protected attribute of an individual, the cause may be distilled\nas perceived racial discrimination an individual experiences, which in turn can\nbe affected by neighborhood-level factors. This multi-level conceptualization\nis relevant to questions of fairness, as it may not only be important to take\ninto account if the individual belonged to another demographic group, but also\nif the individual received advantaged treatment at the macro-level. In this\npaper, we formalize the problem of multi-level fairness using tools from causal\ninference in a manner that allows one to assess and account for effects of\nsensitive attributes at multiple levels. We show importance of the problem by\nillustrating residual unfairness if macro-level sensitive attributes are not\naccounted for, or included without accounting for their multi-level nature.\nFurther, in the context of a real-world task of predicting income based on\nmacro and individual-level attributes, we demonstrate an approach for\nmitigating unfairness, a result of multi-level sensitive attributes.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07343v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07280v4",
    "title": "On Fair Division under Heterogeneous Matroid Constraints",
    "authors": [
      "Amitay Dror",
      "Michal Feldman",
      "Erel Segal-Halevi"
    ],
    "author_ids": [],
    "abstract": "We study fair allocation of indivisible goods among additive agents with\nfeasibility constraints. In these settings, every agent is restricted to get a\nbundle among a specified set of feasible bundles. Such scenarios have been of\ngreat interest to the AI community due to their applicability to real-world\nproblems. Following some impossibility results, we restrict attention to\nmatroid feasibility constraints that capture natural scenarios, such as the\nallocation of shifts to medical doctors, and the allocation of conference\npapers to referees.\n  We focus on the common fairness notion of envy-freeness up to one good (EF1).\nPrevious algorithms for finding EF1 allocations are either restricted to agents\nwith identical feasibility constraints, or allow free disposal of items. An\nopen problem is the existence of EF1 complete allocations among heterogeneous\nagents, where the heterogeneity is both in the agents' feasibility constraints\nand in their valuations. In this work, we make progress on this problem by\nproviding positive and negative results for different matroid and valuation\ntypes. Among other results, we devise polynomial-time algorithms for finding\nEF1 allocations in the following settings: (i) $n$ agents with heterogeneous\npartition matroids and heterogeneous binary valuations, (ii) 2 agents with\nheterogeneous partition matroids and heterogeneous additive valuations, and\n(iii) at most 3 agents with heterogeneous binary valuations and identical\nbase-orderable matroid constraints.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07280v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07249v5",
    "title": "Environment Inference for Invariant Learning",
    "authors": [
      "Elliot Creager",
      "Jörn-Henrik Jacobsen",
      "Richard Zemel"
    ],
    "author_ids": [],
    "abstract": "Learning models that gracefully handle distribution shifts is central to\nresearch on domain generalization, robust optimization, and fairness. A\npromising formulation is domain-invariant learning, which identifies the key\nissue of learning which features are domain-specific versus domain-invariant.\nAn important assumption in this area is that the training examples are\npartitioned into \"domains\" or \"environments\". Our focus is on the more common\nsetting where such partitions are not provided. We propose EIIL, a general\nframework for domain-invariant learning that incorporates Environment Inference\nto directly infer partitions that are maximally informative for downstream\nInvariant Learning. We show that EIIL outperforms invariant learning methods on\nthe CMNIST benchmark without using environment labels, and significantly\noutperforms ERM on worst-group performance in the Waterbirds and CivilComments\ndatasets. Finally, we establish connections between EIIL and algorithmic\nfairness, which enables EIIL to improve accuracy and calibration in a fair\nprediction problem.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07249v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07139v4",
    "title": "Fairness for Freshness: Optimal Age of Information Based OFDMA Scheduling with Minimal Knowledge",
    "authors": [
      "Bin Han",
      "Yao Zhu",
      "Zhiyuan Jiang",
      "Muxia Sun",
      "Hans D. Schotten"
    ],
    "author_ids": [],
    "abstract": "It is becoming increasingly clear that an important task for wireless\nnetworks is to minimize the age of information (AoI), i.e., the timeliness of\ninformation delivery. While mainstream approaches generally rely on the\nreal-time observation of user AoI and channel state, there has been little\nattention to solve the problem in a complete (or partial) absence of such\nknowledge. In this article, we present a novel study to address the optimal\nblind radio resource scheduling problem in orthogonal frequency division\nmultiplexing access (OFDMA) systems towards minimizing long-term average AoI,\nwhich is proven to be the composition of time-domain-fair clustered round-robin\nand frequency-domain-fair intra-cluster sub-carrier assignment. Heuristic\nsolutions that are near-optimal as shown by simulation results are also\nproposed to effectively improve the performance upon presence of various\ndegrees of extra knowledge, e.g., channel state and AoI.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07139v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.06820v1",
    "title": "Equitable Allocation of Healthcare Resources with Fair Cox Models",
    "authors": [
      "Kamrun Naher Keya",
      "Rashidul Islam",
      "Shimei Pan",
      "Ian Stockwell",
      "James R. Foulds"
    ],
    "author_ids": [],
    "abstract": "Healthcare programs such as Medicaid provide crucial services to vulnerable\npopulations, but due to limited resources, many of the individuals who need\nthese services the most languish on waiting lists. Survival models, e.g. the\nCox proportional hazards model, can potentially improve this situation by\npredicting individuals' levels of need, which can then be used to prioritize\nthe waiting lists. Providing care to those in need can prevent\ninstitutionalization for those individuals, which both improves quality of life\nand reduces overall costs. While the benefits of such an approach are clear,\ncare must be taken to ensure that the prioritization process is fair or\nindependent of demographic information-based harmful stereotypes. In this work,\nwe develop multiple fairness definitions for survival models and corresponding\nfair Cox proportional hazards models to ensure equitable allocation of\nhealthcare resources. We demonstrate the utility of our methods in terms of\nfairness and predictive accuracy on two publicly available survival datasets.",
    "published_date": "2020-10-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06820v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06682v2",
    "title": "Are all negatives created equal in contrastive instance discrimination?",
    "authors": [
      "Tiffany Tianhui Cai",
      "Jonathan Frankle",
      "David J. Schwab",
      "Ari S. Morcos"
    ],
    "author_ids": [],
    "abstract": "Self-supervised learning has recently begun to rival supervised learning on\ncomputer vision tasks. Many of the recent approaches have been based on\ncontrastive instance discrimination (CID), in which the network is trained to\nrecognize two augmented versions of the same instance (a query and positive)\nwhile discriminating against a pool of other instances (negatives). The learned\nrepresentation is then used on downstream tasks such as image classification.\nUsing methodology from MoCo v2 (Chen et al., 2020), we divided negatives by\ntheir difficulty for a given query and studied which difficulty ranges were\nmost important for learning useful representations. We found a minority of\nnegatives -- the hardest 5% -- were both necessary and sufficient for the\ndownstream task to reach nearly full accuracy. Conversely, the easiest 95% of\nnegatives were unnecessary and insufficient. Moreover, the very hardest 0.1% of\nnegatives were unnecessary and sometimes detrimental. Finally, we studied the\nproperties of negatives that affect their hardness, and found that hard\nnegatives were more semantically similar to the query, and that some negatives\nwere more consistently easy or hard than we would expect by chance. Together,\nour results indicate that negatives vary in importance and that CID may benefit\nfrom more intelligent negative treatment.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06682v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.10359v1",
    "title": "Performance of Dual-Augmented Lagrangian Method and Common Spatial Patterns applied in classification of Motor-Imagery BCI",
    "authors": [
      "Aleksandar Miladinović",
      "Miloš Ajčević",
      "Agostino Accardo"
    ],
    "author_ids": [],
    "abstract": "Motor-imagery based brain-computer interfaces (MI-BCI) have the potential to\nbecome ground-breaking technologies for neurorehabilitation, the\nreestablishment of non-muscular communication and commands for patients\nsuffering from neuronal disorders and disabilities, but also outside of\nclinical practice, for video game control and other entertainment purposes.\nHowever, due to the noisy nature of the used EEG signal, reliable BCI systems\nrequire specialized procedures for features optimization and extraction. This\npaper compares the two approaches, the Common Spatial Patterns with Linear\nDiscriminant Analysis classifier (CSP-LDA), widely used in BCI for extracting\nfeatures in Motor Imagery (MI) tasks, and the Dual-Augmented Lagrangian (DAL)\nframework with three different regularization methods: group sparsity with row\ngroups (DAL-GLR), dual-spectrum (DAL-DS) and l1-norm regularization (DAL-L1).\nThe test has been performed on 7 healthy subjects performing 5 BCI-MI sessions\neach. The preliminary results show that DAL-GLR method outperforms standard\nCSP-LDA, presenting 6.9% lower misclassification error (p-value = 0.008) and\ndemonstrate the advantage of DAL framework for MI-BCI.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.10359v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06667v1",
    "title": "Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings",
    "authors": [
      "Vinith M. Suriyakumar",
      "Nicolas Papernot",
      "Anna Goldenberg",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "Machine learning models in health care are often deployed in settings where\nit is important to protect patient privacy. In such settings, methods for\ndifferentially private (DP) learning provide a general-purpose approach to\nlearn models with privacy guarantees. Modern methods for DP learning ensure\nprivacy through mechanisms that censor information judged as too unique. The\nresulting privacy-preserving models, therefore, neglect information from the\ntails of a data distribution, resulting in a loss of accuracy that can\ndisproportionately affect small groups. In this paper, we study the effects of\nDP learning in health care. We use state-of-the-art methods for DP learning to\ntrain privacy-preserving models in clinical prediction tasks, including x-ray\nclassification of images and mortality prediction in time series data. We use\nthese models to perform a comprehensive empirical investigation of the\ntradeoffs between privacy, utility, robustness to dataset shift, and fairness.\nOur results highlight lesser-known limitations of methods for DP learning in\nhealth care, models that exhibit steep tradeoffs between privacy and utility,\nand models whose predictions are disproportionately influenced by large\ndemographic groups in the training data. We discuss the costs and benefits of\ndifferentially private learning in health care.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06667v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06529v5",
    "title": "On the Fairness of Causal Algorithmic Recourse",
    "authors": [
      "Julius von Kügelgen",
      "Amir-Hossein Karimi",
      "Umang Bhatt",
      "Isabel Valera",
      "Adrian Weller",
      "Bernhard Schölkopf"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness is typically studied from the perspective of\npredictions. Instead, here we investigate fairness from the perspective of\nrecourse actions suggested to individuals to remedy an unfavourable\nclassification. We propose two new fairness criteria at the group and\nindividual level, which -- unlike prior work on equalising the average\ngroup-wise distance from the decision boundary -- explicitly account for causal\nrelationships between features, thereby capturing downstream effects of\nrecourse actions performed in the physical world. We explore how our criteria\nrelate to others, such as counterfactual fairness, and show that fairness of\nrecourse is complementary to fairness of prediction. We study theoretically and\nempirically how to enforce fair causal recourse by altering the classifier and\nperform a case study on the Adult dataset. Finally, we discuss whether fairness\nviolations in the data generating process revealed by our criteria may be\nbetter addressed by societal interventions as opposed to constraints on the\nclassifier.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06529v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06401v1",
    "title": "On the Complexity of Some Facet-Defining Inequalities of the QAP-polytope",
    "authors": [
      "Pawan Aurora",
      "Hans Raj Tiwary"
    ],
    "author_ids": [],
    "abstract": "The Quadratic Assignment Problem (QAP) is a well-known NP-hard problem that\nis equivalent to optimizing a linear objective function over the QAP polytope.\nThe QAP polytope with parameter $n$ - \\qappolytope{n} - is defined as the\nconvex hull of rank-$1$ matrices $xx^T$ with $x$ as the vectorized $n\\times n$\npermutation matrices.\n  In this paper we consider all the known exponential-sized families of\nfacet-defining inequalities of the QAP-polytope. We describe a new family of\nvalid inequalities that we show to be facet-defining. We also show that\nmembership testing (and hence optimizing) over some of the known classes of\ninequalities is coNP-complete. We complement our hardness results by showing a\nlower bound of $2^{\\Omega(n)}$ on the extension complexity of all relaxations\nof \\qappolytope{n} for which any of the known classes of inequalities are\nvalid.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CC",
      "68Q17",
      "F.2; G.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06401v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.06398v1",
    "title": "ProportionNet: Balancing Fairness and Revenue for Auction Design with Deep Learning",
    "authors": [
      "Kevin Kuo",
      "Anthony Ostuni",
      "Elizabeth Horishny",
      "Michael J. Curry",
      "Samuel Dooley",
      "Ping-yeh Chiang",
      "Tom Goldstein",
      "John P. Dickerson"
    ],
    "author_ids": [],
    "abstract": "The design of revenue-maximizing auctions with strong incentive guarantees is\na core concern of economic theory. Computational auctions enable online\nadvertising, sourcing, spectrum allocation, and myriad financial markets.\nAnalytic progress in this space is notoriously difficult; since Myerson's 1981\nwork characterizing single-item optimal auctions, there has been limited\nprogress outside of restricted settings. A recent paper by D\\\"utting et al.\ncircumvents analytic difficulties by applying deep learning techniques to,\ninstead, approximate optimal auctions. In parallel, new research from Ilvento\net al. and other groups has developed notions of fairness in the context of\nauction design. Inspired by these advances, in this paper, we extend techniques\nfor approximating auctions using deep learning to address concerns of fairness\nwhile maintaining high revenue and strong incentive guarantees.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06398v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06314v2",
    "title": "Linear Matrix Inequality Design of Exponentially Stabilizing Observer-Based State Feedback Port-Hamiltonian Controllers",
    "authors": [
      "Jesus Toledo",
      "Hector Ramirez",
      "Yongxin Wu",
      "Yann Le Gorrec"
    ],
    "author_ids": [],
    "abstract": "The design of an observer-based state feedback (OBSF) controller with\nguaranteed passivity properties for port-Hamiltonian systems (PHS) is addressed\nusing linear matrix inequalities (LMIs). The observer gain is freely chosen and\nthe LMIs conditions such that the state feedback is equivalent to control by\ninterconnection with an input strictly passive (ISP) and/or an output strictly\npassive (OSP) and zero state detectable (ZSD) port-Hamiltonian controller are\nestablished. It is shown that the proposed controller exponentially stabilizes\na class of infinite-dimensional PHS and asymptotically stabilizes a class of\nfinite-dimensional non-linear PHS. A Timoshenko beam model and a\nmicroelectromechanical system are used to illustrate the proposed approach.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06314v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.06203v2",
    "title": "Mitigating Gender Bias in Machine Translation with Target Gender Annotations",
    "authors": [
      "Artūrs Stafanovičs",
      "Toms Bergmanis",
      "Mārcis Pinnis"
    ],
    "author_ids": [],
    "abstract": "When translating \"The secretary asked for details.\" to a language with\ngrammatical gender, it might be necessary to determine the gender of the\nsubject \"secretary\". If the sentence does not contain the necessary\ninformation, it is not always possible to disambiguate. In such cases, machine\ntranslation systems select the most common translation option, which often\ncorresponds to the stereotypical translations, thus potentially exacerbating\nprejudice and marginalisation of certain groups and people. We argue that the\ninformation necessary for an adequate translation can not always be deduced\nfrom the sentence being translated or even might depend on external knowledge.\nTherefore, in this work, we propose to decouple the task of acquiring the\nnecessary information from the task of learning to translate correctly when\nsuch information is available. To that end, we present a method for training\nmachine translation systems to use word-level annotations containing\ninformation about subject's gender. To prepare training data, we annotate\nregular source language words with grammatical gender information of the\ncorresponding target language words. Using such data to train machine\ntranslation systems reduces their reliance on gender stereotypes when\ninformation about the subject's gender is available. Our experiments on five\nlanguage pairs show that this allows improving accuracy on the WinoMT test set\nby up to 25.8 percentage points.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06203v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06121v2",
    "title": "To be Robust or to be Fair: Towards Fairness in Adversarial Training",
    "authors": [
      "Han Xu",
      "Xiaorui Liu",
      "Yaxin Li",
      "Anil K. Jain",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "Adversarial training algorithms have been proved to be reliable to improve\nmachine learning models' robustness against adversarial examples. However, we\nfind that adversarial training algorithms tend to introduce severe disparity of\naccuracy and robustness between different groups of data. For instance, a PGD\nadversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67%\nPGD l-infty-8 robust accuracy on the class \"automobile\" but only 65% and 17% on\nthe class \"cat\". This phenomenon happens in balanced datasets and does not\nexist in naturally trained models when only using clean samples. In this work,\nwe empirically and theoretically show that this phenomenon can happen under\ngeneral adversarial training algorithms which minimize DNN models' robust\nerrors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL)\nframework to mitigate this unfairness problem when doing adversarial defenses.\nExperimental results validate the effectiveness of FRL.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06121v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06113v1",
    "title": "FaiR-N: Fair and Robust Neural Networks for Structured Data",
    "authors": [
      "Shubham Sharma",
      "Alan H. Gee",
      "David Paydarfar",
      "Joydeep Ghosh"
    ],
    "author_ids": [],
    "abstract": "Fairness in machine learning is crucial when individuals are subject to\nautomated decisions made by models in high-stake domains. Organizations that\nemploy these models may also need to satisfy regulations that promote\nresponsible and ethical A.I. While fairness metrics relying on comparing model\nerror rates across subpopulations have been widely investigated for the\ndetection and mitigation of bias, fairness in terms of the equalized ability to\nachieve recourse for different protected attribute groups has been relatively\nunexplored. We present a novel formulation for training neural networks that\nconsiders the distance of data points to the decision boundary such that the\nnew objective: (1) reduces the average distance to the decision boundary\nbetween two groups for individuals subject to a negative outcome in each group,\ni.e. the network is more fair with respect to the ability to obtain recourse,\nand (2) increases the average distance of data points to the boundary to\npromote adversarial robustness. We demonstrate that training with this loss\nyields more fair and robust neural networks with similar accuracies to models\ntrained without it. Moreover, we qualitatively motivate and empirically show\nthat reducing recourse disparity across groups also improves fairness measures\nthat rely on error rates. To the best of our knowledge, this is the first time\nthat recourse capabilities across groups are considered to train fairer neural\nnetworks, and a relation between error rates based fairness and recourse based\nfairness is investigated.",
    "published_date": "2020-10-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06113v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06059v1",
    "title": "A Framework for Addressing the Risks and Opportunities In AI-Supported Virtual Health Coaches",
    "authors": [
      "Sonia Baee",
      "Mark Rucker",
      "Anna Baglione",
      "Mawulolo K. Ameko",
      "Laura Barnes"
    ],
    "author_ids": [],
    "abstract": "Virtual coaching has rapidly evolved into a foundational component of modern\nclinical practice. At a time when healthcare professionals are in short supply\nand the demand for low-cost treatments is ever-increasing, virtual health\ncoaches (VHCs) offer intervention-on-demand for those limited by finances or\ngeographic access to care. More recently, AI-powered virtual coaches have\nbecome a viable complement to human coaches. However, the push for AI-powered\ncoaching systems raises several important issues for researchers, designers,\nclinicians, and patients. In this paper, we present a novel framework to guide\nthe design and development of virtual coaching systems. This framework augments\na traditional data science pipeline with four key guiding goals: reliability,\nfairness, engagement, and ethics.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06059v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06047v1",
    "title": "Artificial Intelligence, speech and language processing approaches to monitoring Alzheimer's Disease: a systematic review",
    "authors": [
      "Sofia de la Fuente Garcia",
      "Craig Ritchie",
      "Saturnino Luz"
    ],
    "author_ids": [],
    "abstract": "Language is a valuable source of clinical information in Alzheimer's Disease,\nas it declines concurrently with neurodegeneration. Consequently, speech and\nlanguage data have been extensively studied in connection with its diagnosis.\nThis paper summarises current findings on the use of artificial intelligence,\nspeech and language processing to predict cognitive decline in the context of\nAlzheimer's Disease, detailing current research procedures, highlighting their\nlimitations and suggesting strategies to address them. We conducted a\nsystematic review of original research between 2000 and 2019, registered in\nPROSPERO (reference CRD42018116606). An interdisciplinary search covered six\ndatabases on engineering (ACM and IEEE), psychology (PsycINFO), medicine\n(PubMed and Embase) and Web of Science. Bibliographies of relevant papers were\nscreened until December 2019. From 3,654 search results 51 articles were\nselected against the eligibility criteria. Four tables summarise their\nfindings: study details (aim, population, interventions, comparisons, methods\nand outcomes), data details (size, type, modalities, annotation, balance,\navailability and language of study), methodology (pre-processing, feature\ngeneration, machine learning, evaluation and results) and clinical\napplicability (research implications, clinical potential, risk of bias and\nstrengths/limitations). While promising results are reported across nearly all\n51 studies, very few have been implemented in clinical research or practice. We\nconcluded that the main limitations of the field are poor standardisation,\nlimited comparability of results, and a degree of disconnect between study aims\nand clinical applications. Attempts to close these gaps should support\ntranslation of future research into clinical practice.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CL",
      "eess.AS",
      "J.3; I.2.7; I.2.6; I.5.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06047v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06018v1",
    "title": "Gender Coreference and Bias Evaluation at WMT 2020",
    "authors": [
      "Tom Kocmi",
      "Tomasz Limisiewicz",
      "Gabriel Stanovsky"
    ],
    "author_ids": [],
    "abstract": "Gender bias in machine translation can manifest when choosing gender\ninflections based on spurious gender correlations. For example, always\ntranslating doctors as men and nurses as women. This can be particularly\nharmful as models become more popular and deployed within commercial systems.\nOur work presents the largest evidence for the phenomenon in more than 19\nsystems submitted to the WMT over four diverse target languages: Czech, German,\nPolish, and Russian. To achieve this, we use WinoMT, a recent automatic test\nsuite which examines gender coreference and bias when translating from English\nto languages with grammatical gender. We extend WinoMT to handle two new\nlanguages tested in WMT: Polish and Czech. We find that all systems\nconsistently use spurious correlations in the data rather than meaningful\ncontextual information.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06018v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05995v2",
    "title": "A Skew-Sensitive Evaluation Framework for Imbalanced Data Classification",
    "authors": [
      "Min Du",
      "Nesime Tatbul",
      "Brian Rivers",
      "Akhilesh Kumar Gupta",
      "Lucas Hu",
      "Wei Wang",
      "Ryan Marcus",
      "Shengtian Zhou",
      "Insup Lee",
      "Justin Gottschlich"
    ],
    "author_ids": [],
    "abstract": "Class distribution skews in imbalanced datasets may lead to models with\nprediction bias towards majority classes, making fair assessment of classifiers\na challenging task. Metrics such as Balanced Accuracy are commonly used to\nevaluate a classifier's prediction performance under such scenarios. However,\nthese metrics fall short when classes vary in importance. In this paper, we\npropose a simple and general-purpose evaluation framework for imbalanced data\nclassification that is sensitive to arbitrary skews in class cardinalities and\nimportances. Experiments with several state-of-the-art classifiers tested on\nreal-world datasets from three different domains show the effectiveness of our\nframework - not only in evaluating and ranking classifiers, but also training\nthem.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05995v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05994v1",
    "title": "Improving Text Generation with Student-Forcing Optimal Transport",
    "authors": [
      "Guoyin Wang",
      "Chunyuan Li",
      "Jianqiao Li",
      "Hao Fu",
      "Yuh-Chen Lin",
      "Liqun Chen",
      "Yizhe Zhang",
      "Chenyang Tao",
      "Ruiyi Zhang",
      "Wenlin Wang",
      "Dinghan Shen",
      "Qian Yang",
      "Lawrence Carin"
    ],
    "author_ids": [],
    "abstract": "Neural language models are often trained with maximum likelihood estimation\n(MLE), where the next word is generated conditioned on the ground-truth word\ntokens. During testing, however, the model is instead conditioned on previously\ngenerated tokens, resulting in what is termed exposure bias. To reduce this gap\nbetween training and testing, we propose using optimal transport (OT) to match\nthe sequences generated in these two modes. An extension is further proposed to\nimprove the OT learning, based on the structural and contextual information of\nthe text sequences. The effectiveness of the proposed method is validated on\nmachine translation, text summarization, and text generation tasks.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05994v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.12019v2",
    "title": "A New Charter of Ethics and Rights of Artificial Consciousness in a Human World",
    "authors": [
      "Markian Hromiak"
    ],
    "author_ids": [],
    "abstract": "Taking the stance that artificially conscious agents should be given\nhuman-like rights, in this paper we attempt to define consciousness, aggregate\nexisting universal human rights, analyze robotic laws with roots in both\nreality and science fiction, and synthesize everything to create a new\nrobot-ethical charter. By restricting the problem-space of possible levels of\nconscious beings to human-like, we succeed in developing a working definition\nof consciousness for social strong AI which focuses on human-like creativity\nbeing exhibited as a third-person observable phenomenon. Creativity is then\nextrapolated to represent first-person functionality, fulfilling the\nfirst/third-person feature of consciousness. Next, several sources of existing\nrights and rules, both for humans and robots, are analyzed and, along with\nsupplementary informal reports, synthesized to create articles for an additive\ncharter which compliments the United Nation's Universal Declaration of Human\nRights. Finally, the charter is presented and the paper concludes with the\nconditions for amending the charter, as well as recommendations for further\ncharters.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12019v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05544v3",
    "title": "A New Cooperative Framework for a Fair and Cost-Optimal Allocation of Resources within a Low Voltage Electricity Community",
    "authors": [
      "Martin Hupez",
      "Jean-François Toubeau",
      "Zacharie De Grève",
      "François Vallée"
    ],
    "author_ids": [],
    "abstract": "This paper presents an original collaborative framework for power exchanges\ninside a low voltage community. The community seeks to minimize its total costs\nby scheduling on a daily basis the resources of its members. In this respect,\ntheir flexibility such as excess storage capacity, unused local generation or\nshiftable load are exploited. Total costs include not only the energy\ncommodity, but also grid fees associated to the community operation, through\nthe integration of power flow constraints. In order to share the community\ncosts in a fair manner, two different cost distributions are proposed. The\nfirst one adopts a distribution key based on the Shapley value, while the other\nrelies on a natural consensus defined by a Nash equilibrium. Outcomes show that\nboth collaboration schemes lead to important savings for all individual\nmembers. In particular, it is observed that the Shapley-based solution gives\nmore value to mobilized flexible resources, whereas the Nash equilibrium\nrewards the potential flexibility consent of end-users.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.GT",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05544v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.05495v1",
    "title": "Increasing the Robustness of Semantic Segmentation Models with Painting-by-Numbers",
    "authors": [
      "Christoph Kamann",
      "Burkhard Güssefeld",
      "Robin Hutmacher",
      "Jan Hendrik Metzen",
      "Carsten Rother"
    ],
    "author_ids": [],
    "abstract": "For safety-critical applications such as autonomous driving, CNNs have to be\nrobust with respect to unavoidable image corruptions, such as image noise.\nWhile previous works addressed the task of robust prediction in the context of\nfull-image classification, we consider it for dense semantic segmentation. We\nbuild upon an insight from image classification that output robustness can be\nimproved by increasing the network-bias towards object shapes. We present a new\ntraining schema that increases this shape bias. Our basic idea is to\nalpha-blend a portion of the RGB training images with faked images, where each\nclass-label is given a fixed, randomly chosen color that is not likely to\nappear in real imagery. This forces the network to rely more strongly on shape\ncues. We call this data augmentation technique ``Painting-by-Numbers''. We\ndemonstrate the effectiveness of our training schema for DeepLabv3+ with\nvarious network backbones, MobileNet-V2, ResNets, and Xception, and evaluate it\non the Cityscapes dataset. With respect to our 16 different types of image\ncorruptions and 5 different network backbones, we are in 74% better than\ntraining with clean data. For cases where we are worse than a model trained\nwithout our training schema, it is mostly only marginally worse. However, for\nsome image corruptions such as images with noise, we see a considerable\nperformance gain of up to 25%.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05495v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05434v2",
    "title": "Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness",
    "authors": [
      "Jessie Finocchiaro",
      "Roland Maio",
      "Faidra Monachou",
      "Gourab K Patro",
      "Manish Raghavan",
      "Ana-Andreea Stoica",
      "Stratis Tsirtsis"
    ],
    "author_ids": [],
    "abstract": "Decision-making systems increasingly orchestrate our world: how to intervene\non the algorithmic components to build fair and equitable systems is therefore\na question of utmost importance; one that is substantially complicated by the\ncontext-dependent nature of fairness and discrimination. Modern decision-making\nsystems that involve allocating resources or information to people (e.g.,\nschool choice, advertising) incorporate machine-learned predictions in their\npipelines, raising concerns about potential strategic behavior or constrained\nallocation, concerns usually tackled in the context of mechanism design.\nAlthough both machine learning and mechanism design have developed frameworks\nfor addressing issues of fairness and equity, in some complex decision-making\nsystems, neither framework is individually sufficient. In this paper, we\ndevelop the position that building fair decision-making systems requires\novercoming these limitations which, we argue, are inherent to each field. Our\nultimate objective is to build an encompassing framework that cohesively\nbridges the individual frameworks of mechanism design and machine learning. We\nbegin to lay the ground work towards this goal by comparing the perspective\neach discipline takes on fair decision-making, teasing out the lessons each\nfield has taught and can teach the other, and highlighting application domains\nthat require a strong collaboration between these disciplines.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05434v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06499v1",
    "title": "LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis",
    "authors": [
      "Quan Huu Cap",
      "Hiroki Tani",
      "Hiroyuki Uga",
      "Satoshi Kagiwada",
      "Hitoshi Iyatomi"
    ],
    "author_ids": [],
    "abstract": "The collection of high-resolution training data is crucial in building robust\nplant disease diagnosis systems, since such data have a significant impact on\ndiagnostic performance. However, they are very difficult to obtain and are not\nalways available in practice. Deep learning-based techniques, and particularly\ngenerative adversarial networks (GANs), can be applied to generate high-quality\nsuper-resolution images, but these methods often produce unexpected artifacts\nthat can lower the diagnostic performance. In this paper, we propose a novel\nartifact-suppression super-resolution method that is specifically designed for\ndiagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution\n(LASSR). Thanks to its own artifact removal module that detects and suppresses\nartifacts to a considerable extent, LASSR can generate much more pleasing,\nhigh-quality images compared to the state-of-the-art ESRGAN model. Experiments\nbased on a five-class cucumber disease (including healthy) discrimination model\nshow that training with data generated by LASSR significantly boosts the\nperformance on an unseen test dataset by nearly 22% compared with the baseline,\nand that our approach is more than 2% better than a model trained with images\ngenerated by ESRGAN.",
    "published_date": "2020-10-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06499v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05365v1",
    "title": "ArXiving Before Submission Helps Everyone",
    "authors": [
      "Dmytro Mishkin",
      "Amy Tabb",
      "Jiri Matas"
    ],
    "author_ids": [],
    "abstract": "We claim, and present evidence, that allowing arXiv publication before a\nconference or journal submission benefits researchers, especially early career,\nas well as the whole scientific community. Specifically, arXiving helps\nprofessional identity building, protects against independent re-discovery, idea\ntheft and gate-keeping; it facilitates open research result distribution and\nreduces inequality. The advantages dwarf the drawbacks -- mainly the relative\nincrease in acceptance rate of papers of well-known authors -- which studies\nshow to be marginal. Analyzing the pros and cons of arXiving papers, we\nconclude that requiring preprints be anonymous is nearly as detrimental as not\nallowing them. We see no reasons why anyone but the authors should decide\nwhether to arXiv or not.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05365v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05358v1",
    "title": "Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)",
    "authors": [
      "Alex Warstadt",
      "Yian Zhang",
      "Haau-Sing Li",
      "Haokun Liu",
      "Samuel R. Bowman"
    ],
    "author_ids": [],
    "abstract": "One reason pretraining on self-supervised linguistic tasks is effective is\nthat it teaches models features that are helpful for language understanding.\nHowever, we want pretrained models to learn not only to represent linguistic\nfeatures, but also to use those features preferentially during fine-turning.\nWith this goal in mind, we introduce a new English-language diagnostic set\ncalled MSGS (the Mixed Signals Generalization Set), which consists of 20\nambiguous binary classification tasks that we use to test whether a pretrained\nmodel prefers linguistic or surface generalizations during fine-tuning. We\npretrain RoBERTa models from scratch on quantities of data ranging from 1M to\n1B words and compare their performance on MSGS to the publicly available\nRoBERTa-base. We find that models can learn to represent linguistic features\nwith little pretraining data, but require far more data to learn to prefer\nlinguistic generalizations over surface ones. Eventually, with about 30B words\nof pretraining data, RoBERTa-base does demonstrate a linguistic bias with some\nregularity. We conclude that while self-supervised pretraining is an effective\nway to learn helpful inductive biases, there is likely room to improve the rate\nat which models learn which features matter.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05358v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05353v1",
    "title": "Local Connectivity in Centroid Clustering",
    "authors": [
      "Deepak P"
    ],
    "author_ids": [],
    "abstract": "Clustering is a fundamental task in unsupervised learning, one that targets\nto group a dataset into clusters of similar objects. There has been recent\ninterest in embedding normative considerations around fairness within\nclustering formulations. In this paper, we propose 'local connectivity' as a\ncrucial factor in assessing membership desert in centroid clustering. We use\nlocal connectivity to refer to the support offered by the local neighborhood of\nan object towards supporting its membership to the cluster in question. We\nmotivate the need to consider local connectivity of objects in cluster\nassignment, and provide ways to quantify local connectivity in a given\nclustering. We then exploit concepts from density-based clustering and devise\nLOFKM, a clustering method that seeks to deepen local connectivity in\nclustering outputs, while staying within the framework of centroid clustering.\nThrough an empirical evaluation over real-world datasets, we illustrate that\nLOFKM achieves notable improvements in local connectivity at reasonable costs\nto clustering quality, illustrating the effectiveness of the method.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05353v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07054v1",
    "title": "Representativity Fairness in Clustering",
    "authors": [
      "Deepak P",
      "Savitha Sam Abraham"
    ],
    "author_ids": [],
    "abstract": "Incorporating fairness constructs into machine learning algorithms is a topic\nof much societal importance and recent interest. Clustering, a fundamental task\nin unsupervised learning that manifests across a number of web data scenarios,\nhas also been subject of attention within fair ML research. In this paper, we\ndevelop a novel notion of fairness in clustering, called representativity\nfairness. Representativity fairness is motivated by the need to alleviate\ndisparity across objects' proximity to their assigned cluster representatives,\nto aid fairer decision making. We illustrate the importance of representativity\nfairness in real-world decision making scenarios involving clustering and\nprovide ways of quantifying objects' representativity and fairness over it. We\ndevelop a new clustering formulation, RFKM, that targets to optimize for\nrepresentativity fairness along with clustering quality. Inspired by the\n$K$-Means framework, RFKM incorporates novel loss terms to formulate an\nobjective function. The RFKM objective and optimization approach guides it\ntowards clustering configurations that yield higher representativity fairness.\nThrough an empirical evaluation over a variety of public datasets, we establish\nthe effectiveness of our method. We illustrate that we are able to\nsignificantly improve representativity fairness at only marginal impact to\nclustering quality.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07054v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05338v1",
    "title": "We Can Detect Your Bias: Predicting the Political Ideology of News Articles",
    "authors": [
      "Ramy Baly",
      "Giovanni Da San Martino",
      "James Glass",
      "Preslav Nakov"
    ],
    "author_ids": [],
    "abstract": "We explore the task of predicting the leading political ideology or bias of\nnews articles. First, we collect and release a large dataset of 34,737 articles\nthat were manually annotated for political ideology -left, center, or right-,\nwhich is well-balanced across both topics and media. We further use a\nchallenging experimental setup where the test examples come from media that\nwere not seen during training, which prevents the model from learning to detect\nthe source of the target news article instead of predicting its political\nideology. From a modeling perspective, we propose an adversarial media\nadaptation, as well as a specially adapted triplet loss. We further add\nbackground information about the source, and we show that it is quite helpful\nfor improving article-level prediction. Our experimental results show very\nsizable improvements over using state-of-the-art pre-trained Transformers in\nthis challenging setup.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05333v1",
    "title": "Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task",
    "authors": [
      "Danielle Saunders",
      "Bill Byrne"
    ],
    "author_ids": [],
    "abstract": "The 2020 WMT Biomedical translation task evaluated Medline abstract\ntranslations. This is a small-domain translation task, meaning limited relevant\ntraining data with very distinct style and vocabulary. Models trained on such\ndata are susceptible to exposure bias effects, particularly when training\nsentence pairs are imperfect translations of each other. This can result in\npoor behaviour during inference if the model learns to neglect the source\nsentence.\n  The UNICAM entry addresses this problem during fine-tuning using a robust\nvariant on Minimum Risk Training. We contrast this approach with data-filtering\nto remove `problem' training examples. Under MRT fine-tuning we obtain good\nresults for both directions of English-German and English-Spanish biomedical\ntranslation. In particular we achieve the best English-to-Spanish translation\nresult and second-best Spanish-to-English result, despite using only single\nmodels with no ensembling.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05332v2",
    "title": "Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It",
    "authors": [
      "Danielle Saunders",
      "Rosie Sallis",
      "Bill Byrne"
    ],
    "author_ids": [],
    "abstract": "Neural Machine Translation (NMT) has been shown to struggle with grammatical\ngender that is dependent on the gender of human referents, which can cause\ngender bias effects. Many existing approaches to this problem seek to control\ngender inflection in the target language by explicitly or implicitly adding a\ngender feature to the source sentence, usually at the sentence level.\n  In this paper we propose schemes for incorporating explicit word-level gender\ninflection tags into NMT. We explore the potential of this gender-inflection\ncontrolled translation when the gender feature can be determined from a human\nreference, or when a test sentence can be automatically gender-tagged,\nassessing on English-to-Spanish and English-to-German translation.\n  We find that simple existing approaches can over-generalize a gender-feature\nto multiple entities in a sentence, and suggest effective alternatives in the\nform of tagged coreference adaptation data. We also propose an extension to\nassess translations of gender-neutral entities from English given a\ncorresponding linguistic convention, such as a non-binary inflection, in the\ntarget language.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05332v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05170v3",
    "title": "What causes the test error? Going beyond bias-variance via ANOVA",
    "authors": [
      "Licong Lin",
      "Edgar Dobriban"
    ],
    "author_ids": [],
    "abstract": "Modern machine learning methods are often overparametrized, allowing\nadaptation to the data at a fine level. This can seem puzzling; in the worst\ncase, such models do not need to generalize. This puzzle inspired a great\namount of work, arguing when overparametrization reduces test error, in a\nphenomenon called \"double descent\". Recent work aimed to understand in greater\ndepth why overparametrization is helpful for generalization. This leads to\ndiscovering the unimodality of variance as a function of the level of\nparametrization, and to decomposing the variance into that arising from label\nnoise, initialization, and randomness in the training data to understand the\nsources of the error.\n  In this work we develop a deeper understanding of this area. Specifically, we\npropose using the analysis of variance (ANOVA) to decompose the variance in the\ntest error in a symmetric way, for studying the generalization performance of\ncertain two-layer linear and non-linear networks. The advantage of the analysis\nof variance is that it reveals the effects of initialization, label noise, and\ntraining data more clearly than prior approaches. Moreover, we also study the\nmonotonicity and unimodality of the variance components. While prior work\nstudied the unimodality of the overall variance, we study the properties of\neach term in variance decomposition.\n  One key insight is that in typical settings, the interaction between training\nsamples and initialization can dominate the variance; surprisingly being larger\nthan their marginal effect. Also, we characterize \"phase transitions\" where the\nvariance changes from unimodal to monotone. On a technical level, we leverage\nadvanced deterministic equivalent techniques for Haar random matrices, that --\nto our knowledge -- have not yet been used in the area. We also verify our\nresults in numerical simulations and on empirical data examples.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05170v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05166v3",
    "title": "Robust Fairness under Covariate Shift",
    "authors": [
      "Ashkan Rezaei",
      "Anqi Liu",
      "Omid Memarrast",
      "Brian Ziebart"
    ],
    "author_ids": [],
    "abstract": "Making predictions that are fair with regard to protected group membership\n(race, gender, age, etc.) has become an important requirement for\nclassification algorithms. Existing techniques derive a fair model from sampled\nlabeled data relying on the assumption that training and testing data are\nidentically and independently drawn (iid) from the same distribution. In\npractice, distribution shift can and does occur between training and testing\ndatasets as the characteristics of individuals interacting with the machine\nlearning system change. We investigate fairness under covariate shift, a\nrelaxation of the iid assumption in which the inputs or covariates change while\nthe conditional label distribution remains the same. We seek fair decisions\nunder these assumptions on target data with unknown labels. We propose an\napproach that obtains the predictor that is robust to the worst-case in terms\nof target performance while satisfying target fairness requirements and\nmatching statistical properties of the source data. We demonstrate the benefits\nof our approach on benchmark prediction tasks.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05166v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05137v2",
    "title": "An Open Review of OpenReview: A Critical Analysis of the Machine Learning Conference Review Process",
    "authors": [
      "David Tran",
      "Alex Valtchanov",
      "Keshav Ganapathy",
      "Raymond Feng",
      "Eric Slud",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "author_ids": [],
    "abstract": "Mainstream machine learning conferences have seen a dramatic increase in the\nnumber of participants, along with a growing range of perspectives, in recent\nyears. Members of the machine learning community are likely to overhear\nallegations ranging from randomness of acceptance decisions to institutional\nbias. In this work, we critically analyze the review process through a\ncomprehensive study of papers submitted to ICLR between 2017 and 2020. We\nquantify reproducibility/randomness in review scores and acceptance decisions,\nand examine whether scores correlate with paper impact. Our findings suggest\nstrong institutional bias in accept/reject decisions, even after controlling\nfor paper quality. Furthermore, we find evidence for a gender gap, with female\nauthors receiving lower scores, lower acceptance rates, and fewer citations per\npaper than their male counterparts. We conclude our work with recommendations\nfor future conference organizers.",
    "published_date": "2020-10-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05137v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05084v2",
    "title": "Reflexive Design for Fairness and Other Human Values in Formal Models",
    "authors": [
      "Benjamin Fish",
      "Luke Stark"
    ],
    "author_ids": [],
    "abstract": "Algorithms and other formal models purportedly incorporating human values\nlike fairness have grown increasingly popular in computer science. In response\nto sociotechnical challenges in the use of these models, designers and\nresearchers have taken widely divergent positions on how formal models\nincorporating aspects of human values should be used: encouraging their use,\nmoving away from them, or ignoring the normative consequences altogether. In\nthis paper, we seek to resolve these divergent positions by identifying the\nmain conceptual limits of formal modeling, and develop four reflexive\nvalues--value fidelity, appropriate accuracy, value legibility, and value\ncontestation--vital for incorporating human values adequately into formal\nmodels. We then provide a brief methodology for reflexively designing formal\nmodels incorporating human values.",
    "published_date": "2020-10-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05084v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.05057v1",
    "title": "Fairness-aware Agnostic Federated Learning",
    "authors": [
      "Wei Du",
      "Depeng Xu",
      "Xintao Wu",
      "Hanghang Tong"
    ],
    "author_ids": [],
    "abstract": "Federated learning is an emerging framework that builds centralized machine\nlearning models with training data distributed across multiple devices. Most of\nthe previous works about federated learning focus on the privacy protection and\ncommunication cost reduction. However, how to achieve fairness in federated\nlearning is under-explored and challenging especially when testing data\ndistribution is different from training distribution or even unknown.\nIntroducing simple fairness constraints on the centralized model cannot achieve\nmodel fairness on unknown testing data. In this paper, we develop a\nfairness-aware agnostic federated learning framework (AgnosticFair) to deal\nwith the challenge of unknown testing distribution. We use kernel reweighing\nfunctions to assign a reweighing value on each training sample in both loss\nfunction and fairness constraint. Therefore, the centralized model built from\nAgnosticFair can achieve high accuracy and fairness guarantee on unknown\ntesting data. Moreover, the built model can be directly applied to local sites\nas it guarantees fairness on local data distributions. To our best knowledge,\nthis is the first work to achieve fairness in federated learning. Experimental\nresults on two real datasets demonstrate the effectiveness in terms of both\nutility and fairness under data shift scenarios.",
    "published_date": "2020-10-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05057v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04840v1",
    "title": "CryptoCredit: Securely Training Fair Models",
    "authors": [
      "Leo de Castro",
      "Jiahao Chen",
      "Antigoni Polychroniadou"
    ],
    "author_ids": [],
    "abstract": "When developing models for regulated decision making, sensitive features like\nage, race and gender cannot be used and must be obscured from model developers\nto prevent bias. However, the remaining features still need to be tested for\ncorrelation with sensitive features, which can only be done with the knowledge\nof those features. We resolve this dilemma using a fully homomorphic encryption\nscheme, allowing model developers to train linear regression and logistic\nregression models and test them for possible bias without ever revealing the\nsensitive features in the clear. We demonstrate how it can be applied to\nleave-one-out regression testing, and show using the adult income data set that\nour method is practical to run.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML",
      "62J02, 68P25, 94A60",
      "E.3; I.2.0; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04840v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04824v1",
    "title": "A Cross-Level Information Transmission Network for Predicting Phenotype from New Genotype: Application to Cancer Precision Medicine",
    "authors": [
      "Di He",
      "Lei Xie"
    ],
    "author_ids": [],
    "abstract": "An unsolved fundamental problem in biology and ecology is to predict\nobservable traits (phenotypes) from a new genetic constitution (genotype) of an\norganism under environmental perturbations (e.g., drug treatment). The\nemergence of multiple omics data provides new opportunities but imposes great\nchallenges in the predictive modeling of genotype-phenotype associations.\nFirstly, the high-dimensionality of genomics data and the lack of labeled data\noften make the existing supervised learning techniques less successful.\nSecondly, it is a challenging task to integrate heterogeneous omics data from\ndifferent resources. Finally, the information transmission from DNA to\nphenotype involves multiple intermediate levels of RNA, protein, metabolite,\netc. The higher-level features (e.g., gene expression) usually have stronger\ndiscriminative power than the lower level features (e.g., somatic mutation). To\naddress above issues, we proposed a novel Cross-LEvel Information Transmission\nnetwork (CLEIT) framework. CLEIT aims to explicitly model the asymmetrical\nmulti-level organization of the biological system. Inspired by domain\nadaptation, CLEIT first learns the latent representation of high-level domain\nthen uses it as ground-truth embedding to improve the representation learning\nof the low-level domain in the form of contrastive loss. In addition, we adopt\na pre-training-fine-tuning approach to leveraging the unlabeled heterogeneous\nomics data to improve the generalizability of CLEIT. We demonstrate the\neffectiveness and performance boost of CLEIT in predicting anti-cancer drug\nsensitivity from somatic mutations via the assistance of gene expressions when\ncompared with state-of-the-art methods.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "q-bio.GN"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04824v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04809v2",
    "title": "Lattice (List) Decoding Near Minkowski's Inequality",
    "authors": [
      "Ethan Mook",
      "Chris Peikert"
    ],
    "author_ids": [],
    "abstract": "Minkowski proved that any $n$-dimensional lattice of unit determinant has a\nnonzero vector of Euclidean norm at most $\\sqrt{n}$; in fact, there are\n$2^{\\Omega(n)}$ such lattice vectors. Lattices whose minimum distances come\nclose to Minkowski's bound provide excellent sphere packings and\nerror-correcting codes in $\\mathbb{R}^{n}$.\n  The focus of this work is a certain family of efficiently constructible\n$n$-dimensional lattices due to Barnes and Sloane, whose minimum distances are\nwithin an $O(\\sqrt{\\log n})$ factor of Minkowski's bound. Our primary\ncontribution is a polynomial-time algorithm that list decodes this family to\ndistances approaching $1/\\sqrt{2}$ of the minimum distance. The main technique\nis to decode Reed-Solomon codes under error measured in the Euclidean norm,\nusing the Koetter-Vardy \"soft decision\" variant of the Guruswami-Sudan\nlist-decoding algorithm.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.DS",
      "math.IT",
      "E.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04809v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.04687v2",
    "title": "A Series of Unfortunate Counterfactual Events: the Role of Time in Counterfactual Explanations",
    "authors": [
      "Andrea Ferrario",
      "Michele Loi"
    ],
    "author_ids": [],
    "abstract": "Counterfactual explanations are a prominent example of post-hoc\ninterpretability methods in the explainable Artificial Intelligence research\ndomain. They provide individuals with alternative scenarios and a set of\nrecommendations to achieve a sought-after machine learning model outcome.\nRecently, the literature has identified desiderata of counterfactual\nexplanations, such as feasibility, actionability and sparsity that should\nsupport their applicability in real-world contexts. However, we show that the\nliterature has neglected the problem of the time dependency of counterfactual\nexplanations. We argue that, due to their time dependency and because of the\nprovision of recommendations, even feasible, actionable and sparse\ncounterfactual explanations may not be appropriate in real-world applications.\nThis is due to the possible emergence of what we call \"unfortunate\ncounterfactual events.\" These events may occur due to the retraining of machine\nlearning models whose outcomes have to be explained via counterfactual\nexplanation. Series of unfortunate counterfactual events frustrate the efforts\nof those individuals who successfully implemented the recommendations of\ncounterfactual explanations. This negatively affects people's trust in the\nability of institutions to provide machine learning-supported decisions\nconsistently. We introduce an approach to address the problem of the emergence\nof unfortunate counterfactual events that makes use of histories of\ncounterfactual explanations. In the final part of the paper we propose an\nethical analysis of two distinct strategies to cope with the challenge of\nunfortunate counterfactual events. We show that they respond to an ethically\nresponsible imperative to preserve the trustworthiness of credit lending\norganizations, the decision models they employ, and the social-economic\nfunction of credit lending.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04687v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05785v3",
    "title": "Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification",
    "authors": [
      "Oren Nuriel",
      "Sagie Benaim",
      "Lior Wolf"
    ],
    "author_ids": [],
    "abstract": "Recent work has shown that convolutional neural network classifiers overly\nrely on texture at the expense of shape cues. We make a similar but different\ndistinction between shape and local image cues, on the one hand, and global\nimage statistics, on the other. Our method, called Permuted Adaptive Instance\nNormalization (pAdaIN), reduces the representation of global statistics in the\nhidden layers of image classifiers. pAdaIN samples a random permutation $\\pi$\nthat rearranges the samples in a given batch. Adaptive Instance Normalization\n(AdaIN) is then applied between the activations of each (non-permuted) sample\n$i$ and the corresponding activations of the sample $\\pi(i)$, thus swapping\nstatistics between the samples of the batch. Since the global image statistics\nare distorted, this swapping procedure causes the network to rely on cues, such\nas shape or texture. By choosing the random permutation with probability $p$\nand the identity permutation otherwise, one can control the effect's strength.\n  With the correct choice of $p$, fixed apriori for all experiments and\nselected without considering test data, our method consistently outperforms\nbaselines in multiple settings. In image classification, our method improves on\nboth CIFAR100 and ImageNet using multiple architectures. In the setting of\nrobustness, our method improves on both ImageNet-C and Cifar-100-C for multiple\narchitectures. In the setting of domain adaptation and domain generalization,\nour method achieves state of the art results on the transfer learning task from\nGTAV to Cityscapes and on the PACS benchmark.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "I.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05785v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04658v2",
    "title": "Case Study: Deontological Ethics in NLP",
    "authors": [
      "Shrimai Prabhumoye",
      "Brendon Boldt",
      "Ruslan Salakhutdinov",
      "Alan W Black"
    ],
    "author_ids": [],
    "abstract": "Recent work in natural language processing (NLP) has focused on ethical\nchallenges such as understanding and mitigating bias in data and algorithms;\nidentifying objectionable content like hate speech, stereotypes and offensive\nlanguage; and building frameworks for better system design and data handling\npractices. However, there has been little discussion about the ethical\nfoundations that underlie these efforts. In this work, we study one ethical\ntheory, namely deontological ethics, from the perspective of NLP. In\nparticular, we focus on the generalization principle and the respect for\nautonomy through informed consent. We provide four case studies to demonstrate\nhow these principles can be used with NLP systems. We also recommend directions\nto avoid the ethical issues in these systems.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04658v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04396v5",
    "title": "Dropping Standardized Testing for Admissions Trades Off Information and Access",
    "authors": [
      "Nikhil Garg",
      "Hannah Li",
      "Faidra Monachou"
    ],
    "author_ids": [],
    "abstract": "We study the role of information and access in capacity-constrained selection\nproblems with fairness concerns. We develop a theoretical statistical\ndiscrimination framework, where each applicant has multiple features and is\npotentially strategic. The model formalizes the trade-off between the\n(potentially positive) informational role of a feature and its (negative)\nexclusionary nature when members of different social groups have unequal access\nto this feature.\n  Our framework finds a natural application to recent policy debates on\ndropping standardized testing in college admissions. Our primary takeaway is\nthat the decision to drop a feature (such as test scores) cannot be made\nwithout the joint context of the information provided by other features and how\nthe requirement affects the applicant pool composition. Dropping a feature may\nexacerbate disparities by decreasing the amount of information available for\neach applicant, especially those from non-traditional backgrounds. However, in\nthe presence of access barriers to a feature, the interaction between the\ninformational environment and the effect of access barriers on the applicant\npool size becomes highly complex. In this case, we provide a threshold\ncharacterization regarding when removing a feature improves both academic merit\nand diversity. Finally, using calibrated simulations in both the strategic and\nnon-strategic settings, we demonstrate the presence of practical instances\nwhere the decision to eliminate standardized testing improves or worsens all\nmetrics.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04396v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.04327v1",
    "title": "Bias and Variance of Post-processing in Differential Privacy",
    "authors": [
      "Keyu Zhu",
      "Pascal Van Hentenryck",
      "Ferdinando Fioretto"
    ],
    "author_ids": [],
    "abstract": "Post-processing immunity is a fundamental property of differential privacy:\nit enables the application of arbitrary data-independent transformations to the\nresults of differentially private outputs without affecting their privacy\nguarantees. When query outputs must satisfy domain constraints, post-processing\ncan be used to project the privacy-preserving outputs onto the feasible region.\nMoreover, when the feasible region is convex, a widely adopted class of\npost-processing steps is also guaranteed to improve accuracy. Post-processing\nhas been applied successfully in many applications including census\ndata-release, energy systems, and mobility. However, its effects on the noise\ndistribution is poorly understood: It is often argued that post-processing may\nintroduce bias and increase variance. This paper takes a first step towards\nunderstanding the properties of post-processing. It considers the release of\ncensus data and examines, both theoretically and empirically, the behavior of a\nwidely adopted class of post-processing functions.",
    "published_date": "2020-10-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04327v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04260v1",
    "title": "Fake Reviews Detection through Analysis of Linguistic Features",
    "authors": [
      "Faranak Abri",
      "Luis Felipe Gutierrez",
      "Akbar Siami Namin",
      "Keith S. Jones",
      "David R. W. Sears"
    ],
    "author_ids": [],
    "abstract": "Online reviews play an integral part for success or failure of businesses.\nPrior to purchasing services or goods, customers first review the online\ncomments submitted by previous customers. However, it is possible to\nsuperficially boost or hinder some businesses through posting counterfeit and\nfake reviews. This paper explores a natural language processing approach to\nidentify fake reviews. We present a detailed analysis of linguistic features\nfor distinguishing fake and trustworthy online reviews. We study 15 linguistic\nfeatures and measure their significance and importance towards the\nclassification schemes employed in this study. Our results indicate that fake\nreviews tend to include more redundant terms and pauses, and generally contain\nlonger sentences. The application of several machine learning classification\nalgorithms revealed that we were able to discriminate fake from real reviews\nwith high accuracy using these linguistic features.",
    "published_date": "2020-10-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04260v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04050v2",
    "title": "A survey of algorithmic recourse: definitions, formulations, solutions, and prospects",
    "authors": [
      "Amir-Hossein Karimi",
      "Gilles Barthe",
      "Bernhard Schölkopf",
      "Isabel Valera"
    ],
    "author_ids": [],
    "abstract": "Machine learning is increasingly used to inform decision-making in sensitive\nsituations where decisions have consequential effects on individuals' lives. In\nthese settings, in addition to requiring models to be accurate and robust,\nsocially relevant values such as fairness, privacy, accountability, and\nexplainability play an important role for the adoption and impact of said\ntechnologies. In this work, we focus on algorithmic recourse, which is\nconcerned with providing explanations and recommendations to individuals who\nare unfavourably treated by automated decision-making systems. We first perform\nan extensive literature review, and align the efforts of many authors by\npresenting unified definitions, formulations, and solutions to recourse. Then,\nwe provide an overview of the prospective research directions towards which the\ncommunity may engage, challenging existing assumptions and making explicit\nconnections to other ethical challenges such as security, privacy, and\nfairness.",
    "published_date": "2020-10-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04050v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03986v1",
    "title": "Metrics and methods for a systematic comparison of fairness-aware machine learning algorithms",
    "authors": [
      "Gareth P. Jones",
      "James M. Hickey",
      "Pietro G. Di Stefano",
      "Charanpal Dhanjal",
      "Laura C. Stoddart",
      "Vlasios Vasileiou"
    ],
    "author_ids": [],
    "abstract": "Understanding and removing bias from the decisions made by machine learning\nmodels is essential to avoid discrimination against unprivileged groups.\nDespite recent progress in algorithmic fairness, there is still no clear answer\nas to which bias-mitigation approaches are most effective. Evaluation\nstrategies are typically use-case specific, rely on data with unclear bias, and\nemploy a fixed policy to convert model outputs to decision outcomes. To address\nthese problems, we performed a systematic comparison of a number of popular\nfairness algorithms applicable to supervised classification. Our study is the\nmost comprehensive of its kind. It utilizes three real and four synthetic\ndatasets, and two different ways of converting model outputs to decisions. It\nconsiders fairness, predictive-performance, calibration quality, and speed of\n28 different modelling pipelines, corresponding to both fairness-unaware and\nfairness-aware algorithms. We found that fairness-unaware algorithms typically\nfail to produce adequately fair models and that the simplest algorithms are not\nnecessarily the fairest ones. We also found that fairness-aware algorithms can\ninduce fairness without material drops in predictive power. Finally, we found\nthat dataset idiosyncracies (e.g., degree of intrinsic unfairness, nature of\ncorrelations) do affect the performance of fairness-aware approaches. Our\nresults allow the practitioner to narrow down the approach(es) they would like\nto adopt without having to know in advance their fairness requirements.",
    "published_date": "2020-10-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03986v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03933v2",
    "title": "Assessing Classifier Fairness with Collider Bias",
    "authors": [
      "Zhenlong Xu",
      "Ziqi Xu",
      "Jixue Liu",
      "Debo Cheng",
      "Jiuyong Li",
      "Lin Liu",
      "Ke Wang",
      "Ziqi Xu",
      "Zhenlong Xu contributed equally to this paper"
    ],
    "author_ids": [],
    "abstract": "The increasing application of machine learning techniques in everyday\ndecision-making processes has brought concerns about the fairness of\nalgorithmic decision-making. This paper concerns the problem of collider bias\nwhich produces spurious associations in fairness assessment and develops\ntheorems to guide fairness assessment avoiding the collider bias. We consider a\nreal-world application of auditing a trained classifier by an audit agency. We\npropose an unbiased assessment algorithm by utilising the developed theorems to\nreduce collider biases in the assessment. Experiments and simulations show the\nproposed algorithm reduces collider biases significantly in the assessment and\nis promising in auditing trained classifiers.",
    "published_date": "2020-10-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03933v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03869v2",
    "title": "A Combinatorial Characterization of Self-Stabilizing Population Protocols",
    "authors": [
      "Shaan Mathur",
      "Rafail Ostrovsky"
    ],
    "author_ids": [],
    "abstract": "We fully characterize self-stabilizing functions in population protocols for\ncomplete interaction graphs. In particular, we investigate self-stabilization\nin systems of $n$ finite state agents in which a malicious scheduler selects an\narbitrary sequence of pairwise interactions under a global fairness condition.\nWe show a necessary and sufficient condition for self-stabilization.\nSpecifically we show that functions without certain set-theoretic conditions\nare impossible to compute in a self-stabilizing manner. Our main contribution\nis in the converse, where we construct a self-stabilizing protocol for all\nother functions that meet this characterization. Our positive construction uses\nDickson's Lemma to develop the notion of the root set, a concept that turns out\nto fundamentally characterize self-stabilization in this model. We believe it\nmay lend to characterizing self-stabilization in more general models as well.",
    "published_date": "2020-10-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03869v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.03856v6",
    "title": "Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift",
    "authors": [
      "Federico Barbero",
      "Feargus Pendlebury",
      "Fabio Pierazzi",
      "Lorenzo Cavallaro"
    ],
    "author_ids": [],
    "abstract": "Machine learning for malware classification shows encouraging results, but\nreal deployments suffer from performance degradation as malware authors adapt\ntheir techniques to evade detection. This phenomenon, known as concept drift,\noccurs as new malware examples evolve and become less and less like the\noriginal training examples. One promising method to cope with concept drift is\nclassification with rejection in which examples that are likely to be\nmisclassified are instead quarantined until they can be expertly analyzed.\n  We propose TRANSCENDENT, a rejection framework built on Transcend, a recently\nproposed strategy based on conformal prediction theory. In particular, we\nprovide a formal treatment of Transcend, enabling us to refine conformal\nevaluation theory -- its underlying statistical engine -- and gain a better\nunderstanding of the theoretical reasons for its effectiveness. In the process,\nwe develop two additional conformal evaluators that match or surpass the\nperformance of the original while significantly decreasing the computational\noverhead. We evaluate TRANSCENDENT on a malware dataset spanning 5 years that\nremoves sources of experimental bias present in the original evaluation.\nTRANSCENDENT outperforms state-of-the-art approaches while generalizing across\ndifferent malware domains and classifiers.\n  To further assist practitioners, we determine the optimal operational\nsettings for a TRANSCENDENT deployment and show how it can be applied to many\npopular learning algorithms. These insights support both old and new empirical\nfindings, making Transcend a sound and practical solution for the first time.\nTo this end, we release TRANSCENDENT as open source, to aid the adoption of\nrejection strategies by the security community.",
    "published_date": "2020-10-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03856v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03677v1",
    "title": "Agent Based Computational Model Aided Approach to Improvise the Inequality-Adjusted Human Development Index (IHDI) for Greater Parity in Real Scenario Assessments",
    "authors": [
      "Pradipta Banerjee",
      "Subhrabrata Choudhury"
    ],
    "author_ids": [],
    "abstract": "To design, evaluate and tune policies for all-inclusive human development,\nthe primary requisite is to assess the true state of affairs of the society.\nStatistical indices like GDP, Gini Coefficients have been developed to\naccomplish the evaluation of the socio-economic systems. They have remained\nprevalent in the conventional economic theories but little do they have in the\noffing regarding true well-being and development of humans. Human Development\nIndex (HDI) and thereafter Inequality-adjusted Human Development Index (IHDI)\nhas been the path changing composite-index having the focus on human\ndevelopment. However, even though its fundamental philosophy has an\nall-inclusive human development focus, the composite-indices appear to be\nunable to grasp the actual assessment in several scenarios. This happens due to\nthe dynamic non-linearity of social-systems where superposition principle\ncannot be applied between all of its inputs and outputs of the system as the\nsystem's own attributes get altered upon each input. We would discuss the\napparent shortcomings and probable refinement of the existing index using an\nagent based computational system model approach.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.SI",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03677v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.03665v2",
    "title": "A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization",
    "authors": [
      "André F. Cruz",
      "Pedro Saleiro",
      "Catarina Belém",
      "Carlos Soares",
      "Pedro Bizarro"
    ],
    "author_ids": [],
    "abstract": "Considerable research effort has been guided towards algorithmic fairness but\nthere is still no major breakthrough. In practice, an exhaustive search over\nall possible techniques and hyperparameters is needed to find optimal\nfairness-accuracy trade-offs. Hence, coupled with the lack of tools for ML\npractitioners, real-world adoption of bias reduction methods is still scarce.\nTo address this, we present Fairband, a bandit-based fairness-aware\nhyperparameter optimization (HO) algorithm. Fairband is conceptually simple,\nresource-efficient, easy to implement, and agnostic to both the objective\nmetrics, model types and the hyperparameter space being explored. Moreover, by\nintroducing fairness notions into HO, we enable seamless and efficient\nintegration of fairness objectives into real-world ML pipelines. We compare\nFairband with popular HO methods on four real-world decision-making datasets.\nWe show that Fairband can efficiently navigate the fairness-accuracy trade-off\nthrough hyperparameter optimization. Furthermore, without extra training cost,\nit consistently finds configurations attaining substantially improved fairness\nat a comparatively small decrease in predictive accuracy.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03665v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03561v1",
    "title": "Ensembling geophysical models with Bayesian Neural Networks",
    "authors": [
      "Ushnish Sengupta",
      "Matt Amos",
      "J. Scott Hosking",
      "Carl Edward Rasmussen",
      "Matthew Juniper",
      "Paul J. Young"
    ],
    "author_ids": [],
    "abstract": "Ensembles of geophysical models improve projection accuracy and express\nuncertainties. We develop a novel data-driven ensembling strategy for combining\ngeophysical models using Bayesian Neural Networks, which infers\nspatiotemporally varying model weights and bias while accounting for\nheteroscedastic uncertainties in the observations. This produces more accurate\nand uncertainty-aware projections without sacrificing interpretability. Applied\nto the prediction of total column ozone from an ensemble of 15\nchemistry-climate models, we find that the Bayesian neural network ensemble\n(BayNNE) outperforms existing ensembling methods, achieving a 49.4% reduction\nin RMSE for temporal extrapolation, and a 67.4% reduction in RMSE for polar\ndata voids, compared to a weighted mean. Uncertainty is also\nwell-characterized, with 90.6% of the data points in our extrapolation\nvalidation dataset lying within 2 standard deviations and 98.5% within 3\nstandard deviations.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "physics.geo-ph",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03561v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03494v2",
    "title": "TeaForN: Teacher-Forcing with N-grams",
    "authors": [
      "Sebastian Goodman",
      "Nan Ding",
      "Radu Soricut"
    ],
    "author_ids": [],
    "abstract": "Sequence generation models trained with teacher-forcing suffer from issues\nrelated to exposure bias and lack of differentiability across timesteps. Our\nproposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these\nproblems directly, through the use of a stack of N decoders trained to decode\nalong a secondary time axis that allows model parameter updates based on N\nprediction steps. TeaForN can be used with a wide class of decoder\narchitectures and requires minimal modifications from a standard\nteacher-forcing setup. Empirically, we show that TeaForN boosts generation\nquality on one Machine Translation benchmark, WMT 2014 English-French, and two\nNews Summarization benchmarks, CNN/Dailymail and Gigaword.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03494v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03438v4",
    "title": "Fairness in Influence Maximization through Randomization",
    "authors": [
      "Ruben Becker",
      "Gianlorenzo D'Angelo",
      "Sajjad Ghobadi",
      "Hugo Gilbert"
    ],
    "author_ids": [],
    "abstract": "The influence maximization paradigm has been used by researchers in various\nfields in order to study how information spreads in social networks. While\npreviously the attention was mostly on efficiency, more recently fairness\nissues have been taken into account in this scope. In this paper, we propose to\nuse randomization as a mean for achieving fairness. Similar to previous works\nlike Fish et al. (WWW '19) and Tsang et al. (IJCAI '19), we study the maximin\ncriterion for (group) fairness. In contrast to their work however, we model the\nproblem in such a way that, when choosing the seed sets, probabilistic\nstrategies are possible rather than only deterministic ones. We introduce two\ndifferent variants of this probabilistic problem, one that entails\nprobabilistic strategies over nodes (node-based problem) and a second one that\nentails probabilistic strategies over sets of nodes (set-based problem). While\nthe original deterministic problem involving the maximin criterion has been\nshown to be inapproximable, interestingly, we show that both probabilistic\nvariants permit approximation algorithms that achieve a constant multiplicative\nfactor of 1-1/e plus an additive arbitrarily small error that is due to the\nsimulation of the information spread. For an experimental study, we provide\nimplementations of multiplicative-weight routines for both problems and compare\nthe achieved fairness values to existing methods. Maybe non-surprisingly, we\nshow that the ex-ante values of the computed probabilistic strategies are\nsignificantly larger than the (ex-post) fairness values of previous methods.\nThis indicates that studying fairness via randomization is a worthwhile path to\nfollow. Interestingly and maybe more surprisingly, we observe that even the\nex-post fairness values computed by our routines, dominate over the fairness\nachieved by previous methods on most of the instances tested.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03438v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.03362v1",
    "title": "The Short Anthropological Guide to the Study of Ethical AI",
    "authors": [
      "Alexandrine Royer"
    ],
    "author_ids": [],
    "abstract": "Over the next few years, society as a whole will need to address what core\nvalues it wishes to protect when dealing with technology. Anthropology, a field\ndedicated to the very notion of what it means to be human, can provide some\ninteresting insights into how to cope and tackle these changes in our Western\nsociety and other areas of the world. It can be challenging for social science\npractitioners to grasp and keep up with the pace of technological innovation,\nwith many being unfamiliar with the jargon of AI. This short guide serves as\nboth an introduction to AI ethics and social science and anthropological\nperspectives on the development of AI. It intends to provide those unfamiliar\nwith the field with an insight into the societal impact of AI systems and how,\nin turn, these systems can lead us to rethink how our world operates.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03362v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03975v2",
    "title": "Evaluating the Clinical Realism of Synthetic Chest X-Rays Generated Using Progressively Growing GANs",
    "authors": [
      "Bradley Segal",
      "David M. Rubin",
      "Grace Rubin",
      "Adam Pantanowitz"
    ],
    "author_ids": [],
    "abstract": "Chest x-rays are a vital tool in the workup of many patients. Similar to most\nmedical imaging modalities, they are profoundly multi-modal and are capable of\nvisualising a variety of combinations of conditions. There is an ever pressing\nneed for greater quantities of labelled data to develop new diagnostic tools,\nhowever this is in direct opposition to concerns regarding patient\nconfidentiality which constrains access through permission requests and ethics\napprovals. Previous work has sought to address these concerns by creating\nclass-specific GANs that synthesise images to augment training data. These\napproaches cannot be scaled as they introduce computational trade offs between\nmodel size and class number which places fixed limits on the quality that such\ngenerates can achieve. We address this concern by introducing latent class\noptimisation which enables efficient, multi-modal sampling from a GAN and with\nwhich we synthesise a large archive of labelled generates. We apply a PGGAN to\nthe task of unsupervised x-ray synthesis and have radiologists evaluate the\nclinical realism of the resultant samples. We provide an in depth review of the\nproperties of varying pathologies seen on generates as well as an overview of\nthe extent of disease diversity captured by the model. We validate the\napplication of the Fr\\'echet Inception Distance (FID) to measure the quality of\nx-ray generates and find that they are similar to other high resolution tasks.\nWe quantify x-ray clinical realism by asking radiologists to distinguish\nbetween real and fake scans and find that generates are more likely to be\nclassed as real than by chance, but there is still progress required to achieve\ntrue realism. We confirm these findings by evaluating synthetic classification\nmodel performance on real scans. We conclude by discussing the limitations of\nPGGAN generates and how to achieve controllable, realistic generates.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03975v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03338v1",
    "title": "Improving QA Generalization by Concurrent Modeling of Multiple Biases",
    "authors": [
      "Mingzhu Wu",
      "Nafise Sadat Moosavi",
      "Andreas Rücklé",
      "Iryna Gurevych"
    ],
    "author_ids": [],
    "abstract": "Existing NLP datasets contain various biases that models can easily exploit\nto achieve high performances on the corresponding evaluation sets. However,\nfocusing on dataset-specific biases limits their ability to learn more\ngeneralizable knowledge about the task from more general data patterns. In this\npaper, we investigate the impact of debiasing methods for improving\ngeneralization and propose a general framework for improving the performance on\nboth in-domain and out-of-domain datasets by concurrent modeling of multiple\nbiases in the training data. Our framework weights each example based on the\nbiases it contains and the strength of those biases in the training data. It\nthen uses these weights in the training objective so that the model relies less\non examples with high bias weights. We extensively evaluate our framework on\nextractive question answering with training data from various domains with\nmultiple biases of different strengths. We perform the evaluations in two\ndifferent settings, in which the model is trained on a single domain or\nmultiple domains simultaneously, and show its effectiveness in both settings\ncompared to state-of-the-art debiasing methods.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03300v1",
    "title": "CD-UAP: Class Discriminative Universal Adversarial Perturbation",
    "authors": [
      "Chaoning Zhang",
      "Philipp Benz",
      "Tooba Imtiaz",
      "In So Kweon"
    ],
    "author_ids": [],
    "abstract": "A single universal adversarial perturbation (UAP) can be added to all natural\nimages to change most of their predicted class labels. It is of high practical\nrelevance for an attacker to have flexible control over the targeted classes to\nbe attacked, however, the existing UAP method attacks samples from all classes.\nIn this work, we propose a new universal attack method to generate a single\nperturbation that fools a target network to misclassify only a chosen group of\nclasses, while having limited influence on the remaining classes. Since the\nproposed attack generates a universal adversarial perturbation that is\ndiscriminative to targeted and non-targeted classes, we term it class\ndiscriminative universal adversarial perturbation (CD-UAP). We propose one\nsimple yet effective algorithm framework, under which we design and compare\nvarious loss function configurations tailored for the class discriminative\nuniversal attack. The proposed approach has been evaluated with extensive\nexperiments on various benchmark datasets. Additionally, our proposed approach\nachieves state-of-the-art performance for the original task of UAP attacking\nall classes, which demonstrates the effectiveness of our approach.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03300v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03288v1",
    "title": "Double Targeted Universal Adversarial Perturbations",
    "authors": [
      "Philipp Benz",
      "Chaoning Zhang",
      "Tooba Imtiaz",
      "In So Kweon"
    ],
    "author_ids": [],
    "abstract": "Despite their impressive performance, deep neural networks (DNNs) are widely\nknown to be vulnerable to adversarial attacks, which makes it challenging for\nthem to be deployed in security-sensitive applications, such as autonomous\ndriving. Image-dependent perturbations can fool a network for one specific\nimage, while universal adversarial perturbations are capable of fooling a\nnetwork for samples from all classes without selection. We introduce a double\ntargeted universal adversarial perturbations (DT-UAPs) to bridge the gap\nbetween the instance-discriminative image-dependent perturbations and the\ngeneric universal perturbations. This universal perturbation attacks one\ntargeted source class to sink class, while having a limited adversarial effect\non other non-targeted source classes, for avoiding raising suspicions.\nTargeting the source and sink class simultaneously, we term it double targeted\nattack (DTA). This provides an attacker with the freedom to perform precise\nattacks on a DNN model while raising little suspicion. We show the\neffectiveness of the proposed DTA algorithm on a wide range of datasets and\nalso demonstrate its potential as a physical attack.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03288v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03240v2",
    "title": "Bias and Debias in Recommender System: A Survey and Future Directions",
    "authors": [
      "Jiawei Chen",
      "Hande Dong",
      "Xiang Wang",
      "Fuli Feng",
      "Meng Wang",
      "Xiangnan He"
    ],
    "author_ids": [],
    "abstract": "While recent years have witnessed a rapid growth of research papers on\nrecommender system (RS), most of the papers focus on inventing machine learning\nmodels to better fit user behavior data. However, user behavior data is\nobservational rather than experimental. This makes various biases widely exist\nin the data, including but not limited to selection bias, position bias,\nexposure bias, and popularity bias. Blindly fitting the data without\nconsidering the inherent biases will result in many serious issues, e.g., the\ndiscrepancy between offline evaluation and online metrics, hurting user\nsatisfaction and trust on the recommendation service, etc. To transform the\nlarge volume of research models into practical improvements, it is highly\nurgent to explore the impacts of the biases and perform debiasing when\nnecessary. When reviewing the papers that consider biases in RS, we find that,\nto our surprise, the studies are rather fragmented and lack a systematic\norganization. The terminology ``bias'' is widely used in the literature, but\nits definition is usually vague and even inconsistent across papers. This\nmotivates us to provide a systematic survey of existing work on RS biases. In\nthis paper, we first summarize seven types of biases in recommendation, along\nwith their definitions and characteristics. We then provide a taxonomy to\nposition and organize the existing work on recommendation debiasing. Finally,\nwe identify some open challenges and envision some future directions, with the\nhope of inspiring more research work on this important yet less investigated\ntopic. The summary of debiasing methods reviewed in this survey can be found at\n\\url{https://github.com/jiawei-chen/RecDebiasing}.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03240v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03228v2",
    "title": "FairMixRep : Self-supervised Robust Representation Learning for Heterogeneous Data with Fairness constraints",
    "authors": [
      "Souradip Chakraborty",
      "Ekansh Verma",
      "Saswata Sahoo",
      "Jyotishka Datta"
    ],
    "author_ids": [],
    "abstract": "Representation Learning in a heterogeneous space with mixed variables of\nnumerical and categorical types has interesting challenges due to its complex\nfeature manifold. Moreover, feature learning in an unsupervised setup, without\nclass labels and a suitable learning loss function, adds to the problem\ncomplexity. Further, the learned representation and subsequent predictions\nshould not reflect discriminatory behavior towards certain sensitive groups or\nattributes. The proposed feature map should preserve maximum variations present\nin the data and needs to be fair with respect to the sensitive variables. We\npropose, in the first phase of our work, an efficient encoder-decoder framework\nto capture the mixed-domain information. The second phase of our work focuses\non de-biasing the mixed space representations by adding relevant fairness\nconstraints. This ensures minimal information loss between the representations\nbefore and after the fairness-preserving projections. Both the information\ncontent and the fairness aspect of the final representation learned has been\nvalidated through several metrics where it shows excellent performance. Our\nwork (FairMixRep) addresses the problem of Mixed Space Fair Representation\nlearning from an unsupervised perspective and learns a Universal representation\nthat is timely, unique, and a novel research contribution.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03228v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.05887v1",
    "title": "Fairness Perception from a Network-Centric Perspective",
    "authors": [
      "Farzan Masrour",
      "Pang-Ning Tan",
      "Abdol-Hossein Esfahanian"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness is a major concern in recent years as the influence of\nmachine learning algorithms becomes more widespread. In this paper, we\ninvestigate the issue of algorithmic fairness from a network-centric\nperspective. Specifically, we introduce a novel yet intuitive function known as\nnetwork-centric fairness perception and provide an axiomatic approach to\nanalyze its properties. Using a peer-review network as case study, we also\nexamine its utility in terms of assessing the perception of fairness in paper\nacceptance decisions. We show how the function can be extended to a group\nfairness metric known as fairness visibility and demonstrate its relationship\nto demographic parity. We also illustrate a potential pitfall of the fairness\nvisibility measure that can be exploited to mislead individuals into perceiving\nthat the algorithmic decisions are fair. We demonstrate how the problem can be\nalleviated by increasing the local neighborhood size of the fairness perception\nfunction.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05887v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03127v1",
    "title": "A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions",
    "authors": [
      "Takuma Udagawa",
      "Takato Yamazaki",
      "Akiko Aizawa"
    ],
    "author_ids": [],
    "abstract": "Recent models achieve promising results in visually grounded dialogues.\nHowever, existing datasets often contain undesirable biases and lack\nsophisticated linguistic analyses, which make it difficult to understand how\nwell current models recognize their precise linguistic structures. To address\nthis problem, we make two design choices: first, we focus on OneCommon Corpus\n\\citep{udagawa2019natural,udagawa2020annotated}, a simple yet challenging\ncommon grounding dataset which contains minimal bias by design. Second, we\nanalyze their linguistic structures based on \\textit{spatial expressions} and\nprovide comprehensive and reliable annotation for 600 dialogues. We show that\nour annotation captures important linguistic structures including\npredicate-argument structure, modification and ellipsis. In our experiments, we\nassess the model's understanding of these structures through reference\nresolution. We demonstrate that our annotation can reveal both the strengths\nand weaknesses of baseline models in essential levels of detail. Overall, we\npropose a novel framework and resource for investigating fine-grained language\nunderstanding in visually grounded dialogues.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03127v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03111v1",
    "title": "Bayesian Distance Weighted Discrimination",
    "authors": [
      "Eric F. Lock"
    ],
    "author_ids": [],
    "abstract": "Distance weighted discrimination (DWD) is a linear discrimination method that\nis particularly well-suited for classification tasks with high-dimensional\ndata. The DWD coefficients minimize an intuitive objective function, which can\nsolved very efficiently using state-of-the-art optimization techniques.\nHowever, DWD has not yet been cast into a model-based framework for statistical\ninference. In this article we show that DWD identifies the mode of a proper\nBayesian posterior distribution, that results from a particular link function\nfor the class probabilities and a shrinkage-inducing proper prior distribution\non the coefficients. We describe a relatively efficient Markov chain Monte\nCarlo (MCMC) algorithm to simulate from the true posterior under this Bayesian\nframework. We show that the posterior is asymptotically normal and derive the\nmean and covariance matrix of its limiting distribution. Through several\nsimulation studies and an application to breast cancer genomics we demonstrate\nhow the Bayesian approach to DWD can be used to (1) compute well-calibrated\nposterior class probabilities, (2) assess uncertainty in the DWD coefficients\nand resulting sample scores, (3) improve power via semi-supervised analysis\nwhen not all class labels are available, and (4) automatically determine a\npenalty tuning parameter within the model-based framework. R code to perform\nBayesian DWD is available at https://github.com/lockEF/BayesianDWD .",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03111v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.03103v1",
    "title": "On optimal recovery in $L_2$",
    "authors": [
      "V. Temlyakov"
    ],
    "author_ids": [],
    "abstract": "We prove that the optimal error of recovery in the $L_2$ norm of functions\nfrom a class $\\bF$ can be bounded above by the value of the Kolmogorov width of\n$\\bF$ in the uniform norm. We demonstrate on a number of examples of $\\bF$ from\nclasses of functions with mixed smoothness that the obtained inequality\nprovides a powerful tool for estimating errors of optimal recovery.",
    "published_date": "2020-10-07T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.FA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03103v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.03058v2",
    "title": "Characterising Bias in Compressed Models",
    "authors": [
      "Sara Hooker",
      "Nyalleng Moorosi",
      "Gregory Clark",
      "Samy Bengio",
      "Emily Denton"
    ],
    "author_ids": [],
    "abstract": "The popularity and widespread use of pruning and quantization is driven by\nthe severe resource constraints of deploying deep neural networks to\nenvironments with strict latency, memory and energy requirements. These\ntechniques achieve high levels of compression with negligible impact on\ntop-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides\ndisproportionately high errors on a small subset of examples; we call this\nsubset Compression Identified Exemplars (CIE). We further establish that for\nCIE examples, compression amplifies existing algorithmic bias. Pruning\ndisproportionately impacts performance on underrepresented features, which\noften coincides with considerations of fairness. Given that CIE is a relatively\nsmall subset but a great contributor of error in the model, we propose its use\nas a human-in-the-loop auditing tool to surface a tractable subset of the\ndataset for further inspection or annotation by a domain expert. We provide\nqualitative and quantitative support that CIE surfaces the most challenging\nexamples in the data distribution for human-in-the-loop auditing.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.03058v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02986v2",
    "title": "Compositional Demographic Word Embeddings",
    "authors": [
      "Charles Welch",
      "Jonathan K. Kummerfeld",
      "Verónica Pérez-Rosas",
      "Rada Mihalcea"
    ],
    "author_ids": [],
    "abstract": "Word embeddings are usually derived from corpora containing text from many\nindividuals, thus leading to general purpose representations rather than\nindividually personalized representations. While personalized embeddings can be\nuseful to improve language model performance and other language processing\ntasks, they can only be computed for people with a large amount of longitudinal\ndata, which is not the case for new users. We propose a new form of\npersonalized word embeddings that use demographic-specific word representations\nderived compositionally from full or partial demographic information for a user\n(i.e., gender, age, location, religion). We show that the resulting\ndemographic-aware word representations outperform generic word representations\non two tasks for English: language modeling and word associations. We further\nexplore the trade-off between the number of available attributes and their\nrelative effectiveness and discuss the ethical implications of using them.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02986v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07022v1",
    "title": "Towards a Policy-as-a-Service Framework to Enable Compliant, Trustworthy AI and HRI Systems in the Wild",
    "authors": [
      "Alexis Morris",
      "Hallie Siegel",
      "Jonathan Kelly"
    ],
    "author_ids": [],
    "abstract": "Building trustworthy autonomous systems is challenging for many reasons\nbeyond simply trying to engineer agents that 'always do the right thing.' There\nis a broader context that is often not considered within AI and HRI: that the\nproblem of trustworthiness is inherently socio-technical and ultimately\ninvolves a broad set of complex human factors and multidimensional\nrelationships that can arise between agents, humans, organizations, and even\ngovernments and legal institutions, each with their own understanding and\ndefinitions of trust. This complexity presents a significant barrier to the\ndevelopment of trustworthy AI and HRI systems---while systems developers may\ndesire to have their systems 'always do the right thing,' they generally lack\nthe practical tools and expertise in law, regulation, policy and ethics to\nensure this outcome. In this paper, we emphasize the \"fuzzy\" socio-technical\naspects of trustworthiness and the need for their careful consideration during\nboth design and deployment. We hope to contribute to the discussion of\ntrustworthy engineering in AI and HRI by i) describing the policy landscape\nthat must be considered when addressing trustworthy computing and the need for\nusable trust models, ii) highlighting an opportunity for trustworthy-by-design\nintervention within the systems engineering process, and iii) introducing the\nconcept of a \"policy-as-a-service\" (PaaS) framework that can be readily applied\nby AI systems engineers to address the fuzzy problem of trust during the\ndevelopment and (eventually) runtime process. We envision that the PaaS\napproach, which offloads the development of policy design parameters and\nmaintenance of policy standards to policy experts, will enable runtime trust\ncapabilities intelligent systems in the wild.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.RO",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07022v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02867v1",
    "title": "LOGAN: Local Group Bias Detection by Clustering",
    "authors": [
      "Jieyu Zhao",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Machine learning techniques have been widely used in natural language\nprocessing (NLP). However, as revealed by many recent studies, machine learning\nmodels often inherit and amplify the societal biases in data. Various metrics\nhave been proposed to quantify biases in model predictions. In particular,\nseveral of them evaluate disparity in model performance between protected\ngroups and advantaged groups in the test corpus. However, we argue that\nevaluating bias at the corpus level is not enough for understanding how biases\nare embedded in a model. In fact, a model with similar aggregated performance\nbetween different groups on the entire data may behave differently on instances\nin a local region. To analyze and detect such local bias, we propose LOGAN, a\nnew bias detection technique based on clustering. Experiments on toxicity\nclassification and object classification tasks show that LOGAN identifies bias\nin a local region and allows us to better analyze the biases in model\npredictions.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02867v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02847v2",
    "title": "Robustness and Reliability of Gender Bias Assessment in Word Embeddings: The Role of Base Pairs",
    "authors": [
      "Haiyang Zhang",
      "Alison Sneyd",
      "Mark Stevenson"
    ],
    "author_ids": [],
    "abstract": "It has been shown that word embeddings can exhibit gender bias, and various\nmethods have been proposed to quantify this. However, the extent to which the\nmethods are capturing social stereotypes inherited from the data has been\ndebated. Bias is a complex concept and there exist multiple ways to define it.\nPrevious work has leveraged gender word pairs to measure bias and extract\nbiased analogies. We show that the reliance on these gendered pairs has strong\nlimitations: bias measures based off of them are not robust and cannot identify\ncommon types of real-world bias, whilst analogies utilising them are unsuitable\nindicators of bias. In particular, the well-known analogy \"man is to\ncomputer-programmer as woman is to homemaker\" is due to word similarity rather\nthan societal bias. This has important implications for work on measuring bias\nin embeddings and related work debiasing embeddings.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02847v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02542v5",
    "title": "Astraea: Grammar-based Fairness Testing",
    "authors": [
      "Ezekiel Soremekun",
      "Sakshi Udeshi",
      "Sudipta Chattopadhyay"
    ],
    "author_ids": [],
    "abstract": "Software often produces biased outputs. In particular, machine learning (ML)\nbased software are known to produce erroneous predictions when processing\ndiscriminatory inputs. Such unfair program behavior can be caused by societal\nbias. In the last few years, Amazon, Microsoft and Google have provided\nsoftware services that produce unfair outputs, mostly due to societal bias\n(e.g. gender or race). In such events, developers are saddled with the task of\nconducting fairness testing. Fairness testing is challenging; developers are\ntasked with generating discriminatory inputs that reveal and explain biases.\n  We propose a grammar-based fairness testing approach (called ASTRAEA) which\nleverages context-free grammars to generate discriminatory inputs that reveal\nfairness violations in software systems. Using probabilistic grammars, ASTRAEA\nalso provides fault diagnosis by isolating the cause of observed software bias.\nASTRAEA's diagnoses facilitate the improvement of ML fairness.\n  ASTRAEA was evaluated on 18 software systems that provide three major natural\nlanguage processing (NLP) services. In our evaluation, ASTRAEA generated\nfairness violations with a rate of ~18%. ASTRAEA generated over 573K\ndiscriminatory test cases and found over 102K fairness violations. Furthermore,\nASTRAEA improves software fairness by ~76%, via model-retraining.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02542v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02448v1",
    "title": "On the Branching Bias of Syntax Extracted from Pre-trained Language Models",
    "authors": [
      "Huayang Li",
      "Lemao Liu",
      "Guoping Huang",
      "Shuming Shi"
    ],
    "author_ids": [],
    "abstract": "Many efforts have been devoted to extracting constituency trees from\npre-trained language models, often proceeding in two stages: feature definition\nand parsing. However, this kind of methods may suffer from the branching bias\nissue, which will inflate the performances on languages with the same branch it\nbiases to. In this work, we propose quantitatively measuring the branching bias\nby comparing the performance gap on a language and its reversed language, which\nis agnostic to both language models and extracting methods. Furthermore, we\nanalyze the impacts of three factors on the branching bias, namely parsing\nalgorithms, feature definitions, and language models. Experiments show that\nseveral existing works exhibit branching biases, and some implementations of\nthese three factors can introduce the branching bias.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02448v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02430v2",
    "title": "Shot in the Dark: Few-Shot Learning with No Base-Class Labels",
    "authors": [
      "Zitian Chen",
      "Subhransu Maji",
      "Erik Learned-Miller"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning aims to build classifiers for new classes from a small\nnumber of labeled examples and is commonly facilitated by access to examples\nfrom a distinct set of 'base classes'. The difference in data distribution\nbetween the test set (novel classes) and the base classes used to learn an\ninductive bias often results in poor generalization on the novel classes. To\nalleviate problems caused by the distribution shift, previous research has\nexplored the use of unlabeled examples from the novel classes, in addition to\nlabeled examples of the base classes, which is known as the transductive\nsetting. In this work, we show that, surprisingly, off-the-shelf\nself-supervised learning outperforms transductive few-shot methods by 3.9% for\n5-shot accuracy on miniImageNet without using any base class labels. This\nmotivates us to examine more carefully the role of features learned through\nself-supervision in few-shot learning. Comprehensive experiments are conducted\nto compare the transferability, robustness, efficiency, and the complementarity\nof supervised and self-supervised features.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02430v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02428v3",
    "title": "UnQovering Stereotyping Biases via Underspecified Questions",
    "authors": [
      "Tao Li",
      "Tushar Khot",
      "Daniel Khashabi",
      "Ashish Sabharwal",
      "Vivek Srikumar"
    ],
    "author_ids": [],
    "abstract": "While language embeddings have been shown to have stereotyping biases, how\nthese biases affect downstream question answering (QA) models remains\nunexplored. We present UNQOVER, a general framework to probe and quantify\nbiases through underspecified questions. We show that a naive use of model\nscores can lead to incorrect bias estimates due to two forms of reasoning\nerrors: positional dependence and question independence. We design a formalism\nthat isolates the aforementioned errors. As case studies, we use this metric to\nanalyze four important classes of stereotypes: gender, nationality, ethnicity,\nand religion. We probe five transformer-based QA models trained on two QA\ndatasets, along with their underlying language models. Our broad study reveals\nthat (1) all these models, with and without fine-tuning, have notable\nstereotyping biases in these classes; (2) larger models often have higher bias;\nand (3) the effect of fine-tuning on bias varies strongly with the dataset and\nthe model size.",
    "published_date": "2020-10-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02428v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02387v2",
    "title": "Metadata-Based Detection of Child Sexual Abuse Material",
    "authors": [
      "Mayana Pereira",
      "Rahul Dodhia",
      "Hyrum Anderson",
      "Richard Brown"
    ],
    "author_ids": [],
    "abstract": "Child Sexual Abuse Media (CSAM) is any visual record of a sexually-explicit\nactivity involving minors. CSAM impacts victims differently from the actual\nabuse because the distribution never ends, and images are permanent. Machine\nlearning-based solutions can help law enforcement quickly identify CSAM and\nblock digital distribution. However, collecting CSAM imagery to train machine\nlearning models has many ethical and legal constraints, creating a barrier to\nresearch development. With such restrictions in place, the development of CSAM\nmachine learning detection systems based on file metadata uncovers several\nopportunities. Metadata is not a record of a crime, and it does not have legal\nrestrictions. Therefore, investing in detection systems based on metadata can\nincrease the rate of discovery of CSAM and help thousands of victims. We\npropose a framework for training and evaluating deployment-ready machine\nlearning models for CSAM identification. Our framework provides guidelines to\nevaluate CSAM detection models against intelligent adversaries and models'\nperformance with open data. We apply the proposed framework to the problem of\nCSAM detection based on file paths. In our experiments, the best-performing\nmodel is based on convolutional neural networks and achieves an accuracy of\n0.97. Our evaluation shows that the CNN model is robust against offenders\nactively trying to evade detection by evaluating the model against\nadversarially modified data. Experiments with open datasets confirm that the\nmodel generalizes well and is deployment-ready.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02387v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02375v2",
    "title": "Investigating representations of verb bias in neural language models",
    "authors": [
      "Robert D. Hawkins",
      "Takateru Yamakoshi",
      "Thomas L. Griffiths",
      "Adele E. Goldberg"
    ],
    "author_ids": [],
    "abstract": "Languages typically provide more than one grammatical construction to express\ncertain types of messages. A speaker's choice of construction is known to\ndepend on multiple factors, including the choice of main verb -- a phenomenon\nknown as \\emph{verb bias}. Here we introduce DAIS, a large benchmark dataset\ncontaining 50K human judgments for 5K distinct sentence pairs in the English\ndative alternation. This dataset includes 200 unique verbs and systematically\nvaries the definiteness and length of arguments. We use this dataset, as well\nas an existing corpus of naturally occurring data, to evaluate how well recent\nneural language models capture human preferences. Results show that larger\nmodels perform better than smaller models, and transformer architectures (e.g.\nGPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under\ncomparable parameter and training settings. Additional analyses of internal\nfeature representations suggest that transformers may better integrate specific\nlexical information with grammatical constructions.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02375v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07023v1",
    "title": "Understanding bias in facial recognition technologies",
    "authors": [
      "David Leslie"
    ],
    "author_ids": [],
    "abstract": "Over the past couple of years, the growing debate around automated facial\nrecognition has reached a boiling point. As developers have continued to\nswiftly expand the scope of these kinds of technologies into an almost\nunbounded range of applications, an increasingly strident chorus of critical\nvoices has sounded concerns about the injurious effects of the proliferation of\nsuch systems. Opponents argue that the irresponsible design and use of facial\ndetection and recognition technologies (FDRTs) threatens to violate civil\nliberties, infringe on basic human rights and further entrench structural\nracism and systemic marginalisation. They also caution that the gradual creep\nof face surveillance infrastructures into every domain of lived experience may\neventually eradicate the modern democratic forms of life that have long\nprovided cherished means to individual flourishing, social solidarity and human\nself-creation. Defenders, by contrast, emphasise the gains in public safety,\nsecurity and efficiency that digitally streamlined capacities for facial\nidentification, identity verification and trait characterisation may bring. In\nthis explainer, I focus on one central aspect of this debate: the role that\ndynamics of bias and discrimination play in the development and deployment of\nFDRTs. I examine how historical patterns of discrimination have made inroads\ninto the design and implementation of FDRTs from their very earliest moments.\nAnd, I explain the ways in which the use of biased FDRTs can lead\ndistributional and recognitional injustices. The explainer concludes with an\nexploration of broader ethical questions around the potential proliferation of\npervasive face-based surveillance infrastructures and makes some\nrecommendations for cultivating more responsible approaches to the development\nand governance of these technologies.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07023v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02217v1",
    "title": "CO2: Consistent Contrast for Unsupervised Visual Representation Learning",
    "authors": [
      "Chen Wei",
      "Huiyu Wang",
      "Wei Shen",
      "Alan Yuille"
    ],
    "author_ids": [],
    "abstract": "Contrastive learning has been adopted as a core method for unsupervised\nvisual representation learning. Without human annotation, the common practice\nis to perform an instance discrimination task: Given a query image crop, this\ntask labels crops from the same image as positives, and crops from other\nrandomly sampled images as negatives. An important limitation of this label\nassignment strategy is that it can not reflect the heterogeneous similarity\nbetween the query crop and each crop from other images, taking them as equally\nnegative, while some of them may even belong to the same semantic class as the\nquery. To address this issue, inspired by consistency regularization in\nsemi-supervised learning on unlabeled data, we propose Consistent Contrast\n(CO2), which introduces a consistency regularization term into the current\ncontrastive learning framework. Regarding the similarity of the query crop to\neach crop from other images as \"unlabeled\", the consistency term takes the\ncorresponding similarity of a positive crop as a pseudo label, and encourages\nconsistency between these two similarities. Empirically, CO2 improves Momentum\nContrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and\n1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also\ntransfers to image classification, object detection, and semantic segmentation\non PASCAL VOC. This shows that CO2 learns better visual representations for\nthese downstream tasks.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02217v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.02117v1",
    "title": "Statistical Reliability of 10 Years of Cyber Security User Studies (Extended Version)",
    "authors": [
      "Thomas Groß"
    ],
    "author_ids": [],
    "abstract": "Background. In recent years, cyber security security user studies have been\nappraised in meta-research, mostly focusing on the completeness of their\nstatistical inferences and the fidelity of their statistical reporting.\nHowever, estimates of the field's distribution of statistical power and its\npublication bias have not received much attention. Aim. In this study, we aim\nto estimate the effect sizes and their standard errors present as well as the\nimplications on statistical power and publication bias. Method. We built upon a\npublished systematic literature review of $146$ user studies in cyber security\n(2006--2016). We took into account $431$ statistical inferences including $t$-,\n$\\chi^2$-, $r$-, one-way $F$-tests, and $Z$-tests. In addition, we coded the\ncorresponding total sample sizes, group sizes and test families. Given these\ndata, we established the observed effect sizes and evaluated the overall\npublication bias. We further computed the statistical power vis-{\\`a}-vis of\nparametrized population thresholds to gain unbiased estimates of the power\ndistribution. Results. We obtained a distribution of effect sizes and their\nconversion into comparable log odds ratios together with their standard errors.\nWe, further, gained funnel-plot estimates of the publication bias present in\nthe sample as well as insights into the power distribution and its\nconsequences. Conclusions. Through the lenses of power and publication bias, we\nshed light on the statistical reliability of the studies in the field. The\nupshot of this introspection is practical recommendations on conducting and\nevaluating studies to advance the field.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02117v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.01809v4",
    "title": "Long-tailed Recognition by Routing Diverse Distribution-Aware Experts",
    "authors": [
      "Xudong Wang",
      "Long Lian",
      "Zhongqi Miao",
      "Ziwei Liu",
      "Stella X. Yu"
    ],
    "author_ids": [],
    "abstract": "Natural data are often long-tail distributed over semantic classes. Existing\nrecognition methods tackle this imbalanced classification by placing more\nemphasis on the tail data, through class re-balancing/re-weighting or\nensembling over different data groups, resulting in increased tail accuracies\nbut reduced head accuracies.\n  We take a dynamic view of the training data and provide a principled model\nbias and variance analysis as the training data fluctuates: Existing long-tail\nclassifiers invariably increase the model variance and the head-tail model bias\ngap remains large, due to more and larger confusion with hard negatives for the\ntail.\n  We propose a new long-tailed classifier called RoutIng Diverse Experts\n(RIDE). It reduces the model variance with multiple experts, reduces the model\nbias with a distribution-aware diversity loss, reduces the computational cost\nwith a dynamic expert routing module. RIDE outperforms the state-of-the-art by\n5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is\nalso a universal framework that is applicable to various backbone networks,\nlong-tailed algorithms, and training mechanisms for consistent performance\ngains. Our code is available at:\nhttps://github.com/frank-xwang/RIDE-LongTailRecognition.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01809v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01786v1",
    "title": "Corpora Evaluation and System Bias Detection in Multi-document Summarization",
    "authors": [
      "Alvin Dey",
      "Tanya Chowdhury",
      "Yash Kumar Atri",
      "Tanmoy Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Multi-document summarization (MDS) is the task of reflecting key points from\nany set of documents into a concise text paragraph. In the past, it has been\nused to aggregate news, tweets, product reviews, etc. from various sources.\nOwing to no standard definition of the task, we encounter a plethora of\ndatasets with varying levels of overlap and conflict between participating\ndocuments. There is also no standard regarding what constitutes summary\ninformation in MDS. Adding to the challenge is the fact that new systems report\nresults on a set of chosen datasets, which might not correlate with their\nperformance on the other datasets. In this paper, we study this heterogeneous\ntask with the help of a few widely used MDS corpora and a suite of\nstate-of-the-art models. We make an attempt to quantify the quality of\nsummarization corpus and prescribe a list of points to consider while proposing\na new MDS corpus. Next, we analyze the reason behind the absence of an MDS\nsystem which achieves superior performance across all corpora. We then observe\nthe extent to which system metrics are influenced, and bias is propagated due\nto corpus properties. The scripts to reproduce the experiments in this work are\navailable at https://github.com/LCS2-IIITD/summarization_bias.git.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01786v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01773v3",
    "title": "MetaPhys: Few-Shot Adaptation for Non-Contact Physiological Measurement",
    "authors": [
      "Xin Liu",
      "Ziheng Jiang",
      "Josh Fromm",
      "Xuhai Xu",
      "Shwetak Patel",
      "Daniel McDuff"
    ],
    "author_ids": [],
    "abstract": "There are large individual differences in physiological processes, making\ndesigning personalized health sensing algorithms challenging. Existing machine\nlearning systems struggle to generalize well to unseen subjects or contexts and\ncan often contain problematic biases. Video-based physiological measurement is\nnot an exception. Therefore, learning personalized or customized models from a\nsmall number of unlabeled samples is very attractive as it would allow fast\ncalibrations to improve generalization and help correct biases. In this paper,\nwe present a novel meta-learning approach called MetaPhys for personalized\nvideo-based cardiac measurement for contactless pulse and heart rate\nmonitoring. Our method uses only 18-seconds of video for customization and\nworks effectively in both supervised and unsupervised manners. We evaluate our\nproposed approach on two benchmark datasets and demonstrate superior\nperformance in cross-dataset evaluation with substantial reductions (42% to\n44%) in errors compared with state-of-the-art approaches. We have also\ndemonstrated our proposed method significantly helps reduce the bias in skin\ntype.",
    "published_date": "2020-10-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01773v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.04053v1",
    "title": "Fairness in Machine Learning: A Survey",
    "authors": [
      "Simon Caton",
      "Christian Haas"
    ],
    "author_ids": [],
    "abstract": "As Machine Learning technologies become increasingly used in contexts that\naffect citizens, companies as well as researchers need to be confident that\ntheir application of these methods will not have unexpected social\nimplications, such as bias towards gender, ethnicity, and/or people with\ndisabilities. There is significant literature on approaches to mitigate bias\nand promote fairness, yet the area is complex and hard to penetrate for\nnewcomers to the domain. This article seeks to provide an overview of the\ndifferent schools of thought and approaches to mitigating (social) biases and\nincrease fairness in the Machine Learning literature. It organises approaches\ninto the widely accepted framework of pre-processing, in-processing, and\npost-processing methods, subcategorizing into a further 11 method areas.\nAlthough much of the literature emphasizes binary classification, a discussion\nof fairness in regression, recommender systems, unsupervised learning, and\nnatural language processing is also provided along with a selection of\ncurrently available open source libraries. The article concludes by summarising\nopen challenges articulated as four dilemmas for fairness research.",
    "published_date": "2020-10-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04053v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01485v1",
    "title": "Improving Lesion Detection by exploring bias on Skin Lesion dataset",
    "authors": [
      "Anusua Trivedi",
      "Sreya Muppalla",
      "Shreyaan Pathak",
      "Azadeh Mobasher",
      "Pawel Janowski",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "author_ids": [],
    "abstract": "All datasets contain some biases, often unintentional, due to how they were\nacquired and annotated. These biases distort machine-learning models'\nperformance, creating spurious correlations that the models can unfairly\nexploit, or, contrarily destroying clear correlations that the models could\nlearn. With the popularity of deep learning models, automated skin lesion\nanalysis is starting to play an essential role in the early detection of\nMelanoma. The ISIC Archive is one of the most used skin lesion sources to\nbenchmark deep learning-based tools. Bissoto et al. experimented with different\nbounding-box based masks and showed that deep learning models could classify\nskin lesion images without clinically meaningful information in the input data.\nTheir findings seem confounding since the ablated regions (random rectangular\nboxes) are not significant. The shape of the lesion is a crucial factor in the\nclinical characterization of a skin lesion. In that context, we performed a set\nof experiments that generate shape-preserving masks instead of rectangular\nbounding-box based masks. A deep learning model trained on these\nshape-preserving masked images does not outperform models trained on images\nwithout clinically meaningful information. That strongly suggests spurious\ncorrelations guiding the models. We propose use of general adversarial network\n(GAN) to mitigate the underlying bias.",
    "published_date": "2020-10-04T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01485v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01473v3",
    "title": "Spatial Frequency Bias in Convolutional Generative Adversarial Networks",
    "authors": [
      "Mahyar Khayatkhoei",
      "Ahmed Elgammal"
    ],
    "author_ids": [],
    "abstract": "As the success of Generative Adversarial Networks (GANs) on natural images\nquickly propels them into various real-life applications across different\ndomains, it becomes more and more important to clearly understand their\nlimitations. Specifically, understanding GANs' capability across the full\nspectrum of spatial frequencies, i.e. beyond the low-frequency dominant\nspectrum of natural images, is critical for assessing the reliability of GAN\ngenerated data in any detail-sensitive application (e.g. denoising, filling and\nsuper-resolution in medical and satellite images). In this paper, we show that\nthe ability of convolutional GANs to learn a distribution is significantly\naffected by the spatial frequency of the underlying carrier signal, that is,\nGANs have a bias against learning high spatial frequencies. Crucially, we show\nthat this bias is not merely a result of the scarcity of high frequencies in\nnatural images, rather, it is a systemic bias hindering the learning of high\nfrequencies regardless of their prominence in a dataset. Furthermore, we\nexplain why large-scale GANs' ability to generate fine details on natural\nimages does not exclude them from the adverse effects of this bias. Finally, we\npropose a method for manipulating this bias with minimal computational\noverhead. This method can be used to explicitly direct computational resources\ntowards any specific spatial frequency of interest in a dataset, extending the\nflexibility of GANs.",
    "published_date": "2020-10-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01473v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01470v3",
    "title": "User Fairness, Item Fairness, and Diversity for Rankings in Two-Sided Markets",
    "authors": [
      "Lequn Wang",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "Ranking items by their probability of relevance has long been the goal of\nconventional ranking systems. While this maximizes traditional criteria of\nranking performance, there is a growing understanding that it is an\noversimplification in online platforms that serve not only a diverse user\npopulation, but also the producers of the items. In particular, ranking\nalgorithms are expected to be fair in how they serve all groups of users -- not\njust the majority group -- and they also need to be fair in how they divide\nexposure among the items. These fairness considerations can partially be met by\nadding diversity to the rankings, as done in several recent works. However, we\nshow in this paper that user fairness, item fairness and diversity are\nfundamentally different concepts. In particular, we find that algorithms that\nconsider only one of the three desiderata can fail to satisfy and even harm the\nother two. To overcome this shortcoming, we present the first ranking algorithm\nthat explicitly enforces all three desiderata. The algorithm optimizes user and\nitem fairness as a convex optimization problem which can be solved optimally.\nFrom its solution, a ranking policy can be derived via a novel Birkhoff-von\nNeumann decomposition algorithm that optimizes diversity. Beyond the\ntheoretical analysis, we investigate empirically on a new benchmark dataset how\neffectively the proposed ranking algorithm can control user fairness, item\nfairness and diversity, as well as the trade-offs between them.",
    "published_date": "2020-10-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.AI",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01470v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01316v1",
    "title": "Accounts, Accountability and Agency for Safe and Ethical AI",
    "authors": [
      "Rob Procter",
      "Mark Rouncefield",
      "Peter Tolmie"
    ],
    "author_ids": [],
    "abstract": "We examine the problem of explainable AI (xAI) and explore what delivering\nxAI means in practice, particularly in contexts that involve formal or informal\nand ad-hoc collaboration where agency and accountability in decision-making are\nachieved and sustained interactionally. We use an example from an earlier study\nof collaborative decision-making in screening mammography and the difficulties\nusers faced when trying to interpret the behavior of an AI tool to illustrate\nthe challenges of delivering usable and effective xAI. We conclude by setting\nout a study programme for future research to help advance our understanding of\nxAI requirements for safe and ethical AI.",
    "published_date": "2020-10-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "H.1.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01316v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01306v2",
    "title": "Valid inequalities, preprocessing, and an effective heuristic for the uncapacitated three-level lot-sizing and replenishment problem with a distribution structure",
    "authors": [
      "Jesus O. Cunha",
      "Rafael A. Melo"
    ],
    "author_ids": [],
    "abstract": "We consider the uncapacitated three-level lot-sizing and replenishment\nproblem with a distribution structure. In this NP-hard problem, a single\nproduction plant sends the produced items to replenish warehouses from where\nthey are dispatched to the retailers in order to satisfy their demands over a\nfinite planning horizon. The goal of the problem is to determine an integrated\nproduction and distribution plan minimizing the total costs, which comprehends\nfixed production and transportation setup as well as variable inventory holding\ncosts. We describe new valid inequalities both in the space of a standard mixed\ninteger programming (MIP) formulation and in that of a new alternative extended\nMIP formulation. We show that using such extended formulation, valid\ninequalities having similar structures to those in the standard one allow\nachieving tighter linear relaxation bounds. Furthermore, we propose a\npreprocessing approach to reduce the size of a multi-commodity MIP formulation\nand a multi-start randomized bottom-up dynamic programming based heuristic.\nComputational experiments indicate that the use of the valid inequalities in a\nbranch-and-cut approach significantly increase the ability of a MIP solver to\nsolve instances to optimality. Additionally, the valid inequalities for the\nextended formulation outperform those for the standard one in terms of number\nof solved instances, running time and number of enumerated nodes. Moreover, the\nproposed heuristic is able to generate solutions with considerably low\noptimality gaps within very short computational times even for large instances.\nCombining the preprocessing approach with the heuristic, one can achieve an\nincrease in the number of solutions solved to optimality within the time limit\ntogether with significant reductions on the average times for solving them.",
    "published_date": "2020-10-03T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01306v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.01285v1",
    "title": "Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness",
    "authors": [
      "Lingjuan Lyu",
      "Xuanli He",
      "Yitong Li"
    ],
    "author_ids": [],
    "abstract": "It has been demonstrated that hidden representation learned by a deep model\ncan encode private information of the input, hence can be exploited to recover\nsuch information with reasonable accuracy. To address this issue, we propose a\nnovel approach called Differentially Private Neural Representation (DPNR) to\npreserve the privacy of the extracted representation from text. DPNR utilises\nDifferential Privacy (DP) to provide a formal privacy guarantee. Further, we\nshow that masking words via dropout can further enhance privacy. To maintain\nutility of the learned representation, we integrate DP-noisy representation\ninto a robust training process to derive a robust target model, which also\nhelps for model fairness over various demographic variables. Experimental\nresults on benchmark datasets under various parameter settings demonstrate that\nDPNR largely reduces privacy leakage without significantly sacrificing the main\ntask performance.",
    "published_date": "2020-10-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01285v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01101v2",
    "title": "Commuting Network Spillovers and COVID-19 Deaths Across US Counties",
    "authors": [
      "Christopher Seto",
      "Aria Khademi",
      "Corina Graif",
      "Vasant G. Honavar"
    ],
    "author_ids": [],
    "abstract": "This study explored how population mobility flows form commuting networks\nacross US counties and influence the spread of COVID-19. We utilized 3-level\nmixed effects negative binomial regression models to estimate the impact of\nnetwork COVID-19 exposure on county confirmed cases and deaths over time. We\nalso conducted weighting-based analyses to estimate the causal effect of\nnetwork exposure. Results showed that commuting networks matter for COVID-19\ndeaths and cases, net of spatial proximity, socioeconomic, and demographic\nfactors. Different local racial and ethnic concentrations are also associated\nwith unequal outcomes. These findings suggest that commuting is an important\ncausal mechanism in the spread of COVID-19 and highlight the significance of\ninterconnected of communities. The results suggest that local level mitigation\nand prevention efforts are more effective when complemented by similar efforts\nin the network of connected places. Implications for research on inequality in\nhealth and flexible work arrangements are discussed.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.CY",
      "cs.LG",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01101v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01084v2",
    "title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction",
    "authors": [
      "Wei Deng",
      "Qi Feng",
      "Georgios Karagiannis",
      "Guang Lin",
      "Faming Liang"
    ],
    "author_ids": [],
    "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown\npromise in accelerating the convergence in non-convex learning; however, an\nexcessively large correction for avoiding biases from noisy energy estimators\nhas limited the potential of the acceleration. To address this issue, we study\nthe variance reduction for noisy energy estimators, which promotes much more\neffective swaps. Theoretically, we provide a non-asymptotic analysis on the\nexponential acceleration for the underlying continuous-time Markov jump\nprocess; moreover, we consider a generalized Girsanov theorem which includes\nthe change of Poisson measure to overcome the crude discretization based on the\nGr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein\n($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and\nobtain the state-of-the-art results in optimization and uncertainty estimates\nfor synthetic experiments and image data.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "stat.CO",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01084v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.01079v6",
    "title": "On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach",
    "authors": [
      "Junpei Komiyama",
      "Shunya Noda"
    ],
    "author_ids": [],
    "abstract": "We analyze statistical discrimination in hiring markets using a multi-armed\nbandit model. Myopic firms face workers arriving with heterogeneous observable\ncharacteristics. The association between the worker's skill and characteristics\nis unknown ex ante; thus, firms need to learn it. Laissez-faire causes\nperpetual underestimation: minority workers are rarely hired, and therefore,\nthe underestimation tends to persist. Even a marginal imbalance in the\npopulation ratio frequently results in perpetual underestimation. We propose\ntwo policy solutions: a novel subsidy rule (the hybrid mechanism) and the\nRooney Rule. Our results indicate that temporary affirmative actions\neffectively alleviate discrimination stemming from insufficient data.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "econ.TH",
      "cs.GT",
      "econ.EM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01079v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.00975v2",
    "title": "Taking Modality-free Human Identification as Zero-shot Learning",
    "authors": [
      "Zhizhe Liu",
      "Xingxing Zhang",
      "Zhenfeng Zhu",
      "Shuai Zheng",
      "Yao Zhao",
      "Jian Cheng"
    ],
    "author_ids": [],
    "abstract": "Human identification is an important topic in event detection, person\ntracking, and public security. There have been numerous methods proposed for\nhuman identification, such as face identification, person re-identification,\nand gait identification. Typically, existing methods predominantly classify a\nqueried image to a specific identity in an image gallery set (I2I). This is\nseriously limited for the scenario where only a textual description of the\nquery or an attribute gallery set is available in a wide range of video\nsurveillance applications (A2I or I2A). However, very few efforts have been\ndevoted towards modality-free identification, i.e., identifying a query in a\ngallery set in a scalable way. In this work, we take an initial attempt, and\nformulate such a novel Modality-Free Human Identification (named MFHI) task as\na generic zero-shot learning model in a scalable way. Meanwhile, it is capable\nof bridging the visual and semantic modalities by learning a discriminative\nprototype of each identity. In addition, the semantics-guided spatial attention\nis enforced on visual modality to obtain representations with both high global\ncategory-level and local attribute-level discrimination. Finally, we design and\nconduct an extensive group of experiments on two common challenging\nidentification tasks, including face identification and person\nre-identification, demonstrating that our method outperforms a wide variety of\nstate-of-the-art methods on modality-free human identification.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.00975v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.00947v1",
    "title": "MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination",
    "authors": [
      "Shengyu Zhang",
      "Donghui Wang",
      "Zhou Zhao",
      "Siliang Tang",
      "Di Xie",
      "Fei Wu"
    ],
    "author_ids": [],
    "abstract": "In this paper, we investigate the problem of text-to-pedestrian synthesis,\nwhich has many potential applications in art, design, and video surveillance.\nExisting methods for text-to-bird/flower synthesis are still far from solving\nthis fine-grained image generation problem, due to the complex structure and\nheterogeneous appearance that the pedestrians naturally take on. To this end,\nwe propose the Multi-Grained Discrimination enhanced Generative Adversarial\nNetwork, that capitalizes a human-part-based Discriminator (HPD) and a\nself-cross-attended (SCA) global Discriminator in order to capture the\ncoherence of the complex body structure. A fined-grained word-level attention\nmechanism is employed in the HPD module to enforce diversified appearance and\nvivid details. In addition, two pedestrian generation metrics, named Pose Score\nand Pose Variance, are devised to evaluate the generation quality and\ndiversity, respectively. We conduct extensive experiments and ablation studies\non the caption-annotated pedestrian dataset, CUHK Person Description Dataset.\nThe substantial improvement over the various metrics demonstrates the efficacy\nof MGD-GAN on the text-to-pedestrian synthesis scenario.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.00947v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.00820v1",
    "title": "Discriminative and Generative Models for Anatomical Shape Analysison Point Clouds with Deep Neural Networks",
    "authors": [
      "Benjamin Gutierrez Becker",
      "Ignacio Sarasua",
      "Christian Wachinger"
    ],
    "author_ids": [],
    "abstract": "We introduce deep neural networks for the analysis of anatomical shapes that\nlearn a low-dimensional shape representation from the given task, instead of\nrelying on hand-engineered representations. Our framework is modular and\nconsists of several computing blocks that perform fundamental shape processing\ntasks. The networks operate on unordered point clouds and provide invariance to\nsimilarity transformations, avoiding the need to identify point correspondences\nbetween shapes. Based on the framework, we assemble a discriminative model for\ndisease classification and age regression, as well as a generative model for\nthe accruate reconstruction of shapes. In particular, we propose a conditional\ngenerative model, where the condition vector provides a mechanism to control\nthe generative process. instance, it enables to assess shape variations\nspecific to a particular diagnosis, when passing it as side information. Next\nto working on single shapes, we introduce an extension for the joint analysis\nof multiple anatomical structures, where the simultaneous modeling of multiple\nstructures can lead to a more compact encoding and a better understanding of\ndisorders. We demonstrate the advantages of our framework in comprehensive\nexperiments on real and synthetic data. The key insights are that (i) learning\na shape representation specific to the given task yields higher performance\nthan alternative shape descriptors, (ii) multi-structure analysis is both more\nefficient and more accurate than single-structure analysis, and (iii) point\nclouds generated by our model capture morphological differences associated to\nAlzheimers disease, to the point that they can be used to train a\ndiscriminative model for disease classification. Our framework naturally scales\nto the analysis of large datasets, giving it the potential to learn\ncharacteristic variations in large populations.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "68T07(Primary)",
      "I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.00820v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.00753v3",
    "title": "Model-sharing Games: Analyzing Federated Learning Under Voluntary Participation",
    "authors": [
      "Kate Donahue",
      "Jon Kleinberg"
    ],
    "author_ids": [],
    "abstract": "Federated learning is a setting where agents, each with access to their own\ndata source, combine models from local data to create a global model. If agents\nare drawing their data from different distributions, though, federated learning\nmight produce a biased global model that is not optimal for each agent. This\nmeans that agents face a fundamental question: should they choose the global\nmodel or their local model? We show how this situation can be naturally\nanalyzed through the framework of coalitional game theory.\n  We propose the following game: there are heterogeneous players with different\nmodel parameters governing their data distribution and different amounts of\ndata they have noisily drawn from their own distribution. Each player's goal is\nto obtain a model with minimal expected mean squared error (MSE) on their own\ndistribution. They have a choice of fitting a model based solely on their own\ndata, or combining their learned parameters with those of some subset of the\nother players. Combining models reduces the variance component of their error\nthrough access to more data, but increases the bias because of the\nheterogeneity of distributions.\n  Here, we derive exact expected MSE values for problems in linear regression\nand mean estimation. We then analyze the resulting game in the framework of\nhedonic game theory; we study how players might divide into coalitions, where\neach set of players within a coalition jointly construct model(s). We analyze\nthree methods of federation, modeling differing degrees of customization. In\nuniform federation, the agents collectively produce a single model. In\ncoarse-grained federation, each agent can weight the global model together with\ntheir local model. In fine-grained federation, each agent can flexibly combine\nmodels from all other agents in the federation. For each method, we analyze the\nstable partitions of players into coalitions.",
    "published_date": "2020-10-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.CY",
      "cs.DC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.00753v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.00600v1",
    "title": "#Election2020: The First Public Twitter Dataset on the 2020 US Presidential Election",
    "authors": [
      "Emily Chen",
      "Ashok Deb",
      "Emilio Ferrara"
    ],
    "author_ids": [],
    "abstract": "The integrity of democratic political discourse is at the core to guarantee\nfree and fair elections. With social media often dictating the tones and trends\nof politics-related discussion, it is of paramount important to be able to\nstudy online chatter, especially in the run up to important voting events, like\nin the case of the upcoming November 3, 2020 U.S. Presidential Election.\nLimited access to social media data is often the first barrier to impede,\nhinder, or slow down progress, and ultimately our understanding of online\npolitical discourse. To mitigate this issue and try to empower the\nComputational Social Science research community, we decided to publicly release\na massive-scale, longitudinal dataset of U.S. politics- and election-related\ntweets. This multilingual dataset that we have been collecting for over one\nyear encompasses hundreds of millions of tweets and tracks all salient U.S.\npolitics trends, actors, and events between 2019 and 2020. It predates and\nspans the whole period of Republican and Democratic primaries, with real-time\ntracking of all presidential contenders of both sides of the isle. After that,\nit focuses on presidential and vice-presidential candidates. Our dataset\nrelease is curated, documented and will be constantly updated on a\nweekly-basis, until the November 3, 2020 election and beyond. We hope that the\nacademic community, computational journalists, and research practitioners alike\nwill all take advantage of our dataset to study relevant scientific and social\nissues, including problems like misinformation, information manipulation,\ninterference, and distortion of online political discourse that have been\nprevalent in the context of recent election events in the United States and\nworldwide.\n  Our dataset is available at:\nhttps://github.com/echen102/us-pres-elections-2020",
    "published_date": "2020-10-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.00600v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.00133v1",
    "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    "authors": [
      "Nikita Nangia",
      "Clara Vania",
      "Rasika Bhalerao",
      "Samuel R. Bowman"
    ],
    "author_ids": [],
    "abstract": "Pretrained language models, especially masked language models (MLMs) have\nseen success across many NLP tasks. However, there is ample evidence that they\nuse the cultural biases that are undoubtedly present in the corpora they are\ntrained on, implicitly creating harm with biased representations. To measure\nsome forms of social bias in language models against protected demographic\ngroups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark\n(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing\nwith nine types of bias, like race, religion, and age. In CrowS-Pairs a model\nis presented with two sentences: one that is more stereotyping and another that\nis less stereotyping. The data focuses on stereotypes about historically\ndisadvantaged groups and contrasts them with advantaged groups. We find that\nall three of the widely-used MLMs we evaluate substantially favor sentences\nthat express stereotypes in every category in CrowS-Pairs. As work on building\nless biased models advances, this dataset can be used as a benchmark to\nevaluate progress.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.00133v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.14793v1",
    "title": "Approximating Nash Social Welfare under Rado Valuations",
    "authors": [
      "Jugal Garg",
      "Edin Husic",
      "Laszlo A. Vegh"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of approximating maximum Nash social welfare (NSW)\nwhile allocating a set of indivisible items to $n$ agents. The NSW is a popular\nobjective that provides a balanced tradeoff between the often conflicting\nrequirements of fairness and efficiency, defined as the weighted geometric mean\nof agents' valuations. For the symmetric additive case of the problem, where\nagents have the same weight with additive valuations, the first constant-factor\napproximation algorithm was obtained in 2015. This led to a flurry of work\nobtaining constant-factor approximation algorithms for the symmetric case under\nmild generalizations of additive, and $O(n)$-approximation algorithms for more\ngeneral valuations and for the asymmetric case.\n  In this paper, we make significant progress towards both symmetric and\nasymmetric NSW problems. We present the first constant-factor approximation\nalgorithm for the symmetric case under Rado valuations. Rado valuations form a\ngeneral class of valuation functions that arise from maximum cost independent\nmatching problems, including as special cases assignment (OXS) valuations and\nweighted matroid rank functions. Furthermore, our approach also gives the first\nconstant-factor approximation algorithm for the asymmetric case under Rado\nvaluations, provided that the maximum ratio between the weights is bounded by a\nconstant.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DM",
      "cs.DS",
      "cs.MA",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.14793v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.07035v1",
    "title": "MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces",
    "authors": [
      "Marlesson R. O. Santana",
      "Luckeciano C. Melo",
      "Fernando H. F. Camargo",
      "Bruno Brandão",
      "Anderson Soares",
      "Renan M. Oliveira",
      "Sandor Caetano"
    ],
    "author_ids": [],
    "abstract": "Recommender Systems are especially challenging for marketplaces since they\nmust maximize user satisfaction while maintaining the healthiness and fairness\nof such ecosystems. In this context, we observed a lack of resources to design,\ntrain, and evaluate agents that learn by interacting within these environments.\nFor this matter, we propose MARS-Gym, an open-source framework to empower\nresearchers and engineers to quickly build and evaluate Reinforcement Learning\nagents for recommendations in marketplaces. MARS-Gym addresses the whole\ndevelopment pipeline: data processing, model design and optimization, and\nmulti-sided evaluation. We also provide the implementation of a diverse set of\nbaseline agents, with a metrics-driven analysis of them in the Trivago\nmarketplace dataset, to illustrate how to conduct a holistic assessment using\nthe available metrics of recommendation, off-policy estimation, and fairness.\nWith MARS-Gym, we expect to bridge the gap between academic research and\nproduction systems, as well as to facilitate the design of new algorithms and\napplications.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.HC",
      "cs.LG",
      "stat.ML",
      "I.6.5; H.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07035v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.14574v1",
    "title": "Sequential Algorithms and Independent Sets Discovering on Large Sparse Random Graphs",
    "authors": [
      "Paola Bermolen",
      "Matthieu Jonckheere",
      "Federico Larroca",
      "Manuel Saenz"
    ],
    "author_ids": [],
    "abstract": "Computing the size of maximum independent sets is a NP-hard problem for fixed\ngraphs. Characterizing and designing efficient algorithms to estimate this\nindependence number for random graphs are notoriously difficult and still\nlargely open issues. In a companion paper, we showed that a low complexity\ndegree-greedy exploration is actually asymptotically optimal on a large class\nof sparse random graphs. Encouraged by this result, we present and study two\nvariants of sequential exploration algorithms: static and dynamic degree-aware\nexplorations. We derive hydrodynamic limits for both of them, which in turn\nallow us to compute the size of the resulting independent set. Whereas the\nformer is simpler to compute, the latter may be used to arbitrarily approximate\nthe degree-greedy algorithm. Both can be implemented in a distributed manner.\nThe corresponding hydrodynamic limits constitute an efficient method to compute\nor bound the independence number for a large class of sparse random graphs. As\nan application, we then show how our method may be used to estimate the\ncapacity of a large 802.11-based wireless network. We finally consider further\nindicators such as the fairness of the resulting configuration, and show how an\nunexpected trade-off between fairness and capacity can be achieved.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "math.PR",
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.14574v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.14573v6",
    "title": "Rain-Code Fusion : Code-to-code ConvLSTM Forecasting Spatiotemporal Precipitation",
    "authors": [
      "Takato Yasuno",
      "Akira Ishii",
      "Masazumi Amakata"
    ],
    "author_ids": [],
    "abstract": "Recently, flood damage has become a social problem owing to unexperienced\nweather conditions arising from climate change. An immediate response to heavy\nrain is important for the mitigation of economic losses and also for rapid\nrecovery. Spatiotemporal precipitation forecasts may enhance the accuracy of\ndam inflow prediction, more than 6 hours forward for flood damage mitigation.\nHowever, the ordinary ConvLSTM has the limitation of predictable range more\nthan 3-timesteps in real-world precipitation forecasting owing to the\nirreducible bias between target prediction and ground-truth value. This paper\nproposes a rain-code approach for spatiotemporal precipitation code-to-code\nforecasting. We propose a novel rainy feature that represents a temporal rainy\nprocess using multi-frame fusion for the timestep reduction. We perform\nrain-code studies with various term ranges based on the standard ConvLSTM. We\napplied to a dam region within the Japanese rainy term hourly precipitation\ndata, under 2006 to 2019 approximately 127 thousands hours, every year from May\nto October. We apply the radar analysis hourly data on the central broader\nregion with an area of 136 x 148 km2 . Finally we have provided sensitivity\nstudies between the rain-code size and hourly accuracy within the several\nforecasting range.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "physics.ao-ph",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.14573v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.14474v2",
    "title": "User-item matching for recommendation fairness",
    "authors": [
      "Qiang Dong",
      "Shuang-Shuang Xie",
      "Wen-Jun Li"
    ],
    "author_ids": [],
    "abstract": "As we all know, users and item-providers are two main parties of participants\nin recommender systems. However, most existing research efforts on\nrecommendation were focused on better serving users and overlooked the purpose\nof item-providers. This paper is devoted to improve the item exposure fairness\nfor item-providers' objective, and keep the recommendation accuracy not\ndecreased or even improved for users' objective. We propose to set stock volume\nconstraints on items, to be specific, limit the maximally allowable recommended\ntimes of an item to be proportional to the frequency of its being interacted in\nthe past, which is validated to achieve superior item exposure fairness to\ncommon recommenders and thus mitigates the Matthew Effect on item popularity.\nWith the two constraints of pre-existing recommendation length of users and our\nstock volumes of items, a heuristic strategy based on normalized scores and a\nMinimum Cost Maximum Flow (MCMF) based model are proposed to solve the optimal\nuser-item matching problem, whose accuracy performances are even better than\nthat of baseline algorithm in regular recommendation context, and in line with\nstate-of-the-art enhancement of the baseline. What's more, our MCMF based\nstrategy is parameter-free, while those counterpart algorithms have to resort\nto parameter traversal process to achieve their best performance.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.14474v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.14361v1",
    "title": "Ethically Collecting Multi-Modal Spontaneous Conversations with People that have Cognitive Impairments",
    "authors": [
      "Angus Addlesee",
      "Pierre Albert"
    ],
    "author_ids": [],
    "abstract": "In order to make spoken dialogue systems (such as Amazon Alexa or Google\nAssistant) more accessible and naturally interactive for people with cognitive\nimpairments, appropriate data must be obtainable. Recordings of multi-modal\nspontaneous conversations with vulnerable user groups are scarce however and\nthis valuable data is challenging to collect. Researchers that call for this\ndata are commonly inexperienced in ethical and legal issues around working with\nvulnerable participants. Additionally, standard recording equipment is insecure\nand should not be used to capture sensitive data. We spent a year consulting\nexperts on how to ethically capture and share recordings of multi-modal\nspontaneous conversations with vulnerable user groups. In this paper we provide\nguidance, collated from these experts, on how to ethically collect such data\nand we present a new system - \"CUSCO\" - to capture, transport and exchange\nsensitive data securely. This framework is intended to be easily followed and\nimplemented to encourage further publications of similar corpora. Using this\nguide and secure recording system, researchers can review and refine their\nethical measures.",
    "published_date": "2020-09-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.14361v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07029v2",
    "title": "Basic principles and concept design of a real-time clinical decision support system for managing medical emergencies on missions to Mars",
    "authors": [
      "Juan M Garcia-Gomez"
    ],
    "author_ids": [],
    "abstract": "Space agencies and private companies prepare the beginning of human space\nexploration for the 2030s with missions to put the first human on the Mars\nsurface. The absence of gravity and radiation, along with distance, isolation\nand hostile environments, are expected to increase medical events where\npreviously unseen manifestations may arise. The current healthcare strategy\nbased on telemedicine and the possibility to stabilize and transport the\ninjured crewmember to a terrestrial definitive medical facility is not\napplicable in exploration class missions. Therefore, the need for deploying the\nfull autonomous capability to solve medical emergencies may guide the design of\nfuture onboard healthcare systems. We present ten basic principles and concept\ndesign of a software suite to bring onboard decision support to help the crew\ndealing with medical emergencies taking into consideration physiological\ndisturbances in space and spaceflight restrictions. 1) give real-time support\nfor emergency medical decision making, 2) give patient-specific advice for\nexecutive problem-solving, 3) take into account available information from life\nsupport and monitoring of crewmembers, 4) be fully autonomous from remote\nfacilities, 5) continuously adapt predictions to physiological disturbance and\nchanging conditions, 6) optimize emergency medical decision making in terms of\nmission fundamental priorities, 7) take into account medical supplies and\nequipment on board, 8) apply health standards for the level of care V, 9)\nimplement ethics responsibilities for spaceflights, and 10) apply ethical\nstandards for artificial intelligence. Based on these principles, we propose an\nautonomous clinical decision support system (CDSS) to provide real-time advice\nfor emergency medical interventions on board of space exploration missions.",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "physics.med-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07029v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.14285v1",
    "title": "Decentralized Patient Centric e-Health Record Management System using Blockchain and IPFS",
    "authors": [
      "Gaganjeet Reen",
      "Manasi Mohandas",
      "S Venkatesan"
    ],
    "author_ids": [],
    "abstract": "Electronic Health Records(EHR) are gaining a lot of popularity all over the\nworld. The current EHR systems however have their fair share of problems\nrelated to privacy and security. We have proposed a mechanism which provides a\nsolution to most of these problems. Using a permissioned Ethereum blockchain\nallows the hospitals and patients across the world to be connected to each\nother. Our mechanism uses a combination of symmetric and asymmetric key\ncryptography to ensure the secure storage and selective access of records. It\ngives patients full control over their health records and also allows them to\ngrant or revoke a hospital's access to his/her records. We have used IPFS(inter\nplanetary file system) to store records which has the advantage of being\ndistributed and ensures immutability of records. The proposed model also\nmaintains the statistics of diseases without violating the privacy of any\npatient.",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.14285v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.13953v1",
    "title": "One-Shot learning based classification for segregation of plastic waste",
    "authors": [
      "Shivaank Agarwal",
      "Ravindra Gudi",
      "Paresh Saxena"
    ],
    "author_ids": [],
    "abstract": "The problem of segregating recyclable waste is fairly daunting for many\ncountries. This article presents an approach for image based classification of\nplastic waste using one-shot learning techniques. The proposed approach\nexploits discriminative features generated via the siamese and triplet loss\nconvolutional neural networks to help differentiate between 5 types of plastic\nwaste based on their resin codes. The approach achieves an accuracy of 99.74%\non the WaDaBa Database",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13953v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13888v4",
    "title": "Utility is in the Eye of the User: A Critique of NLP Leaderboards",
    "authors": [
      "Kawin Ethayarajh",
      "Dan Jurafsky"
    ],
    "author_ids": [],
    "abstract": "Benchmarks such as GLUE have helped drive advances in NLP by incentivizing\nthe creation of more accurate models. While this leaderboard paradigm has been\nremarkably successful, a historical focus on performance-based evaluation has\nbeen at the expense of other qualities that the NLP community values in models,\nsuch as compactness, fairness, and energy efficiency. In this opinion paper, we\nstudy the divergence between what is incentivized by leaderboards and what is\nuseful in practice through the lens of microeconomic theory. We frame both the\nleaderboard and NLP practitioners as consumers and the benefit they get from a\nmodel as its utility to them. With this framing, we formalize how leaderboards\n-- in their current form -- can be poor proxies for the NLP community at large.\nFor example, a highly inefficient model would provide less utility to\npractitioners but not to a leaderboard, since it is a cost that only the former\nmust bear. To allow practitioners to better estimate a model's utility to them,\nwe advocate for more transparency on leaderboards, such as the reporting of\nstatistics that are of practical concern (e.g., model size, energy efficiency,\nand inference latency).",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13888v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13871v2",
    "title": "Signs for Ethical AI: A Route Towards Transparency",
    "authors": [
      "Dario Garcia-Gasulla",
      "Atia Cortés",
      "Sergio Alvarez-Napagao",
      "Ulises Cortés"
    ],
    "author_ids": [],
    "abstract": "Today, Artificial Intelligence (AI) has a direct impact on the daily life of\nbillions of people. Being applied to sectors like finance, health, security and\nadvertisement, AI fuels some of the biggest companies and research institutions\nin the world. Its impact in the near future seems difficult to predict or\nbound. In contrast to all this power, society remains mostly ignorant of the\ncapabilities and standard practices of AI today. To address this imbalance,\nimproving current interactions between people and AI systems, we propose a\ntransparency scheme to be implemented on any AI system open to the public. The\nscheme is based on two pillars: Data Privacy and AI Transparency. The first\nrecognizes the relevance of data for AI, and is supported by GDPR. The second\nconsiders aspects of AI transparency currently unregulated: AI capabilities,\npurpose and source. We design this pillar based on ethical principles. For each\nof the two pillars, we define a three-level display. The first level is based\non visual signs, inspired by traffic signs managing the interaction between\npeople and cars, and designed for quick and universal interpretability. The\nsecond level uses factsheets, providing limited details. The last level\nprovides access to all available information. After detailing and exemplifying\nthe proposed transparency scheme, we define a set of principles for creating\ntransparent by design software, to be used during the integration of AI\ncomponents on user-oriented services.",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13871v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13859v1",
    "title": "Fake News Spreader Detection on Twitter using Character N-Grams. Notebook for PAN at CLEF 2020",
    "authors": [
      "Inna Vogel",
      "Meghana Meghana"
    ],
    "author_ids": [],
    "abstract": "The authors of fake news often use facts from verified news sources and mix\nthem with misinformation to create confusion and provoke unrest among the\nreaders. The spread of fake news can thereby have serious implications on our\nsociety. They can sway political elections, push down the stock price or crush\nreputations of corporations or public figures. Several websites have taken on\nthe mission of checking rumors and allegations, but are often not fast enough\nto check the content of all the news being disseminated. Especially social\nmedia websites have offered an easy platform for the fast propagation of\ninformation. Towards limiting fake news from being propagated among social\nmedia users, the task of this year's PAN 2020 challenge lays the focus on the\nfake news spreaders. The aim of the task is to determine whether it is possible\nto discriminate authors that have shared fake news in the past from those that\nhave never done it. In this notebook, we describe our profiling system for the\nfake news detection task on Twitter. For this, we conduct different feature\nextraction techniques and learning experiments from a multilingual perspective,\nnamely English and Spanish. Our final submitted systems use character n-grams\nas features in combination with a linear SVM for English and Logistic\nRegression for the Spanish language. Our submitted models achieve an overall\naccuracy of 73% and 79% on the English and Spanish official test set,\nrespectively. Our experiments show that it is difficult to differentiate\nsolidly fake news spreaders on Twitter from users who share credible\ninformation leaving room for further investigations. Our model ranked 3rd out\nof 72 competitors.",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13859v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13825v2",
    "title": "Information theoretic network approach to socioeconomic correlations",
    "authors": [
      "Alec Kirkley"
    ],
    "author_ids": [],
    "abstract": "Due to its wide reaching implications for everything from identifying\nhotspots of income inequality to political redistricting, there is a rich body\nof literature across the sciences quantifying spatial patterns in socioeconomic\ndata. In particular, the variability of indicators relevant to social and\neconomic well-being between localized populations is of great interest, as it\npertains to the spatial manifestations of inequality and segregation. However,\nheterogeneity in population density, sensitivity of statistical analyses to\nspatial aggregation, and the importance of pre-drawn political boundaries for\npolicy intervention may decrease the efficacy and relevance of existing methods\nfor analyzing spatial socioeconomic data. Additionally, these measures commonly\nlack either a framework for comparing results for qualitative and quantitative\ndata on the same scale, or a mechanism for generalization to multi-region\ncorrelations. To mitigate these issues associated with traditional spatial\nmeasures, here we view local deviations in socioeconomic variables from a\ntopological lens rather than a spatial one, and use a novel information\ntheoretic network approach based on the Generalized Jensen Shannon Divergence\nto distinguish distributional quantities across adjacent regions. We apply our\nmethodology in a series of experiments to study the network of neighboring\ncensus tracts in the continental US, quantifying the decay in two-point\ndistributional correlations across the network, examining the county-level\nsocioeconomic disparities induced from the aggregation of tracts, and\nconstructing an algorithm for the division of a city into homogeneous clusters.\nThese results provide a new framework for analyzing the variation of attributes\nacross regional populations, and shed light on new, universal patterns in\nsocioeconomic attributes.",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13825v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.13741v2",
    "title": "Competition Alleviates Present Bias in Task Completion",
    "authors": [
      "Aditya Saraf",
      "Anna R. Karlin",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "We build upon recent work [Kleinberg and Oren, 2014, Kleinberg et al., 2016,\n2017] that considers present biased agents, who place more weight on costs they\nmust incur now than costs they will incur in the future. They consider a graph\ntheoretic model where agents must complete a task and show that present biased\nagents can take exponentially more expensive paths than optimal. We propose a\ntheoretical model that adds competition into the mix -- two agents compete to\nfinish a task first. We show that, in a wide range of settings, a small amount\nof competition can alleviate the harms of present bias. This can help explain\nwhy biased agents may not perform so poorly in naturally competitive settings,\nand can guide task designers on how to protect present biased agents from harm.\nOur work thus paints a more positive picture than much of the existing\nliterature on present bias.",
    "published_date": "2020-09-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13741v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.13674v1",
    "title": "Vaccination strategies against COVID-19 and the diffusion of anti-vaccination views",
    "authors": [
      "Rafael Prieto Curiel",
      "Humberto González Ramírez"
    ],
    "author_ids": [],
    "abstract": "Miss-information is usually adjusted to fit distinct narratives and can\npropagate rapidly through communities of interest, which work as echo chambers,\ncause reinforcement and foster confirmation bias. False beliefs, once adopted,\nare rarely corrected. Amidst the COVID-19 crisis, pandemic-deniers and people\nwho oppose wearing face masks or quarantines have already been a substantial\naspect of the development of the pandemic. With a potential vaccine for\nCOVID-19, different anti-vaccine narratives will be created and, likely,\nadopted by large population groups, with critical consequences. Here, we\nanalyse epidemic spreading and optimal vaccination strategies, measured with\nthe average years of life lost, in two network topologies (scale-free and\nsmall-world) assuming full adherence to vaccine administration. We consider the\nspread of anti-vaccine views in the network, using a similar diffusion model as\nthe one used in epidemics, which are adopted based on a persuasiveness\nparameter of anti-vaccine views. Results show that even if an anti-vaccine\nnarrative has a small persuasiveness, a large part of the population will be\nrapidly exposed to them. Assuming that all individuals are equally likely to\nadopt anti-vaccine views after being exposed, more central nodes in the network\nare more exposed and therefore are more likely to adopt them. Comparing years\nof life lost, anti-vaccine views could have a significant cost not only on\nthose who share them, since the core social benefits of a limited vaccination\nstrategy (reduction of susceptible hosts, network disruptions and slowing the\nspread of the disease) are substantially shortened.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13674v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13650v1",
    "title": "Towards a Measure of Individual Fairness for Deep Learning",
    "authors": [
      "Krystal Maughan",
      "Joseph P. Near"
    ],
    "author_ids": [],
    "abstract": "Deep learning has produced big advances in artificial intelligence, but\ntrained neural networks often reflect and amplify bias in their training data,\nand thus produce unfair predictions. We propose a novel measure of individual\nfairness, called prediction sensitivity, that approximates the extent to which\na particular prediction is dependent on a protected attribute. We show how to\ncompute prediction sensitivity using standard automatic differentiation\ncapabilities present in modern deep learning frameworks, and present\npreliminary empirical results suggesting that prediction sensitivity may be\neffective for measuring bias in individual predictions.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13650v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13472v5",
    "title": "Targeted VAE: Variational and Targeted Learning for Causal Inference",
    "authors": [
      "Matthew James Vowels",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "author_ids": [],
    "abstract": "Undertaking causal inference with observational data is incredibly useful\nacross a wide range of tasks including the development of medical treatments,\nadvertisements and marketing, and policy making. There are two significant\nchallenges associated with undertaking causal inference using observational\ndata: treatment assignment heterogeneity (\\textit{i.e.}, differences between\nthe treated and untreated groups), and an absence of counterfactual data\n(\\textit{i.e.}, not knowing what would have happened if an individual who did\nget treatment, were instead to have not been treated). We address these two\nchallenges by combining structured inference and targeted learning. In terms of\nstructure, we factorize the joint distribution into risk, confounding,\ninstrumental, and miscellaneous factors, and in terms of targeted learning, we\napply a regularizer derived from the influence curve in order to reduce\nresidual bias. An ablation study is undertaken, and an evaluation on benchmark\ndatasets demonstrates that TVAE has competitive and state of the art\nperformance.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13472v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13447v3",
    "title": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients",
    "authors": [
      "Jing An",
      "Lexing Ying",
      "Yuhua Zhu"
    ],
    "author_ids": [],
    "abstract": "A data set sampled from a certain population is biased if the subgroups of\nthe population are sampled at proportions that are significantly different from\ntheir underlying proportions. Training machine learning models on biased data\nsets requires correction techniques to compensate for the bias. We consider two\ncommonly-used techniques, resampling and reweighting, that rebalance the\nproportions of the subgroups to maintain the desired objective function. Though\nstatistically equivalent, it has been observed that resampling outperforms\nreweighting when combined with stochastic gradient algorithms. By analyzing\nillustrative examples, we explain the reason behind this phenomenon using tools\nfrom dynamical stability and stochastic asymptotics. We also present\nexperiments from regression, classification, and off-policy prediction to\ndemonstrate that this is a general phenomenon. We argue that it is imperative\nto consider the objective function design and the optimization algorithm\ntogether while addressing the sampling bias.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13447v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13377v2",
    "title": "Convergence of gradient-based block coordinate descent algorithms for non-orthogonal joint approximate diagonalization of matrices",
    "authors": [
      "Jianze Li",
      "Konstantin Usevich",
      "Pierre Comon"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a gradient-based block coordinate descent (BCD-G)\nframework to solve the joint approximate diagonalization of matrices defined on\nthe product of the complex Stiefel manifold and the special linear group.\nInstead of the cyclic fashion, we choose a block optimization based on the\nRiemannian gradient. To update the first block variable in the complex Stiefel\nmanifold, we use the well-known line search descent method. To update the\nsecond block variable in the special linear group, based on four kinds of\ndifferent elementary transformations, we construct three classes: GLU, GQU and\nGU, and then get three BCD-G algorithms: BCD-GLU, BCD-GQU and BCD-GU. We\nestablish the global and weak convergence of these three algorithms using the\n\\L{}ojasiewicz gradient inequality under the assumption that the iterates are\nbounded. We also propose a gradient-based Jacobi-type framework to solve the\njoint approximate diagonalization of matrices defined on the special linear\ngroup. As in the BCD-G case, using the GLU and GQU classes of elementary\ntransformations, we focus on the Jacobi-GLU and Jacobi-GQU algorithms and\nestablish their global and weak convergence. All the algorithms and convergence\nresults described in this paper also apply to the real case.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "49M30, 65F99, 90C30, 15A23"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13377v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.13323v2",
    "title": "AI Progress in Skin Lesion Analysis",
    "authors": [
      "Philippe M. Burlina",
      "William Paul",
      "Phil A. Mathew",
      "Neil J. Joshi",
      "Alison W. Rebman",
      "John N. Aucott"
    ],
    "author_ids": [],
    "abstract": "We examine progress in the use of AI for detecting skin lesions, with\nparticular emphasis on the erythema migrans rash of acute Lyme disease, and\nother lesions, such as those from conditions like herpes zoster (shingles),\ntinea corporis, erythema multiforme, cellulitis, insect bites, or tick bites.\nWe discuss important challenges for these applications, in particular the\nproblems of AI bias regarding the lack of skin images in dark skinned\nindividuals, being able to accurately detect, delineate, and segment lesions or\nregions of interest compared to normal skin in images, and low shot learning\n(addressing classification with a paucity of training images). Solving these\nproblems ranges from being highly desirable requirements -- e.g. for\ndelineation, which may be useful to disambiguate between similar types of\nlesions, and perform improved diagnostics -- or required, as is the case for AI\nde-biasing, to allow for the deployment of fair AI techniques in the clinic for\nskin lesion analysis. For the problem of low shot learning in particular, we\nreport skin analysis algorithms that gracefully degrade and still perform well\nat low shots, when compared to baseline algorithms: when using a little as 10\ntraining exemplars per class, the baseline DL algorithm performance\nsignificantly degrades, with accuracy of 56.41%, close to chance, whereas the\nbest performing low shot algorithm yields an accuracy of 85.26%.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13323v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.07039v1",
    "title": "Risk of Interruption of Doctoral Studies and Mental Health in PhD Students",
    "authors": [
      "Sara M. González-Betancor",
      "Pablo Dorta-González"
    ],
    "author_ids": [],
    "abstract": "PhD students report a higher prevalence of mental illness symptoms than\nhighly educated individuals in the general population. This situation presents\na serious problem for universities. Thus, the knowledge about this phenomenon\nis of great importance in decision-making. In this paper we use the Nature PhD\nsurvey 2019 and estimate several binomial logistic regression models to analyze\nthe risk of interrupting doctoral studies. This risk is measured through the\ndesire of change in either the supervisor or the area of expertise, or the wish\nof not pursue a PhD. Among the explanatory factors, we focus on the influence\nof anxiety/depression, discrimination, and bullying. As control variables we\nuse demographic characteristics and others related with the doctoral program.\nInsufficient contact time with supervisors, and exceeding time spent studying\n-crossing the 50-h week barrier-, are risk factors of PhD studies interruption,\nbut the most decisive risk factor is poor mental health. Universities should\ntherefore foster an environment of well-being, which allows the development of\nautonomy and resilience of their PhD students or, when necessary, which fosters\nthe development of conflict resolution skills.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07039v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.13198v1",
    "title": "Discrimination of attractors with noisy nodes in Boolean networks",
    "authors": [
      "Xiaoqing Cheng",
      "Wai-Ki Ching",
      "Sini Guo",
      "Tatsuya Akutsu"
    ],
    "author_ids": [],
    "abstract": "Observing the internal state of the whole system using a small number of\nsensor nodes is important in analysis of complex networks. Here, we study the\nproblem of determining the minimum number of sensor nodes to discriminate\nattractors under the assumption that each attractor has at most K noisy nodes.\nWe present exact and approximation algorithms for this minimization problem.\nThe effectiveness of the algorithms is also demonstrated by computational\nexperiments using both synthetic data and realistic biological data.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13198v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.13165v1",
    "title": "Quantal synaptic dilution enhances sparse encoding and dropout regularisation in deep networks",
    "authors": [
      "Gardave S Bhumbra"
    ],
    "author_ids": [],
    "abstract": "Dropout is a technique that silences the activity of units stochastically\nwhile training deep networks to reduce overfitting. Here we introduce Quantal\nSynaptic Dilution (QSD), a biologically plausible model of dropout\nregularisation based on the quantal properties of neuronal synapses, that\nincorporates heterogeneities in response magnitudes and release probabilities\nfor vesicular quanta. QSD outperforms standard dropout in ReLU multilayer\nperceptrons, with enhanced sparse encoding at test time when dropout masks are\nreplaced with identity functions, without shifts in trainable weight or bias\ndistributions. For convolutional networks, the method also improves\ngeneralisation in computer vision tasks with and without inclusion of\nadditional forms of regularisation. QSD also outperforms standard dropout in\nrecurrent networks for language modelling and sentiment analysis. An advantage\nof QSD over many variations of dropout is that it can be implemented generally\nin all conventional deep networks where standard dropout is applicable.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13165v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13055v3",
    "title": "Rotated Binary Neural Network",
    "authors": [
      "Mingbao Lin",
      "Rongrong Ji",
      "Zihan Xu",
      "Baochang Zhang",
      "Yan Wang",
      "Yongjian Wu",
      "Feiyue Huang",
      "Chia-Wen Lin"
    ],
    "author_ids": [],
    "abstract": "Binary Neural Network (BNN) shows its predominance in reducing the complexity\nof deep neural networks. However, it suffers severe performance degradation.\nOne of the major impediments is the large quantization error between the\nfull-precision weight vector and its binary vector. Previous works focus on\ncompensating for the norm gap while leaving the angular bias hardly touched. In\nthis paper, for the first time, we explore the influence of angular bias on the\nquantization error and then introduce a Rotated Binary Neural Network (RBNN),\nwhich considers the angle alignment between the full-precision weight vector\nand its binarized version. At the beginning of each training epoch, we propose\nto rotate the full-precision weight vector to its binary vector to reduce the\nangular bias. To avoid the high complexity of learning a large rotation matrix,\nwe further introduce a bi-rotation formulation that learns two smaller rotation\nmatrices. In the training stage, we devise an adjustable rotated weight vector\nfor binarization to escape the potential local optimum. Our rotation leads to\naround 50% weight flips which maximize the information gain. Finally, we\npropose a training-aware approximation of the sign function for the gradient\nbackward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of\nRBNN over many state-of-the-arts. Our source code, experimental settings,\ntraining logs and binary models are available at\nhttps://github.com/lmbxmu/RBNN.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13055v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13028v2",
    "title": "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning",
    "authors": [
      "Haochen Liu",
      "Wentao Wang",
      "Yiqi Wang",
      "Hui Liu",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "Dialogue systems play an increasingly important role in various aspects of\nour daily life. It is evident from recent research that dialogue systems\ntrained on human conversation data are biased. In particular, they can produce\nresponses that reflect people's gender prejudice. Many debiasing methods have\nbeen developed for various NLP tasks, such as word embedding. However, they are\nnot directly applicable to dialogue systems because they are likely to force\ndialogue models to generate similar responses for different genders. This\ngreatly degrades the diversity of the generated responses and immensely hurts\nthe performance of the dialogue models. In this paper, we propose a novel\nadversarial learning framework Debiased-Chat to train dialogue models free from\ngender bias while keeping their performance. Extensive experiments on two\nreal-world conversation datasets show that our framework significantly reduces\ngender bias in dialogue models while maintaining the response quality. The\nimplementation of the proposed framework is released.",
    "published_date": "2020-09-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13028v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12979v1",
    "title": "Moral Framing and Ideological Bias of News",
    "authors": [
      "Negar Mokhberian",
      "Andrés Abeliuk",
      "Patrick Cummings",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "News outlets are a primary source for many people to learn what is going on\nin the world. However, outlets with different political slants, when talking\nabout the same news story, usually emphasize various aspects and choose their\nlanguage framing differently. This framing implicitly shows their biases and\nalso affects the reader's opinion and understanding. Therefore, understanding\nthe framing in the news stories is fundamental for realizing what kind of view\nthe writer is conveying with each news story. In this paper, we describe\nmethods for characterizing moral frames in the news. We capture the frames\nbased on the Moral Foundation Theory. This theory is a psychological concept\nwhich explains how every kind of morality and opinion can be summarized and\npresented with five main dimensions. We propose an unsupervised method that\nextracts the framing Bias and the framing Intensity without any external\nframing annotations provided. We validate the performance on an annotated\ntwitter dataset and then use it to quantify the framing bias and partisanship\nof news.",
    "published_date": "2020-09-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12979v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.12853v1",
    "title": "Persuasion Meets AI: Ethical Considerations for the Design of Social Engineering Countermeasures",
    "authors": [
      "Nicolas E. Díaz Ferreyra",
      "Esma Aïmeur",
      "Hicham Hage",
      "Maritta Heisel",
      "Catherine García van Hoogstraten"
    ],
    "author_ids": [],
    "abstract": "Privacy in Social Network Sites (SNSs) like Facebook or Instagram is closely\nrelated to people's self-disclosure decisions and their ability to foresee the\nconsequences of sharing personal information with large and diverse audiences.\nNonetheless, online privacy decisions are often based on spurious risk\njudgements that make people liable to reveal sensitive data to untrusted\nrecipients and become victims of social engineering attacks. Artificial\nIntelligence (AI) in combination with persuasive mechanisms like nudging is a\npromising approach for promoting preventative privacy behaviour among the users\nof SNSs. Nevertheless, combining behavioural interventions with high levels of\npersonalization can be a potential threat to people's agency and autonomy even\nwhen applied to the design of social engineering countermeasures. This paper\nelaborates on the ethical challenges that nudging mechanisms can introduce to\nthe development of AI-based countermeasures, particularly to those addressing\nunsafe self-disclosure practices in SNSs. Overall, it endorses the elaboration\nof personalized risk awareness solutions as i) an ethical approach to\ncounteract social engineering, and ii) as an effective means for promoting\nreflective privacy decisions.",
    "published_date": "2020-09-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12853v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12675v3",
    "title": "A Primal-Dual Subgradient Approachfor Fair Meta Learning",
    "authors": [
      "Chen Zhao",
      "Feng Chen",
      "Zhuoyi Wang",
      "Latifur Khan"
    ],
    "author_ids": [],
    "abstract": "The problem of learning to generalize to unseen classes during training,\nknown as few-shot classification, has attracted considerable attention.\nInitialization based methods, such as the gradient-based model agnostic\nmeta-learning (MAML), tackle the few-shot learning problem by \"learning to\nfine-tune\". The goal of these approaches is to learn proper model\ninitialization, so that the classifiers for new classes can be learned from a\nfew labeled examples with a small number of gradient update steps. Few shot\nmeta-learning is well-known with its fast-adapted capability and accuracy\ngeneralization onto unseen tasks. Learning fairly with unbiased outcomes is\nanother significant hallmark of human intelligence, which is rarely touched in\nfew-shot meta-learning. In this work, we propose a Primal-Dual Fair\nMeta-learning framework, namely PDFM, which learns to train fair machine\nlearning models using only a few examples based on data from related tasks. The\nkey idea is to learn a good initialization of a fair model's primal and dual\nparameters so that it can adapt to a new fair learning task via a few gradient\nupdate steps. Instead of manually tuning the dual parameters as hyperparameters\nvia a grid search, PDFM optimizes the initialization of the primal and dual\nparameters jointly for fair meta-learning via a subgradient primal-dual\napproach. We further instantiate examples of bias controlling using mean\ndifference and decision boundary covariance as fairness constraints to each\ntask for supervised regression and classification, respectively. We demonstrate\nthe versatility of our proposed approach by applying our approach to various\nreal-world datasets. Our experiments show substantial improvements over the\nbest prior work for this setting.",
    "published_date": "2020-09-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12675v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12562v1",
    "title": "Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach",
    "authors": [
      "Cuong Tran",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck"
    ],
    "author_ids": [],
    "abstract": "A critical concern in data-driven decision making is to build models whose\noutcomes do not discriminate against some demographic groups, including gender,\nethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of\nthe sensitive attributes is essential, while, in practice, these attributes may\nnot be available due to legal and ethical requirements. To address this\nchallenge, this paper studies a model that protects the privacy of the\nindividuals sensitive information while also allowing it to learn\nnon-discriminatory predictors. The method relies on the notion of differential\nprivacy and the use of Lagrangian duality to design neural networks that can\naccommodate fairness constraints while guaranteeing the privacy of sensitive\nattributes. The paper analyses the tension between accuracy, privacy, and\nfairness and the experimental evaluation illustrates the benefits of the\nproposed model on several prediction tasks.",
    "published_date": "2020-09-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12547v2",
    "title": "Causal Intervention for Weakly-Supervised Semantic Segmentation",
    "authors": [
      "Dong Zhang",
      "Hanwang Zhang",
      "Jinhui Tang",
      "Xiansheng Hua",
      "Qianru Sun"
    ],
    "author_ids": [],
    "abstract": "We present a causal inference framework to improve Weakly-Supervised Semantic\nSegmentation (WSSS). Specifically, we aim to generate better pixel-level\npseudo-masks by using only image-level labels -- the most crucial step in WSSS.\nWe attribute the cause of the ambiguous boundaries of pseudo-masks to the\nconfounding context, e.g., the correct image-level classification of \"horse\"\nand \"person\" may be not only due to the recognition of each instance, but also\ntheir co-occurrence context, making the model inspection (e.g., CAM) hard to\ndistinguish between the boundaries. Inspired by this, we propose a structural\ncausal model to analyze the causalities among images, contexts, and class\nlabels. Based on it, we develop a new method: Context Adjustment (CONTA), to\nremove the confounding bias in image-level classification and thus provide\nbetter pseudo-masks as ground-truth for the subsequent segmentation model. On\nPASCAL VOC 2012 and MS-COCO, we show that CONTA boosts various popular WSSS\nmethods to new state-of-the-arts.",
    "published_date": "2020-09-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12547v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12405v1",
    "title": "Fair and Efficient Online Allocations with Normalized Valuations",
    "authors": [
      "Vasilis Gkatzelis",
      "Alexandros Psomas",
      "Xizhi Tan"
    ],
    "author_ids": [],
    "abstract": "A set of divisible resources becomes available over a sequence of rounds and\nneeds to be allocated immediately and irrevocably. Our goal is to distribute\nthese resources to maximize fairness and efficiency. Achieving any non-trivial\nguarantees in an adversarial setting is impossible. However, we show that\nnormalizing the agent values, a very common assumption in fair division, allows\nus to escape this impossibility. Our main result is an online algorithm for the\ncase of two agents that ensures the outcome is envy-free while guaranteeing\n91.6% of the optimal social welfare. We also show that this is near-optimal:\nthere is no envy-free algorithm that guarantees more than 93.3% of the optimal\nsocial welfare.",
    "published_date": "2020-09-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12405v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.12303v4",
    "title": "Towards Debiasing NLU Models from Unknown Biases",
    "authors": [
      "Prasetya Ajie Utama",
      "Nafise Sadat Moosavi",
      "Iryna Gurevych"
    ],
    "author_ids": [],
    "abstract": "NLU models often exploit biases to achieve high dataset-specific performance\nwithout properly learning the intended task. Recently proposed debiasing\nmethods are shown to be effective in mitigating this tendency. However, these\nmethods rely on a major assumption that the types of bias should be known\na-priori, which limits their application to many NLU tasks and datasets. In\nthis work, we present the first step to bridge this gap by introducing a\nself-debiasing framework that prevents models from mainly utilizing biases\nwithout knowing them in advance. The proposed framework is general and\ncomplementary to the existing debiasing methods. We show that it allows these\nexisting methods to retain the improvement on the challenge datasets (i.e.,\nsets of examples designed to expose models' reliance on biases) without\nspecifically targeting certain biases. Furthermore, the evaluation suggests\nthat applying the framework results in improved overall robustness.",
    "published_date": "2020-09-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12303v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12152v1",
    "title": "Ethical conceptual replication of visualization research considering sources of methodological bias and practical significance",
    "authors": [
      "Ian T. Ruginski"
    ],
    "author_ids": [],
    "abstract": "General design principles for visualization have been relatively\nwell-established based on a combination of cognitive and perceptual theory and\nempirical evaluations over the past 20 years. To determine how these principles\nhold up across use contexts and end-users, I argue that we should emphasize\nconceptual replication focused on determining practical significance and\nreducing methodological biases. This shift in thinking aims to determine how\ndesign principles interact with methodological approaches, laying the\ngroundwork for visualization meta-science.",
    "published_date": "2020-09-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "H.5.2; H.1.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12152v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.12040v1",
    "title": "Fairness in Semi-supervised Learning: Unlabeled Data Help to Reduce Discrimination",
    "authors": [
      "Tao Zhang",
      "Tianqing Zhu",
      "Jing Li",
      "Mengde Han",
      "Wanlei Zhou",
      "Philip S. Yu"
    ],
    "author_ids": [],
    "abstract": "A growing specter in the rise of machine learning is whether the decisions\nmade by machine learning models are fair. While research is already underway to\nformalize a machine-learning concept of fairness and to design frameworks for\nbuilding fair models with sacrifice in accuracy, most are geared toward either\nsupervised or unsupervised learning. Yet two observations inspired us to wonder\nwhether semi-supervised learning might be useful to solve discrimination\nproblems. First, previous study showed that increasing the size of the training\nset may lead to a better trade-off between fairness and accuracy. Second, the\nmost powerful models today require an enormous of data to train which, in\npractical terms, is likely possible from a combination of labeled and unlabeled\ndata. Hence, in this paper, we present a framework of fair semi-supervised\nlearning in the pre-processing phase, including pseudo labeling to predict\nlabels for unlabeled data, a re-sampling method to obtain multiple fair\ndatasets and lastly, ensemble learning to improve accuracy and decrease\ndiscrimination. A theoretical decomposition analysis of bias, variance and\nnoise highlights the different sources of discrimination and the impact they\nhave on fairness in semi-supervised learning. A set of experiments on\nreal-world and synthetic datasets show that our method is able to use unlabeled\ndata to achieve a better trade-off between accuracy and discrimination.",
    "published_date": "2020-09-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12040v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13300v1",
    "title": "Target Privacy Threat Modeling for COVID-19 Exposure Notification Systems",
    "authors": [
      "Ananya Gangavarapu",
      "Ellie Daw",
      "Abhishek Singh",
      "Rohan Iyer",
      "Gabriel Harp",
      "Sam Zimmerman",
      "Ramesh Raskar"
    ],
    "author_ids": [],
    "abstract": "The adoption of digital contact tracing (DCT) technology during the\nCOVID-19pandemic has shown multiple benefits, including helping to slow the\nspread of infectious disease and to improve the dissemination of accurate\ninformation. However, to support both ethical technology deployment and user\nadoption, privacy must be at the forefront. With the loss of privacy being a\ncritical threat, thorough threat modeling will help us to strategize and\nprotect privacy as digital contact tracing technologies advance. Various threat\nmodeling frameworks exist today, such as LINDDUN, STRIDE, PASTA, and NIST,\nwhich focus on software system privacy, system security, application security,\nand data-centric risk, respectively. When applied to the exposure notification\nsystem (ENS) context, these models provide a thorough view of the software side\nbut fall short in addressing the integrated nature of hardware, humans,\nregulations, and software involved in such systems. Our approach addresses\nENSsas a whole and provides a model that addresses the privacy complexities of\na multi-faceted solution. We define privacy principles, privacy threats,\nattacker capabilities, and a comprehensive threat model. Finally, we outline\nthreat mitigation strategies that address the various threats defined in our\nmodel",
    "published_date": "2020-09-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13300v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.11982v2",
    "title": "Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias",
    "authors": [
      "Ana Valeria Gonzalez",
      "Maria Barrett",
      "Rasmus Hvingelby",
      "Kellie Webster",
      "Anders Søgaard"
    ],
    "author_ids": [],
    "abstract": "The one-sided focus on English in previous studies of gender bias in NLP\nmisses out on opportunities in other languages: English challenge datasets such\nas GAP and WinoGender highlight model preferences that are \"hallucinatory\",\ne.g., disambiguating gender-ambiguous occurrences of 'doctor' as male doctors.\nWe show that for languages with type B reflexivization, e.g., Swedish and\nRussian, we can construct multi-task challenge datasets for detecting gender\nbias that lead to unambiguously wrong model predictions: In these languages,\nthe direct translation of 'the doctor removed his mask' is not ambiguous\nbetween a coreferential reading and a disjoint reading. Instead, the\ncoreferential reading requires a non-gendered pronoun, and the gendered,\npossessive pronouns are anti-reflexive. We present a multilingual, multi-task\nchallenge dataset, which spans four languages and four NLP tasks and focuses\nonly on this phenomenon. We find evidence for gender bias across all\ntask-language combinations and correlate model bias with national labor market\nstatistics.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11982v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11736v1",
    "title": "Graph Sparsification with Generative Adversarial Network",
    "authors": [
      "Hang-Yang Wu",
      "Yi-Ling Chen"
    ],
    "author_ids": [],
    "abstract": "Graph sparsification aims to reduce the number of edges of a network while\nmaintaining its accuracy for given tasks. In this study, we propose a novel\nmethod called GSGAN, which is able to sparsify networks for community detection\ntasks. GSGAN is able to capture those relationships that are not shown in the\noriginal graph but are relatively important, and creating artificial edges to\nreflect these relationships and further increase the effectiveness of the\ncommunity detection task. We adopt GAN as the learning model and guide the\ngenerator to produce random walks that are able to capture the structure of a\nnetwork. Specifically, during the training phase, in addition to judging the\nauthenticity of the random walk, discriminator also considers the relationship\nbetween nodes at the same time. We design a reward function to guide the\ngenerator creating random walks that contain useful hidden relation\ninformation. These random walks are then combined to form a new social network\nthat is efficient and effective for community detection. Experiments with\nreal-world networks demonstrate that the proposed GSGAN is much more effective\nthan the baselines, and GSGAN can be applied and helpful to various clustering\nalgorithms of community detection.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11736v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2010.06986v2",
    "title": "On the Problem of Underranking in Group-Fair Ranking",
    "authors": [
      "Sruthi Gorantla",
      "Amit Deshpande",
      "Anand Louis"
    ],
    "author_ids": [],
    "abstract": "Search and recommendation systems, such as search engines, recruiting tools,\nonline marketplaces, news, and social media, output ranked lists of content,\nproducts, and sometimes, people. Credit ratings, standardized tests, risk\nassessments output only a score, but are also used implicitly for ranking. Bias\nin such ranking systems, especially among the top ranks, can worsen social and\neconomic inequalities, polarize opinions, and reinforce stereotypes. On the\nother hand, a bias correction for minority groups can cause more harm if\nperceived as favoring group-fair outcomes over meritocracy. In this paper, we\nformulate the problem of underranking in group-fair rankings, which was not\naddressed in previous work. Most group-fair ranking algorithms post-process a\ngiven ranking and output a group-fair ranking. We define underranking based on\nhow close the group-fair rank of each item is to its original rank, and prove a\nlower bound on the trade-off achievable for simultaneous underranking and group\nfairness in ranking. We give a fair ranking algorithm that takes any given\nranking and outputs another ranking with simultaneous underranking and group\nfairness guarantees comparable to the lower bound we prove. Our algorithm works\nwith group fairness constraints for any number of groups. Our experimental\nresults confirm the theoretical trade-off between underranking and group\nfairness, and also show that our algorithm achieves the best of both when\ncompared to the state-of-the-art baselines.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.06986v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11677v1",
    "title": "Legally grounded fairness objectives",
    "authors": [
      "Dylan Holden-Sim",
      "Gavin Leech",
      "Laurence Aitchison"
    ],
    "author_ids": [],
    "abstract": "Recent work has identified a number of formally incompatible operational\nmeasures for the unfairness of a machine learning (ML) system. As these\nmeasures all capture intuitively desirable aspects of a fair system, choosing\n\"the one true\" measure is not possible, and instead a reasonable approach is to\nminimize a weighted combination of measures. However, this simply raises the\nquestion of how to choose the weights. Here, we formulate Legally Grounded\nFairness Objectives (LGFO), which uses signals from the legal system to\nnon-arbitrarily measure the social cost of a specific degree of unfairness. The\nLGFO is the expected damages under a putative lawsuit that might be awarded to\nthose who were wrongly classified, in the sense that the ML system made a\ndecision different to that which would have be made under the court's preferred\nmeasure. Notably, the two quantities necessary to compute the LGFO, the court's\npreferences about fairness measures, and the expected damages, are unknown but\nwell-defined, and can be estimated by legal advice. Further, as the damages\nawarded by the legal system are designed to measure and compensate for the harm\ncaused to an individual by an unfair classification, the LGFO aligns closely\nwith society's estimate of the social cost.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.12362v1",
    "title": "Self-Weighted Robust LDA for Multiclass Classification with Edge Classes",
    "authors": [
      "Caixia Yan",
      "Xiaojun Chang",
      "Minnan Luo",
      "Qinghua Zheng",
      "Xiaoqin Zhang",
      "Zhihui Li",
      "Feiping Nie"
    ],
    "author_ids": [],
    "abstract": "Linear discriminant analysis (LDA) is a popular technique to learn the most\ndiscriminative features for multi-class classification. A vast majority of\nexisting LDA algorithms are prone to be dominated by the class with very large\ndeviation from the others, i.e., edge class, which occurs frequently in\nmulti-class classification. First, the existence of edge classes often makes\nthe total mean biased in the calculation of between-class scatter matrix.\nSecond, the exploitation of l2-norm based between-class distance criterion\nmagnifies the extremely large distance corresponding to edge class. In this\nregard, a novel self-weighted robust LDA with l21-norm based pairwise\nbetween-class distance criterion, called SWRLDA, is proposed for multi-class\nclassification especially with edge classes. SWRLDA can automatically avoid the\noptimal mean calculation and simultaneously learn adaptive weights for each\nclass pair without setting any additional parameter. An efficient re-weighted\nalgorithm is exploited to derive the global optimum of the challenging l21-norm\nmaximization problem. The proposed SWRLDA is easy to implement, and converges\nfast in practice. Extensive experiments demonstrate that SWRLDA performs\nfavorably against other compared methods on both synthetic and real-world\ndatasets, while presenting superior computational efficiency in comparison with\nother techniques.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12362v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11524v1",
    "title": "Adversarial Brain Multiplex Prediction From a Single Network for High-Order Connectional Gender-Specific Brain Mapping",
    "authors": [
      "Ahmed Nebli",
      "Islem Rekik"
    ],
    "author_ids": [],
    "abstract": "Brain connectivity networks, derived from magnetic resonance imaging (MRI),\nnon-invasively quantify the relationship in function, structure, and morphology\nbetween two brain regions of interest (ROIs) and give insights into\ngender-related connectional differences. However, to the best of our knowledge,\nstudies on gender differences in brain connectivity were limited to\ninvestigating pairwise (i.e., low-order) relationship ROIs, overlooking the\ncomplex high-order interconnectedness of the brain as a network. To address\nthis limitation, brain multiplexes have been introduced to model the\nrelationship between at least two different brain networks. However, this\ninhibits their application to datasets with single brain networks such as\nfunctional networks. To fill this gap, we propose the first work on predicting\nbrain multiplexes from a source network to investigate gender differences.\nRecently, generative adversarial networks (GANs) submerged the field of medical\ndata synthesis. However, although conventional GANs work well on images, they\ncannot handle brain networks due to their non-Euclidean topological structure.\nDifferently, in this paper, we tap into the nascent field of geometric-GANs\n(G-GAN) to design a deep multiplex prediction architecture comprising (i) a\ngeometric source to target network translator mimicking a U-Net architecture\nwith skip connections and (ii) a conditional discriminator which classifies\npredicted target intra-layers by conditioning on the multiplex source\nintra-layers. Such architecture simultaneously learns the latent source network\nrepresentation and the deep non-linear mapping from the source to target\nmultiplex intra-layers. Our experiments on a large dataset demonstrated that\npredicted multiplexes significantly boost gender classification accuracy\ncompared with source networks and identifies both low and high-order\ngender-specific multiplex connections.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11524v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11491v1",
    "title": "Understanding Fairness of Gender Classification Algorithms Across Gender-Race Groups",
    "authors": [
      "Anoop Krishnan",
      "Ali Almadan",
      "Ajita Rattani"
    ],
    "author_ids": [],
    "abstract": "Automated gender classification has important applications in many domains,\nsuch as demographic research, law enforcement, online advertising, as well as\nhuman-computer interaction. Recent research has questioned the fairness of this\ntechnology across gender and race. Specifically, the majority of the studies\nraised the concern of higher error rates of the face-based gender\nclassification system for darker-skinned people like African-American and for\nwomen. However, to date, the majority of existing studies were limited to\nAfrican-American and Caucasian only. The aim of this paper is to investigate\nthe differential performance of the gender classification algorithms across\ngender-race groups. To this aim, we investigate the impact of (a) architectural\ndifferences in the deep learning algorithms and (b) training set imbalance, as\na potential source of bias causing differential performance across gender and\nrace. Experimental investigations are conducted on two latest large-scale\npublicly available facial attribute datasets, namely, UTKFace and FairFace. The\nexperimental results suggested that the algorithms with architectural\ndifferences varied in performance with consistency towards specific gender-race\ngroups. For instance, for all the algorithms used, Black females (Black race in\ngeneral) always obtained the least accuracy rates. Middle Eastern males and\nLatino females obtained higher accuracy rates most of the time. Training set\nimbalance further widens the gap in the unequal accuracy rates across all\ngender-race groups. Further investigations using facial landmarks suggested\nthat facial morphological differences due to the bone structure influenced by\ngenetic and environmental factors could be the cause of the least performance\nof Black females and Black race, in general.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11491v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11429v2",
    "title": "Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks",
    "authors": [
      "Xiaokang Liu",
      "Haijun Song"
    ],
    "author_ids": [],
    "abstract": "Petrographic analysis based on microfacies identification in thin sections is\nwidely used in sedimentary environment interpretation and paleoecological\nreconstruction. Fossil recognition from microfacies is an essential procedure\nfor petrographers to complete this task. Distinguishing the morphological and\nmicrostructural diversity of skeletal fragments requires extensive prior\nknowledge of fossil morphotypes in microfacies and long training sessions under\nthe microscope. This requirement engenders certain challenges for\nsedimentologists and paleontologists, especially novices. However, a machine\nclassifier can help address this challenge. In this study, we collected a\nmicrofacies image dataset comprising both public data from 1,149 references and\nour own materials (including 30,815 images of 22 fossil and abiotic grain\ngroups). We employed a high-performance workstation to implement four classic\ndeep convolutional neural networks (DCNNs), which have proven to be highly\nefficient in computer vision over the last several years. Our framework uses a\ntransfer learning technique, which reuses the pre-trained parameters that are\ntrained on a larger ImageNet dataset as initialization for the network to\nachieve high accuracy with low computing costs. We obtained up to 95% of the\ntop one and 99% of the top three test accuracies in the Inception ResNet v2\narchitecture. The machine classifier exhibited 0.99 precision on minerals, such\nas dolomite and pyrite. Although it had some difficulty on samples having\nsimilar morphologies, such as the bivalve, brachiopod, and ostracod, it\nnevertheless obtained 0.88 precision. Our machine learning framework\ndemonstrated high accuracy with reproducibility and bias avoidance that was\ncomparable to those of human classifiers. Its application can thus eliminate\nmuch of the tedious, manually intensive efforts by human experts conducting\nroutine identification.",
    "published_date": "2020-09-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11429v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11406v1",
    "title": "Unfairness Discovery and Prevention For Few-Shot Regression",
    "authors": [
      "Chen Zhao",
      "Feng Chen"
    ],
    "author_ids": [],
    "abstract": "We study fairness in supervised few-shot meta-learning models that are\nsensitive to discrimination (or bias) in historical data. A machine learning\nmodel trained based on biased data tends to make unfair predictions for users\nfrom minority groups. Although this problem has been studied before, existing\nmethods mainly aim to detect and control the dependency effect of the protected\nvariables (e.g. race, gender) on target prediction based on a large amount of\ntraining data. These approaches carry two major drawbacks that (1) lacking\nshowing a global cause-effect visualization for all variables; (2) lacking\ngeneralization of both accuracy and fairness to unseen tasks. In this work, we\nfirst discover discrimination from data using a causal Bayesian knowledge graph\nwhich not only demonstrates the dependency of the protected variable on target\nbut also indicates causal effects between all variables. Next, we develop a\nnovel algorithm based on risk difference in order to quantify the\ndiscriminatory influence for each protected variable in the graph. Furthermore,\nto protect prediction from unfairness, a fast-adapted bias-control approach in\nmeta-learning is proposed, which efficiently mitigates statistical disparity\nfor each task and it thus ensures independence of protected attributes on\npredictions based on biased and few-shot data samples. Distinct from existing\nmeta-learning models, group unfairness of tasks are efficiently reduced by\nleveraging the mean difference between (un)protected groups for regression\nproblems. Through extensive experiments on both synthetic and real-world data\nsets, we demonstrate that our proposed unfairness discovery and prevention\napproaches efficiently detect discrimination and mitigate biases on model\noutput as well as generalize both accuracy and fairness to unseen tasks with a\nlimited amount of training samples.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11406v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.13516v1",
    "title": "Fair Meta-Learning For Few-Shot Classification",
    "authors": [
      "Chen Zhao",
      "Changbin Li",
      "Jincheng Li",
      "Feng Chen"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence nowadays plays an increasingly prominent role in our\nlife since decisions that were once made by humans are now delegated to\nautomated systems. A machine learning algorithm trained based on biased data,\nhowever, tends to make unfair predictions. Developing classification algorithms\nthat are fair with respect to protected attributes of the data thus becomes an\nimportant problem. Motivated by concerns surrounding the fairness effects of\nsharing and few-shot machine learning tools, such as the Model Agnostic\nMeta-Learning framework, we propose a novel fair fast-adapted few-shot\nmeta-learning approach that efficiently mitigates biases during meta-train by\nensuring controlling the decision boundary covariance that between the\nprotected variable and the signed distance from the feature vectors to the\ndecision boundary. Through extensive experiments on two real-world image\nbenchmarks over three state-of-the-art meta-learning algorithms, we empirically\ndemonstrate that our proposed approach efficiently mitigates biases on model\noutput and generalizes both accuracy and fairness to unseen tasks with a\nlimited amount of training samples.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.13516v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11405v1",
    "title": "Rank-Based Multi-task Learning for Fair Regression",
    "authors": [
      "Chen Zhao",
      "Feng Chen"
    ],
    "author_ids": [],
    "abstract": "In this work, we develop a novel fairness learning approach for multi-task\nregression models based on a biased training dataset, using a popular\nrank-based non-parametric independence test, i.e., Mann Whitney U statistic,\nfor measuring the dependency between target variable and protected variables.\nTo solve this learning problem efficiently, we first reformulate the problem as\na new non-convex optimization problem, in which a non-convex constraint is\ndefined based on group-wise ranking functions of individual objects. We then\ndevelop an efficient model-training algorithm based on the framework of\nnon-convex alternating direction method of multipliers (NC-ADMM), in which one\nof the main challenges is to implement an efficient projection oracle to the\npreceding non-convex set defined based on ranking functions. Through the\nextensive experiments on both synthetic and real-world datasets, we validated\nthe out-performance of our new approach against several state-of-the-art\ncompetitive methods on several popular metrics relevant to fairness learning.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11405v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11374v2",
    "title": "Fast Adaptation Nonlinear Observer for SLAM",
    "authors": [
      "Trevor P. Drayton",
      "Abdul A. Jaiyeola",
      "Nazmul Hoque",
      "Mikhayla Maurer",
      "Hashim A. Hashim"
    ],
    "author_ids": [],
    "abstract": "The process of simultaneously mapping the environment in three dimensional\n(3D) space and localizing a moving vehicle's pose (orientation and position) is\ntermed Simultaneous Localization and Mapping (SLAM). SLAM is a core task in\nrobotics applications. In the SLAM problem, each of the vehicle's pose and the\nenvironment are assumed to be completely unknown. This paper takes the\nconventional SLAM design as a basis and proposes a novel approach that ensures\nfast adaptation of the nonlinear observer for SLAM. Due to the fact that the\ntrue SLAM problem is nonlinear and is modeled on the Lie group of\n$\\mathbb{SLAM}_{n}\\left(3\\right)$, the proposed observer for SLAM is nonlinear\nand modeled on $\\mathbb{SLAM}_{n}\\left(3\\right)$. The proposed observer\ncompensates for unknown bias attached to velocity measurements. The results of\nthe simulation illustrate the robustness of the proposed approach.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11374v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.11360v1",
    "title": "EPEM: Efficient Parameter Estimation for Multiple Class Monotone Missing Data",
    "authors": [
      "Thu Nguyen",
      "Duy H. M. Nguyen",
      "Huy Nguyen",
      "Binh T. Nguyen",
      "Bruce A. Wade"
    ],
    "author_ids": [],
    "abstract": "The problem of monotone missing data has been broadly studied during the last\ntwo decades and has many applications in different fields such as\nbioinformatics or statistics. Commonly used imputation techniques require\nmultiple iterations through the data before yielding convergence. Moreover,\nthose approaches may introduce extra noises and biases to the subsequent\nmodeling. In this work, we derive exact formulas and propose a novel algorithm\nto compute the maximum likelihood estimators (MLEs) of a multiple class,\nmonotone missing dataset when all the covariance matrices of all categories are\nassumed to be equal, namely EPEM. We then illustrate an application of our\nproposed methods in Linear Discriminant Analysis (LDA). As the computation is\nexact, our EPEM algorithm does not require multiple iterations through the data\nas other imputation approaches, thus promising to handle much less\ntime-consuming than other methods. This effectiveness was validated by\nempirical results when EPEM reduced the error rates significantly and required\na short computation time compared to several imputation-based approaches. We\nalso release all codes and data of our experiments in one GitHub repository to\ncontribute to the research community related to this problem.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11274v1",
    "title": "The Agent Web Model -- Modelling web hacking for reinforcement learning",
    "authors": [
      "Laszlo Erdodi",
      "Fabio Massimo Zennaro"
    ],
    "author_ids": [],
    "abstract": "Website hacking is a frequent attack type used by malicious actors to obtain\nconfidential information, modify the integrity of web pages or make websites\nunavailable. The tools used by attackers are becoming more and more automated\nand sophisticated, and malicious machine learning agents seems to be the next\ndevelopment in this line. In order to provide ethical hackers with similar\ntools, and to understand the impact and the limitations of artificial agents,\nwe present in this paper a model that formalizes web hacking tasks for\nreinforcement learning agents. Our model, named Agent Web Model, considers web\nhacking as a capture-the-flag style challenge, and it defines reinforcement\nlearning problems at seven different levels of abstraction. We discuss the\ncomplexity of these problems in terms of actions and states an agent has to\ndeal with, and we show that such a model allows to represent most of the\nrelevant web vulnerabilities. Aware that the driver of advances in\nreinforcement learning is the availability of standardized challenges, we\nprovide an implementation for the first three abstraction layers, in the hope\nthat the community would consider these challenges in order to develop\nintelligent web hacking agents.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11274v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11232v1",
    "title": "A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention",
    "authors": [
      "Binjie Zhang",
      "Yu Li",
      "Chun Yuan",
      "Dejing Xu",
      "Pin Jiang",
      "Ying Shan"
    ],
    "author_ids": [],
    "abstract": "The task of language-guided video temporal grounding is to localize the\nparticular video clip corresponding to a query sentence in an untrimmed video.\nThough progress has been made continuously in this field, some issues still\nneed to be resolved. First, most of the existing methods rely on the\ncombination of multiple complicated modules to solve the task. Second, due to\nthe semantic gaps between the two different modalities, aligning the\ninformation at different granularities (local and global) between the video and\nthe language is significant, which is less addressed. Last, previous works do\nnot consider the inevitable annotation bias due to the ambiguities of action\nboundaries. To address these limitations, we propose a simple two-branch\nCross-Modality Attention (CMA) module with intuitive structure design, which\nalternatively modulates two modalities for better matching the information both\nlocally and globally. In addition, we introduce a new task-specific regression\nloss function, which improves the temporal grounding accuracy by alleviating\nthe impact of annotation bias. We conduct extensive experiments to validate our\nmethod, and the results show that just with this simple model, it can\noutperform the state of the arts on both Charades-STA and ActivityNet Captions\ndatasets.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11232v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11054v1",
    "title": "Supervised Multi-topology Network Cross-diffusion for Population-driven Brain Network Atlas Estimation",
    "authors": [
      "Islem Mhiri",
      "Mohamed Ali Mahjoub",
      "Islem Rekik"
    ],
    "author_ids": [],
    "abstract": "Estimating a representative and discriminative brain network atlas (BNA) is a\nnascent research field in mapping a population of brain networks in health and\ndisease. Although limited, existing BNA estimation methods have several\nlimitations. First, they primarily rely on a similarity network diffusion and\nfusion technique, which only considers node degree as a topological measure in\nthe cross-network diffusion process, thereby overlooking rich topological\nmeasures of the brain network (e.g., centrality). Second, both diffusion and\nfusion techniques are implemented in fully unsupervised manner, which might\ndecrease the discriminative power of the estimated BNAs. To fill these gaps, we\npropose a supervised multi-topology network cross-diffusion (SM-netFusion)\nframework for estimating a BNA satisfying : (i) well-representativeness\n(captures shared traits across subjects), (ii) well-centeredness (optimally\nclose to all subjects), and (iii) high discriminativeness (can easily and\nefficiently identify discriminative brain connections that distinguish between\ntwo populations). For a specific class, given the cluster labels of the\ntraining data, we learn a weighted combination of the topological diffusion\nkernels derived from degree, closeness and eigenvector centrality measures in a\nsupervised manner. Specifically, we learn the cross-diffusion process by\nnormalizing the training brain networks using the learned diffusion kernels.\nOur SM-netFusion produces the most centered and representative template in\ncomparison with its variants and state-of-the-art methods and further boosted\nthe classification of autistic subjects by 5-15%. SM-netFusion presents the\nfirst work for supervised network cross-diffusion based on graph topological\nmeasures, which can be further leveraged to design an efficient graph feature\nselection method for training predictive learners in network neuroscience.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11054v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11023v2",
    "title": "The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal Sufficient Subsets",
    "authors": [
      "Oana-Maria Camburu",
      "Eleonora Giunchiglia",
      "Jakob Foerster",
      "Thomas Lukasiewicz",
      "Phil Blunsom"
    ],
    "author_ids": [],
    "abstract": "For neural models to garner widespread public trust and ensure fairness, we\nmust have human-intelligible explanations for their predictions. Recently, an\nincreasing number of works focus on explaining the predictions of neural models\nin terms of the relevance of the input features. In this work, we show that\nfeature-based explanations pose problems even for explaining trivial models. We\nshow that, in certain cases, there exist at least two ground-truth\nfeature-based explanations, and that, sometimes, neither of them is enough to\nprovide a complete view of the decision-making process of the model. Moreover,\nwe show that two popular classes of explainers, Shapley explainers and minimal\nsufficient subsets explainers, target fundamentally different types of\nground-truth explanations, despite the apparently implicit assumption that\nexplainers should look for one specific feature-based explanation. These\nfindings bring an additional dimension to consider in both developing and\nchoosing explainers.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11023v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10990v2",
    "title": "Accurate and Interpretable Machine Learning for Transparent Pricing of Health Insurance Plans",
    "authors": [
      "Rohun Kshirsagar",
      "Li-Yen Hsu",
      "Vatshank Chaturvedi",
      "Charles H. Greenberg",
      "Matthew McClelland",
      "Anushadevi Mohan",
      "Wideet Shende",
      "Nicolas P. Tilmans",
      "Renzo Frigato",
      "Min Guo",
      "Ankit Chheda",
      "Meredith Trotter",
      "Shonket Ray",
      "Arnold Lee",
      "Miguel Alvarado"
    ],
    "author_ids": [],
    "abstract": "Health insurance companies cover half of the United States population through\ncommercial employer-sponsored health plans and pay 1.2 trillion US dollars\nevery year to cover medical expenses for their members. The actuary and\nunderwriter roles at a health insurance company serve to assess which risks to\ntake on and how to price those risks to ensure profitability of the\norganization. While Bayesian hierarchical models are the current standard in\nthe industry to estimate risk, interest in machine learning as a way to improve\nupon these existing methods is increasing. Lumiata, a healthcare analytics\ncompany, ran a study with a large health insurance company in the United\nStates. We evaluated the ability of machine learning models to predict the per\nmember per month cost of employer groups in their next renewal period,\nespecially those groups who will cost less than 95\\% of what an actuarial model\npredicts (groups with \"concession opportunities\"). We developed a sequence of\ntwo models, an individual patient-level and an employer-group-level model, to\npredict the annual per member per month allowed amount for employer groups,\nbased on a population of 14 million patients. Our models performed 20\\% better\nthan the insurance carrier's existing pricing model, and identified 84\\% of the\nconcession opportunities. This study demonstrates the application of a machine\nlearning system to compute an accurate and fair price for health insurance\nproducts and analyzes how explainable machine learning models can exceed\nactuarial models' predictive accuracy while maintaining interpretability.",
    "published_date": "2020-09-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10990v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10808v1",
    "title": "Using Machine Learning to Develop a Novel COVID-19 Vulnerability Index (C19VI)",
    "authors": [
      "Anuj Tiwari",
      "Arya V. Dadhania",
      "Vijay Avin Balaji Ragunathrao",
      "Edson R. A. Oliveira"
    ],
    "author_ids": [],
    "abstract": "COVID19 is now one of the most leading causes of death in the United States.\nSystemic health, social and economic disparities have put the minorities and\neconomically poor communities at a higher risk than others. There is an\nimmediate requirement to develop a reliable measure of county-level\nvulnerabilities that can capture the heterogeneity of both vulnerable\ncommunities and the COVID19 pandemic. This study reports a COVID19\nVulnerability Index (C19VI) for identification and mapping of vulnerable\ncounties in the United States. We proposed a Random Forest machine learning\nbased COVID19 vulnerability model using CDC sociodemographic and\nCOVID19-specific themes. An innovative COVID19 Impact Assessment algorithm was\nalso developed using homogeneity and trend assessment technique for evaluating\nseverity of the pandemic in all counties and train RF model. Developed C19VI\nwas statistically validated and compared with the CDC COVID19 Community\nVulnerability Index (CCVI). Finally, using C19VI along with census data, we\nexplored racial inequalities and economic disparities in COVID19 health\noutcomes amongst different regions in the United States. Our C19VI index\nindicates that 18.30% of the counties falls into very high vulnerability class,\n24.34% in high, 23.32% in moderate, 22.34% in low, and 11.68% in very low.\nFurthermore, C19VI reveals that 75.57% of racial minorities and 82.84% of\neconomically poor communities are very high or high COVID19 vulnerable regions.\nThe proposed approach of vulnerability modeling takes advantage of both the\nwell-established field of statistical analysis and the fast-evolving domain of\nmachine learning. C19VI provides an accurate and more reliable way to measure\ncounty level vulnerability in the United States. This index aims at helping\nemergency planners to develop more effective mitigation strategies especially\nfor the disproportionately impacted communities.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10808v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10623v5",
    "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time",
    "authors": [
      "Ferran Alet",
      "Maria Bauza",
      "Kenji Kawaguchi",
      "Nurullah Giray Kuru",
      "Tomas Lozano-Perez",
      "Leslie Pack Kaelbling"
    ],
    "author_ids": [],
    "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10623v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10622v7",
    "title": "Non-asymptotic oracle inequalities for the Lasso in high-dimensional mixture of experts",
    "authors": [
      "TrungTin Nguyen",
      "Hien D Nguyen",
      "Faicel Chamroukhi",
      "Geoffrey J McLachlan"
    ],
    "author_ids": [],
    "abstract": "We investigate the estimation properties of the mixture of experts (MoE)\nmodel in a high-dimensional setting, where the number of predictors is much\nlarger than the sample size, and for which the literature is particularly\nlacking in theoretical results. We consider the class of softmax-gated Gaussian\nMoE (SGMoE) models, defined as MoE models with softmax gating functions and\nGaussian experts, and focus on the theoretical properties of their\n$l_1$-regularized estimation via the Lasso. To the best of our knowledge, we\nare the first to investigate the $l_1$-regularization properties of SGMoE\nmodels from a non-asymptotic perspective, under the mildest assumptions, namely\nthe boundedness of the parameter space. We provide a lower bound on the\nregularization parameter of the Lasso penalty that ensures non-asymptotic\ntheoretical control of the Kullback--Leibler loss of the Lasso estimator for\nSGMoE models. Finally, we carry out a simulation study to empirically validate\nour theoretical findings.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "math.ST",
      "cs.AI",
      "cs.LG",
      "stat.ME",
      "stat.ML",
      "stat.TH",
      "62E17 (Primary) 62H12, 62H30 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10622v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10576v3",
    "title": "Ethical Machine Learning in Health Care",
    "authors": [
      "Irene Y. Chen",
      "Emma Pierson",
      "Sherri Rose",
      "Shalmali Joshi",
      "Kadija Ferryman",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "The use of machine learning (ML) in health care raises numerous ethical\nconcerns, especially as models can amplify existing health inequities. Here, we\noutline ethical considerations for equitable ML in the advancement of health\ncare. Specifically, we frame ethics of ML in health care through the lens of\nsocial justice. We describe ongoing efforts and outline challenges in a\nproposed pipeline of ethical ML in health, ranging from problem selection to\npost-deployment considerations. We close by summarizing recommendations to\naddress these challenges.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10576v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10385v4",
    "title": "A narrowing of AI research?",
    "authors": [
      "Joel Klinger",
      "Juan Mateos-Garcia",
      "Konstantinos Stathoulopoulos"
    ],
    "author_ids": [],
    "abstract": "The arrival of deep learning techniques able to infer patterns from large\ndatasets has dramatically improved the performance of Artificial Intelligence\n(AI) systems. Deep learning's rapid development and adoption, in great part led\nby large technology companies, has however created concerns about a premature\nnarrowing in the technological trajectory of AI research despite its\nweaknesses, which include lack of robustness, high environmental costs, and\npotentially unfair outcomes. We seek to improve the evidence base with a\nsemantic analysis of AI research in arXiv, a popular pre-prints database. We\nstudy the evolution of the thematic diversity of AI research, compare the\nthematic diversity of AI research in academia and the private sector and\nmeasure the influence of private companies in AI research through the citations\nthey receive and their collaborations with other institutions. Our results\nsuggest that diversity in AI research has stagnated in recent years, and that\nAI research involving the private sector tends to be less diverse and more\ninfluential than research in academia. We also find that private sector AI\nresearchers tend to specialise in data-hungry and computationally intensive\ndeep learning methods at the expense of research involving other AI methods,\nresearch that considers the societal and ethical implications of AI, and\napplications in sectors like health. Our results provide a rationale for policy\naction to prevent a premature narrowing of AI research that could constrain its\nsocietal benefits, but we note the informational, incentive and scale hurdles\nstanding in the way of such interventions.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10385v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10343v1",
    "title": "Gamma distribution-based sampling for imbalanced data",
    "authors": [
      "Firuz Kamalov",
      "Dmitry Denisov"
    ],
    "author_ids": [],
    "abstract": "Imbalanced class distribution is a common problem in a number of fields\nincluding medical diagnostics, fraud detection, and others. It causes bias in\nclassification algorithms leading to poor performance on the minority class\ndata. In this paper, we propose a novel method for balancing the class\ndistribution in data through intelligent resampling of the minority class\ninstances. The proposed method is based on generating new minority instances in\nthe neighborhood of the existing minority points via a gamma distribution. Our\nmethod offers a natural and coherent approach to balancing the data. We conduct\na comprehensive numerical analysis of the new sampling technique. The\nexperimental results show that the proposed method outperforms the existing\nstate-of-the-art methods for imbalanced data. Concretely, the new sampling\ntechnique produces the best results on 12 out of 24 real life as well as\nsynthetic datasets. For comparison, the SMOTE method achieves the top score on\nonly 1 dataset. We conclude that the new technique offers a simple yet\neffective sampling approach to balance data.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10343v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10277v1",
    "title": "Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application",
    "authors": [
      "Chris J. Kennedy",
      "Geoff Bacon",
      "Alexander Sahn",
      "Claudia von Vacano"
    ],
    "author_ids": [],
    "abstract": "We propose a general method for measuring complex variables on a continuous,\ninterval spectrum by combining supervised deep learning with the Constructing\nMeasures approach to faceted Rasch item response theory (IRT). We decompose the\ntarget construct, hate speech in our case, into multiple constituent components\nthat are labeled as ordinal survey items. Those survey responses are\ntransformed via IRT into a debiased, continuous outcome measure. Our method\nestimates the survey interpretation bias of the human labelers and eliminates\nthat influence on the generated continuous measure. We further estimate the\nresponse quality of each labeler using faceted IRT, allowing responses from\nlow-quality labelers to be removed.\n  Our faceted Rasch scaling procedure integrates naturally with a multitask\ndeep learning architecture for automated prediction on new data. The ratings on\nthe theorized components of the target outcome are used as supervised, ordinal\nvariables for the neural networks' internal concept learning. We test the use\nof an activation function (ordinal softmax) and loss function (ordinal\ncross-entropy) designed to exploit the structure of ordinal outcome variables.\nOur multitask architecture leads to a new form of model interpretation because\neach continuous prediction can be directly explained by the constituent\ncomponents in the penultimate layer.\n  We demonstrate this new method on a dataset of 50,000 social media comments\nsourced from YouTube, Twitter, and Reddit and labeled by 11,000 U.S.-based\nAmazon Mechanical Turk workers to measure a continuous spectrum from hate\nspeech to counterspeech. We evaluate Universal Sentence Encoders, BERT, and\nRoBERTa as language representation models for the comment text, and compare our\npredictive accuracy to Google Jigsaw's Perspective API models, showing\nsignificant improvement over this standard benchmark.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SI",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10277v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.11100v1",
    "title": "Engaging Teachers to Co-Design Integrated AI Curriculum for K-12 Classrooms",
    "authors": [
      "Jessica Van Brummelen",
      "Phoebe Lin"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) education is an increasingly popular topic area\nfor K-12 teachers. However, little research has investigated how AI education\ncan be designed to be more accessible to all learners. We organized co-design\nworkshops with 15 K-12 teachers to identify opportunities to integrate AI\neducation into core curriculum to leverage learners' interests. During the\nco-design workshops, teachers and researchers co-created lesson plans where AI\nconcepts were embedded into various core subjects. We found that K-12 teachers\nneed additional scaffolding in the curriculum to facilitate ethics and data\ndiscussions, and value supports for learner engagement, collaboration, and\nreflection. We identify opportunities for researchers and teachers to\ncollaborate to make AI education more accessible, and present an exemplar\nlesson plan that shows entry points for teaching AI in non-computing subjects.\nWe also reflect on co-designing with K-12 teachers in a remote setting.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "physics.ed-ph",
      "cs.CY",
      "K.3.2; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11100v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10238v1",
    "title": "Justifications for Goal-Directed Constraint Answer Set Programming",
    "authors": [
      "Joaquín Arias",
      "Manuel Carro",
      "Zhuo Chen",
      "Gopal Gupta"
    ],
    "author_ids": [],
    "abstract": "Ethical and legal concerns make it necessary for programs that may directly\ninfluence the life of people (via, e.g., legal or health counseling) to justify\nin human-understandable terms the advice given. Answer Set Programming has a\nrich semantics that makes it possible to very concisely express complex\nknowledge. However, justifying why an answer is a consequence from an ASP\nprogram may be non-trivial -- even more so when the user is an expert in a\ngiven domain, but not necessarily knowledgeable in ASP. Most ASP systems\ngenerate answers using SAT-solving procedures on ground rules that do not match\nhow humans perceive reasoning. We propose using s(CASP), a query-driven,\ntop-down execution model for predicate ASP with constraints to generate\njustification trees of (constrained) answer sets. The operational semantics of\ns(CASP) relies on backward chaining, which is intuitive to follow and lends\nitself to generating explanations that are easier to translate into natural\nlanguage. We show how s(CASP) provides minimal justifications for, among\nothers, relevant examples proposed in the literature, both as search trees but,\nmore importantly, as explanations in natural language. We validate our design\nwith real ASP applications and evaluate the cost of generating s(CASP)\njustification trees.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LO",
      "cs.PL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10238v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.11186v1",
    "title": "Logic Programming and Machine Ethics",
    "authors": [
      "Abeer Dyoub",
      "Stefania Costantini",
      "Francesca A. Lisi"
    ],
    "author_ids": [],
    "abstract": "Transparency is a key requirement for ethical machines. Verified ethical\nbehavior is not enough to establish justified trust in autonomous intelligent\nagents: it needs to be supported by the ability to explain decisions. Logic\nProgramming (LP) has a great potential for developing such perspective ethical\nsystems, as in fact logic rules are easily comprehensible by humans.\nFurthermore, LP is able to model causality, which is crucial for ethical\ndecision making.",
    "published_date": "2020-09-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.LO",
      "D.1.6; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.11186v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.10194v2",
    "title": "Dark Patterns and the Legal Requirements of Consent Banners: An Interaction Criticism Perspective",
    "authors": [
      "Colin M. Gray",
      "Cristiana Santos",
      "Nataliia Bielova",
      "Michael Toth",
      "Damian Clifford"
    ],
    "author_ids": [],
    "abstract": "User engagement with data privacy and security through consent banners has\nbecome a ubiquitous part of interacting with internet services. While previous\nwork has addressed consent banners from either interaction design, legal, and\nethics-focused perspectives, little research addresses the connections among\nmultiple disciplinary approaches, including tensions and opportunities that\ntranscend disciplinary boundaries. In this paper, we draw together perspectives\nand commentary from HCI, design, privacy and data protection, and legal\nresearch communities, using the language and strategies of \"dark patterns\" to\nperform an interaction criticism reading of three different types of consent\nbanners. Our analysis builds upon designer, interface, user, and social context\nlenses to raise tensions and synergies that arise together in complex,\ncontingent, and conflicting ways in the act of designing consent banners. We\nconclude with opportunities for transdisciplinary dialogue across legal,\nethical, computer science, and interactive systems scholarship to translate\nmatters of ethical concern into public policy.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10194v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.10187v1",
    "title": "Security, Privacy and Ethical Concerns of IoT Implementations in Hospitality Domain",
    "authors": [
      "Suat Mercan",
      "Kemal Akkaya",
      "Lisa Cain",
      "John Thomas"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things (IoT) has been on the rise in the last decade as it\nfinds applications in various domains. Hospitality is one of the pioneer\nsectors that has adopted this technology to create novel services such as smart\nhotel rooms, personalized services etc. Hotels, restaurants, theme parks, and\ncruise ships are some specific application areas to improve customer\nsatisfaction by creating an intense interactive environment and data collection\nwith the use of appropriate sensors and actuators. However, applying IoT\nsolutions in the hospitality environment has some unique challenges such as\neasy physical access to devices. In addition, due to the very nature of these\ndomains, the customers are at the epicenter of these IoT technologies that\nresult in a massive amount of data collection from them. Such data and its\nmanagement along with business purposes also raises new concerns regarding\nprivacy and ethical considerations. Therefore, this paper surveys and analyzes\nsecurity, privacy and ethical issues regarding the utilization of IoT devices\nby focusing on the hospitality industry specifically. We explore some exemplary\nuses, cases, potential problems and solutions in order to contribute to better\nunderstanding and guiding the business operators in this sector.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10187v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.10127v1",
    "title": "On uncertainty inequalities related to subcube partitions and additive energy",
    "authors": [
      "Norbert Hegyvari"
    ],
    "author_ids": [],
    "abstract": "The additive energy plays a central role in combinatorial number theory. We\nshow an uncertainty inequality which indicates how the additive energy of\nsupport of a Boolean function, its degree and subcube partition are related.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10127v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.10050v2",
    "title": "Measuring justice in machine learning",
    "authors": [
      "Alan Lundgard"
    ],
    "author_ids": [],
    "abstract": "How can we build more just machine learning systems? To answer this question,\nwe need to know both what justice is and how to tell whether one system is more\nor less just than another. That is, we need both a definition and a measure of\njustice. Theories of distributive justice hold that justice can be measured (in\npart) in terms of the fair distribution of benefits and burdens across people\nin society. Recently, the field known as fair machine learning has turned to\nJohn Rawls's theory of distributive justice for inspiration and\noperationalization. However, philosophers known as capability theorists have\nlong argued that Rawls's theory uses the wrong measure of justice, thereby\nencoding biases against people with disabilities. If these theorists are right,\nis it possible to operationalize Rawls's theory in machine learning systems\nwithout also encoding its biases? In this paper, I draw on examples from fair\nmachine learning to suggest that the answer to this question is no: the\ncapability theorists' arguments against Rawls's theory carry over into machine\nlearning systems. But capability theorists don't only argue that Rawls's theory\nuses the wrong measure, they also offer an alternative measure. Which measure\nof justice is right? And has fair machine learning been using the wrong one?",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "I.2.0; K.4.1; J.1.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10050v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09990v2",
    "title": "Super-teams or fair leagues? Parity policies by powerful regulators don't prevent capture",
    "authors": [
      "Adam Sawyer",
      "Seth Frey"
    ],
    "author_ids": [],
    "abstract": "Much of modern society is founded on orchestrating institutions that produce\nsocial goods by fostering motivated teams, pitting them against each other, and\ndistributing the fruits of the arms races that ensue. However, even when the\n\"market maker\" is willing and able to maintain parity between teams, it may\nfail to maintain a level playing field, as some teams acquire enough advantage\nwithin the system to gain influence over it and institutionalize their\nadvantage. Using outcomes of over 60,000 games from four professional\nbasketball leagues and more than 100 years' worth of seasons, we compute the\nevolving rate of transitivity violations (A>B, B>C, but C>A) to measure the\nability of leagues to maintain parity between teams, and support the efficient\ngeneration and distribution of innovation. Comparing against a baseline of\nrandomly permuted outcomes, we find that basketball has become less competitive\nover time, suggesting that teams diverge in performance, and reflecting a\npossible failure of market makers to tame their overpowered teams. Our results\nsuggest that rich-get-richer dynamics are so pernicious that they can even\nemerge under the watch of a powerful administrator that is motivated to prevent\nthem.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09990v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2010.07103v2",
    "title": "Breaking Symmetries of the Reservoir Equations in Echo State Networks",
    "authors": [
      "Joschka Herteux",
      "Christoph Räth"
    ],
    "author_ids": [],
    "abstract": "Reservoir computing has repeatedly been shown to be extremely successful in\nthe prediction of nonlinear time-series. However, there is no complete\nunderstanding of the proper design of a reservoir yet. We find that the\nsimplest popular setup has a harmful symmetry, which leads to the prediction of\nwhat we call mirror-attractor. We prove this analytically. Similar problems can\narise in a general context, and we use them to explain the success or failure\nof some designs. The symmetry is a direct consequence of the hyperbolic tangent\nactivation function. Further, four ways to break the symmetry are compared\nnumerically: A bias in the output, a shift in the input, a quadratic term in\nthe readout, and a mixture of even and odd activation functions. Firstly, we\ntest their susceptibility to the mirror-attractor. Secondly, we evaluate their\nperformance on the task of predicting Lorenz data with the mean shifted to\nzero. The short-time prediction is measured with the forecast horizon while the\nlargest Lyapunov exponent and the correlation dimension are used to represent\nthe climate. Finally, the same analysis is repeated on a combined dataset of\nthe Lorenz attractor and the Halvorsen attractor, which we designed to reveal\npotential problems with symmetry. We find that all methods except the output\nbias are able to fully break the symmetry with input shift and quadratic\nreadout performing the best overall.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "physics.data-an",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07103v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09975v1",
    "title": "Grey-Box Learning of Register Automata",
    "authors": [
      "Bharat Garhewal",
      "Frits Vaandrager",
      "Falk Howar",
      "Timo Schrijvers",
      "Toon Lenaerts",
      "Rob Smits"
    ],
    "author_ids": [],
    "abstract": "Model learning (a.k.a. active automata learning) is a highly effective\ntechnique for obtaining black-box finite state models of software components.\nThus far, generalisation to infinite state systems with inputs/outputs that\ncarry data parameters has been challenging. Existing model learning tools for\ninfinite state systems face scalability problems and can only be applied to\nrestricted classes of systems (register automata with equality/inequality). In\nthis article, we show how we can boost the performance of model learning\ntechniques by extracting the constraints on input and output parameters from a\nrun, and making this grey-box information available to the learner. More\nspecifically, we provide new implementations of the tree oracle and equivalence\noracle from RALib, which use the derived constraints. We extract the\nconstraints from runs of Python programs using an existing tainting library for\nPython, and compare our grey-box version of RALib with the existing black-box\nversion on several benchmarks, including some data structures from Python's\nstandard library. Our proof-of-principle implementation results in almost two\norders of magnitude improvement in terms of numbers of inputs sent to the\nsoftware system. Our approach, which can be generalised to richer model\nclasses, also enables RALib to learn models that are out of reach of black-box\ntechniques, such as combination locks.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.FL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09975v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09961v4",
    "title": "Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference",
    "authors": [
      "Galen Weld",
      "Peter West",
      "Maria Glenski",
      "David Arbour",
      "Ryan Rossi",
      "Tim Althoff"
    ],
    "author_ids": [],
    "abstract": "Causal inference studies using textual social media data can provide\nactionable insights on human behavior. Making accurate causal inferences with\ntext requires controlling for confounding which could otherwise impart bias.\nRecently, many different methods for adjusting for confounders have been\nproposed, and we show that these existing methods disagree with one another on\ntwo datasets inspired by previous social media studies. Evaluating causal\nmethods is challenging, as ground truth counterfactuals are almost never\navailable. Presently, no empirical evaluation framework for causal methods\nusing text exists, and as such, practitioners must select their methods without\nguidance. We contribute the first such framework, which consists of five tasks\ndrawn from real world studies. Our framework enables the evaluation of any\ncasual inference method using text. Across 648 experiments and two datasets, we\nevaluate every commonly used causal inference method and identify their\nstrengths and weaknesses to inform social media researchers seeking to use such\nmethods, and guide future improvements. We make all tasks, data, and models\npublic to inform applications and encourage additional research.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09961v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09959v1",
    "title": "Domain-Embeddings Based DGA Detection with Incremental Training Method",
    "authors": [
      "Xin Fang",
      "Xiaoqing Sun",
      "Jiahai Yang",
      "Xinran Liu"
    ],
    "author_ids": [],
    "abstract": "DGA-based botnet, which uses Domain Generation Algorithms (DGAs) to evade\nsupervision, has become a part of the most destructive threats to network\nsecurity. Over the past decades, a wealth of defense mechanisms focusing on\ndomain features have emerged to address the problem. Nonetheless, DGA detection\nremains a daunting and challenging task due to the big data nature of Internet\ntraffic and the potential fact that the linguistic features extracted only from\nthe domain names are insufficient and the enemies could easily forge them to\ndisturb detection. In this paper, we propose a novel DGA detection system which\nemploys an incremental word-embeddings method to capture the interactions\nbetween end hosts and domains, characterize time-series patterns of DNS queries\nfor each IP address and therefore explore temporal similarities between\ndomains. We carefully modify the Word2Vec algorithm and leverage it to\nautomatically learn dynamic and discriminative feature representations for over\n1.9 million domains, and develop an simple classifier for distinguishing\nmalicious domains from the benign. Given the ability to identify temporal\npatterns of domains and update models incrementally, the proposed scheme makes\nthe progress towards adapting to the changing and evolving strategies of DGA\ndomains. Our system is evaluated and compared with the state-of-art system\nFANCI and two deep-learning methods CNN and LSTM, with data from a large\nuniversity's network named TUNET. The results suggest that our system\noutperforms the strong competitors by a large margin on multiple metrics and\nmeanwhile achieves a remarkable speed-up on model updating.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09918v1",
    "title": "Beyond Identity: What Information Is Stored in Biometric Face Templates?",
    "authors": [
      "Philipp Terhörst",
      "Daniel Fährmann",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "author_ids": [],
    "abstract": "Deeply-learned face representations enable the success of current face\nrecognition systems. Despite the ability of these representations to encode the\nidentity of an individual, recent works have shown that more information is\nstored within, such as demographics, image characteristics, and social traits.\nThis threatens the user's privacy, since for many applications these templates\nare expected to be solely used for recognition purposes. Knowing the encoded\ninformation in face templates helps to develop bias-mitigating and\nprivacy-preserving face recognition technologies. This work aims to support the\ndevelopment of these two branches by analysing face templates regarding 113\nattributes. Experiments were conducted on two publicly available face\nembeddings. For evaluating the predictability of the attributes, we trained a\nmassive attribute classifier that is additionally able to accurately state its\nprediction confidence. This allows us to make more sophisticated statements\nabout the attribute predictability. The results demonstrate that up to 74\nattributes can be accurately predicted from face templates. Especially\nnon-permanent attributes, such as age, hairstyles, haircolors, beards, and\nvarious accessories, found to be easily-predictable. Since face recognition\nsystems aim to be robust against these variations, future research might build\non this work to develop more understandable privacy preserving solutions and\nbuild robust and fair face templates.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09918v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09907v1",
    "title": "Optimal Stable Nonlinear Approximation",
    "authors": [
      "Albert Cohen",
      "Ronald DeVore",
      "Guergana Petrova",
      "Przemyslaw Wojtaszczyk"
    ],
    "author_ids": [],
    "abstract": "While it is well known that nonlinear methods of approximation can often\nperform dramatically better than linear methods, there are still questions on\nhow to measure the optimal performance possible for such methods. This paper\nstudies nonlinear methods of approximation that are compatible with numerical\nimplementation in that they are required to be numerically stable. A measure of\noptimal performance, called {\\em stable manifold widths}, for approximating a\nmodel class $K$ in a Banach space $X$ by stable manifold methods is introduced.\nFundamental inequalities between these stable manifold widths and the entropy\nof $K$ are established. The effects of requiring stability in the settings of\ndeep learning and compressed sensing are discussed.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09907v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09780v4",
    "title": "Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images",
    "authors": [
      "Lucas O. Teixeira",
      "Rodolfo M. Pereira",
      "Diego Bertolini",
      "Luiz S. Oliveira",
      "Loris Nanni",
      "George D. C. Cavalcanti",
      "Yandre M. G. Costa"
    ],
    "author_ids": [],
    "abstract": "COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging\nexams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,\nand uses less radiation. Here, we demonstrate the impact of lung segmentation\nin COVID-19 identification using CXR images and evaluate which contents of the\nimage influenced the most. Semantic segmentation was performed using a U-Net\nCNN architecture, and the classification using three CNN architectures (VGG,\nResNet, and Inception). Explainable Artificial Intelligence techniques were\nemployed to estimate the impact of segmentation. A three-classes database was\ncomposed: lung opacity (pneumonia), COVID-19, and normal. We assessed the\nimpact of creating a CXR image database from different sources, and the\nCOVID-19 generalization from one source to another. The segmentation achieved a\nJaccard distance of 0.034 and a Dice coefficient of 0.982. The classification\nusing segmented images achieved an F1-Score of 0.88 for the multi-class setup,\nand 0.83 for COVID-19 identification. In the cross-dataset scenario, we\nobtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for\nCOVID-19 identification using segmented images. Experiments support the\nconclusion that even after segmentation, there is a strong bias introduced by\nunderlying factors from different sources.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09780v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09672v2",
    "title": "Alleviating the Inequality of Attention Heads for Neural Machine Translation",
    "authors": [
      "Zewei Sun",
      "Shujian Huang",
      "Xin-Yu Dai",
      "Jiajun Chen"
    ],
    "author_ids": [],
    "abstract": "Recent studies show that the attention heads in Transformer are not equal. We\nrelate this phenomenon to the imbalance training of multi-head attention and\nthe model dependence on specific heads. To tackle this problem, we propose a\nsimple masking method: HeadMask, in two specific ways. Experiments show that\ntranslation improvements are achieved on multiple language pairs. Subsequent\nempirical analyses also support our assumption and confirm the effectiveness of\nthe method.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09672v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09556v1",
    "title": "Open-set Short Utterance Forensic Speaker Verification using Teacher-Student Network with Explicit Inductive Bias",
    "authors": [
      "Mufan Sang",
      "Wei Xia",
      "John H. L. Hansen"
    ],
    "author_ids": [],
    "abstract": "In forensic applications, it is very common that only small naturalistic\ndatasets consisting of short utterances in complex or unknown acoustic\nenvironments are available. In this study, we propose a pipeline solution to\nimprove speaker verification on a small actual forensic field dataset. By\nleveraging large-scale out-of-domain datasets, a knowledge distillation based\nobjective function is proposed for teacher-student learning, which is applied\nfor short utterance forensic speaker verification. The objective function\ncollectively considers speaker classification loss, Kullback-Leibler\ndivergence, and similarity of embeddings. In order to advance the trained deep\nspeaker embedding network to be robust for a small target dataset, we introduce\na novel strategy to fine-tune the pre-trained student model towards a forensic\ntarget domain by utilizing the model as a finetuning start point and a\nreference in regularization. The proposed approaches are evaluated on the\n1st48-UTD forensic corpus, a newly established naturalistic dataset of actual\nhomicide investigations consisting of short utterances recorded in uncontrolled\nconditions. We show that the proposed objective function can efficiently\nimprove the performance of teacher-student learning on short utterances and\nthat our fine-tuning strategy outperforms the commonly used weight decay method\nby providing an explicit inductive bias towards the pre-trained model.",
    "published_date": "2020-09-21T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09556v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09535v1",
    "title": "Stochastic Gradient Langevin Dynamics Algorithms with Adaptive Drifts",
    "authors": [
      "Sehwan Kim",
      "Qifan Song",
      "Faming Liang"
    ],
    "author_ids": [],
    "abstract": "Bayesian deep learning offers a principled way to address many issues\nconcerning safety of artificial intelligence (AI), such as model\nuncertainty,model interpretability, and prediction bias. However, due to the\nlack of efficient Monte Carlo algorithms for sampling from the posterior of\ndeep neural networks (DNNs), Bayesian deep learning has not yet powered our AI\nsystem. We propose a class of adaptive stochastic gradient Markov chain Monte\nCarlo (SGMCMC) algorithms, where the drift function is biased to enhance escape\nfrom saddle points and the bias is adaptively adjusted according to the\ngradient of past samples. We establish the convergence of the proposed\nalgorithms under mild conditions, and demonstrate via numerical examples that\nthe proposed algorithms can significantly outperform the existing SGMCMC\nalgorithms, such as stochastic gradient Langevin dynamics (SGLD), stochastic\ngradient Hamiltonian Monte Carlo (SGHMC) and preconditioned SGLD, in both\nsimulation and optimization tasks.",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09535v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09524v1",
    "title": "Two and Three-Party Digital Goods Auctions: Scalable Privacy Analysis",
    "authors": [
      "Patrick Ah-Fat",
      "Michael Huth"
    ],
    "author_ids": [],
    "abstract": "A digital goods auction is a type of auction where potential buyers bid the\nmaximal price that they are willing to pay for a certain item, which a seller\ncan produce at a negligible cost and in unlimited quantity. To maximise her\nbenefits, the aim for the seller is to find the optimal sales price, which\nevery buyer whose bid is not lower will pay. For fairness and privacy purposes,\nbuyers may be concerned about protecting the confidentiality of their bids.\nSecure Multi-Party Computation is a domain of Cryptography that would allow the\nseller to compute the optimal sales price while guaranteeing that the bids\nremain secret. Paradoxically, as a function of the buyers' bids, the sales\nprice inevitably reveals some private information. Generic frameworks and\nentropy-based techniques based on Quantitative Information Flow have been\ndeveloped in order to quantify and restrict those leakages. Due to their\ncombinatorial nature, these techniques do not scale to large input spaces. In\nthis work, we aim at scaling those privacy analyses to large input spaces in\nthe particular case of digital goods auctions. We derive closed-form formulas\nfor the posterior min-entropy of private inputs in two and three-party\nauctions, which enables us to effectively quantify the information leaks for\narbitrarily large input spaces. We also provide supportive experimental\nevidence that enables us to formulate a conjecture that would allow us to\nextend our results to any number of parties.",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09524v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09467v1",
    "title": "Addressing reward bias in Adversarial Imitation Learning with neutral reward functions",
    "authors": [
      "Rohit Jena",
      "Siddharth Agrawal",
      "Katia Sycara"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Imitation Learning suffers from the fundamental\nproblem of reward bias stemming from the choice of reward functions used in the\nalgorithm. Different types of biases also affect different types of\nenvironments - which are broadly divided into survival and task-based\nenvironments. We provide a theoretical sketch of why existing reward functions\nwould fail in imitation learning scenarios in task based environments with\nmultiple terminal states. We also propose a new reward function for GAIL which\noutperforms existing GAIL methods on task based environments with single and\nmultiple terminal states and effectively overcomes both survival and\ntermination bias.",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09467v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09435v4",
    "title": "Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation",
    "authors": [
      "Francisco Vargas",
      "Ryan Cotterell"
    ],
    "author_ids": [],
    "abstract": "Bolukbasi et al. (2016) presents one of the first gender bias mitigation\ntechniques for word representations. Their method takes pre-trained word\nrepresentations as input and attempts to isolate a linear subspace that\ncaptures most of the gender bias in the representations. As judged by an\nanalogical evaluation task, their method virtually eliminates gender bias in\nthe representations. However, an implicit and untested assumption of their\nmethod is that the bias subspace is actually linear. In this work, we\ngeneralize their method to a kernelized, nonlinear version. We take inspiration\nfrom kernel principal component analysis and derive a nonlinear bias isolation\ntechnique. We discuss and overcome some of the practical drawbacks of our\nmethod for non-linear gender bias mitigation in word representations and\nanalyze empirically whether the bias subspace is actually linear. Our analysis\nshows that gender bias is in fact well captured by a linear subspace,\njustifying the assumption of Bolukbasi et al. (2016).",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09435v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09422v1",
    "title": "Epidemic mitigation by statistical inference from contact tracing data",
    "authors": [
      "Antoine Baker",
      "Indaco Biazzo",
      "Alfredo Braunstein",
      "Giovanni Catania",
      "Luca Dall'Asta",
      "Alessandro Ingrosso",
      "Florent Krzakala",
      "Fabio Mazza",
      "Marc Mézard",
      "Anna Paola Muntoni",
      "Maria Refinetti",
      "Stefano Sarao Mannelli",
      "Lenka Zdeborová"
    ],
    "author_ids": [],
    "abstract": "Contact-tracing is an essential tool in order to mitigate the impact of\npandemic such as the COVID-19. In order to achieve efficient and scalable\ncontact-tracing in real time, digital devices can play an important role. While\na lot of attention has been paid to analyzing the privacy and ethical risks of\nthe associated mobile applications, so far much less research has been devoted\nto optimizing their performance and assessing their impact on the mitigation of\nthe epidemic. We develop Bayesian inference methods to estimate the risk that\nan individual is infected. This inference is based on the list of his recent\ncontacts and their own risk levels, as well as personal information such as\nresults of tests or presence of syndromes. We propose to use probabilistic risk\nestimation in order to optimize testing and quarantining strategies for the\ncontrol of an epidemic. Our results show that in some range of epidemic\nspreading (typically when the manual tracing of all contacts of infected people\nbecomes practically impossible, but before the fraction of infected people\nreaches the scale where a lock-down becomes unavoidable), this inference of\nindividuals at risk could be an efficient way to mitigate the epidemic. Our\napproaches translate into fully distributed algorithms that only require\ncommunication between individuals who have recently been in contact. Such\ncommunication may be encrypted and anonymized and thus compatible with privacy\npreserving standards. We conclude that probabilistic risk estimation is capable\nto enhance performance of digital contact tracing and should be considered in\nthe currently developed mobile applications.",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.PE",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.LG",
      "G.3; G.4; I.2.11; J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09422v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09351v1",
    "title": "Counteracting Inequality in Markets via Convex Pricing",
    "authors": [
      "Ashish Goel",
      "Benjamin Plaut"
    ],
    "author_ids": [],
    "abstract": "We study market mechanisms for allocating divisible goods to competing agents\nwith quasilinear utilities. For \\emph{linear} pricing (i.e., the cost of a good\nis proportional to the quantity purchased), the First Welfare Theorem states\nthat Walrasian equilibria maximize the sum of agent valuations. This ensures\nefficiency, but can lead to extreme inequality across individuals. Many\nreal-world markets -- especially for water -- use \\emph{convex} pricing\ninstead, often known as increasing block tariffs (IBTs). IBTs are thought to\npromote equality, but there is a dearth of theoretical support for this claim.\n  In this paper, we study a simple convex pricing rule and show that the\nresulting equilibria are guaranteed to maximize a CES welfare function.\nFurthermore, a parameter of the pricing rule directly determines which CES\nwelfare function is implemented; by tweaking this parameter, the social planner\ncan precisely control the tradeoff between equality and efficiency. Our result\nholds for any valuations that are homogeneous, differentiable, and concave. We\nalso give an iterative algorithm for computing these pricing rules, derive a\ntruthful mechanism for the case of a single good, and discuss Sybil attacks.",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09351v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09336v1",
    "title": "Almost Envy-free Repeated Matching in Two-sided Markets",
    "authors": [
      "Sreenivas Gollapudi",
      "Kostas Kollias",
      "Benjamin Plaut"
    ],
    "author_ids": [],
    "abstract": "A two-sided market consists of two sets of agents, each of whom have\npreferences over the other (Airbnb, Upwork, Lyft, Uber, etc.). We propose and\nanalyze a repeated matching problem, where some set of matches occur on each\ntime step, and our goal is to ensure fairness with respect to the cumulative\nallocations over an infinite time horizon. Our main result is a polynomial-time\nalgorithm for additive, symmetric (v_i(j) = v_j(i)), and binary (v_i(j) \\in\n\\{a,1\\}) valuations that both (1) guarantees \"envy-freeness up to a single\nmatch\" (EF1) and (2) selects a maximum weight matching on each time step. Thus\nfor this class of valuations, fairness can be achieved without sacrificing\neconomic efficiency. This result holds even for \"dynamic valuations\", i.e.,\nvaluations that change over time. Although symmetry is a strong assumption, we\nshow that this result cannot be extended to asymmetric binary valuations: (1)\nand (2) together are impossible even when valuations do not change over time,\nand for dynamic valuations, even (1) alone is impossible. To our knowledge,\nthis is the first analysis of envy-freeness in a repeated matching setting.",
    "published_date": "2020-09-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09336v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09247v2",
    "title": "Bias Field Poses a Threat to DNN-based X-Ray Recognition",
    "authors": [
      "Binyu Tian",
      "Qing Guo",
      "Felix Juefei-Xu",
      "Wen Le Chan",
      "Yupeng Cheng",
      "Xiaohong Li",
      "Xiaofei Xie",
      "Shengchao Qin"
    ],
    "author_ids": [],
    "abstract": "The chest X-ray plays a key role in screening and diagnosis of many lung\ndiseases including the COVID-19. More recently, many works construct deep\nneural networks (DNNs) for chest X-ray images to realize automated and\nefficient diagnosis of lung diseases. However, bias field caused by the\nimproper medical image acquisition process widely exists in the chest X-ray\nimages while the robustness of DNNs to the bias field is rarely explored, which\ndefinitely poses a threat to the X-ray-based automated diagnosis system. In\nthis paper, we study this problem based on the recent adversarial attack and\npropose a brand new attack, i.e., the adversarial bias field attack where the\nbias field instead of the additive noise works as the adversarial perturbations\nfor fooling the DNNs. This novel attack posts a key problem: how to locally\ntune the bias field to realize high attack success rate while maintaining its\nspatial smoothness to guarantee high realisticity. These two goals contradict\neach other and thus has made the attack significantly challenging. To overcome\nthis challenge, we propose the adversarial-smooth bias field attack that can\nlocally tune the bias field with joint smooth & adversarial constraints. As a\nresult, the adversarial X-ray images can not only fool the DNNs effectively but\nalso retain very high level of realisticity. We validate our method on real\nchest X-ray datasets with powerful DNNs, e.g., ResNet50, DenseNet121, and\nMobileNet, and show different properties to the state-of-the-art attacks in\nboth image realisticity and attack transferability. Our method reveals the\npotential threat to the DNN-based X-ray automated diagnosis and can definitely\nbenefit the development of bias-field-robust automated diagnosis system.",
    "published_date": "2020-09-19T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09247v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09174v1",
    "title": "Aggressive Language Detection with Joint Text Normalization via Adversarial Multi-task Learning",
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Donghong Ji"
    ],
    "author_ids": [],
    "abstract": "Aggressive language detection (ALD), detecting the abusive and offensive\nlanguage in texts, is one of the crucial applications in NLP community. Most\nexisting works treat ALD as regular classification with neural models, while\nignoring the inherent conflicts of social media text that they are quite\nunnormalized and irregular. In this work, we target improving the ALD by\njointly performing text normalization (TN), via an adversarial multi-task\nlearning framework. The private encoders for ALD and TN focus on the\ntask-specific features retrieving, respectively, and the shared encoder learns\nthe underlying common features over two tasks. During adversarial training, a\ntask discriminator distinguishes the separate learning of ALD or TN.\nExperimental results on four ALD datasets show that our model outperforms all\nbaselines under differing settings by large margins, demonstrating the\nnecessity of joint learning the TN with ALD. Further analysis is conducted for\na better understanding of our method.",
    "published_date": "2020-09-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09174v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09049v1",
    "title": "Examining the Impact of Algorithm Awareness on Wikidata's Recommender System Recoin",
    "authors": [
      "Jesse Josua Benjamin",
      "Claudia Müller-Birn",
      "Simon Razniewski"
    ],
    "author_ids": [],
    "abstract": "The global infrastructure of the Web, designed as an open and transparent\nsystem, has a significant impact on our society. However, algorithmic systems\nof corporate entities that neglect those principles increasingly populated the\nWeb. Typical representatives of these algorithmic systems are recommender\nsystems that influence our society both on a scale of global politics and\nduring mundane shopping decisions. Recently, such recommender systems have come\nunder critique for how they may strengthen existing or even generate new kinds\nof biases. To this end, designers and engineers are increasingly urged to make\nthe functioning and purpose of recommender systems more transparent. Our\nresearch relates to the discourse of algorithm awareness, that reconsiders the\nrole of algorithm visibility in interface design. We conducted online\nexperiments with 105 participants using MTurk for the recommender system\nRecoin, a gadget for Wikidata. In these experiments, we presented users with\none of a set of three different designs of Recoin's user interface, each of\nthem exhibiting a varying degree of explainability and interactivity. Our\nfindings include a positive correlation between comprehension of and trust in\nan algorithmic system in our interactive redesign. However, our results are not\nconclusive yet, and suggest that the measures of comprehension, fairness,\naccuracy and trust are not yet exhaustive for the empirical study of algorithm\nawareness. Our qualitative insights provide a first indication for further\nmeasures. Our study participants, for example, were less concerned with the\ndetails of understanding an algorithmic calculation than with who or what is\njudging the result of the algorithm.",
    "published_date": "2020-09-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09049v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09031v2",
    "title": "Group Fairness by Probabilistic Modeling with Latent Fair Decisions",
    "authors": [
      "YooJung Choi",
      "Meihua Dang",
      "Guy Van den Broeck"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems are increasingly being used to make impactful\ndecisions such as loan applications and criminal justice risk assessments, and\nas such, ensuring fairness of these systems is critical. This is often\nchallenging as the labels in the data are biased. This paper studies learning\nfair probability distributions from biased data by explicitly modeling a latent\nvariable that represents a hidden, unbiased label. In particular, we aim to\nachieve demographic parity by enforcing certain independencies in the learned\nmodel. We also show that group fairness guarantees are meaningful only if the\ndistribution used to provide those guarantees indeed captures the real-world\ndata. In order to closely model the data distribution, we employ probabilistic\ncircuits, an expressive and tractable probabilistic model, and propose an\nalgorithm to learn them from incomplete data. We evaluate our approach on a\nsynthetic dataset in which observed labels indeed come from fair labels but\nwith added bias, and demonstrate that the fair labels are successfully\nretrieved. Moreover, we show on real-world datasets that our approach not only\nis a better model than existing methods of how the data was generated but also\nachieves competitive accuracy.",
    "published_date": "2020-09-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09031v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09026v3",
    "title": "Adversarial Robustness through Bias Variance Decomposition: A New Perspective for Federated Learning",
    "authors": [
      "Yao Zhou",
      "Jun Wu",
      "Haixun Wang",
      "Jingrui He"
    ],
    "author_ids": [],
    "abstract": "Federated learning learns a neural network model by aggregating the knowledge\nfrom a group of distributed clients under the privacy-preserving constraint. In\nthis work, we show that this paradigm might inherit the adversarial\nvulnerability of the centralized neural network, i.e., it has deteriorated\nperformance on adversarial examples when the model is deployed. This is even\nmore alarming when federated learning paradigm is designed to approximate the\nupdating behavior of a centralized neural network. To solve this problem, we\npropose an adversarially robust federated learning framework, named Fed_BVA,\nwith improved server and client update mechanisms. This is motivated by our\nobservation that the generalization error in federated learning can be\nnaturally decomposed into the bias and variance triggered by multiple clients'\npredictions. Thus, we propose to generate the adversarial examples via\nmaximizing the bias and variance during server update, and learn the\nadversarially robust model updates with those examples during client update. As\na result, an adversarially robust neural network can be aggregated from these\nimproved local clients' model updates. The experiments are conducted on\nmultiple benchmark data sets using several prevalent neural network models, and\nthe empirical results show that our framework is robust against white-box and\nblack-box adversarial corruptions under both IID and non-IID settings.",
    "published_date": "2020-09-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09026v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08704v1",
    "title": "Learning Emotional-Blinded Face Representations",
    "authors": [
      "Alejandro Peña",
      "Julian Fierrez",
      "Agata Lapedriza",
      "Aythami Morales"
    ],
    "author_ids": [],
    "abstract": "We propose two face representations that are blind to facial expressions\nassociated to emotional responses. This work is in part motivated by new\ninternational regulations for personal data protection, which enforce data\ncontrollers to protect any kind of sensitive information involved in automatic\nprocesses. The advances in Affective Computing have contributed to improve\nhuman-machine interfaces but, at the same time, the capacity to monitorize\nemotional responses triggers potential risks for humans, both in terms of\nfairness and privacy. We propose two different methods to learn these\nexpression-blinded facial features. We show that it is possible to eliminate\ninformation related to emotion recognition tasks, while the performance of\nsubject verification, gender recognition, and ethnicity classification are just\nslightly affected. We also present an application to train fairer classifiers\nin a case study of attractiveness classification with respect to a protected\nfacial expression attribute. The results demonstrate that it is possible to\nreduce emotional information in the face representation while retaining\ncompetitive performance in other face-based artificial intelligence tasks.",
    "published_date": "2020-09-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08704v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08687v3",
    "title": "Chemical Property Prediction Under Experimental Biases",
    "authors": [
      "Yang Liu",
      "Hisashi Kashima"
    ],
    "author_ids": [],
    "abstract": "Predicting the chemical properties of compounds is crucial in discovering\nnovel materials and drugs with specific desired characteristics. Recent\nsignificant advances in machine learning technologies have enabled automatic\npredictive modeling from past experimental data reported in the literature.\nHowever, these datasets are often biased because of various reasons, such as\nexperimental plans and publication decisions, and the prediction models trained\nusing such biased datasets often suffer from over-fitting to the biased\ndistributions and perform poorly on subsequent uses. Hence, this study focused\non mitigating bias in the experimental datasets. We adopted two techniques from\ncausal inference combined with graph neural networks that can represent\nmolecular structures. The experimental results in four possible bias scenarios\nindicated that the inverse propensity scoring-based method and the\ncounter-factual regression-based method made solid improvements.",
    "published_date": "2020-09-18T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08687v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08632v3",
    "title": "Approximately Socially-Optimal Decentralized Coalition Formation with Application to P2P Energy Sharing",
    "authors": [
      "Sid Chi-Kin Chau",
      "Khaled Elbassioni",
      "Yue Zhou"
    ],
    "author_ids": [],
    "abstract": "The paradigm of P2P (peer-to-peer) economy has emerged in diverse areas. \"P2P\nenergy sharing\" is a new form of P2P economy in the energy sector, which allows\nusers to establish longer-term sharing arrangements of their local energy\nresources (e.g., rooftop PVs, home batteries) with joint optimized energy\nmanagement. In such a P2P setting, a coalition of users is formed for sharing\nresources in a decentralized manner by self-interested users based on their\nindividual preferences. A likely outcome of decentralized coalition formation\nwill be a stable coalition structure, where no group of users could\ncooperatively opt out to form another coalition that induces higher preferences\nto all its members. Remarkably, there exist a number of fair cost-sharing\nmechanisms (e.g., equal-split, proportional-split, egalitarian and Nash\nbargaining solutions of bargaining games) that model practical cost-sharing\napplications with desirable properties, such as the existence of a stable\ncoalition structure with a small strong price-of-anarchy (SPoA) to approximate\nthe social optimum. In this paper, we provide general results of decentralized\ncoalition formation: (1) We establish a logarithmic lower bound on SPoA, and\nhence, show several previously known fair cost-sharing mechanisms are the best\npractical mechanisms with minimal SPoA. (2) We show that the SPoA of\negalitarian and Nash bargaining cost-sharing mechanisms to match the lower\nbound. (3) We derive the SPoA of a mix of different cost-sharing mechanisms.\n(4) We present a decentralized algorithm to form a stable coalition structure.\n(5) Finally, we apply our general results to P2P energy sharing and present an\nempirical study of decentralized coalition formation in a real-world project.\nWe study the empirical SPoA, which is observed within 95% of the social optimal\ncost with coalitions of 2 and 3 users, via fair cost-sharing mechanisms.",
    "published_date": "2020-09-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08632v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.08552v2",
    "title": "Structured Attention for Unsupervised Dialogue Structure Induction",
    "authors": [
      "Liang Qiu",
      "Yizhou Zhao",
      "Weiyan Shi",
      "Yuan Liang",
      "Feng Shi",
      "Tao Yuan",
      "Zhou Yu",
      "Song-Chun Zhu"
    ],
    "author_ids": [],
    "abstract": "Inducing a meaningful structural representation from one or a set of\ndialogues is a crucial but challenging task in computational linguistics.\nAdvancement made in this area is critical for dialogue system design and\ndiscourse analysis. It can also be extended to solve grammatical inference. In\nthis work, we propose to incorporate structured attention layers into a\nVariational Recurrent Neural Network (VRNN) model with discrete latent states\nto learn dialogue structure in an unsupervised fashion. Compared to a vanilla\nVRNN, structured attention enables a model to focus on different parts of the\nsource sentence embeddings while enforcing a structural inductive bias.\nExperiments show that on two-party dialogue datasets, VRNN with structured\nattention learns semantic structures that are similar to templates used to\ngenerate this dialogue corpus. While on multi-party dialogue datasets, our\nmodel learns an interactive structure demonstrating its capability of\ndistinguishing speakers or addresses, automatically disentangling dialogues\nwithout explicit human annotation.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08552v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08410v1",
    "title": "Population Mapping in Informal Settlements with High-Resolution Satellite Imagery and Equitable Ground-Truth",
    "authors": [
      "Konstantin Klemmer",
      "Godwin Yeboah",
      "João Porto de Albuquerque",
      "Stephen A Jarvis"
    ],
    "author_ids": [],
    "abstract": "We propose a generalizable framework for the population estimation of dense,\ninformal settlements in low-income urban areas--so called 'slums'--using\nhigh-resolution satellite imagery. Precise population estimates are a crucial\nfactor for efficient resource allocations by government authorities and NGO's,\nfor instance in medical emergencies. We utilize equitable ground-truth data,\nwhich is gathered in collaboration with local communities: Through training and\ncommunity mapping, the local population contributes their unique domain\nknowledge, while also maintaining agency over their data. This practice allows\nus to avoid carrying forward potential biases into the modeling pipeline, which\nmight arise from a less rigorous ground-truthing approach. We contextualize our\napproach in respect to the ongoing discussion within the machine learning\ncommunity, aiming to make real-world machine learning applications more\ninclusive, fair and accountable. Because of the resource intensive ground-truth\ngeneration process, our training data is limited. We propose a gridded\npopulation estimation model, enabling flexible and customizable spatial\nresolutions. We test our pipeline on three experimental site in Nigeria,\nutilizing pre-trained and fine-tune vision networks to overcome data sparsity.\nOur findings highlight the difficulties of transferring common benchmark models\nto real-world tasks. We discuss this and propose steps forward.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08410v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08282v1",
    "title": "Improving in-home appliance identification using fuzzy-neighbors-preserving analysis based QR-decomposition",
    "authors": [
      "Yassine Himeur",
      "Abdullah Alsalemi",
      "Faycal Bensaali",
      "Abbes Amira"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a new appliance identification scheme by introducing a\nnovel approach for extracting highly discriminative characteristic sets that\ncan considerably distinguish between various appliance footprints. In this\ncontext, a precise and powerful characteristic projection technique depending\non fuzzy-neighbors-preserving analysis based QR-decomposition (FNPA-QR) is\napplied on the extracted energy consumption time-domain features. The FNPA-QR\naims to diminish the distance among the between class features and increase the\ngap among features of dissimilar categories. Following, a novel bagging\ndecision tree (BDT) classifier is also designed to further improve the\nclassification accuracy. The proposed technique is then validated on three\nappliance energy consumption datasets, which are collected at both low and high\nfrequency. The practical results obtained point out the outstanding\nclassification rate of the time-domain based FNPA-QR and BDT.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08282v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.08281v1",
    "title": "A Linked Aggregate Code for Processing Faces (Revised Version)",
    "authors": [
      "Michael Lyons",
      "Kazunori Morikawa"
    ],
    "author_ids": [],
    "abstract": "A model of face representation, inspired by the biology of the visual system,\nis compared to experimental data on the perception of facial similarity. The\nface representation model uses aggregate primary visual cortex (V1) cell\nresponses topographically linked to a grid covering the face, allowing\ncomparison of shape and texture at corresponding points in two facial images.\nWhen a set of relatively similar faces was used as stimuli, this Linked\nAggregate Code (LAC) predicted human performance in similarity judgment\nexperiments. When faces of perceivable categories were used, dimensions such as\napparent sex and race emerged from the LAC model without training. The\ndimensional structure of the LAC similarity measure for the mixed category task\ndisplayed some psychologically plausible features but also highlighted\ndifferences between the model and the human similarity judgements. The human\njudgements exhibited a racial perceptual bias that was not shared by the LAC\nmodel. The results suggest that the LAC based similarity measure may offer a\nfertile starting point for further modelling studies of face representation in\nhigher visual areas, including studies of the development of biases in face\nperception.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "68T01",
      "I.2.0; K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08281v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08270v4",
    "title": "Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals",
    "authors": [
      "Saloni Dash",
      "Vineeth N Balasubramanian",
      "Amit Sharma"
    ],
    "author_ids": [],
    "abstract": "Counterfactual examples for an input -- perturbations that change specific\nfeatures but not others -- have been shown to be useful for evaluating bias of\nmachine learning models, e.g., against specific demographic groups. However,\ngenerating counterfactual examples for images is non-trivial due to the\nunderlying causal structure on the various features of an image. To be\nmeaningful, generated perturbations need to satisfy constraints implied by the\ncausal model. We present a method for generating counterfactuals by\nincorporating a structural causal model (SCM) in an improved variant of\nAdversarially Learned Inference (ALI), that generates counterfactuals in\naccordance with the causal relationships between attributes of an image. Based\non the generated counterfactuals, we show how to explain a pre-trained machine\nlearning classifier, evaluate its bias, and mitigate the bias using a\ncounterfactual regularizer. On the Morpho-MNIST dataset, our method generates\ncounterfactuals comparable in quality to prior work on SCM-based\ncounterfactuals (DeepSCM), while on the more complex CelebA dataset our method\noutperforms DeepSCM in generating high-quality valid counterfactuals. Moreover,\ngenerated counterfactuals are indistinguishable from reconstructed images in a\nhuman evaluation experiment and we subsequently use them to evaluate the\nfairness of a standard classifier trained on CelebA data. We show that the\nclassifier is biased w.r.t. skin and hair color, and how counterfactual\nregularization can remove those biases.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08270v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08267v3",
    "title": "Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling",
    "authors": [
      "Timothy Rumbell",
      "Jaimit Parikh",
      "James Kozloski",
      "Viatcheslav Gurev"
    ],
    "author_ids": [],
    "abstract": "Predictions for physical systems often rely upon knowledge acquired from\nensembles of entities, e.g., ensembles of cells in biological sciences. For\nqualitative and quantitative analysis, these ensembles are simulated with\nparametric families of mechanistic models (MM). Two classes of methodologies,\nbased on Bayesian inference and Population of Models, currently prevail in\nparameter estimation for physical systems. However, in Bayesian analysis,\nuninformative priors for MM parameters introduce undesirable bias. Here, we\npropose how to infer parameters within the framework of stochastic inverse\nproblems (SIP), also termed data-consistent inversion, wherein the prior\ntargets only uncertainties that arise due to MM non-invertibility. To\ndemonstrate, we introduce new methods to solve SIP based on rejection sampling,\nMarkov chain Monte Carlo, and generative adversarial networks (GANs). In\naddition, to overcome limitations of SIP, we reformulate SIP based on\nconstrained optimization and present a novel GAN to solve the constrained\noptimization problem.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08267v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08127v1",
    "title": "Addressing Cognitive Biases in Augmented Business Decision Systems",
    "authors": [
      "Thomas Baudel",
      "Manon Verbockhaven",
      "Guillaume Roy",
      "Victoire Cousergue",
      "Rida Laarach"
    ],
    "author_ids": [],
    "abstract": "How do algorithmic decision aids introduced in business decision processes\naffect task performance? In a first experiment, we study effective\ncollaboration. Faced with a decision, subjects alone have a success rate of\n72%; Aided by a recommender that has a 75% success rate, their success rate\nreaches 76%. The human-system collaboration had thus a greater success rate\nthan each taken alone. However, we noted a complacency/authority bias that\ndegraded the quality of decisions by 5% when the recommender was wrong. This\nsuggests that any lingering algorithmic bias may be amplified by decision aids.\nIn a second experiment, we evaluated the effectiveness of 5 presentation\nvariants in reducing complacency bias. We found that optional presentation\nincreases subjects' resistance to wrong recommendations. We conclude by arguing\nthat our metrics, in real usage scenarios, where decision aids are embedded as\nsystem-wide features in Business Process Management software, can lead to\nenhanced benefits.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "H.4.1; H.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08127v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.08097v1",
    "title": "An Extension of Fano's Inequality for Characterizing Model Susceptibility to Membership Inference Attacks",
    "authors": [
      "Sumit Kumar Jha",
      "Susmit Jha",
      "Rickard Ewetz",
      "Sunny Raj",
      "Alvaro Velasquez",
      "Laura L. Pullum",
      "Ananthram Swami"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks have been shown to be vulnerable to membership inference\nattacks wherein the attacker aims to detect whether specific input data were\nused to train the model. These attacks can potentially leak private or\nproprietary data. We present a new extension of Fano's inequality and employ it\nto theoretically establish that the probability of success for a membership\ninference attack on a deep neural network can be bounded using the mutual\ninformation between its inputs and its activations. This enables the use of\nmutual information to measure the susceptibility of a DNN model to membership\ninference attacks. In our empirical evaluation, we show that the correlation\nbetween the mutual information and the susceptibility of the DNN model to\nmembership inference attacks is 0.966, 0.996, and 0.955 for CIFAR-10, SVHN and\nGTSRB models, respectively.",
    "published_date": "2020-09-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08097v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07838v2",
    "title": "FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition",
    "authors": [
      "Tomáš Sixta",
      "Julio C. S. Jacques Junior",
      "Pau Buch-Cardona",
      "Neil M. Robertson",
      "Eduard Vazquez",
      "Sergio Escalera"
    ],
    "author_ids": [],
    "abstract": "This work summarizes the 2020 ChaLearn Looking at People Fair Face\nRecognition and Analysis Challenge and provides a description of the\ntop-winning solutions and analysis of the results. The aim of the challenge was\nto evaluate accuracy and bias in gender and skin colour of submitted algorithms\non the task of 1:1 face verification in the presence of other confounding\nattributes. Participants were evaluated using an in-the-wild dataset based on\nreannotated IJB-C, further enriched by 12.5K new images and additional labels.\nThe dataset is not balanced, which simulates a real world scenario where\nAI-based models supposed to present fair outcomes are trained and evaluated on\nimbalanced data. The challenge attracted 151 participants, who made more than\n1.8K submissions in total. The final phase of the challenge attracted 36 active\nteams out of which 10 exceeded 0.999 AUC-ROC while achieving very low scores in\nthe proposed bias metrics. Common strategies by the participants were face\npre-processing, homogenization of data distributions, the use of bias aware\nloss functions and ensemble models. The analysis of top-10 teams shows higher\nfalse positive rates (and lower false negative rates) for females with dark\nskin tone as well as the potential of eyeglasses and young age to increase the\nfalse positive rates too.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07838v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07828v2",
    "title": "Human biases in body measurement estimation",
    "authors": [
      "Kirill Martynov",
      "Kiran Garimella",
      "Robert West"
    ],
    "author_ids": [],
    "abstract": "Body measurements, including weight and height, are key indicators of health.\nBeing able to visually assess body measurements reliably is a step towards\nincreased awareness of overweight and obesity and is thus important for public\nhealth. Nevertheless it is currently not well understood how accurately humans\ncan assess weight and height from images, and when and how they fail. To bridge\nthis gap, we start from 1,682 images of persons collected from the Web, each\nannotated with the true weight and height, and ask crowd workers to estimate\nthe weight and height for each image. We conduct a faceted analysis taking into\naccount characteristics of the images as well as the crowd workers assessing\nthe images, revealing several novel findings: (1) Even after aggregation, the\ncrowd's accuracy is overall low. (2) We find strong evidence of contraction\nbias toward a reference value, such that the weight (height) of light (short)\npeople is overestimated, whereas that of heavy (tall) people is underestimated.\n(3) We estimate workers' individual reference values using a Bayesian model,\nfinding that reference values strongly correlate with workers' own height and\nweight, indicating that workers are better at estimating people similar to\nthemselves. (4) The weight of tall people is underestimated more than that of\nshort people; yet, knowing the height decreases the weight error only mildly.\n(5) Accuracy is higher on images of females than of males, but female and male\nworkers are no different in terms of accuracy. (6) Crowd workers improve over\ntime if given feedback on previous guesses. Finally, we explore various bias\ncorrection models for improving the crowd's accuracy, but find that this only\nleads to modest gains. Overall, this work provides important insights on biases\nin body measurement estimation as obesity related conditions are on the rise.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07828v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.07776v3",
    "title": "Characterizing Attitudinal Network Graphs through Frustration Cloud",
    "authors": [
      "Lucas Rusnak",
      "Jelena Tešić"
    ],
    "author_ids": [],
    "abstract": "Attitudinal Network Graphs are signed graphs where edges capture an expressed\nopinion; two vertices connected by an edge can be agreeable (positive) or\nantagonistic (negative). A signed graph is called balanced if each of its\ncycles includes an even number of negative edges. Balance is often\ncharacterized by the frustration index or by finding a single convergent\nbalanced state of network consensus. In this paper, we propose to expand the\nmeasures of consensus from a single balanced state associated with the\nfrustration index to the set of nearest balanced states. We introduce the\nfrustration cloud as a set of all nearest balanced states and use a\ngraph-balancing algorithm to find all nearest balanced states in a\ndeterministic way. Computational concerns are addressed by measuring consensus\nprobabilistically, and we introduce new vertex and edge metrics to quantify\nstatus, agreement, and influence. We also introduce a new global measure of\ncontroversy for a given signed graph and show that vertex status is a zero-sum\ngame in the signed network. We propose an efficient scalable algorithm for\ncalculating frustration cloud-based measures in social network and survey data\nof up to 80,000 vertices and half-a-million edges. We also demonstrate the\npower of the proposed approach to provide discriminant features for community\ndiscovery when compared to spectral clustering and to automatically identify\ndominant vertices and anomalous decisions in the network.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.IR",
      "cs.SY",
      "eess.SY",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07776v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.07712v4",
    "title": "Collaborative Group Learning",
    "authors": [
      "Shaoxiong Feng",
      "Hongshen Chen",
      "Xuancheng Ren",
      "Zhuoye Ding",
      "Kan Li",
      "Xu Sun"
    ],
    "author_ids": [],
    "abstract": "Collaborative learning has successfully applied knowledge transfer to guide a\npool of small student networks towards robust local minima. However, previous\napproaches typically struggle with drastically aggravated student\nhomogenization when the number of students rises. In this paper, we propose\nCollaborative Group Learning, an efficient framework that aims to diversify the\nfeature representation and conduct an effective regularization. Intuitively,\nsimilar to the human group study mechanism, we induce students to learn and\nexchange different parts of course knowledge as collaborative groups. First,\neach student is established by randomly routing on a modular neural network,\nwhich facilitates flexible knowledge communication between students due to\nrandom levels of representation sharing and branching. Second, to resist the\nstudent homogenization, students first compose diverse feature sets by\nexploiting the inductive bias from sub-sets of training data, and then\naggregate and distill different complementary knowledge by imitating a random\nsub-group of students at each time step. Overall, the above mechanisms are\nbeneficial for maximizing the student population to further improve the model\ngeneralization without sacrificing computational efficiency. Empirical\nevaluations on both image and text tasks indicate that our method significantly\noutperforms various state-of-the-art collaborative approaches whilst enhancing\ncomputational efficiency.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07712v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07628v1",
    "title": "Proceedings 13th Interaction and Concurrency Experience",
    "authors": [
      "Julien Lange",
      "Anastasia Mavridou",
      "Larisa Safina",
      "Alceste Scalas"
    ],
    "author_ids": [],
    "abstract": "This volume contains the proceedings of ICE'20, the 13th Interaction and\nConcurrency Experience, which was held online on the 19th of June 2020, as a\nsatellite event of DisCoTec'20. The ICE workshop series features a\ndistinguishing review and selection procedure, allowing PC members to interact\nanonymously with authors. As in the past 12 editions, this interaction\nconsiderably improved the accuracy of the feedback from the reviewers and the\nquality of accepted papers, and offered the basis for lively discussion during\nthe workshop. The 2020 edition of ICE included double blind reviewing of\noriginal research papers, in order to increase fairness and avoid bias in\nreviewing. Each paper was reviewed by three PC members, and altogether 5 papers\nwere accepted for publication - plus 5 oral presentations which are not part of\nthis volume. We were proud to host 2 invited talks, by Cinzia Di Giusto and\nKaroliina Lehtinen. The abstracts of these talks are included in this volume\ntogether with the regular papers. The final versions of the contributions,\ntaking into account the discussion at the workshop, are included.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.PL",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07628v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09068v1",
    "title": "Hacking with God: a Common Programming Language of Robopsychology and Robophilosophy",
    "authors": [
      "Norbert Bátfai"
    ],
    "author_ids": [],
    "abstract": "This note is a sketch of how the concept of robopsychology and robophilosophy\ncould be reinterpreted and repositioned in the spirit of the original vocation\nof psychology and philosophy. The notion of the robopsychology as a fictional\nscience and a fictional occupation was introduced by Asimov in the middle of\nthe last century. The robophilosophy, on the other hand, is only a few years\nold today. But at this moment, none of these new emerging disciplines focus on\nthe fundamental and overall issues of the development of artificial general\nintelligence. Instead, they focus only on issues that, although are extremely\nimportant, play a complementary role, such as moral or ethical ones, rather\nthan the big questions of life. We try to outline a conception in which the\nrobophilosophy and robopsychology will be able to play a similar leading rule\nin the progress of artificial intelligence than the philosophy and psychology\nhave done in the progress of human intelligence. To facilitate this, we outline\nthe idea of a visual artificial language and interactive theorem prover-based\ncomputer application called Prime Convo Assistant. The question to be decided\nin the future is whether we can develop such an application. And if so, can we\nbuild a computer game on it, or even an esport game? It may be an interesting\nquestion in order for this game will be able to transform human thinking on the\nwidest possible social scale and will be able to develop a standard\nmathematical logic-based communication channel between human and machine\nintelligence.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09068v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07503v2",
    "title": "Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation Extraction",
    "authors": [
      "Ranran Haoran Zhang",
      "Qianying Liu",
      "Aysa Xuemo Fan",
      "Heng Ji",
      "Daojian Zeng",
      "Fei Cheng",
      "Daisuke Kawahara",
      "Sadao Kurohashi"
    ],
    "author_ids": [],
    "abstract": "Joint entity and relation extraction aims to extract relation triplets from\nplain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models\nfor triplet sequence generation. However, Seq2Seq enforces an unnecessary order\non the unordered triplets and involves a large decoding length associated with\nerror accumulation. These introduce exposure bias, which may cause the models\noverfit to the frequent label combination, thus deteriorating the\ngeneralization. We propose a novel Sequence-to-Unordered-Multi-Tree\n(Seq2UMTree) model to minimize the effects of exposure bias by limiting the\ndecoding length to three within a triplet and removing the order among\ntriplets. We evaluate our model on two datasets, DuIE and NYT, and\nsystematically study how exposure bias alters the performance of Seq2Seq\nmodels. Experiments show that the state-of-the-art Seq2Seq model overfits to\nboth datasets while Seq2UMTree shows significantly better generalization. Our\ncode is available at https://github.com/WindChimeRan/OpenJERE .",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07503v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07451v3",
    "title": "Information Bottleneck Constrained Latent Bidirectional Embedding for Zero-Shot Learning",
    "authors": [
      "Yang Liu",
      "Lei Zhou",
      "Xiao Bai",
      "Lin Gu",
      "Tatsuya Harada",
      "Jun Zhou"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes by transferring\nsemantic knowledge from seen classes to unseen classes. Though many ZSL methods\nrely on a direct mapping between the visual and the semantic space, the\ncalibration deviation and hubness problem limit the generalization capability\nto unseen classes. Recently emerged generative ZSL methods generate unseen\nimage features to transform ZSL into a supervised classification problem.\nHowever, most generative models still suffer from the seen-unseen bias problem\nas only seen data is used for training. To address these issues, we propose a\nnovel bidirectional embedding based generative model with a tight\nvisual-semantic coupling constraint. We learn a unified latent space that\ncalibrates the embedded parametric distributions of both visual and semantic\nspaces. Since the embedding from high-dimensional visual features comprise much\nnon-semantic information, the alignment of visual and semantic in latent space\nwould inevitably been deviated. Therefore, we introduce information bottleneck\n(IB) constraint to ZSL for the first time to preserve essential attribute\ninformation during the mapping. Specifically, we utilize the uncertainty\nestimation and the wake-sleep procedure to alleviate the feature noises and\nimprove model abstraction capability. In addition, our method can be easily\nextended to transductive ZSL setting by generating labels for unseen images. We\nthen introduce a robust loss to solve this label noise problem. Extensive\nexperimental results show that our method outperforms the state-of-the-art\nmethods in different ZSL settings on most benchmark datasets. The code will be\navailable at https://github.com/osierboy/IBZSL.",
    "published_date": "2020-09-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07451v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07165v3",
    "title": "Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses",
    "authors": [
      "Kaivalya Rawal",
      "Himabindu Lakkaraju"
    ],
    "author_ids": [],
    "abstract": "As predictive models are increasingly being deployed in high-stakes\ndecision-making, there has been a lot of interest in developing algorithms\nwhich can provide recourses to affected individuals. While developing such\ntools is important, it is even more critical to analyse and interpret a\npredictive model, and vet it thoroughly to ensure that the recourses it offers\nare meaningful and non-discriminatory before it is deployed in the real world.\nTo this end, we propose a novel model agnostic framework called Actionable\nRecourse Summaries (AReS) to construct global counterfactual explanations which\nprovide an interpretable and accurate summary of recourses for the entire\npopulation. We formulate a novel objective which simultaneously optimizes for\ncorrectness of the recourses and interpretability of the explanations, while\nminimizing overall recourse costs across the entire population. More\nspecifically, our objective enables us to learn, with optimality guarantees on\nrecourse correctness, a small number of compact rule sets each of which capture\nrecourses for well defined subpopulations within the data. We also demonstrate\ntheoretically that several of the prior approaches proposed to generate\nrecourses for individuals are special cases of our framework. Experimental\nevaluation with real world datasets and user studies demonstrate that our\nframework can provide decision makers with a comprehensive overview of\nrecourses corresponding to any black box model, and consequently help detect\nundesirable model biases and discrimination.",
    "published_date": "2020-09-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07165v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07139v1",
    "title": "Analyzing the effect of APOE on Alzheimer's disease progression using an event-based model for stratified populations",
    "authors": [
      "Vikram Venkatraghavan",
      "Stefan Klein",
      "Lana Fani",
      "Leontine S. Ham",
      "Henri Vrooman",
      "M. Kamran Ikram",
      "Wiro J. Niessen",
      "Esther E. Bron"
    ],
    "author_ids": [],
    "abstract": "Alzheimer's disease (AD) is the most common form of dementia and is\nphenotypically heterogeneous. APOE is a triallelic gene which correlates with\nphenotypic heterogeneity in AD. In this work, we determined the effect of APOE\nalleles on the disease progression timeline of AD using a discriminative\nevent-based model (DEBM). Since DEBM is a data-driven model, stratification\ninto smaller disease subgroups would lead to more inaccurate models as compared\nto fitting the model on the entire dataset. Hence our secondary aim is to\npropose and evaluate novel approaches in which we split the different steps of\nDEBM into group-aspecific and group-specific parts, where the entire dataset is\nused to train the group-aspecific parts and only the data from a specific group\nis used to train the group-specific parts of the DEBM. We performed simulation\nexperiments to benchmark the accuracy of the proposed approaches and to select\nthe optimal approach. Subsequently, the chosen approach was applied to the\nbaseline data of 417 cognitively normal, 235 mild cognitively impaired who\nconvert to AD within 3 years, and 342 AD patients from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset to gain new insights into the effect of\nAPOE carriership on the disease progression timeline of AD. The presented\nmodels could aid understanding of the disease, and in selecting homogeneous\ngroup of presymptomatic subjects at-risk of developing symptoms for clinical\ntrials.",
    "published_date": "2020-09-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07139v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06859v1",
    "title": "A Satisficing Control Design Framework with Safety and Performance Guarantees for Constrained Systems under Disturbances",
    "authors": [
      "Yuzhen Han",
      "Hamidreza Modares"
    ],
    "author_ids": [],
    "abstract": "This paper presents a safe robust policy iteration (SR-PI) algorithm to\ndesign controllers with satisficing (good enough) performance and safety\nguarantee. This is in contrast to standard PI-based control design methods with\nno safety certification. It also moves away from existing safe control design\napproaches that perform pointwise optimization and are thus myopic. Safety\nassurance requires satisfying a control barrier function (CBF), which might be\nin conflict with the performance-driven Lyapunov solution to the Bellman\nequation arising in each iteration of the PI. Therefore, a new development is\nrequired to robustly certify the safety of an improved policy at each iteration\nof the PI. The proposed SR-PI algorithm unifies performance guarantee (provided\nby a Bellman inequality) with safety guarantee (provided by a robust CBF) at\neach iteration. The Bellman inequality resembles the satisficing decision\nmaking framework and parameterizes the sacrifice on the performance with an\naspiration level when there is a conflict with safety. This aspiration level is\noptimized at each iteration to minimize the sacrifice on the performance. It is\nshown that the presented satisficing control policies obtained at each\niteration of the SR-PI guarantees robust safety and performance. Robust\nstability is also guaranteed when there is no conflict with safety. Sum of\nsquares (SOS) program is employed to implement the proposed SR-PI algorithm\niteratively. Finally, numerical simulations are carried out to illustrate the\nproposed satisficing control framework.",
    "published_date": "2020-09-15T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06859v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.06773v1",
    "title": "Truth or Square: Aspect Ratio Biases Recall of Position Encodings",
    "authors": [
      "Cristina R. Ceja",
      "Caitlyn M. McColeman",
      "Cindy Xiong",
      "Steven L. Franconeri"
    ],
    "author_ids": [],
    "abstract": "Bar charts are among the most frequently used visualizations, in part because\ntheir position encoding leads them to convey data values precisely. Yet\nreproductions of single bars or groups of bars within a graph can be biased.\nCuriously, some previous work found that this bias resulted in an\noverestimation of reproduced data values, while other work found an\nunderestimation. Across three empirical studies, we offer an explanation for\nthese conflicting findings: this discrepancy is a consequence of the differing\naspect ratios of the tested bar marks. Viewers are biased to remember a bar\nmark as being more similar to a prototypical square, leading to an\noverestimation of bars with a wide aspect ratio, and an underestimation of bars\nwith a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of\nthe bar marks indeed influenced the direction of this bias. Experiment 3\nconfirmed that this pattern of misestimation bias was present for reproductions\nfrom memory, suggesting that this bias may arise when comparing values across\nsequential displays or views. We describe additional visualization designs that\nmight be prone to this bias beyond bar charts (e.g., Mekko charts and\ntreemaps), and speculate that other visual channels might hold similar biases\ntoward prototypical values.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06773v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.06700v1",
    "title": "Biased RSA private keys: Origin attribution of GCD-factorable keys",
    "authors": [
      "Adam Janovsky",
      "Matus Nemec",
      "Petr Svenda",
      "Peter Sekan",
      "Vashek Matyas"
    ],
    "author_ids": [],
    "abstract": "In 2016, Svenda et al. (USENIX 2016, The Million-key Question) reported that\nthe implementation choices in cryptographic libraries allow for qualified\nguessing about the origin of public RSA keys. We extend the technique to two\nnew scenarios when not only public but also private keys are available for the\norigin attribution - analysis of a source of GCD-factorable keys in IPv4-wide\nTLS scans and forensic investigation of an unknown source. We learn several\nrepresentatives of the bias from the private keys to train a model on more than\n150 million keys collected from 70 cryptographic libraries, hardware security\nmodules and cryptographic smartcards. Our model not only doubles the number of\ndistinguishable groups of libraries (compared to public keys from Svenda et\nal.) but also improves more than twice in accuracy w.r.t. random guessing when\na single key is classified. For a forensic scenario where at least 10 keys from\nthe same source are available, the correct origin library is correctly\nidentified with average accuracy of 89% compared to 4% accuracy of a random\nguess. The technique was also used to identify libraries producing\nGCD-factorable TLS keys, showing that only three groups are the probable\nsuspects.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06700v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.06679v1",
    "title": "Data Augmentation and Clustering for Vehicle Make/Model Classification",
    "authors": [
      "Mohamed Nafzi",
      "Michael Brauckmann",
      "Tobias Glasmachers"
    ],
    "author_ids": [],
    "abstract": "Vehicle shape information is very important in Intelligent Traffic Systems\n(ITS). In this paper we present a way to exploit a training data set of\nvehicles released in different years and captured under different perspectives.\nAlso the efficacy of clustering to enhance the make/model classification is\npresented. Both steps led to improved classification results and a greater\nrobustness. Deeper convolutional neural network based on ResNet architecture\nhas been designed for the training of the vehicle make/model classification.\nThe unequal class distribution of training data produces an a priori\nprobability. Its elimination, obtained by removing of the bias and through hard\nnormalization of the centroids in the classification layer, improves the\nclassification results. A developed application has been used to test the\nvehicle re-identification on video data manually based on make/model and color\nclassification. This work was partially funded under the grant.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06367v2",
    "title": "GeDi: Generative Discriminator Guided Sequence Generation",
    "authors": [
      "Ben Krause",
      "Akhilesh Deepak Gotmare",
      "Bryan McCann",
      "Nitish Shirish Keskar",
      "Shafiq Joty",
      "Richard Socher",
      "Nazneen Fatema Rajani"
    ],
    "author_ids": [],
    "abstract": "While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06367v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06516v2",
    "title": "Justicia: A Stochastic SAT Approach to Formally Verify Fairness",
    "authors": [
      "Bishwamittra Ghosh",
      "Debabrota Basu",
      "Kuldeep S. Meel"
    ],
    "author_ids": [],
    "abstract": "As a technology ML is oblivious to societal good or bad, and thus, the field\nof fair machine learning has stepped up to propose multiple mathematical\ndefinitions, algorithms, and systems to ensure different notions of fairness in\nML applications. Given the multitude of propositions, it has become imperative\nto formally verify the fairness metrics satisfied by different algorithms on\ndifferent datasets. In this paper, we propose a stochastic satisfiability\n(SSAT) framework, Justicia, that formally verifies different fairness measures\nof supervised learning algorithms with respect to the underlying data\ndistribution. We instantiate Justicia on multiple classification and bias\nmitigation algorithms, and datasets to verify different fairness metrics, such\nas disparate impact, statistical parity, and equalized odds. Justicia is\nscalable, accurate, and operates on non-Boolean and compound sensitive\nattributes unlike existing distribution-based verifiers, such as FairSquare and\nVeriFair. Being distribution-based by design, Justicia is more robust than the\nverifiers, such as AIF360, that operate on specific test samples. We also\ntheoretically bound the finite-sample error of the verified fairness measure.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06516v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06433v1",
    "title": "Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations",
    "authors": [
      "Fabian Sperrle",
      "Mennatallah El-Assady",
      "Grace Guo",
      "Duen Horng Chau",
      "Alex Endert",
      "Daniel Keim"
    ],
    "author_ids": [],
    "abstract": "This paper systematically derives design dimensions for the structured\nevaluation of explainable artificial intelligence (XAI) approaches. These\ndimensions enable a descriptive characterization, facilitating comparisons\nbetween different study designs. They further structure the design space of\nXAI, converging towards a precise terminology required for a rigorous study of\nXAI. Our literature review differentiates between comparative studies and\napplication papers, revealing methodological differences between the fields of\nmachine learning, human-computer interaction, and visual analytics. Generally,\neach of these disciplines targets specific parts of the XAI process. Bridging\nthe resulting gaps enables a holistic evaluation of XAI in real-world\nscenarios, as proposed by our conceptual model characterizing bias sources and\ntrust-building. Furthermore, we identify and discuss the potential for future\nwork based on observed research gaps that should lead to better coverage of the\nproposed model.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06433v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06251v1",
    "title": "Active Fairness Instead of Unawareness",
    "authors": [
      "Boris Ruf",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "The possible risk that AI systems could promote discrimination by reproducing\nand enforcing unwanted bias in data has been broadly discussed in research and\nsociety. Many current legal standards demand to remove sensitive attributes\nfrom data in order to achieve \"fairness through unawareness\". We argue that\nthis approach is obsolete in the era of big data where large datasets with\nhighly correlated attributes are common. In the contrary, we propose the active\nuse of sensitive attributes with the purpose of observing and controlling any\nkind of discrimination, and thus leading to fair results.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06251v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06226v1",
    "title": "Prior Knowledge about Attributes: Learning a More Effective Potential Space for Zero-Shot Recognition",
    "authors": [
      "Chunlai Chai",
      "Yukuan Lou",
      "Shijin Zhang"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes accurately by\nlearning seen classes and known attributes, but correlations in attributes were\nignored by previous study which lead to classification results confused. To\nsolve this problem, we build an Attribute Correlation Potential Space\nGeneration (ACPSG) model which uses a graph convolution network and attribute\ncorrelation to generate a more discriminating potential space. Combining\npotential discrimination space and user-defined attribute space, we can better\nclassify unseen classes. Our approach outperforms some existing\nstate-of-the-art methods on several benchmark datasets, whether it is\nconventional ZSL or generalized ZSL.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06226v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06206v5",
    "title": "On Robustness and Bias Analysis of BERT-based Relation Extraction",
    "authors": [
      "Luoqiu Li",
      "Xiang Chen",
      "Hongbin Ye",
      "Zhen Bi",
      "Shumin Deng",
      "Ningyu Zhang",
      "Huajun Chen"
    ],
    "author_ids": [],
    "abstract": "Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06206v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06190v1",
    "title": "Fairness Constraints in Semi-supervised Learning",
    "authors": [
      "Tao Zhang",
      "Tianqing Zhu",
      "Mengde Han",
      "Jing Li",
      "Wanlei Zhou",
      "Philip S. Yu"
    ],
    "author_ids": [],
    "abstract": "Fairness in machine learning has received considerable attention. However,\nmost studies on fair learning focus on either supervised learning or\nunsupervised learning. Very few consider semi-supervised settings. Yet, in\nreality, most machine learning tasks rely on large datasets that contain both\nlabeled and unlabeled data. One of key issues with fair learning is the balance\nbetween fairness and accuracy. Previous studies arguing that increasing the\nsize of the training set can have a better trade-off. We believe that\nincreasing the training set with unlabeled data may achieve the similar result.\nHence, we develop a framework for fair semi-supervised learning, which is\nformulated as an optimization problem. This includes classifier loss to\noptimize accuracy, label propagation loss to optimize unlabled data prediction,\nand fairness constraints over labeled and unlabeled data to optimize the\nfairness level. The framework is conducted in logistic regression and support\nvector machines under the fairness metrics of disparate impact and disparate\nmistreatment. We theoretically analyze the source of discrimination in\nsemi-supervised learning via bias, variance and noise decomposition. Extensive\nexperiments show that our method is able to achieve fair semi-supervised\nlearning, and reach a better trade-off between accuracy and fairness than fair\nsupervised learning.",
    "published_date": "2020-09-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06190v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06042v2",
    "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection",
    "authors": [
      "Shayan Monadjemi",
      "Roman Garnett",
      "Alvitta Ottley"
    ],
    "author_ids": [],
    "abstract": "Analyzing interaction data provides an opportunity to learn about users,\nuncover their underlying goals, and create intelligent visualization systems.\nThe first step for intelligent response in visualizations is to enable\ncomputers to infer user goals and strategies through observing their\ninteractions with a system. Researchers have proposed multiple techniques to\nmodel users, however, their frameworks often depend on the visualization\ndesign, interaction space, and dataset. Due to these dependencies, many\ntechniques do not provide a general algorithmic solution to user exploration\nmodeling. In this paper, we construct a series of models based on the dataset\nand pose user exploration modeling as a Bayesian model selection problem where\nwe maintain a belief over numerous competing models that could explain user\ninteractions. Each of these competing models represent an exploration strategy\nthe user could adopt during a session. The goal of our technique is to make\nhigh-level and in-depth inferences about the user by observing their low-level\ninteractions. Although our proposed idea is applicable to various probabilistic\nmodel spaces, we demonstrate a specific instance of encoding exploration\npatterns as competing models to infer information relevance. We validate our\ntechnique's ability to infer exploration bias, predict future interactions, and\nsummarize an analytic session using user study datasets. Our results indicate\nthat depending on the application, our method outperforms established baselines\nfor bias detection and future interaction prediction. Finally, we discuss\nfuture research directions based on our proposed modeling paradigm and suggest\nhow practitioners can use this method to build intelligent visualization\nsystems that understand users' goals and adapt to improve the exploration\nprocess.",
    "published_date": "2020-09-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06042v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.06037v5",
    "title": "Genetic Programming is Naturally Suited to Evolve Bagging Ensembles",
    "authors": [
      "Marco Virgolin"
    ],
    "author_ids": [],
    "abstract": "Learning ensembles by bagging can substantially improve the generalization\nperformance of low-bias, high-variance estimators, including those evolved by\nGenetic Programming (GP). To be efficient, modern GP algorithms for evolving\n(bagging) ensembles typically rely on several (often inter-connected)\nmechanisms and respective hyper-parameters, ultimately compromising ease of\nuse. In this paper, we provide experimental evidence that such complexity might\nnot be warranted. We show that minor changes to fitness evaluation and\nselection are sufficient to make a simple and otherwise-traditional GP\nalgorithm evolve ensembles efficiently. The key to our proposal is to exploit\nthe way bagging works to compute, for each individual in the population,\nmultiple fitness values (instead of one) at a cost that is only marginally\nhigher than the one of a normal fitness evaluation. Experimental comparisons on\nclassification and regression tasks taken and reproduced from prior studies\nshow that our algorithm fares very well against state-of-the-art ensemble and\nnon-ensemble GP algorithms. We further provide insights into the proposed\napproach by (i) scaling the ensemble size, (ii) ablating the changes to\nselection, (iii) observing the evolvability induced by traditional subtree\nvariation. Code: https://github.com/marcovirgolin/2SEGP.",
    "published_date": "2020-09-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06037v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.07025v1",
    "title": "FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment",
    "authors": [
      "Alejandro Peña",
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "author_ids": [],
    "abstract": "With the aim of studying how current multimodal AI algorithms based on\nheterogeneous sources of information are affected by sensitive elements and\ninner biases in the data, this demonstrator experiments over an automated\nrecruitment testbed based on Curriculum Vitae: FairCVtest. The presence of\ndecision-making algorithms in society is rapidly increasing nowadays, while\nconcerns about their transparency and the possibility of these algorithms\nbecoming new sources of discrimination are arising. This demo shows the\ncapacity of the Artificial Intelligence (AI) behind a recruitment tool to\nextract sensitive information from unstructured data, and exploit it in\ncombination to data biases in undesirable (unfair) ways. Aditionally, the demo\nincludes a new algorithm (SensitiveNets) for discrimination-aware learning\nwhich eliminates sensitive information in our multimodal AI framework.",
    "published_date": "2020-09-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.07025v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.05823v4",
    "title": "On Achieving Leximin Fairness and Stability in Many-to-One Matchings",
    "authors": [
      "Shivika Narang",
      "Arpita Biswas",
      "Y Narahari"
    ],
    "author_ids": [],
    "abstract": "The past few years have seen a surge of work on fairness in allocation\nproblems where items must be fairly divided among agents having individual\npreferences. In comparison, fairness in settings with preferences on both\nsides, that is, where agents have to be matched to other agents, has received\nmuch less attention. Moreover, two-sided matching literature has largely\nfocused on ordinal preferences. This paper initiates the study of fairness in\nstable many-to-one matchings under cardinal valuations. Motivated by real-world\nsettings, we study leximin optimality over stable many-to-one matchings. We\nfirst investigate matching problems with ranked valuations where all agents on\neach side have the same preference orders or rankings over the agents on the\nother side (but not necessarily the same valuations). Here, we provide a\ncomplete characterisation of the space of stable matchings. This leads to FaSt,\na novel and efficient algorithm to compute a leximin optimal stable matching\nunder ranked isometric valuations (where, for each pair of agents, the\nvaluation of one agent for the other is the same). Building upon FaSt, we\npresent an efficient algorithm, FaSt-Gen, that finds the leximin optimal stable\nmatching for a more general ranked setting. When there are exactly two agents\non one side who may be matched to many agents on the other, strict preferences\nare enough to guarantee an efficient algorithm. We next establish that, in the\nabsence of rankings and under strict preferences (with no restriction on the\nnumber of agents on either side), finding a leximin optimal stable matching is\nNP-Hard. Further, with weak rankings, the problem is strongly NP-Hard, even\nunder isometric valuations. In fact, when additivity and non-negativity are the\nonly assumptions, we show that, unless P=NP, no efficient polynomial factor\napproximation is possible.",
    "published_date": "2020-09-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05823v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.05653v1",
    "title": "Teaching Tech to Talk: K-12 Conversational Artificial Intelligence Literacy Curriculum and Development Tools",
    "authors": [
      "Jessica Van Brummelen",
      "Tommy Heng",
      "Viktoriya Tabunshchyk"
    ],
    "author_ids": [],
    "abstract": "With children talking to smart-speakers, smart-phones and even\nsmart-microwaves daily, it is increasingly important to educate students on how\nthese agents work-from underlying mechanisms to societal implications.\nResearchers are developing tools and curriculum to teach K-12 students broadly\nabout artificial intelligence (AI); however, few studies have evaluated these\ntools with respect to AI-specific learning outcomes, and even fewer have\naddressed student learning about AI-based conversational agents. We evaluate\nour Conversational Agent Interface for MIT App Inventor and workshop curriculum\nwith respect to eight AI competencies from the literature. Furthermore, we\nanalyze teacher (n=9) and student (n=47) feedback from workshops with the\ninterface and recommend that future work leverages design considerations from\nthe literature to optimize engagement, collaborates with teachers, and\naddresses a range of student abilities through pacing and opportunities for\nextension. We found students struggled most with the concepts of AI ethics and\nlearning, and recommend emphasizing these topics when teaching.\n  The appendix, including a demo video, can be found here:\nhttps://gist.github.com/jessvb/1cd959e32415a6ad4389761c49b54bbf",
    "published_date": "2020-09-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; I.2.5; I.2.7; K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05653v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.05283v6",
    "title": "Fair and accurate age prediction using distribution aware data curation and augmentation",
    "authors": [
      "Yushi Cao",
      "David Berend",
      "Palina Tolmach",
      "Guy Amit",
      "Moshe Levy",
      "Yang Liu",
      "Asaf Shabtai",
      "Yuval Elovici"
    ],
    "author_ids": [],
    "abstract": "Deep learning-based facial recognition systems have experienced increased\nmedia attention due to exhibiting unfair behavior. Large enterprises, such as\nIBM, shut down their facial recognition and age prediction systems as a\nconsequence. Age prediction is an especially difficult application with the\nissue of fairness remaining an open research problem (e.g., predicting age for\ndifferent ethnicity equally accurate). One of the main causes of unfair\nbehavior in age prediction methods lies in the distribution and diversity of\nthe training data. In this work, we present two novel approaches for dataset\ncuration and data augmentation in order to increase fairness through balanced\nfeature curation and increase diversity through distribution aware\naugmentation. To achieve this, we introduce out-of-distribution detection to\nthe facial recognition domain which is used to select the data most relevant to\nthe deep neural network's (DNN) task when balancing the data among age,\nethnicity, and gender. Our approach shows promising results. Our best-trained\nDNN model outperformed all academic and industrial baselines in terms of\nfairness by up to 4.92 times and also enhanced the DNN's ability to generalize\noutperforming Amazon AWS and Microsoft Azure public cloud systems by 31.88% and\n10.95%, respectively.",
    "published_date": "2020-09-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05283v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.05184v1",
    "title": "STEP-GAN: A Step-by-Step Training for Multi Generator GANs with application to Cyber Security in Power Systems",
    "authors": [
      "Mohammad Adiban",
      "Arash Safari",
      "Giampiero Salvi"
    ],
    "author_ids": [],
    "abstract": "In this study, we introduce a novel unsupervised countermeasure for smart\ngrid power systems, based on generative adversarial networks (GANs). Given the\npivotal role of smart grid systems (SGSs) in urban life, their security is of\nparticular importance. In recent years, however, advances in the field of\nmachine learning, have raised concerns about cyber attacks on these systems.\nPower systems, among the most important components of urban infrastructure,\nhave, for example, been widely attacked by adversaries. Attackers disrupt power\nsystems using false data injection attacks (FDIA), resulting in a breach of\navailability, integrity, or confidential principles of the system. Our model\nsimulates possible attacks on power systems using multiple generators in a\nstep-by-step interaction with a discriminator in the training phase. As a\nconsequence, our system is robust to unseen attacks. Moreover, the proposed\nmodel considerably reduces the well-known mode collapse problem of GAN-based\nmodels. Our method is general and it can be potentially employed in a wide\nrange of one of one-class classification tasks. The proposed model has low\ncomputational complexity and outperforms baseline systems about 14% and 41% in\nterms of accuracy on the highly imbalanced publicly available industrial\ncontrol system (ICS) cyber attack power system dataset.",
    "published_date": "2020-09-11T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05184v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.06389v3",
    "title": "Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy",
    "authors": [
      "Tom Farrand",
      "Fatemehsadat Mireshghallah",
      "Sahib Singh",
      "Andrew Trask"
    ],
    "author_ids": [],
    "abstract": "Deployment of deep learning in different fields and industries is growing day\nby day due to its performance, which relies on the availability of data and\ncompute. Data is often crowd-sourced and contains sensitive information about\nits contributors, which leaks into models that are trained on it. To achieve\nrigorous privacy guarantees, differentially private training mechanisms are\nused. However, it has recently been shown that differential privacy can\nexacerbate existing biases in the data and have disparate impacts on the\naccuracy of different subgroups of data. In this paper, we aim to study these\neffects within differentially private deep learning. Specifically, we aim to\nstudy how different levels of imbalance in the data affect the accuracy and the\nfairness of the decisions made by the model, given different levels of privacy.\nWe demonstrate that even small imbalances and loose privacy guarantees can\ncause disparate impacts.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06389v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.05021v1",
    "title": "Investigating Gender Bias in BERT",
    "authors": [
      "Rishabh Bhardwaj",
      "Navonil Majumder",
      "Soujanya Poria"
    ],
    "author_ids": [],
    "abstract": "Contextual language models (CLMs) have pushed the NLP benchmarks to a new\nheight. It has become a new norm to utilize CLM provided word embeddings in\ndownstream tasks such as text classification. However, unless addressed, CLMs\nare prone to learn intrinsic gender-bias in the dataset. As a result,\npredictions of downstream NLP models can vary noticeably by varying gender\nwords, such as replacing \"he\" to \"she\", or even gender-neutral words. In this\npaper, we focus our analysis on a popular CLM, i.e., BERT. We analyse the\ngender-bias it induces in five downstream tasks related to emotion and\nsentiment intensity prediction. For each task, we train a simple regressor\nutilizing BERT's word embeddings. We then evaluate the gender-bias in\nregressors using an equity evaluation corpus. Ideally and from the specific\ndesign, the models should discard gender informative features from the input.\nHowever, the results show a significant dependence of the system's predictions\non gender-particular words and phrases. We claim that such biases can be\nreduced by removing genderspecific features from word embedding. Hence, for\neach layer in BERT, we identify directions that primarily encode gender\ninformation. The space formed by such directions is referred to as the gender\nsubspace in the semantic space of word embeddings. We propose an algorithm that\nfinds fine-grained gender directions, i.e., one primary direction for each BERT\nlayer. This obviates the need of realizing gender subspace in multiple\ndimensions and prevents other crucial information from being omitted.\nExperiments show that removing embedding components in such directions achieves\ngreat success in reducing BERT-induced bias in the downstream tasks.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05021v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.05015v1",
    "title": "Bias Variance Tradeoff in Analysis of Online Controlled Experiments",
    "authors": [
      "Ali Mahmoudzadeh",
      "Sophia Liu",
      "Sol Sadeghi",
      "Paul Luo Li",
      "Somit Gupta"
    ],
    "author_ids": [],
    "abstract": "Many organizations utilize large-scale online controlled experiments (OCEs)\nto accelerate innovation. Having high statistical power to detect small\ndifferences between control and treatment accurately is critical, as even small\nchanges in key metrics can be worth millions of dollars or indicate user\ndissatisfaction for a very large number of users. For large-scale OCE, the\nduration is typically short (e.g. two weeks) to expedite changes and\nimprovements to the product. In this paper, we examine two common approaches\nfor analyzing usage data collected from users within the time window of an\nexperiment, which can differ in accuracy and power. The open approach includes\nall relevant usage data from all active users for the entire duration of the\nexperiment. The bounded approach includes data from a fixed period of\nobservation for each user (e.g. seven days after exposure) after the first time\na user became active in the experiment window.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.SE",
      "62K99"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05015v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.09795v2",
    "title": "Biases in Data Science Lifecycle",
    "authors": [
      "Dinh-An Ho",
      "Oya Beyan"
    ],
    "author_ids": [],
    "abstract": "In recent years, data science has become an indispensable part of our\nsociety. Over time, we have become reliant on this technology because of its\nopportunity to gain value and new insights from data in any field - business,\nsocializing, research and society. At the same time, it raises questions about\nhow justified we are in placing our trust in these technologies. There is a\nrisk that such powers may lead to biased, inappropriate or unintended actions.\nTherefore, ethical considerations which might occur as the result of data\nscience practices should be carefully considered and these potential problems\nshould be identified during the data science lifecycle and mitigated if\npossible. However, a typical data scientist has not enough knowledge for\nidentifying these challenges and it is not always possible to include an ethics\nexpert during data science production. The aim of this study is to provide a\npractical guideline to data scientists and increase their awareness. In this\nwork, we reviewed different sources of biases and grouped them under different\nstages of the data science lifecycle. The work is still under progress. The aim\nof early publishing is to collect community feedback and improve the curated\nknowledge base for bias types and solutions.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09795v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.04822v2",
    "title": "Generalized Multi-Output Gaussian Process Censored Regression",
    "authors": [
      "Daniele Gammelli",
      "Kasper Pryds Rolsted",
      "Dario Pacino",
      "Filipe Rodrigues"
    ],
    "author_ids": [],
    "abstract": "When modelling censored observations, a typical approach in current\nregression methods is to use a censored-Gaussian (i.e. Tobit) model to describe\nthe conditional output distribution. In this paper, as in the case of missing\ndata, we argue that exploiting correlations between multiple outputs can enable\nmodels to better address the bias introduced by censored data. To do so, we\nintroduce a heteroscedastic multi-output Gaussian process model which combines\nthe non-parametric flexibility of GPs with the ability to leverage information\nfrom correlated outputs under input-dependent noise conditions. To address the\nresulting inference intractability, we further devise a variational bound to\nthe marginal log-likelihood suitable for stochastic optimization. We\nempirically evaluate our model against other generative models for censored\ndata on both synthetic and real world tasks and further show how it can be\ngeneralized to deal with arbitrary likelihood functions. Results show how the\nadded flexibility allows our model to better estimate the underlying\nnon-censored (i.e. true) process under potentially complex censoring dynamics.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04822v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.09936v1",
    "title": "Prune Responsibly",
    "authors": [
      "Michela Paganini"
    ],
    "author_ids": [],
    "abstract": "Irrespective of the specific definition of fairness in a machine learning\napplication, pruning the underlying model affects it. We investigate and\ndocument the emergence and exacerbation of undesirable per-class performance\nimbalances, across tasks and architectures, for almost one million categories\nconsidered across over 100K image classification models that undergo a pruning\nprocess.We demonstrate the need for transparent reporting, inclusive of bias,\nfairness, and inclusion metrics, in real-life engineering decision-making\naround neural network pruning. In response to the calls for quantitative\nevaluation of AI models to be population-aware, we present neural network\npruning as a tangible application domain where the ways in which\naccuracy-efficiency trade-offs disproportionately affect underrepresented or\noutlier groups have historically been overlooked. We provide a simple,\nPareto-based framework to insert fairness considerations into value-based\noperating point selection processes, and to re-evaluate pruning technique\nchoices.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.09936v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.04661v1",
    "title": "A Framework for Fairer Machine Learning in Organizations",
    "authors": [
      "Lily Morse",
      "Mike H. M. Teodorescu",
      "Yazeed Awwad",
      "Gerald Kane"
    ],
    "author_ids": [],
    "abstract": "With the increase in adoption of machine learning tools by organizations\nrisks of unfairness abound, especially when human decision processes in\noutcomes of socio-economic importance such as hiring, housing, lending, and\nadmissions are automated. We reveal sources of unfair machine learning, review\nfairness criteria, and provide a framework which, if implemented, would enable\nan organization to both avoid implementing an unfair machine learning model,\nbut also to avoid the common situation that as an algorithm learns with more\ndata it can become unfair over time. Issues of behavioral ethics in machine\nlearning implementations by organizations have not been thoroughly addressed in\nthe literature, because many of the necessary concepts are dispersed across\nthree literatures: ethics, machine learning, and management. Further, tradeoffs\nbetween fairness criteria in machine learning have not been addressed with\nregards to organizations. We advance the research by introducing an organizing\nframework for selecting and implementing fair algorithms in organizations.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "68T05",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04661v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.04640v2",
    "title": "On the Fairness of 'Fake' Data in Legal AI",
    "authors": [
      "Lauren Boswell",
      "Arjun Prakash"
    ],
    "author_ids": [],
    "abstract": "The economics of smaller budgets and larger case numbers necessitates the use\nof AI in legal proceedings. We examine the concept of disparate impact and how\nbiases in the training data lead to the search for fairer AI. This paper seeks\nto begin the discourse on what such an implementation would actually look like\nwith a criticism of pre-processing methods in a legal context . We outline how\npre-processing is used to correct biased data and then examine the legal\nimplications of effectively changing cases in order to achieve a fairer outcome\nincluding the black box problem and the slow encroachment on legal precedent.\nFinally we present recommendations on how to avoid the pitfalls of\npre-processed data with methods that either modify the classifier or correct\nthe output in the final step.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04640v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.04612v1",
    "title": "How Political is the Spread of COVID-19 in the United States? An Analysis using Transportation and Weather Data",
    "authors": [
      "Karan Vombatkere",
      "Hanjia Lyu",
      "Jiebo Luo"
    ],
    "author_ids": [],
    "abstract": "We investigate the difference in the spread of COVID-19 between the states\nwon by Donald Trump (Red) and the states won by Hillary Clinton (Blue) in the\n2016 presidential election, by mining transportation patterns of US residents\nfrom March 2020 to July 2020. To ensure a fair comparison, we first use a\nK-means clustering method to group the 50 states into five clusters according\nto their population, area and population density. We then characterize daily\ntransportation patterns of the residents of different states using the mean\npercentage of residents traveling and the number of trips per person. For each\nstate, we study the correlations between travel patterns and infection rate for\na 2-month period before and after the official states reopening dates. We\nobserve that during the lock-down, Red and Blue states both displayed strong\npositive correlations between their travel patterns and infection rates.\nHowever, after states reopened we find that Red states had higher\ntravel-infection correlations than Blue states in all five state clusters. We\nfind that the residents of both Red and Blue states displayed similar travel\npatterns during the period post the reopening of states, leading us to conclude\nthat, on average, the residents in Red states might be mobilizing less safely\nthan the residents in Blue states. Furthermore, we use temperature data to\nattempt to explain the difference in the way residents travel and practice\nsafety measures.",
    "published_date": "2020-09-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04612v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.04442v1",
    "title": "From Two-Class Linear Discriminant Analysis to Interpretable Multilayer Perceptron Design",
    "authors": [
      "Ruiyuan Lin",
      "Zhiruo Zhou",
      "Suya You",
      "Raghuveer Rao",
      "C. -C. Jay Kuo"
    ],
    "author_ids": [],
    "abstract": "A closed-form solution exists in two-class linear discriminant analysis\n(LDA), which discriminates two Gaussian-distributed classes in a\nmulti-dimensional feature space. In this work, we interpret the multilayer\nperceptron (MLP) as a generalization of a two-class LDA system so that it can\nhandle an input composed by multiple Gaussian modalities belonging to multiple\nclasses. Besides input layer $l_{in}$ and output layer $l_{out}$, the MLP of\ninterest consists of two intermediate layers, $l_1$ and $l_2$. We propose a\nfeedforward design that has three stages: 1) from $l_{in}$ to $l_1$: half-space\npartitionings accomplished by multiple parallel LDAs, 2) from $l_1$ to $l_2$:\nsubspace isolation where one Gaussian modality is represented by one neuron, 3)\nfrom $l_2$ to $l_{out}$: class-wise subspace mergence, where each Gaussian\nmodality is connected to its target class. Through this process, we present an\nautomatic MLP design that can specify the network architecture (i.e., the layer\nnumber and the neuron number at a layer) and all filter weights in a\nfeedforward one-pass fashion. This design can be generalized to an arbitrary\ndistribution by leveraging the Gaussian mixture model (GMM). Experiments are\nconducted to compare the performance of the traditional backpropagation-based\nMLP (BP-MLP) and the new feedforward MLP (FF-MLP).",
    "published_date": "2020-09-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04442v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.04441v3",
    "title": "Addressing Fairness in Classification with a Model-Agnostic Multi-Objective Algorithm",
    "authors": [
      "Kirtan Padh",
      "Diego Antognini",
      "Emma Lejal Glaude",
      "Boi Faltings",
      "Claudiu Musat"
    ],
    "author_ids": [],
    "abstract": "The goal of fairness in classification is to learn a classifier that does not\ndiscriminate against groups of individuals based on sensitive attributes, such\nas race and gender. One approach to designing fair algorithms is to use\nrelaxations of fairness notions as regularization terms or in a constrained\noptimization problem. We observe that the hyperbolic tangent function can\napproximate the indicator function. We leverage this property to define a\ndifferentiable relaxation that approximates fairness notions provably better\nthan existing relaxations. In addition, we propose a model-agnostic\nmulti-objective architecture that can simultaneously optimize for multiple\nfairness notions and multiple sensitive attributes and supports all statistical\nparity-based notions of fairness. We use our relaxation with the\nmulti-objective architecture to learn fair classifiers. Experiments on public\ndatasets show that our method suffers a significantly lower loss of accuracy\nthan current debiasing algorithms relative to the unconstrained model.",
    "published_date": "2020-09-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04441v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.04383v1",
    "title": "On the Identification of Fair Auditors to Evaluate Recommender Systems based on a Novel Non-Comparative Fairness Notion",
    "authors": [
      "Mukund Telukunta",
      "Venkata Sriram Siddhardh Nadendla"
    ],
    "author_ids": [],
    "abstract": "Decision-support systems are information systems that offer support to\npeople's decisions in various applications such as judiciary, real-estate and\nbanking sectors. Lately, these support systems have been found to be\ndiscriminatory in the context of many practical deployments. In an attempt to\nevaluate and mitigate these biases, algorithmic fairness literature has been\nnurtured using notions of comparative justice, which relies primarily on\ncomparing two/more individuals or groups within the society that is supported\nby such systems. However, such a fairness notion is not very useful in the\nidentification of fair auditors who are hired to evaluate latent biases within\ndecision-support systems. As a solution, we introduce a paradigm shift in\nalgorithmic fairness via proposing a new fairness notion based on the principle\nof non-comparative justice. Assuming that the auditor makes fairness\nevaluations based on some (potentially unknown) desired properties of the\ndecision-support system, the proposed fairness notion compares the system's\noutcome with that of the auditor's desired outcome. We show that the proposed\nfairness notion also provides guarantees in terms of comparative fairness\nnotions by proving that any system can be deemed fair from the perspective of\ncomparative fairness (e.g. individual fairness and statistical parity) if it is\nnon-comparatively fair with respect to an auditor who has been deemed fair with\nrespect to the same fairness notions. We also show that the converse holds true\nin the context of individual fairness. A brief discussion is also presented\nregarding how our fairness notion can be used to identify fair and reliable\nauditors, and how we can use them to quantify biases in decision-support\nsystems.",
    "published_date": "2020-09-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.04383v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.03538v3",
    "title": "An IMM-based Decentralized Cooperative Localization with LoS and NLoS UWB Inter-agent Ranging",
    "authors": [
      "Jianan Zhu",
      "Solmaz S. Kia"
    ],
    "author_ids": [],
    "abstract": "This paper investigates an infra-structure free global localization of a\ngroup of communicating mobile agents (e.g., first responders or exploring\nrobots) via an ultra-wideband (UWB) inter-agent ranging aided dead-reckoning.\nWe propose a loosely coupled cooperative localization algorithm that acts as an\naugmentation atop the local dead-reckoning system of each mobile agent. This\naugmentation becomes active only when an agent wants to process a relative\nmeasurement it has taken. The main contribution of this paper is addressing the\nchallenges in the proper processing of the UWB range measurements in the\nframework of a loosely coupled cooperative localization. Even though UWB offers\na decimeter level accuracy in line-of-sight (LoS) ranging, its accuracy\ndegrades significantly in non-line-of-sight (NLoS) due to the significant\nunknown positive bias in the measurements. Thus, the measurement models for the\nUWB LoS and NLoS ranging conditions are different, and proper processing of\nNLoS measurements requires a bias compensation measure. We also show that, in\npractice, the measurement modal discriminators determine the type of UWB range\nmeasurements should be probabilistic. To take into account the probabilistic\nnature of the NLoS identifiers when processing UWB inter-agent ranging\nfeedback, we employ an interacting multiple model (IMM) estimator in our\nlocalization filter. We also propose a bias compensation method for NLoS UWB\nmeasurements. The effectiveness of our cooperative localization is demonstrated\nvia an experiment for a group of pedestrians who use UWB relative range\nmeasurements among themselves to improve their shoe-mounted INS geolocation.",
    "published_date": "2020-09-08T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.03538v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.03698v1",
    "title": "Efficient Quantification of Profile Matching Risk in Social Networks",
    "authors": [
      "Anisa Halimi",
      "Erman Ayday"
    ],
    "author_ids": [],
    "abstract": "Anonymous data sharing has been becoming more challenging in today's\ninterconnected digital world, especially for individuals that have both\nanonymous and identified online activities. The most prominent example of such\ndata sharing platforms today are online social networks (OSNs). Many\nindividuals have multiple profiles in different OSNs, including anonymous and\nidentified ones (depending on the nature of the OSN). Here, the privacy threat\nis profile matching: if an attacker links anonymous profiles of individuals to\ntheir real identities, it can obtain privacy-sensitive information which may\nhave serious consequences, such as discrimination or blackmailing. Therefore,\nit is very important to quantify and show to the OSN users the extent of this\nprivacy risk. Existing attempts to model profile matching in OSNs are\ninadequate and computationally inefficient for real-time risk quantification.\nThus, in this work, we develop algorithms to efficiently model and quantify\nprofile matching attacks in OSNs as a step towards real-time privacy risk\nquantification. For this, we model the profile matching problem using a graph\nand develop a belief propagation (BP)-based algorithm to solve this problem in\na significantly more efficient and accurate way compared to the\nstate-of-the-art. We evaluate the proposed framework on three real-life\ndatasets (including data from four different social networks) and show how\nusers' profiles in different OSNs can be matched efficiently and with high\nprobability. We show that the proposed model generation has linear complexity\nin terms of number of user pairs, which is significantly more efficient than\nthe state-of-the-art (which has cubic complexity). Furthermore, it provides\ncomparable accuracy, precision, and recall compared to state-of-the-art.",
    "published_date": "2020-09-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.03698v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.03183v1",
    "title": "Learning Unbiased Representations via Rényi Minimization",
    "authors": [
      "Vincent Grari",
      "Oualid El Hajouji",
      "Sylvain Lamprier",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "In recent years, significant work has been done to include fairness\nconstraints in the training objective of machine learning algorithms. Many\nstate-of the-art algorithms tackle this challenge by learning a fair\nrepresentation which captures all the relevant information to predict the\noutput Y while not containing any information about a sensitive attribute S. In\nthis paper, we propose an adversarial algorithm to learn unbiased\nrepresentations via the Hirschfeld-Gebelein-Renyi (HGR) maximal correlation\ncoefficient. We leverage recent work which has been done to estimate this\ncoefficient by learning deep neural network transformations and use it as a\nminmax game to penalize the intrinsic bias in a multi dimensional latent\nrepresentation. Compared to other dependence measures, the HGR coefficient\ncaptures more information about the non-linear dependencies with the sensitive\nvariable, making the algorithm more efficient in mitigating bias in the\nrepresentation. We empirically evaluate and compare our approach and\ndemonstrate significant improvements over existing works in the field.",
    "published_date": "2020-09-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.03183v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.02969v1",
    "title": "Palettailor: Discriminable Colorization for Categorical Data",
    "authors": [
      "Kecheng Lu",
      "Mi Feng",
      "Xin Chen",
      "Michael Sedlmair",
      "Oliver Deussen",
      "Dani Lischinski",
      "Zhanglin Cheng",
      "Yunhai Wang"
    ],
    "author_ids": [],
    "abstract": "We present an integrated approach for creating and assigning color palettes\nto different visualizations such as multi-class scatterplots, line, and bar\ncharts. While other methods separate the creation of colors from their\nassignment, our approach takes data characteristics into account to produce\ncolor palettes, which are then assigned in a way that fosters better visual\ndiscrimination of classes. To do so, we use a customized optimization based on\nsimulated annealing to maximize the combination of three carefully designed\ncolor scoring functions: point distinctness, name difference, and color\ndiscrimination. We compare our approach to state-ofthe-art palettes with a\ncontrolled user study for scatterplots and line charts, furthermore we\nperformed a case study. Our results show that Palettailor, as a fully-automated\napproach, generates color palettes with a higher discrimination quality than\nexisting approaches. The efficiency of our optimization allows us also to\nincorporate user modifications into the color selection process.",
    "published_date": "2020-09-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02969v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.02714v1",
    "title": "New Results on Delay Robustness of Consensus Algorithms",
    "authors": [
      "Anton V. Proskurnikov",
      "Guiseppe Calafiore"
    ],
    "author_ids": [],
    "abstract": "Consensus of autonomous agents is a benchmark problem in cooperative control.\nIn this paper, we consider standard continuous-time averaging consensus\npolicies (or Laplacian flows) over time-varying graphs and focus on robustness\nof consensus against communication delays. Such a robustness has been proved\nunder the assumption of uniform quasi-strong connectivity of the graph. It is\nknown, however, that the uniform connectivity is not necessary for consensus.\nFor instance, in the case of undirected graph and undelayed communication\nconsensus requires a much weaker condition of integral connectivity. In this\npaper, we show that the latter results remain valid in presence of unknown but\nbounded communication delays, furthermore, the condition of undirected graph\ncan be substantially relaxed and replaced by the conditions of\nnon-instantaneous type-symmetry. Furthermore, consensus can be proved for any\nfeasible solution of the delay differential inequalities associated to the\nconsensus algorithm. Such inequalities naturally arise in problems of\ncontainment control, distributed optimization and models of social dynamics.",
    "published_date": "2020-09-06T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.MA",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02714v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.02707v3",
    "title": "Synthesising Realistic Calcium Traces of Neuronal Populations Using GAN",
    "authors": [
      "Bryan M. Li",
      "Theoklitos Amvrosiadis",
      "Nathalie Rochefort",
      "Arno Onken"
    ],
    "author_ids": [],
    "abstract": "Calcium imaging has become a powerful and popular technique to monitor the\nactivity of large populations of neurons in vivo. However, for ethical\nconsiderations and despite recent technical developments, recordings are still\nconstrained to a limited number of trials and animals. This limits the amount\nof data available from individual experiments and hinders the development of\nanalysis techniques and models for more realistic sizes of neuronal\npopulations. The ability to artificially synthesize realistic neuronal calcium\nsignals could greatly alleviate this problem by scaling up the number of\ntrials. Here, we propose a Generative Adversarial Network (GAN) model to\ngenerate realistic calcium signals as seen in neuronal somata with calcium\nimaging. To this end, we propose CalciumGAN, a model based on the WaveGAN\narchitecture and train it on calcium fluorescent signals with the Wasserstein\ndistance. We test the model on artificial data with known ground-truth and show\nthat the distribution of the generated signals closely resembles the underlying\ndata distribution. Then, we train the model on real calcium traces recorded\nfrom the primary visual cortex of behaving mice and confirm that the\ndeconvolved spike trains match the statistics of the recorded data. Together,\nthese results demonstrate that our model can successfully generate realistic\ncalcium traces, thereby providing the means to augment existing datasets of\nneuronal activity for enhanced data exploration and modelling.",
    "published_date": "2020-09-06T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02707v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.02590v1",
    "title": "\"And the Winner Is...\": Dynamic Lotteries for Multi-group Fairness-Aware Recommendation",
    "authors": [
      "Nasim Sonboli",
      "Robin Burke",
      "Nicholas Mattei",
      "Farzad Eskandanian",
      "Tian Gao"
    ],
    "author_ids": [],
    "abstract": "As recommender systems are being designed and deployed for an increasing\nnumber of socially-consequential applications, it has become important to\nconsider what properties of fairness these systems exhibit. There has been\nconsiderable research on recommendation fairness. However, we argue that the\nprevious literature has been based on simple, uniform and often uni-dimensional\nnotions of fairness assumptions that do not recognize the real-world\ncomplexities of fairness-aware applications. In this paper, we explicitly\nrepresent the design decisions that enter into the trade-off between accuracy\nand fairness across multiply-defined and intersecting protected groups,\nsupporting multiple fairness metrics. The framework also allows the recommender\nto adjust its performance based on the historical view of recommendations that\nhave been delivered over a time horizon, dynamically rebalancing between\nfairness concerns. Within this framework, we formulate lottery-based mechanisms\nfor choosing between fairness concerns, and demonstrate their performance in\ntwo recommendation domains.",
    "published_date": "2020-09-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02590v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08952v1",
    "title": "HyperFair: A Soft Approach to Integrating Fairness Criteria",
    "authors": [
      "Charles Dickens",
      "Rishika Singh",
      "Lise Getoor"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are being employed across an increasingly diverse set of\ndomains that can potentially make a significant social and individual impact.\nFor this reason, considering fairness is a critical step in the design and\nevaluation of such systems. In this paper, we introduce HyperFair, a general\nframework for enforcing soft fairness constraints in a hybrid recommender\nsystem. HyperFair models integrate variations of fairness metrics as a\nregularization of a joint inference objective function. We implement our\napproach using probabilistic soft logic and show that it is particularly\nwell-suited for this task as it is expressive and structural constraints can be\nadded to the system in a concise and interpretable manner. We propose two ways\nto employ the methods we introduce: first as an extension of a probabilistic\nsoft logic recommender system template; second as a fair retrofitting technique\nthat can be used to improve the fairness of predictions from a black-box model.\nWe empirically validate our approach by implementing multiple HyperFair hybrid\nrecommenders and compare them to a state-of-the-art fair recommender. We also\nrun experiments showing the effectiveness of our methods for the task of\nretrofitting a black-box model and the trade-off between the amount of fairness\nenforced and the prediction performance.",
    "published_date": "2020-09-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08952v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.02437v2",
    "title": "GazeMAE: General Representations of Eye Movements using a Micro-Macro Autoencoder",
    "authors": [
      "Louise Gillian C. Bautista",
      "Prospero C. Naval Jr"
    ],
    "author_ids": [],
    "abstract": "Eye movements are intricate and dynamic events that contain a wealth of\ninformation about the subject and the stimuli. We propose an abstract\nrepresentation of eye movements that preserve the important nuances in gaze\nbehavior while being stimuli-agnostic. We consider eye movements as raw\nposition and velocity signals and train separate deep temporal convolutional\nautoencoders. The autoencoders learn micro-scale and macro-scale\nrepresentations that correspond to the fast and slow features of eye movements.\nWe evaluate the joint representations with a linear classifier fitted on\nvarious classification tasks. Our work accurately discriminates between gender\nand age groups, and outperforms previous works on biometrics and stimuli\nclasification. Further experiments highlight the validity and generalizability\nof this method, bringing eye tracking research closer to real-world\napplications.",
    "published_date": "2020-09-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02437v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.02423v1",
    "title": "A General Framework for Fairness in Multistakeholder Recommendations",
    "authors": [
      "Harshal A. Chaudhari",
      "Sangdi Lin",
      "Ondrej Linda"
    ],
    "author_ids": [],
    "abstract": "Contemporary recommender systems act as intermediaries on multi-sided\nplatforms serving high utility recommendations from sellers to buyers. Such\nsystems attempt to balance the objectives of multiple stakeholders including\nsellers, buyers, and the platform itself. The difficulty in providing\nrecommendations that maximize the utility for a buyer, while simultaneously\nrepresenting all the sellers on the platform has lead to many interesting\nresearch problems.Traditionally, they have been formulated as integer linear\nprograms which compute recommendations for all the buyers together in an\n\\emph{offline} fashion, by incorporating coverage constraints so that the\nindividual sellers are proportionally represented across all the recommended\nitems. Such approaches can lead to unforeseen biases wherein certain buyers\nconsistently receive low utility recommendations in order to meet the global\nseller coverage constraints. To remedy this situation, we propose a general\nformulation that incorporates seller coverage objectives alongside individual\nbuyer objectives in a real-time personalized recommender system. In addition,\nwe leverage highly scalable submodular optimization algorithms to provide\nrecommendations to each buyer with provable theoretical quality bounds.\nFurthermore, we empirically evaluate the efficacy of our approach using data\nfrom an online real-estate marketplace.",
    "published_date": "2020-09-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.IR",
      "I.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02423v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.02207v2",
    "title": "Fair and Useful Cohort Selection",
    "authors": [
      "Konstantina Bairaktari",
      "Paul Langton",
      "Huy L. Nguyen",
      "Niklas Smedemark-Margulies",
      "Jonathan Ullman"
    ],
    "author_ids": [],
    "abstract": "A challenge in fair algorithm design is that, while there are compelling\nnotions of individual fairness, these notions typically do not satisfy\ndesirable composition properties, and downstream applications based on fair\nclassifiers might not preserve fairness. To study fairness under composition,\nDwork and Ilvento introduced an archetypal problem called fair-cohort-selection\nproblem, where a single fair classifier is composed with itself to select a\ngroup of candidates of a given size, and proposed a solution to this problem.\nIn this work we design algorithms for selecting cohorts that not only preserve\nfairness, but also maximize the utility of the selected cohort under two\nnotions of utility that we introduce and motivate. We give optimal (or\napproximately optimal) polynomial-time algorithms for this problem in both an\noffline setting, and an online setting where candidates arrive one at a time\nand are classified as they arrive.",
    "published_date": "2020-09-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02207v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01972v1",
    "title": "Attribute Adaptive Margin Softmax Loss using Privileged Information",
    "authors": [
      "Seyed Mehdi Iranmanesh",
      "Ali Dabouei",
      "Nasser M. Nasrabadi"
    ],
    "author_ids": [],
    "abstract": "We present a novel framework to exploit privileged information for\nrecognition which is provided only during the training phase. Here, we focus on\nrecognition task where images are provided as the main view and soft biometric\ntraits (attributes) are provided as the privileged data (only available during\ntraining phase). We demonstrate that more discriminative feature space can be\nlearned by enforcing a deep network to adjust adaptive margins between classes\nutilizing attributes. This tight constraint also effectively reduces the class\nimbalance inherent in the local data neighborhood, thus carving more balanced\nclass boundaries locally and using feature space more efficiently. Extensive\nexperiments are performed on five different datasets and the results show the\nsuperiority of our method compared to the state-of-the-art models in both tasks\nof face recognition and person re-identification.",
    "published_date": "2020-09-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01972v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01970v2",
    "title": "Imitation of Success Leads to Cost of Living Mediated Fairness in the Ultimatum Game",
    "authors": [
      "Yunong Chen",
      "Andrew Belmonte",
      "Christopher Griffin"
    ],
    "author_ids": [],
    "abstract": "The mechanism behind the emergence of cooperation in both biological and\nsocial systems is currently not understood. In particular, human behavior in\nthe Ultimatum game is almost always irrational, preferring mutualistic sharing\nstrategies, while chimpanzees act rationally and selfishly. However, human\nbehavior varies with geographic and cultural differences leading to distinct\nbehaviors. In this paper, we analyze a social imitation model that incorporates\ninternal energy caches (e.g., food/money savings), cost of living, death, and\nreproduction. We show that when imitation (and death) occurs, a natural\ncorrelation between selfishness and cost of living emerges. However, in all\nsocieties that do not collapse, non-Nash sharing strategies emerge as the de\nfacto result of imitation. We explain these results by constructing a\nmean-field approximation of the internal energy cache informed by time-varying\ndistributions extracted from experimental data. Results from a meta-analysis on\ngeographically diverse ultimatum game studies in humans, show the proposed\nmodel captures some of the qualitative aspects of the real-world data and\nsuggests further experimentation.",
    "published_date": "2020-09-04T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01970v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.03184v1",
    "title": "A New Screening Method for COVID-19 based on Ocular Feature Recognition by Machine Learning Tools",
    "authors": [
      "Yanwei Fu",
      "Feng Li",
      "Wenxuan Wang",
      "Haicheng Tang",
      "Xuelin Qian",
      "Mengwei Gu",
      "Xiangyang Xue"
    ],
    "author_ids": [],
    "abstract": "The Coronavirus disease 2019 (COVID-19) has affected several million people.\nWith the outbreak of the epidemic, many researchers are devoting themselves to\nthe COVID-19 screening system. The standard practices for rapid risk screening\nof COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction).\nHowever, these methods demand professional efforts of the acquisition of CT\nimages and saliva samples, a certain amount of waiting time, and most\nimportantly prohibitive examination fee in some countries. Recently, some\nliteratures have shown that the COVID-19 patients usually accompanied by ocular\nmanifestations consistent with the conjunctivitis, including conjunctival\nhyperemia, chemosis, epiphora, or increased secretions. After more than four\nmonths study, we found that the confirmed cases of COVID-19 present the\nconsistent ocular pathological symbols; and we propose a new screening method\nof analyzing the eye-region images, captured by common CCD and CMOS cameras,\ncould reliably make a rapid risk screening of COVID-19 with very high accuracy.\nWe believe a system implementing such an algorithm should assist the triage\nmanagement or the clinical diagnosis. To further evaluate our algorithm and\napproved by the Ethics Committee of Shanghai public health clinic center of\nFudan University, we conduct a study of analyzing the eye-region images of 303\npatients (104 COVID-19, 131 pulmonary, and 68 ocular patients), as well as 136\nhealthy people. Remarkably, our results of COVID-19 patients in testing set\nconsistently present similar ocular pathological symbols; and very high testing\nresults have been achieved in terms of sensitivity and specificity. We hope\nthis study can be inspiring and helpful for encouraging more researches in this\ntopic.",
    "published_date": "2020-09-04T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.03184v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01798v2",
    "title": "Ramifications of Approximate Posterior Inference for Bayesian Deep Learning in Adversarial and Out-of-Distribution Settings",
    "authors": [
      "John Mitros",
      "Arjun Pakrashi",
      "Brian Mac Namee"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks have been successful in diverse discriminative\nclassification tasks, although, they are poorly calibrated often assigning high\nprobability to misclassified predictions. Potential consequences could lead to\ntrustworthiness and accountability of the models when deployed in real\napplications, where predictions are evaluated based on their confidence scores.\nExisting solutions suggest the benefits attained by combining deep neural\nnetworks and Bayesian inference to quantify uncertainty over the models'\npredictions for ambiguous datapoints. In this work we propose to validate and\ntest the efficacy of likelihood based models in the task of out of distribution\ndetection (OoD). Across different datasets and metrics we show that Bayesian\ndeep learning models on certain occasions marginally outperform conventional\nneural networks and in the event of minimal overlap between in/out distribution\nclasses, even the best models exhibit a reduction in AUC scores in detecting\nOoD data. Preliminary investigations indicate the potential inherent role of\nbias due to choices of initialisation, architecture or activation functions. We\nhypothesise that the sensitivity of neural networks to unseen inputs could be a\nmulti-factor phenomenon arising from the different architectural design choices\noften amplified by the curse of dimensionality. Furthermore, we perform a study\nto find the effect of the adversarial noise resistance methods on in and\nout-of-distribution performance, as well as, also investigate adversarial noise\nrobustness of Bayesian deep learners.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01798v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01772v2",
    "title": "Reading In-Between the Lines: An Analysis of Dissenter",
    "authors": [
      "Erik Rye",
      "Jeremy Blackburn",
      "Robert Beverly"
    ],
    "author_ids": [],
    "abstract": "Efforts by content creators and social networks to enforce legal and\npolicy-based norms, e.g. blocking hate speech and users, has driven the rise of\nunrestricted communication platforms. One such recent effort is Dissenter, a\nbrowser and web application that provides a conversational overlay for any web\npage. These conversations hide in plain sight - users of Dissenter can see and\nparticipate in this conversation, whereas visitors using other browsers are\noblivious to their existence. Further, the website and content owners have no\npower over the conversation as it resides in an overlay outside their control.\n  In this work, we obtain a history of Dissenter comments, users, and the\nwebsites being discussed, from the initial release of Dissenter in Feb. 2019\nthrough Apr. 2020 (14 months). Our corpus consists of approximately 1.68M\ncomments made by 101k users commenting on 588k distinct URLs. We first analyze\nmacro characteristics of the network, including the user-base, comment\ndistribution, and growth. We then use toxicity dictionaries, Perspective API,\nand a Natural Language Processing model to understand the nature of the\ncomments and measure the propensity of particular websites and content to\nelicit hateful and offensive Dissenter comments. Using curated rankings of\nmedia bias, we examine the conditional probability of hateful comments given\nleft and right-leaning content. Finally, we study Dissenter as a social\nnetwork, and identify a core group of users with high comment toxicity.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01772v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01715v2",
    "title": "Exploring Artist Gender Bias in Music Recommendation",
    "authors": [
      "Dougal Shakespeare",
      "Lorenzo Porcaro",
      "Emilia Gómez",
      "Carlos Castillo"
    ],
    "author_ids": [],
    "abstract": "Music Recommender Systems (mRS) are designed to give personalised and\nmeaningful recommendations of items (i.e. songs, playlists or artists) to a\nuser base, thereby reflecting and further complementing individual users'\nspecific music preferences. Whilst accuracy metrics have been widely applied to\nevaluate recommendations in mRS literature, evaluating a user's item utility\nfrom other impact-oriented perspectives, including their potential for\ndiscrimination, is still a novel evaluation practice in the music domain. In\nthis work, we center our attention on a specific phenomenon for which we want\nto estimate if mRS may exacerbate its impact: gender bias. Our work presents an\nexploratory study, analyzing the extent to which commonly deployed state of the\nart Collaborative Filtering(CF) algorithms may act to further increase or\ndecrease artist gender bias. To assess group biases introduced by CF, we deploy\na recently proposed metric of bias disparity on two listening event datasets:\nthe LFM-1b dataset, and the earlier constructed Celma's dataset. Our work\ntraces the causes of disparity to variations in input gender distributions and\nuser-item preferences, highlighting the effect such configurations can have on\nuser's gender bias after recommendation generation.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01715v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.01713v1",
    "title": "Unique Exams: Designing assessments for integrity and fairness",
    "authors": [
      "Gili Rusak",
      "Lisa Yan"
    ],
    "author_ids": [],
    "abstract": "Educators have faced new challenges in effective course assessment during the\nrecent, unprecedented shift to remote online learning during the COVID-19\npandemic. In place of typical proctored, timed exams, instructors must now\nrethink their methodology for assessing course-level learning goals. Are exams\nappropriate---or even feasible---in this new online, open-internet learning\nenvironment? In this experience paper, we discuss the unique exams framework:\nour framework for upholding exam integrity and student privacy. In our\nProbability for Computer Scientists Course at an R1 University, we developed\nautogenerated, unique exams where each student had the same four problem\nskeletons with unique numeric variations per problem. Without changing the\nprocess of the traditional exam, unique exams provide a layer of security for\nboth students and instructors about exam reliability for any classroom\nenvironment---in-person or online. In addition to sharing our experience\ndesigning unique exams, we also present a simple end-to-end tool and example\nquestion templates for different CS subjects that other instructors can adapt\nto their own courses.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01713v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.01534v3",
    "title": "Fairness in the Eyes of the Data: Certifying Machine-Learning Models",
    "authors": [
      "Shahar Segal",
      "Yossi Adi",
      "Benny Pinkas",
      "Carsten Baum",
      "Chaya Ganesh",
      "Joseph Keshet"
    ],
    "author_ids": [],
    "abstract": "We present a framework that allows to certify the fairness degree of a model\nbased on an interactive and privacy-preserving test. The framework verifies any\ntrained model, regardless of its training process and architecture. Thus, it\nallows us to evaluate any deep learning model on multiple fairness definitions\nempirically. We tackle two scenarios, where either the test data is privately\navailable only to the tester or is publicly known in advance, even to the model\ncreator. We investigate the soundness of the proposed approach using\ntheoretical analysis and present statistical guarantees for the interactive\ntest. Finally, we provide a cryptographic technique to automate fairness\ntesting and certified inference with only black-box access to the model at hand\nwhile hiding the participants' sensitive data.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01534v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01456v1",
    "title": "DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation Spaces",
    "authors": [
      "Minhyuk Sung",
      "Zhenyu Jiang",
      "Panos Achlioptas",
      "Niloy J. Mitra",
      "Leonidas J. Guibas"
    ],
    "author_ids": [],
    "abstract": "Shape deformation is an important component in any geometry processing\ntoolbox. The goal is to enable intuitive deformations of single or multiple\nshapes or to transfer example deformations to new shapes while preserving the\nplausibility of the deformed shape(s). Existing approaches assume access to\npoint-level or part-level correspondence or establish them in a preprocessing\nphase, thus limiting the scope and generality of such approaches. We propose\nDeformSyncNet, a new approach that allows consistent and synchronized shape\ndeformations without requiring explicit correspondence information.\nTechnically, we achieve this by encoding deformations into a class-specific\nidealized latent space while decoding them into an individual, model-specific\nlinear deformation action space, operating directly in 3D. The underlying\nencoding and decoding are performed by specialized (jointly trained) neural\nnetworks. By design, the inductive bias of our networks results in a\ndeformation space with several desirable properties, such as path invariance\nacross different deformation pathways, which are then also approximately\npreserved in real space. We qualitatively and quantitatively evaluate our\nframework against multiple alternative approaches and demonstrate improved\nperformance.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01456v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.01454v5",
    "title": "Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information",
    "authors": [
      "Enyan Dai",
      "Suhang Wang"
    ],
    "author_ids": [],
    "abstract": "Graph neural networks (GNNs) have shown great power in modeling graph\nstructured data. However, similar to other machine learning models, GNNs may\nmake predictions biased on protected sensitive attributes, e.g., skin color and\ngender. Because machine learning algorithms including GNNs are trained to\nreflect the distribution of the training data which often contains historical\nbias towards sensitive attributes. In addition, the discrimination in GNNs can\nbe magnified by graph structures and the message-passing mechanism. As a\nresult, the applications of GNNs in sensitive domains such as crime rate\nprediction would be largely limited. Though extensive studies of fair\nclassification have been conducted on i.i.d data, methods to address the\nproblem of discrimination on non-i.i.d data are rather limited. Furthermore,\nthe practical scenario of sparse annotations in sensitive attributes is rarely\nconsidered in existing works. Therefore, we study the novel and important\nproblem of learning fair GNNs with limited sensitive attribute information.\nFairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node\nclassification accuracy by leveraging graph structures and limited sensitive\ninformation. Our theoretical analysis shows that FairGNN can ensure the\nfairness of GNNs under mild conditions given limited nodes with known sensitive\nattributes. Extensive experiments on real-world datasets also demonstrate the\neffectiveness of FairGNN in debiasing and keeping high accuracy.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01454v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01442v2",
    "title": "FairXGBoost: Fairness-aware Classification in XGBoost",
    "authors": [
      "Srinivasan Ravichandran",
      "Drona Khurana",
      "Bharath Venkatesh",
      "Narayanan Unny Edakunni"
    ],
    "author_ids": [],
    "abstract": "Highly regulated domains such as finance have long favoured the use of\nmachine learning algorithms that are scalable, transparent, robust and yield\nbetter performance. One of the most prominent examples of such an algorithm is\nXGBoost. Meanwhile, there is also a growing interest in building fair and\nunbiased models in these regulated domains and numerous bias-mitigation\nalgorithms have been proposed to this end. However, most of these\nbias-mitigation methods are restricted to specific model families such as\nlogistic regression or support vector machine models, thus leaving modelers\nwith a difficult decision of choosing between fairness from the bias-mitigation\nalgorithms and scalability, transparency, performance from algorithms such as\nXGBoost. We aim to leverage the best of both worlds by proposing a fair variant\nof XGBoost that enjoys all the advantages of XGBoost, while also matching the\nlevels of fairness from the state-of-the-art bias-mitigation algorithms.\nFurthermore, the proposed solution requires very little in terms of changes to\nthe original XGBoost library, thus making it easy for adoption. We provide an\nempirical analysis of our proposed method on standard benchmark datasets used\nin the fairness community.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01442v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01438v1",
    "title": "Tasks Integrated Networks: Joint Detection and Retrieval for Image Search",
    "authors": [
      "Lei Zhang",
      "Zhenwei He",
      "Yi Yang",
      "Liang Wang",
      "Xinbo Gao"
    ],
    "author_ids": [],
    "abstract": "The traditional object retrieval task aims to learn a discriminative feature\nrepresentation with intra-similarity and inter-dissimilarity, which supposes\nthat the objects in an image are manually or automatically pre-cropped exactly.\nHowever, in many real-world searching scenarios (e.g., video surveillance), the\nobjects (e.g., persons, vehicles, etc.) are seldom accurately detected or\nannotated. Therefore, object-level retrieval becomes intractable without\nbounding-box annotation, which leads to a new but challenging topic, i.e.\nimage-level search. In this paper, to address the image search issue, we first\nintroduce an end-to-end Integrated Net (I-Net), which has three merits: 1) A\nSiamese architecture and an on-line pairing strategy for similar and dissimilar\nobjects in the given images are designed. 2) A novel on-line pairing (OLP) loss\nis introduced with a dynamic feature dictionary, which alleviates the\nmulti-task training stagnation problem, by automatically generating a number of\nnegative pairs to restrict the positives. 3) A hard example priority (HEP)\nbased softmax loss is proposed to improve the robustness of classification task\nby selecting hard categories. With the philosophy of divide and conquer, we\nfurther propose an improved I-Net, called DC-I-Net, which makes two new\ncontributions: 1) two modules are tailored to handle different tasks separately\nin the integrated framework, such that the task specification is guaranteed. 2)\nA class-center guided HEP loss (C2HEP) by exploiting the stored class centers\nis proposed, such that the intra-similarity and inter-dissimilarity can be\ncaptured for ultimate retrieval. Extensive experiments on famous image-level\nsearch oriented benchmark datasets demonstrate that the proposed DC-I-Net\noutperforms the state-of-the-art tasks-integrated and tasks-separated image\nsearch models.",
    "published_date": "2020-09-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01438v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01334v1",
    "title": "Gender Stereotype Reinforcement: Measuring the Gender Bias Conveyed by Ranking Algorithms",
    "authors": [
      "Alessandro Fabris",
      "Alberto Purpura",
      "Gianmaria Silvello",
      "Gian Antonio Susto"
    ],
    "author_ids": [],
    "abstract": "Search Engines (SE) have been shown to perpetuate well-known gender\nstereotypes identified in psychology literature and to influence users\naccordingly. Similar biases were found encoded in Word Embeddings (WEs) learned\nfrom large online corpora. In this context, we propose the Gender Stereotype\nReinforcement (GSR) measure, which quantifies the tendency of a SE to support\ngender stereotypes, leveraging gender-related information encoded in WEs.\nThrough the critical lens of construct validity, we validate the proposed\nmeasure on synthetic and real collections. Subsequently, we use GSR to compare\nwidely-used Information Retrieval ranking algorithms, including lexical,\nsemantic, and neural models. We check if and how ranking algorithms based on\nWEs inherit the biases of the underlying embeddings. We also consider the most\ncommon debiasing approaches for WEs proposed in the literature and test their\nimpact in terms of GSR and common performance measures. To the best of our\nknowledge, GSR is the first specifically tailored measure for IR, capable of\nquantifying representational harms.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01334v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.01311v2",
    "title": "Comparing Fair Ranking Metrics",
    "authors": [
      "Amifa Raj",
      "Michael D. Ekstrand"
    ],
    "author_ids": [],
    "abstract": "Ranked lists are frequently used by information retrieval (IR) systems to\npresent results believed to be relevant to the users information need. Fairness\nis a relatively new but important aspect of these rankings to measure, joining\na rich set of metrics that go beyond traditional accuracy or utility constructs\nto provide a more holistic understanding of IR system behavior. In the last few\nyears, several metrics have been proposed to quantify the (un)fairness of\nrankings, particularly with respect to particular group(s) of content\nproviders, but comparative analyses of these metrics -- particularly for IR --\nis lacking. There is limited guidance, therefore, to decide what fairness\nmetrics are applicable to a specific scenario, or assessment of the extent to\nwhich metrics agree or disagree applied to real data. In this paper, we\ndescribe several fair ranking metrics from existing literature in a common\nnotation, enabling direct comparison of their assumptions, goals, and design\nchoices; we then empirically compare them on multiple data sets covering both\nsearch and recommendation tasks.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01311v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.01272v4",
    "title": "Understanding the wiring evolution in differentiable neural architecture search",
    "authors": [
      "Sirui Xie",
      "Shoukang Hu",
      "Xinjiang Wang",
      "Chunxiao Liu",
      "Jianping Shi",
      "Xunying Liu",
      "Dahua Lin"
    ],
    "author_ids": [],
    "abstract": "Controversy exists on whether differentiable neural architecture search\nmethods discover wiring topology effectively. To understand how wiring topology\nevolves, we study the underlying mechanism of several existing differentiable\nNAS frameworks. Our investigation is motivated by three observed searching\npatterns of differentiable NAS: 1) they search by growing instead of pruning;\n2) wider networks are more preferred than deeper ones; 3) no edges are selected\nin bi-level optimization. To anatomize these phenomena, we propose a unified\nview on searching algorithms of existing frameworks, transferring the global\noptimization to local cost minimization. Based on this reformulation, we\nconduct empirical and theoretical analyses, revealing implicit inductive biases\nin the cost's assignment mechanism and evolution dynamics that cause the\nobserved phenomena. These biases indicate strong discrimination towards certain\ntopologies. To this end, we pose questions that future differentiable methods\nfor neural wiring discovery need to confront, hoping to evoke a discussion and\nrethinking on how much bias has been enforced implicitly in existing NAS\nmethods.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01272v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.01133v2",
    "title": "Bound-preserving flux limiting for high-order explicit Runge-Kutta time discretizations of hyperbolic conservation laws",
    "authors": [
      "Dmitri Kuzmin",
      "Manuel Quezada de Luna",
      "David I. Ketcheson",
      "Johanna Grüll"
    ],
    "author_ids": [],
    "abstract": "We introduce a general framework for enforcing local or global maximum\nprinciples in high-order space-time discretizations of a scalar hyperbolic\nconservation law. We begin with sufficient conditions for a space\ndiscretization to be bound preserving (BP) and satisfy a semi-discrete maximum\nprinciple. Next, we propose a global monolithic convex (GMC) flux limiter which\nhas the structure of a flux-corrected transport (FCT) algorithm but is\napplicable to spatial semi-discretizations and ensures the BP property of the\nfully discrete scheme for strong stability preserving (SSP) Runge-Kutta time\ndiscretizations. To circumvent the order barrier for SSP time integrators, we\nconstrain the intermediate stages and/or the final stage of a general\nhigh-order RK method using GMC-type limiters. In this work, our theoretical and\nnumerical studies are restricted to explicit schemes which are provably BP for\nsufficiently small time steps. The new GMC limiting framework offers the\npossibility of relaxing the bounds of inequality constraints to achieve higher\naccuracy at the cost of more stringent time step restrictions. The ability of\nthe presented limiters to preserve global bounds and recognize well-resolved\nsmooth solutions is verified numerically for three representative RK methods\ncombined with weighted essentially nonoscillatory (WENO) finite volume space\ndiscretizations of linear and nonlinear test problems in 1D.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01133v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.00971v2",
    "title": "Coalgebraic Reasoning with Global Assumptions in Arithmetic Modal Logics",
    "authors": [
      "Clemens Kupke",
      "Dirk Pattinson",
      "Lutz Schröder"
    ],
    "author_ids": [],
    "abstract": "We establish a generic upper bound ExpTime for reasoning with global\nassumptions (also known as TBoxes) in coalgebraic modal logics. Unlike earlier\nresults of this kind, our bound does not require a tractable set of tableau\nrules for the instance logics, so that the result applies to wider classes of\nlogics. Examples are Presburger modal logic, which extends graded modal logic\nwith linear inequalities over numbers of successors, and probabilistic modal\nlogic with polynomial inequalities over probabilities. We establish the\ntheoretical upper bound using a type elimination algorithm. We also provide a\nglobal caching algorithm that potentially avoids building the entire\nexponential-sized space of candidate states, and thus offers a basis for\npractical reasoning. This algorithm still involves frequent fixpoint\ncomputations; we show how these can be handled efficiently in a concrete\nalgorithm modelled on Liu and Smolka's linear-time fixpoint algorithm. Finally,\nwe show that the upper complexity bound is preserved under adding nominals to\nthe logic, i.e. in coalgebraic hybrid logic.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LO",
      "03B70, 03B45, 03B35",
      "F.4.1; I.2.3; I.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.00971v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.00893v1",
    "title": "PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation",
    "authors": [
      "Shaotian Yan",
      "Chen Shen",
      "Zhongming Jin",
      "Jianqiang Huang",
      "Rongxin Jiang",
      "Yaowu Chen",
      "Xian-Sheng Hua"
    ],
    "author_ids": [],
    "abstract": "Today, scene graph generation(SGG) task is largely limited in realistic\nscenarios, mainly due to the extremely long-tailed bias of predicate annotation\ndistribution. Thus, tackling the class imbalance trouble of SGG is critical and\nchallenging. In this paper, we first discover that when predicate labels have\nstrong correlation with each other, prevalent re-balancing strategies(e.g.,\nre-sampling and re-weighting) will give rise to either over-fitting the tail\ndata(e.g., bench sitting on sidewalk rather than on), or still suffering the\nadverse effect from the original uneven distribution(e.g., aggregating varied\nparked on/standing on/sitting on into on). We argue the principal reason is\nthat re-balancing strategies are sensitive to the frequencies of predicates yet\nblind to their relatedness, which may play a more important role to promote the\nlearning of predicate features. Therefore, we propose a novel\nPredicate-Correlation Perception Learning(PCPL for short) scheme to adaptively\nseek out appropriate loss weights by directly perceiving and utilizing the\ncorrelation among predicate classes. Moreover, our PCPL framework is further\nequipped with a graph encoder module to better extract context features.\nExtensive experiments on the benchmark VG150 dataset show that the proposed\nPCPL performs markedly better on tail classes while well-preserving the\nperformance on head ones, which significantly outperforms previous\nstate-of-the-art methods.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.00893v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.08955v1",
    "title": "Neural Fair Collaborative Filtering",
    "authors": [
      "Rashidul Islam",
      "Kamrun Naher Keya",
      "Ziqian Zeng",
      "Shimei Pan",
      "James Foulds"
    ],
    "author_ids": [],
    "abstract": "A growing proportion of human interactions are digitized on social media\nplatforms and subjected to algorithmic decision-making, and it has become\nincreasingly important to ensure fair treatment from these algorithms. In this\nwork, we investigate gender bias in collaborative-filtering recommender systems\ntrained on social media data. We develop neural fair collaborative filtering\n(NFCF), a practical framework for mitigating gender bias in recommending\nsensitive items (e.g. jobs, academic concentrations, or courses of study) using\na pre-training and fine-tuning approach to neural collaborative filtering,\naugmented with bias correction techniques. We show the utility of our methods\nfor gender de-biased career and college major recommendations on the MovieLens\ndataset and a Facebook dataset, respectively, and achieve better performance\nand fairer behavior than several state-of-the-art models.",
    "published_date": "2020-09-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08955v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.00335v1",
    "title": "Landscape of Machine Implemented Ethics",
    "authors": [
      "Vivek Nallur"
    ],
    "author_ids": [],
    "abstract": "This paper surveys the state-of-the-art in machine ethics, that is,\nconsiderations of how to implement ethical behaviour in robots, unmanned\nautonomous vehicles, or software systems. The emphasis is on covering the\nbreadth of ethical theories being considered by implementors, as well as the\nimplementation techniques being used. There is no consensus on which ethical\ntheory is best suited for any particular domain, nor is there any agreement on\nwhich technique is best placed to implement a particular theory. Another\nunresolved problem in these implementations of ethical theories is how to\nobjectively validate the implementations. The paper discusses the dilemmas\nbeing used as validating 'whetstones' and whether any alternative validation\nmechanism exists. Finally, it speculates that an intermediate step of creating\ndomain-specific ethics might be a possible stepping stone towards creating\nmachines that exhibit ethical behaviour.",
    "published_date": "2020-09-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.00335v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.00215v2",
    "title": "On The Usage Of Average Hausdorff Distance For Segmentation Performance Assessment: Hidden Bias When Used For Ranking",
    "authors": [
      "Orhun Utku Aydin",
      "Abdel Aziz Taha",
      "Adam Hilbert",
      "Ahmed A. Khalil",
      "Ivana Galinovic",
      "Jochen B. Fiebach",
      "Dietmar Frey",
      "Vince Istvan Madai"
    ],
    "author_ids": [],
    "abstract": "Average Hausdorff Distance (AVD) is a widely used performance measure to\ncalculate the distance between two point sets. In medical image segmentation,\nAVD is used to compare ground truth images with segmentation results allowing\ntheir ranking. We identified, however, a ranking bias of AVD making it less\nsuitable for segmentation ranking. To mitigate this bias, we present a modified\ncalculation of AVD that we have coined balanced AVD (bAVD). To simulate\nsegmentations for ranking, we manually created non-overlapping segmentation\nerrors common in cerebral vessel segmentation as our use-case. Adding the\ncreated errors consecutively and randomly to the ground truth, we created sets\nof simulated segmentations with increasing number of errors. Each set of\nsimulated segmentations was ranked using AVD and bAVD. We calculated the\nKendall-rank-correlation-coefficient between the segmentation ranking and the\nnumber of errors in each simulated segmentation. The rankings produced by bAVD\nhad a significantly higher average correlation (0.969) than those of AVD\n(0.847). In 200 total rankings, bAVD misranked 52 and AVD misranked 179\nsegmentations. Our proposed evaluation measure, bAVD, alleviates AVDs ranking\nbias making it more suitable for rankings and quality assessment of\nsegmentations.",
    "published_date": "2020-09-01T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.00215v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.00146v2",
    "title": "Nash Social Distancing Games with Equity Constraints: How Inequality Aversion Affects the Spread of Epidemics",
    "authors": [
      "Ioannis Kordonis",
      "Athanasios-Rafail Lagos",
      "George P. Papavassilopoulos"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a game-theoretic model describing voluntary social\ndistancing during the spread of an epidemic. The payoffs of the agents depend\non the social distancing they practice and on the probability of getting\ninfected. We consider two types of agents, the non-vulnerable agents who have a\nsmall cost if they get infected, and the vulnerable agents who have a higher\ncost. For the modeling of the epidemic outbreak, we consider a variant of the\nSIR (Susceptible-Infected-Removed) model involving populations of susceptible,\ninfected, and removed persons of vulnerable and non-vulnerable types. The Nash\nequilibria of this social distancing game are studied. The main contribution of\nthis work is the analysis of the case where the players, desiring to achieve a\nlow social inequality, pose a bound on the variance of the payoffs. In this\ncase, we introduce and characterize a notion of Generalized Nash Equilibrium\n(GNE) for games with a continuum of players. Through numerical studies, we show\nthat inequality constraints result in a slower spread of the epidemic and an\nimproved cost for the vulnerable players. Furthermore, it is possible that\ninequality constraints are beneficial for non-vulnerable players as well.",
    "published_date": "2020-08-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.00146v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.13710v1",
    "title": "Initial Classifier Weights Replay for Memoryless Class Incremental Learning",
    "authors": [
      "Eden Belouadah",
      "Adrian Popescu",
      "Ioannis Kanellos"
    ],
    "author_ids": [],
    "abstract": "Incremental Learning (IL) is useful when artificial systems need to deal with\nstreams of data and do not have access to all data at all times. The most\nchallenging setting requires a constant complexity of the deep model and an\nincremental model update without access to a bounded memory of past data. Then,\nthe representations of past classes are strongly affected by catastrophic\nforgetting. To mitigate its negative effect, an adapted fine tuning which\nincludes knowledge distillation is usually deployed. We propose a different\napproach based on a vanilla fine tuning backbone. It leverages initial\nclassifier weights which provide a strong representation of past classes\nbecause they are trained with all class data. However, the magnitude of\nclassifiers learned in different states varies and normalization is needed for\na fair handling of all classes. Normalization is performed by standardizing the\ninitial classifier weights, which are assumed to be normally distributed. In\naddition, a calibration of prediction scores is done by using state level\nstatistics to further improve classification fairness. We conduct a thorough\nevaluation with four public datasets in a memoryless incremental learning\nsetting. Results show that our method outperforms existing techniques by a\nlarge margin for large-scale datasets.",
    "published_date": "2020-08-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13710v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.13589v2",
    "title": "Biased Opinion Dynamics: When the Devil Is in the Details",
    "authors": [
      "Aris Anagnostopoulos",
      "Luca Becchetti",
      "Emilio Cruciani",
      "Francesco Pasquale",
      "Sara Rizzo"
    ],
    "author_ids": [],
    "abstract": "We investigate opinion dynamics in multi-agent networks when a bias toward\none of two possible opinions exists; for example, reflecting a status quo vs a\nsuperior alternative. Starting with all agents sharing an initial opinion\nrepresenting the status quo, the system evolves in steps. In each step, one\nagent selected uniformly at random adopts the superior opinion with some\nprobability $\\alpha$, and with probability $1 - \\alpha$ it follows an\nunderlying update rule to revise its opinion on the basis of those held by its\nneighbors. We analyze convergence of the resulting process under two well-known\nupdate rules, namely majority and voter. The framework we propose exhibits a\nrich structure, with a non-obvious interplay between topology and underlying\nupdate rule. For example, for the voter rule we show that the speed of\nconvergence bears no significant dependence on the underlying topology, whereas\nthe picture changes completely under the majority rule, where network density\nnegatively affects convergence. We believe that the model we propose is at the\nsame time simple, rich, and modular, affording mathematical characterization of\nthe interplay between bias, underlying opinion dynamics, and social structure\nin a unified setting.",
    "published_date": "2020-08-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MA",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13589v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.13404v1",
    "title": "Beyond Our Behavior: The GDPR and Humanistic Personalization",
    "authors": [
      "Travis Greene",
      "Galit Shmueli"
    ],
    "author_ids": [],
    "abstract": "Personalization should take the human person seriously. This requires a\ndeeper understanding of how recommender systems can shape both our\nself-understanding and identity. We unpack key European humanistic and\nphilosophical ideas underlying the General Data Protection Regulation (GDPR)\nand propose a new paradigm of humanistic personalization. Humanistic\npersonalization responds to the IEEE's call for Ethically Aligned Design (EAD)\nand is based on fundamental human capacities and values. Humanistic\npersonalization focuses on narrative accuracy: the subjective fit between a\nperson's self-narrative and both the input (personal data) and output of a\nrecommender system. In doing so, we re-frame the distinction between implicit\nand explicit data collection as one of nonconscious (\"organismic\") behavior and\nconscious (\"reflective\") action. This distinction raises important ethical and\ninterpretive issues related to agency, self-understanding, and political\nparticipation. Finally, we discuss how an emphasis on narrative accuracy can\nreduce opportunities for epistemic injustice done to data subjects.",
    "published_date": "2020-08-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13404v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.13369v1",
    "title": "Introducing Representations of Facial Affect in Automated Multimodal Deception Detection",
    "authors": [
      "Leena Mathur",
      "Maja J Matarić"
    ],
    "author_ids": [],
    "abstract": "Automated deception detection systems can enhance health, justice, and\nsecurity in society by helping humans detect deceivers in high-stakes\nsituations across medical and legal domains, among others. This paper presents\na novel analysis of the discriminative power of dimensional representations of\nfacial affect for automated deception detection, along with interpretable\nfeatures from visual, vocal, and verbal modalities. We used a video dataset of\npeople communicating truthfully or deceptively in real-world, high-stakes\ncourtroom situations. We leveraged recent advances in automated emotion\nrecognition in-the-wild by implementing a state-of-the-art deep neural network\ntrained on the Aff-Wild database to extract continuous representations of\nfacial valence and facial arousal from speakers. We experimented with unimodal\nSupport Vector Machines (SVM) and SVM-based multimodal fusion methods to\nidentify effective features, modalities, and modeling approaches for detecting\ndeception. Unimodal models trained on facial affect achieved an AUC of 80%, and\nfacial affect contributed towards the highest-performing multimodal approach\n(adaptive boosting) that achieved an AUC of 91% when tested on speakers who\nwere not part of training sets. This approach achieved a higher AUC than\nexisting automated machine learning approaches that used interpretable visual,\nvocal, and verbal features to detect deception in this dataset, but did not use\nfacial affect. Across all videos, deceptive and truthful speakers exhibited\nsignificant differences in facial valence and facial arousal, contributing\ncomputational support to existing psychological theories on affect and\ndeception. The demonstrated importance of facial affect in our models informs\nand motivates the future development of automated, affect-aware machine\nlearning approaches for modeling and detecting deception and other social\nbehaviors in-the-wild.",
    "published_date": "2020-08-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13369v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.13336v3",
    "title": "Shape Defense Against Adversarial Attacks",
    "authors": [
      "Ali Borji"
    ],
    "author_ids": [],
    "abstract": "Humans rely heavily on shape information to recognize objects. Conversely,\nconvolutional neural networks (CNNs) are biased more towards texture. This is\nperhaps the main reason why CNNs are vulnerable to adversarial examples. Here,\nwe explore how shape bias can be incorporated into CNNs to improve their\nrobustness. Two algorithms are proposed, based on the observation that edges\nare invariant to moderate imperceptible perturbations. In the first one, a\nclassifier is adversarially trained on images with the edge map as an\nadditional channel. At inference time, the edge map is recomputed and\nconcatenated to the image. In the second algorithm, a conditional GAN is\ntrained to translate the edge maps, from clean and/or perturbed images, into\nclean images. Inference is done over the generated image corresponding to the\ninput's edge map. Extensive experiments over 10 datasets demonstrate the\neffectiveness of the proposed algorithms against FGSM and $\\ell_\\infty$ PGD-40\nattacks. Further, we show that a) edge information can also benefit other\nadversarial training methods, and b) CNNs trained on edge-augmented inputs are\nmore robust against natural image corruptions such as motion blur, impulse\nnoise and JPEG compression, than CNNs trained solely on RGB images. From a\nbroader perspective, our study suggests that CNNs do not adequately account for\nimage structures that are crucial for robustness. Code is available\nat:~\\url{https://github.com/aliborji/Shapedefense.git}.",
    "published_date": "2020-08-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13336v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.13122v1",
    "title": "Adversarial Learning for Counterfactual Fairness",
    "authors": [
      "Vincent Grari",
      "Sylvain Lamprier",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "In recent years, fairness has become an important topic in the machine\nlearning research community. In particular, counterfactual fairness aims at\nbuilding prediction models which ensure fairness at the most individual level.\nRather than globally considering equity over the entire population, the idea is\nto imagine what any individual would look like with a variation of a given\nattribute of interest, such as a different gender or race for instance.\nExisting approaches rely on Variational Auto-encoding of individuals, using\nMaximum Mean Discrepancy (MMD) penalization to limit the statistical dependence\nof inferred representations with their corresponding sensitive attributes. This\nenables the simulation of counterfactual samples used for training the target\nfair model, the goal being to produce similar outcomes for every alternate\nversion of any individual. In this work, we propose to rely on an adversarial\nneural learning approach, that enables more powerful inference than with MMD\npenalties, and is particularly better fitted for the continuous setting, where\nvalues of sensitive attributes cannot be exhaustively enumerated. Experiments\nshow significant improvements in term of counterfactual fairness for both the\ndiscrete and the continuous settings.",
    "published_date": "2020-08-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13122v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.13002v1",
    "title": "Longitudinal Image Registration with Temporal-order and Subject-specificity Discrimination",
    "authors": [
      "Qianye Yang",
      "Yunguan Fu",
      "Francesco Giganti",
      "Nooshin Ghavami",
      "Qingchao Chen",
      "J. Alison Noble",
      "Tom Vercauteren",
      "Dean Barratt",
      "Yipeng Hu"
    ],
    "author_ids": [],
    "abstract": "Morphological analysis of longitudinal MR images plays a key role in\nmonitoring disease progression for prostate cancer patients, who are placed\nunder an active surveillance program. In this paper, we describe a\nlearning-based image registration algorithm to quantify changes on regions of\ninterest between a pair of images from the same patient, acquired at two\ndifferent time points. Combining intensity-based similarity and gland\nsegmentation as weak supervision, the population-data-trained registration\nnetworks significantly lowered the target registration errors (TREs) on holdout\npatient data, compared with those before registration and those from an\niterative registration algorithm. Furthermore, this work provides a\nquantitative analysis on several longitudinal-data-sampling strategies and, in\nturn, we propose a novel regularisation method based on maximum mean\ndiscrepancy, between differently-sampled training image pairs. Based on 216 3D\nMR images from 86 patients, we report a mean TRE of 5.6 mm and show\nstatistically significant differences between the different training data\nsampling strategies.",
    "published_date": "2020-08-29T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13002v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12965v2",
    "title": "Patch-based Brain Age Estimation from MR Images",
    "authors": [
      "Kyriaki-Margarita Bintsi",
      "Vasileios Baltatzis",
      "Arinbjörn Kolbeinsson",
      "Alexander Hammers",
      "Daniel Rueckert"
    ],
    "author_ids": [],
    "abstract": "Brain age estimation from Magnetic Resonance Images (MRI) derives the\ndifference between a subject's biological brain age and their chronological\nage. This is a potential biomarker for neurodegeneration, e.g. as part of\nAlzheimer's disease. Early detection of neurodegeneration manifesting as a\nhigher brain age can potentially facilitate better medical care and planning\nfor affected individuals. Many studies have been proposed for the prediction of\nchronological age from brain MRI using machine learning and specifically deep\nlearning techniques. Contrary to most studies, which use the whole brain\nvolume, in this study, we develop a new deep learning approach that uses 3D\npatches of the brain as well as convolutional neural networks (CNNs) to develop\na localised brain age estimator. In this way, we can obtain a visualization of\nthe regions that play the most important role for estimating brain age, leading\nto more anatomically driven and interpretable results, and thus confirming\nrelevant literature which suggests that the ventricles and the hippocampus are\nthe areas that are most informative. In addition, we leverage this knowledge in\norder to improve the overall performance on the task of age estimation by\ncombining the results of different patches using an ensemble method, such as\naveraging or linear regression. The network is trained on the UK Biobank\ndataset and the method achieves state-of-the-art results with a Mean Absolute\nError of 2.46 years for purely regional estimates, and 2.13 years for an\nensemble of patches before bias correction, while 1.96 years after bias\ncorrection.",
    "published_date": "2020-08-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12965v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12948v1",
    "title": "The evolution of trust and trustworthiness",
    "authors": [
      "Aanjaneya Kumar",
      "Valerio Capraro",
      "Matjaz Perc"
    ],
    "author_ids": [],
    "abstract": "Trust and trustworthiness form the basis for continued social and economic\ninteractions, and they are also fundamental for cooperation, fairness, honesty,\nand indeed for many other forms of prosocial and moral behavior. However, trust\nentails risks, and building a trustworthy reputation requires effort. So how\ndid trust and trustworthiness evolve, and under which conditions do they\nthrive? To find answers, we operationalize trust and trustworthiness using the\ntrust game with the trustor's investment and the trustee's return of the\ninvestment as the two key parameters. We study this game on different networks,\nincluding the complete network, random and scale-free networks, and in the\nwell-mixed limit. We show that in all but one case the network structure has\nlittle effect on the evolution of trust and trustworthiness. Specifically, for\nwell-mixed populations, lattices, random and scale-free networks, we find that\ntrust never evolves, while trustworthiness evolves with some probability\ndepending on the game parameters and the updating dynamics. Only for the\nscale-free network with degree non-normalized dynamics, we find parameter\nvalues for which trust evolves but trustworthiness does not, as well as values\nfor which both trust and trustworthiness evolve. We conclude with a discussion\nabout mechanisms that could lead to the evolution of trust and outline\ndirections for future work.",
    "published_date": "2020-08-29T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12948v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.12904v1",
    "title": "On segmentation of pectoralis muscle in digital mammograms by means of deep learning",
    "authors": [
      "Hossein Soleimani",
      "Oleg V. Michailovich"
    ],
    "author_ids": [],
    "abstract": "Computer-aided diagnosis (CAD) has long become an integral part of\nradiological management of breast disease, facilitating a number of important\nclinical applications, including quantitative assessment of breast density and\nearly detection of malignancies based on X-ray mammography. Common to such\napplications is the need to automatically discriminate between breast tissue\nand adjacent anatomy, with the latter being predominantly represented by\npectoralis major (or pectoral muscle). Especially in the case of mammograms\nacquired in the mediolateral oblique (MLO) view, the muscle is easily\nconfusable with some elements of breast anatomy due to their morphological and\nphotometric similarity. As a result, the problem of automatic detection and\nsegmentation of pectoral muscle in MLO mammograms remains a challenging task,\ninnovative approaches to which are still required and constantly searched for.\nTo address this problem, the present paper introduces a two-step segmentation\nstrategy based on a combined use of data-driven prediction (deep learning) and\ngraph-based image processing. In particular, the proposed method employs a\nconvolutional neural network (CNN) which is designed to predict the location of\nbreast-pectoral boundary at different levels of spatial resolution.\nSubsequently, the predictions are used by the second stage of the algorithm, in\nwhich the desired boundary is recovered as a solution to the shortest path\nproblem on a specially designed graph. The proposed algorithm has been tested\non three different datasets (i.e., MIAS, CBIS-DDSm and InBreast) using a range\nof quantitative metrics. The results of comparative analysis show considerable\nimprovement over state-of-the-art, while offering the possibility of model-free\nand fully automatic processing.",
    "published_date": "2020-08-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12904v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12829v2",
    "title": "A Rigorous Machine Learning Analysis Pipeline for Biomedical Binary Classification: Application in Pancreatic Cancer Nested Case-control Studies with Implications for Bias Assessments",
    "authors": [
      "Ryan J. Urbanowicz",
      "Pranshu Suri",
      "Yuhan Cui",
      "Jason H. Moore",
      "Karen Ruth",
      "Rachael Stolzenberg-Solomon",
      "Shannon M. Lynch"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) offers a collection of powerful approaches for\ndetecting and modeling associations, often applied to data having a large\nnumber of features and/or complex associations. Currently, there are many tools\nto facilitate implementing custom ML analyses (e.g. scikit-learn). Interest is\nalso increasing in automated ML packages, which can make it easier for\nnon-experts to apply ML and have the potential to improve model performance. ML\npermeates most subfields of biomedical research with varying levels of rigor\nand correct usage. Tremendous opportunities offered by ML are frequently offset\nby the challenge of assembling comprehensive analysis pipelines, and the ease\nof ML misuse. In this work we have laid out and assembled a complete, rigorous\nML analysis pipeline focused on binary classification (i.e. case/control\nprediction), and applied this pipeline to both simulated and real world data.\nAt a high level, this 'automated' but customizable pipeline includes a)\nexploratory analysis, b) data cleaning and transformation, c) feature\nselection, d) model training with 9 established ML algorithms, each with\nhyperparameter optimization, and e) thorough evaluation, including appropriate\nmetrics, statistical analyses, and novel visualizations. This pipeline\norganizes the many subtle complexities of ML pipeline assembly to illustrate\nbest practices to avoid bias and ensure reproducibility. Additionally, this\npipeline is the first to compare established ML algorithms to 'ExSTraCS', a\nrule-based ML algorithm with the unique capability of interpretably modeling\nheterogeneous patterns of association. While designed to be widely applicable\nwe apply this pipeline to an epidemiological investigation of established and\nnewly identified risk factors for pancreatic cancer to evaluate how different\nsources of bias might be handled by ML algorithms.",
    "published_date": "2020-08-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12829v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.13632v1",
    "title": "TRUSTD: Combat Fake Content using Blockchain and Collective Signature Technologies",
    "authors": [
      "Zakwan Jaroucheh",
      "Mohamad Alissa",
      "William J Buchanan"
    ],
    "author_ids": [],
    "abstract": "The growing trend of sharing news/contents, through social media platforms\nand the World Wide Web has been seen to impact our perception of the truth,\naltering our views about politics, economics, relationships, needs and wants.\nThis is because of the growing spread of misinformation and disinformation\nintentionally or unintentionally by individuals and organizations. This trend\nhas grave political, social, ethical, and privacy implications for society due\nto 1) the rapid developments in the field of Machine Learning (ML) and Deep\nLearning (DL) algorithms in creating realistic-looking yet fake digital content\n(such as text, images, and videos), 2) the ability to customize the content\nfeeds and to create a polarized so-called \"filter-bubbles\" leveraging the\navailability of the big-data. Therefore, there is an ethical need to combat the\nflow of fake content. This paper attempts to resolve some of the aspects of\nthis combat by presenting a high-level overview of TRUSTD, a blockchain and\ncollective signature-based ecosystem to help content creators in getting their\ncontent backed by the community, and to help users judge on the credibility and\ncorrectness of these contents.",
    "published_date": "2020-08-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.13632v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12528v1",
    "title": "Researcher Bias in Software Engineering Experiments: a Qualitative Investigation",
    "authors": [
      "Simone Romano",
      "Davide Fucci",
      "Giuseppe Scanniello",
      "Maria Teresa Baldassarre",
      "Burak Turhan",
      "Natalia Juristo"
    ],
    "author_ids": [],
    "abstract": "Researcher Bias (RB) occurs when researchers influence the results of an\nempirical study based on their expectations.RB might be due to the use of\nQuestionable Research Practices(QRPs). In research fields like medicine,\nblinding techniques have been applied to counteract RB. We conducted an\nexplorative qualitative survey to investigate RB in Software Engineering\n(SE)experiments, with respect to (i) QRPs potentially leading to RB, (ii)\ncauses behind RB, and (iii) possible actions to counteract including blinding\ntechniques. Data collection was based on semi-structured interviews. We\ninterviewed nine active experts in the empirical SE community. We then analyzed\nthe transcripts of these interviews through thematic analysis. We found that\nsome QRPs are acceptable in certain cases. Also, it appears that the presence\nof RB is perceived in SE and, to counteract RB, a number of solutions have been\nhighlighted: some are intended for SE researchers and others for the boards of\nSE research outlets.",
    "published_date": "2020-08-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12528v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.12438v1",
    "title": "Exact and Approximation Algorithms for Sparse PCA",
    "authors": [
      "Yongchun Li",
      "Weijun Xie"
    ],
    "author_ids": [],
    "abstract": "Sparse PCA (SPCA) is a fundamental model in machine learning and data\nanalytics, which has witnessed a variety of application areas such as finance,\nmanufacturing, biology, healthcare. To select a prespecified-size principal\nsubmatrix from a covariance matrix to maximize its largest eigenvalue for the\nbetter interpretability purpose, SPCA advances the conventional PCA with both\nfeature selection and dimensionality reduction. This paper proposes two exact\nmixed-integer SDPs (MISDPs) by exploiting the spectral decomposition of the\ncovariance matrix and the properties of the largest eigenvalues. We then\nanalyze the theoretical optimality gaps of their continuous relaxation values\nand prove that they are stronger than that of the state-of-art one. We further\nshow that the continuous relaxations of two MISDPs can be recast as saddle\npoint problems without involving semi-definite cones, and thus can be\neffectively solved by first-order methods such as the subgradient method. Since\noff-the-shelf solvers, in general, have difficulty in solving MISDPs, we\napproximate SPCA with arbitrary accuracy by a mixed-integer linear program\n(MILP) of a similar size as MISDPs. To be more scalable, we also analyze greedy\nand local search algorithms, prove their first-known approximation ratios, and\nshow that the approximation ratios are tight. Our numerical study demonstrates\nthat the continuous relaxation values of the proposed MISDPs are quite close to\noptimality, the proposed MILP model can solve small and medium-size instances\nto optimality, and the approximation algorithms work very well for all the\ninstances. Finally, we extend the analyses to Rank-one Sparse SVD (R1-SSVD)\nwith non-symmetric matrices and Sparse Fair PCA (SFPCA) when there are multiple\ncovariance matrices, each corresponding to a protected group.",
    "published_date": "2020-08-28T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12438v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12405v1",
    "title": "Adversarial Training for Multi-Channel Sign Language Production",
    "authors": [
      "Ben Saunders",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "author_ids": [],
    "abstract": "Sign Languages are rich multi-channel languages, requiring articulation of\nboth manual (hands) and non-manual (face and body) features in a precise,\nintricate manner. Sign Language Production (SLP), the automatic translation\nfrom spoken to sign languages, must embody this full sign morphology to be\ntruly understandable by the Deaf community. Previous work has mainly focused on\nmanual feature production, with an under-articulated output caused by\nregression to the mean.\n  In this paper, we propose an Adversarial Multi-Channel approach to SLP. We\nframe sign production as a minimax game between a transformer-based Generator\nand a conditional Discriminator. Our adversarial discriminator evaluates the\nrealism of sign production conditioned on the source text, pushing the\ngenerator towards a realistic and articulate output. Additionally, we fully\nencapsulate sign articulators with the inclusion of non-manual features,\nproducing facial features and mouthing patterns.\n  We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T)\ndataset, and report state-of-the art SLP back-translation performance for\nmanual production. We set new benchmarks for the production of multi-channel\nsign to underpin future research into realistic SLP.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12405v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2009.02267v1",
    "title": "Zero-Bias Deep Learning for Accurate Identification of Internet of Things (IoT) Devices",
    "authors": [
      "Yongxin Liu",
      "Jian Wang",
      "Jianqiang Li",
      "Houbing Song",
      "Thomas Yang",
      "Shuteng Niu",
      "Zhong Ming"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things (IoT) provides applications and services that would\notherwise not be possible. However, the open nature of IoT make it vulnerable\nto cybersecurity threats. Especially, identity spoofing attacks, where an\nadversary passively listens to existing radio communications and then mimic the\nidentity of legitimate devices to conduct malicious activities. Existing\nsolutions employ cryptographic signatures to verify the trustworthiness of\nreceived information. In prevalent IoT, secret keys for cryptography can\npotentially be disclosed and disable the verification mechanism.\nNon-cryptographic device verification is needed to ensure trustworthy IoT. In\nthis paper, we propose an enhanced deep learning framework for IoT device\nidentification using physical layer signals. Specifically, we enable our\nframework to report unseen IoT devices and introduce the zero-bias layer to\ndeep neural networks to increase robustness and interpretability. We have\nevaluated the effectiveness of the proposed framework using real data from\nADS-B (Automatic Dependent Surveillance-Broadcast), an application of IoT in\naviation. The proposed framework has the potential to be applied to accurate\nidentification of IoT devices in a variety of IoT applications and services.\nCodes and data are available in IEEE Dataport.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02267v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12371v1",
    "title": "Improving the Segmentation of Scanning Probe Microscope Images using Convolutional Neural Networks",
    "authors": [
      "Steff Farley",
      "Jo E. A. Hodgkinson",
      "Oliver M. Gordon",
      "Joanna Turner",
      "Andrea Soltoggio",
      "Philip J. Moriarty",
      "Eugenie Hunsicker"
    ],
    "author_ids": [],
    "abstract": "A wide range of techniques can be considered for segmentation of images of\nnanostructured surfaces. Manually segmenting these images is time-consuming and\nresults in a user-dependent segmentation bias, while there is currently no\nconsensus on the best automated segmentation methods for particular techniques,\nimage classes, and samples. Any image segmentation approach must minimise the\nnoise in the images to ensure accurate and meaningful statistical analysis can\nbe carried out. Here we develop protocols for the segmentation of images of 2D\nassemblies of gold nanoparticles formed on silicon surfaces via deposition from\nan organic solvent. The evaporation of the solvent drives far-from-equilibrium\nself-organisation of the particles, producing a wide variety of nano- and\nmicro-structured patterns. We show that a segmentation strategy using the U-Net\nconvolutional neural network outperforms traditional automated approaches and\nhas particular potential in the processing of images of nanostructured systems.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12260v2",
    "title": "Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning",
    "authors": [
      "Aurick Qiao",
      "Sang Keun Choe",
      "Suhas Jayaram Subramanya",
      "Willie Neiswanger",
      "Qirong Ho",
      "Hao Zhang",
      "Gregory R. Ganger",
      "Eric P. Xing"
    ],
    "author_ids": [],
    "abstract": "Pollux improves scheduling performance in deep learning (DL) clusters by\nadaptively co-optimizing inter-dependent factors both at the per-job level and\nat the cluster-wide level. Most existing schedulers expect users to specify the\nnumber of resources for each job, often leading to inefficient resource use.\nSome recent schedulers choose job resources for users, but do so without\nawareness of how DL training can be re-optimized to better utilize the provided\nresources.\n  Pollux simultaneously considers both aspects. By monitoring the status of\neach job during training, Pollux models how their goodput (a novel metric we\nintroduce that combines system throughput with statistical efficiency) would\nchange by adding or removing resources. Leveraging these information, Pollux\ndynamically (re-)assigns resources to improve cluster-wide goodput, while\nrespecting fairness and continually optimizing each DL job to better utilize\nthose resources.\n  In experiments with real DL jobs and with trace-driven simulations, Pollux\nreduces average job completion times by 37-50% relative to state-of-the-art DL\nschedulers, even when they are provided with ideal resource and training\nconfigurations for every job. Pollux promotes fairness among DL jobs competing\nfor resources based on a more meaningful measure of useful job progress, and\nreveals a new opportunity for reducing DL cost in cloud environments. Pollux is\nimplemented and publicly available as part of an open-source project at\nhttps://github.com/petuum/adaptdl.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12260v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12161v2",
    "title": "Collaborative Fairness in Federated Learning",
    "authors": [
      "Lingjuan Lyu",
      "Xinyi Xu",
      "Qian Wang"
    ],
    "author_ids": [],
    "abstract": "In current deep learning paradigms, local training or the Standalone\nframework tends to result in overfitting and thus poor generalizability. This\nproblem can be addressed by Distributed or Federated Learning (FL) that\nleverages a parameter server to aggregate model updates from individual\nparticipants. However, most existing Distributed or FL frameworks have\noverlooked an important aspect of participation: collaborative fairness. In\nparticular, all participants can receive the same or similar models, regardless\nof their contributions. To address this issue, we investigate the collaborative\nfairness in FL, and propose a novel Collaborative Fair Federated Learning\n(CFFL) framework which utilizes reputation to enforce participants to converge\nto different models, thus achieving fairness without compromising the\npredictive performance. Extensive experiments on benchmark datasets demonstrate\nthat CFFL achieves high fairness, delivers comparable accuracy to the\nDistributed framework, and outperforms the Standalone framework.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12161v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.12091v3",
    "title": "Limitations of Implicit Bias in Matrix Sensing: Initialization Rank Matters",
    "authors": [
      "Armin Eftekhari",
      "Konstantinos Zygalakis"
    ],
    "author_ids": [],
    "abstract": "In matrix sensing, we first numerically identify the sensitivity to the\ninitialization rank as a new limitation of the implicit bias of gradient flow.\nWe will partially quantify this phenomenon mathematically, where we establish\nthat the gradient flow of the empirical risk is implicitly biased towards\nlow-rank outcomes and successfully learns the planted low-rank matrix, provided\nthat the initialization is low-rank and within a specific \"capture\nneighborhood\". This capture neighborhood is far larger than the corresponding\nneighborhood in local refinement results; the former contains all models with\nzero training error whereas the latter is a small neighborhood of a model with\nzero test error. These new insights enable us to design an alternative\nalgorithm for matrix sensing that complements the high-rank and near-zero\ninitialization scheme which is predominant in the existing literature.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.12091v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.11873v1",
    "title": "Adaptively-Accumulated Knowledge Transfer for Partial Domain Adaptation",
    "authors": [
      "Taotao Jing",
      "Haifeng Xia",
      "Zhengming Ding"
    ],
    "author_ids": [],
    "abstract": "Partial domain adaptation (PDA) attracts appealing attention as it deals with\na realistic and challenging problem when the source domain label space\nsubstitutes the target domain. Most conventional domain adaptation (DA) efforts\nconcentrate on learning domain-invariant features to mitigate the distribution\ndisparity across domains. However, it is crucial to alleviate the negative\ninfluence caused by the irrelevant source domain categories explicitly for PDA.\nIn this work, we propose an Adaptively-Accumulated Knowledge Transfer framework\n(A$^2$KT) to align the relevant categories across two domains for effective\ndomain adaptation. Specifically, an adaptively-accumulated mechanism is\nexplored to gradually filter out the most confident target samples and their\ncorresponding source categories, promoting positive transfer with more\nknowledge across two domains. Moreover, a dual distinct classifier architecture\nconsisting of a prototype classifier and a multilayer perceptron classifier is\nbuilt to capture intrinsic data distribution knowledge across domains from\nvarious perspectives. By maximizing the inter-class center-wise discrepancy and\nminimizing the intra-class sample-wise compactness, the proposed model is able\nto obtain more domain-invariant and task-specific discriminative\nrepresentations of the shared categories data. Comprehensive experiments on\nseveral partial domain adaptation benchmarks demonstrate the effectiveness of\nour proposed model, compared with the state-of-the-art PDA methods.",
    "published_date": "2020-08-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11873v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.11572v1",
    "title": "On the Composition and Limitations of Publicly Available COVID-19 X-Ray Imaging Datasets",
    "authors": [
      "Beatriz Garcia Santa Cruz",
      "Jan Sölter",
      "Matias Nicolas Bossa",
      "Andreas Dominik Husch"
    ],
    "author_ids": [],
    "abstract": "Machine learning based methods for diagnosis and progression prediction of\nCOVID-19 from imaging data have gained significant attention in the last\nmonths, in particular by the use of deep learning models. In this context\nhundreds of models where proposed with the majority of them trained on public\ndatasets. Data scarcity, mismatch between training and target population, group\nimbalance, and lack of documentation are important sources of bias, hindering\nthe applicability of these models to real-world clinical practice. Considering\nthat datasets are an essential part of model building and evaluation, a deeper\nunderstanding of the current landscape is needed. This paper presents an\noverview of the currently public available COVID-19 chest X-ray datasets. Each\ndataset is briefly described and potential strength, limitations and\ninteractions between datasets are identified. In particular, some key\nproperties of current datasets that could be potential sources of bias,\nimpairing models trained on them are pointed out. These descriptions are useful\nfor model building on those datasets, to choose the best dataset according the\nmodel goal, to take into account the specific limitations to avoid reporting\noverconfident benchmark results, and to discuss their impact on the\ngeneralisation capabilities in a specific clinical setting",
    "published_date": "2020-08-26T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11572v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.11463v1",
    "title": "Ethical behavior in humans and machines -- Evaluating training data quality for beneficial machine learning",
    "authors": [
      "Thilo Hagendorff"
    ],
    "author_ids": [],
    "abstract": "Machine behavior that is based on learning algorithms can be significantly\ninfluenced by the exposure to data of different qualities. Up to now, those\nqualities are solely measured in technical terms, but not in ethical ones,\ndespite the significant role of training and annotation data in supervised\nmachine learning. This is the first study to fill this gap by describing new\ndimensions of data quality for supervised machine learning applications. Based\non the rationale that different social and psychological backgrounds of\nindividuals correlate in practice with different modes of\nhuman-computer-interaction, the paper describes from an ethical perspective how\nvarying qualities of behavioral data that individuals leave behind while using\ndigital technologies have socially relevant ramification for the development of\nmachine learning applications. The specific objective of this study is to\ndescribe how training data can be selected according to ethical assessments of\nthe behavior it originates from, establishing an innovative filter regime to\ntransition from the big data rationale n = all to a more selective way of\nprocessing data for training sets in machine learning. The overarching aim of\nthis research is to promote methods for achieving beneficial machine learning\napplications that could be widely useful for industry as well as academia.",
    "published_date": "2020-08-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11463v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.11428v2",
    "title": "Popularity and Centrality in Spotify Networks: Critical transitions in eigenvector centrality",
    "authors": [
      "Tobin South",
      "Matthew Roughan",
      "Lewis Mitchell"
    ],
    "author_ids": [],
    "abstract": "The modern age of digital music access has increased the availability of data\nabout music consumption and creation, facilitating the large-scale analysis of\nthe complex networks that connect music together. Data about user streaming\nbehaviour, and the musical collaboration networks are particularly important\nwith new data-driven recommendation systems. Without thorough analysis, such\ncollaboration graphs can lead to false or misleading conclusions. Here we\npresent a new collaboration network of artists from the online music streaming\nservice Spotify, and demonstrate a critical change in the eigenvector\ncentrality of artists, as low popularity artists are removed. The critical\nchange in centrality, from classical artists to rap artists, demonstrates\ndeeper structural properties of the network. A Social Group Centrality model is\npresented to simulate this critical transition behaviour, and switching between\ndominant eigenvectors is observed. This model presents a novel investigation of\nthe effect of popularity bias on how centrality and importance are measured,\nand provides a new tool for examining such flaws in networks.",
    "published_date": "2020-08-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11428v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.11360v1",
    "title": "Discriminative Cross-Domain Feature Learning for Partial Domain Adaptation",
    "authors": [
      "Taotao Jing",
      "Ming Shao",
      "Zhengming Ding"
    ],
    "author_ids": [],
    "abstract": "Partial domain adaptation aims to adapt knowledge from a larger and more\ndiverse source domain to a smaller target domain with less number of classes,\nwhich has attracted appealing attention. Recent practice on domain adaptation\nmanages to extract effective features by incorporating the pseudo labels for\nthe target domain to better fight off the cross-domain distribution\ndivergences. However, it is essential to align target data with only a small\nset of source data. In this paper, we develop a novel Discriminative\nCross-Domain Feature Learning (DCDF) framework to iteratively optimize target\nlabels with a cross-domain graph in a weighted scheme. Specifically, a weighted\ncross-domain center loss and weighted cross-domain graph propagation are\nproposed to couple unlabeled target data to related source samples for\ndiscriminative cross-domain feature learning, where irrelevant source centers\nwill be ignored, to alleviate the marginal and conditional disparities\nsimultaneously. Experimental evaluations on several popular benchmarks\ndemonstrate the effectiveness of our proposed approach on facilitating the\nrecognition for the unlabeled target domain, through comparing it to the\nstate-of-the-art partial domain adaptation approaches.",
    "published_date": "2020-08-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.11353v1",
    "title": "Detection of Genuine and Posed Facial Expressions of Emotion: A Review",
    "authors": [
      "Shan Jia",
      "Shuo Wang",
      "Chuanbo Hu",
      "Paula Webster",
      "Xin Li"
    ],
    "author_ids": [],
    "abstract": "Facial expressions of emotion play an important role in human social\ninteractions. However, posed acting is not always the same as genuine feeling.\nTherefore, the credibility assessment of facial expressions, namely, the\ndiscrimination of genuine (spontaneous) expressions from\nposed(deliberate/volitional/deceptive) ones, is a crucial yet challenging task\nin facial expression understanding. Rapid progress has been made in recent\nyears for automatic detection of genuine and posed facial expressions. This\npaper presents a general review of the relevant research, including several\nspontaneous vs. posed (SVP) facial expression databases and various computer\nvision based detection methods. In addition, a variety of factors that will\ninfluence the performance of SVP detection methods are discussed along with\nopen issues and technical challenges.",
    "published_date": "2020-08-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11353v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.11348v4",
    "title": "Variance-Reduced Splitting Schemes for Monotone Stochastic Generalized Equations",
    "authors": [
      "Shisheng Cui",
      "Uday V. Shanbhag"
    ],
    "author_ids": [],
    "abstract": "We consider monotone inclusion problems where the operators may be\nexpectation-valued, a class of problems that subsumes convex stochastic\noptimization problems as well as subclasses of stochastic variational\ninequality and equilibrium problems. A direct application of splitting schemes\nis complicated by the need to resolve problems with expectation-valued maps at\neach step, a concern that is addressed by using sampling. Accordingly, we\npropose an avenue for addressing uncertainty in the mapping: Variance-reduced\nstochastic modified forward-backward splitting scheme (vr-SMFBS). In\nconstrained settings, we consider structured settings when the map can be\ndecomposed into an expectation-valued map A and a maximal monotone map B with a\ntractable resolvent. We show that the proposed schemes are equipped with a.s.\nconvergence guarantees, linear (strongly monotone A) and O(1/k) (monotone A)\nrates of convergence while achieving optimal oracle complexity bounds. The rate\nstatements in monotone regimes appear to be amongst the first and rely on\nleveraging the Fitzpatrick gap function for monotone inclusions. Furthermore,\nthe schemes rely on weaker moment requirements on noise and allow for weakening\nunbiasedness requirements on oracles in strongly monotone regimes. Preliminary\nnumerics on a class of two-stage stochastic variational inequality problems\nreflect these findings and show that the variance-reduced schemes outperform\nstochastic approximation schemes and sample-average approximation approaches.\nThe benefits of attaining deterministic rates of convergence become even more\nsalient when resolvent computation is expensive.",
    "published_date": "2020-08-26T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11348v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.11185v1",
    "title": "Bias-Awareness for Zero-Shot Learning the Seen and Unseen",
    "authors": [
      "William Thong",
      "Cees G. M. Snoek"
    ],
    "author_ids": [],
    "abstract": "Generalized zero-shot learning recognizes inputs from both seen and unseen\nclasses. Yet, existing methods tend to be biased towards the classes seen\nduring training. In this paper, we strive to mitigate this bias. We propose a\nbias-aware learner to map inputs to a semantic embedding space for generalized\nzero-shot learning. During training, the model learns to regress to real-valued\nclass prototypes in the embedding space with temperature scaling, while a\nmargin-based bidirectional entropy term regularizes seen and unseen\nprobabilities. Relying on a real-valued semantic embedding space provides a\nversatile approach, as the model can operate on different types of semantic\ninformation for both seen and unseen classes. Experiments are carried out on\nfour benchmarks for generalized zero-shot learning and demonstrate the benefits\nof the proposed bias-aware classifier, both as a stand-alone method or in\ncombination with generated features.",
    "published_date": "2020-08-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.11185v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10880v1",
    "title": "Improving Fair Predictions Using Variational Inference In Causal Models",
    "authors": [
      "Rik Helwegen",
      "Christos Louizos",
      "Patrick Forré"
    ],
    "author_ids": [],
    "abstract": "The importance of algorithmic fairness grows with the increasing impact\nmachine learning has on people's lives. Recent work on fairness metrics shows\nthe need for causal reasoning in fairness constraints. In this work, a\npractical method named FairTrade is proposed for creating flexible prediction\nmodels which integrate fairness constraints on sensitive causal paths. The\nmethod uses recent advances in variational inference in order to account for\nunobserved confounders. Further, a method outline is proposed which uses the\ncausal mechanism estimates to audit black box models. Experiments are conducted\non simulated data and on a real dataset in the context of detecting unlawful\nsocial welfare. This research aims to contribute to machine learning techniques\nwhich honour our ethical and legal boundaries.",
    "published_date": "2020-08-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10880v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10850v2",
    "title": "Discriminability Distillation in Group Representation Learning",
    "authors": [
      "Manyuan Zhang",
      "Guanglu Song",
      "Hang Zhou",
      "Yu Liu"
    ],
    "author_ids": [],
    "abstract": "Learning group representation is a commonly concerned issue in tasks where\nthe basic unit is a group, set, or sequence. Previously, the research community\ntries to tackle it by aggregating the elements in a group based on an indicator\neither defined by humans such as the quality and saliency, or generated by a\nblack box such as the attention score. This article provides a more essential\nand explicable view. We claim the most significant indicator to show whether\nthe group representation can be benefited from one of its element is not the\nquality or an inexplicable score, but the discriminability w.r.t. the model. We\nexplicitly design the discrimiability using embedded class centroids on a proxy\nset. We show the discrimiability knowledge has good properties that can be\ndistilled by a light-weight distillation network and can be generalized on the\nunseen target set. The whole procedure is denoted as discriminability\ndistillation learning (DDL). The proposed DDL can be flexibly plugged into many\ngroup-based recognition tasks without influencing the original training\nprocedures. Comprehensive experiments on various tasks have proven the\neffectiveness of DDL for both accuracy and efficiency. Moreover, it pushes\nforward the state-of-the-art results on these tasks by an impressive margin.",
    "published_date": "2020-08-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10850v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10797v2",
    "title": "The Fairness-Accuracy Pareto Front",
    "authors": [
      "Susan Wei",
      "Marc Niethammer"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness seeks to identify and correct sources of bias in machine\nlearning algorithms. Confoundingly, ensuring fairness often comes at the cost\nof accuracy. We provide formal tools in this work for reconciling this\nfundamental tension in algorithm fairness. Specifically, we put to use the\nconcept of Pareto optimality from multi-objective optimization and seek the\nfairness-accuracy Pareto front of a neural network classifier. We demonstrate\nthat many existing algorithmic fairness methods are performing the so-called\nlinear scalarization scheme which has severe limitations in recovering Pareto\noptimal solutions. We instead apply the Chebyshev scalarization scheme which is\nprovably superior theoretically and no more computationally burdensome at\nrecovering Pareto optimal solutions compared to the linear scheme.",
    "published_date": "2020-08-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10797v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10736v1",
    "title": "LULC Segmentation of RGB Satellite Image Using FCN-8",
    "authors": [
      "Abu Bakar Siddik Nayem",
      "Anis Sarker",
      "Ovi Paul",
      "Amin Ali",
      "Md. Ashraful Amin",
      "AKM Mahbubur Rahman"
    ],
    "author_ids": [],
    "abstract": "This work presents use of Fully Convolutional Network (FCN-8) for semantic\nsegmentation of high-resolution RGB earth surface satel-lite images into land\nuse land cover (LULC) categories. Specically, we propose a non-overlapping\ngrid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16\nweights to segment satellite im-ages into four (forest, built-up, farmland and\nwater) classes. The FCN-8 semantically projects the discriminating features in\nlower resolution learned by the encoder onto the pixel space in higher\nresolution to get a dense classi cation. We experimented the proposed system\nwith Gaofen-2 image dataset, that contains 150 images of over 60 di erent\ncities in china. For comparison, we used available ground-truth along with\nimages segmented using a widely used commeriial GIS software called\neCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8\nobtains signi cantly improved performance, than the eCognition soft-ware. Our\nmodel achieves average accuracy of 91.0% and average Inter-section over Union\n(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is\n0.60. This paper also reports a detail analysis of errors occurred at the LULC\nboundary.",
    "published_date": "2020-08-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10736v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10733v1",
    "title": "Precision Health Data: Requirements, Challenges and Existing Techniques for Data Security and Privacy",
    "authors": [
      "Chandra Thapa",
      "Seyit Camtepe"
    ],
    "author_ids": [],
    "abstract": "Precision health leverages information from various sources, including omics,\nlifestyle, environment, social media, medical records, and medical insurance\nclaims to enable personalized care, prevent and predict illness, and precise\ntreatments. It extensively uses sensing technologies (e.g., electronic health\nmonitoring devices), computations (e.g., machine learning), and communication\n(e.g., interaction between the health data centers). As health data contain\nsensitive private information, including the identity of patient and carer and\nmedical conditions of the patient, proper care is required at all times.\nLeakage of these private information affects the personal life, including\nbullying, high insurance premium, and loss of job due to the medical history.\nThus, the security, privacy of and trust on the information are of utmost\nimportance. Moreover, government legislation and ethics committees demand the\nsecurity and privacy of healthcare data. Herein, in the light of precision\nhealth data security, privacy, ethical and regulatory requirements, finding the\nbest methods and techniques for the utilization of the health data, and thus\nprecision health is essential. In this regard, firstly, this paper explores the\nregulations, ethical guidelines around the world, and domain-specific needs.\nThen it presents the requirements and investigates the associated challenges.\nSecondly, this paper investigates secure and privacy-preserving machine\nlearning methods suitable for the computation of precision health data along\nwith their usage in relevant health projects. Finally, it illustrates the best\navailable techniques for precision health data security and privacy with a\nconceptual system model that enables compliance, ethics clearance, consent\nmanagement, medical innovations, and developments in the health domain.",
    "published_date": "2020-08-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10733v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10467v2",
    "title": "On-line Capacity Estimation for Lithium-ion Battery Cells via an Electrochemical Model-based Adaptive Interconnected Observer",
    "authors": [
      "Anirudh Allam",
      "Simona Onori"
    ],
    "author_ids": [],
    "abstract": "Battery aging is a natural process that contributes to capacity and power\nfade, resulting in a gradual performance degradation over time and usage. State\nof Charge (SOC) and State of Health (SOH) monitoring of an aging battery poses\na challenging task to the Battery Management System (BMS) due to the lack of\ndirect measurements. Estimation algorithms based on an electrochemical model\nthat take into account the impact of aging on physical battery parameters can\nprovide accurate information on lithium concentration and cell capacity over a\nbattery's usable lifespan. A temperature-dependent electrochemical model, the\nEnhanced Single Particle Model (ESPM), forms the basis for the synthesis of an\nadaptive interconnected observer that exploits the relationship between\ncapacity and power fade, due to the growth of Solid Electrolyte Interphase\nlayer (SEI), to enable combined estimation of states (lithium concentration in\nboth electrodes and cell capacity) and aging-sensitive transport parameters\n(anode diffusion coefficient and SEI layer ionic conductivity). The practical\nstability conditions for the adaptive observer are derived using Lyapunov's\ntheory. Validation results against experimental data show a bounded capacity\nestimation error within 2% of its true value. Further, effectiveness of\ncapacity estimation is tested for two cells at different stages of aging.\nRobustness of capacity estimates under measurement noise and sensor bias are\nstudied.",
    "published_date": "2020-08-24T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10467v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.10249v1",
    "title": "Information Constrained Optimal Transport: From Talagrand, to Marton, to Cover",
    "authors": [
      "Yikun Bai",
      "Xiugang Wu",
      "Ayfer Ozgur"
    ],
    "author_ids": [],
    "abstract": "The optimal transport problem studies how to transport one measure to another\nin the most cost-effective way and has wide range of applications from\neconomics to machine learning. In this paper, we introduce and study an\ninformation constrained variation of this problem. Our study yields a\nstrengthening and generalization of Talagrand's celebrated transportation cost\ninequality. Following Marton's approach, we show that the new transportation\ncost inequality can be used to recover old and new concentration of measure\nresults. Finally, we provide an application of this new inequality to network\ninformation theory. We show that it can be used to recover almost immediately a\nrecent solution to a long-standing open problem posed by Cover regarding the\ncapacity of the relay channel.",
    "published_date": "2020-08-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.FA",
      "math.IT",
      "math.PR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10249v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.10165v1",
    "title": "Learning Kernel for Conditional Moment-Matching Discrepancy-based Image Classification",
    "authors": [
      "Chuan-Xian Ren",
      "Pengfei Ge",
      "Dao-Qing Dai",
      "Hong Yan"
    ],
    "author_ids": [],
    "abstract": "Conditional Maximum Mean Discrepancy (CMMD) can capture the discrepancy\nbetween conditional distributions by drawing support from nonlinear kernel\nfunctions, thus it has been successfully used for pattern classification.\nHowever, CMMD does not work well on complex distributions, especially when the\nkernel function fails to correctly characterize the difference between\nintra-class similarity and inter-class similarity. In this paper, a new kernel\nlearning method is proposed to improve the discrimination performance of CMMD.\nIt can be operated with deep network features iteratively and thus denoted as\nKLN for abbreviation. The CMMD loss and an auto-encoder (AE) are used to learn\nan injective function. By considering the compound kernel, i.e., the injective\nfunction with a characteristic kernel, the effectiveness of CMMD for data\ncategory description is enhanced. KLN can simultaneously learn a more\nexpressive kernel and label prediction distribution, thus, it can be used to\nimprove the classification performance in both supervised and semi-supervised\nlearning scenarios. In particular, the kernel-based similarities are\niteratively learned on the deep network features, and the algorithm can be\nimplemented in an end-to-end manner. Extensive experiments are conducted on\nfour benchmark datasets, including MNIST, SVHN, CIFAR-10 and CIFAR-100. The\nresults indicate that KLN achieves state-of-the-art classification performance.",
    "published_date": "2020-08-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.10165v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09994v1",
    "title": "Discriminative Residual Analysis for Image Set Classification with Posture and Age Variations",
    "authors": [
      "Chuan-Xian Ren",
      "You-Wei Luo",
      "Xiao-Lin Xu",
      "Dao-Qing Dai",
      "Hong Yan"
    ],
    "author_ids": [],
    "abstract": "Image set recognition has been widely applied in many practical problems like\nreal-time video retrieval and image caption tasks. Due to its superior\nperformance, it has grown into a significant topic in recent years. However,\nimages with complicated variations, e.g., postures and human ages, are\ndifficult to address, as these variations are continuous and gradual with\nrespect to image appearance. Consequently, the crucial point of image set\nrecognition is to mine the intrinsic connection or structural information from\nthe image batches with variations. In this work, a Discriminant Residual\nAnalysis (DRA) method is proposed to improve the classification performance by\ndiscovering discriminant features in related and unrelated groups.\nSpecifically, DRA attempts to obtain a powerful projection which casts the\nresidual representations into a discriminant subspace. Such a projection\nsubspace is expected to magnify the useful information of the input space as\nmuch as possible, then the relation between the training set and the test set\ndescribed by the given metric or distance will be more precise in the\ndiscriminant subspace. We also propose a nonfeasance strategy by defining\nanother approach to construct the unrelated groups, which help to reduce\nfurthermore the cost of sampling errors. Two regularization approaches are used\nto deal with the probable small sample size problem. Extensive experiments are\nconducted on benchmark databases, and the results show superiority and\nefficiency of the new methods.",
    "published_date": "2020-08-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09994v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09985v1",
    "title": "Detecting signal from science:The structure of research communities and prior knowledge improves prediction of genetic regulatory experiments",
    "authors": [
      "Alexander V. Belikov",
      "Andrey Rzhetsky",
      "James Evans"
    ],
    "author_ids": [],
    "abstract": "The explosive growth of scientists, scientific journals, articles and\nfindings in recent years exponentially increases the difficulty scientists face\nin navigating prior knowledge. This challenge is exacerbated by uncertainty\nabout the reproducibility of published findings. The availability of massive\ndigital archives, machine reading and extraction tools on the one hand, and\nautomated high-throughput experiments on the other, allow us to evaluate these\nchallenges at scale and identify novel opportunities for accelerating\nscientific advance. Here we demonstrate a Bayesian calculus that enables the\npositive prediction of robust, replicable scientific claims with findings\nautomatically extracted from published literature on gene interactions. We\nmatched these findings, filtered by science, with unfiltered gene interactions\nmeasured by the massive LINCS L1000 high-throughput experiment to identify and\ncounteract sources of bias. Our calculus is built on easily extracted\npublication meta-data regarding the position of a scientific claim within the\nweb of prior knowledge, and its breadth of support across institutions, authors\nand communities, revealing that scientifically focused but socially and\ninstitutionally independent research activity is most likely to replicate.\nThese findings recommend policies that go against the common practice of\nchanneling biomedical research funding into centralized research consortia and\ninstitutes rather than dispersing it more broadly. Our results demonstrate that\nrobust scientific findings hinge upon a delicate balance of shared focus and\nindependence, and that this complex pattern can be computationally exploited to\ndecode bias and predict the replicability of published findings. These insights\nprovide guidance for scientists navigating the research literature and for\nscience funders seeking to improve it.",
    "published_date": "2020-08-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph",
      "q-bio.MN",
      "J.2; I.6.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09985v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.09912v2",
    "title": "Reimagining City Configuration: Automated Urban Planning via Adversarial Learning",
    "authors": [
      "Dongjie Wang",
      "Yanjie Fu",
      "Pengyang Wang",
      "Bo Huang",
      "Chang-Tien Lu"
    ],
    "author_ids": [],
    "abstract": "Urban planning refers to the efforts of designing land-use configurations.\nEffective urban planning can help to mitigate the operational and social\nvulnerability of a urban system, such as high tax, crimes, traffic congestion\nand accidents, pollution, depression, and anxiety. Due to the high complexity\nof urban systems, such tasks are mostly completed by professional planners.\nBut, human planners take longer time. The recent advance of deep learning\nmotivates us to ask: can machines learn at a human capability to automatically\nand quickly calculate land-use configuration, so human planners can finally\nadjust machine-generated plans for specific needs? To this end, we formulate\nthe automated urban planning problem into a task of learning to configure\nland-uses, given the surrounding spatial contexts. To set up the task, we\ndefine a land-use configuration as a longitude-latitude-channel tensor, where\neach channel is a category of POIs and the value of an entry is the number of\nPOIs. The objective is then to propose an adversarial learning framework that\ncan automatically generate such tensor for an unplanned area. In particular, we\nfirst characterize the contexts of surrounding areas of an unplanned area by\nlearning representations from spatial graphs using geographic and human\nmobility data. Second, we combine each unplanned area and its surrounding\ncontext representation as a tuple, and categorize all the tuples into positive\n(well-planned areas) and negative samples (poorly-planned areas). Third, we\ndevelop an adversarial land-use configuration approach, where the surrounding\ncontext representation is fed into a generator to generate a land-use\nconfiguration, and a discriminator learns to distinguish among positive and\nnegative samples.",
    "published_date": "2020-08-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09912v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09892v1",
    "title": "Few-Shot Learning with Intra-Class Knowledge Transfer",
    "authors": [
      "Vivek Roy",
      "Yan Xu",
      "Yu-Xiong Wang",
      "Kris Kitani",
      "Ruslan Salakhutdinov",
      "Martial Hebert"
    ],
    "author_ids": [],
    "abstract": "We consider the few-shot classification task with an unbalanced dataset, in\nwhich some classes have sufficient training samples while other classes only\nhave limited training samples. Recent works have proposed to solve this task by\naugmenting the training data of the few-shot classes using generative models\nwith the few-shot training samples as the seeds. However, due to the limited\nnumber of the few-shot seeds, the generated samples usually have small\ndiversity, making it difficult to train a discriminative classifier for the\nfew-shot classes. To enrich the diversity of the generated samples, we propose\nto leverage the intra-class knowledge from the neighbor many-shot classes with\nthe intuition that neighbor classes share similar statistical information. Such\nintra-class information is obtained with a two-step mechanism. First, a\nregressor trained only on the many-shot classes is used to evaluate the\nfew-shot class means from only a few samples. Second, superclasses are\nclustered, and the statistical mean and feature variance of each superclass are\nused as transferable knowledge inherited by the children few-shot classes. Such\nknowledge is then used by a generator to augment the sparse training data to\nhelp the downstream classification tasks. Extensive experiments show that our\nmethod achieves state-of-the-art across different datasets and $n$-shot\nsettings.",
    "published_date": "2020-08-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09892v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09662v1",
    "title": "Biased Mixtures Of Experts: Enabling Computer Vision Inference Under Data Transfer Limitations",
    "authors": [
      "Alhabib Abbas",
      "Yiannis Andreopoulos"
    ],
    "author_ids": [],
    "abstract": "We propose a novel mixture-of-experts class to optimize computer vision\nmodels in accordance with data transfer limitations at test time. Our approach\npostulates that the minimum acceptable amount of data allowing for\nhighly-accurate results can vary for different input space partitions.\nTherefore, we consider mixtures where experts require different amounts of\ndata, and train a sparse gating function to divide the input space for each\nexpert. By appropriate hyperparameter selection, our approach is able to bias\nmixtures of experts towards selecting specific experts over others. In this\nway, we show that the data transfer optimization between visual sensing and\nprocessing can be solved as a convex optimization problem.To demonstrate the\nrelation between data availability and performance, we evaluate biased mixtures\non a range of mainstream computer vision problems, namely: (i) single shot\ndetection, (ii) image super resolution, and (iii) realtime video action\nclassification. For all cases, and when experts constitute modified baselines\nto meet different limits on allowed data utility, biased mixtures significantly\noutperform previous work optimized to meet the same constraints on available\ndata.",
    "published_date": "2020-08-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09662v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09656v1",
    "title": "Auditing Digital Platforms for Discrimination in Economic Opportunity Advertising",
    "authors": [
      "Sara Kingsley",
      "Clara Wang",
      "Alex Mikhalenko",
      "Proteeti Sinha",
      "Chinmay Kulkarni"
    ],
    "author_ids": [],
    "abstract": "Digital platforms, including social networks, are major sources of economic\ninformation. Evidence suggests that digital platforms display different\nsocioeconomic opportunities to demographic groups. Our work addresses this\nissue by presenting a methodology and software to audit digital platforms for\nbias and discrimination. To demonstrate, an audit of the Facebook platform and\nadvertising network was conducted. Between October 2019 and May 2020, we\ncollected 141,063 ads from the Facebook Ad Library API. Using machine learning\nclassifiers, each ad was automatically labeled by the primary marketing\ncategory (housing, employment, credit, political, other). For each of the\ncategories, we analyzed the distribution of the ad content by age group and\ngender. From the audit findings, we considered and present the limitations,\nneeds, infrastructure and policies that would enable researchers to conduct\nmore systematic audits in the future and advocate for why this work must be\ndone. We also discuss how biased distributions impact what socioeconomic\nopportunities people have, especially when on digital platforms some\ndemographic groups are disproportionately excluded from the population(s) that\nreceive(s) content regulated by law.",
    "published_date": "2020-08-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09656v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09490v1",
    "title": "Beyond Individual and Group Fairness",
    "authors": [
      "Pranjal Awasthi",
      "Corinna Cortes",
      "Yishay Mansour",
      "Mehryar Mohri"
    ],
    "author_ids": [],
    "abstract": "We present a new data-driven model of fairness that, unlike existing static\ndefinitions of individual or group fairness is guided by the unfairness\ncomplaints received by the system. Our model supports multiple fairness\ncriteria and takes into account their potential incompatibilities. We consider\nboth a stochastic and an adversarial setting of our model. In the stochastic\nsetting, we show that our framework can be naturally cast as a Markov Decision\nProcess with stochastic losses, for which we give efficient vanishing regret\nalgorithmic solutions. In the adversarial setting, we design efficient\nalgorithms with competitive ratio guarantees. We also report the results of\nexperiments with our algorithms and the stochastic framework on artificial\ndatasets, to demonstrate their effectiveness empirically.",
    "published_date": "2020-08-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09490v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09297v1",
    "title": "FairFly: A Fair Motion Planner for Fleets of Autonomous UAVs in Urban Airspace",
    "authors": [
      "Connor Kurtz",
      "Houssam Abbas"
    ],
    "author_ids": [],
    "abstract": "We present a solution to the problem of fairly planning a fleet of Unmanned\nAerial Vehicles (UAVs) that have different missions and operators, such that no\none operator unfairly gets to finish its missions early at the expense of\nothers - unless this was explicitly negotiated. When hundreds of UAVs share an\nurban airspace, the relevant authorities should allocate corridors to them such\nthat they complete their missions, but no one vehicle is accidentally given an\nexceptionally fast path at the expense of another, which is thus forced to wait\nand waste energy. Our solution, FairFly, addresses the fair planning question\nfor general autonomous systems, including UAV fleets, subject to complex\nmissions typical of urban applications. FairFly formalizes each mission in\ntemporal logic. An offline search finds the fairest paths that satisfy the\nmissions and can be flown by the UAVs, leading to lighter online control load.\nIt allows explicit negotiation between UAVs to enable imbalanced path durations\nif desired. We present three fairness notions, including one that reduces\nenergy consumption. We validate our results in simulation, and demonstrate a\nlighter computational load and less UAV energy consumption as a result of\nflying fair trajectories.",
    "published_date": "2020-08-21T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09297v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.09273v1",
    "title": "The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation",
    "authors": [
      "Himan Abdollahpouri",
      "Masoud Mansoury",
      "Robin Burke",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Recently there has been a growing interest in fairness-aware recommender\nsystems including fairness in providing consistent performance across different\nusers or groups of users. A recommender system could be considered unfair if\nthe recommendations do not fairly represent the tastes of a certain group of\nusers while other groups receive recommendations that are consistent with their\npreferences. In this paper, we use a metric called miscalibration for measuring\nhow a recommendation algorithm is responsive to users' true preferences and we\nconsider how various algorithms may result in different degrees of\nmiscalibration for different users. In particular, we conjecture that\npopularity bias which is a well-known phenomenon in recommendation is one\nimportant factor leading to miscalibration in recommendation. Our experimental\nresults using two real-world datasets show that there is a connection between\nhow different user groups are affected by algorithmic popularity bias and their\nlevel of interest in popular items. Moreover, we show that the more a group is\naffected by the algorithmic popularity bias, the more their recommendations are\nmiscalibrated.",
    "published_date": "2020-08-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09273v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.09194v2",
    "title": "On Attribution of Deepfakes",
    "authors": [
      "Baiwu Zhang",
      "Jin Peng Zhou",
      "Ilia Shumailov",
      "Nicolas Papernot"
    ],
    "author_ids": [],
    "abstract": "Progress in generative modelling, especially generative adversarial networks,\nhave made it possible to efficiently synthesize and alter media at scale.\nMalicious individuals now rely on these machine-generated media, or deepfakes,\nto manipulate social discourse. In order to ensure media authenticity, existing\nresearch is focused on deepfake detection. Yet, the adversarial nature of\nframeworks used for generative modeling suggests that progress towards\ndetecting deepfakes will enable more realistic deepfake generation. Therefore,\nit comes at no surprise that developers of generative models are under the\nscrutiny of stakeholders dealing with misinformation campaigns. At the same\ntime, generative models have a lot of positive applications. As such, there is\na clear need to develop tools that ensure the transparent use of generative\nmodeling, while minimizing the harm caused by malicious applications.\n  Our technique optimizes over the source of entropy of each generative model\nto probabilistically attribute a deepfake to one of the models. We evaluate our\nmethod on the seminal example of face synthesis, demonstrating that our\napproach achieves 97.62% attribution accuracy, and is less sensitive to\nperturbations and adversarial examples. We discuss the ethical implications of\nour work, identify where our technique can be used, and highlight that a more\nmeaningful legislative framework is required for a more transparent and ethical\nuse of generative modeling. Finally, we argue that model developers should be\ncapable of claiming plausible deniability and propose a second framework to do\nso -- this allows a model developer to produce evidence that they did not\nproduce media that they are being accused of having produced.",
    "published_date": "2020-08-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09194v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.09094v2",
    "title": "Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes",
    "authors": [
      "Nicholas Lourie",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "As AI systems become an increasing part of people's everyday lives, it\nbecomes ever more important that they understand people's ethical norms.\nMotivated by descriptive ethics, a field of study that focuses on people's\ndescriptive judgments rather than theoretical prescriptions on morality, we\ninvestigate a novel, data-driven approach to machine ethics.\n  We introduce Scruples, the first large-scale dataset with 625,000 ethical\njudgments over 32,000 real-life anecdotes. Each anecdote recounts a complex\nethical situation, often posing moral dilemmas, paired with a distribution of\njudgments contributed by the community members. Our dataset presents a major\nchallenge to state-of-the-art neural language models, leaving significant room\nfor improvement. However, when presented with simplified moral situations, the\nresults are considerably more promising, suggesting that neural models can\neffectively learn simpler ethical building blocks.\n  A key take-away of our empirical analysis is that norms are not always\nclean-cut; many situations are naturally divisive. We present a new method to\nestimate the best possible performance on such tasks with inherently diverse\nlabel distributions, and explore likelihood functions that separate intrinsic\nfrom model uncertainty.",
    "published_date": "2020-08-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.09094v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.08909v1",
    "title": "Co-Saliency Detection with Co-Attention Fully Convolutional Network",
    "authors": [
      "Guangshuai Gao",
      "Wenting Zhao",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "author_ids": [],
    "abstract": "Co-saliency detection aims to detect common salient objects from a group of\nrelevant images. Some attempts have been made with the Fully Convolutional\nNetwork (FCN) framework and achieve satisfactory detection results. However,\ndue to stacking convolution layers and pooling operation, the boundary details\ntend to be lost. In addition, existing models often utilize the extracted\nfeatures without discrimination, leading to redundancy in representation since\nactually not all features are helpful to the final prediction and some even\nbring distraction. In this paper, we propose a co-attention module embedded FCN\nframework, called as Co-Attention FCN (CA-FCN). Specifically, the co-attention\nmodule is plugged into the high-level convolution layers of FCN, which can\nassign larger attention weights on the common salient objects and smaller ones\non the background and uncommon distractors to boost final detection\nperformance. Extensive experiments on three popular co-saliency benchmark\ndatasets demonstrate the superiority of the proposed CA-FCN, which outperforms\nstate-of-the-arts in most cases. Besides, the effectiveness of our new\nco-attention module is also validated with ablation studies.",
    "published_date": "2020-08-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.08909v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.08798v4",
    "title": "Existence of EFX for Two Additive Valuations",
    "authors": [
      "Ryoga Mahara"
    ],
    "author_ids": [],
    "abstract": "Fair division of indivisible items is a well-studied topic in Economics and\nComputer Science. The objective is to allocate items to agents in a fair\nmanner, where each agent has a valuation for each subset of items.\nEnvy-freeness is one of the most widely studied notions of fairness. Since\ncomplete envy-free allocations do not always exist when items are indivisible,\nseveral relaxations have been considered. Among them, possibly the most\ncompelling one is envy-freeness up to any item (EFX), where no agent envies\nanother agent after the removal of any single item from the other agent's\nbundle. However, despite significant efforts by many researchers for several\nyears, it is known that a complete EFX allocation always exists only in limited\ncases. In this paper, we show that a complete EFX allocation always exists when\neach agent is of one of two given types, where agents of the same type have\nidentical additive valuations. This is the first such existence result for\nnon-identical valuations when there are any number of agents and items and no\nlimit on the number of distinct values an agent can have for individual items.\nWe give a constructive proof, in which we iteratively obtain a Pareto\ndominating (partial) EFX allocation from an existing partial EFX allocation.",
    "published_date": "2020-08-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "Computer Science"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.08798v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.08721v4",
    "title": "The Quantum Supremacy Tsirelson Inequality",
    "authors": [
      "William Kretschmer"
    ],
    "author_ids": [],
    "abstract": "A leading proposal for verifying near-term quantum supremacy experiments on\nnoisy random quantum circuits is linear cross-entropy benchmarking. For a\nquantum circuit $C$ on $n$ qubits and a sample $z \\in \\{0,1\\}^n$, the benchmark\ninvolves computing $|\\langle z|C|0^n \\rangle|^2$, i.e. the probability of\nmeasuring $z$ from the output distribution of $C$ on the all zeros input. Under\na strong conjecture about the classical hardness of estimating output\nprobabilities of quantum circuits, no polynomial-time classical algorithm given\n$C$ can output a string $z$ such that $|\\langle z|C|0^n\\rangle|^2$ is\nsubstantially larger than $\\frac{1}{2^n}$ (Aaronson and Gunn, 2019). On the\nother hand, for a random quantum circuit $C$, sampling $z$ from the output\ndistribution of $C$ achieves $|\\langle z|C|0^n\\rangle|^2 \\approx \\frac{2}{2^n}$\non average (Arute et al., 2019).\n  In analogy with the Tsirelson inequality from quantum nonlocal correlations,\nwe ask: can a polynomial-time quantum algorithm do substantially better than\n$\\frac{2}{2^n}$? We study this question in the query (or black box) model,\nwhere the quantum algorithm is given oracle access to $C$. We show that, for\nany $\\varepsilon \\ge \\frac{1}{\\mathrm{poly}(n)}$, outputting a sample $z$ such\nthat $|\\langle z|C|0^n\\rangle|^2 \\ge \\frac{2 + \\varepsilon}{2^n}$ on average\nrequires at least $\\Omega\\left(\\frac{2^{n/4}}{\\mathrm{poly}(n)}\\right)$ queries\nto $C$, but not more than $O\\left(2^{n/3}\\right)$ queries to $C$, if $C$ is\neither a Haar-random $n$-qubit unitary, or a canonical state preparation oracle\nfor a Haar-random $n$-qubit state. We also show that when $C$ samples from the\nFourier distribution of a random Boolean function, the naive algorithm that\nsamples from $C$ is the optimal 1-query algorithm for maximizing $|\\langle\nz|C|0^n\\rangle|^2$ on average.",
    "published_date": "2020-08-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CC",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.08721v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.08551v1",
    "title": "Popularity Bias in Recommendation: A Multi-stakeholder Perspective",
    "authors": [
      "Himan Abdollahpouri"
    ],
    "author_ids": [],
    "abstract": "Traditionally, especially in academic research in recommender systems, the\nfocus has been solely on the satisfaction of the end-user. While user\nsatisfaction has, indeed, been associated with the success of the business, it\nis not the only factor. In many recommendation domains, there are other\nstakeholders whose needs should be taken into account in the recommendation\ngeneration and evaluation. In this dissertation, I describe the notion of\nmulti-stakeholder recommendation. In particular, I study one of the most\nimportant challenges in recommendation research, popularity bias, from a\nmulti-stakeholder perspective since, as I show later in this dissertation, it\nimpacts different stakeholders in a recommender system. Popularity bias is a\nwell-known phenomenon in recommender systems where popular items are\nrecommended even more frequently than their popularity would warrant,\namplifying long-tail effects already present in many recommendation domains.\nPrior research has examined various approaches for mitigating popularity bias\nand enhancing the recommendation of long-tail items overall. The effectiveness\nof these approaches, however, has not been assessed in multi-stakeholder\nenvironments. In this dissertation, I study the impact of popularity bias in\nrecommender systems from a multi-stakeholder perspective. In addition, I\npropose several algorithms each approaching the popularity bias mitigation from\na different angle and compare their performances using several metrics with\nsome other state-of-the-art approaches in the literature. I show that, often,\nthe standard evaluation measures of popularity bias mitigation in the\nliterature do not reflect the real picture of an algorithm's performance when\nit is evaluated from a multi-stakeholder point of view.",
    "published_date": "2020-08-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.08551v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.08525v1",
    "title": "\"Name that manufacturer\". Relating image acquisition bias with task complexity when training deep learning models: experiments on head CT",
    "authors": [
      "Giorgio Pietro Biondetti",
      "Romane Gauriau",
      "Christopher P. Bridge",
      "Charles Lu",
      "Katherine P. Andriole"
    ],
    "author_ids": [],
    "abstract": "As interest in applying machine learning techniques for medical images\ncontinues to grow at a rapid pace, models are starting to be developed and\ndeployed for clinical applications. In the clinical AI model development\nlifecycle (described by Lu et al. [1]), a crucial phase for machine learning\nscientists and clinicians is the proper design and collection of the data\ncohort. The ability to recognize various forms of biases and distribution\nshifts in the dataset is critical at this step. While it remains difficult to\naccount for all potential sources of bias, techniques can be developed to\nidentify specific types of bias in order to mitigate their impact. In this work\nwe analyze how the distribution of scanner manufacturers in a dataset can\ncontribute to the overall bias of deep learning models. We evaluate\nconvolutional neural networks (CNN) for both classification and segmentation\ntasks, specifically two state-of-the-art models: ResNet [2] for classification\nand U-Net [3] for segmentation. We demonstrate that CNNs can learn to\ndistinguish the imaging scanner manufacturer and that this bias can\nsubstantially impact model performance for both classification and segmentation\ntasks. By creating an original synthesis dataset of brain data mimicking the\npresence of more or less subtle lesions we also show that this bias is related\nto the difficulty of the task. Recognition of such bias is critical to develop\nrobust, generalizable models that will be crucial for clinical applications in\nreal-world data distributions.",
    "published_date": "2020-08-19T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.08525v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.08186v2",
    "title": "Prevalence of Neural Collapse during the terminal phase of deep learning training",
    "authors": [
      "Vardan Papyan",
      "X. Y. Han",
      "David L. Donoho"
    ],
    "author_ids": [],
    "abstract": "Modern practice for training classification deepnets involves a Terminal\nPhase of Training (TPT), which begins at the epoch where training error first\nvanishes; During TPT, the training error stays effectively zero while training\nloss is pushed towards zero. Direct measurements of TPT, for three prototypical\ndeepnet architectures and across seven canonical classification datasets,\nexpose a pervasive inductive bias we call Neural Collapse, involving four\ndeeply interconnected phenomena: (NC1) Cross-example within-class variability\nof last-layer training activations collapses to zero, as the individual\nactivations themselves collapse to their class-means; (NC2) The class-means\ncollapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up\nto rescaling, the last-layer classifiers collapse to the class-means, or in\nother words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a\ngiven activation, the classifier's decision collapses to simply choosing\nwhichever class has the closest train class-mean, i.e. the Nearest Class Center\n(NCC) decision rule. The symmetric and very simple geometry induced by the TPT\nconfers important benefits, including better generalization performance, better\nrobustness, and better interpretability.",
    "published_date": "2020-08-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.08186v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07960v3",
    "title": "Dataset Bias in Few-shot Image Recognition",
    "authors": [
      "Shuqiang Jiang",
      "Yaohui Zhu",
      "Chenlong Liu",
      "Xinhang Song",
      "Xiangyang Li",
      "Weiqing Min"
    ],
    "author_ids": [],
    "abstract": "The goal of few-shot image recognition (FSIR) is to identify novel categories\nwith a small number of annotated samples by exploiting transferable knowledge\nfrom training data (base categories). Most current studies assume that the\ntransferable knowledge can be well used to identify novel categories. However,\nsuch transferable capability may be impacted by the dataset bias, and this\nproblem has rarely been investigated before. Besides, most of few-shot learning\nmethods are biased to different datasets, which is also an important issue that\nneeds to be investigated deeply. In this paper, we first investigate the impact\nof transferable capabilities learned from base categories. Specifically, we use\nthe relevance to measure relationships between base categories and novel\ncategories. Distributions of base categories are depicted via the instance\ndensity and category diversity. The FSIR model learns better transferable\nknowledge from relevant training data. In the relevant data, dense instances or\ndiverse categories can further enrich the learned knowledge. Experimental\nresults on different sub-datasets of ImagNet demonstrate category relevance,\ninstance density and category diversity can depict transferable bias from base\ncategories. Second, we investigate performance differences on different\ndatasets from dataset structures and different few-shot learning methods.\nSpecifically, we introduce image complexity, intra-concept visual consistency,\nand inter-concept visual similarity to quantify characteristics of dataset\nstructures. We use these quantitative characteristics and four few-shot\nlearning methods to analyze performance differences on five different datasets.\nBased on the experimental analysis, some insightful observations are obtained\nfrom the perspective of both dataset structures and few-shot learning methods.\nWe hope these observations are useful to guide future FSIR research.",
    "published_date": "2020-08-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07960v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07832v1",
    "title": "Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models",
    "authors": [
      "Tzu-Jui Julius Wang",
      "Selen Pehlivan",
      "Jorma Laaksonen"
    ],
    "author_ids": [],
    "abstract": "Predicting a scene graph that captures visual entities and their interactions\nin an image has been considered a crucial step towards full scene\ncomprehension. Recent scene graph generation (SGG) models have shown their\ncapability of capturing the most frequent relations among visual entities.\nHowever, the state-of-the-art results are still far from satisfactory, e.g.\nmodels can obtain 31% in overall recall R@100, whereas the likewise important\nmean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The\ndiscrepancy between R and mR results urges to shift the focus from pursuing a\nhigh R to a high mR with a still competitive R. We suspect that the observed\ndiscrepancy stems from both the annotation bias and sparse annotations in VG,\nin which many visual entity pairs are either not annotated at all or only with\na single relation when multiple ones could be valid. To address this particular\nissue, we propose a novel SGG training scheme that capitalizes on self-learned\nknowledge. It involves two relation classifiers, one offering a less biased\nsetting for the other to base on. The proposed scheme can be applied to most of\nthe existing SGG models and is straightforward to implement. We observe\nsignificant relative improvements in mR (between +6.6% and +20.4%) and\ncompetitive or better R (between -2.4% and 0.3%) across all standard SGG tasks.",
    "published_date": "2020-08-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07832v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07687v2",
    "title": "Estimation of causal effects of multiple treatments in healthcare database studies with rare outcomes",
    "authors": [
      "Liangyuan Hu",
      "Chenyang Gu"
    ],
    "author_ids": [],
    "abstract": "The preponderance of large-scale healthcare databases provide abundant\nopportunities for comparative effectiveness research. Evidence necessary to\nmaking informed treatment decisions often relies on comparing effectiveness of\nmultiple treatment options on outcomes of interest observed in a small number\nof individuals. Causal inference with multiple treatments and rare outcomes is\na subject that has been treated sparingly in the literature. This paper designs\nthree sets of simulations, representative of the structure of our healthcare\ndatabase study, and propose causal analysis strategies for such settings. We\ninvestigate and compare the operating characteristics of three types of methods\nand their variants: Bayesian Additive Regression Trees (BART), regression\nadjustment on multivariate spline of generalized propensity scores (RAMS) and\ninverse probability of treatment weighting (IPTW) with multinomial logistic\nregression or generalized boosted models. Our results suggest that BART and\nRAMS provide lower bias and mean squared error, and the widely used IPTW\nmethods deliver unfavorable operating characteristics. We illustrate the\nmethods using a case study evaluating the comparative effectiveness of\nrobotic-assisted surgery, video-assisted thoracoscopic surgery and open\nthoracotomy for treating non-small cell lung cancer.",
    "published_date": "2020-08-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07687v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2009.06357v1",
    "title": "Automatic elimination of the pectoral muscle in mammograms based on anatomical features",
    "authors": [
      "Jairo A. Ayala-Godoy",
      "Rosa E. Lillo",
      "Juan Romo"
    ],
    "author_ids": [],
    "abstract": "Digital mammogram inspection is the most popular technique for early\ndetection of abnormalities in human breast tissue. When mammograms are analyzed\nthrough a computational method, the presence of the pectoral muscle might\naffect the results of breast lesions detection. This problem is particularly\nevident in the mediolateral oblique view (MLO), where pectoral muscle occupies\na large part of the mammography. Therefore, identifying and eliminating the\npectoral muscle are essential steps for improving the automatic discrimination\nof breast tissue. In this paper, we propose an approach based on anatomical\nfeatures to tackle this problem. Our method consists of two steps: (1) a\nprocess to remove the noisy elements such as labels, markers, scratches and\nwedges, and (2) application of an intensity transformation based on the Beta\ndistribution. The novel methodology is tested with 322 digital mammograms from\nthe Mammographic Image Analysis Society (mini-MIAS) database and with a set of\n84 mammograms for which the area normalized error was previously calculated.\nThe results show a very good performance of the method.",
    "published_date": "2020-08-17T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06357v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07520v2",
    "title": "Do face masks introduce bias in speech technologies? The case of automated scoring of speaking proficiency",
    "authors": [
      "Anastassia Loukina",
      "Keelan Evanini",
      "Matthew Mulholland",
      "Ian Blood",
      "Klaus Zechner"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic has led to a dramatic increase in the use of face masks\nworldwide. Face coverings can affect both acoustic properties of the signal as\nwell as speech patterns and have unintended effects if the person wearing the\nmask attempts to use speech processing technologies. In this paper we explore\nthe impact of wearing face masks on the automated assessment of English\nlanguage proficiency. We use a dataset from a large-scale speaking test for\nwhich test-takers were required to wear face masks during the test\nadministration, and we compare it to a matched control sample of test-takers\nwho took the same test before the mask requirements were put in place. We find\nthat the two samples differ across a range of acoustic measures and also show a\nsmall but significant difference in speech patterns. However, these differences\ndo not lead to differences in human or automated scores of English language\nproficiency. Several measures of bias showed no differences in scores between\nthe two groups.",
    "published_date": "2020-08-17T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.HC",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07520v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07438v3",
    "title": "Analysis and Optimization for Large-Scale LoRa Networks: Throughput Fairness and Scalability",
    "authors": [
      "Jiangbin Lyu",
      "Dan Yu",
      "Liqun Fu"
    ],
    "author_ids": [],
    "abstract": "LoRa networks are pivotally enabling Long Range connectivity to low-cost and\npower-constrained user equipments (UEs) in a wide area, whereas a critical\nissue is to effectively allocate wireless resources to support potentially\nmassive UEs while resolving the prominent near-far fairness issue, which is\nchallenging due to the lack of tractable analytical model and the practical\nrequirement for low-complexity and low-overhead design. Leveraging on\nstochastic geometry, especially the Poisson rain model, we derive (semi-)\nclosed-form formulas for the aggregate interference distribution, packet\nsuccess probability and hence system throughput in both single-cell and\nmulti-cell setups with frequency reuse, by accounting for channel fading,\nrandom UE distribution, partial packet overlapping, and/or multi-gateway packet\nreception. The analytical formulas require only average channel statistics and\nspatial UE distribution, which enable tractable network performance evaluation\nand incubate our proposed Iterative Balancing (IB) method that quickly yields\nhigh-level policies of joint spreading factor (SF) allocation, power control,\nand duty cycle adjustment for gauging the average max-min UE throughput or\nsupported UE density with rate requirements. Numerical results validate the\nanalytical formulas and the effectiveness of our proposed optimization scheme,\nwhich greatly alleviates the near-far fairness issue and reduces the spatial\npower consumption, while significantly improving the cell-edge throughput as\nwell as the spatial (sum) throughput for the majority of UEs, by adapting to\nthe UE/gateway densities.",
    "published_date": "2020-08-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.NI",
      "cs.SY",
      "eess.SY",
      "math.IT",
      "math.PR",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07438v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.06966v2",
    "title": "Automated Detection of Congenital Heart Disease in Fetal Ultrasound Screening",
    "authors": [
      "Jeremy Tan",
      "Anselm Au",
      "Qingjie Meng",
      "Sandy FinesilverSmith",
      "John Simpson",
      "Daniel Rueckert",
      "Reza Razavi",
      "Thomas Day",
      "David Lloyd",
      "Bernhard Kainz"
    ],
    "author_ids": [],
    "abstract": "Prenatal screening with ultrasound can lower neonatal mortality significantly\nfor selected cardiac abnormalities. However, the need for human expertise,\ncoupled with the high volume of screening cases, limits the practically\nachievable detection rates. In this paper we discuss the potential for deep\nlearning techniques to aid in the detection of congenital heart disease (CHD)\nin fetal ultrasound. We propose a pipeline for automated data curation and\nclassification. During both training and inference, we exploit an auxiliary\nview classification task to bias features toward relevant cardiac structures.\nThis bias helps to improve in F1-scores from 0.72 and 0.77 to 0.87 and 0.85 for\nhealthy and CHD classes respectively.",
    "published_date": "2020-08-16T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06966v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.06840v2",
    "title": "We Learn Better Road Pothole Detection: from Attention Aggregation to Adversarial Domain Adaptation",
    "authors": [
      "Rui Fan",
      "Hengli Wang",
      "Mohammud J. Bocus",
      "Ming Liu"
    ],
    "author_ids": [],
    "abstract": "Manual visual inspection performed by certified inspectors is still the main\nform of road pothole detection. This process is, however, not only tedious,\ntime-consuming and costly, but also dangerous for the inspectors. Furthermore,\nthe road pothole detection results are always subjective, because they depend\nentirely on the individual experience. Our recently introduced disparity (or\ninverse depth) transformation algorithm allows better discrimination between\ndamaged and undamaged road areas, and it can be easily deployed to any semantic\nsegmentation network for better road pothole detection results. To boost the\nperformance, we propose a novel attention aggregation (AA) framework, which\ntakes the advantages of different types of attention modules. In addition, we\ndevelop an effective training set augmentation technique based on adversarial\ndomain adaptation, where the synthetic road RGB images and transformed road\ndisparity (or inverse depth) images are generated to enhance the training of\nsemantic segmentation networks. The experimental results demonstrate that,\nfirstly, the transformed disparity (or inverse depth) images become more\ninformative; secondly, AA-UNet and AA-RTFNet, our best performing\nimplementations, respectively outperform all other state-of-the-art\nsingle-modal and data-fusion networks for road pothole detection; and finally,\nthe training set augmentation technique based on adversarial domain adaptation\nnot only improves the accuracy of the state-of-the-art semantic segmentation\nnetworks, but also accelerates their convergence.",
    "published_date": "2020-08-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06840v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.06814v1",
    "title": "Cascaded channel pruning using hierarchical self-distillation",
    "authors": [
      "Roy Miles",
      "Krystian Mikolajczyk"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose an approach for filter-level pruning with\nhierarchical knowledge distillation based on the teacher, teaching-assistant,\nand student framework. Our method makes use of teaching assistants at\nintermediate pruning levels that share the same architecture and weights as the\ntarget student. We propose to prune each model independently using the gradient\ninformation from its corresponding teacher. By considering the relative sizes\nof each student-teacher pair, this formulation provides a natural trade-off\nbetween the capacity gap for knowledge distillation and the bias of the filter\nsaliency updates. Our results show improvements in the attainable accuracy and\nmodel compression across the CIFAR10 and ImageNet classification tasks using\nthe VGG16and ResNet50 architectures. We provide an extensive evaluation that\ndemonstrates the benefits of using a varying number of teaching assistant\nmodels at different sizes.",
    "published_date": "2020-08-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.06755v1",
    "title": "Tackling COVID-19 through Responsible AI Innovation: Five Steps in the Right Direction",
    "authors": [
      "David Leslie"
    ],
    "author_ids": [],
    "abstract": "Innovations in data science and AI/ML have a central role to play in\nsupporting global efforts to combat COVID-19. The versatility of AI/ML\ntechnologies enables scientists and technologists to address an impressively\nbroad range of biomedical, epidemiological, and socioeconomic challenges. This\nwide-reaching scientific capacity, however, also raises a diverse array of\nethical challenges. The need for researchers to act quickly and globally in\ntackling SARS-CoV-2 demands unprecedented practices of open research and\nresponsible data sharing at a time when innovation ecosystems are hobbled by\nproprietary protectionism, inequality, and a lack of public trust. Moreover,\nsocietally impactful interventions like digital contact tracing are raising\nfears of surveillance creep and are challenging widely held commitments to\nprivacy, autonomy, and civil liberties. Prepandemic concerns that data-driven\ninnovations may function to reinforce entrenched dynamics of societal inequity\nhave likewise intensified given the disparate impact of the virus on vulnerable\nsocial groups and the life-and-death consequences of biased and discriminatory\npublic health outcomes. To address these concerns, I offer five steps that need\nto be taken to encourage responsible research and innovation. These provide a\npractice-based path to responsible AI/ML design and discovery centered on open,\naccountable, equitable, and democratically governed processes and products.\nWhen taken from the start, these steps will not only enhance the capacity of\ninnovators to tackle COVID-19 responsibly, they will, more broadly, help to\nbetter equip the data science and AI/ML community to cope with future pandemics\nand to support a more humane, rational, and just society.",
    "published_date": "2020-08-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.OT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06755v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.06680v2",
    "title": "A VCG-based Fair Incentive Mechanism for Federated Learning",
    "authors": [
      "Mingshu Cong",
      "Han Yu",
      "Xi Weng",
      "Jiabao Qu",
      "Yang Liu",
      "Siu Ming Yiu"
    ],
    "author_ids": [],
    "abstract": "The enduring value of the Vickrey-Clarke-Groves (VCG) mechanism has been\nhighlighted due to its adoption by Facebook ad auctions. Our research delves\ninto its utility in the collaborative virtual goods production (CVGP) game,\nwhich finds application in realms like federated learning and crowdsourcing, in\nwhich bidders take on the roles of suppliers rather than consumers. We\nintroduce the Procurement-VCG (PVCG) sharing rule into existing VCG mechanisms\nsuch that they can handle capacity limits and the continuous strategy space\ncharacteristic of the reverse auction setting in CVGP games. Our main\ntheoretical contribution provides mathematical proofs to show that PVCG is the\nfirst in the CVGP game context to simultaneously achieve truthfulness, Pareto\nefficiency, individual rationality, and weak budget balance. These properties\nsuggest the potential for Pareto-efficient production in the digital planned\neconomy. Moreover, to compute the PVCG payments in a noisy economic\nenvironment, we propose the Report-Interpolation-Maximization (RIM) method. RIM\nfacilitates the learning of the optimal procurement level and PVCG payments\nthrough iterative interactions with suppliers.",
    "published_date": "2020-08-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06680v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.06460v2",
    "title": "Hate Speech Detection and Racial Bias Mitigation in Social Media based on BERT model",
    "authors": [
      "Marzieh Mozafari",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ],
    "author_ids": [],
    "abstract": "Disparate biases associated with datasets and trained classifiers in hateful\nand abusive content identification tasks have raised many concerns recently.\nAlthough the problem of biased datasets on abusive language detection has been\naddressed more frequently, biases arising from trained classifiers have not yet\nbeen a matter of concern. Here, we first introduce a transfer learning approach\nfor hate speech detection based on an existing pre-trained language model\ncalled BERT and evaluate the proposed model on two publicly available datasets\nannotated for racism, sexism, hate or offensive content on Twitter. Next, we\nintroduce a bias alleviation mechanism in hate speech detection task to\nmitigate the effect of bias in training set during the fine-tuning of our\npre-trained BERT-based model. Toward that end, we use an existing\nregularization method to reweight input samples, thereby decreasing the effects\nof high correlated training set' s n-grams with class labels, and then\nfine-tune our pre-trained BERT-based model with the new re-weighted samples. To\nevaluate our bias alleviation mechanism, we employ a cross-domain approach in\nwhich we use the trained classifiers on the aforementioned datasets to predict\nthe labels of two new datasets from Twitter, AAE-aligned and White-aligned\ngroups, which indicate tweets written in African-American English (AAE) and\nStandard American English (SAE) respectively. The results show the existence of\nsystematic racial bias in trained classifiers as they tend to assign tweets\nwritten in AAE from AAE-aligned group to negative classes such as racism,\nsexism, hate, and offensive more often than tweets written in SAE from\nWhite-aligned. However, the racial bias in our classifiers reduces\nsignificantly after our bias alleviation mechanism is incorporated. This work\ncould institute the first step towards debiasing hate speech and abusive\nlanguage detection systems.",
    "published_date": "2020-08-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06460v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07433v1",
    "title": "LiFT: A Scalable Framework for Measuring Fairness in ML Applications",
    "authors": [
      "Sriram Vasudevan",
      "Krishnaram Kenthapadi"
    ],
    "author_ids": [],
    "abstract": "Many internet applications are powered by machine learned models, which are\nusually trained on labeled datasets obtained through either implicit / explicit\nuser feedback signals or human judgments. Since societal biases may be present\nin the generation of such datasets, it is possible for the trained models to be\nbiased, thereby resulting in potential discrimination and harms for\ndisadvantaged groups. Motivated by the need for understanding and addressing\nalgorithmic bias in web-scale ML systems and the limitations of existing\nfairness toolkits, we present the LinkedIn Fairness Toolkit (LiFT), a framework\nfor scalable computation of fairness metrics as part of large ML systems. We\nhighlight the key requirements in deployed settings, and present the design of\nour fairness measurement system. We discuss the challenges encountered in\nincorporating fairness tools in practice and the lessons learned during\ndeployment at LinkedIn. Finally, we provide open problems based on practical\nexperience.",
    "published_date": "2020-08-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07433v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.06392v3",
    "title": "Deep Domain Adaptation for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labelled Videos",
    "authors": [
      "R. Gnana Praveen",
      "Eric Granger",
      "Patrick Cardinal"
    ],
    "author_ids": [],
    "abstract": "Estimation of pain intensity from facial expressions captured in videos has\nan immense potential for health care applications. Given the challenges related\nto subjective variations of facial expressions, and operational capture\nconditions, the accuracy of state-of-the-art DL models for recognizing facial\nexpressions may decline. Domain adaptation has been widely explored to\nalleviate the problem of domain shifts that typically occur between video data\ncaptured across various source and target domains. Moreover, given the\nlaborious task of collecting and annotating videos, and subjective bias due to\nambiguity among adjacent intensity levels, weakly-supervised learning is\ngaining attention in such applications. State-of-the-art WSL models are\ntypically formulated as regression problems, and do not leverage the ordinal\nrelationship among pain intensity levels, nor temporal coherence of multiple\nconsecutive frames. This paper introduces a new DL model for weakly-supervised\nDA with ordinal regression that can be adapted using target domain videos with\ncoarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal\nrelationships among intensity levels assigned to target sequences, and\nassociates multiple relevant frames to sequence-level labels. In particular, it\nlearns discriminant and domain-invariant feature representations by integrating\nmultiple instance learning with deep adversarial DA, where soft Gaussian labels\nare used to efficiently represent the weak ordinal sequence-level labels from\ntarget domain. The proposed approach was validated using RECOLA video dataset\nas fully-labeled source domain data, and UNBC-McMaster shoulder pain video\ndataset as weakly-labeled target domain data. We have also validated WSDA-OR on\nBIOVID and Fatigue datasets for sequence level estimation.",
    "published_date": "2020-08-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06392v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.05758v2",
    "title": "Conservative Stochastic Optimization with Expectation Constraints",
    "authors": [
      "Zeeshan Akhtar",
      "Amrit Singh Bedi",
      "Ketan Rajawat"
    ],
    "author_ids": [],
    "abstract": "This paper considers stochastic convex optimization problems where the\nobjective and constraint functions involve expectations with respect to the\ndata indices or environmental variables, in addition to deterministic convex\nconstraints on the domain of the variables. Although the setting is generic and\narises in different machine learning applications, online and efficient\napproaches for solving such problems have not been widely studied. Since the\nunderlying data distribution is unknown a priori, a closed-form solution is\ngenerally not available, and classical deterministic optimization paradigms are\nnot applicable. State-of-the-art approaches, such as those using the saddle\npoint framework, can ensure that the optimality gap as well as the constraint\nviolation decay as $\\O\\left(T^{-\\frac{1}{2}}\\right)$ where $T$ is the number of\nstochastic gradients. The domain constraints are assumed simple and handled via\nprojection at every iteration. In this work, we propose a novel conservative\nstochastic optimization algorithm (CSOA) that achieves zero constraint\nviolation and $\\O\\left(T^{-\\frac{1}{2}}\\right)$ optimality gap.\n  Further, the projection operation (for scenarios when calculating projection\nis expensive) in the proposed algorithm can be avoided by considering the\nconditional gradient or Frank-Wolfe (FW) variant of the algorithm. The\nstate-of-the-art stochastic FW variants achieve an optimality gap of\n$\\O\\left(T^{-\\frac{1}{3}}\\right)$ after $T$ iterations, though these algorithms\nhave not been applied to problems with functional expectation constraints. In\nthis work, we propose the FW-CSOA algorithm that is not only projection-free\nbut also achieves zero constraint violation with\n$\\O\\left(T^{-\\frac{1}{4}}\\right)$ decay of the optimality gap. The efficacy of\nthe proposed algorithms is tested on two relevant problems: fair classification\nand structured matrix completion.",
    "published_date": "2020-08-13T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05758v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.05701v1",
    "title": "The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation Objectively?",
    "authors": [
      "Kevin Roitero",
      "Michael Soprano",
      "Beatrice Portelli",
      "Damiano Spina",
      "Vincenzo Della Mea",
      "Giuseppe Serra",
      "Stefano Mizzaro",
      "Gianluca Demartini"
    ],
    "author_ids": [],
    "abstract": "Misinformation is an ever increasing problem that is difficult to solve for\nthe research community and has a negative impact on the society at large. Very\nrecently, the problem has been addressed with a crowdsourcing-based approach to\nscale up labeling efforts: to assess the truthfulness of a statement, instead\nof relying on a few experts, a crowd of (non-expert) judges is exploited. We\nfollow the same approach to study whether crowdsourcing is an effective and\nreliable method to assess statements truthfulness during a pandemic. We\nspecifically target statements related to the COVID-19 health emergency, that\nis still ongoing at the time of the study and has arguably caused an increase\nof the amount of misinformation that is spreading online (a phenomenon for\nwhich the term \"infodemic\" has been used). By doing so, we are able to address\n(mis)information that is both related to a sensitive and personal issue like\nhealth and very recent as compared to when the judgment is done: two issues\nthat have not been analyzed in related work. In our experiment, crowd workers\nare asked to assess the truthfulness of statements, as well as to provide\nevidence for the assessments as a URL and a text justification. Besides showing\nthat the crowd is able to accurately judge the truthfulness of the statements,\nwe also report results on many different aspects, including: agreement among\nworkers, the effect of different aggregation functions, of scales\ntransformations, and of workers background / bias. We also analyze workers\nbehavior, in terms of queries submitted, URLs found / selected, text\njustifications, and other behavioral data like clicks and mouse actions\ncollected by means of an ad hoc logger.",
    "published_date": "2020-08-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CL",
      "68P20",
      "H.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05701v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.05533v4",
    "title": "Overcoming Model Bias for Robust Offline Deep Reinforcement Learning",
    "authors": [
      "Phillip Swazinna",
      "Steffen Udluft",
      "Thomas Runkler"
    ],
    "author_ids": [],
    "abstract": "State-of-the-art reinforcement learning algorithms mostly rely on being\nallowed to directly interact with their environment to collect millions of\nobservations. This makes it hard to transfer their success to industrial\ncontrol problems, where simulations are often very costly or do not exist, and\nexploring in the real environment can potentially lead to catastrophic events.\nRecently developed, model-free, offline RL algorithms, can learn from a single\ndataset (containing limited exploration) by mitigating extrapolation error in\nvalue functions. However, the robustness of the training process is still\ncomparatively low, a problem known from methods using value functions. To\nimprove robustness and stability of the learning process, we use dynamics\nmodels to assess policy performance instead of value functions, resulting in\nMOOSE (MOdel-based Offline policy Search with Ensembles), an algorithm which\nensures low model bias by keeping the policy within the support of the data. We\ncompare MOOSE with state-of-the-art model-free, offline RL algorithms { BRAC,}\nBEAR and BCQ on the Industrial Benchmark and MuJoCo continuous control tasks in\nterms of robust performance, and find that MOOSE outperforms its model-free\ncounterparts in almost all considered cases, often even by far.",
    "published_date": "2020-08-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05533v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.05532v1",
    "title": "Entropy Power Inequality in Fermionic Quantum Computation",
    "authors": [
      "N. J. B. Aza",
      "D. A. Barbosa T"
    ],
    "author_ids": [],
    "abstract": "We study quantum computation relations on unital finite-dimensional CAR\n$C^{*}$-algebras. We prove an entropy power inequality (EPI) in a fermionic\nsetting, which presumably will permit understanding the capacities in fermionic\nlinear optics. Similar relations to the bosonic case are shown, and alternative\nproofs of known facts are given. Clifford algebras and the Grassmann\nrepresentation can thus be used to obtain mathematical results regarding\ncoherent fermion states.",
    "published_date": "2020-08-12T00:00:00",
    "year": 2020,
    "categories": [
      "math-ph",
      "cs.IT",
      "math.IT",
      "math.MP",
      "math.OA",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05532v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.05261v1",
    "title": "Vindication, Virtue and Vitriol: A study of online engagement and abuse toward British MPs during the COVID-19 Pandemic",
    "authors": [
      "Tracie Farrell",
      "Genevieve Gorrell",
      "Kalina Bontcheva"
    ],
    "author_ids": [],
    "abstract": "COVID-19 has given rise to malicious content online, including online abuse\nand hate toward British MPs. In order to understand and contextualise the level\nof abuse MPs receive, we consider how ministers use social media to communicate\nabout the crisis, and the citizen engagement that this generates. The focus of\nthe paper is on a large-scale, mixed methods study of abusive and antagonistic\nresponses to UK politicians during the pandemic from early February to late May\n2020. We find that pressing subjects such as financial concerns attract high\nlevels of engagement, but not necessarily abusive dialogue. Rather, criticising\nauthorities appears to attract higher levels of abuse. In particular, those who\ncarry the flame for subjects like racism and inequality, may be accused of\nvirtue signalling or receive higher abuse levels due to the topics they are\nrequired by their role to address. This work contributes to the wider\nunderstanding of abusive language online, in particular that which is directed\nat public officials.",
    "published_date": "2020-08-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05261v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.05248v1",
    "title": "Null-sampling for Interpretable and Fair Representations",
    "authors": [
      "Thomas Kehrenberg",
      "Myles Bartlett",
      "Oliver Thomas",
      "Novi Quadrianto"
    ],
    "author_ids": [],
    "abstract": "We propose to learn invariant representations, in the data domain, to achieve\ninterpretability in algorithmic fairness. Invariance implies a selectivity for\nhigh level, relevant correlations w.r.t. class label annotations, and a\nrobustness to irrelevant correlations with protected characteristics such as\nrace or gender. We introduce a non-trivial setup in which the training set\nexhibits a strong bias such that class label annotations are irrelevant and\nspurious correlations cannot be distinguished. To address this problem, we\nintroduce an adversarially trained model with a null-sampling procedure to\nproduce invariant representations in the data domain. To enable\ndisentanglement, a partially-labelled representative set is used. By placing\nthe representations into the data domain, the changes made by the model are\neasily examinable by human auditors. We show the effectiveness of our method on\nboth image and tabular datasets: Coloured MNIST, the CelebA and the Adult\ndataset.",
    "published_date": "2020-08-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05248v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.05122v1",
    "title": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",
    "authors": [
      "Ian Tenney",
      "James Wexler",
      "Jasmijn Bastings",
      "Tolga Bolukbasi",
      "Andy Coenen",
      "Sebastian Gehrmann",
      "Ellen Jiang",
      "Mahima Pushkarna",
      "Carey Radebaugh",
      "Emily Reif",
      "Ann Yuan"
    ],
    "author_ids": [],
    "abstract": "We present the Language Interpretability Tool (LIT), an open-source platform\nfor visualization and understanding of NLP models. We focus on core questions\nabout model behavior: Why did my model make this prediction? When does it\nperform poorly? What happens under a controlled change in the input? LIT\nintegrates local explanations, aggregate analysis, and counterfactual\ngeneration into a streamlined, browser-based interface to enable rapid\nexploration and error analysis. We include case studies for a diverse set of\nworkflows, including exploring counterfactuals for sentiment analysis,\nmeasuring gender bias in coreference systems, and exploring local behavior in\ntext generation. LIT supports a wide range of models--including classification,\nseq2seq, and structured prediction--and is highly extensible through a\ndeclarative, framework-agnostic API. LIT is under active development, with code\nand full documentation available at https://github.com/pair-code/lit.",
    "published_date": "2020-08-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05122v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.05091v2",
    "title": "Rate-Splitting Multiple Access for Multigroup Multicast and Multibeam Satellite Systems",
    "authors": [
      "Longfei Yin",
      "Bruno Clerckx"
    ],
    "author_ids": [],
    "abstract": "This work focuses on the promising Rate-Splitting Multiple Access (RSMA) and\nits beamforming design problem to achieve max-min fairness (MMF) among multiple\nco-channel multicast groups with imperfect channel state information at the\ntransmitter (CSIT). Contrary to the conventional linear precoding (NoRS) that\nrelies on fully treating any residual interference as noise, we consider a\nnovel multigroup multicast beamforming strategy based on RSMA. RSMA relies on\nlinearly precoded Rate-Splitting (RS) at the transmitter and Successive\nInterference Cancellation (SIC) at the receivers, and has recently been shown\nto enable a flexible framework for non-orthogonal transmission and robust\ninterference management in multi-antenna wireless networks. In this work, we\ncharacterize the MMF Degrees-of-Freedom (DoF) achieved by RS and NoRS in\nmultigroup multicast with imperfect CSIT and demonstrate the benefits of RS\nstrategies for both underloaded and overloaded scenarios. Motivated by the DoF\nanalysis, we then formulate a generic transmit power constrained optimization\nproblem to achieve MMF rate performance. The superiority of RS-based multigroup\nmulticast beamforming compared with NoRS is demonstrated via simulations in\nboth terrestrial and multibeam satellite systems. In particular, due to the\ncharacteristics and challenges of multibeam satellite communications, our\nproposed RS strategy is shown promising to manage its interbeam interference.",
    "published_date": "2020-08-12T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05091v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.04618v1",
    "title": "An ad-hoc social network generation approach",
    "authors": [
      "Maurice Tchoupé Tchendji",
      "Martin Xavier Tchembé",
      "Armelle Linda Maténé Kakeu"
    ],
    "author_ids": [],
    "abstract": "The use of social networks is still confined to infrastructure-based networks\nsuch as the Internet. However, many situations (conferences, fairs, etc.) may\nrequire the implementation and rapid deployment of an ad-hoc application for\ndisseminating information: we call this type of application, Ad-hoc Social\nNetwork. These applications are necessarily distributed, deployable on mobile\nunits, etc. They therefore inevitably share the same characteristics as those\ninherent in ad-hoc mobile networks and make them good candidates for their\ndeployment. In this paper, by using techniques from the field of generative\nprogramming, we propose an approach to produce environments for generating such\napplications from their specifications in a domain-specific language. By\napplying this approach, we have developed SMGenerator, an environment for\ngenerating mobile ad-hoc social network applications for Android devices.\nMoreover by using SMGenerator, we easily generated the ConfInfo application: an\nad-hoc social network application for disseminating information to participants\nin a scientific manifestation.",
    "published_date": "2020-08-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04618v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.07309v1",
    "title": "Bias and Discrimination in AI: a cross-disciplinary perspective",
    "authors": [
      "Xavier Ferrer",
      "Tom van Nuenen",
      "Jose M. Such",
      "Mark Coté",
      "Natalia Criado"
    ],
    "author_ids": [],
    "abstract": "With the widespread and pervasive use of Artificial Intelligence (AI) for\nautomated decision-making systems, AI bias is becoming more apparent and\nproblematic. One of its negative consequences is discrimination: the unfair, or\nunequal treatment of individuals based on certain characteristics. However, the\nrelationship between bias and discrimination is not always clear. In this\npaper, we survey relevant literature about bias and discrimination in AI from\nan interdisciplinary perspective that embeds technical, legal, social and\nethical dimensions. We show that finding solutions to bias and discrimination\nin AI requires robust cross-disciplinary collaborations.",
    "published_date": "2020-08-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "68T01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07309v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07326v1",
    "title": "Progressing Towards Responsible AI",
    "authors": [
      "Teresa Scantamburlo",
      "Atia Cortés",
      "Marie Schacht"
    ],
    "author_ids": [],
    "abstract": "The field of Artificial Intelligence (AI) and, in particular, the Machine\nLearning area, counts on a wide range of performance metrics and benchmark data\nsets to assess the problem-solving effectiveness of its solutions. However, the\nappearance of research centres, projects or institutions addressing AI\nsolutions from a multidisciplinary and multi-stakeholder perspective suggests a\nnew approach to assessment comprising ethical guidelines, reports or tools and\nframeworks to help both academia and business to move towards a responsible\nconceptualisation of AI. They all highlight the relevance of three key aspects:\n(i) enhancing cooperation among the different stakeholders involved in the\ndesign, deployment and use of AI; (ii) promoting multidisciplinary dialogue,\nincluding different domains of expertise in this process; and (iii) fostering\npublic engagement to maximise a trusted relation with new technologies and\npractitioners. In this paper, we introduce the Observatory on Society and\nArtificial Intelligence (OSAI), an initiative grew out of the project AI4EU\naimed at stimulating reflection on a broad spectrum of issues of AI (ethical,\nlegal, social, economic and cultural). In particular, we describe our work in\nprogress around OSAI and suggest how this and similar initiatives can promote a\nwider appraisal of progress in AI. This will give us the opportunity to present\nour vision and our modus operandi to enhance the implementation of these three\nfundamental dimensions.",
    "published_date": "2020-08-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07326v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04530v1",
    "title": "Report prepared by the Montreal AI Ethics Institute In Response to Mila's Proposal for a Contact Tracing App",
    "authors": [
      "Allison Cohen",
      "Abhishek Gupta"
    ],
    "author_ids": [],
    "abstract": "Contact tracing has grown in popularity as a promising solution to the\nCOVID-19 pandemic. The benefits of automated contact tracing are two-fold.\nContact tracing promises to reduce the number of infections by being able to:\n1) systematically identify all of those that have been in contact with someone\nwho has had COVID; and, 2) ensure those that have been exposed to the virus do\nnot unknowingly infect others. \"COVI\" is the name of a recent contact tracing\napp developed by Mila and was proposed to help combat COVID-19 in Canada. The\napp was designed to inform each individual of their relative risk of being\ninfected with the virus, which Mila claimed would empower citizens to make\ninformed decisions about their movement and allow for a data-driven approach to\npublic health policy; all the while ensuring data is safeguarded from\ngovernments, companies, and individuals. This article will provide a critical\nresponse to Mila's COVI White Paper. Specifically, this article will discuss:\nthe extent to which diversity has been considered in the design of the app,\nassumptions surrounding users' interaction with the app and the app's utility,\nas well as unanswered questions surrounding transparency, accountability, and\nsecurity. We see this as an opportunity to supplement the excellent risk\nanalysis done by the COVI team to surface insights that can be applied to other\ncontact- and proximity-tracing apps that are being developed and deployed\nacross the world. Our hope is that, through a meaningful dialogue, we can\nultimately help organizations develop better solutions that respect the\nfundamental rights and values of the communities these solutions are meant to\nserve.",
    "published_date": "2020-08-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04530v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04520v1",
    "title": "Montreal AI Ethics Institute's (MAIEI) Submission to the World Intellectual Property Organization (WIPO) Conversation on Intellectual Property (IP) and Artificial Intelligence (AI) Second Session",
    "authors": [
      "Allison Cohen",
      "Abhishek Gupta"
    ],
    "author_ids": [],
    "abstract": "This document posits that, at best, a tenuous case can be made for providing\nAI exclusive IP over their \"inventions\". Furthermore, IP protections for AI are\nunlikely to confer the benefit of ensuring regulatory compliance. Rather, IP\nprotections for AI \"inventors\" present a host of negative externalities and\nobscures the fact that the genuine inventor, deserving of IP, is the human\nagent. This document will conclude by recommending strategies for WIPO to bring\nIP law into the 21st century, enabling it to productively account for AI\n\"inventions\".\n  Theme: IP Protection for AI-Generated and AI-Assisted Works Based on insights\nfrom the Montreal AI Ethics Institute (MAIEI) staff and supplemented by\nworkshop contributions from the AI Ethics community convened by MAIEI on July\n5, 2020.",
    "published_date": "2020-08-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04520v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04469v2",
    "title": "Key-Nets: Optical Transformation Convolutional Networks for Privacy Preserving Vision Sensors",
    "authors": [
      "Jeffrey Byrne",
      "Brian DeCann",
      "Scott Bloom"
    ],
    "author_ids": [],
    "abstract": "Modern cameras are not designed with computer vision or machine learning as\nthe target application. There is a need for a new class of vision sensors that\nare privacy preserving by design, that do not leak private information and\ncollect only the information necessary for a target machine learning task. In\nthis paper, we introduce key-nets, which are convolutional networks paired with\na custom vision sensor which applies an optical/analog transform such that the\nkey-net can perform exact encrypted inference on this transformed image, but\nthe image is not interpretable by a human or any other key-net. We provide five\nsufficient conditions for an optical transformation suitable for a key-net, and\nshow that generalized stochastic matrices (e.g. scale, bias and fractional\npixel shuffling) satisfy these conditions. We motivate the key-net by showing\nthat without it there is a utility/privacy tradeoff for a network fine-tuned\ndirectly on optically transformed images for face identification and object\ndetection. Finally, we show that a key-net is equivalent to homomorphic\nencryption using a Hill cipher, with an upper bound on memory and runtime that\nscales quadratically with a user specified privacy parameter. Therefore, the\nkey-net is the first practical, efficient and privacy preserving vision sensor\nbased on optical homomorphic encryption.",
    "published_date": "2020-08-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04469v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04393v1",
    "title": "GANBERT: Generative Adversarial Networks with Bidirectional Encoder Representations from Transformers for MRI to PET synthesis",
    "authors": [
      "Hoo-Chang Shin",
      "Alvin Ihsani",
      "Swetha Mandava",
      "Sharath Turuvekere Sreenivas",
      "Christopher Forster",
      "Jiook Cha",
      "Alzheimer's Disease Neuroimaging Initiative"
    ],
    "author_ids": [],
    "abstract": "Synthesizing medical images, such as PET, is a challenging task due to the\nfact that the intensity range is much wider and denser than those in\nphotographs and digital renderings and are often heavily biased toward zero.\nAbove all, intensity values in PET have absolute significance, and are used to\ncompute parameters that are reproducible across the population. Yet, usually\nmuch manual adjustment has to be made in pre-/post- processing when\nsynthesizing PET images, because its intensity ranges can vary a lot, e.g.,\nbetween -100 to 1000 in floating point values. To overcome these challenges, we\nadopt the Bidirectional Encoder Representations from Transformers (BERT)\nalgorithm that has had great success in natural language processing (NLP),\nwhere wide-range floating point intensity values are represented as integers\nranging between 0 to 10000 that resemble a dictionary of natural language\nvocabularies. BERT is then trained to predict a proportion of masked values\nimages, where its \"next sentence prediction (NSP)\" acts as GAN discriminator.\nOur proposed approach, is able to generate PET images from MRI images in wide\nintensity range, with no manual adjustments in pre-/post- processing. It is a\nmethod that can scale and ready to deploy.",
    "published_date": "2020-08-10T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04393v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04254v1",
    "title": "Informative Dropout for Robust Representation Learning: A Shape-bias Perspective",
    "authors": [
      "Baifeng Shi",
      "Dinghuai Zhang",
      "Qi Dai",
      "Zhanxing Zhu",
      "Yadong Mu",
      "Jingdong Wang"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNNs) are known to rely more on local texture\nrather than global shape when making decisions. Recent work also indicates a\nclose relationship between CNN's texture-bias and its robustness against\ndistribution shift, adversarial perturbation, random corruption, etc. In this\nwork, we attempt at improving various kinds of robustness universally by\nalleviating CNN's texture bias. With inspiration from the human visual system,\nwe propose a light-weight model-agnostic method, namely Informative Dropout\n(InfoDrop), to improve interpretability and reduce texture bias. Specifically,\nwe discriminate texture from shape based on local self-information in an image,\nand adopt a Dropout-like algorithm to decorrelate the model output from the\nlocal texture. Through extensive experiments, we observe enhanced robustness\nunder various scenarios (domain generalization, few-shot classification, image\ncorruption, and adversarial perturbation). To the best of our knowledge, this\nwork is one of the earliest attempts to improve different kinds of robustness\nin a unified model, shedding new light on the relationship between shape-bias\nand robustness, also on new approaches to trustworthy machine learning\nalgorithms. Code is available at https://github.com/bfshi/InfoDrop.",
    "published_date": "2020-08-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04254v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.03848v1",
    "title": "Domain Private and Agnostic Feature for Modality Adaptive Face Recognition",
    "authors": [
      "Yingguo Xu",
      "Lei Zhang",
      "Qingyan Duan"
    ],
    "author_ids": [],
    "abstract": "Heterogeneous face recognition is a challenging task due to the large\nmodality discrepancy and insufficient cross-modal samples. Most existing works\nfocus on discriminative feature transformation, metric learning and cross-modal\nface synthesis. However, the fact that cross-modal faces are always coupled by\ndomain (modality) and identity information has received little attention.\nTherefore, how to learn and utilize the domain-private feature and\ndomain-agnostic feature for modality adaptive face recognition is the focus of\nthis work. Specifically, this paper proposes a Feature Aggregation Network\n(FAN), which includes disentangled representation module (DRM), feature fusion\nmodule (FFM) and adaptive penalty metric (APM) learning session. First, in DRM,\ntwo subnetworks, i.e. domain-private network and domain-agnostic network are\nspecially designed for learning modality features and identity features,\nrespectively. Second, in FFM, the identity features are fused with domain\nfeatures to achieve cross-modal bi-directional identity feature transformation,\nwhich, to a large extent, further disentangles the modality information and\nidentity information. Third, considering that the distribution imbalance\nbetween easy and hard pairs exists in cross-modal datasets, which increases the\nrisk of model bias, the identity preserving guided metric learning with\nadaptive hard pairs penalization is proposed in our FAN. The proposed APM also\nguarantees the cross-modality intra-class compactness and inter-class\nseparation. Extensive experiments on benchmark cross-modal face datasets show\nthat our FAN outperforms SOTA methods.",
    "published_date": "2020-08-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03848v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.03813v5",
    "title": "Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination",
    "authors": [
      "Xudong Wang",
      "Ziwei Liu",
      "Stella X. Yu"
    ],
    "author_ids": [],
    "abstract": "Unsupervised feature learning has made great strides with contrastive\nlearning based on instance discrimination and invariant mapping, as benchmarked\non curated class-balanced datasets. However, natural data could be highly\ncorrelated and long-tail distributed. Natural between-instance similarity\nconflicts with the presumed instance distinction, causing unstable training and\npoor performance.\n  Our idea is to discover and integrate between-instance similarity into\ncontrastive learning, not directly by instance grouping, but by cross-level\ndiscrimination (CLD) between instances and local instance groups. While\ninvariant mapping of each instance is imposed by attraction within its\naugmented views, between-instance similarity could emerge from common repulsion\nagainst instance groups.\n  Our batch-wise and cross-view comparisons also greatly improve the\npositive/negative sample ratio of contrastive learning and achieve better\ninvariant mapping. To effect both grouping and discrimination objectives, we\nimpose them on features separately derived from a shared representation. In\naddition, we propose normalized projection heads and unsupervised\nhyper-parameter tuning for the first time.\n  Our extensive experimentation demonstrates that CLD is a lean and powerful\nadd-on to existing methods such as NPID, MoCo, InfoMin, and BYOL on highly\ncorrelated, long-tail, or balanced datasets. It not only achieves new\nstate-of-the-art on self-supervision, semi-supervision, and transfer learning\nbenchmarks, but also beats MoCo v2 and SimCLR on every reported performance\nattained with a much larger compute. CLD effectively brings unsupervised\nlearning closer to natural data and real-world applications. Our code is\npublicly available at: https://github.com/frank-xwang/CLD-UnsupervisedLearning.",
    "published_date": "2020-08-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03813v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.03808v2",
    "title": "Diverse Group Formation Based on Multiple Demographic Features",
    "authors": [
      "Mohammed Alqahtani",
      "Susan Gauch",
      "Omar Salman",
      "Mohammed Ibrahim",
      "Reem Al-Saffar"
    ],
    "author_ids": [],
    "abstract": "The goal of group formation is to build a team to accomplish a specific task.\nAlgorithms are employed to improve the effectiveness of the team so formed and\nthe efficiency of the group selection process. However, there is concern that\nteam formation algorithms could be biased against minorities due to the\nalgorithms themselves or the data on which they are trained. Hence, it is\nessential to build fair team formation systems that incorporate demographic\ninformation into the process of building the group. Although there has been\nextensive work on modeling individuals expertise for expert recommendation and\nor team formation, there has been relatively little prior work on modeling\ndemographics and incorporating demographics into the group formation process.\n  We propose a novel method to represent experts demographic profiles based on\nmultidimensional demographic features. Moreover, we introduce two diversity\nranking algorithms that form a group by considering demographic features along\nwith the minimum required skills. Unlike many ranking algorithms that consider\none Boolean demographic feature (e.g., gender or race), our diversity ranking\nalgorithms consider multiple multivalued demographic attributes simultaneously.\nWe evaluate our proposed algorithms using a real dataset based on members of a\ncomputer science program committee. The result shows that our algorithms form a\nprogram committee that is more diverse with an acceptable loss in utility.",
    "published_date": "2020-08-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03808v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.06312v1",
    "title": "Judging a Book by Its Cover: The Effect of Facial Perception on Centrality in Social Networks",
    "authors": [
      "Dongyu Zhang",
      "Teng Guo",
      "Hanxiao Pan",
      "Jie Hou",
      "Zhitao Feng",
      "Liang Yang",
      "Hongfei Lin",
      "Feng Xia"
    ],
    "author_ids": [],
    "abstract": "Facial appearance matters in social networks. Individuals frequently make\ntrait judgments from facial clues. Although these face-based impressions lack\nthe evidence to determine validity, they are of vital importance, because they\nmay relate to human network-based social behavior, such as seeking certain\nindividuals for help, advice, dating, and cooperation, and thus they may relate\nto centrality in social networks. However, little to no work has investigated\nthe apparent facial traits that influence network centrality, despite the large\namount of research on attributions of the central position including\npersonality and behavior. In this paper, we examine whether perceived traits\nbased on facial appearance affect network centrality by exploring the initial\nstage of social network formation in a first-year college residential area. We\ntook face photos of participants who are freshmen living in the same\nresidential area, and we asked them to nominate community members linking to\ndifferent networks. We then collected facial perception data by requiring other\nparticipants to rate facial images for three main attributions: dominance,\ntrustworthiness, and attractiveness. Meanwhile, we proposed a framework to\ndiscover how facial appearance affects social networks. Our results revealed\nthat perceived facial traits were correlated with the network centrality and\nthat they were indicative to predict the centrality of people in different\nnetworks. Our findings provide psychological evidence regarding the interaction\nbetween faces and network centrality. Our findings also offer insights in to a\ncombination of psychological and social network techniques, and they highlight\nthe function of facial bias in cuing and signaling social traits. To the best\nof our knowledge, we are the first to explore the influence of facial\nperception on centrality in social networks.",
    "published_date": "2020-08-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.06312v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.05395v1",
    "title": "User Popularity-based Packet Scheduling for Congestion Control in Ad-hoc Social Networks",
    "authors": [
      "Feng Xia",
      "Hannan Bin Liaqat",
      "Ahmedin Mohammed Ahmed",
      "Li Liu",
      "Jianhua Ma",
      "Runhe Huang",
      "Amr Tolba"
    ],
    "author_ids": [],
    "abstract": "Traditional ad-hoc network packet scheduling schemes cannot fulfill the\nrequirements of proximity-based ad-hoc social networks (ASNETs) and they do not\nbehave properly in congested environments. To address this issue, we propose a\nuser popularity-based packet scheduling scheme for congestion control in ASNETs\ncalled Pop-aware. The proposed algorithm exploits social popularity of sender\nnodes to prioritize all incoming flows. Pop-aware also provides fairness of\nservice received by each flow. We evaluate the performance of Pop-aware through\na series of simulations. In comparison with some existing scheduling\nalgorithms, Pop-aware performs better in terms of control overhead, total\noverhead, average throughput, packet loss rate, packet delivery rate and\naverage delay.",
    "published_date": "2020-08-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.05395v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.03617v1",
    "title": "Speaker discrimination in humans and machines: Effects of speaking style variability",
    "authors": [
      "Amber Afshan",
      "Jody Kreiman",
      "Abeer Alwan"
    ],
    "author_ids": [],
    "abstract": "Does speaking style variation affect humans' ability to distinguish\nindividuals from their voices? How do humans compare with automatic systems\ndesigned to discriminate between voices? In this paper, we attempt to answer\nthese questions by comparing human and machine speaker discrimination\nperformance for read speech versus casual conversations. Thirty listeners were\nasked to perform a same versus different speaker task. Their performance was\ncompared to a state-of-the-art x-vector/PLDA-based automatic speaker\nverification system. Results showed that both humans and machines performed\nbetter with style-matched stimuli, and human performance was better when\nlisteners were native speakers of American English. Native listeners performed\nbetter than machines in the style-matched conditions (EERs of 6.96% versus\n14.35% for read speech, and 15.12% versus 19.87%, for conversations), but for\nstyle-mismatched conditions, there was no significant difference between native\nlisteners and machines. In all conditions, fusing human responses with machine\nresults showed improvements compared to each alone, suggesting that humans and\nmachines have different approaches to speaker discrimination tasks. Differences\nin the approaches were further confirmed by examining results for individual\nspeakers which showed that the perception of distinct and confused speakers\ndiffered between human listeners and machines.",
    "published_date": "2020-08-08T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03617v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.03549v2",
    "title": "Learning CNN filters from user-drawn image markers for coconut-tree image classification",
    "authors": [
      "Italos Estilon de Souza",
      "Alexandre Xavier Falcão"
    ],
    "author_ids": [],
    "abstract": "Identifying species of trees in aerial images is essential for land-use\nclassification, plantation monitoring, and impact assessment of natural\ndisasters. The manual identification of trees in aerial images is tedious,\ncostly, and error-prone, so automatic classification methods are necessary.\nConvolutional Neural Network (CNN) models have well succeeded in image\nclassification applications from different domains. However, CNN models usually\nrequire intensive manual annotation to create large training sets. One may\nconceptually divide a CNN into convolutional layers for feature extraction and\nfully connected layers for feature space reduction and classification. We\npresent a method that needs a minimal set of user-selected images to train the\nCNN's feature extractor, reducing the number of required images to train the\nfully connected layers. The method learns the filters of each convolutional\nlayer from user-drawn markers in image regions that discriminate classes,\nallowing better user control and understanding of the training process. It does\nnot rely on optimization based on backpropagation, and we demonstrate its\nadvantages on the binary classification of coconut-tree aerial images against\none of the most popular CNN models.",
    "published_date": "2020-08-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03549v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04793v4",
    "title": "Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of AI/AGI Using Multiple Intelligences and Learning Styles",
    "authors": [
      "Andrzej Cichocki",
      "Alexander P. Kuleshov"
    ],
    "author_ids": [],
    "abstract": "This article discusses some trends and concepts in developing new generation\nof future Artificial General Intelligence (AGI) systems which relate to complex\nfacets and different types of human intelligence, especially social, emotional,\nattentional and ethical intelligence. We describe various aspects of multiple\nhuman intelligences and learning styles, which may impact on a variety of AI\nproblem domains. Using the concept of 'multiple intelligences' rather than a\nsingle type of intelligence, we categorize and provide working definitions of\nvarious AGI depending on their cognitive skills or capacities. Future AI\nsystems will be able not only to communicate with human users and each other,\nbut also to efficiently exchange knowledge and wisdom with abilities of\ncooperation, collaboration and even co-creating something new and valuable and\nhave meta-learning capacities. Multi-agent systems such as these can be used to\nsolve problems that would be difficult to solve by any individual intelligent\nagent.\n  Key words: Artificial General Intelligence (AGI), multiple intelligences,\nlearning styles, physical intelligence, emotional intelligence, social\nintelligence, attentional intelligence, moral-ethical intelligence, responsible\ndecision making, creative-innovative intelligence, cognitive functions,\nmeta-learning of AI systems.",
    "published_date": "2020-08-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04793v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.03276v2",
    "title": "Total Variation Diminishing (TVD) method for Elastohydrodynamic Lubrication (EHL) problem on Parallel Computers",
    "authors": [
      "Peeyush Singh",
      "Pravir Dutt"
    ],
    "author_ids": [],
    "abstract": "In this article, we offer a novel numerical approach for the solution of\nelastohydrodynamic lubrication line and point contact problems using a class of\ntotal variation diminishing (TVD) schemes on parallel computers. A direct\nparallel approach is presented by introducing a novel solver named as projected\nalternate quadrant interlocking factorization (PAQIF) by solving discrete\nvariational inequality. For one-dimensional EHL case, we use weighted change in\nNewton-Raphson approximation to compute the Jacobian matrix in the form of a\nbanded matrix by dividing two subregions on the whole computation domain. Such\nsubregion matrices are assembled by measuring the ratio of diffusive\ncoefficient and discrete grid length on the domain of the interest. The banded\nmatrix is then processed to parallel computers for solving discrete linearized\ncomplementarity system using PAQIF algorithm. The idea is easily extended in\ntwo-dimensional EHL case by taking appropriate splitting in x and y alternating\ndirections respectively. Numerical experiments are performed and analyzed to\nvalidate the performance of computed solution on serial and parallel computers.",
    "published_date": "2020-08-07T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03276v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.03071v1",
    "title": "Oversampling Adversarial Network for Class-Imbalanced Fault Diagnosis",
    "authors": [
      "Masoumeh Zareapoor",
      "Pourya Shamsolmoali",
      "Jie Yang"
    ],
    "author_ids": [],
    "abstract": "The collected data from industrial machines are often imbalanced, which poses\na negative effect on learning algorithms. However, this problem becomes more\nchallenging for a mixed type of data or while there is overlapping between\nclasses. Class-imbalance problem requires a robust learning system which can\ntimely predict and classify the data. We propose a new adversarial network for\nsimultaneous classification and fault detection. In particular, we restore the\nbalance in the imbalanced dataset by generating faulty samples from the\nproposed mixture of data distribution. We designed the discriminator of our\nmodel to handle the generated faulty samples to prevent outlier and\noverfitting. We empirically demonstrate that; (i) the discriminator trained\nwith a generator to generates samples from a mixture of normal and faulty data\ndistribution which can be considered as a fault detector; (ii), the quality of\nthe generated faulty samples outperforms the other synthetic resampling\ntechniques. Experimental results show that the proposed model performs well\nwhen comparing to other fault diagnosis methods across several evaluation\nmetrics; in particular, coalescing of generative adversarial network (GAN) and\nfeature matching function is effective at recognizing faulty samples.",
    "published_date": "2020-08-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.03071v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.04095v1",
    "title": "Fighting Deepfake by Exposing the Convolutional Traces on Images",
    "authors": [
      "Luca Guarnera",
      "Oliver Giudice",
      "Sebastiano Battiato"
    ],
    "author_ids": [],
    "abstract": "Advances in Artificial Intelligence and Image Processing are changing the way\npeople interacts with digital images and video. Widespread mobile apps like\nFACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\nproduce extreme transformations on human face photos such gender swap, aging,\netc. The results are utterly realistic and extremely easy to be exploited even\nfor non-experienced users. This kind of media object took the name of Deepfake\nand raised a new challenge in the multimedia forensics field: the Deepfake\ndetection challenge. Indeed, discriminating a Deepfake from a real image could\nbe a difficult task even for human eyes but recent works are trying to apply\nthe same technology used for generating images for discriminating them with\npreliminary good results but with many limitations: employed Convolutional\nNeural Networks are not so robust, demonstrate to be specific to the context\nand tend to extract semantics from images. In this paper, a new approach aimed\nto extract a Deepfake fingerprint from images is proposed. The method is based\non the Expectation-Maximization algorithm trained to detect and extract a\nfingerprint that represents the Convolutional Traces (CT) left by GANs during\nimage generation. The CT demonstrates to have high discriminative power\nachieving better results than state-of-the-art in the Deepfake detection task\nalso proving to be robust to different attacks. Achieving an overall\nclassification accuracy of over 98%, considering Deepfakes from 10 different\nGAN architectures not only involved in images of faces, the CT demonstrates to\nbe reliable and without any dependence on image semantic. Finally, tests\ncarried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\nfake detection task, demonstrated the effectiveness of the proposed technique\non a real-case scenario.",
    "published_date": "2020-08-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02871v2",
    "title": "Fatigue Assessment using ECG and Actigraphy Sensors",
    "authors": [
      "Yang Bai",
      "Yu Guan",
      "Wan-Fai Ng"
    ],
    "author_ids": [],
    "abstract": "Fatigue is one of the key factors in the loss of work efficiency and\nhealth-related quality of life, and most fatigue assessment methods were based\non self-reporting, which may suffer from many factors such as recall bias. To\naddress this issue, we developed an automated system using wearable sensing and\nmachine learning techniques for objective fatigue assessment. ECG/Actigraphy\ndata were collected from subjects in free-living environments. Preprocessing\nand feature engineering methods were applied, before interpretable solution and\ndeep learning solution were introduced. Specifically, for interpretable\nsolution, we proposed a feature selection approach which can select less\ncorrelated and high informative features for better understanding system's\ndecision-making process. For deep learning solution, we used state-of-the-art\nself-attention model, based on which we further proposed a consistency\nself-attention (CSA) mechanism for fatigue assessment. Extensive experiments\nwere conducted, and very promising results were achieved.",
    "published_date": "2020-08-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02871v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02866v3",
    "title": "Improving Explainability of Image Classification in Scenarios with Class Overlap: Application to COVID-19 and Pneumonia",
    "authors": [
      "Edward Verenich",
      "Alvaro Velasquez",
      "Nazar Khan",
      "Faraz Hussain"
    ],
    "author_ids": [],
    "abstract": "Trust in predictions made by machine learning models is increased if the\nmodel generalizes well on previously unseen samples and when inference is\naccompanied by cogent explanations of the reasoning behind predictions. In the\nimage classification domain, generalization can be assessed through accuracy,\nsensitivity, and specificity. Explainability can be assessed by how well the\nmodel localizes the object of interest within an image. However, both\ngeneralization and explainability through localization are degraded in\nscenarios with significant overlap between classes. We propose a method based\non binary expert networks that enhances the explainability of image\nclassifications through better localization by mitigating the model uncertainty\ninduced by class overlap. Our technique performs discriminative localization on\nimages that contain features with significant class overlap, without explicitly\ntraining for localization. Our method is particularly promising in real-world\nclass overlap scenarios, such as COVID-19 and pneumonia, where expertly labeled\ndata for localization is not readily available. This can be useful for early,\nrapid, and trustworthy screening for COVID-19.",
    "published_date": "2020-08-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02866v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02754v2",
    "title": "Discovering and Categorising Language Biases in Reddit",
    "authors": [
      "Xavier Ferrer",
      "Tom van Nuenen",
      "Jose M. Such",
      "Natalia Criado"
    ],
    "author_ids": [],
    "abstract": "We present a data-driven approach using word embeddings to discover and\ncategorise language biases on the discussion platform Reddit. As spaces for\nisolated user communities, platforms such as Reddit are increasingly connected\nto issues of racism, sexism and other forms of discrimination. Hence, there is\na need to monitor the language of these groups. One of the most promising AI\napproaches to trace linguistic biases in large textual datasets involves word\nembeddings, which transform text into high-dimensional dense vectors and\ncapture semantic relations between words. Yet, previous studies require\npredefined sets of potential biases to study, e.g., whether gender is more or\nless associated with particular types of jobs. This makes these approaches\nunfit to deal with smaller and community-centric datasets such as those on\nReddit, which contain smaller vocabularies and slang, as well as biases that\nmay be particular to that community. This paper proposes a data-driven approach\nto automatically discover language biases encoded in the vocabulary of online\ndiscourse communities on Reddit. In our approach, protected attributes are\nconnected to evaluative words found in the data, which are then categorised\nthrough a semantic analysis system. We verify the effectiveness of our method\nby comparing the biases we discover in the Google News dataset with those found\nin previous literature. We then successfully discover gender bias, religion\nbias, and ethnic bias in different Reddit communities. We conclude by\ndiscussing potential application scenarios and limitations of this data-driven\nbias discovery method.",
    "published_date": "2020-08-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "68T50, 68T09, 91D30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02754v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02479v1",
    "title": "Modeling of time series using random forests: theoretical developments",
    "authors": [
      "Richard A. Davis",
      "Mikkel S. Nielsen"
    ],
    "author_ids": [],
    "abstract": "In this paper we study asymptotic properties of random forests within the\nframework of nonlinear time series modeling. While random forests have been\nsuccessfully applied in various fields, the theoretical justification has not\nbeen considered for their use in a time series setting. Under mild conditions,\nwe prove a uniform concentration inequality for regression trees built on\nnonlinear autoregressive processes and, subsequently, we use this result to\nprove consistency for a large class of random forests. The results are\nsupported by various simulations.",
    "published_date": "2020-08-06T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH",
      "62G05, 62G08, 60G10, 60J05, 62M05, 62M10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02479v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02447v3",
    "title": "Functional Regularization for Representation Learning: A Unified Theoretical Perspective",
    "authors": [
      "Siddhant Garg",
      "Yingyu Liang"
    ],
    "author_ids": [],
    "abstract": "Unsupervised and self-supervised learning approaches have become a crucial\ntool to learn representations for downstream prediction tasks. While these\napproaches are widely used in practice and achieve impressive empirical gains,\ntheir theoretical understanding largely lags behind. Towards bridging this gap,\nwe present a unifying perspective where several such approaches can be viewed\nas imposing a regularization on the representation via a learnable function\nusing unlabeled data. We propose a discriminative theoretical framework for\nanalyzing the sample complexity of these approaches, which generalizes the\nframework of (Balcan and Blum, 2010) to allow learnable regularization\nfunctions. Our sample complexity bounds show that, with carefully chosen\nhypothesis classes to exploit the structure in the data, these learnable\nregularization functions can prune the hypothesis space, and help reduce the\namount of labeled data needed. We then provide two concrete examples of\nfunctional regularization, one using auto-encoders and the other using masked\nself-supervision, and apply our framework to quantify the reduction in the\nsample complexity bound of labeled data. We also provide complementary\nempirical results to support our analysis.",
    "published_date": "2020-08-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02447v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02441v1",
    "title": "Group Activity Prediction with Sequential Relational Anticipation Model",
    "authors": [
      "Junwen Chen",
      "Wentao Bao",
      "Yu Kong"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a novel approach to predict group activities given\nthe beginning frames with incomplete activity executions. Existing action\nprediction approaches learn to enhance the representation power of the partial\nobservation. However, for group activity prediction, the relation evolution of\npeople's activity and their positions over time is an important cue for\npredicting group activity. To this end, we propose a sequential relational\nanticipation model (SRAM) that summarizes the relational dynamics in the\npartial observation and progressively anticipates the group representations\nwith rich discriminative information. Our model explicitly anticipates both\nactivity features and positions by two graph auto-encoders, aiming to learn a\ndiscriminative group representation for group activity prediction. Experimental\nresults on two popularly used datasets demonstrate that our approach\nsignificantly outperforms the state-of-the-art activity prediction methods.",
    "published_date": "2020-08-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02441v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02385v1",
    "title": "Efficient MDI Adaptation for n-gram Language Models",
    "authors": [
      "Ruizhe Huang",
      "Ke Li",
      "Ashish Arora",
      "Dan Povey",
      "Sanjeev Khudanpur"
    ],
    "author_ids": [],
    "abstract": "This paper presents an efficient algorithm for n-gram language model\nadaptation under the minimum discrimination information (MDI) principle, where\nan out-of-domain language model is adapted to satisfy the constraints of\nmarginal probabilities of the in-domain data. The challenge for MDI language\nmodel adaptation is its computational complexity. By taking advantage of the\nbackoff structure of n-gram model and the idea of hierarchical training method,\noriginally proposed for maximum entropy (ME) language models, we show that MDI\nadaptation can be computed in linear-time complexity to the inputs in each\niteration. The complexity remains the same as ME models, although MDI is more\ngeneral than ME. This makes MDI adaptation practical for large corpus and\nvocabulary. Experimental results confirm the scalability of our algorithm on\nvery large datasets, while MDI adaptation gets slightly worse perplexity but\nbetter word error rate results compared to simple linear interpolation.",
    "published_date": "2020-08-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02385v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02359v2",
    "title": "Risk, Trust, and Bias: Causal Regulators of Biometric-Enabled Decision Support",
    "authors": [
      "Kenneth Lai",
      "Helder C. R. Oliveira",
      "Ming Hou",
      "Svetlana N. Yanushkevich",
      "Vlad P. Shmerko"
    ],
    "author_ids": [],
    "abstract": "Biometrics and biometric-enabled decision support systems (DSS) have become a\nmandatory part of complex dynamic systems such as security checkpoints,\npersonal health monitoring systems, autonomous robots, and epidemiological\nsurveillance. Risk, trust, and bias (R-T-B) are emerging measures of\nperformance of such systems. The existing studies on the R-T-B impact on system\nperformance mostly ignore the complementary nature of R-T-B and their causal\nrelationships, for instance, risk of trust, risk of bias, and risk of trust\nover biases. This paper offers a complete taxonomy of the R-T-B causal\nperformance regulators for the biometric-enabled DSS. The proposed novel\ntaxonomy links the R-T-B assessment to the causal inference mechanism for\nreasoning in decision making. Practical details of the R-T-B assessment in the\nDSS are demonstrated using the experiments of assessing the trust in synthetic\nbiometric and the risk of bias in face biometrics. The paper also outlines the\nemerging applications of the proposed approach beyond biometrics, including\ndecision support for epidemiological surveillance such as for COVID-19\npandemics.",
    "published_date": "2020-08-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02359v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.02275v6",
    "title": "Aligning AI With Shared Human Values",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andrew Critch",
      "Jerry Li",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "author_ids": [],
    "abstract": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.",
    "published_date": "2020-08-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02275v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02214v1",
    "title": "Machine Learning Fairness in Justice Systems: Base Rates, False Positives, and False Negatives",
    "authors": [
      "Jesse Russell"
    ],
    "author_ids": [],
    "abstract": "Machine learning best practice statements have proliferated, but there is a\nlack of consensus on what the standards should be. For fairness standards in\nparticular, there is little guidance on how fairness might be achieved in\npractice. Specifically, fairness in errors (both false negatives and false\npositives) can pose a problem of how to set weights, how to make unavoidable\ntradeoffs, and how to judge models that present different kinds of errors\nacross racial groups. This paper considers the consequences of having higher\nrates of false positives for one racial group and higher rates of false\nnegatives for another racial group. The paper examines how different errors in\njustice settings can present problems for machine learning applications, the\nlimits of computation for resolving tradeoffs, and how solutions might have to\nbe crafted through courageous conversations with leadership, line workers,\nstakeholders, and impacted communities.",
    "published_date": "2020-08-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02214v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02047v1",
    "title": "Multiple Texts as a Limiting Factor in Online Learning: Quantifying (Dis-)similarities of Knowledge Networks across Languages",
    "authors": [
      "Alexander Mehler",
      "Wahed Hemati",
      "Pascal Welke",
      "Maxim Konca",
      "Tolga Uslu"
    ],
    "author_ids": [],
    "abstract": "We test the hypothesis that the extent to which one obtains information on a\ngiven topic through Wikipedia depends on the language in which it is consulted.\nControlling the size factor, we investigate this hypothesis for a number of 25\nsubject areas. Since Wikipedia is a central part of the web-based information\nlandscape, this indicates a language-related, linguistic bias. The article\ntherefore deals with the question of whether Wikipedia exhibits this kind of\nlinguistic relativity or not. From the perspective of educational science, the\narticle develops a computational model of the information landscape from which\nmultiple texts are drawn as typical input of web-based reading. For this\npurpose, it develops a hybrid model of intra- and intertextual similarity of\ndifferent parts of the information landscape and tests this model on the\nexample of 35 languages and corresponding Wikipedias. In this way the article\nbuilds a bridge between reading research, educational science, Wikipedia\nresearch and computational linguistics.",
    "published_date": "2020-08-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "68T50 (Primary) 68T30, 91F20 (Secondary)",
      "I.2.7; J.5; K.3.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02047v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01942v1",
    "title": "A feature-supervised generative adversarial network for environmental monitoring during hazy days",
    "authors": [
      "Ke Wang",
      "Siyuan Zhang",
      "Junlan Chen",
      "Fan Ren",
      "Lei Xiao"
    ],
    "author_ids": [],
    "abstract": "The adverse haze weather condition has brought considerable difficulties in\nvision-based environmental applications. While, until now, most of the existing\nenvironmental monitoring studies are under ordinary conditions, and the studies\nof complex haze weather conditions have been ignored. Thence, this paper\nproposes a feature-supervised learning network based on generative adversarial\nnetworks (GAN) for environmental monitoring during hazy days. Its main idea is\nto train the model under the supervision of feature maps from the ground truth.\nFour key technical contributions are made in the paper. First, pairs of hazy\nand clean images are used as inputs to supervise the encoding process and\nobtain high-quality feature maps. Second, the basic GAN formulation is modified\nby introducing perception loss, style loss, and feature regularization loss to\ngenerate better results. Third, multi-scale images are applied as the input to\nenhance the performance of discriminator. Finally, a hazy remote sensing\ndataset is created for testing our dehazing method and environmental detection.\nExtensive experimental results show that the proposed method has achieved\nbetter performance than current state-of-the-art methods on both synthetic\ndatasets and real-world remote sensing images.",
    "published_date": "2020-08-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01942v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01864v1",
    "title": "From Human Mesenchymal Stromal Cells to Osteosarcoma Cells Classification by Deep Learning",
    "authors": [
      "Mario D'Acunto",
      "Massimo Martinelli",
      "Davide Moroni"
    ],
    "author_ids": [],
    "abstract": "Early diagnosis of cancer often allows for a more vast choice of therapy\nopportunities. After a cancer diagnosis, staging provides essential information\nabout the extent of disease in the body and the expected response to a\nparticular treatment. The leading importance of classifying cancer patients at\nthe early stage into high or low-risk groups has led many research teams, both\nfrom the biomedical and bioinformatics field, to study the application of Deep\nLearning (DL) methods. The ability of DL to detect critical features from\ncomplex datasets is a significant achievement in early diagnosis and cell\ncancer progression. In this paper, we focus the attention on osteosarcoma.\nOsteosarcoma is one of the primary malignant bone tumors which usually afflicts\npeople in adolescence. Our contribution to the classification of osteosarcoma\ncells is made as follows: a DL approach is applied to discriminate human\nMesenchymal Stromal Cells (MSCs) from osteosarcoma cells and to classify the\ndifferent cell populations under investigation. Glass slides of differ-ent cell\npopulations were cultured including MSCs, differentiated in healthy bone cells\n(osteoblasts) and osteosarcoma cells, both single cell populations or mixed.\nImages of such samples of isolated cells (single-type of mixed) are recorded\nwith traditional optical microscopy. DL is then applied to identify and\nclassify single cells. Proper data augmentation techniques and cross-fold\nvalidation are used to appreciate the capabilities of a convolutional neural\nnetwork to address the cell detection and classification problem. Based on the\nresults obtained on individual cells, and to the versatility and scalability of\nour DL approach, the next step will be its application to discriminate and\nclassify healthy or cancer tissues to advance digital pathology.",
    "published_date": "2020-08-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "I.4; I.4.6; I.2.1; I.2.10"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01864v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01779v5",
    "title": "Cumulative deviation of a subpopulation from the full population",
    "authors": [
      "Mark Tygert"
    ],
    "author_ids": [],
    "abstract": "Assessing equity in treatment of a subpopulation often involves assigning\nnumerical \"scores\" to all individuals in the full population such that similar\nindividuals get similar scores; matching via propensity scores or appropriate\ncovariates is common, for example. Given such scores, individuals with similar\nscores may or may not attain similar outcomes independent of the individuals'\nmemberships in the subpopulation. The traditional graphical methods for\nvisualizing inequities are known as \"reliability diagrams\" or \"calibrations\nplots,\" which bin the scores into a partition of all possible values, and for\neach bin plot both the average outcomes for only individuals in the\nsubpopulation as well as the average outcomes for all individuals; comparing\nthe graph for the subpopulation with that for the full population gives some\nsense of how the averages for the subpopulation deviate from the averages for\nthe full population. Unfortunately, real data sets contain only finitely many\nobservations, limiting the usable resolution of the bins, and so the\nconventional methods can obscure important variations due to the binning.\nFortunately, plotting cumulative deviation of the subpopulation from the full\npopulation as proposed in this paper sidesteps the problematic coarse binning.\nThe cumulative plots encode subpopulation deviation directly as the slopes of\nsecant lines for the graphs. Slope is easy to perceive even when the constant\noffsets of the secant lines are irrelevant. The cumulative approach avoids\nbinning that smooths over deviations of the subpopulation from the full\npopulation. Such cumulative aggregation furnishes both high-resolution\ngraphical methods and simple scalar summary statistics (analogous to those of\nKuiper and of Kolmogorov and Smirnov used in statistical significance testing\nfor comparing probability distributions).",
    "published_date": "2020-08-04T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01779v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.01635v2",
    "title": "Land Use and Land Cover Classification using a Human Group based Particle Swarm Optimization Algorithm with a LSTM classifier on hybrid-pre-processing Remote Sensing Images",
    "authors": [
      "R. Ganesh Babu",
      "K. Uma Maheswari",
      "C. Zarro",
      "B. D. Parameshachari",
      "S. L. Ullo"
    ],
    "author_ids": [],
    "abstract": "Land use and land cover (LULC) classification using remote sensing imagery\nplays a vital role in many environment modeling and land use inventories. In\nthis study, a hybrid feature optimization algorithm along with a deep learning\nclassifier is proposed to improve the performance of LULC classification,\nhelping to predict wildlife habitat, deteriorating environmental quality,\nhaphazard, etc. LULC classification is assessed using Sat 4, Sat 6 and Eurosat\ndatasets. After the selection of remote sensing images, normalization and\nhistogram equalization methods are used to improve the quality of the images.\nThen, a hybrid optimization is accomplished by using the Local Gabor Binary\nPattern Histogram Sequence (LGBPHS), the Histogram of Oriented Gradient (HOG)\nand Haralick texture features, for the feature extraction from the selected\nimages. The benefits of this hybrid optimization are a high discriminative\npower and invariance to color and grayscale images. Next, a Human Group based\nParticle Swarm Optimization (PSO) algorithm is applied to select the optimal\nfeatures, whose benefits are fast convergence rate and easy to implement. After\nselecting the optimal feature values, a Long Short Term Memory (LSTM) network\nis utilized to classify the LULC classes. Experimental results showed that the\nHuman Group based PSO algorithm with a LSTM classifier effectively well\ndifferentiates the land use and land cover classes in terms of classification\naccuracy, recall and precision. An improvement of 2.56% in accuracy is achieved\ncompared to the existing models GoogleNet, VGG, AlexNet, ConvNet, when the\nproposed method is applied.",
    "published_date": "2020-08-04T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01635v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01475v2",
    "title": "Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap",
    "authors": [
      "Lingxi Xie",
      "Xin Chen",
      "Kaifeng Bi",
      "Longhui Wei",
      "Yuhui Xu",
      "Zhengsu Chen",
      "Lanfei Wang",
      "An Xiao",
      "Jianlong Chang",
      "Xiaopeng Zhang",
      "Qi Tian"
    ],
    "author_ids": [],
    "abstract": "Neural architecture search (NAS) has attracted increasing attentions in both\nacademia and industry. In the early age, researchers mostly applied individual\nsearch methods which sample and evaluate the candidate architectures separately\nand thus incur heavy computational overheads. To alleviate the burden,\nweight-sharing methods were proposed in which exponentially many architectures\nshare weights in the same super-network, and the costly training procedure is\nperformed only once. These methods, though being much faster, often suffer the\nissue of instability. This paper provides a literature review on NAS, in\nparticular the weight-sharing methods, and points out that the major challenge\ncomes from the optimization gap between the super-network and the\nsub-architectures. From this perspective, we summarize existing approaches into\nseveral categories according to their efforts in bridging the gap, and analyze\nboth advantages and disadvantages of these methodologies. Finally, we share our\nopinions on the future directions of NAS and AutoML. Due to the expertise of\nthe authors, this paper mainly focuses on the application of NAS to computer\nvision problems and may bias towards the work in our group.",
    "published_date": "2020-08-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01475v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01430v1",
    "title": "A non-discriminatory approach to ethical deep learning",
    "authors": [
      "Enzo Tartaglione",
      "Marco Grangetto"
    ],
    "author_ids": [],
    "abstract": "Artificial neural networks perform state-of-the-art in an ever-growing number\nof tasks, nowadays they are used to solve an incredibly large variety of tasks.\nHowever, typical training strategies do not take into account lawful, ethical\nand discriminatory potential issues the trained ANN models could incur in. In\nthis work we propose NDR, a non-discriminatory regularization strategy to\nprevent the ANN model to solve the target task using some discriminatory\nfeatures like, for example, the ethnicity in an image classification task for\nhuman faces. In particular, a part of the ANN model is trained to hide the\ndiscriminatory information such that the rest of the network focuses in\nlearning the given learning task. Our experiments show that NDR can be\nexploited to achieve non-discriminatory models with both minimal computational\noverhead and performance loss.",
    "published_date": "2020-08-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01246v2",
    "title": "Noise Contrastive Estimation for Autoencoding-based One-Class Collaborative Filtering",
    "authors": [
      "Jin Peng Zhou",
      "Ga Wu",
      "Zheda Mai",
      "Scott Sanner"
    ],
    "author_ids": [],
    "abstract": "One-class collaborative filtering (OC-CF) is a common class of recommendation\nproblem where only the positive class is explicitly observed (e.g., purchases,\nclicks). Autoencoder based recommenders such as AutoRec and variants\ndemonstrate strong performance on many OC-CF benchmarks, but also empirically\nsuffer from a strong popularity bias. While a careful choice of negative\nsamples in the OC-CF setting can mitigate popularity bias, Negative Sampling\n(NS) is often better for training embeddings than for the end task itself. To\naddress this, we propose a two-headed AutoRec to first train an embedding layer\nvia one head using Negative Sampling then to train for the final task via the\nsecond head. While this NS-AutoRec improves results for AutoRec and outperforms\nmany state-of-the-art baselines on OC-CF problems, we notice that Negative\nSampling can still take a large amount of time to train. Since Negative\nSampling is known to be a special case of Noise Contrastive Estimation (NCE),\nwe adapt a recently proposed closed-form NCE solution for collaborative\nfiltering to AutoRec yielding NCE-AutoRec. Overall, we show that our novel\ntwo-headed AutoRec models (NCE-AutoRec and NS-AutoRec) successfully mitigate\nthe popularity bias issue and maintain competitive performance in comparison to\nstate-of-the-art recommenders on multiple real-world datasets.",
    "published_date": "2020-08-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01246v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01132v3",
    "title": "Accuracy and Fairness Trade-offs in Machine Learning: A Stochastic Multi-Objective Approach",
    "authors": [
      "Suyun Liu",
      "Luis Nunes Vicente"
    ],
    "author_ids": [],
    "abstract": "In the application of machine learning to real-life decision-making systems,\ne.g., credit scoring and criminal justice, the prediction outcomes might\ndiscriminate against people with sensitive attributes, leading to unfairness.\nThe commonly used strategy in fair machine learning is to include fairness as a\nconstraint or a penalization term in the minimization of the prediction loss,\nwhich ultimately limits the information given to decision-makers. In this\npaper, we introduce a new approach to handle fairness by formulating a\nstochastic multi-objective optimization problem for which the corresponding\nPareto fronts uniquely and comprehensively define the accuracy-fairness\ntrade-offs. We have then applied a stochastic approximation-type method to\nefficiently obtain well-spread and accurate Pareto fronts, and by doing so we\ncan handle training data arriving in a streaming way.",
    "published_date": "2020-08-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01132v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01068v2",
    "title": "Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination",
    "authors": [
      "Peng-Shuai Wang",
      "Yu-Qi Yang",
      "Qian-Fang Zou",
      "Zhirong Wu",
      "Yang Liu",
      "Xin Tong"
    ],
    "author_ids": [],
    "abstract": "Although unsupervised feature learning has demonstrated its advantages to\nreducing the workload of data labeling and network design in many fields,\nexisting unsupervised 3D learning methods still cannot offer a generic network\nfor various shape analysis tasks with competitive performance to supervised\nmethods. In this paper, we propose an unsupervised method for learning a\ngeneric and efficient shape encoding network for different shape analysis\ntasks. The key idea of our method is to jointly encode and learn shape and\npoint features from unlabeled 3D point clouds. For this purpose, we adapt\nHR-Net to octree-based convolutional neural networks for jointly encoding shape\nand point features with fused multiresolution subnetworks and design a\nsimple-yet-efficient Multiresolution Instance Discrimination (MID) loss for\njointly learning the shape and point features. Our network takes a 3D point\ncloud as input and output both shape and point features. After training, the\nnetwork is concatenated with simple task-specific back-end layers and\nfine-tuned for different shape analysis tasks. We evaluate the efficacy and\ngenerality of our method and validate our network and loss design with a set of\nshape analysis tasks, including shape classification, semantic shape\nsegmentation, as well as shape registration tasks. With simple back-ends, our\nnetwork demonstrates the best performance among all unsupervised methods and\nachieves competitive performance to supervised methods, especially in tasks\nwith a small labeled dataset. For fine-grained shape segmentation, our method\neven surpasses existing supervised methods by a large margin.",
    "published_date": "2020-08-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01068v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.00859v2",
    "title": "Adversarial Graph Representation Adaptation for Cross-Domain Facial Expression Recognition",
    "authors": [
      "Yuan Xie",
      "Tianshui Chen",
      "Tao Pu",
      "Hefeng Wu",
      "Liang Lin"
    ],
    "author_ids": [],
    "abstract": "Data inconsistency and bias are inevitable among different facial expression\nrecognition (FER) datasets due to subjective annotating process and different\ncollecting conditions. Recent works resort to adversarial mechanisms that learn\ndomain-invariant features to mitigate domain shift. However, most of these\nworks focus on holistic feature adaptation, and they ignore local features that\nare more transferable across different datasets. Moreover, local features carry\nmore detailed and discriminative content for expression recognition, and thus\nintegrating local features may enable fine-grained adaptation. In this work, we\npropose a novel Adversarial Graph Representation Adaptation (AGRA) framework\nthat unifies graph representation propagation with adversarial learning for\ncross-domain holistic-local feature co-adaptation. To achieve this, we first\nbuild a graph to correlate holistic and local regions within each domain and\nanother graph to correlate these regions across different domains. Then, we\nlearn the per-class statistical distribution of each domain and extract\nholistic-local features from the input image to initialize the corresponding\ngraph nodes. Finally, we introduce two stacked graph convolution networks to\npropagate holistic-local feature within each domain to explore their\ninteraction and across different domains for holistic-local feature\nco-adaptation. In this way, the AGRA framework can adaptively learn\nfine-grained domain-invariant features and thus facilitate cross-domain\nexpression recognition. We conduct extensive and fair experiments on several\npopular benchmarks and show that the proposed AGRA framework achieves superior\nperformance over previous state-of-the-art methods.",
    "published_date": "2020-08-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00859v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.00397v2",
    "title": "SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space",
    "authors": [
      "Liu Yang"
    ],
    "author_ids": [],
    "abstract": "In this work, we formulate a visual dialog as an information flow in which\neach piece of information is encoded with the joint visual-linguistic\nrepresentation of a single dialog round. Based on this formulation, we consider\nthe visual dialog task as a sequence problem consisting of ordered\nvisual-linguistic vectors. For featurization, we use a Dense Symmetric\nCo-Attention network as a lightweight vison-language joint representation\ngenerator to fuse multimodal features (i.e., image and text), yielding better\ncomputation and data efficiencies. For inference, we propose two Sequential\nDialog Networks (SeqDialN): the first uses LSTM for information propagation\n(IP) and the second uses a modified Transformer for multi-step reasoning (MR).\nOur architecture separates the complexity of multimodal feature fusion from\nthat of inference, which allows simpler design of the inference engine. IP\nbased SeqDialN is our baseline with a simple 2-layer LSTM design that achieves\ndecent performance. MR based SeqDialN, on the other hand, recurrently refines\nthe semantic question/history representations through the self-attention stack\nof Transformer and produces promising results on the visual dialog task. On\nVisDial v1.0 test-std dataset, our best single generative SeqDialN achieves\n62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78%\nNDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialog\nmodel. We fine-tune discriminative SeqDialN with dense annotations and boost\nthe performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss the\nextensive experiments we have conducted to demonstrate the effectiveness of our\nmodel components. We also provide visualization for the reasoning process from\nthe relevant conversation rounds and discuss our fine-tuning methods. Our code\nis available at https://github.com/xiaoxiaoheimei/SeqDialN",
    "published_date": "2020-08-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00397v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.00371v1",
    "title": "Análisis jurídico de la discriminación algorítmica en los procesos de selección laboral",
    "authors": [
      "Andrés Páez",
      "Natalia Ramírez-Bustamante"
    ],
    "author_ids": [],
    "abstract": "The use of machine learning systems in processing job applications has made\nthe process agile and efficient, but at the same time it has created problems\nin terms of equality, reliability and transparency. In this paper we explain\nsome of the uses of ML in job selection processes in the United States, and we\npresent some the racial and sexual biases that have been detected. There are\nboth practical and legal obstacles that impede the detection and analysis of\nthese biases. It is also unclear how to approach algorithmic discrimination\nfrom a legal point of view. A possible analytical tool is provided by the\nAmerican doctrine of Disparate Impact, but we show some of its limitations and\nproblems when adapted to other legal systems, such as Colombian law. To\nconclude, we offer some desiderata that any legal analysis of algorithmic\ndiscrimination should provide.",
    "published_date": "2020-08-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.00285v1",
    "title": "Dividing Bads is Harder than Dividing Goods: On the Complexity of Fair and Efficient Division of Chores",
    "authors": [
      "Bhaskar Ray Chaudhury",
      "Jugal Garg",
      "Peter McGlaughlin",
      "Ruta Mehta"
    ],
    "author_ids": [],
    "abstract": "We study the chore division problem where a set of agents needs to divide a\nset of chores (bads) among themselves fairly and efficiently. We assume that\nagents have linear disutility (cost) functions. Like for the case of goods,\ncompetitive division is known to be arguably the best mechanism for the bads as\nwell. However, unlike goods, there are multiple competitive divisions with very\ndifferent disutility value profiles in bads. Although all competitive divisions\nsatisfy the standard notions of fairness and efficiency, some divisions are\nsignificantly fairer and efficient than the others. This raises two important\nnatural questions: Does there exist a competitive division in which no agent is\nassigned a chore that she hugely dislikes? Are there simple sufficient\nconditions for the existence and polynomial-time algorithms assuming them?\n  We investigate both these questions in this paper. We show that the first\nproblem is strongly NP-hard. Further, we derive a simple sufficient condition\nfor the existence, and we show that finding a competitive division is PPAD-hard\nassuming the condition. These results are in sharp contrast to the case of\ngoods where both problems are strongly polynomial-time solvable. To the best of\nour knowledge, these are the first hardness results for the chore division\nproblem, and, in fact, for any economic model under linear preferences.",
    "published_date": "2020-08-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "Computer Science"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00285v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.00229v1",
    "title": "Standardized Green View Index and Quantification of Different Metrics of Urban Green Vegetation",
    "authors": [
      "Yusuke Kumakoshi",
      "Sau Yee Chan",
      "Hideki Koizumi",
      "Xiaojiang Li",
      "Yuji Yoshimura"
    ],
    "author_ids": [],
    "abstract": "Urban greenery is considered an important factor in relation to sustainable\ndevelopment and people's quality of life in the city. Although ways to measure\nurban greenery have been proposed, the characteristics of each metric have not\nbeen fully established, rendering previous researches vulnerable to changes in\ngreenery metrics. To make estimation more robust, this study aims to (1)\npropose an improved indicator of greenery visibility for analytical use\n(standardized GVI; sGVI), and (2) quantify the relation between sGVI and other\ngreenery metrics. Analyzing a data set for Yokohama city, Japan, it is shown\nthat the sGVI, a weighted form of GVI aggregated to an area, mitigates the bias\nof densely located measurement sites. Also, by comparing sGVI and NDVI at city\nblock level, we found that sGVI captures the presence of vegetation better in\nthe city center, whereas NDVI is better in capturing vegetation in parks and\nforests. These tools provide a foundation for accessing the effect of\nvegetation in urban landscapes in a more robust matter, enabling comparison on\nany arbitrary geographical scale.",
    "published_date": "2020-08-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00229v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.00207v1",
    "title": "Online Task Scheduling for Fog Computing with Multi-Resource Fairness",
    "authors": [
      "Simeng Bian",
      "Xi Huang",
      "Ziyu Shao"
    ],
    "author_ids": [],
    "abstract": "In fog computing systems, one key challenge is online task scheduling, i.e.,\nto decide the resource allocation for tasks that are continuously generated\nfrom end devices. The design is challenging because of various uncertainties\nmanifested in fog computing systems; e.g., tasks' resource demands remain\nunknown before their actual arrivals. Recent works have applied deep\nreinforcement learning (DRL) techniques to conduct online task scheduling and\nimprove various objectives. However, they overlook the multi-resource fairness\nfor different tasks, which is key to achieving fair resource sharing among\ntasks but in general non-trivial to achieve. Thusly, it is still an open\nproblem to design an online task scheduling scheme with multi-resource\nfairness. In this paper, we address the above challenges. Particularly, by\nleveraging DRL techniques and adopting the idea of dominant resource fairness\n(DRF), we propose FairTS, an online task scheduling scheme that learns directly\nfrom experience to effectively shorten average task slowdown while ensuring\nmulti-resource fairness among tasks. Simulation results show that FairTS\noutperforms state-of-the-art schemes with an ultra-low task slowdown and better\nresource fairness.",
    "published_date": "2020-08-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00207v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07366v1",
    "title": "Using LDA and LSTM Models to Study Public Opinions and Critical Groups Towards Congestion Pricing in New York City through 2007 to 2019",
    "authors": [
      "Qian Ye",
      "Xiaohong Chen",
      "Onur Kalan",
      "Kaan Ozbay"
    ],
    "author_ids": [],
    "abstract": "This study explores how people view and respond to the proposals of NYC\ncongestion pricing evolve in time. To understand these responses, Twitter data\nis collected and analyzed. Critical groups in the recurrent process are\ndetected by statistically analyzing the active users and the most mentioned\naccounts, and the trends of people's attitudes and concerns over the years are\nidentified with text mining and hybrid Nature Language Processing techniques,\nincluding LDA topic modeling and LSTM sentiment classification. The result\nshows that multiple interest groups were involved and played crucial roles\nduring the proposal, especially Mayor and Governor, MTA, and outer-borough\nrepresentatives. The public shifted the concern of focus from the plan details\nto a wider city's sustainability and fairness. Furthermore, the plan's approval\nrelies on several elements, the joint agreement reached in the political\nprocess, strong motivation in the real-world, the scheme based on balancing\nmultiple interests, and groups' awareness of tolling's benefits and necessity.",
    "published_date": "2020-08-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07366v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.00144v1",
    "title": "Solving Elliptic Equations with Brownian Motion: Bias Reduction and Temporal Difference Learning",
    "authors": [
      "Cameron Martin",
      "Hongyuan Zhang",
      "Julia Costacurta",
      "Mihai Nica",
      "Adam R Stinchcombe"
    ],
    "author_ids": [],
    "abstract": "The Feynman-Kac formula provides a way to understand solutions to elliptic\npartial differential equations in terms of expectations of continuous time\nMarkov processes. This connection allows for the creation of numerical schemes\nfor solutions based on samples of these Markov processes which have advantages\nover traditional numerical methods in some cases. However, na\\\"ive numerical\nimplementations suffer from statistical bias and sampling error. We present\nmethods to discretize the stochastic process appearing in the Feynman-Kac\nformula that reduce the bias of the numerical scheme. We also propose using\ntemporal difference learning to assemble information from random samples in a\nway that is more efficient than the traditional Monte Carlo method.",
    "published_date": "2020-08-01T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.PR",
      "65N75 (Primary) 65C05 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00144v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.00138v1",
    "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
    "authors": [
      "Hossein Aboutalebi",
      "Mohammad Javad Shafiee",
      "Michelle Karg",
      "Christian Scharfenberger",
      "Alexander Wong"
    ],
    "author_ids": [],
    "abstract": "Prior studies have unveiled the vulnerability of the deep neural networks in\nthe context of adversarial machine learning, leading to great recent attention\ninto this area. One interesting question that has yet to be fully explored is\nthe bias-variance relationship of adversarial machine learning, which can\npotentially provide deeper insights into this behaviour. The notion of bias and\nvariance is one of the main approaches to analyze and evaluate the\ngeneralization and reliability of a machine learning model. Although it has\nbeen extensively used in other machine learning models, it is not well explored\nin the field of deep learning and it is even less explored in the area of\nadversarial machine learning.\n  In this study, we investigate the effect of adversarial machine learning on\nthe bias and variance of a trained deep neural network and analyze how\nadversarial perturbations can affect the generalization of a network. We derive\nthe bias-variance trade-off for both classification and regression applications\nbased on two main loss functions: (i) mean squared error (MSE), and (ii)\ncross-entropy. Furthermore, we perform quantitative analysis with both\nsimulated and real data to empirically evaluate consistency with the derived\nbias-variance tradeoffs. Our analysis sheds light on why the deep neural\nnetworks have poor performance under adversarial perturbation from a\nbias-variance point of view and how this type of perturbation would change the\nperformance of a network. Moreover, given these new theoretical findings, we\nintroduce a new adversarial machine learning algorithm with lower computational\ncomplexity than well-known adversarial machine learning strategies (e.g., PGD)\nwhile providing a high success rate in fooling deep neural networks in lower\nperturbation magnitudes.",
    "published_date": "2020-08-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00138v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.00104v2",
    "title": "Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach",
    "authors": [
      "Martin Mladenov",
      "Elliot Creager",
      "Omer Ben-Porat",
      "Kevin Swersky",
      "Richard Zemel",
      "Craig Boutilier"
    ],
    "author_ids": [],
    "abstract": "Most recommender systems (RS) research assumes that a user's utility can be\nmaximized independently of the utility of the other agents (e.g., other users,\ncontent providers). In realistic settings, this is often not true---the\ndynamics of an RS ecosystem couple the long-term utility of all agents. In this\nwork, we explore settings in which content providers cannot remain viable\nunless they receive a certain level of user engagement. We formulate the\nrecommendation problem in this setting as one of equilibrium selection in the\ninduced dynamical system, and show that it can be solved as an optimal\nconstrained matching problem. Our model ensures the system reaches an\nequilibrium with maximal social welfare supported by a sufficiently diverse set\nof viable providers. We demonstrate that even in a simple, stylized dynamical\nRS model, the standard myopic approach to recommendation---always matching a\nuser to the best provider---performs poorly. We develop several scalable\ntechniques to solve the matching problem, and also draw connections to various\nnotions of user regret and fairness, arguing that these outcomes are fairer in\na utilitarian sense.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.00104v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.16117v3",
    "title": "Predictability and Fairness in Social Sensing",
    "authors": [
      "Ramen Ghosh",
      "Jakub Marecek",
      "Wynita M. Griggs",
      "Matheus Souza",
      "Robert N. Shorten"
    ],
    "author_ids": [],
    "abstract": "We consider the design of distributed algorithms that govern the manner in\nwhich agents contribute to a social sensing platform. Specifically, we are\ninterested in situations where fairness among the agents contributing to the\nplatform is needed. A notable example are platforms operated by public bodies,\nwhere fairness is a legal requirement. The design of such distributed systems\nis challenging due to the fact that we wish to simultaneously realise an\nefficient social sensing platform, but also deliver a predefined quality of\nservice to the agents (for example, a fair opportunity to contribute to the\nplatform). In this paper, we introduce iterated function systems (IFS) as a\ntool for the design and analysis of systems of this kind. We show how the IFS\nframework can be used to realise systems that deliver a predictable quality of\nservice to agents, can be used to underpin contracts governing the interaction\nof agents with the social sensing platform, and which are efficient.\n  To illustrate our design via a use case, we consider a large, high-density\nnetwork of participating parked vehicles. When awoken by an administrative\ncentre, this network proceeds to search for moving missing entities of interest\nusing RFID-based techniques. We regulate which vehicles are actively searching\nfor the moving entity of interest at any point in time. In doing so, we seek to\nequalise vehicular energy consumption across the network. This is illustrated\nthrough simulations of a search for a missing Alzheimer's patient in Melbourne,\nAustralia. Experimental results are presented to illustrate the efficacy of our\nsystem and the predictability of access of agents to the platform independent\nof initial conditions.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.16117v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.16054v2",
    "title": "Learning to Learn to Compress",
    "authors": [
      "Nannan Zou",
      "Honglei Zhang",
      "Francesco Cricri",
      "Hamed R. Tavakoli",
      "Jani Lainema",
      "Miska Hannuksela",
      "Emre Aksu",
      "Esa Rahtu"
    ],
    "author_ids": [],
    "abstract": "In this paper we present an end-to-end meta-learned system for image\ncompression. Traditional machine learning based approaches to image compression\ntrain one or more neural network for generalization performance. However, at\ninference time, the encoder or the latent tensor output by the encoder can be\noptimized for each test image. This optimization can be regarded as a form of\nadaptation or benevolent overfitting to the input content. In order to reduce\nthe gap between training and inference conditions, we propose a new training\nparadigm for learned image compression, which is based on meta-learning. In a\nfirst phase, the neural networks are trained normally. In a second phase, the\nModel-Agnostic Meta-learning approach is adapted to the specific case of image\ncompression, where the inner-loop performs latent tensor overfitting, and the\nouter loop updates both encoder and decoder neural networks based on the\noverfitting performance. Furthermore, after meta-learning, we propose to\noverfit and cluster the bias terms of the decoder on training image patches, so\nthat at inference time the optimal content-specific bias terms can be selected\nat encoder-side. Finally, we propose a new probability model for lossless\ncompression, which combines concepts from both multi-scale and super-resolution\nprobability model approaches. We show the benefits of all our proposed ideas\nvia carefully designed experiments.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.16054v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15864v2",
    "title": "RoboTed: a case study in Ethical Risk Assessment",
    "authors": [
      "Alan F. T. Winfield",
      "Katie Winkle"
    ],
    "author_ids": [],
    "abstract": "Risk Assessment is a well known and powerful method for discovering and\nmitigating risks, and hence improving safety. Ethical Risk Assessment uses the\nsame approach but extends the envelope of risk to cover ethical risks in\naddition to safety risks. In this paper we outline Ethical Risk Assessment\n(ERA) and set ERA within the broader framework of Responsible Robotics. We then\nillustrate ERA with a case study of a hypothetical smart robot toy teddy bear:\nRoboTed. The case study shows the value of ERA and how consideration of ethical\nrisks can prompt design changes, resulting in a more ethical and sustainable\nrobot.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15864v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.15863v1",
    "title": "Indian Political Twitter and Caste Discrimination -- How Representation Does Not Equal Inclusion in Lok Sabha Networks",
    "authors": [
      "Palashi Vaghela",
      "Ramaravind Kommiya Mothilal",
      "Joyojeet Pal"
    ],
    "author_ids": [],
    "abstract": "Caste privilege persists in the form of upper caste \"networks\" in India made\nup of political, social, and economic relations that tend to actively exclude\nlower caste members. In this study, we examine this pernicious expression of\ncaste in the Twitter networks of politicians from India's highest legislative\nbody - the Lok Sabha. We find that caste has a significant relationship with\nthe centrality, connectivity and engagement of an MP in the Lok Sabha Twitter\nnetwork. The higher the caste of a Member of the Parliament (MP) the more\nlikely they are to be important in the network, to have reciprocal connections\nwith other MPs, and to get retweeted by an upper caste MPs.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15863v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.15861v1",
    "title": "Saliency-driven Class Impressions for Feature Visualization of Deep Neural Networks",
    "authors": [
      "Sravanti Addepalli",
      "Dipesh Tamboli",
      "R. Venkatesh Babu",
      "Biplab Banerjee"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a data-free method of extracting Impressions of\neach class from the classifier's memory. The Deep Learning regime empowers\nclassifiers to extract distinct patterns (or features) of a given class from\ntraining data, which is the basis on which they generalize to unseen data.\nBefore deploying these models on critical applications, it is advantageous to\nvisualize the features considered to be essential for classification. Existing\nvisualization methods develop high confidence images consisting of both\nbackground and foreground features. This makes it hard to judge what the\ncrucial features of a given class are. In this work, we propose a\nsaliency-driven approach to visualize discriminative features that are\nconsidered most important for a given task. Another drawback of existing\nmethods is that confidence of the generated visualizations is increased by\ncreating multiple instances of the given class. We restrict the algorithm to\ndevelop a single object per image, which helps further in extracting features\nof high confidence and also results in better visualizations. We further\ndemonstrate the generation of negative images as naturally fused images of two\nor more classes.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15861v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15816v2",
    "title": "Denoising individual bias for a fairer binary submatrix detection",
    "authors": [
      "Changlin Wan",
      "Wennan Chang",
      "Tong Zhao",
      "Sha Cao",
      "Chi Zhang"
    ],
    "author_ids": [],
    "abstract": "Low rank representation of binary matrix is powerful in disentangling sparse\nindividual-attribute associations, and has received wide applications. Existing\nbinary matrix factorization (BMF) or co-clustering (CC) methods often assume\ni.i.d background noise. However, this assumption could be easily violated in\nreal data, where heterogeneous row- or column-wise probability of binary\nentries results in disparate element-wise background distribution, and\nparalyzes the rationality of existing methods. We propose a binary data\ndenoising framework, namely BIND, which optimizes the detection of true\npatterns by estimating the row- or column-wise mixture distribution of patterns\nand disparate background, and eliminating the binary attributes that are more\nlikely from the background. BIND is supported by thoroughly derived\nmathematical property of the row- and column-wise mixture distributions. Our\nexperiment on synthetic and real-world data demonstrated BIND effectively\nremoves background noise and drastically increases the fairness and accuracy of\nstate-of-the arts BMF and CC methods.",
    "published_date": "2020-07-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15816v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15769v2",
    "title": "Instrument variable detection with graph learning : an application to high dimensional GIS-census data for house pricing",
    "authors": [
      "Ning Xu",
      "Timothy C. G. Fisher",
      "Jian Hong"
    ],
    "author_ids": [],
    "abstract": "Endogeneity bias and instrument variable validation have always been\nimportant topics in statistics and econometrics. In the era of big data, such\nissues typically combine with dimensionality issues and, hence, require even\nmore attention. In this paper, we merge two well-known tools from machine\nlearning and biostatistics---variable selection algorithms and probablistic\ngraphs---to estimate house prices and the corresponding causal structure using\n2010 data on Sydney. The estimation uses a 200-gigabyte ultrahigh dimensional\ndatabase consisting of local school data, GIS information, census data, house\ncharacteristics and other socio-economic records. Using \"big data\", we show\nthat it is possible to perform a data-driven instrument selection efficiently\nand purge out the invalid instruments. Our approach improves the sparsity of\nvariable selection, stability and robustness in the presence of high\ndimensionality, complicated causal structures and the consequent\nmulticollinearity, and recovers a sparse and intuitive causal structure. The\napproach also reveals an efficiency and effectiveness in endogeneity detection,\ninstrument validation, weak instrument pruning and the selection of valid\ninstruments. From the perspective of machine learning, the estimation results\nboth align with and confirms the facts of Sydney house market, the classical\neconomic theories and the previous findings of simultaneous equations modeling.\nMoreover, the estimation results are consistent with and supported by classical\neconometric tools such as two-stage least square regression and different\ninstrument tests. All the code may be found at\n\\url{https://github.com/isaac2math/solar_graph_learning}.",
    "published_date": "2020-07-30T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15769v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15550v2",
    "title": "Combining distributive ethics and causal Inference to make trade-offs between austerity and population health",
    "authors": [
      "Adel Daoud",
      "Anders Herlitz",
      "SV Subramanian"
    ],
    "author_ids": [],
    "abstract": "The International Monetary Fund (IMF) provides financial assistance to its\nmember-countries in economic turmoil, but requires at the same time that these\ncountries reform their public policies. In several contexts, these reforms are\nat odds with population health. While researchers have empirically analyzed the\nconsequences of these reforms on health, no analysis exist on identifying fair\ntradeoffs between consequences on population health and economic outcomes. Our\narticle analyzes and identifies the principles governing these tradeoffs.\nFirst, this article reviews existing policy-evaluation studies, which show, on\nbalance, that IMF policies frequently cause adverse effects on child health and\nmaterial standards in the pursuit of macroeconmic improvement. Second, this\narticle discusses four theories in distributive ethics (maximization,\negalitarianianism, prioritarianiasm, and sufficientarianism) to identify which\nis the most compatible with the core mission of the IMF, that is, improved\nmacroeconomics (Articles of Agreement) while at the same time balancing\nconsequences on health. Using a distributive-ethics analyses of IMF polices, we\nargue that sufficientarianism is the most compatible theory. Third, this\narticle offer a qualitative rearticulation of the Articles of Agreement, and\nformalize sufficientarian principles in the language of causal inference. We\nalso offer a framework on how to empirically measure, from observational data,\nthe extent that IMF policies trade off fairly between population health and\neconomic outcomes. We conclude with policy recommendations and suggestions for\nfuture research.",
    "published_date": "2020-07-30T00:00:00",
    "year": 2020,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15550v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.15270v2",
    "title": "Fairness-Aware Online Personalization",
    "authors": [
      "G Roshan Lal",
      "Sahin Cem Geyik",
      "Krishnaram Kenthapadi"
    ],
    "author_ids": [],
    "abstract": "Decision making in crucial applications such as lending, hiring, and college\nadmissions has witnessed increasing use of algorithmic models and techniques as\na result of a confluence of factors such as ubiquitous connectivity, ability to\ncollect, aggregate, and process large amounts of fine-grained data using cloud\ncomputing, and ease of access to applying sophisticated machine learning\nmodels. Quite often, such applications are powered by search and recommendation\nsystems, which in turn make use of personalized ranking algorithms. At the same\ntime, there is increasing awareness about the ethical and legal challenges\nposed by the use of such data-driven systems. Researchers and practitioners\nfrom different disciplines have recently highlighted the potential for such\nsystems to discriminate against certain population groups, due to biases in the\ndatasets utilized for learning their underlying recommendation models. We\npresent a study of fairness in online personalization settings involving the\nranking of individuals. Starting from a fair warm-start machine-learned model,\nwe first demonstrate that online personalization can cause the model to learn\nto act in an unfair manner if the user is biased in his/her responses. For this\npurpose, we construct a stylized model for generating training data with\npotentially biased features as well as potentially biased labels and quantify\nthe extent of bias that is learned by the model when the user responds in a\nbiased manner as in many real-world scenarios. We then formulate the problem of\nlearning personalized models under fairness constraints and present a\nregularization based approach for mitigating biases in machine learning. We\ndemonstrate the efficacy of our approach through extensive simulations with\ndifferent parameter settings. Code:\nhttps://github.com/groshanlal/Fairness-Aware-Online-Personalization",
    "published_date": "2020-07-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15270v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15182v2",
    "title": "Visual Analysis of Discrimination in Machine Learning",
    "authors": [
      "Qianwen Wang",
      "Zhenhua Xu",
      "Zhutian Chen",
      "Yong Wang",
      "Shixia Liu",
      "Huamin Qu"
    ],
    "author_ids": [],
    "abstract": "The growing use of automated decision-making in critical applications, such\nas crime prediction and college admission, has raised questions about fairness\nin machine learning. How can we decide whether different treatments are\nreasonable or discriminatory? In this paper, we investigate discrimination in\nmachine learning from a visual analytics perspective and propose an interactive\nvisualization tool, DiscriLens, to support a more comprehensive analysis. To\nreveal detailed information on algorithmic discrimination, DiscriLens\nidentifies a collection of potentially discriminatory itemsets based on causal\nmodeling and classification rules mining. By combining an extended Euler\ndiagram with a matrix-based visualization, we develop a novel set visualization\nto facilitate the exploration and interpretation of discriminatory itemsets. A\nuser study shows that users can interpret the visually encoded information in\nDiscriLens quickly and accurately. Use cases demonstrate that DiscriLens\nprovides informative guidance in understanding and reducing algorithmic\ndiscrimination.",
    "published_date": "2020-07-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15182v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15131v3",
    "title": "Learning To Pay Attention To Mistakes",
    "authors": [
      "Mou-Cheng Xu",
      "Neil P. Oxtoby",
      "Daniel C. Alexander",
      "Joseph Jacob"
    ],
    "author_ids": [],
    "abstract": "In convolutional neural network based medical image segmentation, the\nperiphery of foreground regions representing malignant tissues may be\ndisproportionately assigned as belonging to the background class of healthy\ntissues\n\\cite{attenUnet}\\cite{AttenUnet2018}\\cite{InterSeg}\\cite{UnetFrontNeuro}\\cite{LearnActiveContour}.\nThis leads to high false negative detection rates. In this paper, we propose a\nnovel attention mechanism to directly address such high false negative rates,\ncalled Paying Attention to Mistakes. Our attention mechanism steers the models\ntowards false positive identification, which counters the existing bias towards\nfalse negatives. The proposed mechanism has two complementary implementations:\n(a) \"explicit\" steering of the model to attend to a larger Effective Receptive\nField on the foreground areas; (b) \"implicit\" steering towards false positives,\nby attending to a smaller Effective Receptive Field on the background areas. We\nvalidated our methods on three tasks: 1) binary dense prediction between\nvehicles and the background using CityScapes; 2) Enhanced Tumour Core\nsegmentation with multi-modal MRI scans in BRATS2018; 3) segmenting stroke\nlesions using ultrasound images in ISLES2018. We compared our methods with\nstate-of-the-art attention mechanisms in medical imaging, including\nself-attention, spatial-attention and spatial-channel mixed attention. Across\nall of the three different tasks, our models consistently outperform the\nbaseline models in Intersection over Union (IoU) and/or Hausdorff Distance\n(HD). For instance, in the second task, the \"explicit\" implementation of our\nmechanism reduces the HD of the best baseline by more than $26\\%$, whilst\nimproving the IoU by more than $3\\%$. We believe our proposed attention\nmechanism can benefit a wide range of medical and computer vision tasks, which\nsuffer from over-detection of background.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15131v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15032v1",
    "title": "A Flexible and Modular Body-Machine Interface for Individuals Living with Severe Disabilities",
    "authors": [
      "Cheikh Latyr Fall",
      "Ulysse Côté-Allard",
      "Quentin Mascret",
      "Alexandre Campeau-Lecours",
      "Mounir Boukadoum",
      "Clément Gosselin",
      "Benoit Gosselin"
    ],
    "author_ids": [],
    "abstract": "This paper presents a control interface to translate the residual body\nmotions of individuals living with severe disabilities, into control commands\nfor body-machine interaction. A custom, wireless, wearable multi-sensor network\nis used to collect motion data from multiple points on the body in real-time.\nThe solution proposed successfully leverage electromyography gesture\nrecognition techniques for the recognition of inertial measurement units-based\ncommands (IMU), without the need for cumbersome and noisy surface electrodes.\nMotion pattern recognition is performed using a computationally inexpensive\nclassifier (Linear Discriminant Analysis) so that the solution can be deployed\nonto lightweight embedded platforms. Five participants (three able-bodied and\ntwo living with upper-body disabilities) presenting different motion\nlimitations (e.g. spasms, reduced motion range) were recruited. They were asked\nto perform up to 9 different motion classes, including head, shoulder, finger,\nand foot motions, with respect to their residual functional capacities. The\nmeasured prediction performances show an average accuracy of 99.96% for\nable-bodied individuals and 91.66% for participants with upper-body\ndisabilities. The recorded dataset has also been made available online to the\nresearch community. Proof of concept for the real-time use of the system is\ngiven through an assembly task replicating activities of daily living using the\nJACO arm from Kinova Robotics.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15032v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.14964v2",
    "title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "authors": [
      "David Borland",
      "Jonathan Zhang",
      "Smiti Kaul",
      "David Gotz"
    ],
    "author_ids": [],
    "abstract": "The collection and visual analysis of large-scale data from complex systems,\nsuch as electronic health records or clickstream data, has become increasingly\ncommon across a wide range of industries. This type of retrospective visual\nanalysis, however, is prone to a variety of selection bias effects, especially\nfor high-dimensional data where only a subset of dimensions is visualized at\nany given time. The risk of selection bias is even higher when analysts\ndynamically apply filters or perform grouping operations during ad hoc\nanalyses. These bias effects threatens the validity and generalizability of\ninsights discovered during visual analysis as the basis for decision making.\nPast work has focused on bias transparency, helping users understand when\nselection bias may have occurred. However, countering the effects of selection\nbias via bias mitigation is typically left for the user to accomplish as a\nseparate process. Dynamic reweighting (DR) is a novel computational approach to\nselection bias mitigation that helps users craft bias-corrected visualizations.\nThis paper describes the DR workflow, introduces key DR visualization designs,\nand presents statistical methods that support the DR process. Use cases from\nthe medical domain, as well as findings from domain expert user interviews, are\nalso reported.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14964v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.14826v2",
    "title": "Enhanced well-being assessment as basis for the practical implementation of ethical and rights-based normative principles for AI",
    "authors": [
      "Marek Havrda",
      "Bogdana Rakova"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) has an increasing impact on all areas of\npeople's livelihoods. A detailed look at existing interdisciplinary and\ntransdisciplinary metrics frameworks could bring new insights and enable\npractitioners to navigate the challenge of understanding and assessing the\nimpact of Autonomous and Intelligent Systems (A/IS). There has been emerging\nconsensus on fundamental ethical and rights-based AI principles proposed by\nscholars, governments, civil rights organizations, and technology companies. In\norder to move from principles to real-world implementation, we adopt a lens\nmotivated by regulatory impact assessments and the well-being movement in\npublic policy. Similar to public policy interventions, outcomes of AI systems\nimplementation may have far-reaching complex impacts. In public policy,\nindicators are only part of a broader toolbox, as metrics inherently lead to\ngaming and dissolution of incentives and objectives. Similarly, in the case of\nA/IS, there's a need for a larger toolbox that allows for the iterative\nassessment of identified impacts, inclusion of new impacts in the analysis, and\nidentification of emerging trade-offs. In this paper, we propose the practical\napplication of an enhanced well-being impact assessment framework for A/IS that\ncould be employed to address ethical and rights-based normative principles in\nAI. This process could enable a human-centered algorithmically-supported\napproach to the understanding of the impacts of AI systems. Finally, we propose\na new testing infrastructure which would allow for governments, civil rights\norganizations, and others, to engage in cooperating with A/IS developers\ntowards implementation of enhanced well-being impact assessments.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14826v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.14806v1",
    "title": "Towards Domain-Specific Characterization of Misinformation",
    "authors": [
      "Fariha Afsana",
      "Muhammad Ashad Kabir",
      "Naeemul Hassan",
      "Manoranjan Paul"
    ],
    "author_ids": [],
    "abstract": "The rapid dissemination of health misinformation poses an increasing risk to\npublic health. To best understand the way of combating health misinformation,\nit is important to acknowledge how the fundamental characteristics of\nmisinformation differ from domain to domain. This paper presents a pathway\ntowards domain-specific characterization of misinformation so that we can\naddress the concealed behavior of health misinformation compared to others and\ntake proper initiative accordingly for combating it. With this aim, we have\nmentioned several possible approaches to identify discriminating features of\nmedical misinformation from other types of misinformation. Thereafter, we\nbriefly propose a research plan followed by possible challenges to meet up. The\nfindings of the proposed research idea will provide new directions to the\nmisinformation research community.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14806v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.14777v1",
    "title": "PDCOVIDNet: A Parallel-Dilated Convolutional Neural Network Architecture for Detecting COVID-19 from Chest X-Ray Images",
    "authors": [
      "Nihad Karim Chowdhury",
      "Md. Muhtadir Rahman",
      "Muhammad Ashad Kabir"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic continues to severely undermine the prosperity of the\nglobal health system. To combat this pandemic, effective screening techniques\nfor infected patients are indispensable. There is no doubt that the use of\nchest X-ray images for radiological assessment is one of the essential\nscreening techniques. Some of the early studies revealed that the patient's\nchest X-ray images showed abnormalities, which is natural for patients infected\nwith COVID-19. In this paper, we proposed a parallel-dilated convolutional\nneural network (CNN) based COVID-19 detection system from chest x-ray images,\nnamed as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available\nchest X-ray collection fully preloaded and enhanced, and then classified by the\nproposed method. Differing convolution dilation rate in a parallel form\ndemonstrates the proof-of-principle for using PDCOVIDNet to extract\nradiological features for COVID-19 detection. Accordingly, we have assisted our\nmethod with two visualization methods, which are specifically designed to\nincrease understanding of the key components associated with COVID-19\ninfection. Both visualization methods compute gradients for a given image\ncategory related to feature maps of the last convolutional layer to create a\nclass-discriminative region. In our experiment, we used a total of 2,905 chest\nX-ray images, comprising three cases (such as COVID-19, normal, and viral\npneumonia), and empirical evaluations revealed that the proposed method\nextracted more significant features expeditiously related to the suspected\ndisease. The experimental results demonstrate that our proposed method\nsignificantly improves performance metrics: accuracy, precision, recall, and F1\nscores reach 96.58%, 96.58%, 96.59%, and 96.58%, respectively, which is\ncomparable or enhanced compared with the state-of-the-art methods.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14777v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.14775v2",
    "title": "Intersectional Affirmative Action Policies for Top-k Candidates Selection",
    "authors": [
      "Giorgio Barnabo'",
      "Carlos Castillo",
      "Michael Mathioudakis",
      "Sergio Celis"
    ],
    "author_ids": [],
    "abstract": "We study the problem of selecting the top-k candidates from a pool of\napplicants, where each candidate is associated with a score indicating his/her\naptitude. Depending on the specific scenario, such as job search or college\nadmissions, these scores may be the results of standardized tests or other\npredictors of future performance and utility. We consider a situation in which\nsome groups of candidates experience historical and present disadvantage that\nmakes their chances of being accepted much lower than other groups. In these\ncircumstances, we wish to apply an affirmative action policy to reduce\nacceptance rate disparities, while avoiding any large decrease in the aptitude\nof the candidates that are eventually selected. Our algorithmic design is\nmotivated by the frequently observed phenomenon that discrimination\ndisproportionately affects individuals who simultaneously belong to multiple\ndisadvantaged groups, defined along intersecting dimensions such as gender,\nrace, sexual orientation, socio-economic status, and disability. In short, our\nalgorithm's objective is to simultaneously: select candidates with high\nutility, and level up the representation of disadvantaged intersectional\nclasses. This naturally involves trade-offs and is computationally challenging\ndue to the the combinatorial explosion of potential subgroups as more\nattributes are considered. We propose two algorithms to solve this problem,\nanalyze them, and evaluate them experimentally using a dataset of university\napplication scores and admissions to bachelor degrees in an OECD country. Our\nconclusion is that it is possible to significantly reduce disparities in\nadmission rates affecting intersectional classes with a small loss in terms of\nselected candidate aptitude. To the best of our knowledge, we are the first to\nstudy fairness constraints with regards to intersectional classes in the\ncontext of top-k selection.",
    "published_date": "2020-07-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14775v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.14495v3",
    "title": "Class maps for visualizing classification results",
    "authors": [
      "Jakob Raymaekers",
      "Peter J. Rousseeuw",
      "Mia Hubert"
    ],
    "author_ids": [],
    "abstract": "Classification is a major tool of statistics and machine learning. A\nclassification method first processes a training set of objects with given\nclasses (labels), with the goal of afterward assigning new objects to one of\nthese classes. When running the resulting prediction method on the training\ndata or on test data, it can happen that an object is predicted to lie in a\nclass that differs from its given label. This is sometimes called label bias,\nand raises the question whether the object was mislabeled. The proposed class\nmap reflects the probability that an object belongs to an alternative class,\nhow far it is from the other objects in its given class, and whether some\nobjects lie far from all classes. The goal is to visualize aspects of the\nclassification results to obtain insight in the data. The display is\nconstructed for discriminant analysis, the k-nearest neighbor classifier,\nsupport vector machines, logistic regression, and coupling pairwise\nclassifications. It is illustrated on several benchmark datasets, including\nsome about images and texts.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14495v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07341v1",
    "title": "Data, Power and Bias in Artificial Intelligence",
    "authors": [
      "Susan Leavy",
      "Barry O'Sullivan",
      "Eugenia Siapera"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence has the potential to exacerbate societal bias and set\nback decades of advances in equal rights and civil liberty. Data used to train\nmachine learning algorithms may capture social injustices, inequality or\ndiscriminatory attitudes that may be learned and perpetuated in society.\nAttempts to address this issue are rapidly emerging from different perspectives\ninvolving technical solutions, social justice and data governance measures.\nWhile each of these approaches are essential to the development of a\ncomprehensive solution, often discourse associated with each seems disparate.\nThis paper reviews ongoing work to ensure data justice, fairness and bias\nmitigation in AI systems from different domains exploring the interrelated\ndynamics of each and examining whether the inevitability of bias in AI training\ndata may in fact be used for social good. We highlight the complexity\nassociated with defining policies for dealing with bias. We also consider\ntechnical challenges in addressing issues of societal bias.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07341v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.14302v1",
    "title": "Ethics of Artificial Intelligence in Surgery",
    "authors": [
      "Frank Rudzicz",
      "Raeid Saqur"
    ],
    "author_ids": [],
    "abstract": "Here we discuss the four key principles of bio-medical ethics from surgical\ncontext. We elaborate on the definition of 'fairness' and its implications in\nAI system design, with taxonomy of algorithmic biases in AI. We discuss the\nshifts in ethical paradigms as the degree of autonomy in AI systems continue to\nevolve. We also emphasize the need for continuous revisions of ethics in AI due\nto evolution and dynamic nature of AI systems and technologies.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14302v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.14247v5",
    "title": "Downlink channel access performance of NR-U: Impact of numerology and mini-slots on coexistence with Wi-Fi in the 5 GHz band",
    "authors": [
      "Katarzyna Kosek-Szott",
      "Alice Lo Valvo",
      "Szymon Szott",
      "Pierluigi Gallo",
      "Ilenia Tinnirello"
    ],
    "author_ids": [],
    "abstract": "Coexistence between cellular systems and Wi-Fi gained the attention of the\nresearch community when LTE License Assisted Access (LAA) entered the\nunlicensed band. The recent introduction of NR-U as part of 5G introduces new\ncoexistence opportunities because it implements scalable numerology (flexible\nsubcarrier spacing and OFDM symbol lengths), and non-slot based scheduling\n(mini-slots), which considerably impact channel access. This paper analyzes the\nimpact of NR-U settings on its coexistence with Wi-Fi networks and compares it\nwith LAA operation using simulations and experiments. First, we propose a\ndownlink channel access simulation model, which addresses the problem of the\ndependency and non-uniformity of transmission attempts of different nodes, as a\nresult of the synchronization mechanism introduced by NR-U. Second, we validate\nthe accuracy of the proposed model using FPGA-based LAA, NR-U, and Wi-Fi\nprototypes with over-the-air transmissions. Additionally, we show that\nreplacing LAA with NR-U would not only allow to overcome the problem of\nbandwidth wastage caused by reservation signals but also, in some cases, to\npreserve fairness in channel access for both scheduled and random-access\nsystems. Finally, we conclude that fair coexistence of the aforementioned\nsystems in unlicensed bands is not guaranteed in general, and novel mechanisms\nare necessary for improving the sharing of resources between scheduled and\ncontention-based technologies.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "91A06, 91A10, 91A80",
      "C.2.0; C.2.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14247v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.14189v4",
    "title": "TrajGAIL: Generating Urban Vehicle Trajectories using Generative Adversarial Imitation Learning",
    "authors": [
      "Seongjin Choi",
      "Jiwon Kim",
      "Hwasoo Yeo"
    ],
    "author_ids": [],
    "abstract": "Recently, an abundant amount of urban vehicle trajectory data has been\ncollected in road networks. Many studies have used machine learning algorithms\nto analyze patterns in vehicle trajectories to predict location sequences of\nindividual travelers. Unlike the previous studies that used a discriminative\nmodeling approach, this research suggests a generative modeling approach to\nlearn the underlying distributions of urban vehicle trajectory data. A\ngenerative model for urban vehicle trajectories can better generalize from\ntraining data by learning the underlying distribution of the training data and,\nthus, produce synthetic vehicle trajectories similar to real vehicle\ntrajectories with limited observations. Synthetic trajectories can provide\nsolutions to data sparsity or data privacy issues in using location data. This\nresearch proposesTrajGAIL, a generative adversarial imitation learning\nframework for the urban vehicle trajectory generation. In TrajGAIL, learning\nlocation sequences in observed trajectories is formulated as an imitation\nlearning problem in a partially observable Markov decision process. The model\nis trained by the generative adversarial framework, which uses the reward\nfunction from the adversarial discriminator. The model is tested with both\nsimulation and real-world datasets, and the results show that the proposed\nmodel obtained significant performance gains compared to existing models in\nsequence modeling.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14189v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.14071v1",
    "title": "Emotion Correlation Mining Through Deep Learning Models on Natural Language Text",
    "authors": [
      "Xinzhi Wang",
      "Luyao Kou",
      "Vijayan Sugumaran",
      "Xiangfeng Luo",
      "Hui Zhang"
    ],
    "author_ids": [],
    "abstract": "Emotion analysis has been attracting researchers' attention. Most previous\nworks in the artificial intelligence field focus on recognizing emotion rather\nthan mining the reason why emotions are not or wrongly recognized. Correlation\namong emotions contributes to the failure of emotion recognition. In this\npaper, we try to fill the gap between emotion recognition and emotion\ncorrelation mining through natural language text from web news. Correlation\namong emotions, expressed as the confusion and evolution of emotion, is\nprimarily caused by human emotion cognitive bias. To mine emotion correlation\nfrom emotion recognition through text, three kinds of features and two deep\nneural network models are presented. The emotion confusion law is extracted\nthrough orthogonal basis. The emotion evolution law is evaluated from three\nperspectives, one-step shift, limited-step shifts, and shortest path transfer.\nThe method is validated using three datasets-the titles, the bodies, and the\ncomments of news articles, covering both objective and subjective texts in\nvarying lengths (long and short). The experimental results show that, in\nsubjective comments, emotions are easily mistaken as anger. Comments tend to\narouse emotion circulations of love-anger and sadness-anger. In objective news,\nit is easy to recognize text emotion as love and cause fear-joy circulation.\nThat means, journalists may try to attract attention using fear and joy words\nbut arouse the emotion love instead; After news release, netizens generate\nemotional comments to express their intense emotions, i.e., anger, sadness, and\nlove. These findings could provide insights for applications regarding\naffective interaction such as network public sentiment, social media\ncommunication, and human-computer interaction.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.14071v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01548v1",
    "title": "Defining and Evaluating Fair Natural Language Generation",
    "authors": [
      "Catherine Yeo",
      "Alyssa Chen"
    ],
    "author_ids": [],
    "abstract": "Our work focuses on the biases that emerge in the natural language generation\n(NLG) task of sentence completion. In this paper, we introduce a framework of\nfairness for NLG followed by an evaluation of gender biases in two\nstate-of-the-art language models. Our analysis provides a theoretical\nformulation for biases in NLG and empirical evidence that existing language\ngeneration models embed gender bias.",
    "published_date": "2020-07-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01548v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13891v1",
    "title": "Work Practices and Perceptions from Women Core Developers in OSS Communities",
    "authors": [
      "Edna Dias Canedo",
      "Rodrigo Bonifácio",
      "Márcio Vinícius Okimoto",
      "Alexander Serebrenik",
      "Gustavo Pinto",
      "Eduardo Monteiro"
    ],
    "author_ids": [],
    "abstract": "The effect of gender diversity in open source communities has gained\nincreasing attention from practitioners and researchers. For instance,\norganizations such as the Python Software Foundation and the OpenStack\nFoundation started actions to increase gender diversity and promote women to\ntop positions in the communities. Although the general underrepresentation of\nwomen (a.k.a. horizontal segregation) in open source communities has been\nexplored in a number of research studies, little is known about the vertical\nsegregation in open source communities -- which occurs when there are fewer\nwomen in high-level positions. To address this research gap, in this paper we\npresent the results of a mixed-methods study on gender diversity and work\npractices of core developers contributing to open-source communities. In the\nfirst study, we used mining-software repositories procedures to identify the\ncore developers of 711 open source projects, in order to understand how common\nare women core developers in open source communities and characterize their\nwork practices. In the second study, we surveyed the women core developers we\nidentified in the first study to collect their perceptions of gender diversity\nand gender bias they might have observed while contributing to open source\nsystems. Our findings show that open source communities present both horizontal\nand vertical segregation (only 2.3% of the core developers are women).\nNevertheless, differently from previous studies, most of the women core\ndevelopers (65.7%) report never having experienced gender discrimination when\ncontributing to an open source project. Finally, we did not note substantial\ndifferences between the work practices among women and men core developers. We\nreflect on these findings and present some ideas that might increase the\nparticipation of women in open source communities.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13891v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.13798v1",
    "title": "Linguistic Taboos and Euphemisms in Nepali",
    "authors": [
      "Nobal B. Niraula",
      "Saurab Dulal",
      "Diwa Koirala"
    ],
    "author_ids": [],
    "abstract": "Languages across the world have words, phrases, and behaviors -- the taboos\n-- that are avoided in public communication considering them as obscene or\ndisturbing to the social, religious, and ethical values of society. However,\npeople deliberately use these linguistic taboos and other language constructs\nto make hurtful, derogatory, and obscene comments. It is nearly impossible to\nconstruct a universal set of offensive or taboo terms because offensiveness is\ndetermined entirely by different factors such as socio-physical setting,\nspeaker-listener relationship, and word choices. In this paper, we present a\ndetailed corpus-based study of offensive language in Nepali. We identify and\ndescribe more than 18 different categories of linguistic offenses including\npolitics, religion, race, and sex. We discuss 12 common euphemisms such as\nsynonym, metaphor and circumlocution. In addition, we introduce a manually\nconstructed data set of over 1000 offensive and taboo terms popular among\ncontemporary speakers. This in-depth study of offensive language and resource\nwill provide a foundation for several downstream tasks such as offensive\nlanguage detection and language learning.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13798v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13667v1",
    "title": "Performance-Aware Predictive-Model-Based On-Chip Body-Bias Regulation Strategy for an ULP Multi-Core Cluster in 28nm UTBB FD-SOI",
    "authors": [
      "Alfio Di Mauro",
      "Davide Rossi",
      "Antonio Pullini",
      "Philippe Flatresse",
      "Luca Benini"
    ],
    "author_ids": [],
    "abstract": "The performance and reliability of Ultra-Low-Power (ULP) computing platforms\nare adversely affected by environmental temperature and process variations.\nMitigating the effect of these phenomena becomes crucial when these devices\noperate near-threshold, due to the magnification of process variations and to\nthe strong temperature inversion effect that affects advanced technology nodes\nin low-voltage corners, which causes huge overhead due to margining for timing\nclosure. Supporting an extended range of reverse and forward body-bias, UTBB\nFD-SOI technology provides a powerful knob to compensate for such variations.\nIn this work we propose a methodology to maximize energy efficiency at run-time\nexploiting body biasing on a ULP platform operating near-threshold. The\nproposed method relies on on-line performance measurements by means of Process\nMonitoring Blocks (PMBs) coupled with an on-chip low-power body bias generator.\nWe correlate the measurement performed by the PMBs to the maximum achievable\nfrequency of the system, deriving a predictive model able to estimate it with\nan error of 9.7% at 0.7V. To minimize the effect of process variations we\npropose a calibration procedure that allows to use a PMB model affected by only\nthe temperature-induced error, which reduces the frequency estimation error by\n2.4x (from 9.7% to 4%). We finally propose a controller architecture relying on\nthe derived models to automatically regulate at run-time the body bias voltage.\nWe demonstrate that adjusting the body bias voltage against environmental\ntemperature variations leads up to 2X reduction in the leakage power and a 15%\nimprovement on the global energy consumption when the system operates at 0.7V\nand 170MHz",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13667v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.13657v1",
    "title": "Towards Learning Convolutions from Scratch",
    "authors": [
      "Behnam Neyshabur"
    ],
    "author_ids": [],
    "abstract": "Convolution is one of the most essential components of architectures used in\ncomputer vision. As machine learning moves towards reducing the expert bias and\nlearning it from data, a natural next step seems to be learning\nconvolution-like structures from scratch. This, however, has proven elusive.\nFor example, current state-of-the-art architecture search algorithms use\nconvolution as one of the existing modules rather than learning it from data.\nIn an attempt to understand the inductive bias that gives rise to convolutions,\nwe investigate minimum description length as a guiding principle and show that\nin some settings, it can indeed be indicative of the performance of\narchitectures. To find architectures with small description length, we propose\n$\\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on\nfully-connected networks for image classification tasks, learns architectures\nwith local connections and achieves state-of-the-art accuracies for training\nfully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%)\nbridging the gap between fully-connected and convolutional nets.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13657v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13632v2",
    "title": "Towards Accuracy-Fairness Paradox: Adversarial Example-based Data Augmentation for Visual Debiasing",
    "authors": [
      "Yi Zhang",
      "Jitao Sang"
    ],
    "author_ids": [],
    "abstract": "Machine learning fairness concerns about the biases towards certain protected\nor sensitive group of people when addressing the target tasks. This paper\nstudies the debiasing problem in the context of image classification tasks. Our\ndata analysis on facial attribute recognition demonstrates (1) the attribution\nof model bias from imbalanced training data distribution and (2) the potential\nof adversarial examples in balancing data distribution. We are thus motivated\nto employ adversarial example to augment the training data for visual\ndebiasing. Specifically, to ensure the adversarial generalization as well as\ncross-task transferability, we propose to couple the operations of target task\nclassifier training, bias task classifier training, and adversarial example\ngeneration. The generated adversarial examples supplement the target task\ntraining dataset via balancing the distribution over bias variables in an\nonline fashion. Results on simulated and real-world debiasing experiments\ndemonstrate the effectiveness of the proposed solution in simultaneously\nimproving model accuracy and fairness. Preliminary experiment on few-shot\nlearning further shows the potential of adversarial attack-based pseudo sample\ngeneration as alternative solution to make up for the training data lackage.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13632v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13428v1",
    "title": "Two-Level Residual Distillation based Triple Network for Incremental Object Detection",
    "authors": [
      "Dongbao Yang",
      "Yu Zhou",
      "Dayan Wu",
      "Can Ma",
      "Fei Yang",
      "Weiping Wang"
    ],
    "author_ids": [],
    "abstract": "Modern object detection methods based on convolutional neural network suffer\nfrom severe catastrophic forgetting in learning new classes without original\ndata. Due to time consumption, storage burden and privacy of old data, it is\ninadvisable to train the model from scratch with both old and new data when new\nobject classes emerge after the model trained. In this paper, we propose a\nnovel incremental object detector based on Faster R-CNN to continuously learn\nfrom new object classes without using old data. It is a triple network where an\nold model and a residual model as assistants for helping the incremental model\nlearning on new classes without forgetting the previous learned knowledge. To\nbetter maintain the discrimination of features between old and new classes, the\nresidual model is jointly trained on new classes in the incremental learning\nprocedure. In addition, a corresponding distillation scheme is designed to\nguide the training process, which consists of a two-level residual distillation\nloss and a joint classification distillation loss. Extensive experiments on\nVOC2007 and COCO are conducted, and the results demonstrate that the proposed\nmethod can effectively learn to incrementally detect objects of new classes,\nand the problem of catastrophic forgetting is mitigated in this context.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13428v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13314v3",
    "title": "Rethinking Generative Zero-Shot Learning: An Ensemble Learning Perspective for Recognising Visual Patches",
    "authors": [
      "Zhi Chen",
      "Sen Wang",
      "Jingjing Li",
      "Zi Huang"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) is commonly used to address the very pervasive\nproblem of predicting unseen classes in fine-grained image classification and\nother tasks. One family of solutions is to learn synthesised unseen visual\nsamples produced by generative models from auxiliary semantic information, such\nas natural language descriptions. However, for most of these models,\nperformance suffers from noise in the form of irrelevant image backgrounds.\nFurther, most methods do not allocate a calculated weight to each semantic\npatch. Yet, in the real world, the discriminative power of features can be\nquantified and directly leveraged to improve accuracy and reduce computational\ncomplexity. To address these issues, we propose a novel framework called\nmulti-patch generative adversarial nets (MPGAN) that synthesises local patch\nfeatures and labels unseen classes with a novel weighted voting strategy. The\nprocess begins by generating discriminative visual features from noisy text\ndescriptions for a set of predefined local patches using multiple specialist\ngenerative models. The features synthesised from each patch for unseen classes\nare then used to construct an ensemble of diverse supervised classifiers, each\ncorresponding to one local patch. A voting strategy averages the probability\ndistributions output from the classifiers and, given that some patches are more\ndiscriminative than others, a discrimination-based attention mechanism helps to\nweight each patch accordingly. Extensive experiments show that MPGAN has\nsignificantly greater accuracy than state-of-the-art methods.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13314v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13310v2",
    "title": "K-Shot Contrastive Learning of Visual Features with Multiple Instance Augmentations",
    "authors": [
      "Haohang Xu",
      "Hongkai Xiong",
      "Guo-Jun Qi"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose the $K$-Shot Contrastive Learning (KSCL) of visual\nfeatures by applying multiple augmentations to investigate the sample\nvariations within individual instances. It aims to combine the advantages of\ninter-instance discrimination by learning discriminative features to\ndistinguish between different instances, as well as intra-instance variations\nby matching queries against the variants of augmented samples over instances.\nParticularly, for each instance, it constructs an instance subspace to model\nthe configuration of how the significant factors of variations in $K$-shot\naugmentations can be combined to form the variants of augmentations. Given a\nquery, the most relevant variant of instances is then retrieved by projecting\nthe query onto their subspaces to predict the positive instance class. This\ngeneralizes the existing contrastive learning that can be viewed as a special\none-shot case. An eigenvalue decomposition is performed to configure instance\nsubspaces, and the embedding network can be trained end-to-end through the\ndifferentiable subspace configuration. Experiment results demonstrate the\nproposed $K$-shot contrastive learning achieves superior performances to the\nstate-of-the-art unsupervised methods.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13310v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13264v1",
    "title": "Learning Task-oriented Disentangled Representations for Unsupervised Domain Adaptation",
    "authors": [
      "Pingyang Dai",
      "Peixian Chen",
      "Qiong Wu",
      "Xiaopeng Hong",
      "Qixiang Ye",
      "Qi Tian",
      "Rongrong Ji"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptation (UDA) aims to address the domain-shift problem\nbetween a labeled source domain and an unlabeled target domain. Many efforts\nhave been made to address the mismatch between the distributions of training\nand testing data, but unfortunately, they ignore the task-oriented information\nacross domains and are inflexible to perform well in complicated open-set\nscenarios. Many efforts have been made to eliminate the mismatch between the\ndistributions of training and testing data by learning domain-invariant\nrepresentations. However, the learned representations are usually not\ntask-oriented, i.e., being class-discriminative and domain-transferable\nsimultaneously. This drawback limits the flexibility of UDA in complicated\nopen-set tasks where no labels are shared between domains. In this paper, we\nbreak the concept of task-orientation into task-relevance and task-irrelevance,\nand propose a dynamic task-oriented disentangling network (DTDN) to learn\ndisentangled representations in an end-to-end fashion for UDA. The dynamic\ndisentangling network effectively disentangles data representations into two\ncomponents: the task-relevant ones embedding critical information associated\nwith the task across domains, and the task-irrelevant ones with the remaining\nnon-transferable or disturbing information. These two components are\nregularized by a group of task-specific objective functions across domains.\nSuch regularization explicitly encourages disentangling and avoids the use of\ngenerative models or decoders. Experiments in complicated, open-set scenarios\n(retrieval tasks) and empirical benchmarks (classification tasks) demonstrate\nthat the proposed method captures rich disentangled information and achieves\nsuperior performance.",
    "published_date": "2020-07-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13264v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13123v1",
    "title": "Joint reconstruction and bias field correction for undersampled MR imaging",
    "authors": [
      "Mélanie Gaillochet",
      "Kerem C. Tezcan",
      "Ender Konukoglu"
    ],
    "author_ids": [],
    "abstract": "Undersampling the k-space in MRI allows saving precious acquisition time, yet\nresults in an ill-posed inversion problem. Recently, many deep learning\ntechniques have been developed, addressing this issue of recovering the fully\nsampled MR image from the undersampled data. However, these learning based\nschemes are susceptible to differences between the training data and the image\nto be reconstructed at test time. One such difference can be attributed to the\nbias field present in MR images, caused by field inhomogeneities and coil\nsensitivities. In this work, we address the sensitivity of the reconstruction\nproblem to the bias field and propose to model it explicitly in the\nreconstruction, in order to decrease this sensitivity. To this end, we use an\nunsupervised learning based reconstruction algorithm as our basis and combine\nit with a N4-based bias field estimation method, in a joint optimization\nscheme. We use the HCP dataset as well as in-house measured images for the\nevaluations. We show that the proposed method improves the reconstruction\nquality, both visually and in terms of RMSE.",
    "published_date": "2020-07-26T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13123v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.13019v1",
    "title": "Feedback Loop and Bias Amplification in Recommender Systems",
    "authors": [
      "Masoud Mansoury",
      "Himan Abdollahpouri",
      "Mykola Pechenizkiy",
      "Bamshad Mobasher",
      "Robin Burke"
    ],
    "author_ids": [],
    "abstract": "Recommendation algorithms are known to suffer from popularity bias; a few\npopular items are recommended frequently while the majority of other items are\nignored. These recommendations are then consumed by the users, their reaction\nwill be logged and added to the system: what is generally known as a feedback\nloop. In this paper, we propose a method for simulating the users interaction\nwith the recommenders in an offline setting and study the impact of feedback\nloop on the popularity bias amplification of several recommendation algorithms.\nWe then show how this bias amplification leads to several other problems such\nas declining the aggregate diversity, shifting the representation of users'\ntaste over time and also homogenization of the users experience. In particular,\nwe show that the impact of feedback loop is generally stronger for the users\nwho belong to the minority group.",
    "published_date": "2020-07-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.13019v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.12919v1",
    "title": "Interpretabilité des modèles : état des lieux des méthodes et application à l'assurance",
    "authors": [
      "Dimitri Delcaillau",
      "Antoine Ly",
      "Franck Vermet",
      "Alizé Papp"
    ],
    "author_ids": [],
    "abstract": "Since May 2018, the General Data Protection Regulation (GDPR) has introduced\nnew obligations to industries. By setting a legal framework, it notably imposes\nstrong transparency on the use of personal data. Thus, people must be informed\nof the use of their data and must consent the usage of it. Data is the raw\nmaterial of many models which today make it possible to increase the quality\nand performance of digital services. Transparency on the use of data also\nrequires a good understanding of its use through different models. The use of\nmodels, even if efficient, must be accompanied by an understanding at all\nlevels of the process that transform data (upstream and downstream of a model),\nthus making it possible to define the relationships between the individual's\ndata and the choice that an algorithm could make based on the analysis of the\nlatter. (For example, the recommendation of one product or one promotional\noffer or an insurance rate representative of the risk.) Models users must\nensure that models do not discriminate against and that it is also possible to\nexplain its result. The widening of the panel of predictive algorithms - made\npossible by the evolution of computing capacities -- leads scientists to be\nvigilant about the use of models and to consider new tools to better understand\nthe decisions deduced from them . Recently, the community has been particularly\nactive on model transparency with a marked intensification of publications over\nthe past three years. The increasingly frequent use of more complex algorithms\n(\\textit{deep learning}, Xgboost, etc.) presenting attractive performances is\nundoubtedly one of the causes of this interest. This article thus presents an\ninventory of methods of interpreting models and their uses in an insurance\ncontext.",
    "published_date": "2020-07-25T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12919v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.12841v3",
    "title": "Combating Misinformation in Bangladesh: Roles and Responsibilities as Perceived by Journalists, Fact-checkers, and Users",
    "authors": [
      "Md Mahfuzul Haque",
      "Mohammad Yousuf",
      "Ahmed Shatil Alam",
      "Pratyasha Saha",
      "Syed Ishtiaque Ahmed",
      "Naeemul Hassan"
    ],
    "author_ids": [],
    "abstract": "There has been a growing interest within CSCW community in understanding the\ncharacteristics of misinformation propagated through computational media, and\nthe devising techniques to address the associated challenges. However, most\nwork in this area has been concentrated on the cases in the western world\nleaving a major portion of this problem unaddressed that is situated in the\nGlobal South. This paper aims to broaden the scope of this discourse by\nfocusing on this problem in the context of Bangladesh, a country in the Global\nSouth. The spread of misinformation on Facebook in Bangladesh, a country with a\npopulation over 163 million, has resulted in chaos, hate attacks, and killings.\nBy interviewing journalists, fact-checkers, in addition to surveying the\ngeneral public, we analyzed the current state of verifying misinformation in\nBangladesh. Our findings show that most people in the `news audience' want the\nnews media to verify the authenticity of online information that they see\nonline. However, the newspaper journalists say that fact-checking online\ninformation is not a part of their job, and it is also beyond their capacity\ngiven the amount of information being published online everyday. We further\nfind that the voluntary fact-checkers in Bangladesh are not equipped with\nsufficient infrastructural support to fill in this gap. We show how our\nfindings are connected to some of the core concerns of CSCW community around\nsocial media, collaboration, infrastructural politics, and information\ninequality. From our analysis, we also suggest several pathways to increase the\nimpact of fact-checking efforts through collaboration, technology design, and\ninfrastructure development.",
    "published_date": "2020-07-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12841v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.12780v2",
    "title": "A Canonical Architecture For Predictive Analytics on Longitudinal Patient Records",
    "authors": [
      "Parthasarathy Suryanarayanan",
      "Bhavani Iyer",
      "Prithwish Chakraborty",
      "Bibo Hao",
      "Italo Buleje",
      "Piyush Madan",
      "James Codella",
      "Antonio Foncubierta",
      "Divya Pathak",
      "Sarah Miller",
      "Amol Rajmane",
      "Shannon Harrer",
      "Gigi Yuan-Reed",
      "Daby Sow"
    ],
    "author_ids": [],
    "abstract": "Many institutions within the healthcare ecosystem are making significant\ninvestments in AI technologies to optimize their business operations at lower\ncost with improved patient outcomes. Despite the hype with AI, the full\nrealization of this potential is seriously hindered by several systemic\nproblems, including data privacy, security, bias, fairness, and explainability.\nIn this paper, we propose a novel canonical architecture for the development of\nAI models in healthcare that addresses these challenges. This system enables\nthe creation and management of AI predictive models throughout all the phases\nof their life cycle, including data ingestion, model building, and model\npromotion in production environments. This paper describes this architecture in\ndetail, along with a qualitative evaluation of our experience of using it on\nreal world problems.",
    "published_date": "2020-07-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12780v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.12360v1",
    "title": "On the Effectiveness of Image Rotation for Open Set Domain Adaptation",
    "authors": [
      "Silvia Bucci",
      "Mohammad Reza Loghmani",
      "Tatiana Tommasi"
    ],
    "author_ids": [],
    "abstract": "Open Set Domain Adaptation (OSDA) bridges the domain gap between a labeled\nsource domain and an unlabeled target domain, while also rejecting target\nclasses that are not present in the source. To avoid negative transfer, OSDA\ncan be tackled by first separating the known/unknown target samples and then\naligning known target samples with the source data. We propose a novel method\nto addresses both these problems using the self-supervised task of rotation\nrecognition. Moreover, we assess the performance with a new open set metric\nthat properly balances the contribution of recognizing the known classes and\nrejecting the unknown samples. Comparative experiments with existing OSDA\nmethods on the standard Office-31 and Office-Home benchmarks show that: (i) our\nmethod outperforms its competitors, (ii) reproducibility for this field is a\ncrucial issue to tackle, (iii) our metric provides a reliable tool to allow\nfair open set evaluation.",
    "published_date": "2020-07-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.12230v1",
    "title": "Addressing the Multistakeholder Impact of Popularity Bias in Recommendation Through Calibration",
    "authors": [
      "Himan Abdollahpouri",
      "Masoud Mansoury",
      "Robin Burke",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Popularity bias is a well-known phenomenon in recommender systems: popular\nitems are recommended even more frequently than their popularity would warrant,\namplifying long-tail effects already present in many recommendation domains.\nPrior research has examined various approaches for mitigating popularity bias\nand enhancing the recommendation of long-tail items overall. The effectiveness\nof these approaches, however, has not been assessed in multistakeholder\nenvironments where in addition to the users who receive the recommendations,\nthe utility of the suppliers of the recommended items should also be\nconsidered. In this paper, we propose the concept of popularity calibration\nwhich measures the match between the popularity distribution of items in a\nuser's profile and that of the recommended items. We also develop an algorithm\nthat optimizes this metric. In addition, we demonstrate that existing\nevaluation metrics for popularity bias do not reflect the performance of the\nalgorithms when it is measured from the perspective of different stakeholders.\nUsing music and movie datasets, we empirically show that our approach\noutperforms the existing state-of-the-art approaches in addressing popularity\nbias by calibrating the recommendations to users' preferences. We also show\nthat our proposed algorithm has a secondary effect of improving supplier\nfairness.",
    "published_date": "2020-07-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12230v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.11978v5",
    "title": "The Devil is in Classification: A Simple Framework for Long-tail Object Detection and Instance Segmentation",
    "authors": [
      "Tao Wang",
      "Yu Li",
      "Bingyi Kang",
      "Junnan Li",
      "Junhao Liew",
      "Sheng Tang",
      "Steven Hoi",
      "Jiashi Feng"
    ],
    "author_ids": [],
    "abstract": "Most existing object instance detection and segmentation models only work\nwell on fairly balanced benchmarks where per-category training sample numbers\nare comparable, such as COCO. They tend to suffer performance drop on realistic\ndatasets that are usually long-tailed. This work aims to study and address such\nopen challenges. Specifically, we systematically investigate performance drop\nof the state-of-the-art two-stage instance segmentation model Mask R-CNN on the\nrecent long-tail LVIS dataset, and unveil that a major cause is the inaccurate\nclassification of object proposals. Based on such an observation, we first\nconsider various techniques for improving long-tail classification performance\nwhich indeed enhance instance segmentation results. We then propose a simple\ncalibration framework to more effectively alleviate classification head bias\nwith a bi-level class balanced sampling approach. Without bells and whistles,\nit significantly boosts the performance of instance segmentation for tail\nclasses on the recent LVIS dataset and our sampled COCO-LT dataset. Our\nanalysis provides useful insights for solving long-tail instance detection and\nsegmentation problems, and the straightforward \\emph{SimCal} method can serve\nas a simple but strong baseline. With the method we have won the 2019 LVIS\nchallenge. Codes and models are available at https://github.com/twangnh/SimCal.",
    "published_date": "2020-07-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11978v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.11973v1",
    "title": "The societal and ethical relevance of computational creativity",
    "authors": [
      "Michele Loi",
      "Eleonora Viganò",
      "Lonneke van der Plas"
    ],
    "author_ids": [],
    "abstract": "In this paper, we provide a philosophical account of the value of creative\nsystems for individuals and society. We characterize creativity in very broad\nphilosophical terms, encompassing natural, existential, and social creative\nprocesses, such as natural evolution and entrepreneurship, and explain why\ncreativity understood in this way is instrumental for advancing human\nwell-being in the long term. We then explain why current mainstream AI tends to\nbe anti-creative, which means that there are moral costs of employing this type\nof AI in human endeavors, although computational systems that involve\ncreativity are on the rise. In conclusion, there is an argument for ethics to\nbe more hospitable to creativity-enabling AI, which can also be in a trade-off\nwith other values promoted in AI ethics, such as its explainability and\naccuracy.",
    "published_date": "2020-07-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "econ.GN",
      "q-fin.EC",
      "I.2.8"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11973v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.11864v1",
    "title": "Differentiable Hierarchical Graph Grouping for Multi-Person Pose Estimation",
    "authors": [
      "Sheng Jin",
      "Wentao Liu",
      "Enze Xie",
      "Wenhai Wang",
      "Chen Qian",
      "Wanli Ouyang",
      "Ping Luo"
    ],
    "author_ids": [],
    "abstract": "Multi-person pose estimation is challenging because it localizes body\nkeypoints for multiple persons simultaneously. Previous methods can be divided\ninto two streams, i.e. top-down and bottom-up methods. The top-down methods\nlocalize keypoints after human detection, while the bottom-up methods localize\nkeypoints directly and then cluster/group them for different persons, which are\ngenerally more efficient than top-down methods. However, in existing bottom-up\nmethods, the keypoint grouping is usually solved independently from keypoint\ndetection, making them not end-to-end trainable and have sub-optimal\nperformance. In this paper, we investigate a new perspective of human part\ngrouping and reformulate it as a graph clustering task. Especially, we propose\na novel differentiable Hierarchical Graph Grouping (HGG) method to learn the\ngraph grouping in bottom-up multi-person pose estimation task. Moreover, HGG is\neasily embedded into main-stream bottom-up methods. It takes human keypoint\ncandidates as graph nodes and clusters keypoints in a multi-layer graph neural\nnetwork model. The modules of HGG can be trained end-to-end with the keypoint\ndetection network and is able to supervise the grouping process in a\nhierarchical manner. To improve the discrimination of the clustering, we add a\nset of edge discriminators and macro-node discriminators. Extensive experiments\non both COCO and OCHuman datasets demonstrate that the proposed method improves\nthe performance of bottom-up pose estimation methods.",
    "published_date": "2020-07-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11864v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.11506v1",
    "title": "Online Monitoring of Global Attitudes Towards Wildlife",
    "authors": [
      "Joss Wright",
      "Robert Lennox",
      "Diogo Veríssimo"
    ],
    "author_ids": [],
    "abstract": "Human factors are increasingly recognised as central to conservation of\nbiodiversity. Despite this, there are no existing systematic efforts to monitor\nglobal trends in perceptions of wildlife. With traditional news reporting now\nlargely online, the internet presents a powerful means to monitor global\nattitudes towards species. In this work we develop a method using the Global\nDatabase of Events, Language, and Tone (GDELT) to scan global news media,\nallowing us to identify and download conservation-related articles. Applying\nsupervised machine learning techniques, we filter irrelevant articles to create\na continually updated global dataset of news coverage for seven target taxa:\nlion, tiger, saiga, rhinoceros, pangolins, elephants, and orchids, and observe\nthat over two-thirds of articles matching a simple keyword search were\nirrelevant. We examine coverage of each taxa in different regions, and find\nthat elephants, rhinos, tigers, and lions receive the most coverage, with daily\npeaks of around 200 articles. Mean sentiment was positive for all taxa, except\nsaiga for which it was neutral. Coverage was broadly distributed, with articles\nfrom 73 countries across all continents. Elephants and tigers received coverage\nin the most countries overall, whilst orchids and saiga were mentioned in the\nsmallest number of countries. We further find that sentiment towards\ncharismatic megafauna is most positive in non-range countries, with the\nopposite being true for pangolins and orchids. Despite promising results, there\nremain substantial obstacles to achieving globally representative results.\nDisparities in internet access between low and high income countries and users\nis a major source of bias, with the need to focus on a diversity of data\nsources and languages, presenting sizable technical challenges...",
    "published_date": "2020-07-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11506v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.11281v2",
    "title": "Big Issues for Big Data: challenges for critical spatial data analytics",
    "authors": [
      "Chris Brunsdon",
      "Alexis Comber"
    ],
    "author_ids": [],
    "abstract": "In this paper we consider some of the issues of working with big data and big\nspatial data and highlight the need for an open and critical framework. We\nfocus on a set of challenges underlying the collection and analysis of big\ndata. In particular, we consider 1) the issues related to inference when\nworking with usually biased big data, challenging the assumed inferential\nsuperiority of data with observations, n, approaching N, the population (n->N),\nand the need for data science analysis that answer questions of practical\nsignificance or with greater emphasis n the size of the effect, rather than the\ntruth or falsehood of a statistical statement; 2) the need to accept messiness\nin your data and to document all operations undertaken on the data because of\nthis support of openness and reproducibility paradigms; and 3) the need to\nexplicitly seek to understand the causes of bias, messiness etc in the data and\nthe inferential consequences of using such data in analyses, by adopting\ncritical approaches to spatial data science. In particular we consider the need\nto place individual data science studies in a wider social and economic\ncontexts, along the the role of inferential theory in the presence of big data,\nand issues relating to messiness and complexity in big data.",
    "published_date": "2020-07-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11281v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.11206v2",
    "title": "SOCRATES: Towards a Unified Platform for Neural Network Analysis",
    "authors": [
      "Long H. Pham",
      "Jiaying Li",
      "Jun Sun"
    ],
    "author_ids": [],
    "abstract": "Studies show that neural networks, not unlike traditional programs, are\nsubject to bugs, e.g., adversarial samples that cause classification errors and\ndiscriminatory instances that demonstrate the lack of fairness. Given that\nneural networks are increasingly applied in critical applications (e.g.,\nself-driving cars, face recognition systems and personal credit rating\nsystems), it is desirable that systematic methods are developed to analyze\n(e.g., test or verify) neural networks against desirable properties. Recently,\na number of approaches have been developed for analyzing neural networks. These\nefforts are however scattered (i.e., each approach tackles some restricted\nclasses of neural networks against certain particular properties), incomparable\n(i.e., each approach has its own assumptions and input format) and thus hard to\napply, reuse or extend. In this project, we aim to build a unified framework\nfor developing techniques to analyze neural networks. Towards this goal, we\ndevelop a platform called SOCRATES which supports a standardized format for a\nvariety of neural network models, an assertion language for property\nspecification as well as multiple neural network analysis algorithms including\ntwo novel ones for falsifying and probabilistic verification of neural network\nmodels. SOCRATES is extensible and thus existing approaches can be easily\nintegrated. Experiment results show that our platform can handle a wide range\nof networks models and properties. More importantly, it provides a platform for\nsynergistic research on neural network analysis.",
    "published_date": "2020-07-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11206v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.11961v3",
    "title": "Dominant Resource Fairness with Meta-Types",
    "authors": [
      "Steven Yin",
      "Shatian Wang",
      "Lingyi Zhang",
      "Christian Kroer"
    ],
    "author_ids": [],
    "abstract": "Inspired by the recent COVID-19 pandemic, we study a generalization of the\nmulti-resource allocation problem with heterogeneous demands and Leontief\nutilities. Unlike existing settings, we allow each agent to specify\nrequirements to only accept allocations from a subset of the total supply for\neach resource. These requirements can take form in location constraints (e.g. A\nhospital can only accept volunteers who live nearby due to commute\nlimitations). This can also model a type of substitution effect where some\nagents need 1 unit of resource A \\emph{or} B, both belonging to the same\nmeta-type. But some agents specifically want A, and others specifically want B.\nWe propose a new mechanism called Dominant Resource Fairness with Meta Types\nwhich determines the allocations by solving a small number of linear programs.\nThe proposed method satisfies Pareto optimality, envy-freeness,\nstrategy-proofness, and a notion of sharing incentive for our setting. To the\nbest of our knowledge, we are the first to study this problem formulation,\nwhich improved upon existing work by capturing more constraints that often\narise in real life situations. Finally, we show numerically that our method\nscales better to large problems than alternative approaches.",
    "published_date": "2020-07-21T00:00:00",
    "year": 2020,
    "categories": [
      "econ.TH",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11961v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.10970v1",
    "title": "Recommendations for Planning Inclusive Astronomy Conferences",
    "authors": [
      "Inclusive Astronomy 2 Local Organizing Committee",
      ":",
      "Brian Brooks",
      "Keira Brooks",
      "Lea Hagen",
      "Nimish Hathi",
      "Samantha Hoffman",
      "James Paranilam",
      "Laura Prichard"
    ],
    "author_ids": [],
    "abstract": "The Inclusive Astronomy (IA) conference series aims to create a safe space\nwhere community members can listen to the experiences of marginalized\nindividuals in astronomy, discuss actions being taken to address inequities,\nand give recommendations to the community for how to improve diversity, equity,\nand inclusion in astronomy. The first IA was held in Nashville, TN, USA, 17-19\nJune, 2015. The Inclusive Astronomy 2 (IA2) conference was held in Baltimore,\nMD, USA, 14-15 October, 2019. The Inclusive Astronomy 2 (IA2) Local Organizing\nCommittee (LOC) has put together a comprehensive document of recommendations\nfor planning future Inclusive Astronomy conferences based on feedback received\nand lessons learned. While these are specific to the IA series, many parts will\nbe applicable to other conferences as well. Please find the recommendations and\naccompanying letter to the community here:\nhttps://outerspace.stsci.edu/display/IA2/LOC+Recommendations.",
    "published_date": "2020-07-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "astro-ph.IM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10970v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.10854v1",
    "title": "Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification",
    "authors": [
      "Jianing Li",
      "Shiliang Zhang"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptive person Re-IDentification (ReID) is challenging\nbecause of the large domain gap between source and target domains, as well as\nthe lackage of labeled data on the target domain. This paper tackles this\nchallenge through jointly enforcing visual and temporal consistency in the\ncombination of a local one-hot classification and a global multi-class\nclassification. The local one-hot classification assigns images in a training\nbatch with different person IDs, then adopts a Self-Adaptive Classification\n(SAC) model to classify them. The global multi-class classification is achieved\nby predicting labels on the entire unlabeled training set with the Memory-based\nTemporal-guided Cluster (MTC). MTC predicts multi-class labels by considering\nboth visual similarity and temporal consistency to ensure the quality of label\nprediction. The two classification models are combined in a unified framework,\nwhich effectively leverages the unlabeled data for discriminative feature\nlearning. Experimental results on three large-scale ReID datasets demonstrate\nthe superiority of proposed method in both unsupervised and unsupervised domain\nadaptive ReID tasks. For example, under unsupervised setting, our method\noutperforms recent unsupervised domain adaptive methods, which leverage more\nlabels for training.",
    "published_date": "2020-07-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10854v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10712v1",
    "title": "On Analyzing Antisocial Behaviors Amid COVID-19 Pandemic",
    "authors": [
      "Md Rabiul Awal",
      "Rui Cao",
      "Sandra Mitrovic",
      "Roy Ka-Wei Lee"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic has developed to be more than a bio-crisis as global\nnews has reported a sharp rise in xenophobia and discrimination in both online\nand offline communities. Such toxic behaviors take a heavy toll on society,\nespecially during these daunting times. Despite the gravity of the issue, very\nfew studies have studied online antisocial behaviors amid the COVID-19\npandemic. In this paper, we fill the research gap by collecting and annotating\na large dataset of over 40 million COVID-19 related tweets. Specially, we\npropose an annotation framework to annotate the antisocial behavior tweets\nautomatically. We also conduct an empirical analysis of our annotated dataset\nand found that new abusive lexicons are introduced amid the COVID-19 pandemic.\nOur study also identified the vulnerable targets of antisocial behaviors and\nthe factors that influence the spreading of online antisocial content.",
    "published_date": "2020-07-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10712v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10623v2",
    "title": "Learning Structured Latent Factors from Dependent Data:A Generative Model Framework from Information-Theoretic Perspective",
    "authors": [
      "Ruixiang Zhang",
      "Masanori Koyama",
      "Katsuhiko Ishiguro"
    ],
    "author_ids": [],
    "abstract": "Learning controllable and generalizable representation of multivariate data\nwith desired structural properties remains a fundamental problem in machine\nlearning. In this paper, we present a novel framework for learning generative\nmodels with various underlying structures in the latent space. We represent the\ninductive bias in the form of mask variables to model the dependency structure\nin the graphical model and extend the theory of multivariate information\nbottleneck to enforce it. Our model provides a principled approach to learn a\nset of semantically meaningful latent factors that reflect various types of\ndesired structures like capturing correlation or encoding invariance, while\nalso offering the flexibility to automatically estimate the dependency\nstructure from data. We show that our framework unifies many existing\ngenerative models and can be applied to a variety of tasks including\nmulti-modal data modeling, algorithmic fairness, and invariant risk\nminimization.",
    "published_date": "2020-07-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10623v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10609v2",
    "title": "SUBPLEX: Towards a Better Understanding of Black Box Model Explanations at the Subpopulation Level",
    "authors": [
      "Jun Yuan",
      "Gromit Yeuk-Yin Chan",
      "Brian Barr",
      "Kyle Overton",
      "Kim Rees",
      "Luis Gustavo Nonato",
      "Enrico Bertini",
      "Claudio T. Silva"
    ],
    "author_ids": [],
    "abstract": "Understanding the interpretation of machine learning (ML) models has been of\nparamount importance when making decisions with societal impacts such as\ntransport control, financial activities, and medical diagnosis. While current\nmodel interpretation methodologies focus on using locally linear functions to\napproximate the models or creating self-explanatory models that give\nexplanations to each input instance, they do not focus on model interpretation\nat the subpopulation level, which is the understanding of model interpretations\nacross different subset aggregations in a dataset. To address the challenges of\nproviding explanations of an ML model across the whole dataset, we propose\nSUBPLEX, a visual analytics system to help users understand black-box model\nexplanations with subpopulation visual analysis. SUBPLEX is designed through an\niterative design process with machine learning researchers to address three\nusage scenarios of real-life machine learning tasks: model debugging, feature\nselection, and bias detection. The system applies novel subpopulation analysis\non ML model explanations and interactive visualization to explore the\nexplanations on a dataset with different levels of granularity. Based on the\nsystem, we conduct user evaluation to assess how understanding the\ninterpretation at a subpopulation level influences the sense-making process of\ninterpreting ML models from a user's perspective. Our results suggest that by\nproviding model explanations for different groups of data, SUBPLEX encourages\nusers to generate more ingenious ideas to enrich the interpretations. It also\nhelps users to acquire a tight integration between programming workflow and\nvisual analytics workflow. Last but not least, we summarize the considerations\nobserved in applying visualization to machine learning interpretations.",
    "published_date": "2020-07-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10609v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10390v2",
    "title": "Testing linear inequalities of subgraph statistics",
    "authors": [
      "Lior Gishboliner",
      "Asaf Shapira",
      "Henrique Stagni"
    ],
    "author_ids": [],
    "abstract": "Property testers are fast randomized algorithms whose task is to distinguish\nbetween inputs satisfying some predetermined property ${\\cal P}$ and those that\nare far from satisfying it. Since these algorithms operate by inspecting a\nsmall randomly selected portion of the input, the most natural property one\nwould like to be able to test is whether the input does not contain certain\nforbidden small substructures. In the setting of graphs, such a result was\nobtained by Alon et al., who proved that for any finite family of graphs ${\\cal\nF}$, the property of being induced ${\\cal F}$-free (i.e. not containing an\ninduced copy of any $F \\in {\\cal F}$) is testable.\n  It is natural to ask if one can go one step further and prove that more\nelaborate properties involving induced subgraphs are also testable. One such\ngeneralization of the result of Alon et al. was formulated by Goldreich and\nShinkar who conjectured that for any finite family of graphs ${\\cal F}$, and\nany linear inequality involving the densities of the graphs $F \\in {\\cal F}$ in\nthe input graph, the property of satisfying this inequality can be tested in a\ncertain restricted model of graph property testing. Our main result in this\npaper disproves this conjecture in the following strong form: some properties\nof this type are not testable even in the classical (i.e. unrestricted) model\nof graph property testing.\n  The proof deviates significantly from prior non-testability results in this\narea. The main idea is to use a linear inequality relating induced subgraph\ndensities in order to encode the property of being a quasirandom graph.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "math.CO",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10390v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.10306v3",
    "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction",
    "authors": [
      "Stephen R. Pfohl",
      "Agata Foryciarz",
      "Nigam H. Shah"
    ],
    "author_ids": [],
    "abstract": "The use of machine learning to guide clinical decision making has the\npotential to worsen existing health disparities. Several recent works frame the\nproblem as that of algorithmic fairness, a framework that has attracted\nconsiderable attention and criticism. However, the appropriateness of this\nframework is unclear due to both ethical as well as technical considerations,\nthe latter of which include trade-offs between measures of fairness and model\nperformance that are not well-understood for predictive models of clinical\noutcomes. To inform the ongoing debate, we conduct an empirical study to\ncharacterize the impact of penalizing group fairness violations on an array of\nmeasures of model performance and group fairness. We repeat the analyses across\nmultiple observational healthcare databases, clinical outcomes, and sensitive\nattributes. We find that procedures that penalize differences between the\ndistributions of predictions across groups induce nearly-universal degradation\nof multiple performance metrics within groups. On examining the secondary\nimpact of these procedures, we observe heterogeneity of the effect of these\nprocedures on measures of fairness in calibration and ranking across\nexperimental conditions. Beyond the reported trade-offs, we emphasize that\nanalyses of algorithmic fairness in healthcare lack the contextual grounding\nand causal awareness necessary to reason about the mechanisms that lead to\nhealth disparities, as well as about the potential of algorithmic fairness\nmethods to counteract those mechanisms. In light of these limitations, we\nencourage researchers building predictive models for clinical use to step\noutside the algorithmic fairness frame and engage critically with the broader\nsociotechnical context surrounding the use of machine learning in healthcare.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10306v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10235v1",
    "title": "Privacy Implications of Eye Tracking in Mixed Reality",
    "authors": [
      "Diane Hosfelt",
      "Nicole Shadowen"
    ],
    "author_ids": [],
    "abstract": "Mixed Reality (MR) devices require a world with always-on sensors and\nreal-time processing applied to their outputs. We have grappled with some of\nthe ethical concerns presented by this scenario, such as bystander privacy\nissues with smartphones and cameras. However, MR technologies demand that we\ndefine and defend privacy in this new paradigm. This paper focuses on the\nchallenges presented by eye tracking and gaze tracking, techniques that have\ncommonly been deployed in the HCI community for years but are now being\nintegrated into MR devices by default.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10235v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.10083v3",
    "title": "The Geometry of Information Cocoon: Analyzing the Cultural Space with Word Embedding Models",
    "authors": [
      "Huimin Xu",
      "Zhicong Chen",
      "Ruiqi Li",
      "Cheng-Jun Wang"
    ],
    "author_ids": [],
    "abstract": "Accompanied by the development of digital media, the threat of information\ncocoon has become a significant issue. However, little is known about the\nmeasure of information cocoon as a cultural space and its relationship with\nsocial class. This study addresses this problem by constructing the cultural\nspace with word embedding models and random shuffling methods among three\nlarge-scale digital media use datasets. In the light of field theory of\ncultural production, we investigate the information cocoon effect on different\nsocial classes among 979 computer users, 100,000 smartphone users, and 159,373\nmobile reading application users. Our analysis reveals that information cocoons\nwidely exist in the daily use of digital media. Moreover, people of lower\nsocial class have a higher probability of getting stuck in the information\ncocoon filled with the entertainment content. In contrast, the people of higher\nsocial class have more capability to stride over the constraints of the\ninformation cocoon. The results suggest that the disadvantages for vulnerable\ngroups in acquiring knowledge may further widen social inequality.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10083v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10075v3",
    "title": "Investigating Bias and Fairness in Facial Expression Recognition",
    "authors": [
      "Tian Xu",
      "Jennifer White",
      "Sinan Kalkan",
      "Hatice Gunes"
    ],
    "author_ids": [],
    "abstract": "Recognition of expressions of emotions and affect from facial images is a\nwell-studied research problem in the fields of affective computing and computer\nvision with a large number of datasets available containing facial images and\ncorresponding expression labels. However, virtually none of these datasets have\nbeen acquired with consideration of fair distribution across the human\npopulation. Therefore, in this work, we undertake a systematic investigation of\nbias and fairness in facial expression recognition by comparing three different\napproaches, namely a baseline, an attribute-aware and a disentangled approach,\non two well-known datasets, RAF-DB and CelebA. Our results indicate that: (i)\ndata augmentation improves the accuracy of the baseline model, but this alone\nis unable to mitigate the bias effect; (ii) both the attribute-aware and the\ndisentangled approaches fortified with data augmentation perform better than\nthe baseline approach in terms of accuracy and fairness; (iii) the disentangled\napproach is the best for mitigating demographic bias; and (iv) the bias\nmitigation strategies are more suitable in the existence of uneven attribute\ndistribution or imbalanced number of subgroup data.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10075v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10033v1",
    "title": "Universal Loss Reweighting to Balance Lesion Size Inequality in 3D Medical Image Segmentation",
    "authors": [
      "Boris Shirokikh",
      "Alexey Shevtsov",
      "Anvar Kurmukov",
      "Alexandra Dalechina",
      "Egor Krivov",
      "Valery Kostjuchenko",
      "Andrey Golanov",
      "Mikhail Belyaev"
    ],
    "author_ids": [],
    "abstract": "Target imbalance affects the performance of recent deep learning methods in\nmany medical image segmentation tasks. It is a twofold problem: class imbalance\n- positive class (lesion) size compared to negative class (non-lesion) size;\nlesion size imbalance - large lesions overshadows small ones (in the case of\nmultiple lesions per image). While the former was addressed in multiple works,\nthe latter lacks investigation. We propose a loss reweighting approach to\nincrease the ability of the network to detect small lesions. During the\nlearning process, we assign a weight to every image voxel. The assigned weights\nare inversely proportional to the lesion volume, thus smaller lesions get\nlarger weights. We report the benefit from our method for well-known loss\nfunctions, including Dice Loss, Focal Loss, and Asymmetric Similarity Loss.\nAdditionally, we compare our results with other reweighting techniques:\nWeighted Cross-Entropy and Generalized Dice Loss. Our experiments show that\ninverse weighting considerably increases the detection quality, while preserves\nthe delineation quality on a state-of-the-art level. We publish a complete\nexperimental pipeline for two publicly available datasets of CT images: LiTS\nand LUNA16 (https://github.com/neuro-ml/inverse_weighting). We also show\nresults on a private database of MR images for the task of multiple brain\nmetastases delineation.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10033v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09979v2",
    "title": "Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image Classifications",
    "authors": [
      "Lijun Gong",
      "Kai Ma",
      "Yefeng Zheng"
    ],
    "author_ids": [],
    "abstract": "Medical image analysis benefits Computer Aided Diagnosis (CADx). A\nfundamental analyzing approach is the classification of medical images, which\nserves for skin lesion diagnosis, diabetic retinopathy grading, and cancer\nclassification on histological images. When learning these discriminative\nclassifiers, we observe that the convolutional neural networks (CNNs) are\nvulnerable to distractor interference. This is due to the similar sample\nappearances from different categories (i.e., small inter-class distance).\nExisting attempts select distractors from input images by empirically\nestimating their potential effects to the classifier. The essences of how these\ndistractors affect CNN classification are not known. In this paper, we explore\ndistractors from the CNN feature space via proposing a neuron intrinsic\nlearning method. We formulate a novel distractor-aware loss that encourages\nlarge distance between the original image and its distractor in the feature\nspace. The novel loss is combined with the original classification loss to\nupdate network parameters by back-propagation. Neuron intrinsic learning first\nexplores distractors crucial to the deep classifier and then uses them to\nrobustify CNN inherently. Extensive experiments on medical image benchmark\ndatasets indicate that the proposed method performs favorably against the\nstate-of-the-art approaches.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09979v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09941v1",
    "title": "Adaptive force biasing algorithms: new convergence results and tensor approximations of the bias",
    "authors": [
      "Virginie Ehrlacher",
      "Tony Lelièvre",
      "Pierre Monmarché"
    ],
    "author_ids": [],
    "abstract": "A modification of the Adaptive Biasing Force method is introduced, in which\nthe free energy is approximated by a sum of tensor products of one-dimensional\nfunctions. This enables to handle a larger number of reaction coordinates than\nthe classical algorithm. We prove the algorithm is well-defined and prove the\nlong-time convergence toward a regularized version of the free energy for an\nidealized version of the algorithm. Numerical experiments demonstrate that the\nmethod is able to capture correlations between reaction coordinates.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "math.PR",
      "cs.NA",
      "math.NA",
      "65C05, 65N12"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09941v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2008.04068v1",
    "title": "Crowd, Lending, Machine, and Bias",
    "authors": [
      "Runshan Fu",
      "Yan Huang",
      "Param Vir Singh"
    ],
    "author_ids": [],
    "abstract": "Big data and machine learning (ML) algorithms are key drivers of many fintech\ninnovations. While it may be obvious that replacing humans with machine would\nincrease efficiency, it is not clear whether and where machines can make better\ndecisions than humans. We answer this question in the context of crowd lending,\nwhere decisions are traditionally made by a crowd of investors. Using data from\nProsper.com, we show that a reasonably sophisticated ML algorithm predicts\nlisting default probability more accurately than crowd investors. The dominance\nof the machine over the crowd is more pronounced for highly risky listings. We\nthen use the machine to make investment decisions, and find that the machine\nbenefits not only the lenders but also the borrowers. When machine prediction\nis used to select loans, it leads to a higher rate of return for investors and\nmore funding opportunities for borrowers with few alternative funding options.\nWe also find suggestive evidence that the machine is biased in gender and race\neven when it does not use gender and race information as input. We propose a\ngeneral and effective \"debasing\" method that can be applied to any prediction\nfocused ML applications, and demonstrate its use in our context. We show that\nthe debiased ML algorithm, which suffers from lower prediction accuracy, still\nleads to better investment decisions compared with the crowd. These results\nindicate that ML can help crowd lending platforms better fulfill the promise of\nproviding access to financial resources to otherwise underserved individuals\nand ensure fairness in the allocation of these resources.",
    "published_date": "2020-07-20T00:00:00",
    "year": 2020,
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.04068v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07321v1",
    "title": "Expected Utilitarianism",
    "authors": [
      "Heather M. Roff"
    ],
    "author_ids": [],
    "abstract": "We want artificial intelligence (AI) to be beneficial. This is the grounding\nassumption of most of the attitudes towards AI research. We want AI to be\n\"good\" for humanity. We want it to help, not hinder, humans. Yet what exactly\nthis entails in theory and in practice is not immediately apparent.\nTheoretically, this declarative statement subtly implies a commitment to a\nconsequentialist ethics. Practically, some of the more promising machine\nlearning techniques to create a robust AI, and perhaps even an artificial\ngeneral intelligence (AGI) also commit one to a form of utilitarianism. In both\ndimensions, the logic of the beneficial AI movement may not in fact create\n\"beneficial AI\" in either narrow applications or in the form of AGI if the\nethical assumptions are not made explicit and clear.\n  Additionally, as it is likely that reinforcement learning (RL) will be an\nimportant technique for machine learning in this area, it is also important to\ninterrogate how RL smuggles in a particular type of consequentialist reasoning\ninto the AI: particularly, a brute form of hedonistic act utilitarianism. Since\nthe mathematical logic commits one to a maximization function, the result is\nthat an AI will inevitably be seeking more and more rewards. We have two\nconclusions that arise from this. First, is that if one believes that a\nbeneficial AI is an ethical AI, then one is committed to a framework that\nposits 'benefit' is tantamount to the greatest good for the greatest number.\nSecond, if the AI relies on RL, then the way it reasons about itself, the\nenvironment, and other agents, will be through an act utilitarian morality.\nThis proposition may, or may not, in fact be actually beneficial for humanity.",
    "published_date": "2020-07-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07321v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09685v1",
    "title": "Welcome to Gab Alt Right Discourses",
    "authors": [
      "Nga Than",
      "Maria Y. Rodriguez",
      "Diane Yoong",
      "Friederike Windel"
    ],
    "author_ids": [],
    "abstract": "Social media has become an important venue for diverse groups to share\ninformation, discuss political issues, and organize social movements. Recent\nscholarship has shown that the social media ecosystem can affect political\nthinking and expression. Individuals and groups across the political spectrum\nhave engaged in the use of these platforms extensively, even creating their own\nforums with varying approaches to content moderation in pursuit of freer\nstandards of speech. The Gab social media platform arose in this context. Gab\nis a social media platform for the so-called alt right, and much of the popular\npress has opined about the thematic content of discourses on Gab and platforms\nlike it, but little research has examined the content itself. Using a publicly\navailable dataset of all Gab posts from August 2016 until July 2019, the\ncurrent paper explores a five percent random sample of this dataset to explore\nthematic content on the platform. We run multiple structural topic models,\nusing standard procedures to arrive at an optimal k number of topics. The final\nmodel specifies 85 topics for 403,469 documents. We include as prevalence\nvariables whether the source account has been flagged as a bot and the number\nof followers for the source account. Results suggest the most nodal topics in\nthe dataset pertain to the authenticity of the Holocaust, the meaning of red\npill, and the journalistic merit of mainstream media. We conclude by discussing\nthe implications of our findings for work in ethical content moderation, online\ncommunity development, political polarization, and avenues for future research.",
    "published_date": "2020-07-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09685v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.09561v3",
    "title": "Leader-Driven Opinion Dynamics in Signed Social Networks With Asynchronous Trust/Distrust Level Evolution",
    "authors": [
      "Lei Shi",
      "Yuhua Cheng",
      "Jinliang Shao",
      "Xiaofan Wang",
      "Hanmin Sheng"
    ],
    "author_ids": [],
    "abstract": "Trust and distrust are common in the opinion interactions among agents in\nsocial networks, and they are described by the edges with positive and negative\nweights in the signed digraph, respectively. It has been shown in social\npsychology that although the opinions of most agents (followers) tend to\nprevail, sometimes one agent (leader) with a firm stand and strong influence\ncan impact or even overthrow the preferences of followers. This paper aims to\nanalyze how the leader influences the formation of followers' opinions in\nsigned social networks. In addition, this paper considers an asynchronous\nevolution mechanism of trust/distrust level based on opinion difference, in\nwhich the trust/distrust level between neighboring agents is portrayed as a\nnonlinear weight function of their opinion difference, and each agent interacts\nwith the neighbors to update the trust/distrust level and opinion at the times\ndetermined by its own will. Based on the related properties of sub-stochastic\nand super-stochastic matrices, the inequality conditions about positive and\nnegative weights to achieve opinion consensus and polarization are established.\nSome numerical simulations based on two well-known networks called the ``12\nAngry Men\" network and the Karate Club network are provided to verify the\ncorrectness of the theoretical results.",
    "published_date": "2020-07-19T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09561v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.09541v2",
    "title": "Same-Day Delivery with Fairness",
    "authors": [
      "Xinwei Chen",
      "Tong Wang",
      "Barrett W. Thomas",
      "Marlin W. Ulmer"
    ],
    "author_ids": [],
    "abstract": "The demand for same-day delivery (SDD) has increased rapidly in the last few\nyears and has particularly boomed during the COVID-19 pandemic. The fast growth\nis not without its challenge. In 2016, due to low concentrations of memberships\nand far distance from the depot, certain minority neighborhoods were excluded\nfrom receiving Amazon's SDD service, raising concerns about fairness. In this\npaper, we study the problem of offering fair SDD-service to customers. The\nservice area is partitioned into different regions. Over the course of a day,\ncustomers request for SDD service, and the timing of requests and delivery\nlocations are not known in advance. The dispatcher dynamically assigns vehicles\nto make deliveries to accepted customers before their delivery deadline. In\naddition to the overall service rate (utility), we maximize the minimal\nregional service rate across all regions (fairness). We model the problem as a\nmulti-objective Markov decision process and develop a deep Q-learning solution\napproach. We introduce a novel transformation of learning from rates to actual\nservices, which creates a stable and efficient learning process. Computational\nresults demonstrate the effectiveness of our approach in alleviating unfairness\nboth spatially and temporally in different customer geographies. We also show\nthis effectiveness is valid with different depot locations, providing\nbusinesses with an opportunity to achieve better fairness from any location.\nFurther, we consider the impact of ignoring fairness in service, and results\nshow that our policies eventually outperform the utility-driven baseline when\ncustomers have a high expectation on service level.",
    "published_date": "2020-07-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09541v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09530v1",
    "title": "A Distributionally Robust Approach to Fair Classification",
    "authors": [
      "Bahar Taskesen",
      "Viet Anh Nguyen",
      "Daniel Kuhn",
      "Jose Blanchet"
    ],
    "author_ids": [],
    "abstract": "We propose a distributionally robust logistic regression model with an\nunfairness penalty that prevents discrimination with respect to sensitive\nattributes such as gender or ethnicity. This model is equivalent to a tractable\nconvex optimization problem if a Wasserstein ball centered at the empirical\ndistribution on the training data is used to model distributional uncertainty\nand if a new convex unfairness measure is used to incentivize equalized\nopportunities. We demonstrate that the resulting classifier improves fairness\nat a marginal loss of predictive accuracy on both synthetic and real datasets.\nWe also derive linear programming-based confidence bounds on the level of\nunfairness of any pre-trained classifier by leveraging techniques from optimal\nuncertainty quantification over Wasserstein balls.",
    "published_date": "2020-07-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09530v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09416v2",
    "title": "Ethical issues with using Internet of Things devices in citizen science research: A scoping review",
    "authors": [
      "James Scheibner",
      "Anna Jobin",
      "Effy Vayena"
    ],
    "author_ids": [],
    "abstract": "Our chapter presents a scoping review of published scientific studies or case\nstudies of scientific studies that utilise both citizen scientists and Internet\nof Things devices. Specifically, we selected studies where the authors had\nincluded at least a short discussion of the ethical issues encountered during\nthe research process. Having conducted a search of five databases (IEEE Xplore,\nScopus, Web of Science, ProQuest, and PubMed), we identified 631 potential\nresults. Following abstract and title screening, and then full text eligibility\nassessment, we identified 34 published articles that matched our criteria. We\nthen analysed the full text for these articles inductively and deductively,\ncoding ethical issues into three main categories. These categories were\nautonomy and data privacy, data quality, and intellectual property. We also\nanalysed the full text of these articles to see what strategies researchers\ntook to resolve these ethical issues, as well as any legal implications raised.\nFollowing this analysis, our discussion provides recommendations for\nresearchers who wish to integrate citizen scientists and Internet of Things\ndevices into their research. First, all citizen science projects should\nintegrate a data privacy protocol to protect the confidentiality of\nparticipants. Secondly, scientific researchers should consider any potential\nissues of data quality, including whether compromises might be required, before\nestablishing a project. Finally, all intellectual property issues should be\nclarified both at the start of the project and during its lifecycle.\nResearchers should also consider any ethical issues that might flow from the\nuse of commercially available Internet of Things devices for research.",
    "published_date": "2020-07-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09416v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.09370v1",
    "title": "How to Democratise and Protect AI: Fair and Differentially Private Decentralised Deep Learning",
    "authors": [
      "Lingjuan Lyu",
      "Yitong Li",
      "Karthik Nandakumar",
      "Jiangshan Yu",
      "Xingjun Ma"
    ],
    "author_ids": [],
    "abstract": "This paper firstly considers the research problem of fairness in\ncollaborative deep learning, while ensuring privacy. A novel reputation system\nis proposed through digital tokens and local credibility to ensure fairness, in\ncombination with differential privacy to guarantee privacy. In particular, we\nbuild a fair and differentially private decentralised deep learning framework\ncalled FDPDDL, which enables parties to derive more accurate local models in a\nfair and private manner by using our developed two-stage scheme: during the\ninitialisation stage, artificial samples generated by Differentially Private\nGenerative Adversarial Network (DPGAN) are used to mutually benchmark the local\ncredibility of each party and generate initial tokens; during the update stage,\nDifferentially Private SGD (DPSGD) is used to facilitate collaborative\nprivacy-preserving deep learning, and local credibility and tokens of each\nparty are updated according to the quality and quantity of individually\nreleased gradients. Experimental results on benchmark datasets under three\nrealistic settings demonstrate that FDPDDL achieves high fairness, yields\ncomparable accuracy to the centralised and distributed frameworks, and delivers\nbetter accuracy than the standalone framework.",
    "published_date": "2020-07-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09370v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09345v3",
    "title": "Combinatorial and computational investigations of Neighbor-Joining bias",
    "authors": [
      "Ruth Davidson",
      "Abraham Martin del Campo"
    ],
    "author_ids": [],
    "abstract": "The Neighbor-Joining algorithm is a popular distance-based phylogenetic\nmethod that computes a tree metric from a dissimilarity map arising from\nbiological data. Realizing dissimilarity maps as points in Euclidean space, the\nalgorithm partitions the input space into polyhedral regions indexed by the\ncombinatorial type of the trees returned. A full combinatorial description of\nthese regions has not been found yet; different sequences of Neighbor-Joining\nagglomeration events can produce the same combinatorial tree, therefore\nassociating multiple geometric regions to the same algorithmic output. We\nresolve this confusion by defining agglomeration orders on trees, leading to a\nbijection between distinct regions of the output space and weighted Motzkin\npaths. As a result, we give a formula for the number of polyhedral regions\ndepending only on the number of taxa. We conclude with a computational\ncomparison between these polyhedral regions, to unveil biases introduced in any\nimplementation of the algorithm.",
    "published_date": "2020-07-18T00:00:00",
    "year": 2020,
    "categories": [
      "math.CO",
      "cs.CG",
      "cs.NE",
      "92D15, 52B05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09345v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09307v2",
    "title": "Unsupervised Shape Normality Metric for Severity Quantification",
    "authors": [
      "Wenzheng Tao",
      "Riddhish Bhalodia",
      "Erin Anstadt",
      "Ladislav Kavan",
      "Ross T. Whitaker",
      "Jesse A. Goldstein"
    ],
    "author_ids": [],
    "abstract": "This work describes an unsupervised method to objectively quantify the\nabnormality of general anatomical shapes. The severity of an anatomical\ndeformity often serves as a determinant in the clinical management of patients.\nHowever, experiential bias and distinctive random residuals among specialist\nindividuals bring variability in diagnosis and patient management decisions,\nirrespective of the objective deformity degree. Therefore, supervised methods\nare prone to be misled given insufficient labeling of pathological samples that\ninevitably preserve human bias and inconsistency. Furthermore, subjects\ndemonstrating a specific pathology are naturally rare relative to the normal\npopulation. To avoid relying on sufficient pathological samples by fully\nutilizing the power of normal samples, we propose the shape normality metric\n(SNM), which requires learning only from normal samples and zero knowledge\nabout the pathology. We represent shapes by landmarks automatically inferred\nfrom the data and model the normal group by a multivariate Gaussian\ndistribution. Extensive experiments on different anatomical datasets, including\nskulls, femurs, scapulae, and humeri, demonstrate that SNM can provide an\neffective normality measurement, which can significantly detect and indicate\npathology. Therefore, SNM offers promising value in a variety of clinical\napplications.",
    "published_date": "2020-07-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09307v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09222v1",
    "title": "Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation",
    "authors": [
      "Haoran Wang",
      "Tong Shen",
      "Wei Zhang",
      "Lingyu Duan",
      "Tao Mei"
    ],
    "author_ids": [],
    "abstract": "Despite great progress in supervised semantic segmentation,a large\nperformance drop is usually observed when deploying the model in the wild.\nDomain adaptation methods tackle the issue by aligning the source domain and\nthe target domain. However, most existing methods attempt to perform the\nalignment from a holistic view, ignoring the underlying class-level data\nstructure in the target domain. To fully exploit the supervision in the source\ndomain, we propose a fine-grained adversarial learning strategy for class-level\nfeature alignment while preserving the internal structure of semantics across\ndomains. We adopt a fine-grained domain discriminator that not only plays as a\ndomain distinguisher, but also differentiates domains at class level. The\ntraditional binary domain labels are also generalized to domain encodings as\nthe supervision signal to guide the fine-grained feature alignment. An analysis\nwith Class Center Distance (CCD) validates that our fine-grained adversarial\nstrategy achieves better class-level alignment compared to other\nstate-of-the-art methods. Our method is easy to implement and its effectiveness\nis evaluated on three classical domain adaptation tasks, i.e., GTA5 to\nCityscapes, SYNTHIA to Cityscapes and Cityscapes to Cross-City. Large\nperformance gains show that our method outperforms other global feature\nalignment based and class-wise alignment based counterparts. The code is\npublicly available at https://github.com/JDAI-CV/FADA.",
    "published_date": "2020-07-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09222v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.09172v1",
    "title": "Improved Approximations for Min Sum Vertex Cover and Generalized Min Sum Set Cover",
    "authors": [
      "Nikhil Bansal",
      "Jatin Batra",
      "Majid Farhadi",
      "Prasad Tetali"
    ],
    "author_ids": [],
    "abstract": "We study the generalized min sum set cover (GMSSC) problem, wherein given a\ncollection of hyperedges $E$ with arbitrary covering requirements $k_e$, the\ngoal is to find an ordering of the vertices to minimize the total cover time of\nthe hyperedges; a hyperedge $e$ is considered covered by the first time when\n$k_e$ many of its vertices appear in the ordering. We give a $4.642$\napproximation algorithm for GMSSC, coming close to the best possible bound of\n$4$, already for the classical special case (with all $k_e=1$) of min sum set\ncover (MSSC) studied by Feige, Lov\\'{a}sz and Tetali, and improving upon the\nprevious best known bound of $12.4$ due to Im, Sviridenko and van der Zwaan.\nOur algorithm is based on transforming the LP solution by a suitable kernel and\napplying randomized rounding. This also gives an LP-based $4$ approximation for\nMSSC. As part of the analysis of our algorithm, we also derive an inequality on\nthe lower tail of a sum of independent Bernoulli random variables, which might\nbe of independent interest and broader utility.\n  Another well-known special case is the min sum vertex cover (MSVC) problem,\nin which the input hypergraph is a graph and $k_e = 1$, for every edge. We give\na $16/9$ approximation for MSVC, and show a matching integrality gap for the\nnatural LP relaxation. This improves upon the previous best $1.999946$\napproximation of Barenholz, Feige and Peleg. (The claimed $1.79$ approximation\nresult of Iwata, Tetali and Tripathi for the MSVC turned out have an\nunfortunate, seemingly unfixable, mistake in it.) Finally, we revisit MSSC and\nconsider the $\\ell_p$ norm of cover-time of the hyperedges. Using a dual\nfitting argument, we show that the natural greedy algorithm achieves tight, up\nto NP-hardness, approximation guarantees of $(p+1)^{1+1/p}$, for all $p\\ge 1$.\nFor $p=1$, this gives yet another proof of the $4$ approximation for MSSC.",
    "published_date": "2020-07-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.09172v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.08911v3",
    "title": "Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context",
    "authors": [
      "Ehsan Toreini",
      "Mhairi Aitken",
      "Kovila P. L. Coopamootoo",
      "Karen Elliott",
      "Vladimiro Gonzalez Zelaya",
      "Paolo Missier",
      "Magdalene Ng",
      "Aad van Moorsel"
    ],
    "author_ids": [],
    "abstract": "Concerns about the societal impact of AI-based services and systems has\nencouraged governments and other organisations around the world to propose AI\npolicy frameworks to address fairness, accountability, transparency and related\ntopics. To achieve the objectives of these frameworks, the data and software\nengineers who build machine-learning systems require knowledge about a variety\nof relevant supporting tools and techniques. In this paper we provide an\noverview of technologies that support building trustworthy machine learning\nsystems, i.e., systems whose properties justify that people place trust in\nthem. We argue that four categories of system properties are instrumental in\nachieving the policy objectives, namely fairness, explainability, auditability\nand safety & security (FEAS). We discuss how these properties need to be\nconsidered across all stages of the machine learning life cycle, from data\ncollection through run-time model inference. As a consequence, we survey in\nthis paper the main technologies with respect to all four of the FEAS\nproperties, for data-centric as well as model-centric stages of the machine\nlearning system life cycle. We conclude with an identification of open research\nproblems, with a particular focus on the connection between trustworthy machine\nlearning technologies and their implications for individuals and society.",
    "published_date": "2020-07-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08911v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08840v3",
    "title": "Adaptive Gradient Methods for Constrained Convex Optimization and Variational Inequalities",
    "authors": [
      "Alina Ene",
      "Huy L. Nguyen",
      "Adrian Vladu"
    ],
    "author_ids": [],
    "abstract": "We provide new adaptive first-order methods for constrained convex\noptimization. Our main algorithms AdaACSA and AdaAGD+ are accelerated methods,\nwhich are universal in the sense that they achieve nearly-optimal convergence\nrates for both smooth and non-smooth functions, even when they only have access\nto stochastic gradients. In addition, they do not require any prior knowledge\non how the objective function is parametrized, since they automatically adjust\ntheir per-coordinate learning rate. These can be seen as truly accelerated\nAdagrad methods for constrained optimization.\n  We complement them with a simpler algorithm AdaGrad+ which enjoys the same\nfeatures, and achieves the standard non-accelerated convergence rate. We also\npresent a set of new results involving adaptive methods for unconstrained\noptimization and monotone operators.",
    "published_date": "2020-07-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08840v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08740v1",
    "title": "Leveraging both Lesion Features and Procedural Bias in Neuroimaging: An Dual-Task Split dynamics of inverse scale space",
    "authors": [
      "Xinwei Sun",
      "Wenjing Han",
      "Lingjing Hu",
      "Yuan Yao",
      "Yizhou Wang"
    ],
    "author_ids": [],
    "abstract": "The prediction and selection of lesion features are two important tasks in\nvoxel-based neuroimage analysis. Existing multivariate learning models take two\ntasks equivalently and optimize simultaneously. However, in addition to lesion\nfeatures, we observe that there is another type of feature, which is commonly\nintroduced during the procedure of preprocessing steps, which can improve the\nprediction result. We call such a type of feature as procedural bias.\nTherefore, in this paper, we propose that the features/voxels in neuroimage\ndata are consist of three orthogonal parts: lesion features, procedural bias,\nand null features. To stably select lesion features and leverage procedural\nbias into prediction, we propose an iterative algorithm (termed GSplit LBI) as\na discretization of differential inclusion of inverse scale space, which is the\ncombination of Variable Splitting scheme and Linearized Bregman Iteration\n(LBI). Specifically, with a variable the splitting term, two estimators are\nintroduced and split apart, i.e. one is for feature selection (the sparse\nestimator) and the other is for prediction (the dense estimator). Implemented\nwith Linearized Bregman Iteration (LBI), the solution path of both estimators\ncan be returned with different sparsity levels on the sparse estimator for the\nselection of lesion features. Besides, the dense the estimator can additionally\nleverage procedural bias to further improve prediction results. To test the\nefficacy of our method, we conduct experiments on the simulated study and\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. The validity and\nthe benefit of our model can be shown by the improvement of prediction results\nand the interpretability of visualized procedural bias and lesion features.",
    "published_date": "2020-07-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08740v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10791v3",
    "title": "Learning to Match Distributions for Domain Adaptation",
    "authors": [
      "Chaohui Yu",
      "Jindong Wang",
      "Chang Liu",
      "Tao Qin",
      "Renjun Xu",
      "Wenjie Feng",
      "Yiqiang Chen",
      "Tie-Yan Liu"
    ],
    "author_ids": [],
    "abstract": "When the training and test data are from different distributions, domain\nadaptation is needed to reduce dataset bias to improve the model's\ngeneralization ability. Since it is difficult to directly match the\ncross-domain joint distributions, existing methods tend to reduce the marginal\nor conditional distribution divergence using predefined distances such as MMD\nand adversarial-based discrepancies. However, it remains challenging to\ndetermine which method is suitable for a given application since they are built\nwith certain priors or bias. Thus they may fail to uncover the underlying\nrelationship between transferable features and joint distributions. This paper\nproposes Learning to Match (L2M) to automatically learn the cross-domain\ndistribution matching without relying on hand-crafted priors on the matching\nloss. Instead, L2M reduces the inductive bias by using a meta-network to learn\nthe distribution matching loss in a data-driven way. L2M is a general framework\nthat unifies task-independent and human-designed matching features. We design a\nnovel optimization algorithm for this challenging objective with\nself-supervised label propagation. Experiments on public datasets substantiate\nthe superiority of L2M over SOTA methods. Moreover, we apply L2M to transfer\nfrom pneumonia to COVID-19 chest X-ray images with remarkable performance. L2M\ncan also be extended in other distribution matching applications where we show\nin a trial experiment that L2M generates more realistic and sharper MNIST\nsamples.",
    "published_date": "2020-07-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10791v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08666v1",
    "title": "Conservative AI and social inequality: Conceptualizing alternatives to bias through social theory",
    "authors": [
      "Mike Zajko"
    ],
    "author_ids": [],
    "abstract": "In response to calls for greater interdisciplinary involvement from the\nsocial sciences and humanities in the development, governance, and study of\nartificial intelligence systems, this paper presents one sociologist's view on\nthe problem of algorithmic bias and the reproduction of societal bias.\nDiscussions of bias in AI cover much of the same conceptual terrain that\nsociologists studying inequality have long understood using more specific terms\nand theories. Concerns over reproducing societal bias should be informed by an\nunderstanding of the ways that inequality is continually reproduced in society\n-- processes that AI systems are either complicit in, or can be designed to\ndisrupt and counter. The contrast presented here is between conservative and\nradical approaches to AI, with conservatism referring to dominant tendencies\nthat reproduce and strengthen the status quo, while radical approaches work to\ndisrupt systemic forms of inequality. The limitations of conservative\napproaches to class, gender, and racial bias are discussed as specific\nexamples, along with the social structures and processes that biases in these\nareas are linked to. Societal issues can no longer be out of scope for AI and\nmachine learning, given the impact of these systems on human lives. This\nrequires engagement with a growing body of critical AI scholarship that goes\nbeyond biased data to analyze structured ways of perpetuating inequality,\nopening up the possibility for radical alternatives.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08666v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.10112v1",
    "title": "Large scale analysis of generalization error in learning using margin based classification methods",
    "authors": [
      "Hanwen Huang",
      "Qinglong Yang"
    ],
    "author_ids": [],
    "abstract": "Large-margin classifiers are popular methods for classification. We derive\nthe asymptotic expression for the generalization error of a family of\nlarge-margin classifiers in the limit of both sample size $n$ and dimension $p$\ngoing to $\\infty$ with fixed ratio $\\alpha=n/p$. This family covers a broad\nrange of commonly used classifiers including support vector machine, distance\nweighted discrimination, and penalized logistic regression. Our result can be\nused to establish the phase transition boundary for the separability of two\nclasses. We assume that the data are generated from a single multivariate\nGaussian distribution with arbitrary covariance structure. We explore two\nspecial choices for the covariance matrix: spiked population model and two\nlayer neural networks with random first layer weights. The method we used for\nderiving the closed-form expression is from statistical physics known as the\nreplica method. Our asymptotic results match simulations already when $n,p$ are\nof the order of a few hundreds. For two layer neural networks, we reproduce the\nrecently developed `double descent' phenomenology for several classification\nmodels. We also discuss some statistical insights that can be drawn from these\nanalysis.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.10112v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08574v1",
    "title": "Camera Bias in a Fine Grained Classification Task",
    "authors": [
      "Philip T. Jackson",
      "Stephen Bonner",
      "Ning Jia",
      "Christopher Holder",
      "Jon Stonehouse",
      "Boguslaw Obara"
    ],
    "author_ids": [],
    "abstract": "We show that correlations between the camera used to acquire an image and the\nclass label of that image can be exploited by convolutional neural networks\n(CNN), resulting in a model that \"cheats\" at an image classification task by\nrecognizing which camera took the image and inferring the class label from the\ncamera. We show that models trained on a dataset with camera / label\ncorrelations do not generalize well to images in which those correlations are\nabsent, nor to images from unencountered cameras. Furthermore, we investigate\nwhich visual features they are exploiting for camera recognition. Our\nexperiments present evidence against the importance of global color statistics,\nlens deformation and chromatic aberration, and in favor of high frequency\nfeatures, which may be introduced by image processing algorithms built into the\ncameras.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08574v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08479v3",
    "title": "Active Learning under Label Shift",
    "authors": [
      "Eric Zhao",
      "Anqi Liu",
      "Animashree Anandkumar",
      "Yisong Yue"
    ],
    "author_ids": [],
    "abstract": "We address the problem of active learning under label shift: when the class\nproportions of source and target domains differ. We introduce a \"medial\ndistribution\" to incorporate a tradeoff between importance weighting and\nclass-balanced sampling and propose their combined usage in active learning.\nOur method is known as Mediated Active Learning under Label Shift (MALLS). It\nbalances the bias from class-balanced sampling and the variance from importance\nweighting. We prove sample complexity and generalization guarantees for MALLS\nwhich show active learning reduces asymptotic sample complexity even under\narbitrary label shift. We empirically demonstrate MALLS scales to\nhigh-dimensional datasets and can reduce the sample complexity of active\nlearning by 60% in deep active learning tasks.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08479v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.01194v1",
    "title": "Facets of Fairness in Search and Recommendation",
    "authors": [
      "Sahil Verma",
      "Ruoyuan Gao",
      "Chirag Shah"
    ],
    "author_ids": [],
    "abstract": "Several recent works have highlighted how search and recommender systems\nexhibit bias along different dimensions. Counteracting this bias and bringing a\ncertain amount of fairness in search is crucial to not only creating a more\nbalanced environment that considers relevance and diversity but also providing\na more sustainable way forward for both content consumers and content\nproducers. This short paper examines some of the recent works to define\nrelevance, diversity, and related concepts. Then, it focuses on explaining the\nemerging concept of fairness in various recommendation settings. In doing so,\nthis paper presents comparisons and highlights contracts among various\nmeasures, and gaps in our conceptual and evaluative frameworks.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.01194v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.08406v1",
    "title": "The role of collider bias in understanding statistics on racially biased policing",
    "authors": [
      "Norman Fenton",
      "Martin Neil",
      "Steven Frazier"
    ],
    "author_ids": [],
    "abstract": "Contradictory conclusions have been made about whether unarmed blacks are\nmore likely to be shot by police than unarmed whites using the same data. The\nproblem is that, by relying only on data of 'police encounters', there is the\npossibility that genuine bias can be hidden. We provide a causal Bayesian\nnetwork model to explain this bias, which is called collider bias or Berkson's\nparadox, and show how the different conclusions arise from the same model and\ndata. We also show that causal Bayesian networks provide the ideal formalism\nfor considering alternative hypotheses and explanations of bias.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08406v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08318v2",
    "title": "Closed Non-atomic Resource Allocation Games",
    "authors": [
      "Costas Courcoubetis",
      "Antonis Dimakis"
    ],
    "author_ids": [],
    "abstract": "How is efficiency affected when demand excesses over supply are signalled\nthrough waiting in queues? We consider a class of congestion games with a\nnonatomic set of players of a constant mass, based on a formulation of generic\nlinear programs as sequential resource allocation games. Players continuously\nselect activities such that they maximize linear objectives interpreted as\ntime-average of activity rewards, while active resource constraints cause\nqueueing. In turn, the resulting waiting delays enter in the optimization\nproblem of each player. The existence of Wardrop-type equilibria and their\nproperties are investivated by means of a potential function related to\nproportional fairness. The inefficiency of the equilibria relative to optimal\nresource allocation is characterized through the price of anarchy which is 2 if\nall players are of the same type ($\\infty$ if not).",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08318v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.08303v1",
    "title": "Wendy, the Good Little Fairness Widget",
    "authors": [
      "Klaus Kursawe"
    ],
    "author_ids": [],
    "abstract": "The advent of decentralized trading markets introduces a number of new\nchallenges for consensus protocols. In addition to the `usual' attacks -- a\nsubset of the validators trying to prevent disagreement -- there is now the\npossibility of financial fraud, which can abuse properties not normally\nconsidered critical in consensus protocols. We investigate the issues of\nattackers manipulating or exploiting the order in which transactions are\nscheduled in the blockchain. More concretely, we look into relative order\nfairness, i.e., ways we can assure that the relative order of transactions is\nfair. We show that one of the more intuitive definitions of fairness is\nimpossible to achieve. We then present Wendy, a group of low overhead protocols\nthat can implement different concepts of fairness. Wendy acts as an additional\nwidget for an existing blockchain, and is largely agnostic to the underlying\nblockchain and its security assumptions. Furthermore, it is possible to apply a\nthe protocol only for a subset of the transactions, and thus run several\nindependent fair markets on the same chain.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08303v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.08259v2",
    "title": "Transferable Calibration with Lower Bias and Variance in Domain Adaptation",
    "authors": [
      "Ximei Wang",
      "Mingsheng Long",
      "Jianmin Wang",
      "Michael I. Jordan"
    ],
    "author_ids": [],
    "abstract": "Domain Adaptation (DA) enables transferring a learning machine from a labeled\nsource domain to an unlabeled target one. While remarkable advances have been\nmade, most of the existing DA methods focus on improving the target accuracy at\ninference. How to estimate the predictive uncertainty of DA models is vital for\ndecision-making in safety-critical scenarios but remains the boundary to\nexplore. In this paper, we delve into the open problem of Calibration in DA,\nwhich is extremely challenging due to the coexistence of domain shift and the\nlack of target labels. We first reveal the dilemma that DA models learn higher\naccuracy at the expense of well-calibrated probabilities. Driven by this\nfinding, we propose Transferable Calibration (TransCal) to achieve more\naccurate calibration with lower bias and variance in a unified\nhyperparameter-free optimization framework. As a general post-hoc calibration\nmethod, TransCal can be easily applied to recalibrate existing DA methods. Its\nefficacy has been justified both theoretically and empirically.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08259v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08223v1",
    "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19 and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
    "authors": [
      "Ali H. Al-Timemy",
      "Rami N. Khushaba",
      "Zahraa M. Mosa",
      "Javier Escudero"
    ],
    "author_ids": [],
    "abstract": "Clinicians in the frontline need to assess quickly whether a patient with\nsymptoms indeed has COVID-19 or not. The difficulty of this task is exacerbated\nin low resource settings that may not have access to biotechnology tests.\nFurthermore, Tuberculosis (TB) remains a major health problem in several low-\nand middle-income countries and its common symptoms include fever, cough and\ntiredness, similarly to COVID-19. In order to help in the detection of\nCOVID-19, we propose the extraction of deep features (DF) from chest X-ray\nimages, a technology available in most hospitals, and their subsequent\nclassification using machine learning methods that do not require large\ncomputational resources. We compiled a five-class dataset of X-ray chest images\nincluding a balanced number of COVID-19, viral pneumonia, bacterial pneumonia,\nTB, and healthy cases. We compared the performance of pipelines combining 14\nindividual state-of-the-art pre-trained deep networks for DF extraction with\ntraditional machine learning classifiers. A pipeline consisting of ResNet-50\nfor DF computation and ensemble of subspace discriminant classifier was the\nbest performer in the classification of the five classes, achieving a detection\naccuracy of 91.6+ 2.6% (accuracy + 95% Confidence Interval). Furthermore, the\nsame pipeline achieved accuracies of 98.6+1.4% and 99.9+0.5% in simpler\nthree-class and two-class classification problems focused on distinguishing\nCOVID-19, TB and healthy cases; and COVID-19 and healthy images, respectively.\nThe pipeline was computationally efficient requiring just 0.19 second to\nextract DF per X-ray image and 2 minutes for training a traditional classifier\nwith more than 2000 images on a CPU machine. The results suggest the potential\nbenefits of using our pipeline in the detection of COVID-19, particularly in\nresource-limited settings and it can run with limited computational resources.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08223v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.15066v1",
    "title": "An Experimental Study of The Effects of Position Bias on Emotion CauseExtraction",
    "authors": [
      "Jiayuan Ding",
      "Mayank Kejriwal"
    ],
    "author_ids": [],
    "abstract": "Emotion Cause Extraction (ECE) aims to identify emotion causes from a\ndocument after annotating the emotion keywords. Some baselines have been\nproposed to address this problem, such as rule-based, commonsense based and\nmachine learning methods. We show, however, that a simple random selection\napproach toward ECE that does not require observing the text achieves similar\nperformance compared to the baselines. We utilized only position information\nrelative to the emotion cause to accomplish this goal. Since position\ninformation alone without observing the text resulted in higher F-measure, we\ntherefore uncovered a bias in the ECE single genre Sina-news benchmark. Further\nanalysis showed that an imbalance of emotional cause location exists in the\nbenchmark, with a majority of cause clauses immediately preceding the central\nemotion clause. We examine the bias from a linguistic perspective, and show\nthat high accuracy rate of current state-of-art deep learning models that\nutilize location information is only evident in datasets that contain such\nposition biases. The accuracy drastically reduced when a dataset with balanced\nlocation distribution is introduced. We therefore conclude that it is the\ninnate bias in this benchmark that caused high accuracy rate of these deep\nlearning models in ECE. We hope that the case study in this paper presents both\na cautionary lesson, as well as a template for further studies, in interpreting\nthe superior fit of deep learning models without checking for bias.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.15066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.08069v3",
    "title": "Maximizing coverage while ensuring fairness: a tale of conflicting objective",
    "authors": [
      "Abolfazl Asudeh",
      "Tanya Berger-Wolf",
      "Bhaskar DasGupta",
      "Anastasios Sidiropoulos"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness in computational problems has emerged as a $key$ topic\nduring recent years, buoyed by considerations for equitable resource\ndistributions and social justice. It $is$ possible to incorporate fairness in\ncomputational problems from several perspectives, such as using optimization,\ngame-theoretic or machine learning frameworks. In this paper we address the\nproblem of incorporation of fairness from a $combinatorial$ $optimization$\nperspective. We formulate a combinatorial optimization framework, suitable for\nanalysis by researchers in approximation algorithms and related areas, that\nincorporates fairness in maximum coverage problems as an interplay between\n$two$ conflicting objectives. Fairness is imposed in coverage by using coloring\nconstraints that $minimizes$ the discrepancies between number of elements of\ndifferent colors covered by selected sets; this is in contrast to the usual\ndiscrepancy minimization problems studied extensively in the literature where\n(usually two) colors are $not$ given $a$ $priori$ but need to be selected to\nminimize the maximum color discrepancy of $each$ individual set. Our main\nresults are a set of randomized and deterministic approximation algorithms that\nattempts to $simultaneously$ approximate both fairness and coverage in this\nframework.",
    "published_date": "2020-07-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CC",
      "cs.CG",
      "cs.DS",
      "68W25(Primary) 68W20, 68Q25, 68W40 (Secondary)",
      "F.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08069v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07990v4",
    "title": "Static pricing for multi-unit prophet inequalities",
    "authors": [
      "Shuchi Chawla",
      "Nikhil Devanur",
      "Thodoris Lykouris"
    ],
    "author_ids": [],
    "abstract": "We study a pricing problem where a seller has $k$ identical copies of a\nproduct, buyers arrive sequentially, and the seller prices the items aiming to\nmaximize social welfare. When $k=1$, this is the so called \"prophet inequality\"\nproblem for which there is a simple pricing scheme achieving a competitive\nratio of $1/2$. On the other end of the spectrum, as $k$ goes to infinity, the\nasymptotic performance of both static and adaptive pricing is well understood.\n  We provide a static pricing scheme for the small-supply regime: where $k$ is\nsmall but larger than $1$. Prior to our work, the best competitive ratio known\nfor this setting was the $1/2$ that follows from the single-unit prophet\ninequality. Our pricing scheme is easy to describe as well as practical -- it\nis anonymous, non-adaptive, and order-oblivious. We pick a single price that\nequalizes the expected fraction of items sold and the probability that the\nsupply does not sell out before all customers are served; this price is then\noffered to each customer while supply lasts. This extends an approach\nintroduced by Samuel-Cahn for the case of $k=1$. This pricing scheme achieves a\ncompetitive ratio that increases gradually with the supply. Subsequent work by\nJiang, Ma, and Zhang shows that our pricing scheme is the optimal static\npricing for every value of $k$.",
    "published_date": "2020-07-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07990v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.07878v2",
    "title": "Quantifying and Reducing Bias in Maximum Likelihood Estimation of Structured Anomalies",
    "authors": [
      "Uthsav Chitra",
      "Kimberly Ding",
      "Jasper C. H. Lee",
      "Benjamin J. Raphael"
    ],
    "author_ids": [],
    "abstract": "Anomaly estimation, or the problem of finding a subset of a dataset that\ndiffers from the rest of the dataset, is a classic problem in machine learning\nand data mining. In both theoretical work and in applications, the anomaly is\nassumed to have a specific structure defined by membership in an\n$\\textit{anomaly family}$. For example, in temporal data the anomaly family may\nbe time intervals, while in network data the anomaly family may be connected\nsubgraphs. The most prominent approach for anomaly estimation is to compute the\nMaximum Likelihood Estimator (MLE) of the anomaly; however, it was recently\nobserved that for normally distributed data, the MLE is a $\\textit{biased}$\nestimator for some anomaly families. In this work, we demonstrate that in the\nnormal means setting, the bias of the MLE depends on the size of the anomaly\nfamily. We prove that if the number of sets in the anomaly family that contain\nthe anomaly is sub-exponential, then the MLE is asymptotically unbiased. We\nalso provide empirical evidence that the converse is true: if the number of\nsuch sets is exponential, then the MLE is asymptotically biased. Our analysis\nunifies a number of earlier results on the bias of the MLE for specific anomaly\nfamilies. Next, we derive a new anomaly estimator using a mixture model, and we\nprove that our anomaly estimator is asymptotically unbiased regardless of the\nsize of the anomaly family. We illustrate the advantages of our estimator\nversus the MLE on disease outbreak and highway traffic data.",
    "published_date": "2020-07-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07878v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07860v2",
    "title": "Dialect Diversity in Text Summarization on Twitter",
    "authors": [
      "Vijay Keswani",
      "L. Elisa Celis"
    ],
    "author_ids": [],
    "abstract": "Discussions on Twitter involve participation from different communities with\ndifferent dialects and it is often necessary to summarize a large number of\nposts into a representative sample to provide a synopsis. Yet, any such\nrepresentative sample should sufficiently portray the underlying dialect\ndiversity to present the voices of different participating communities\nrepresenting the dialects. Extractive summarization algorithms perform the task\nof constructing subsets that succinctly capture the topic of any given set of\nposts. However, we observe that there is dialect bias in the summaries\ngenerated by common summarization approaches, i.e., they often return summaries\nthat under-represent certain dialects.\n  The vast majority of existing \"fair\" summarization approaches require\nsocially salient attribute labels (in this case, dialect) to ensure that the\ngenerated summary is fair with respect to the socially salient attribute.\nNevertheless, in many applications, these labels do not exist. Furthermore, due\nto the ever-evolving nature of dialects in social media, it is unreasonable to\nlabel or accurately infer the dialect of every social media post. To correct\nfor the dialect bias, we employ a framework that takes an existing text\nsummarization algorithm as a blackbox and, using a small set of dialect-diverse\nsentences, returns a summary that is relatively more dialect-diverse.\nCrucially, this approach does not need the posts being summarized to have\ndialect labels, ensuring that the diversification process is independent of\ndialect classification/identification models. We show the efficacy of our\napproach on Twitter datasets containing posts written in dialects used by\ndifferent social groups defined by race or gender; in all cases, our approach\nleads to improved dialect diversity compared to standard text summarization\napproaches.",
    "published_date": "2020-07-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07860v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.07275v1",
    "title": "Facial Recognition: A cross-national Survey on Public Acceptance, Privacy, and Discrimination",
    "authors": [
      "Léa Steinacker",
      "Miriam Meckel",
      "Genia Kostka",
      "Damian Borth"
    ],
    "author_ids": [],
    "abstract": "With rapid advances in machine learning (ML), more of this technology is\nbeing deployed into the real world interacting with us and our environment. One\nof the most widely applied application of ML is facial recognition as it is\nrunning on millions of devices. While being useful for some people, others\nperceive it as a threat when used by public authorities. This discrepancy and\nthe lack of policy increases the uncertainty in the ML community about the\nfuture direction of facial recognition research and development. In this paper\nwe present results from a cross-national survey about public acceptance,\nprivacy, and discrimination of the use of facial recognition technology (FRT)\nin the public. This study provides insights about the opinion towards FRT from\nChina, Germany, the United Kingdom (UK), and the United States (US), which can\nserve as input for policy makers and legal regulators.",
    "published_date": "2020-07-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07275v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07514v1",
    "title": "The Moral-IT Deck: A Tool for Ethics by Design",
    "authors": [
      "Lachlan Urquhart",
      "Peter Craigon"
    ],
    "author_ids": [],
    "abstract": "This paper presents the design process and empirical evaluation of a new tool\nfor enabling ethics by design: The Moral-IT Cards. Better tools are needed to\nsupport the role of technologists in addressing ethical issues during system\ndesign. These physical cards support reflection by technologists on normative\naspects of technology development, specifically on emerging risks, appropriate\nsafeguards and challenges of implementing these in the system. We discuss how\nthe cards were developed and tested within 5 workshops with 20 participants\nfrom both research and commercial settings. We consider the role of\ntechnologists in ethics from different EU/UK policymaking initiatives and\ndisciplinary perspectives (i.e. Science and Technology Studies (STS), IT Law,\nHuman Computer Interaction (HCI), Computer/Engineering Ethics). We then examine\nexisting ethics by design tools, and other cards based tools before arguing why\ncards can be a useful medium for addressing complex ethical issues. We present\nthe development process for the Moral-IT cards, document key features of our\ncard design, background on the content, the impact assessment board process for\nusing them and how this was formulated. We discuss our study design and\nmethodology before examining key findings which are clustered around three\noverarching themes. These are: the value of our cards as a tool, their impact\non the technology design process and how they structure ethical reflection\npractices. We conclude with key lessons and concepts such as how they level the\nplaying field for debate; enable ethical clustering, sorting and comparison;\nprovide appropriate anchors for discussion and highlighted the intertwined\nnature of ethics.",
    "published_date": "2020-07-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07514v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.07384v1",
    "title": "A Pairwise Fair and Community-preserving Approach to k-Center Clustering",
    "authors": [
      "Brian Brubach",
      "Darshan Chakrabarti",
      "John P. Dickerson",
      "Samir Khuller",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas"
    ],
    "author_ids": [],
    "abstract": "Clustering is a foundational problem in machine learning with numerous\napplications. As machine learning increases in ubiquity as a backend for\nautomated systems, concerns about fairness arise. Much of the current\nliterature on fairness deals with discrimination against protected classes in\nsupervised learning (group fairness). We define a different notion of fair\nclustering wherein the probability that two points (or a community of points)\nbecome separated is bounded by an increasing function of their pairwise\ndistance (or community diameter). We capture the situation where data points\nrepresent people who gain some benefit from being clustered together.\nUnfairness arises when certain points are deterministically separated, either\narbitrarily or by someone who intends to harm them as in the case of\ngerrymandering election districts. In response, we formally define two new\ntypes of fairness in the clustering setting, pairwise fairness and community\npreservation. To explore the practicality of our fairness goals, we devise an\napproach for extending existing $k$-center algorithms to satisfy these fairness\nconstraints. Analysis of this approach proves that reasonable approximations\ncan be achieved while maintaining fairness. In experiments, we compare the\neffectiveness of our approach to classical $k$-center algorithms/heuristics and\nexplore the tradeoff between optimal clustering and fairness.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07384v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07172v1",
    "title": "Attend And Discriminate: Beyond the State-of-the-Art for Human Activity Recognition using Wearable Sensors",
    "authors": [
      "Alireza Abedin",
      "Mahsa Ehsanpour",
      "Qinfeng Shi",
      "Hamid Rezatofighi",
      "Damith C. Ranasinghe"
    ],
    "author_ids": [],
    "abstract": "Wearables are fundamental to improving our understanding of human activities,\nespecially for an increasing number of healthcare applications from\nrehabilitation to fine-grained gait analysis. Although our collective know-how\nto solve Human Activity Recognition (HAR) problems with wearables has\nprogressed immensely with end-to-end deep learning paradigms, several\nfundamental opportunities remain overlooked. We rigorously explore these new\nopportunities to learn enriched and highly discriminating activity\nrepresentations. We propose: i) learning to exploit the latent relationships\nbetween multi-channel sensor modalities and specific activities; ii)\ninvestigating the effectiveness of data-agnostic augmentation for multi-modal\nsensor data streams to regularize deep HAR models; and iii) incorporating a\nclassification loss criterion to encourage minimal intra-class representation\ndifferences whilst maximising inter-class differences to achieve more\ndiscriminative features. Our contributions achieves new state-of-the-art\nperformance on four diverse activity recognition problem benchmarks with large\nmargins -- with up to 6% relative margin improvement. We extensively validate\nthe contributions from our design concepts through extensive experiments,\nincluding activity misalignment measures, ablation studies and insights shared\nthrough both quantitative and qualitative studies.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.HC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07172v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07127v1",
    "title": "Causal Inference using Gaussian Processes with Structured Latent Confounders",
    "authors": [
      "Sam Witty",
      "Kenta Takatsu",
      "David Jensen",
      "Vikash Mansinghka"
    ],
    "author_ids": [],
    "abstract": "Latent confounders---unobserved variables that influence both treatment and\noutcome---can bias estimates of causal effects. In some cases, these\nconfounders are shared across observations, e.g. all students taking a course\nare influenced by the course's difficulty in addition to any educational\ninterventions they receive individually. This paper shows how to\nsemiparametrically model latent confounders that have this structure and\nthereby improve estimates of causal effects. The key innovations are a\nhierarchical Bayesian model, Gaussian processes with structured latent\nconfounders (GP-SLC), and a Monte Carlo inference algorithm for this model\nbased on elliptical slice sampling. GP-SLC provides principled Bayesian\nuncertainty estimates of individual treatment effect with minimal assumptions\nabout the functional forms relating confounders, covariates, treatment, and\noutcome. Finally, this paper shows GP-SLC is competitive with or more accurate\nthan widely used causal inference techniques on three benchmark datasets,\nincluding the Infant Health and Development Program and a dataset showing the\neffect of changing temperatures on state-wide energy consumption across New\nEngland.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07127v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07092v2",
    "title": "A Normative approach to Attest Digital Discrimination",
    "authors": [
      "Natalia Criado",
      "Xavier Ferrer",
      "Jose M. Such"
    ],
    "author_ids": [],
    "abstract": "Digital discrimination is a form of discrimination whereby users are\nautomatically treated unfairly, unethically or just differently based on their\npersonal data by a machine learning (ML) system. Examples of digital\ndiscrimination include low-income neighbourhood's targeted with high-interest\nloans or low credit scores, and women being undervalued by 21% in online\nmarketing. Recently, different techniques and tools have been proposed to\ndetect biases that may lead to digital discrimination. These tools often\nrequire technical expertise to be executed and for their results to be\ninterpreted. To allow non-technical users to benefit from ML, simpler notions\nand concepts to represent and reason about digital discrimination are needed.\nIn this paper, we use norms as an abstraction to represent different situations\nthat may lead to digital discrimination. In particular, we formalise\nnon-discrimination norms in the context of ML systems and propose an algorithm\nto check whether ML systems violate these norms.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY",
      "68T27, 68T01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07092v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07027v1",
    "title": "Almost Envy-freeness, Envy-rank, and Nash Social Welfare Matchings",
    "authors": [
      "Alireza Farhadi",
      "MohammadTaghi Hajiaghayi",
      "Mohamad Latifian",
      "Masoud Seddighin",
      "Hadi Yami"
    ],
    "author_ids": [],
    "abstract": "Envy-free up to one good (EF1) and envy-free up to any good (EFX) are two\nwell-known extensions of envy-freeness for the case of indivisible items. It is\nshown that EF1 can always be guaranteed for agents with subadditive valuations.\nIn sharp contrast, it is unknown whether or not an EFX allocation always\nexists, even for four agents and additive valuations. In addition, the best\napproximation guarantee for EFX is $(\\phi -1) \\simeq 0.61$ by Amanitidis et\nal..\n  In order to find a middle ground to bridge this gap, in this paper we suggest\nanother fairness criterion, namely envy-freeness up to a random good or EFR,\nwhich is weaker than EFX, yet stronger than EF1. For this notion, we provide a\npolynomial-time $0.73$-approximation allocation algorithm. For our algorithm,\nwe introduce Nash Social Welfare Matching which makes a connection between Nash\nSocial Welfare and envy freeness. We believe Nash Social Welfare Matching will\nfind its applications in future work.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07027v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.07016v1",
    "title": "Public Goods From Private Data -- An Efficacy and Justification Paradox for Digital Contact Tracing",
    "authors": [
      "Andrew Buzzell"
    ],
    "author_ids": [],
    "abstract": "Debate about the adoption of digital contact tracing (DCT) apps to control\nthe spread of COVID-19 has focussed on risks to individual privacy (Sharma &\nBashir 2020, Tang 2020). This emphasis reveals significant challenges to\nethical deployment of DCT, but generates constraints which undermine\njustification to implement DCT. It would be a mistake to view this result\nsolely as the successful operation of ethical foresight analysis (Floridi &\nStrait 2020), preventing deployment of potentially harmful technology.\nPrivacy-centric analysis treats data as private property, frames the\nrelationship between individuals and governments as adversarial, entrenches\ntechnology platforms as gatekeepers, and supports a conception of emergency\npublic health authority as limited by individual consent and considerable\ncorporate influence that is in some tension with the more communitarian values\nthat typically inform public health ethics. To overcome the barriers to ethical\nand effective DCT, and develop infrastructure and policy that supports the\nrealization of potential public benefits of digital technology, a public\nresource conception of aggregate data should be developed.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07016v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.07015v2",
    "title": "Convergence analysis of the time-stepping numerical methods for time-fractional nonlinear subdiffusion equations",
    "authors": [
      "Hui Zhang",
      "Fanhai Zeng",
      "Xiaoyun Jiang",
      "George Em Karniadakis"
    ],
    "author_ids": [],
    "abstract": "In 1986, Dixon and McKee developed a discrete fractional Gr\\\"{o}nwall\ninequality [Z. Angew. Math. Mech., 66 (1986), pp. 535--544], which can be seen\nas a generalization of the classical discrete Gr\\\"{o}nwall inequality. However,\nthis generalized discrete Gr\\\"{o}nwall inequality has not been widely applied\nin the numerical analysis of the time-stepping methods for the time-fractional\nevolution equations. The main purpose of this paper is to show how to apply the\ngeneralized discrete Gr\\\"{o}nwall inequality to prove the convergence of a\nclass of time-stepping numerical methods for time-fractional nonlinear\nsubdiffusion equations, including the popular fractional backward difference\ntype methods of order one and two, and the second-order fractional\nCrank-Nicolson type methods. We obtain the optimal $L^2$ error estimate in\nspace discretization. The convergence of the fast time-stepping numerical\nmethods is also proved in a simple manner.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "35E15, 35K55, 65M12,"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07015v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.06908v1",
    "title": "Language, communication and society: a gender based linguistics analysis",
    "authors": [
      "P. Cutugno",
      "D. Chiarella",
      "R. Lucentini",
      "L. Marconi",
      "G. Morgavi"
    ],
    "author_ids": [],
    "abstract": "The purpose of this study is to find evidence for supporting the hypothesis\nthat language is the mirror of our thinking, our prejudices and cultural\nstereotypes. In this analysis, a questionnaire was administered to 537 people.\nThe answers have been analysed to see if gender stereotypes were present such\nas the attribution of psychological and behavioural characteristics. In\nparticular, the aim was to identify, if any, what are the stereotyped images,\nwhich emerge in defining the roles of men and women in modern society.\nMoreover, the results given can be a good starting point to understand if\ngender stereotypes, and the expectations they produce, can result in\npenalization or inequality. If so, the language and its use would create\ninherently a gender bias, which influences evaluations both in work settings\nboth in everyday life.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06908v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06837v6",
    "title": "Top-Related Meta-Learning Method for Few-Shot Object Detection",
    "authors": [
      "Qian Li",
      "Nan Guo",
      "Xiaochun Ye",
      "Duo Wang",
      "Dongrui Fan",
      "Zhimin Tang"
    ],
    "author_ids": [],
    "abstract": "Many meta-learning methods are proposed for few-shot detection. However,\nprevious most methods have two main problems, poor detection APs, and strong\nbias because of imbalance and insufficient datasets. Previous works mainly\nalleviate these issues by additional datasets, multi-relation attention\nmechanisms and sub-modules. However, they require more cost. In this work, for\nmeta-learning, we find that the main challenges focus on related or irrelevant\nsemantic features between categories. Therefore, based on semantic features, we\npropose a Top-C classification loss (i.e., TCL-C) for classification task and a\ncategory-based grouping mechanism for category-based meta-features obtained by\nthe meta-model. The TCL-C exploits the true-label prediction and the most\nlikely C-1 false classification predictions to improve detection performance on\nfew-shot classes. According to similar appearance (i.e., visual appearance,\nshape, and limbs etc.) and environment in which objects often appear, the\ncategory-based grouping mechanism splits categories into disjoint groups to\nmake similar semantic features more compact between categories within a group\nand obtain more significant difference between groups, alleviating the strong\nbias problem and further improving detection APs. The whole training consists\nof the base model and the fine-tuning phases. According to grouping mechanism,\nwe group the meta-features vectors obtained by meta-model, so that the\ndistribution difference between groups is obvious, and the one within each\ngroup is less. Extensive experiments on Pascal VOC dataset demonstrate that\nours which combines the TCL-C with category-based grouping significantly\noutperforms previous state-of-the-art methods for few-shot detection. Compared\nwith previous competitive baseline, ours improves detection APs by almost 4%\nfor few-shot detection.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06837v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06776v1",
    "title": "Verification of ML Systems via Reparameterization",
    "authors": [
      "Jean-Baptiste Tristan",
      "Joseph Tassarotti",
      "Koundinya Vajjha",
      "Michael L. Wick",
      "Anindya Banerjee"
    ],
    "author_ids": [],
    "abstract": "As machine learning is increasingly used in essential systems, it is\nimportant to reduce or eliminate the incidence of serious bugs. A growing body\nof research has developed machine learning algorithms with formal guarantees\nabout performance, robustness, or fairness. Yet, the analysis of these\nalgorithms is often complex, and implementing such systems in practice\nintroduces room for error. Proof assistants can be used to formally verify\nmachine learning systems by constructing machine checked proofs of correctness\nthat rule out such bugs. However, reasoning about probabilistic claims inside\nof a proof assistant remains challenging. We show how a probabilistic program\ncan be automatically represented in a theorem prover using the concept of\n\\emph{reparameterization}, and how some of the tedious proofs of measurability\ncan be generated automatically from the probabilistic program. To demonstrate\nthat this approach is broad enough to handle rather different types of machine\nlearning systems, we verify both a classic result from statistical learning\ntheory (PAC-learnability of decision stumps) and prove that the null model used\nin a Bayesian hypothesis test satisfies a fairness criterion called demographic\nparity.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06776v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06761v2",
    "title": "Can neural networks acquire a structural bias from raw linguistic data?",
    "authors": [
      "Alex Warstadt",
      "Samuel R. Bowman"
    ],
    "author_ids": [],
    "abstract": "We evaluate whether BERT, a widely used neural network for sentence\nprocessing, acquires an inductive bias towards forming structural\ngeneralizations through pretraining on raw data. We conduct four experiments\ntesting its preference for structural vs. linear generalizations in different\nstructure-dependent phenomena. We find that BERT makes a structural\ngeneralization in 3 out of 4 empirical domains---subject-auxiliary inversion,\nreflexive binding, and verb tense detection in embedded clauses---but makes a\nlinear generalization when tested on NPI licensing. We argue that these results\nare the strongest evidence so far from artificial learners supporting the\nproposition that a structural bias can be acquired from raw data. If this\nconclusion is correct, it is tentative evidence that some linguistic universals\ncan be acquired by learners without innate biases. However, the precise\nimplications for human language acquisition are unclear, as humans learn\nlanguage from significantly less data than BERT.",
    "published_date": "2020-07-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06761v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06738v1",
    "title": "Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy",
    "authors": [
      "Edward Moroshko",
      "Suriya Gunasekar",
      "Blake Woodworth",
      "Jason D. Lee",
      "Nathan Srebro",
      "Daniel Soudry"
    ],
    "author_ids": [],
    "abstract": "We provide a detailed asymptotic study of gradient flow trajectories and\ntheir implicit optimization bias when minimizing the exponential loss over\n\"diagonal linear networks\". This is the simplest model displaying a transition\nbetween \"kernel\" and non-kernel (\"rich\" or \"active\") regimes. We show how the\ntransition is controlled by the relationship between the initialization scale\nand how accurately we minimize the training loss. Our results indicate that\nsome limit behaviors of gradient descent only kick in at ridiculous training\naccuracies (well beyond $10^{-100}$). Moreover, the implicit bias at reasonable\ninitialization scales and training accuracies is more complex and not captured\nby these limits.",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06738v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06699v2",
    "title": "Fair Algorithms for Multi-Agent Multi-Armed Bandits",
    "authors": [
      "Safwan Hossain",
      "Evi Micha",
      "Nisarg Shah"
    ],
    "author_ids": [],
    "abstract": "We propose a multi-agent variant of the classical multi-armed bandit problem,\nin which there are $N$ agents and $K$ arms, and pulling an arm generates a\n(possibly different) stochastic reward for each agent. Unlike the classical\nmulti-armed bandit problem, the goal is not to learn the \"best arm\"; indeed,\neach agent may perceive a different arm to be the best for her personally.\nInstead, we seek to learn a fair distribution over the arms. Drawing on a long\nline of research in economics and computer science, we use the Nash social\nwelfare as our notion of fairness. We design multi-agent variants of three\nclassic multi-armed bandit algorithms and show that they achieve sublinear\nregret, which is now measured in terms of the lost Nash social welfare.",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06699v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06570v1",
    "title": "Towards causal benchmarking of bias in face analysis algorithms",
    "authors": [
      "Guha Balakrishnan",
      "Yuanjun Xiong",
      "Wei Xia",
      "Pietro Perona"
    ],
    "author_ids": [],
    "abstract": "Measuring algorithmic bias is crucial both to assess algorithmic fairness,\nand to guide the improvement of algorithms. Current methods to measure\nalgorithmic bias in computer vision, which are based on observational datasets,\nare inadequate for this task because they conflate algorithmic bias with\ndataset bias.\n  To address this problem we develop an experimental method for measuring\nalgorithmic bias of face analysis algorithms, which manipulates directly the\nattributes of interest, e.g., gender and skin tone, in order to reveal causal\nlinks between attribute variation and performance change. Our proposed method\nis based on generating synthetic ``transects'' of matched sample images that\nare designed to differ along specific attributes while leaving other attributes\nconstant. A crucial aspect of our approach is relying on the perception of\nhuman observers, both to guide manipulations, and to measure algorithmic bias.\n  Besides allowing the measurement of algorithmic bias, synthetic transects\nhave other advantages with respect to observational datasets: they sample\nattributes more evenly allowing for more straightforward bias analysis on\nminority and intersectional groups, they enable prediction of bias in new\nscenarios, they greatly reduce ethical and legal challenges, and they are\neconomical and fast to obtain, helping make bias testing affordable and widely\navailable.\n  We validate our method by comparing it to a study that employs the\ntraditional observational method for analyzing bias in gender classification\nalgorithms. The two methods reach different conclusions. While the\nobservational method reports gender and skin color biases, the experimental\nmethod reveals biases due to gender, hair length, age, and facial hair.",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06570v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06418v2",
    "title": "Lessons Learned from the Training of GANs on Artificial Datasets",
    "authors": [
      "Shichang Tang"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GANs) have made great progress in\nsynthesizing realistic images in recent years. However, they are often trained\non image datasets with either too few samples or too many classes belonging to\ndifferent data distributions. Consequently, GANs are prone to underfitting or\noverfitting, making the analysis of them difficult and constrained. Therefore,\nin order to conduct a thorough study on GANs while obviating unnecessary\ninterferences introduced by the datasets, we train them on artificial datasets\nwhere there are infinitely many samples and the real data distributions are\nsimple, high-dimensional and have structured manifolds. Moreover, the\ngenerators are designed such that optimal sets of parameters exist.\nEmpirically, we find that under various distance measures, the generator fails\nto learn such parameters with the GAN training procedure. We also find that\ntraining mixtures of GANs leads to more performance gain compared to increasing\nthe network depth or width when the model complexity is high enough. Our\nexperimental results demonstrate that a mixture of generators can discover\ndifferent modes or different classes automatically in an unsupervised setting,\nwhich we attribute to the distribution of the generation and discrimination\ntasks across multiple generators and discriminators. As an example of the\ngeneralizability of our conclusions to realistic datasets, we train a mixture\nof GANs on the CIFAR-10 dataset and our method significantly outperforms the\nstate-of-the-art in terms of popular metrics, i.e., Inception Score (IS) and\nFr\\'echet Inception Distance (FID).",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06418v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06242v3",
    "title": "Optimal Bounds on the Price of Fairness for Indivisible Goods",
    "authors": [
      "Siddharth Barman",
      "Umang Bhaskar",
      "Nisarg Shah"
    ],
    "author_ids": [],
    "abstract": "In the allocation of resources to a set of agents, how do fairness guarantees\nimpact the social welfare? A quantitative measure of this impact is the price\nof fairness, which measures the worst-case loss of social welfare due to\nfairness constraints. While initially studied for divisible goods, recent work\non the price of fairness also studies the setting of indivisible goods.\n  In this paper, we resolve the price of two well-studied fairness notions for\nthe allocation of indivisible goods: envy-freeness up to one good (EF1), and\napproximate maximin share (MMS). For both EF1 and 1/2-MMS guarantees, we show,\nvia different techniques, that the price of fairness is $O(\\sqrt{n})$, where\n$n$ is the number of agents. From previous work, it follows that our bounds are\ntight. Our bounds are obtained via efficient algorithms. For 1/2-MMS, our bound\nholds for additive valuations, whereas for EF1, our bound holds for the more\ngeneral class of subadditive valuations. This resolves an open problem posed by\nBei et al. (2019).",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DS",
      "91A68",
      "F.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06242v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.06198v2",
    "title": "Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder",
    "authors": [
      "Gouthaman KV",
      "Anurag Mittal"
    ],
    "author_ids": [],
    "abstract": "Recent studies have shown that current VQA models are heavily biased on the\nlanguage priors in the train set to answer the question, irrespective of the\nimage. E.g., overwhelmingly answer \"what sport is\" as \"tennis\" or \"what color\nbanana\" as \"yellow.\" This behavior restricts them from real-world application\nscenarios. In this work, we propose a novel model-agnostic question encoder,\nVisually-Grounded Question Encoder (VGQE), for VQA that reduces this effect.\nVGQE utilizes both visual and language modalities equally while encoding the\nquestion. Hence the question representation itself gets sufficient\nvisual-grounding, and thus reduces the dependency of the model on the language\npriors. We demonstrate the effect of VGQE on three recent VQA models and\nachieve state-of-the-art results on the bias-sensitive split of the VQAv2\ndataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on\nthe standard VQAv2 benchmark, our approach does not drop the accuracy; instead,\nit improves the performance.",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06198v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06141v1",
    "title": "Gender Classification and Bias Mitigation in Facial Images",
    "authors": [
      "Wenying Wu",
      "Pavlos Protopapas",
      "Zheng Yang",
      "Panagiotis Michalatos"
    ],
    "author_ids": [],
    "abstract": "Gender classification algorithms have important applications in many domains\ntoday such as demographic research, law enforcement, as well as human-computer\ninteraction. Recent research showed that algorithms trained on biased benchmark\ndatabases could result in algorithmic bias. However, to date, little research\nhas been carried out on gender classification algorithms' bias towards gender\nminorities subgroups, such as the LGBTQ and the non-binary population, who have\ndistinct characteristics in gender expression. In this paper, we began by\nconducting surveys on existing benchmark databases for facial recognition and\ngender classification tasks. We discovered that the current benchmark databases\nlack representation of gender minority subgroups. We worked on extending the\ncurrent binary gender classifier to include a non-binary gender class. We did\nthat by assembling two new facial image databases: 1) a racially balanced\ninclusive database with a subset of LGBTQ population 2) an inclusive-gender\ndatabase that consists of people with non-binary gender. We worked to increase\nclassification accuracy and mitigate algorithmic biases on our baseline model\ntrained on the augmented benchmark database. Our ensemble model has achieved an\noverall accuracy score of 90.39%, which is a 38.72% increase from the baseline\nbinary gender classifier trained on Adience. While this is an initial attempt\ntowards mitigating bias in gender classification, more work is needed in\nmodeling gender as a continuum by assembling more inclusive databases.",
    "published_date": "2020-07-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06141v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06073v2",
    "title": "Fair Division with Binary Valuations: One Rule to Rule Them All",
    "authors": [
      "Daniel Halpern",
      "Ariel D. Procaccia",
      "Alexandros Psomas",
      "Nisarg Shah"
    ],
    "author_ids": [],
    "abstract": "We study fair allocation of indivisible goods among agents. Prior research\nfocuses on additive agent preferences, which leads to an impossibility when\nseeking truthfulness, fairness, and efficiency. We show that when agents have\nbinary additive preferences, a compelling rule -- maximum Nash welfare (MNW) --\nprovides all three guarantees.\n  Specifically, we show that deterministic MNW with lexicographic tie-breaking\nis group strategyproof in addition to being envy-free up to one good and Pareto\noptimal. We also prove that fractional MNW -- known to be group strategyproof,\nenvy-free, and Pareto optimal -- can be implemented as a distribution over\ndeterministic MNW allocations, which are envy-free up to one good. Our work\nestablishes maximum Nash welfare as the ultimate allocation rule in the realm\nof binary additive preferences.",
    "published_date": "2020-07-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06073v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.06029v2",
    "title": "Ensuring Fairness Beyond the Training Data",
    "authors": [
      "Debmalya Mandal",
      "Samuel Deng",
      "Suman Jana",
      "Jeannette M. Wing",
      "Daniel Hsu"
    ],
    "author_ids": [],
    "abstract": "We initiate the study of fair classifiers that are robust to perturbations in\nthe training distribution. Despite recent progress, the literature on fairness\nhas largely ignored the design of fair and robust classifiers. In this work, we\ndevelop classifiers that are fair not only with respect to the training\ndistribution, but also for a class of distributions that are weighted\nperturbations of the training samples. We formulate a min-max objective\nfunction whose goal is to minimize a distributionally robust training loss, and\nat the same time, find a classifier that is fair with respect to a class of\ndistributions. We first reduce this problem to finding a fair classifier that\nis robust with respect to the class of distributions. Based on online learning\nalgorithm, we develop an iterative algorithm that provably converges to such a\nfair and robust solution. Experiments on standard machine learning fairness\ndatasets suggest that, compared to the state-of-the-art fair classifiers, our\nclassifier retains fairness guarantees and test accuracy for a large class of\nperturbations on the test set. Furthermore, our experiments show that there is\nan inherent trade-off between fairness robustness and accuracy of such\nclassifiers.",
    "published_date": "2020-07-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06029v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07241v1",
    "title": "Learning Frame Level Attention for Environmental Sound Classification",
    "authors": [
      "Zhichao Zhang",
      "Shugong Xu",
      "Shunqing Zhang",
      "Tianhao Qiao",
      "Shan Cao"
    ],
    "author_ids": [],
    "abstract": "Environmental sound classification (ESC) is a challenging problem due to the\ncomplexity of sounds. The classification performance is heavily dependent on\nthe effectiveness of representative features extracted from the environmental\nsounds. However, ESC often suffers from the semantically irrelevant frames and\nsilent frames. In order to deal with this, we employ a frame-level attention\nmodel to focus on the semantically relevant frames and salient frames.\nSpecifically, we first propose a convolutional recurrent neural network to\nlearn spectro-temporal features and temporal correlations. Then, we extend our\nconvolutional RNN model with a frame-level attention mechanism to learn\ndiscriminative feature representations for ESC. We investigated the\nclassification performance when using different attention scaling function and\napplying different layers. Experiments were conducted on ESC-50 and ESC-10\ndatasets. Experimental results demonstrated the effectiveness of the proposed\nmethod and our method achieved the state-of-the-art or competitive\nclassification accuracy with lower computational complexity. We also visualized\nour attention results and observed that the proposed attention mechanism was\nable to lead the network tofocus on the semantically relevant parts of\nenvironmental sounds.",
    "published_date": "2020-07-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07241v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05820v1",
    "title": "Recent Results on Proportional Fair Scheduling for mmWave-based Industrial Wireless Networks",
    "authors": [
      "Jiteng Ma",
      "Adnan Aijaz",
      "Mark Beach"
    ],
    "author_ids": [],
    "abstract": "Millimeter wave (mmWave) communication has recently attracted significant\nattention from both industrial and academic communities. The large bandwidth\navailability as well as low interference nature of mmWave spectrum is\nparticularly attractive for industrial communication. However, inherent\nchallenges such as coverage and blockage of mmWave communication cause highly\nfluctuated channel quality. This paper explores wireless medium access control\n(MAC) schedulers for mmWave-based industrial wireless applications. Our\nobjective is to design a high-performance and enhanced fairness MAC scheduling\nalgorithm that responds rapidly to channel variations. The key contribution of\nour work is a method to modify the standard proportional fair (SPF) scheduler.\nIt introduces more flexibility and dynamic properties. Compared to the SPF, our\nenhanced proportional fair (EPF) scheduler not only improves the priority for\nusers in poor channel conditions but also accelerates the reaction time in\nfluctuated channel conditions. By providing higher fairness for all users and\nenhancing system robustness, it particularly adapts to the scatter-rich\nindustrial mmWave communication environment. Through extensive performance\nevaluation based on the widely accepted network simulator (ns-3), we show that\nthe new scheduler achieves better performance in terms of delivering ultra-low\nlatency and reliable services over mmWave-based industrial communication.",
    "published_date": "2020-07-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05820v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.05720v1",
    "title": "ECML: An Ensemble Cascade Metric Learning Mechanism towards Face Verification",
    "authors": [
      "Fu Xiong",
      "Yang Xiao",
      "Zhiguo Cao",
      "Yancheng Wang",
      "Joey Tianyi Zhou",
      "Jianxi Wu"
    ],
    "author_ids": [],
    "abstract": "Face verification can be regarded as a 2-class fine-grained visual\nrecognition problem. Enhancing the feature's discriminative power is one of the\nkey problems to improve its performance. Metric learning technology is often\napplied to address this need, while achieving a good tradeoff between\nunderfitting and overfitting plays the vital role in metric learning. Hence, we\npropose a novel ensemble cascade metric learning (ECML) mechanism. In\nparticular, hierarchical metric learning is executed in the cascade way to\nalleviate underfitting. Meanwhile, at each learning level, the features are\nsplit into non-overlapping groups. Then, metric learning is executed among the\nfeature groups in the ensemble manner to resist overfitting. Considering the\nfeature distribution characteristics of faces, a robust Mahalanobis metric\nlearning method (RMML) with closed-form solution is additionally proposed. It\ncan avoid the computation failure issue on inverse matrix faced by some\nwell-known metric learning approaches (e.g., KISSME). Embedding RMML into the\nproposed ECML mechanism, our metric learning paradigm (EC-RMML) can run in the\none-pass learning manner. Experimental results demonstrate that EC-RMML is\nsuperior to state-of-the-art metric learning methods for face verification.\nAnd, the proposed ensemble cascade metric learning mechanism is also applicable\nto other metric learning approaches.",
    "published_date": "2020-07-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05720v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05675v3",
    "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding",
    "authors": [
      "Jinhai Yang",
      "Hua Yang",
      "Lin Chen"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.",
    "published_date": "2020-07-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05675v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05625v2",
    "title": "Conservation laws for free-boundary fluid layers",
    "authors": [
      "Ed Bueler"
    ],
    "author_ids": [],
    "abstract": "Time-dependent models of fluid motion in thin layers, subject to signed\nsource terms, represent important sub-problems within climate dynamics.\nExamples include ice sheets, sea ice, and even shallow oceans and lakes. We\naddress these problems as discrete-time sequences of continuous-space weak\nformulations, namely (monotone) variational inequalities or complementarity\nproblems, in which the conserved quantity is the layer thickness. Free\nboundaries wherein the thickness and mass flux both go to zero at the margin of\nthe fluid layer generically arise in such models. After showing these problems\nare well-posed in several cases, we consider the limitations to discrete\nconservation in numerical schemes. A free boundary in a region of negative\nsource -- an ablation-caused margin -- turns out to be a barrier to exact\nconservation in either a continuous- or discrete-space sense. We then propose\ncomputable a posteriori quantities which allow conservation-error bounds in\nfinite volume and finite element schemes.",
    "published_date": "2020-07-10T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "35K86, 65K15, 76A20, 76D27, 76M12"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05625v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.05516v4",
    "title": "A Causal Linear Model to Quantify Edge Flow and Edge Unfairness for UnfairEdge Prioritization and Discrimination Removal",
    "authors": [
      "Pavan Ravishankar",
      "Pranshu Malviya",
      "Balaraman Ravindran"
    ],
    "author_ids": [],
    "abstract": "Law enforcement must prioritize sources of unfairness before mitigating their\nunderlying unfairness, considering that they have limited resources. Unlike\nprevious works that only make cautionary claims of discrimination and de-biases\ndata after its generation, this paper attempts to prioritize unfair sources\nbefore mitigating their unfairness in the real-world. We assume that a causal\nbayesian network, representative of the data generation procedure, along with\nthe sensitive nodes, that result in unfairness, are given. We quantify Edge\nFlow, which is the belief flowing along an edge by attenuating the indirect\npath influences, and use it to quantify Edge Unfairness. We prove that\ncumulative unfairness is non-existent in any decision, like judicial bail,\ntowards any sensitive groups, like race, when the edge unfairness is absent,\ngiven an error-free linear model of conditional probability. We then measure\nthe potential to mitigate the cumulative unfairness when edge unfairness is\ndecreased. Based on these measures, we propose an unfair edge prioritization\nalgorithm that prioritizes the unfair edges and a discrimination removal\nprocedure that de-biases the generated data distribution. The experimental\nsection validates the specifications used for quantifying the above measures.",
    "published_date": "2020-07-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05516v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05456v1",
    "title": "Improved Analysis of UCRL2 with Empirical Bernstein Inequality",
    "authors": [
      "Ronan Fruit",
      "Matteo Pirotta",
      "Alessandro Lazaric"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of exploration-exploitation in communicating Markov\nDecision Processes. We provide an analysis of UCRL2 with Empirical Bernstein\ninequalities (UCRL2B). For any MDP with $S$ states, $A$ actions, $\\Gamma \\leq\nS$ next states and diameter $D$, the regret of UCRL2B is bounded as\n$\\widetilde{O}(\\sqrt{D\\Gamma S A T})$.",
    "published_date": "2020-07-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05456v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05449v1",
    "title": "Information Freshness of Updates Sent over LEO Satellite Multi-Hop Networks",
    "authors": [
      "Federico Chiariotti",
      "Olga Vikhrova",
      "Beatriz Soret",
      "Petar Popovski"
    ],
    "author_ids": [],
    "abstract": "Low Earth Orbit (LEO) satellite constellations are bringing the Internet of\nThings (IoT) to the space arena, also known as non-terrestrial networks.\nSeveral IoT satellite applications for tracking ships and cargo can be seen as\nexemplary cases of intermittent transmission of updates whose main performance\nparameter is the information freshness. This paper analyzes the Age of\nInformation (AoI) of a satellite network with multiple sources and destinations\nthat are very distant and therefore require several consecutive multi-hop\ntransmissions. A packet erasure channel and different queueing policies are\nconsidered. We provide closed-form bounds and tight approximations of the\naverage AoI, as well as an upper bound of the Peak Age of Information (PAoI)\ndistribution as a worst-case metric for the system design. The performance\nevaluation reveals complex trade-offs among age, load, and packet losses. The\noptimal operational point is found when the combination of arrival rates and\npacket losses is such that the system load can ensure fresh information at the\nreceiver; nevertheless, achieving this is highly dependent on the mesh\ntopology. Moreover, the potential of an age-aware scheduling strategy is\ninvestigated and the fairness among users discussed. The results show the need\nto identify the bottleneck nodes for the age, as improving the rate and\nreliability of those critical links will highly impact on the overall\nperformance. The model is general enough to represent other multi-hop mesh\nnetworks.",
    "published_date": "2020-07-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05449v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.05443v3",
    "title": "Algorithmic Fairness in Education",
    "authors": [
      "René F. Kizilcec",
      "Hansol Lee"
    ],
    "author_ids": [],
    "abstract": "Data-driven predictive models are increasingly used in education to support\nstudents, instructors, and administrators. However, there are concerns about\nthe fairness of the predictions and uses of these algorithmic systems. In this\nintroduction to algorithmic fairness in education, we draw parallels to prior\nliterature on educational access, bias, and discrimination, and we examine core\ncomponents of algorithmic systems (measurement, model learning, and action) to\nidentify sources of bias and discrimination in the process of developing and\ndeploying these systems. Statistical, similarity-based, and causal notions of\nfairness are reviewed and contrasted in the way they apply in educational\ncontexts. Recommendations for policy makers and developers of educational\ntechnology offer guidance for how to promote algorithmic fairness in education.",
    "published_date": "2020-07-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05443v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05073v1",
    "title": "Predictive Value Generalization Bounds",
    "authors": [
      "Keshav Vemuri",
      "Nathan Srebro"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study a bi-criterion framework for assessing scoring\nfunctions in the context of binary classification. The positive and negative\npredictive values (ppv and npv, respectively) are conditional probabilities of\nthe true label matching a classifier's predicted label. The usual\nclassification error rate is a linear combination of these probabilities, and\ntherefore, concentration inequalities for the error rate do not yield\nconfidence intervals for the two separate predictive values. We study\ngeneralization properties of scoring functions with respect to predictive\nvalues by deriving new distribution-free large deviation and uniform\nconvergence bounds. The latter bound is stated in terms of a measure of\nfunction class complexity that we call the order coefficient; we relate this\ncombinatorial quantity to the VC-subgraph dimension.",
    "published_date": "2020-07-09T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05073v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.07352v1",
    "title": "Degrees of individual and groupwise backward and forward responsibility in extensive-form games with ambiguity, and their application to social choice problems",
    "authors": [
      "Jobst Heitzig",
      "Sarah Hiller"
    ],
    "author_ids": [],
    "abstract": "Many real-world situations of ethical relevance, in particular those of\nlarge-scale social choice such as mitigating climate change, involve not only\nmany agents whose decisions interact in complicated ways, but also various\nforms of uncertainty, including quantifiable risk and unquantifiable ambiguity.\nIn such problems, an assessment of individual and groupwise moral\nresponsibility for ethically undesired outcomes or their responsibility to\navoid such is challenging and prone to the risk of under- or overdetermination\nof responsibility. In contrast to existing approaches based on strict causation\nor certain deontic logics that focus on a binary classification of\n`responsible' vs `not responsible', we here present several different\nquantitative responsibility metrics that assess responsibility degrees in units\nof probability. For this, we use a framework based on an adapted version of\nextensive-form game trees and an axiomatic approach that specifies a number of\npotentially desirable properties of such metrics, and then test the developed\ncandidate metrics by their application to a number of paradigmatic social\nchoice situations. We find that while most properties one might desire of such\nresponsibility metrics can be fulfilled by some variant, an optimal metric that\nclearly outperforms others has yet to be found.",
    "published_date": "2020-07-09T00:00:00",
    "year": 2020,
    "categories": [
      "econ.TH",
      "cs.AI",
      "91B06, 91A35, 90B50, 91A18",
      "F.4.3; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.07352v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04693v1",
    "title": "Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment",
    "authors": [
      "Abhishek Gupta",
      "Erick Galinkin"
    ],
    "author_ids": [],
    "abstract": "Security and ethics are both core to ensuring that a machine learning system\ncan be trusted. In production machine learning, there is generally a hand-off\nfrom those who build a model to those who deploy a model. In this hand-off, the\nengineers responsible for model deployment are often not privy to the details\nof the model and thus, the potential vulnerabilities associated with its usage,\nexposure, or compromise. Techniques such as model theft, model inversion, or\nmodel misuse may not be considered in model deployment, and so it is incumbent\nupon data scientists and machine learning engineers to understand these\npotential risks so they can communicate them to the engineers deploying and\nhosting their models. This is an open problem in the machine learning community\nand in order to help alleviate this issue, automated systems for validating\nprivacy and security of models need to be developed, which will help to lower\nthe burden of implementing these hand-offs and increasing the ubiquity of their\nadoption.",
    "published_date": "2020-07-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04693v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04611v2",
    "title": "A deep learning approach to identify unhealthy advertisements in street view images",
    "authors": [
      "Gregory Palmer",
      "Mark Green",
      "Emma Boyland",
      "Yales Stefano Rios Vasconcelos",
      "Rahul Savani",
      "Alex Singleton"
    ],
    "author_ids": [],
    "abstract": "While outdoor advertisements are common features within towns and cities,\nthey may reinforce social inequalities in health. Vulnerable populations in\ndeprived areas may have greater exposure to fast food, gambling and alcohol\nadvertisements encouraging their consumption. Understanding who is exposed and\nevaluating potential policy restrictions requires a substantial manual data\ncollection effort. To address this problem we develop a deep learning workflow\nto automatically extract and classify unhealthy advertisements from\nstreet-level images. We introduce the Liverpool 360 Street View (LIV360SV)\ndataset for evaluating our workflow. The dataset contains 25,349, 360 degree,\nstreet-level images collected via cycling with a GoPro Fusion camera, recorded\nJan 14th - 18th 2020. 10,106 advertisements were identified and classified as\nfood (1335), alcohol (217), gambling (149) and other (8405) (e.g., cars and\nbroadband). We find evidence of social inequalities with a larger proportion of\nfood advertisements located within deprived areas and those frequented by\nstudents. Our project presents a novel implementation for the incidental\nclassification of street view images for identifying unhealthy advertisements,\nproviding a means through which to identify areas that can benefit from tougher\nadvertisement restriction policies for tackling social inequalities.",
    "published_date": "2020-07-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04611v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04525v1",
    "title": "PointMask: Towards Interpretable and Bias-Resilient Point Cloud Processing",
    "authors": [
      "Saeid Asgari Taghanaki",
      "Kaveh Hassani",
      "Pradeep Kumar Jayaraman",
      "Amir Hosein Khasahmadi",
      "Tonya Custis"
    ],
    "author_ids": [],
    "abstract": "Deep classifiers tend to associate a few discriminative input variables with\ntheir objective function, which in turn, may hurt their generalization\ncapabilities. To address this, one can design systematic experiments and/or\ninspect the models via interpretability methods. In this paper, we investigate\nboth of these strategies on deep models operating on point clouds. We propose\nPointMask, a model-agnostic interpretable information-bottleneck approach for\nattribution in point cloud models. PointMask encourages exploring the majority\nof variation factors in the input space while gradually converging to a general\nsolution. More specifically, PointMask introduces a regularization term that\nminimizes the mutual information between the input and the latent features used\nto masks out irrelevant variables. We show that coupling a PointMask layer with\nan arbitrary model can discern the points in the input space which contribute\nthe most to the prediction score, thereby leading to interpretability. Through\ndesigned bias experiments, we also show that thanks to its gradual masking\nfeature, our proposed method is effective in handling data bias.",
    "published_date": "2020-07-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04525v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04484v1",
    "title": "Transparency Tools for Fairness in AI (Luskin)",
    "authors": [
      "Mingliang Chen",
      "Aria Shahverdi",
      "Sarah Anderson",
      "Se Yong Park",
      "Justin Zhang",
      "Dana Dachman-Soled",
      "Kristin Lauter",
      "Min Wu"
    ],
    "author_ids": [],
    "abstract": "We propose new tools for policy-makers to use when assessing and correcting\nfairness and bias in AI algorithms. The three tools are:\n  - A new definition of fairness called \"controlled fairness\" with respect to\nchoices of protected features and filters. The definition provides a simple\ntest of fairness of an algorithm with respect to a dataset. This notion of\nfairness is suitable in cases where fairness is prioritized over accuracy, such\nas in cases where there is no \"ground truth\" data, only data labeled with past\ndecisions (which may have been biased).\n  - Algorithms for retraining a given classifier to achieve \"controlled\nfairness\" with respect to a choice of features and filters. Two algorithms are\npresented, implemented and tested. These algorithms require training two\ndifferent models in two stages. We experiment with combinations of various\ntypes of models for the first and second stage and report on which combinations\nperform best in terms of fairness and accuracy.\n  - Algorithms for adjusting model parameters to achieve a notion of fairness\ncalled \"classification parity\". This notion of fairness is suitable in cases\nwhere accuracy is prioritized. Two algorithms are presented, one which assumes\nthat protected features are accessible to the model during testing, and one\nwhich assumes protected features are not accessible during testing.\n  We evaluate our tools on three different publicly available datasets. We find\nthat the tools are useful for understanding various dimensions of bias, and\nthat in practice the algorithms are effective in starkly reducing a given\nobserved bias when tested on new data.",
    "published_date": "2020-07-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04484v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04477v13",
    "title": "Good AI for the Present of Humanity Democratizing AI Governance",
    "authors": [
      "Nicholas Kluge Corrêa",
      "Nythamar de Oliveira"
    ],
    "author_ids": [],
    "abstract": "What do Cyberpunk and AI Ethics have to do with each other? Cyberpunk is a\nsub-genre of science fiction that explores the post-human relationships between\nhuman experience and technology. One similarity between AI Ethics and Cyberpunk\nliterature is that both seek to explore future social and ethical problems that\nour technological advances may bring upon society. In recent years, an\nincreasing number of ethical matters involving AI have been pointed and\ndebated, and several ethical principles and guides have been suggested as\ngovernance policies for the tech industry. However, would this be the role of\nAI Ethics? To serve as a soft and ambiguous version of the law? We would like\nto advocate in this article for a more Cyberpunk way of doing AI Ethics, with a\nmore democratic way of governance. In this study, we will seek to expose some\nof the deficits of the underlying power structures of the AI industry, and\nsuggest that AI governance be subject to public opinion, so that good AI can\nbecome good AI for all.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04477v13",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04361v1",
    "title": "Understanding the impact of the alphabetical ordering of names in user interfaces: a gender bias analysis",
    "authors": [
      "Daniel Sullivan",
      "Carlos Caminha",
      "Victor Dantas",
      "Elizabeth Furtado",
      "Vasco Furtado",
      "Virgílio Almeida"
    ],
    "author_ids": [],
    "abstract": "Listing people alphabetically on an electronic output device is a traditional\ntechnique, since alphabetical order is easily perceived by users and\nfacilitates access to information. However, this apparently harmless technique,\nespecially when the list is ordered by first name, needs to be used with\ncaution by designers and programmers. We show, via empirical data analysis,\nthat when an interface displays people's first name in alphabetical order in\nseveral pages/screens, each page/screen may have imbalances in respect to\ngender of its Top-k individuals.k represents the size of the list of names\nvisualized first, which may be the number of names that fits in a screen page\nof a certain device.The research work was carried out with the analysis of\nactual datasets of names of five different countries. Each dataset has a person\nname and the frequency of adoption of the name in the country.Our analysis\nshows that, even though all countries have exhibit imbalance problems, the\nsamples of individuals with Brazilian and Spanish first names are more prone to\ngender imbalance among their Top-k individuals. These results can be useful for\ndesigners and engineers to construct information systems that avoid gender bias\ninduction.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04361v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.04205v1",
    "title": "Analysis of Predictive Coding Models for Phonemic Representation Learning in Small Datasets",
    "authors": [
      "María Andrea Cruz Blandón",
      "Okko Räsänen"
    ],
    "author_ids": [],
    "abstract": "Neural network models using predictive coding are interesting from the\nviewpoint of computational modelling of human language acquisition, where the\nobjective is to understand how linguistic units could be learned from speech\nwithout any labels. Even though several promising predictive coding -based\nlearning algorithms have been proposed in the literature, it is currently\nunclear how well they generalise to different languages and training dataset\nsizes. In addition, despite that such models have shown to be effective\nphonemic feature learners, it is unclear whether minimisation of the predictive\nloss functions of these models also leads to optimal phoneme-like\nrepresentations. The present study investigates the behaviour of two predictive\ncoding models, Autoregressive Predictive Coding and Contrastive Predictive\nCoding, in a phoneme discrimination task (ABX task) for two languages with\ndifferent dataset sizes. Our experiments show a strong correlation between the\nautoregressive loss and the phoneme discrimination scores with the two\ndatasets. However, to our surprise, the CPC model shows rapid convergence\nalready after one pass over the training data, and, on average, its\nrepresentations outperform those of APC on both languages.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04205v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04068v1",
    "title": "Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence",
    "authors": [
      "Shakir Mohamed",
      "Marie-Therese Png",
      "William Isaac"
    ],
    "author_ids": [],
    "abstract": "This paper explores the important role of critical science, and in particular\nof post-colonial and decolonial theories, in understanding and shaping the\nongoing advances in artificial intelligence. Artificial Intelligence (AI) is\nviewed as amongst the technological advances that will reshape modern societies\nand their relations. Whilst the design and deployment of systems that\ncontinually adapt holds the promise of far-reaching positive change, they\nsimultaneously pose significant risks, especially to already vulnerable\npeoples. Values and power are central to this discussion. Decolonial theories\nuse historical hindsight to explain patterns of power that shape our\nintellectual, political, economic, and social world. By embedding a decolonial\ncritical approach within its technical practice, AI communities can develop\nforesight and tactics that can better align research and technology development\nwith established ethical principles, centring vulnerable peoples who continue\nto bear the brunt of negative impacts of innovation and scientific progress. We\nhighlight problematic applications that are instances of coloniality, and using\na decolonial lens, submit three tactics that can form a decolonial field of\nartificial intelligence: creating a critical technical practice of AI, seeking\nreverse tutelage and reverse pedagogies, and the renewal of affective and\npolitical communities. The years ahead will usher in a wave of new scientific\nbreakthroughs and technologies driven by AI research, making it incumbent upon\nAI communities to strengthen the social contract through ethical foresight and\nthe multiplicity of intellectual perspectives available to us; ultimately\nsupporting future technologies that enable greater well-being, with the goal of\nbeneficence and justice for all.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04068v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.04059v1",
    "title": "Fair Colorful k-Center Clustering",
    "authors": [
      "Xinrui Jia",
      "Kshiteej Sheth",
      "Ola Svensson"
    ],
    "author_ids": [],
    "abstract": "An instance of colorful k-center consists of points in a metric space that\nare colored red or blue, along with an integer k and a coverage requirement for\neach color. The goal is to find the smallest radius \\r{ho} such that there\nexist balls of radius \\r{ho} around k of the points that meet the coverage\nrequirements. The motivation behind this problem is twofold. First, from\nfairness considerations: each color/group should receive a similar service\nguarantee, and second, from the algorithmic challenges it poses: this problem\ncombines the difficulties of clustering along with the subset-sum problem. In\nparticular, we show that this combination results in strong integrality gap\nlower bounds for several natural linear programming relaxations. Our main\nresult is an efficient approximation algorithm that overcomes these\ndifficulties to achieve an approximation guarantee of 3, nearly matching the\ntight approximation guarantee of 2 for the classical k-center problem which\nthis problem generalizes.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "F.2.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04059v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.04028v1",
    "title": "How benign is benign overfitting?",
    "authors": [
      "Amartya Sanyal",
      "Puneet K Dokania",
      "Varun Kanade",
      "Philip H. S. Torr"
    ],
    "author_ids": [],
    "abstract": "We investigate two causes for adversarial vulnerability in deep neural\nnetworks: bad data and (poorly) trained models. When trained with SGD, deep\nneural networks essentially achieve zero training error, even in the presence\nof label noise, while also exhibiting good generalization on natural test data,\nsomething referred to as benign overfitting [2, 10]. However, these models are\nvulnerable to adversarial attacks. We identify label noise as one of the causes\nfor adversarial vulnerability, and provide theoretical and empirical evidence\nin support of this. Surprisingly, we find several instances of label noise in\ndatasets such as MNIST and CIFAR, and that robustly trained models incur\ntraining error on some of these, i.e. they don't fit the noise. However,\nremoving noisy labels alone does not suffice to achieve adversarial robustness.\nStandard training procedures bias neural networks towards learning \"simple\"\nclassification boundaries, which may be less robust than more complex ones. We\nobserve that adversarial training does produce more complex decision\nboundaries. We conjecture that in part the need for complex decision boundaries\narises from sub-optimal representation learning. By means of simple toy\nexamples, we show theoretically how the choice of representation can\ndrastically affect adversarial robustness.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.04028v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03887v2",
    "title": "Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias",
    "authors": [
      "Yunhan Zhao",
      "Shu Kong",
      "Charless Fowlkes"
    ],
    "author_ids": [],
    "abstract": "Monocular depth predictors are typically trained on large-scale training sets\nwhich are naturally biased w.r.t the distribution of camera poses. As a result,\ntrained predictors fail to make reliable depth predictions for testing examples\ncaptured under uncommon camera poses. To address this issue, we propose two\nnovel techniques that exploit the camera pose during training and prediction.\nFirst, we introduce a simple perspective-aware data augmentation that\nsynthesizes new training examples with more diverse views by perturbing the\nexisting ones in a geometrically consistent manner. Second, we propose a\nconditional model that exploits the per-image camera pose as prior knowledge by\nencoding it as a part of the input. We show that jointly applying the two\nmethods improves depth prediction on images captured under uncommon and even\nnever-before-seen camera poses. We show that our methods improve performance\nwhen applied to a range of different predictor architectures. Lastly, we show\nthat explicitly encoding the camera pose distribution improves the\ngeneralization performance of a synthetically trained depth predictor when\nevaluated on real images.",
    "published_date": "2020-07-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03887v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03825v1",
    "title": "Global exponential attitude tracking for spacecraft with gyro bias estimation",
    "authors": [
      "Eduardo Espíndola-López",
      "Yu Tang Xu"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the global exponential attitude tracking of a spacecraft\nwhen gyro measurements are corrupted by bias. Based on contraction analysis, an\nexponentially convergent nonlinear observer is designed first to estimate the\ngyro bias. Relying on this bias estimator and the quaternion logarithm\nrepresentation of the tracking error, an exponentially globally convergent\ncontroller is devised. This controller stabilizes the unique equilibrium of the\nclosed-loop system, where the tracking error is the unit quaternion. For more\nenergy-efficiency and enhancing the robustness in the presence of measurement\nnoise, a hysteretically switching variable as in [1] is incorporated in the\ncontrol loop and an unwinding-free globally exponentially convergent tracking\ncontroller is obtained. Numeric simulations were done to evaluate its\nperformance in terms of tracking errors and energy-efficiency, as well as the\nrobustness to measurement noise and time-varying bias in gyro sensors.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03825v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.03775v1",
    "title": "README: REpresentation learning by fairness-Aware Disentangling MEthod",
    "authors": [
      "Sungho Park",
      "Dohyung Kim",
      "Sunhee Hwang",
      "Hyeran Byun"
    ],
    "author_ids": [],
    "abstract": "Fair representation learning aims to encode invariant representation with\nrespect to the protected attribute, such as gender or age. In this paper, we\ndesign Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair\nrepresentation learning. This network disentangles latent space into three\nsubspaces with a decorrelation loss that encourages each subspace to contain\nindependent information: 1) target attribute information, 2) protected\nattribute information, 3) mutual attribute information. After the\nrepresentation learning, this disentangled representation is leveraged for\nfairer downstream classification by excluding the subspace with the protected\nattribute information. We demonstrate the effectiveness of our model through\nextensive experiments on CelebA and UTK Face datasets. Our method outperforms\nthe previous state-of-the-art method by large margins in terms of equal\nopportunity and equalized odds.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03775v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03626v1",
    "title": "What Gives the Answer Away? Question Answering Bias Analysis on Video QA Datasets",
    "authors": [
      "Jianing Yang",
      "Yuying Zhu",
      "Yongxin Wang",
      "Ruitao Yi",
      "Amir Zadeh",
      "Louis-Philippe Morency"
    ],
    "author_ids": [],
    "abstract": "Question answering biases in video QA datasets can mislead multimodal model\nto overfit to QA artifacts and jeopardize the model's ability to generalize.\nUnderstanding how strong these QA biases are and where they come from helps the\ncommunity measure progress more accurately and provide researchers insights to\ndebug their models. In this paper, we analyze QA biases in popular video\nquestion answering datasets and discover pretrained language models can answer\n37-48% questions correctly without using any multimodal context information,\nfar exceeding the 20% random guess baseline for 5-choose-1 multiple-choice\nquestions. Our ablation study shows biases can come from annotators and type of\nquestions. Specifically, annotators that have been seen during training are\nbetter predicted by the model and reasoning, abstract questions incur more\nbiases than factual, direct questions. We also show empirically that using\nannotator-non-overlapping train-test splits can reduce QA biases for video QA\ndatasets.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03626v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03578v2",
    "title": "A Vision-based Social Distancing and Critical Density Detection System for COVID-19",
    "authors": [
      "Dongfang Yang",
      "Ekim Yurtsever",
      "Vishnu Renganathan",
      "Keith A. Redmill",
      "Ümit Özgüner"
    ],
    "author_ids": [],
    "abstract": "Social distancing has been proven as an effective measure against the spread\nof the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are\nnot used to tracking the required 6-feet (2-meters) distance between themselves\nand their surroundings. An active surveillance system capable of detecting\ndistances between individuals and warning them can slow down the spread of the\ndeadly disease. Furthermore, measuring social density in a region of interest\n(ROI) and modulating inflow can decrease social distancing violation occurrence\nchance.\n  On the other hand, recording data and labeling individuals who do not follow\nthe measures will breach individuals' rights in free-societies. Here we propose\nan Artificial Intelligence (AI) based real-time social distancing detection and\nwarning system considering four important ethical factors: (1) the system\nshould never record/cache data, (2) the warnings should not target the\nindividuals, (3) no human supervisor should be in the detection/warning loop,\nand (4) the code should be open-source and accessible to the public. Against\nthis backdrop, we propose using a monocular camera and deep learning-based\nreal-time object detectors to measure social distancing. If a violation is\ndetected, a non-intrusive audio-visual warning signal is emitted without\ntargeting the individual who breached the social distancing measure. Also, if\nthe social density is over a critical value, the system sends a control signal\nto modulate inflow into the ROI. We tested the proposed method across\nreal-world datasets to measure its generality and performance. The proposed\nmethod is ready for deployment, and our code is open-sourced.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03578v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03485v2",
    "title": "A discrete Weber inequality on three-dimensional hybrid spaces with application to the HHO approximation of magnetostatics",
    "authors": [
      "Florent Chave",
      "Daniele A. Di Pietro",
      "Simon Lemaire"
    ],
    "author_ids": [],
    "abstract": "We prove a discrete version of the first Weber inequality on\nthree-dimensional hybrid spaces spanned by vectors of polynomials attached to\nthe elements and faces of a polyhedral mesh. We then introduce two Hybrid\nHigh-Order methods for the approximation of the magnetostatics model, in both\nits (first-order) field and (second-order) vector potential formulations. These\nmethods are applicable on general polyhedral meshes, and allow for arbitrary\norders of approximation. Leveraging the previously established discrete Weber\ninequality, we perform a comprehensive analysis of the two methods. We finally\nvalidate them on a set of test-cases.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N08, 65N12, 65N30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03485v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.03380v4",
    "title": "Re-thinking Co-Salient Object Detection",
    "authors": [
      "Deng-Ping Fan",
      "Tengpeng Li",
      "Zheng Lin",
      "Ge-Peng Ji",
      "Dingwen Zhang",
      "Ming-Ming Cheng",
      "Huazhu Fu",
      "Jianbing Shen"
    ],
    "author_ids": [],
    "abstract": "In this paper, we conduct a comprehensive study on the co-salient object\ndetection (CoSOD) problem for images. CoSOD is an emerging and rapidly growing\nextension of salient object detection (SOD), which aims to detect the\nco-occurring salient objects in a group of images. However, existing CoSOD\ndatasets often have a serious data bias, assuming that each group of images\ncontains salient objects of similar visual appearances. This bias can lead to\nthe ideal settings and effectiveness of models trained on existing datasets,\nbeing impaired in real-life situations, where similarities are usually semantic\nor conceptual. To tackle this issue, we first introduce a new benchmark, called\nCoSOD3k in the wild, which requires a large amount of semantic context, making\nit more challenging than existing CoSOD datasets. Our CoSOD3k consists of 3,316\nhigh-quality, elaborately selected images divided into 160 groups with\nhierarchical annotations. The images span a wide range of categories, shapes,\nobject sizes, and backgrounds. Second, we integrate the existing SOD techniques\nto build a unified, trainable CoSOD framework, which is long overdue in this\nfield. Specifically, we propose a novel CoEG-Net that augments our prior model\nEGNet with a co-attention projection strategy to enable fast common information\nlearning. CoEG-Net fully leverages previous large-scale SOD datasets and\nsignificantly improves the model scalability and stability. Third, we\ncomprehensively summarize 40 cutting-edge algorithms, benchmarking 18 of them\nover three challenging CoSOD datasets (iCoSeg, CoSal2015, and our CoSOD3k), and\nreporting more detailed (i.e., group-level) performance analysis. Finally, we\ndiscuss the challenges and future works of CoSOD. We hope that our study will\ngive a strong boost to growth in the CoSOD community. The benchmark toolbox and\nresults are available on our project page at http://dpfan.net/CoSOD3K/.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03380v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03291v1",
    "title": "A Classification of Weak Asynchronous Models of Distributed Computing",
    "authors": [
      "Javier Esparza",
      "Fabian Reiter"
    ],
    "author_ids": [],
    "abstract": "We conduct a systematic study of asynchronous models of distributed computing\nconsisting of identical finite-state devices that cooperate in a network to\ndecide if the network satisfies a given graph-theoretical property. Models\ndiscussed in the literature differ in the detection capabilities of the agents\nresiding at the nodes of the network (detecting the set of states of their\nneighbors, or counting the number of neighbors in each state), the notion of\nacceptance (acceptance by halting in a particular configuration, or by stable\nconsensus), the notion of step (synchronous move, interleaving, or arbitrary\ntiming), and the fairness assumptions (non-starving, or stochastic-like). We\nstudy the expressive power of the combinations of these features, and show that\nthe initially twenty possible combinations fit into seven equivalence classes.\nThe classification is the consequence of several equi-expressivity results with\na clear interpretation. In particular, we show that acceptance by halting\nconfiguration only has non-trivial expressive power if it is combined with\ncounting, and that synchronous and interleaving models have the same power as\nthose in which an arbitrary set of nodes can move at the same time. We also\nidentify simple graph properties that distinguish the expressive power of the\nseven classes.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.FL",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03291v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.03244v1",
    "title": "Robust Learning with Frequency Domain Regularization",
    "authors": [
      "Weiyu Guo",
      "Yidong Ouyang"
    ],
    "author_ids": [],
    "abstract": "Convolution neural networks have achieved remarkable performance in many\ntasks of computing vision. However, CNN tends to bias to low frequency\ncomponents. They prioritize capturing low frequency patterns which lead them\nfail when suffering from application scenario transformation. While adversarial\nexample implies the model is very sensitive to high frequency perturbations. In\nthis paper, we introduce a new regularization method by constraining the\nfrequency spectra of the filter of the model. Different from band-limit\ntraining, our method considers the valid frequency range probably entangles in\ndifferent layers rather than continuous and trains the valid frequency range\nend-to-end by backpropagation. We demonstrate the effectiveness of our\nregularization by (1) defensing to adversarial perturbations; (2) reducing the\ngeneralization gap in different architecture; (3) improving the generalization\nability in transfer learning scenario without fine-tune.",
    "published_date": "2020-07-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03244v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03106v2",
    "title": "An Evaluation of Two Commercial Deep Learning-Based Information Retrieval Systems for COVID-19 Literature",
    "authors": [
      "Sarvesh Soni",
      "Kirk Roberts"
    ],
    "author_ids": [],
    "abstract": "The COVID-19 pandemic has resulted in a tremendous need for access to the\nlatest scientific information, primarily through the use of text mining and\nsearch tools. This has led to both corpora for biomedical articles related to\nCOVID-19 (such as the CORD-19 corpus (Wang et al., 2020)) as well as search\nengines to query such data. While most research in search engines is performed\nin the academic field of information retrieval (IR), most academic search\nengines$\\unicode{x2013}$though rigorously evaluated$\\unicode{x2013}$are\nsparsely utilized, while major commercial web search engines (e.g., Google,\nBing) dominate. This relates to COVID-19 because it can be expected that\ncommercial search engines deployed for the pandemic will gain much higher\ntraction than those produced in academic labs, and thus leads to questions\nabout the empirical performance of these search tools. This paper seeks to\nempirically evaluate two such commercial search engines for COVID-19, produced\nby Google and Amazon, in comparison to the more academic prototypes evaluated\nin the context of the TREC-COVID track (Roberts et al., 2020). We performed\nseveral steps to reduce bias in the available manual judgments in order to\nensure a fair comparison of the two systems with those submitted to TREC-COVID.\nWe find that the top-performing system from TREC-COVID on bpref metric\nperformed the best among the different systems evaluated in this study on all\nthe metrics. This has implications for developing biomedical retrieval systems\nfor future health crises as well as trust in popular health search engines.",
    "published_date": "2020-07-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03106v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03676v1",
    "title": "Information-Based Model Discrimination for Digital Twin Behavioral Matching",
    "authors": [
      "Jairo Viola",
      "YangQuan Chen",
      "Jing Wang"
    ],
    "author_ids": [],
    "abstract": "Digital Twin is a breaking technology that allows creating virtual\nrepresentations of complex physical systems based on updated information of the\nsystem and its physical laws. However, making the Digital Twin behavior\nmatching with the real system can be challenging due to the number of unknown\nparameters in each twin. Its search can be done using optimization-based\ntechniques, producing a family of models based on different system datasets,\nso, a discrimination criterion is required to determine the best Digital Twin\nmodel. This paper presents an information theory-based discrimination criterion\nto determine the best Digital Twin model resulting from a behavioral matching\nprocess. The information gain of a model is employed as a discrimination\ncriterion. Box-Jenkins models are used to define the family of models for each\nbehavioral matching result. The proposed method is compared with other\ninformation-based metrics as well as the $\\nu$gap metric. As a study case, the\ndiscrimination method is applied to the Digital Twin for a real-time vision\nfeedback infrared temperature uniformity control system. Obtained results show\nthat information-based methodologies are useful for selecting an accurate\nDigital Twin model representing the system among a family of plants",
    "published_date": "2020-07-06T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03676v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.02893v2",
    "title": "Making Fair ML Software using Trustworthy Explanation",
    "authors": [
      "Joymallya Chakraborty",
      "Kewen Peng",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "Machine learning software is being used in many applications (finance,\nhiring, admissions, criminal justice) having a huge social impact. But\nsometimes the behavior of this software is biased and it shows discrimination\nbased on some sensitive attributes such as sex, race, etc. Prior works\nconcentrated on finding and mitigating bias in ML models. A recent trend is\nusing instance-based model-agnostic explanation methods such as LIME to find\nout bias in the model prediction. Our work concentrates on finding shortcomings\nof current bias measures and explanation methods. We show how our proposed\nmethod based on K nearest neighbors can overcome those shortcomings and find\nthe underlying bias of black-box models. Our results are more trustworthy and\nhelpful for the practitioners. Finally, We describe our future framework\ncombining explanation and planning to build fair software.",
    "published_date": "2020-07-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02893v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.02890v1",
    "title": "Fairness in machine learning: against false positive rate equality as a measure of fairness",
    "authors": [
      "Robert Long"
    ],
    "author_ids": [],
    "abstract": "As machine learning informs increasingly consequential decisions, different\nmetrics have been proposed for measuring algorithmic bias or unfairness. Two\npopular fairness measures are calibration and equality of false positive rate.\nEach measure seems intuitively important, but notably, it is usually impossible\nto satisfy both measures. For this reason, a large literature in machine\nlearning speaks of a fairness tradeoff between these two measures. This framing\nassumes that both measures are, in fact, capturing something important. To\ndate, philosophers have not examined this crucial assumption, and examined to\nwhat extent each measure actually tracks a normatively important property. This\nmakes this inevitable statistical conflict, between calibration and false\npositive rate equality, an important topic for ethics. In this paper, I give an\nethical framework for thinking about these measures and argue that, contrary to\ninitial appearances, false positive rate equality does not track anything about\nfairness, and thus sets an incoherent standard for evaluating the fairness of\nalgorithms.",
    "published_date": "2020-07-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02890v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.02561v2",
    "title": "Learning from Failure: Training Debiased Classifier from Biased Classifier",
    "authors": [
      "Junhyun Nam",
      "Hyuntak Cha",
      "Sungsoo Ahn",
      "Jaeho Lee",
      "Jinwoo Shin"
    ],
    "author_ids": [],
    "abstract": "Neural networks often learn to make predictions that overly rely on spurious\ncorrelation existing in the dataset, which causes the model to be biased. While\nprevious work tackles this issue by using explicit labeling on the spuriously\ncorrelated attributes or presuming a particular bias type, we instead utilize a\ncheaper, yet generic form of human knowledge, which can be widely applicable to\nvarious types of bias. We first observe that neural networks learn to rely on\nthe spurious correlation only when it is \"easier\" to learn than the desired\nknowledge, and such reliance is most prominent during the early phase of\ntraining. Based on the observations, we propose a failure-based debiasing\nscheme by training a pair of neural networks simultaneously. Our main idea is\ntwofold; (a) we intentionally train the first network to be biased by\nrepeatedly amplifying its \"prejudice\", and (b) we debias the training of the\nsecond network by focusing on samples that go against the prejudice of the\nbiased network in (a). Extensive experiments demonstrate that our method\nsignificantly improves the training of the network against various types of\nbiases in both synthetic and real-world datasets. Surprisingly, our framework\neven occasionally outperforms the debiasing methods requiring explicit\nsupervision of the spuriously correlated attributes.",
    "published_date": "2020-07-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02561v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.02520v3",
    "title": "Explaining Fast Improvement in Online Imitation Learning",
    "authors": [
      "Xinyan Yan",
      "Byron Boots",
      "Ching-An Cheng"
    ],
    "author_ids": [],
    "abstract": "Online imitation learning (IL) is an algorithmic framework that leverages\ninteractions with expert policies for efficient policy optimization. Here\npolicies are optimized by performing online learning on a sequence of loss\nfunctions that encourage the learner to mimic expert actions, and if the online\nlearning has no regret, the agent can provably learn an expert-like policy.\nOnline IL has demonstrated empirical successes in many applications and\ninterestingly, its policy improvement speed observed in practice is usually\nmuch faster than existing theory suggests. In this work, we provide an\nexplanation of this phenomenon. Let $\\xi$ denote the policy class bias and\nassume the online IL loss functions are convex, smooth, and non-negative. We\nprove that, after $N$ rounds of online IL with stochastic feedback, the policy\nimproves in $\\tilde{O}(1/N + \\sqrt{\\xi/N})$ in both expectation and high\nprobability. In other words, we show that adopting a sufficiently expressive\npolicy class in online IL has two benefits: both the policy improvement speed\nincreases and the performance bias decreases.",
    "published_date": "2020-07-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02520v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.02250v2",
    "title": "Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback Loops",
    "authors": [
      "Shengyang Chen",
      "Chih-Yung Wen",
      "Yajing Zou",
      "Wu Chen"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a novel stereo visual inertial pose estimation\nmethod. Compared to the widely used filter-based or optimization-based\napproaches, the pose estimation process is modeled as a control system.\nDesigned feedback or feedforward loops are introduced to achieve the stable\ncontrol of the system, which include a gradient decreased feedback loop, a\nroll-pitch feed forward loop and a bias estimation feedback loop. This system,\nnamed FLVIS (Feedforward-feedback Loop-based Visual Inertial System), is\nevaluated on the popular EuRoc MAV dataset. FLVIS achieves high accuracy and\nrobustness with respect to other state-of-the-art visual SLAM approaches. The\nsystem has also been implemented and tested on a UAV platform. The source code\nof this research is public to the research community.",
    "published_date": "2020-07-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02250v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.02934v1",
    "title": "The Effects of Taxes on Wealth Inequality in Artificial Chemistry Models of Economic Activity",
    "authors": [
      "Wolfgang Banzhaf"
    ],
    "author_ids": [],
    "abstract": "We consider a number of Artificial Chemistry models for economic activity and\nwhat consequences they have for the formation of economic inequality. We are\nparticularly interested in what tax measures are effective in dampening\neconomic inequality. By starting from well-known kinetic exchange models, we\nexamine different scenarios for reducing the tendency of economic activity\nmodels to form unequal wealth distribution in equilibrium.",
    "published_date": "2020-07-03T00:00:00",
    "year": 2020,
    "categories": [
      "q-fin.GN",
      "cs.MA",
      "econ.GN",
      "physics.soc-ph",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02934v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.01571v1",
    "title": "Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN",
    "authors": [
      "Zhenwei He",
      "Lei Zhang"
    ],
    "author_ids": [],
    "abstract": "Conventional object detection models inevitably encounter a performance drop\nas the domain disparity exists. Unsupervised domain adaptive object detection\nis proposed recently to reduce the disparity between domains, where the source\ndomain is label-rich while the target domain is label-agnostic. The existing\nmodels follow a parameter shared siamese structure for adversarial domain\nalignment, which, however, easily leads to the collapse and out-of-control risk\nof the source domain and brings negative impact to feature adaption. The main\nreason is that the labeling unfairness (asymmetry) between source and target\nmakes the parameter sharing mechanism unable to adapt. Therefore, in order to\navoid the source domain collapse risk caused by parameter sharing, we propose\nan asymmetric tri-way Faster-RCNN (ATF) for domain adaptive object detection.\nOur ATF model has two distinct merits: 1) A ancillary net supervised by source\nlabel is deployed to learn ancillary target features and simultaneously\npreserve the discrimination of source domain, which enhances the structural\ndiscrimination (object classification vs. bounding box regression) of domain\nalignment. 2) The asymmetric structure consisting of a chief net and an\nindependent ancillary net essentially overcomes the parameter sharing aroused\nsource risk collapse. The adaption safety of the proposed ATF detector is\nguaranteed. Extensive experiments on a number of datasets, including\nCityscapes, Foggy-cityscapes, KITTI, Sim10k, Pascal VOC, Clipart and\nWatercolor, demonstrate the SOTA performance of our method.",
    "published_date": "2020-07-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.01571v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.01447v1",
    "title": "Improved Preterm Prediction Based on Optimized Synthetic Sampling of EHG Signal",
    "authors": [
      "Jinshan Xu",
      "Zhenqin Chen",
      "Yanpei Lu",
      "Xi Yang",
      "Alain Pumir"
    ],
    "author_ids": [],
    "abstract": "Preterm labor is the leading cause of neonatal morbidity and mortality and\nhas attracted research efforts from many scientific areas. The\ninter-relationship between uterine contraction and the underlying electrical\nactivities makes uterine electrohysterogram (EHG) a promising direction for\npreterm detection and prediction. Due the scarcity of EHG signals, especially\nthose of preterm patients, synthetic algorithms are applied to create\nartificial samples of preterm type in order to remove prediction bias towards\nterm, at the expense of a reduction of the feature effectiveness in\nmachine-learning based automatic preterm detecting. To address such problem, we\nquantify the effect of synthetic samples (balance coefficient) on features'\neffectiveness, and form a general performance metric by utilizing multiple\nfeature scores with relevant weights that describe their contributions to class\nseparation. Combined with the activation/inactivation functions that\ncharacterizes the effect of the abundance of training samples in term and\npreterm prediction precision, we obtain an optimal sample balance coefficient\nthat compromise the effect of synthetic samples in removing bias towards the\nmajority and the side-effect of reducing features' importance. Substantial\nimprovement in prediction precision has been achieved through a set of\nnumerical tests on public available TPEHG database, and it verifies the\neffectiveness of the proposed method.",
    "published_date": "2020-07-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.01447v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.01242v1",
    "title": "Evolving Methods for Evaluating and Disseminating Computing Research",
    "authors": [
      "Benjamin Zorn",
      "Tom Conte",
      "Keith Marzullo",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "Social and technical trends have significantly changed methods for evaluating\nand disseminating computing research. Traditional venues for reviewing and\npublishing, such as conferences and journals, worked effectively in the past.\nRecently, trends have created new opportunities but also put new pressures on\nthe process of review and dissemination. For example, many conferences have\nseen large increases in the number of submissions. Likewise, dissemination of\nresearch ideas has become dramatically through publication venues such as\narXiv.org and social media networks. While these trends predate COVID-19, the\npandemic could accelerate longer term changes. Based on interviews with leading\nacademics in computing research, our findings include: (1) Trends impacting\ncomputing research are largely positive and have increased the participation,\nscope, accessibility, and speed of the research process. (2) Challenges remain\nin securing the integrity of the process, including addressing ways to scale\nthe review process, avoiding attempts to misinform or confuse the dissemination\nof results, and ensuring fairness and broad participation in the process\nitself. Based on these findings, we recommend: (1) Regularly polling members of\nthe computing research community, including program and general conference\nchairs, journal editors, authors, reviewers, etc., to identify specific\nchallenges they face to better understand these issues. (2) An influential\nbody, such as the Computing Research Association regularly issues a \"State of\nthe Computing Research Enterprise\" report to update the community on trends,\nboth positive and negative, impacting the computing research enterprise. (3) A\ndeeper investigation, specifically to better understand the influence that\nsocial media and preprint archives have on computing research, is conducted.",
    "published_date": "2020-07-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.01242v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.01162v2",
    "title": "Tilted Empirical Risk Minimization",
    "authors": [
      "Tian Li",
      "Ahmad Beirami",
      "Maziar Sanjabi",
      "Virginia Smith"
    ],
    "author_ids": [],
    "abstract": "Empirical risk minimization (ERM) is typically designed to perform well on\nthe average loss, which can result in estimators that are sensitive to\noutliers, generalize poorly, or treat subgroups unfairly. While many methods\naim to address these problems individually, in this work, we explore them\nthrough a unified framework -- tilted empirical risk minimization (TERM). In\nparticular, we show that it is possible to flexibly tune the impact of\nindividual losses through a straightforward extension to ERM using a\nhyperparameter called the tilt. We provide several interpretations of the\nresulting framework: We show that TERM can increase or decrease the influence\nof outliers, respectively, to enable fairness or robustness; has\nvariance-reduction properties that can benefit generalization; and can be\nviewed as a smooth approximation to a superquantile method. We develop batch\nand stochastic first-order optimization methods for solving TERM, and show that\nthe problem can be efficiently solved relative to common alternatives. Finally,\nwe demonstrate that TERM can be used for a multitude of applications, such as\nenforcing fairness between subgroups, mitigating the effect of outliers, and\nhandling class imbalance. TERM is not only competitive with existing solutions\ntailored to these individual problems, but can also enable entirely new\napplications, such as simultaneously addressing outliers and promoting\nfairness.",
    "published_date": "2020-07-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.01162v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.05461v1",
    "title": "Grading video interviews with fairness considerations",
    "authors": [
      "Abhishek Singhania",
      "Abhishek Unnam",
      "Varun Aggarwal"
    ],
    "author_ids": [],
    "abstract": "There has been considerable interest in predicting human emotions and traits\nusing facial images and videos. Lately, such work has come under criticism for\npoor labeling practices, inconclusive prediction results and fairness\nconsiderations. We present a careful methodology to automatically derive social\nskills of candidates based on their video response to interview questions. We,\nfor the first time, include video data from multiple countries encompassing\nmultiple ethnicities. Also, the videos were rated by individuals from multiple\nracial backgrounds, following several best practices, to achieve a consensus\nand unbiased measure of social skills. We develop two machine-learning models\nto predict social skills. The first model employs expert-guidance to use\nplausibly causal features. The second uses deep learning and depends solely on\nthe empirical correlations present in the data. We compare errors of both these\nmodels, study the specificity of the models and make recommendations. We\nfurther analyze fairness by studying the errors of models by race and gender.\nWe verify the usefulness of our models by determining how well they predict\ninterview outcomes for candidates. Overall, the study provides strong support\nfor using artificial intelligence for video interview scoring, while taking\ncare of fairness and ethical considerations.",
    "published_date": "2020-07-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.05461v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00912v2",
    "title": "Maximizing The Distance To A \"Far Enough\" Point Over The Intersection Of Hyper-Disks",
    "authors": [
      "Marius-Simion Costandin",
      "Bogdan Gavrea",
      "Beniamin Costandin"
    ],
    "author_ids": [],
    "abstract": "We present a novel feasibility criteria for the finite intersection of convex\nsets given by inequalities. This criteria allows us to easily assert the\nfeasibility by analyzing the unconstrained minimum of a speci?fic convex\nfunction, that we form with the given sets. Next an algorithm is presented\nwhich extends the idea to a particular non-convex case: assert the inclusion of\nthe fi?nite intersection of a set of hyper-disks with equal radii in another\nhyper-disk with a different radius.",
    "published_date": "2020-07-02T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00912v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2007.00792v2",
    "title": "Age-Oriented Face Synthesis with Conditional Discriminator Pool and Adversarial Triplet Loss",
    "authors": [
      "Haoyi Wang",
      "Victor Sanchez",
      "Chang-Tsun Li"
    ],
    "author_ids": [],
    "abstract": "The vanilla Generative Adversarial Networks (GAN) are commonly used to\ngenerate realistic images depicting aged and rejuvenated faces. However, the\nperformance of such vanilla GANs in the age-oriented face synthesis task is\noften compromised by the mode collapse issue, which may result in the\ngeneration of faces with minimal variations and a poor synthesis accuracy. In\naddition, recent age-oriented face synthesis methods use the L1 or L2\nconstraint to preserve the identity information on synthesized faces, which\nimplicitly limits the identity permanence capabilities when these constraints\nare associated with a trivial weighting factor. In this paper, we propose a\nmethod for the age-oriented face synthesis task that achieves a high synthesis\naccuracy with strong identity permanence capabilities. Specifically, to achieve\na high synthesis accuracy, our method tackles the mode collapse issue with a\nnovel Conditional Discriminator Pool (CDP), which consists of multiple\ndiscriminators, each targeting one particular age category. To achieve strong\nidentity permanence capabilities, our method uses a novel Adversarial Triplet\nloss. This loss, which is based on the Triplet loss, adds a ranking operation\nto further pull the positive embedding towards the anchor embedding resulting\nin significantly reduced intra-class variances in the feature space. Through\nextensive experiments, we show that our proposed method outperforms\nstate-of-the-art methods in terms of synthesis accuracy and identity permanence\ncapabilities, qualitatively and quantitatively.",
    "published_date": "2020-07-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00792v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00700v1",
    "title": "Response by the Montreal AI Ethics Institute to the Santa Clara Principles on Transparency and Accountability in Online Content Moderation",
    "authors": [
      "Marianna Bergamaschi Ganapini",
      "Camylle Lanteigne",
      "Abhishek Gupta"
    ],
    "author_ids": [],
    "abstract": "In April 2020, the Electronic Frontier Foundation (EFF) publicly called for\ncomments on expanding and improving the Santa Clara Principles on Transparency\nand Accountability (SCP), originally published in May 2018. The Montreal AI\nEthics Institute (MAIEI) responded to this call by drafting a set of\nrecommendations based on insights and analysis by the MAIEI staff and\nsupplemented by workshop contributions from the AI Ethics community convened\nduring two online public consultation workshops.\n  In its submission, MAIEI provides 12 overarching recommendations for the SCP,\nthese include: 1) ensure there is more diversity in the content moderation\nprocess; 2) increase transparency into how platforms guide content-ranking; 3)\ndisclose anonymized data on the training and/or cultural background of the\ncontent moderators for a platform; 4) tailor content moderation tools for\nspecific issues; 5) draft specific guidelines for messaging applications with\nregards to data protection in content moderation; 6) take into account cultural\ndifferences relevant to what constitutes acceptable behavior online; 7) ensure\nplatforms are transparent in regards to political advertising; 8) ensure\ngreater transparency into the user-generated flagging/reporting systems\ndeployed by a platform; 9) clarify if user content is flagged or reported\nthrough an automated system; 10) provide more data on the types of content\nremoved from platforms; 11) provide clear guidelines on the appeal process, as\nwell as data on prior appeals; 12) create a system for periodically revisiting\nthe SCP so it reflects various technological advancements, modifications in law\nand policy, as well as changing trends or movements in content moderation.",
    "published_date": "2020-07-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00700v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00514v4",
    "title": "Regularized Online Allocation Problems: Fairness and Beyond",
    "authors": [
      "Santiago Balseiro",
      "Haihao Lu",
      "Vahab Mirrokni"
    ],
    "author_ids": [],
    "abstract": "Online allocation problems with resource constraints have a rich history in\noperations research. In this paper, we introduce the \\emph{regularized online\nallocation problem}, a variant that includes a non-linear regularizer acting on\nthe total resource consumption. In this problem, requests repeatedly arrive\nover time and, for each request, a decision maker needs to take an action that\ngenerates a reward and consumes resources. The objective is to simultaneously\nmaximize additively separable rewards and the value of a non-separable\nregularizer subject to the resource constraints. Our primary motivation is\nallowing decision makers to trade off separable objectives such as the economic\nefficiency of an allocation with ancillary, non-separable objectives such as\nthe fairness or equity of an allocation. We design an algorithm that is simple,\nfast, and attains good performance with both stochastic i.i.d.~and adversarial\ninputs. In particular, our algorithm is asymptotically optimal under stochastic\ni.i.d. input models and attains a fixed competitive ratio that depends on the\nregularizer when the input is adversarial. Furthermore, the algorithm and\nanalysis do not require convexity or concavity of the reward function and the\nconsumption function, which allows more model flexibility. Numerical\nexperiments confirm the effectiveness of the proposed algorithm and of\nregularization in an internet advertising application.",
    "published_date": "2020-07-01T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00514v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00218v1",
    "title": "Fairness constraints can help exact inference in structured prediction",
    "authors": [
      "Kevin Bello",
      "Jean Honorio"
    ],
    "author_ids": [],
    "abstract": "Many inference problems in structured prediction can be modeled as maximizing\na score function on a space of labels, where graphs are a natural\nrepresentation to decompose the total score into a sum of unary (nodes) and\npairwise (edges) scores. Given a generative model with an undirected connected\ngraph $G$ and true vector of binary labels, it has been previously shown that\nwhen $G$ has good expansion properties, such as complete graphs or $d$-regular\nexpanders, one can exactly recover the true labels (with high probability and\nin polynomial time) from a single noisy observation of each edge and node. We\nanalyze the previously studied generative model by Globerson et al. (2015)\nunder a notion of statistical parity. That is, given a fair binary node\nlabeling, we ask the question whether it is possible to recover the fair\nassignment, with high probability and in polynomial time, from single edge and\nnode observations. We find that, in contrast to the known trade-offs between\nfairness and model performance, the addition of the fairness constraint\nimproves the probability of exact recovery. We effectively explain this\nphenomenon and empirically show how graphs with poor expansion properties, such\nas grids, are now capable to achieve exact recovery with high probability.\nFinally, as a byproduct of our analysis, we provide a tighter\nminimum-eigenvalue bound than that of Weyl's inequality.",
    "published_date": "2020-07-01T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00218v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.03437v1",
    "title": "Group Equivariant Deep Reinforcement Learning",
    "authors": [
      "Arnab Kumar Mondal",
      "Pratheeksha Nair",
      "Kaleem Siddiqi"
    ],
    "author_ids": [],
    "abstract": "In Reinforcement Learning (RL), Convolutional Neural Networks(CNNs) have been\nsuccessfully applied as function approximators in Deep Q-Learning algorithms,\nwhich seek to learn action-value functions and policies in various\nenvironments. However, to date, there has been little work on the learning of\nsymmetry-transformation equivariant representations of the input environment\nstate. In this paper, we propose the use of Equivariant CNNs to train RL agents\nand study their inductive bias for transformation equivariant Q-value\napproximation. We demonstrate that equivariant architectures can dramatically\nenhance the performance and sample efficiency of RL agents in a highly\nsymmetric environment while requiring fewer parameters. Additionally, we show\nthat they are robust to changes in the environment caused by affine\ntransformations.",
    "published_date": "2020-07-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.03437v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00128v5",
    "title": "Approximation Theory of Tree Tensor Networks: Tensorized Univariate Functions -- Part II",
    "authors": [
      "Mazen Ali",
      "Anthony Nouy"
    ],
    "author_ids": [],
    "abstract": "We study the approximation by tensor networks (TNs) of functions from\nclassical smoothness classes. The considered approximation tool combines a\ntensorization of functions in $L^p([0,1))$, which allows to identify a\nunivariate function with a multivariate function (or tensor), and the use of\ntree tensor networks (the tensor train format) for exploiting low-rank\nstructures of multivariate functions. The resulting tool can be interpreted as\na feed-forward neural network, with first layers implementing the\ntensorization, interpreted as a particular featuring step, followed by a\nsum-product network with sparse architecture. In part I of this work, we\npresented several approximation classes associated with different measures of\ncomplexity of tensor networks and studied their properties. In this work (part\nII), we show how classical approximation tools, such as polynomials or splines\n(with fixed or free knots), can be encoded as a tensor network with controlled\ncomplexity. We use this to derive direct (Jackson) inequalities for the\napproximation spaces of tensor networks. This is then utilized to show that\nBesov spaces are continuously embedded into these approximation spaces. In\nother words, we show that arbitrary Besov functions can be approximated with\noptimal or near to optimal rate. We also show that an arbitrary function in the\napproximation class possesses no Besov smoothness, unless one limits the depth\nof the tensor network.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "math.FA",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "41A65, 41A15, 41A10 (primary), 68T05, 42C40, 65D99 (secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00128v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00088v1",
    "title": "Evaluation of Fairness Trade-offs in Predicting Student Success",
    "authors": [
      "Hansol Lee",
      "René F. Kizilcec"
    ],
    "author_ids": [],
    "abstract": "Predictive models for identifying at-risk students early can help teaching\nstaff direct resources to better support them, but there is a growing concern\nabout the fairness of algorithmic systems in education. Predictive models may\ninadvertently introduce bias in who receives support and thereby exacerbate\nexisting inequities. We examine this issue by building a predictive model of\nstudent success based on university administrative records. We find that the\nmodel exhibits gender and racial bias in two out of three fairness measures\nconsidered. We then apply post-hoc adjustments to improve model fairness to\nhighlight trade-offs between the three fairness measures.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00088v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.00049v2",
    "title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings",
    "authors": [
      "Sunipa Dev",
      "Tao Li",
      "Jeff M Phillips",
      "Vivek Srikumar"
    ],
    "author_ids": [],
    "abstract": "Language representations are known to carry stereotypical biases and, as a\nresult, lead to biased predictions in downstream tasks. While existing methods\nare effective at mitigating biases by linear projection, such methods are too\naggressive: they not only remove bias, but also erase valuable information from\nword embeddings. We develop new measures for evaluating specific information\nretention that demonstrate the tradeoff between bias removal and information\nretention. To address this challenge, we propose OSCaR (Orthogonal Subspace\nCorrection and Rectification), a bias-mitigating method that focuses on\ndisentangling biased associations between concepts instead of removing concepts\nwholesale. Our experiments on gender biases show that OSCaR is a well-balanced\napproach that ensures that semantic information is retained in the embeddings\nand bias is also effectively mitigated.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.00049v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16950v1",
    "title": "Bounded Rationality in Las Vegas: Probabilistic Finite Automata PlayMulti-Armed Bandits",
    "authors": [
      "Xinming Liu",
      "Joseph Y. Halpern"
    ],
    "author_ids": [],
    "abstract": "While traditional economics assumes that humans are fully rational agents who\nalways maximize their expected utility, in practice, we constantly observe\napparently irrational behavior. One explanation is that people have limited\ncomputational power, so that they are, quite rationally, making the best\ndecisions they can, given their computational limitations. To test this\nhypothesis, we consider the multi-armed bandit (MAB) problem. We examine a\nsimple strategy for playing an MAB that can be implemented easily by a\nprobabilistic finite automaton (PFA). Roughly speaking, the PFA sets certain\nexpectations, and plays an arm as long as it meets them. If the PFA has\nsufficiently many states, it performs near-optimally. Its performance degrades\ngracefully as the number of states decreases. Moreover, the PFA acts in a\n\"human-like\" way, exhibiting a number of standard human biases, like an\noptimism bias and a negativity bias.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16950v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16745v5",
    "title": "Machine learning fairness notions: Bridging the gap with real-world applications",
    "authors": [
      "Karima Makhlouf",
      "Sami Zhioua",
      "Catuscia Palamidessi"
    ],
    "author_ids": [],
    "abstract": "Fairness emerged as an important requirement to guarantee that Machine\nLearning (ML) predictive systems do not discriminate against specific\nindividuals or entire sub-populations, in particular, minorities. Given the\ninherent subjectivity of viewing the concept of fairness, several notions of\nfairness have been introduced in the literature. This paper is a survey that\nillustrates the subtleties between fairness notions through a large number of\nexamples and scenarios. In addition, unlike other surveys in the literature, it\naddresses the question of: which notion of fairness is most suited to a given\nreal-world scenario and why? Our attempt to answer this question consists in\n(1) identifying the set of fairness-related characteristics of the real-world\nscenario at hand, (2) analyzing the behavior of each fairness notion, and then\n(3) fitting these two elements to recommend the most suitable fairness notion\nin every specific setup. The results are summarized in a decision diagram that\ncan be used by practitioners and policymakers to navigate the relatively large\ncatalog of ML.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16745v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16744v1",
    "title": "Optimal Rates of Distributed Regression with Imperfect Kernels",
    "authors": [
      "Hongwei Sun",
      "Qiang Wu"
    ],
    "author_ids": [],
    "abstract": "Distributed machine learning systems have been receiving increasing\nattentions for their efficiency to process large scale data. Many distributed\nframeworks have been proposed for different machine learning tasks. In this\npaper, we study the distributed kernel regression via the divide and conquer\napproach. This approach has been proved asymptotically minimax optimal if the\nkernel is perfectly selected so that the true regression function lies in the\nassociated reproducing kernel Hilbert space. However, this is usually, if not\nalways, impractical because kernels that can only be selected via prior\nknowledge or a tuning process are hardly perfect. Instead it is more common\nthat the kernel is good enough but imperfect in the sense that the true\nregression can be well approximated by but does not lie exactly in the kernel\nspace. We show distributed kernel regression can still achieves capacity\nindependent optimal rate in this case. To this end, we first establish a\ngeneral framework that allows to analyze distributed regression with response\nweighted base algorithms by bounding the error of such algorithms on a single\ndata set, provided that the error bounds has factored the impact of the\nunexplained variance of the response variable. Then we perform a leave one out\nanalysis of the kernel ridge regression and bias corrected kernel ridge\nregression, which in combination with the aforementioned framework allows us to\nderive sharp error bounds and capacity independent optimal rates for the\nassociated distributed kernel regression algorithms. As a byproduct of the\nthorough analysis, we also prove the kernel ridge regression can achieve rates\nfaster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting\nwhich, to our best knowledge, are first observed and novel in regression\nlearning.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T05, 68Q32, 68W15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16744v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16742v2",
    "title": "FairRec: Fairness-aware News Recommendation with Decomposed Adversarial Learning",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Xiting Wang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "author_ids": [],
    "abstract": "News recommendation is important for online news services. Existing news\nrecommendation models are usually learned from users' news click behaviors.\nUsually the behaviors of users with the same sensitive attributes (e.g.,\ngenders) have similar patterns and news recommendation models can easily\ncapture these patterns. It may lead to some biases related to sensitive user\nattributes in the recommendation results, e.g., always recommending sports news\nto male users, which is unfair since users may not receive diverse news\ninformation. In this paper, we propose a fairness-aware news recommendation\napproach with decomposed adversarial learning and orthogonality regularization,\nwhich can alleviate unfairness in news recommendation brought by the biases of\nsensitive user attributes. In our approach, we propose to decompose the user\ninterest model into two components. One component aims to learn a bias-aware\nuser embedding that captures the bias information on sensitive user attributes,\nand the other aims to learn a bias-free user embedding that only encodes\nattribute-independent user interest information for fairness-aware news\nrecommendation. In addition, we propose to apply an attribute prediction task\nto the bias-aware user embedding to enhance its ability on bias modeling, and\nwe apply adversarial learning to the bias-free user embedding to remove the\nbias information from it. Moreover, we propose an orthogonality regularization\nmethod to encourage the bias-free user embeddings to be orthogonal to the\nbias-aware one to better distinguish the bias-free user embedding from the\nbias-aware one. For fairness-aware news ranking, we only use the bias-free user\nembedding. Extensive experiments on benchmark dataset show that our approach\ncan effectively improve fairness in news recommendation with minor performance\nloss.",
    "published_date": "2020-06-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16742v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16402v1",
    "title": "Reading Between the Demographic Lines: Resolving Sources of Bias in Toxicity Classifiers",
    "authors": [
      "Elizabeth Reichert",
      "Helen Qiu",
      "Jasmine Bayrooti"
    ],
    "author_ids": [],
    "abstract": "The censorship of toxic comments is often left to the judgment of imperfect\nmodels. Perspective API, a creation of Google technology incubator Jigsaw, is\nperhaps the most widely used toxicity classifier in industry; the model is\nemployed by several online communities including The New York Times to identify\nand filter out toxic comments with the goal of preserving online safety.\nUnfortunately, Google's model tends to unfairly assign higher toxicity scores\nto comments containing words referring to the identities of commonly targeted\ngroups (e.g., \"woman,'' \"gay,'' etc.) because these identities are frequently\nreferenced in a disrespectful manner in the training data. As a result,\ncomments generated by marginalized groups referencing their identities are\noften mistakenly censored. It is important to be cognizant of this unintended\nbias and strive to mitigate its effects. To address this issue, we have\nconstructed several toxicity classifiers with the intention of reducing\nunintended bias while maintaining strong classification performance.",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16402v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16380v2",
    "title": "Quantifying Susceptibility to Spear Phishing in a High School Environment Using Signal Detection Theory",
    "authors": [
      "Ploy Unchit",
      "Sanchari Das",
      "Andrew Kim",
      "L. Jean Camp"
    ],
    "author_ids": [],
    "abstract": "Spear phishing is a deceptive attack that uses social engineering to obtain\nconfidential information through targeted victimization. It is distinguished by\nits use of social cues and personalized information to target specific victims.\nPrevious work on resilience to spear phishing has focused on convenience\nsamples, with a disproportionate focus on students. In contrast, here, we\nreport on an evaluation of a high school community. We engaged 57 high school\nstudents and faculty members (12 high school students, 45 staff members) as\nparticipants in research utilizing signal detection theory (SDT). Through\nscenario-based analysis, participants tasked with distinguishing phishing\nemails from authentic emails. The results revealed an overconfidence bias in\nself-detection from the participants, regardless of their technical background.\nThese findings are critical for evaluating the decision-making of\nunderrepresented populations and protecting people from potential spear\nphishing attacks by examining human susceptibility.",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16380v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.16309v2",
    "title": "Adversarial Learning for Debiasing Knowledge Graph Embeddings",
    "authors": [
      "Mario Arduini",
      "Lorenzo Noci",
      "Federico Pirovano",
      "Ce Zhang",
      "Yash Raj Shrestha",
      "Bibek Paudel"
    ],
    "author_ids": [],
    "abstract": "Knowledge Graphs (KG) are gaining increasing attention in both academia and\nindustry. Despite their diverse benefits, recent research have identified\nsocial and cultural biases embedded in the representations learned from KGs.\nSuch biases can have detrimental consequences on different population and\nminority groups as applications of KG begin to intersect and interact with\nsocial spheres. This paper aims at identifying and mitigating such biases in\nKnowledge Graph (KG) embeddings. As a first step, we explore popularity bias --\nthe relationship between node popularity and link prediction accuracy. In case\nof node2vec graph embeddings, we find that prediction accuracy of the embedding\nis negatively correlated with the degree of the node. However, in case of\nknowledge-graph embeddings (KGE), we observe an opposite trend. As a second\nstep, we explore gender bias in KGE, and a careful examination of popular KGE\nalgorithms suggest that sensitive attribute like the gender of a person can be\npredicted from the embedding. This implies that such biases in popular KGs is\ncaptured by the structural properties of the embedding. As a preliminary\nsolution to debiasing KGs, we introduce a novel framework to filter out the\nsensitive attribute information from the KG embeddings, which we call FAN\n(Filtering Adversarial Network). We also suggest the applicability of FAN for\ndebiasing other network embeddings which could be explored in future work.",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16309v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.01052v1",
    "title": "Efficient Mining Cluster Selection for Blockchain-based Cellular V2X Communications",
    "authors": [
      "Furqan Jameel",
      "Muhammad Awais Javed",
      "Sherali Zeadally",
      "Riku Jantti"
    ],
    "author_ids": [],
    "abstract": "Cellular vehicle-to-everything (V2X) communication is expected to herald the\nage of autonomous vehicles in the coming years. With the integration of\nblockchain in such networks, information of all granularity levels, from\ncomplete blocks to individual transactions, would be accessible to vehicles at\nany time. Specifically, the blockchain technology is expected to improve the\nsecurity, immutability, and decentralization of cellular V2X communication\nthrough smart contract and distributed ledgers. Although blockchain-based\ncellular V2X networks hold promise, many challenges need to be addressed to\nenable the future interoperability and accessibility of such large-scale\nplatforms. One such challenge is the offloading of mining tasks in cellular V2X\nnetworks. While transportation authorities may try to balance the network\nmining load, the vehicles may select the nearest mining clusters to offload a\ntask. This may cause congestion and disproportionate use of vehicular network\nresources. To address this issue, we propose a game-theoretic approach for\nbalancing the load at mining clusters while maintaining fairness among\noffloading vehicles. Keeping in mind the low-latency requirements of vehicles,\nwe consider a finite channel blocklength transmission which is more practical\ncompared to the use of infinite blocklength codes. The simulation results\nobtained with our proposed offloading framework show improved performance over\nthe conventional nearest mining cluster selection technique.",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.01052v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.15893v1",
    "title": "Group Envy Freeness and Group Pareto Efficiency in Fair Division with Indivisible Items",
    "authors": [
      "Martin Aleksandrov",
      "Toby Walsh"
    ],
    "author_ids": [],
    "abstract": "We study the fair division of items to agents supposing that agents can form\ngroups. We thus give natural generalizations of popular concepts such as\nenvy-freeness and Pareto efficiency to groups of fixed sizes. Group\nenvy-freeness requires that no group envies another group. Group Pareto\nefficiency requires that no group can be made better off without another group\nbe made worse off. We study these new group properties from an axiomatic\nviewpoint. We thus propose new fairness taxonomies that generalize existing\ntaxonomies. We further study near versions of these group properties as\nallocations for some of them may not exist. We finally give three prices of\ngroup fairness between group properties for three common social welfares (i.e.\nutilitarian, egalitarian, and Nash).",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15893v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.15874v2",
    "title": "Decorrelated Clustering with Data Selection Bias",
    "authors": [
      "Xiao Wang",
      "Shaohua Fan",
      "Kun Kuang",
      "Chuan Shi",
      "Jiawei Liu",
      "Bai Wang"
    ],
    "author_ids": [],
    "abstract": "Most of existing clustering algorithms are proposed without considering the\nselection bias in data. In many real applications, however, one cannot\nguarantee the data is unbiased. Selection bias might bring the unexpected\ncorrelation between features and ignoring those unexpected correlations will\nhurt the performance of clustering algorithms. Therefore, how to remove those\nunexpected correlations induced by selection bias is extremely important yet\nlargely unexplored for clustering. In this paper, we propose a novel\nDecorrelation regularized K-Means algorithm (DCKM) for clustering with data\nselection bias. Specifically, the decorrelation regularizer aims to learn the\nglobal sample weights which are capable of balancing the sample distribution,\nso as to remove unexpected correlations among features. Meanwhile, the learned\nweights are combined with k-means, which makes the reweighted k-means cluster\non the inherent data distribution without unexpected correlation influence.\nMoreover, we derive the updating rules to effectively infer the parameters in\nDCKM. Extensive experiments results on real world datasets well demonstrate\nthat our DCKM algorithm achieves significant performance gains, indicating the\nnecessity of removing unexpected feature correlations induced by selection bias\nwhen clustering.",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15874v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.15772v2",
    "title": "Multi-sided Exposure Bias in Recommendation",
    "authors": [
      "Himan Abdollahpouri",
      "Masoud Mansoury"
    ],
    "author_ids": [],
    "abstract": "Academic research in recommender systems has been greatly focusing on the\naccuracy-related measures of recommendations. Even when non-accuracy measures\nsuch as popularity bias, diversity, and novelty are studied, it is often solely\nfrom the users' perspective. However, many real-world recommenders are often\nmulti-stakeholder environments in which the needs and interests of several\nstakeholders should be addressed in the recommendation process. In this paper,\nwe focus on the popularity bias problem which is a well-known property of many\nrecommendation algorithms where few popular items are over-recommended while\nthe majority of other items do not get proportional attention and address its\nimpact on different stakeholders. Using several recommendation algorithms and\ntwo publicly available datasets in music and movie domains, we empirically show\nthe inherent popularity bias of the algorithms and how this bias impacts\ndifferent stakeholders such as users and suppliers of the items. We also\npropose metrics to measure the exposure bias of recommendation algorithms from\nthe perspective of different stakeholders.",
    "published_date": "2020-06-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15772v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.15731v1",
    "title": "Unsupervised Learning of Video Representations via Dense Trajectory Clustering",
    "authors": [
      "Pavel Tokmakov",
      "Martial Hebert",
      "Cordelia Schmid"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the task of unsupervised learning of representations for\naction recognition in videos. Previous works proposed to utilize future\nprediction, or other domain-specific objectives to train a network, but\nachieved only limited success. In contrast, in the relevant field of image\nrepresentation learning, simpler, discrimination-based methods have recently\nbridged the gap to fully-supervised performance. We first propose to adapt two\ntop performing objectives in this class - instance recognition and local\naggregation, to the video domain. In particular, the latter approach iterates\nbetween clustering the videos in the feature space of a network and updating it\nto respect the cluster with a non-parametric classification loss. We observe\npromising performance, but qualitative analysis shows that the learned\nrepresentations fail to capture motion patterns, grouping the videos based on\nappearance. To mitigate this issue, we turn to the heuristic-based IDT\ndescriptors, that were manually designed to encode motion patterns in videos.\nWe form the clusters in the IDT space, using these descriptors as a an\nunsupervised prior in the iterative local aggregation algorithm. Our\nexperiments demonstrates that this approach outperform prior work on UCF101 and\nHMDB51 action recognition benchmarks. We also qualitatively analyze the learned\nrepresentations and show that they successfully capture video dynamics.",
    "published_date": "2020-06-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.15528v2",
    "title": "DeepACC:Automate Chromosome Classification based on Metaphase Images using Deep Learning Framework Fused with Prior Knowledge",
    "authors": [
      "Chunlong Luo",
      "Tianqi Yu",
      "Yufan Luo",
      "Manqing Wang",
      "Fuhai Yu",
      "Yinhao Li",
      "Chan Tian",
      "Jie Qiao",
      "Li Xiao"
    ],
    "author_ids": [],
    "abstract": "Chromosome classification is an important but difficult and tedious task in\nkaryotyping. Previous methods only classify manually segmented single\nchromosome, which is far from clinical practice. In this work, we propose a\ndetection based method, DeepACC, to locate and fine classify chromosomes\nsimultaneously based on the whole metaphase image. We firstly introduce the\nAdditive Angular Margin Loss to enhance the discriminative power of model. To\nalleviate batch effects, we transform decision boundary of each class\ncase-by-case through a siamese network which make full use of prior knowledges\nthat chromosomes usually appear in pairs. Furthermore, we take the clinically\nseven group criterion as a prior knowledge and design an additional Group\nInner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase\nimages from clinical laboratory are collected and labelled to evaluate the\nperformance. Results show that the new design brings encouraging performance\ngains comparing to the state-of-the-art baselines.",
    "published_date": "2020-06-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15528v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.15429v2",
    "title": "Understanding Gradient Clipping in Private SGD: A Geometric Perspective",
    "authors": [
      "Xiangyi Chen",
      "Zhiwei Steven Wu",
      "Mingyi Hong"
    ],
    "author_ids": [],
    "abstract": "Deep learning models are increasingly popular in many machine learning\napplications where the training data may contain sensitive information. To\nprovide formal and rigorous privacy guarantee, many learning systems now\nincorporate differential privacy by training their models with (differentially)\nprivate SGD. A key step in each private SGD update is gradient clipping that\nshrinks the gradient of an individual example whenever its L2 norm exceeds some\nthreshold. We first demonstrate how gradient clipping can prevent SGD from\nconverging to stationary point. We then provide a theoretical analysis that\nfully quantifies the clipping bias on convergence with a disparity measure\nbetween the gradient distribution and a geometrically symmetric distribution.\nOur empirical evaluation further suggests that the gradient distributions along\nthe trajectory of private SGD indeed exhibit symmetric structure that favors\nconvergence. Together, our results provide an explanation why private SGD with\ngradient clipping remains effective in practice despite its potential clipping\nbias. Finally, we develop a new perturbation-based technique that can provably\ncorrect the clipping bias even for instances with highly asymmetric gradient\ndistributions.",
    "published_date": "2020-06-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15429v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.15351v2",
    "title": "Unsupervised Deep Representation Learning and Few-Shot Classification of PolSAR Images",
    "authors": [
      "Lamei Zhang",
      "Siyu Zhang",
      "Bin Zou",
      "Hongwei Dong"
    ],
    "author_ids": [],
    "abstract": "Deep learning and convolutional neural networks (CNNs) have made progress in\npolarimetric synthetic aperture radar (PolSAR) image classification over the\npast few years. However, a crucial issue has not been addressed, i.e., the\nrequirement of CNNs for abundant labeled samples versus the insufficient human\nannotations of PolSAR images. It is well-known that following the supervised\nlearning paradigm may lead to the overfitting of training data, and the lack of\nsupervision information of PolSAR images undoubtedly aggravates this problem,\nwhich greatly affects the generalization performance of CNN-based classifiers\nin large-scale applications. To handle this problem, in this paper, learning\ntransferrable representations from unlabeled PolSAR data through convolutional\narchitectures is explored for the first time. Specifically, a PolSAR-tailored\ncontrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR\nrepresentation learning and few-shot classification. Different from the\nutilization of optical processing methods, a diversity stimulation mechanism is\nconstructed to narrow the application gap between optics and PolSAR. Beyond the\nconventional supervised methods, PCLNet develops an unsupervised pre-training\nphase based on the proxy objective of instance discrimination to learn useful\nrepresentations from unlabeled PolSAR data. The acquired representations are\ntransferred to the downstream task, i.e., few-shot PolSAR classification.\nExperiments on two widely-used PolSAR benchmark datasets confirm the validity\nof PCLNet. Besides, this work may enlighten how to efficiently utilize the\nmassive unlabeled PolSAR data to alleviate the greedy demands of CNN-based\nmethods for human annotations.",
    "published_date": "2020-06-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15351v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14953v2",
    "title": "What they do when in doubt: a study of inductive biases in seq2seq learners",
    "authors": [
      "Eugene Kharitonov",
      "Rahma Chaabouni"
    ],
    "author_ids": [],
    "abstract": "Sequence-to-sequence (seq2seq) learners are widely used, but we still have\nonly limited knowledge about what inductive biases shape the way they\ngeneralize. We address that by investigating how popular seq2seq learners\ngeneralize in tasks that have high ambiguity in the training data. We use SCAN\nand three new tasks to study learners' preferences for memorization,\narithmetic, hierarchical, and compositional reasoning. Further, we connect to\nSolomonoff's theory of induction and propose to use description length as a\nprincipled and sensitive measure of inductive biases.\n  In our experimental study, we find that LSTM-based learners can learn to\nperform counting, addition, and multiplication by a constant from a single\ntraining example. Furthermore, Transformer and LSTM-based learners show a bias\ntoward the hierarchical induction over the linear one, while CNN-based learners\nprefer the opposite. On the SCAN dataset, we find that CNN-based, and, to a\nlesser degree, Transformer- and LSTM-based learners have a preference for\ncompositional generalization over memorization. Finally, across all our\nexperiments, description length proved to be a sensitive measure of inductive\nbiases.",
    "published_date": "2020-06-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14953v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14763v1",
    "title": "PAC-Bayesian Bound for the Conditional Value at Risk",
    "authors": [
      "Zakaria Mhammedi",
      "Benjamin Guedj",
      "Robert C. Williamson"
    ],
    "author_ids": [],
    "abstract": "Conditional Value at Risk (CVaR) is a family of \"coherent risk measures\"\nwhich generalize the traditional mathematical expectation. Widely used in\nmathematical finance, it is garnering increasing interest in machine learning,\ne.g., as an alternate approach to regularization, and as a means for ensuring\nfairness. This paper presents a generalization bound for learning algorithms\nthat minimize the CVaR of the empirical loss. The bound is of PAC-Bayesian type\nand is guaranteed to be small when the empirical CVaR is small. We achieve this\nby reducing the problem of estimating CVaR to that of merely estimating an\nexpectation. This then enables us, as a by-product, to obtain concentration\ninequalities for CVaR even when the random variable in question is unbounded.",
    "published_date": "2020-06-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14763v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14722v2",
    "title": "Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias",
    "authors": [
      "Satyam Mohla",
      "Anshul Nasery",
      "Biplab Banerjee"
    ],
    "author_ids": [],
    "abstract": "Recent experiments in computer vision demonstrate texture bias as the primary\nreason for supreme results in models employing Convolutional Neural Networks\n(CNNs), conflicting with early works claiming that these networks identify\nobjects using shape. It is believed that the cost function forces the CNN to\ntake a greedy approach and develop a proclivity for local information like\ntexture to increase accuracy, thus failing to explore any global statistics. We\npropose CognitiveCNN, a new intuitive architecture, inspired from feature\nintegration theory in psychology to utilise human interpretable feature like\nshape, texture, edges etc. to reconstruct, and classify the image. We define\nnovel metrics to quantify the \"relevance\" of \"abstract information\" present in\nthese modalities using attention maps. We further introduce a regularisation\nmethod which ensures that each modality like shape, texture etc. gets\nproportionate influence in a given task, as it does for reconstruction; and\nperform experiments to show the resulting boost in accuracy and robustness,\nbesides imparting explainability to these CNNs for achieving superior\nperformance in object recognition.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14722v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14662v1",
    "title": "The State of AI Ethics Report (June 2020)",
    "authors": [
      "Abhishek Gupta",
      "Camylle Lanteigne",
      "Victoria Heath",
      "Marianna Bergamaschi Ganapini",
      "Erick Galinkin",
      "Allison Cohen",
      "Tania De Gasperis",
      "Mo Akif",
      "Renjie Butalid"
    ],
    "author_ids": [],
    "abstract": "These past few months have been especially challenging, and the deployment of\ntechnology in ways hitherto untested at an unrivalled pace has left the\ninternet and technology watchers aghast. Artificial intelligence has become the\nbyword for technological progress and is being used in everything from helping\nus combat the COVID-19 pandemic to nudging our attention in different\ndirections as we all spend increasingly larger amounts of time online. It has\nnever been more important that we keep a sharp eye out on the development of\nthis field and how it is shaping our society and interactions with each other.\nWith this inaugural edition of the State of AI Ethics we hope to bring forward\nthe most important developments that caught our attention at the Montreal AI\nEthics Institute this past quarter. Our goal is to help you navigate this\never-evolving field swiftly and allow you and your organization to make\ninformed decisions. This pulse-check for the state of discourse, research, and\ndevelopment is geared towards researchers and practitioners alike who are\nmaking decisions on behalf of their organizations in considering the societal\nimpacts of AI-enabled solutions. We cover a wide set of areas in this report\nspanning Agency and Responsibility, Security and Risk, Disinformation, Jobs and\nLabor, the Future of AI Ethics, and more. Our staff has worked tirelessly over\nthe past quarter surfacing signal from the noise so that you are equipped with\nthe right tools and knowledge to confidently tread this complex yet\nconsequential domain.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14662v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14563v3",
    "title": "Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for Replay Attack Detection",
    "authors": [
      "Yongqiang Dou",
      "Haocheng Yang",
      "Maolin Yang",
      "Yanyan Xu",
      "Dengfeng Ke"
    ],
    "author_ids": [],
    "abstract": "It becomes urgent to design effective anti-spoofing algorithms for vulnerable\nautomatic speaker verification systems due to the advancement of high-quality\nplayback devices. Current studies mainly treat anti-spoofing as a binary\nclassification problem between bonafide and spoofed utterances, while lack of\nindistinguishable samples makes it difficult to train a robust spoofing\ndetector. In this paper, we argue that for anti-spoofing, it needs more\nattention for indistinguishable samples over easily-classified ones in the\nmodeling process, to make correct discrimination a top priority. Therefore, to\nmitigate the data discrepancy between training and inference, we propose D3M,\nto leverage a balanced focal loss function as the training objective to\ndynamically scale the loss based on the traits of the sample itself. Besides,\nin the experiments, we select three kinds of features that contain both\nmagnitude-based and phase-based information to form complementary and\ninformative features. Experimental results on the ASVspoof2019 dataset\ndemonstrate the superiority of the proposed methods by comparison between our\nsystems and top-performing ones. Systems trained with the balanced focal loss\nperform significantly better than conventional cross-entropy loss. With\ncomplementary features, our fusion system with only three kinds of features\noutperforms other systems containing five or more complex single models by\n22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124\nand 0.55% respectively. Furthermore, we present and discuss the evaluation\nresults on real replay data apart from the simulated ASVspoof2019 data,\nindicating that research for anti-spoofing still has a long way to go. Source\ncode, analysis data, and other details are publicly available at\nhttps://github.com/asvspoof/D3M.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14563v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14550v1",
    "title": "Lifted Disjoint Paths with Application in Multiple Object Tracking",
    "authors": [
      "Andrea Hornakova",
      "Roberto Henschel",
      "Bodo Rosenhahn",
      "Paul Swoboda"
    ],
    "author_ids": [],
    "abstract": "We present an extension to the disjoint paths problem in which additional\n\\emph{lifted} edges are introduced to provide path connectivity priors. We call\nthe resulting optimization problem the lifted disjoint paths problem. We show\nthat this problem is NP-hard by reduction from integer multicommodity flow and\n3-SAT. To enable practical global optimization, we propose several classes of\nlinear inequalities that produce a high-quality LP-relaxation. Additionally, we\npropose efficient cutting plane algorithms for separating the proposed linear\ninequalities. The lifted disjoint path problem is a natural model for multiple\nobject tracking and allows an elegant mathematical formulation for long range\ntemporal interactions. Lifted edges help to prevent id switches and to\nre-identify persons. Our lifted disjoint paths tracker achieves nearly optimal\nassignments with respect to input detections. As a consequence, it leads on all\nthree main benchmarks of the MOT challenge, improving significantly over\nstate-of-the-art.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14550v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14513v1",
    "title": "Blockchain-Aided Flow Insertion and Verification in Software Defined Networks",
    "authors": [
      "Jiejun Hu",
      "Martin Reed",
      "Mays Al-Naday",
      "Nikolaos Thomos"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things (IoT) connected by Software Defined Networking (SDN)\npromises to bring great benefits to cyber-physical systems. However, the\nincreased attack surface offered by the growing number of connected vulnerable\ndevices and complex nature of SDN control plane applications could overturn the\nhuge benefits of such a system. This paper addresses the vulnerability of some\nunspecified security flaw in the SDN control plane application (such as a\nzero-day software vulnerability) which can be exploited to insert malicious\nflow rules in the switch that do not match network policies. Specifically, we\npropose a blockchain-as-a-service (BaaS) based framework that supports switch\nflow verification and insertion; and additionally provides straightforward\ndeployment of blockchain technology within an existing SDN infrastructure.\nWhile use of an external BaaS brings straightforward deployment, it obscures\nknowledge of the blockchain agents who are responsible for flow conformance\ntesting through a smart blockchain contract, leading to potential exploitation.\nThus, we design a strategy to prevent the blockchain agents from acting\narbitrarily, as this would result in what is termed a \"moral hazard\". We\nachieve this by developing a novel mathematical model of the fair reward scheme\nbased on game theory. To understand the performance of our system, we evaluate\nour model using a Matlab based simulation framework. The simulation results\ndemonstrate that the proposed algorithm balances the needs of the blockchain\nagents to maximise the overall social welfare, i.e. the sum of profits across\nall parties.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "C.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14513v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.14479v1",
    "title": "Fair navigation planning: a humanitarian robot use case",
    "authors": [
      "Martim Brandao"
    ],
    "author_ids": [],
    "abstract": "In this paper we investigate potential issues of fairness related to the\nmotion of mobile robots. We focus on the particular use case of humanitarian\nmapping and disaster response. We start by showing that there is a fairness\ndimension to robot navigation, and use a walkthrough example to bring out\ndesign choices and issues that arise during the development of a fair system.\nWe discuss indirect discrimination, fairness-efficiency trade-offs, the\nexistence of counter-productive fairness definitions, privacy and other issues.\nFinally, we conclude with a discussion of the potential of our methodology as a\nconcrete responsible innovation tool for eliciting ethical issues in the design\nof autonomous systems.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14479v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.14937v3",
    "title": "Joints in Random Forests",
    "authors": [
      "Alvaro H. C. Correia",
      "Robert Peharz",
      "Cassio de Campos"
    ],
    "author_ids": [],
    "abstract": "Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative\nlearners and tools of central importance to the everyday machine learning\npractitioner and data scientist. Due to their discriminative nature, however,\nthey lack principled methods to process inputs with missing features or to\ndetect outliers, which requires pairing them with imputation techniques or a\nseparate generative model. In this paper, we demonstrate that DTs and RFs can\nnaturally be interpreted as generative models, by drawing a connection to\nProbabilistic Circuits, a prominent class of tractable probabilistic models.\nThis reinterpretation equips them with a full joint distribution over the\nfeature space and leads to Generative Decision Trees (GeDTs) and Generative\nForests (GeFs), a family of novel hybrid generative-discriminative models. This\nfamily of models retains the overall characteristics of DTs and RFs while\nadditionally being able to handle missing features by means of marginalisation.\nUnder certain assumptions, frequently made for Bayes consistency results, we\nshow that consistency in GeDTs and GeFs extend to any pattern of missing input\nfeatures, if missing at random. Empirically, we show that our models often\noutperform common routines to treat missing data, such as K-nearest neighbour\nimputation, and moreover, that our models can naturally detect outliers by\nmonitoring the marginal probability of input features.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14937v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14329v2",
    "title": "Differentially Private Health Tokens for Estimating COVID-19 Risk",
    "authors": [
      "David Butler",
      "Chris Hicks",
      "James Bell",
      "Carsten Maple",
      "Jon Crowcroft"
    ],
    "author_ids": [],
    "abstract": "In the fight against Covid-19, many governments and businesses are in the\nprocess of evaluating, trialling and even implementing so-called immunity\npassports. Also known as antibody or health certificates, there is a clear\ndemand for any technology that could allow people to return to work and other\ncrowded places without placing others at risk. One of the major criticisms of\nsuch systems is that they could be misused to unfairly discriminate against\nthose without immunity, allowing the formation of an `immuno-privileged' class\nof people. In this work we are motivated to explore an alternative technical\nsolution that is non-discriminatory by design. In particular we propose health\ntokens -- randomised health certificates which, using methods from differential\nprivacy, allow individual test results to be randomised whilst still allowing\nuseful aggregate risk estimates to be calculated. We show that health tokens\ncould mitigate immunity-based discrimination whilst still presenting a viable\nmechanism for estimating the collective transmission risk posed by small groups\nof users. We evaluate the viability of our approach in the context of\nidentity-free and identity-binding use cases and then consider a number of\npossible attacks. Our experimental results show that for groups of size 500 or\nmore, the error associated with our method can be as low as 0.03 on average and\nthus the aggregated results can be useful in a number of identity-free\ncontexts. Finally, we present the results of our open-source prototype which\ndemonstrates the practicality of our solution.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14329v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.14325v1",
    "title": "High-Dimensional Quadratic Discriminant Analysis under Spiked Covariance Model",
    "authors": [
      "Houssem Sifaou",
      "Abla Kammoun",
      "Mohamed-Slim Alouini"
    ],
    "author_ids": [],
    "abstract": "Quadratic discriminant analysis (QDA) is a widely used classification\ntechnique that generalizes the linear discriminant analysis (LDA) classifier to\nthe case of distinct covariance matrices among classes. For the QDA classifier\nto yield high classification performance, an accurate estimation of the\ncovariance matrices is required. Such a task becomes all the more challenging\nin high dimensional settings, wherein the number of observations is comparable\nwith the feature dimension. A popular way to enhance the performance of QDA\nclassifier under these circumstances is to regularize the covariance matrix,\ngiving the name regularized QDA (R-QDA) to the corresponding classifier. In\nthis work, we consider the case in which the population covariance matrix has a\nspiked covariance structure, a model that is often assumed in several\napplications. Building on the classical QDA, we propose a novel quadratic\nclassification technique, the parameters of which are chosen such that the\nfisher-discriminant ratio is maximized. Numerical simulations show that the\nproposed classifier not only outperforms the classical R-QDA for both synthetic\nand real data but also requires lower computational complexity, making it\nsuitable to high dimensional settings.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14325v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14942v2",
    "title": "ELMV: an Ensemble-Learning Approach for Analyzing Electrical Health Records with Significant Missing Values",
    "authors": [
      "Lucas J. Liu",
      "Hongwei Zhang",
      "Jianzhong Di",
      "Jin Chen"
    ],
    "author_ids": [],
    "abstract": "Many real-world Electronic Health Record (EHR) data contains a large\nproportion of missing values. Leaving substantial portion of missing\ninformation unaddressed usually causes significant bias, which leads to invalid\nconclusion to be drawn. On the other hand, training a machine learning model\nwith a much smaller nearly-complete subset can drastically impact the\nreliability and accuracy of model inference. Data imputation algorithms that\nattempt to replace missing data with meaningful values inevitably increase the\nvariability of effect estimates with increased missingness, making it\nunreliable for hypothesis validation. We propose a novel Ensemble-Learning for\nMissing Value (ELMV) framework, which introduces an effective approach to\nconstruct multiple subsets of the original EHR data with a much lower missing\nrate, as well as mobilizing a dedicated support set for the ensemble learning\nin the purpose of reducing the bias caused by substantial missing values. ELMV\nhas been evaluated on a real-world healthcare data for critical feature\nidentification as well as a batch of simulation data with different missing\nrates for outcome prediction. On both experiments, ELMV clearly outperforms\nconventional missing value imputation methods and ensemble learning models.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14942v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14168v2",
    "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness",
    "authors": [
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "In this paper, we cast fair machine learning as invariant machine learning.\nWe first formulate a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias.",
    "published_date": "2020-06-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14168v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.14076v2",
    "title": "The Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification",
    "authors": [
      "Christian Tjandraatmadja",
      "Ross Anderson",
      "Joey Huchette",
      "Will Ma",
      "Krunal Patel",
      "Juan Pablo Vielma"
    ],
    "author_ids": [],
    "abstract": "We improve the effectiveness of propagation- and linear-optimization-based\nneural network verification algorithms with a new tightened convex relaxation\nfor ReLU neurons. Unlike previous single-neuron relaxations which focus only on\nthe univariate input space of the ReLU, our method considers the multivariate\ninput space of the affine pre-activation function preceding the ReLU. Using\nresults from submodularity and convex geometry, we derive an explicit\ndescription of the tightest possible convex relaxation when this multivariate\ninput is over a box domain. We show that our convex relaxation is significantly\nstronger than the commonly used univariate-input relaxation which has been\nproposed as a natural convex relaxation barrier for verification. While our\ndescription of the relaxation may require an exponential number of\ninequalities, we show that they can be separated in linear time and hence can\nbe efficiently incorporated into optimization algorithms on an as-needed basis.\nBased on this novel relaxation, we design two polynomial-time algorithms for\nneural network verification: a linear-programming-based algorithm that\nleverages the full power of our relaxation, and a fast propagation algorithm\nthat generalizes existing approaches. In both cases, we show that for a modest\nincrease in computational effort, our strengthened relaxation enables us to\nverify a significantly larger number of instances compared to similar\nalgorithms.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML",
      "68T07"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14076v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13888v4",
    "title": "RL Unplugged: A Suite of Benchmarks for Offline Reinforcement Learning",
    "authors": [
      "Caglar Gulcehre",
      "Ziyu Wang",
      "Alexander Novikov",
      "Tom Le Paine",
      "Sergio Gomez Colmenarejo",
      "Konrad Zolna",
      "Rishabh Agarwal",
      "Josh Merel",
      "Daniel Mankowitz",
      "Cosmin Paduraru",
      "Gabriel Dulac-Arnold",
      "Jerry Li",
      "Mohammad Norouzi",
      "Matt Hoffman",
      "Ofir Nachum",
      "George Tucker",
      "Nicolas Heess",
      "Nando de Freitas"
    ],
    "author_ids": [],
    "abstract": "Offline methods for reinforcement learning have a potential to help bridge\nthe gap between reinforcement learning research and real-world applications.\nThey make it possible to learn policies from offline datasets, thus overcoming\nconcerns associated with online data collection in the real-world, including\ncost, safety, or ethical concerns. In this paper, we propose a benchmark called\nRL Unplugged to evaluate and compare offline RL methods. RL Unplugged includes\ndata from a diverse range of domains including games (e.g., Atari benchmark)\nand simulated motor control problems (e.g., DM Control Suite). The datasets\ninclude domains that are partially or fully observable, use continuous or\ndiscrete actions, and have stochastic vs. deterministic dynamics. We propose\ndetailed evaluation protocols for each domain in RL Unplugged and provide an\nextensive analysis of supervised learning and offline RL methods using these\nprotocols. We will release data for all our tasks and open-source all\nalgorithms presented in this paper. We hope that our suite of benchmarks will\nincrease the reproducibility of experiments and make it possible to study\nchallenging tasks with a limited computational budget, thus making RL research\nboth more systematic and more accessible across the community. Moving forward,\nwe view RL Unplugged as a living benchmark suite that will evolve and grow with\ndatasets contributed by the research community and ourselves. Our project page\nis available on https://git.io/JJUhd.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13888v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13823v3",
    "title": "Preventing Value Function Collapse in Ensemble {Q}-Learning by Maximizing Representation Diversity",
    "authors": [
      "Hassam Ullah Sheikh",
      "Ladislau Bölöni"
    ],
    "author_ids": [],
    "abstract": "The classic DQN algorithm is limited by the overestimation bias of the\nlearned Q-function. Subsequent algorithms have proposed techniques to reduce\nthis problem, without fully eliminating it. Recently, the Maxmin and Ensemble\nQ-learning algorithms have used different estimates provided by the ensembles\nof learners to reduce the overestimation bias. Unfortunately, these learners\ncan converge to the same point in the parametric or representation space,\nfalling back to the classic single neural network DQN. In this paper, we\ndescribe a regularization technique to maximize ensemble diversity in these\nalgorithms. We propose and compare five regularization functions inspired from\neconomics theory and consensus optimization. We show that the regularized\napproach significantly outperforms the Maxmin and Ensemble Q-learning\nalgorithms as well as non-ensemble baselines.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13823v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13798v1",
    "title": "Bayesian Sampling Bias Correction: Training with the Right Loss Function",
    "authors": [
      "L. Le Folgoc",
      "V. Baltatzis",
      "A. Alansary",
      "S. Desai",
      "A. Devaraj",
      "S. Ellis",
      "O. E. Martinez Manzanera",
      "F. Kanavati",
      "A. Nair",
      "J. Schnabel",
      "B. Glocker"
    ],
    "author_ids": [],
    "abstract": "We derive a family of loss functions to train models in the presence of\nsampling bias. Examples are when the prevalence of a pathology differs from its\nsampling rate in the training dataset, or when a machine learning practioner\nrebalances their training dataset. Sampling bias causes large discrepancies\nbetween model performance in the lab and in more realistic settings. It is\nomnipresent in medical imaging applications, yet is often overlooked at\ntraining time or addressed on an ad-hoc basis. Our approach is based on\nBayesian risk minimization. For arbitrary likelihood models we derive the\nassociated bias corrected loss for training, exhibiting a direct connection to\ninformation gain. The approach integrates seamlessly in the current paradigm of\n(deep) learning using stochastic backpropagation and naturally with Bayesian\nmodels. We illustrate the methodology on case studies of lung nodule malignancy\ngrading.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "q-bio.QM",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13798v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13796v2",
    "title": "A Methodology for Creating AI FactSheets",
    "authors": [
      "John Richards",
      "David Piorkowski",
      "Michael Hind",
      "Stephanie Houde",
      "Aleksandra Mojsilović"
    ],
    "author_ids": [],
    "abstract": "As AI models and services are used in a growing number of highstakes areas, a\nconsensus is forming around the need for a clearer record of how these models\nand services are developed to increase trust. Several proposals for higher\nquality and more consistent AI documentation have emerged to address ethical\nand legal concerns and general social impacts of such systems. However, there\nis little published work on how to create this documentation. This is the first\nwork to describe a methodology for creating the form of AI documentation we\ncall FactSheets. We have used this methodology to create useful FactSheets for\nnearly two dozen models. This paper describes this methodology and shares the\ninsights we have gathered. Within each step of the methodology, we describe the\nissues to consider and the questions to explore with the relevant people in an\norganization who will be creating and consuming the AI facts in a FactSheet.\nThis methodology will accelerate the broader adoption of transparent AI\ndocumentation.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13796v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13699v1",
    "title": "On Fair Selection in the Presence of Implicit Variance",
    "authors": [
      "Vitalii Emelianov",
      "Nicolas Gast",
      "Krishna P. Gummadi",
      "Patrick Loiseau"
    ],
    "author_ids": [],
    "abstract": "Quota-based fairness mechanisms like the so-called Rooney rule or four-fifths\nrule are used in selection problems such as hiring or college admission to\nreduce inequalities based on sensitive demographic attributes. These mechanisms\nare often viewed as introducing a trade-off between selection fairness and\nutility. In recent work, however, Kleinberg and Raghavan showed that, in the\npresence of implicit bias in estimating candidates' quality, the Rooney rule\ncan increase the utility of the selection process.\n  We argue that even in the absence of implicit bias, the estimates of\ncandidates' quality from different groups may differ in another fundamental\nway, namely, in their variance. We term this phenomenon implicit variance and\nwe ask: can fairness mechanisms be beneficial to the utility of a selection\nprocess in the presence of implicit variance (even in the absence of implicit\nbias)? To answer this question, we propose a simple model in which candidates\nhave a true latent quality that is drawn from a group-independent normal\ndistribution. To make the selection, a decision maker receives an unbiased\nestimate of the quality of each candidate, with normal noise, but whose\nvariance depends on the candidate's group. We then compare the utility obtained\nby imposing a fairness mechanism that we term $\\gamma$-rule (it includes\ndemographic parity and the four-fifths rule as special cases), to that of a\ngroup-oblivious selection algorithm that picks the candidates with the highest\nestimated quality independently of their group. Our main result shows that the\ndemographic parity mechanism always increases the selection utility, while any\n$\\gamma$-rule weakly increases it. We extend our model to a two-stage selection\nprocess where the true quality is observed at the second stage. We discuss\nmultiple extensions of our results, in particular to different distributions of\nthe true latent quality.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13699v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13629v1",
    "title": "Robust Domain Adaptation: Representations, Weights and Inductive Bias",
    "authors": [
      "Victor Bouvier",
      "Philippe Very",
      "Clément Chastagnol",
      "Myriam Tami",
      "Céline Hudelot"
    ],
    "author_ids": [],
    "abstract": "Unsupervised Domain Adaptation (UDA) has attracted a lot of attention in the\nlast ten years. The emergence of Domain Invariant Representations (IR) has\nimproved drastically the transferability of representations from a labelled\nsource domain to a new and unlabelled target domain. However, a potential\npitfall of this approach, namely the presence of \\textit{label shift}, has been\nbrought to light. Some works address this issue with a relaxed version of\ndomain invariance obtained by weighting samples, a strategy often referred to\nas Importance Sampling. From our point of view, the theoretical aspects of how\nImportance Sampling and Invariant Representations interact in UDA have not been\nstudied in depth. In the present work, we present a bound of the target risk\nwhich incorporates both weights and invariant representations. Our theoretical\nanalysis highlights the role of inductive bias in aligning distributions across\ndomains. We illustrate it on standard benchmarks by proposing a new learning\nprocedure for UDA. We observed empirically that weak inductive bias makes\nadaptation more robust. The elaboration of stronger inductive bias is a\npromising direction for new UDA algorithms.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13629v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16923v2",
    "title": "Large image datasets: A pyrrhic win for computer vision?",
    "authors": [
      "Vinay Uday Prabhu",
      "Abeba Birhane"
    ],
    "author_ids": [],
    "abstract": "In this paper we investigate problematic practices and consequences of large\nscale vision datasets. We examine broad issues such as the question of consent\nand justice as well as specific concerns such as the inclusion of verifiably\npornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an\nexample, we perform a cross-sectional model-based quantitative census covering\nfactors such as age, gender, NSFW content scoring, class-wise accuracy,\nhuman-cardinality-analysis, and the semanticity of the image class information\nin order to statistically investigate the extent and subtleties of ethical\ntransgressions. We then use the census to help hand-curate a look-up-table of\nimages in the ImageNet-ILSVRC-2012 dataset that fall into the categories of\nverifiably pornographic: shot in a non-consensual setting (up-skirt), beach\nvoyeuristic, and exposed private parts. We survey the landscape of harm and\nthreats both society broadly and individuals face due to uncritical and\nill-considered dataset curation practices. We then propose possible courses of\ncorrection and critique the pros and cons of these. We have duly open-sourced\nall of the code and the census meta-datasets generated in this endeavor for the\ncomputer vision community to build on. By unveiling the severity of the\nthreats, our hope is to motivate the constitution of mandatory Institutional\nReview Boards (IRB) for large scale dataset curation processes.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16923v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13485v1",
    "title": "Fairness with Overlapping Groups",
    "authors": [
      "Forest Yang",
      "Moustapha Cisse",
      "Sanmi Koyejo"
    ],
    "author_ids": [],
    "abstract": "In algorithmically fair prediction problems, a standard goal is to ensure the\nequality of fairness metrics across multiple overlapping groups simultaneously.\nWe reconsider this standard fair classification problem using a probabilistic\npopulation analysis, which, in turn, reveals the Bayes-optimal classifier. Our\napproach unifies a variety of existing group-fair classification methods and\nenables extensions to a wide range of non-decomposable multiclass performance\nmetrics and fairness measures. The Bayes-optimal classifier further inspires\nconsistent procedures for algorithmically fair classification with overlapping\ngroups. On a variety of real datasets, the proposed approach outperforms\nbaselines in terms of its fairness-performance tradeoff.",
    "published_date": "2020-06-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13485v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13198v6",
    "title": "Spectral Bias and Task-Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks",
    "authors": [
      "Abdulkadir Canatar",
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ],
    "author_ids": [],
    "abstract": "Generalization beyond a training dataset is a main goal of machine learning,\nbut theoretical understanding of generalization remains an open problem for\nmany models. The need for a new theory is exacerbated by recent observations in\ndeep neural networks where overparameterization leads to better performance,\ncontradicting the conventional wisdom from classical statistics. In this paper,\nwe investigate generalization error for kernel regression, which, besides being\na popular machine learning method, also includes infinitely overparameterized\nneural networks trained with gradient descent. We use techniques from\nstatistical mechanics to derive an analytical expression for generalization\nerror applicable to any kernel or data distribution. We present applications of\nour theory to real and synthetic datasets, and for many kernels including those\nthat arise from training deep neural networks in the infinite-width limit. We\nelucidate an inductive bias of kernel regression to explain data with \"simple\nfunctions\", which are identified by solving a kernel eigenfunction problem on\nthe data distribution. This notion of simplicity allows us to characterize\nwhether a kernel is compatible with a learning task, facilitating good\ngeneralization performance from a small number of training examples. We show\nthat more data may impair generalization when noisy or not expressible by the\nkernel, leading to non-monotonic learning curves with possibly many peaks. To\nfurther understand these phenomena, we turn to the broad class of rotation\ninvariant kernels, which is relevant to training deep neural networks in the\ninfinite-width limit, and present a detailed mathematical analysis of them when\ndata is drawn from a spherically symmetric distribution and the number of input\ndimensions is large.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13198v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13114v3",
    "title": "Fairness without Demographics through Adversarially Reweighted Learning",
    "authors": [
      "Preethi Lahoti",
      "Alex Beutel",
      "Jilin Chen",
      "Kang Lee",
      "Flavien Prost",
      "Nithum Thain",
      "Xuezhi Wang",
      "Ed H. Chi"
    ],
    "author_ids": [],
    "abstract": "Much of the previous machine learning (ML) fairness literature assumes that\nprotected features such as race and sex are present in the dataset, and relies\nupon them to mitigate fairness concerns. However, in practice factors like\nprivacy and regulation often preclude the collection of protected features, or\ntheir use for training or inference, severely limiting the applicability of\ntraditional fairness research. Therefore we ask: How can we train an ML model\nto improve fairness when we do not even know the protected group memberships?\nIn this work we address this problem by proposing Adversarially Reweighted\nLearning (ARL). In particular, we hypothesize that non-protected features and\ntask labels are valuable for identifying fairness issues, and can be used to\nco-train an adversarial reweighting approach for improving fairness. Our\nresults show that {ARL} improves Rawlsian Max-Min fairness, with notable AUC\nimprovements for worst-case protected groups in multiple datasets,\noutperforming state-of-the-art alternatives.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13114v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13071v1",
    "title": "Domain Adaptation for Semantic Parsing",
    "authors": [
      "Zechang Li",
      "Yuxuan Lai",
      "Yansong Feng",
      "Dongyan Zhao"
    ],
    "author_ids": [],
    "abstract": "Recently, semantic parsing has attracted much attention in the community.\nAlthough many neural modeling efforts have greatly improved the performance, it\nstill suffers from the data scarcity issue. In this paper, we propose a novel\nsemantic parser for domain adaptation, where we have much fewer annotated data\nin the target domain compared to the source domain. Our semantic parser\nbenefits from a two-stage coarse-to-fine framework, thus can provide different\nand accurate treatments for the two stages, i.e., focusing on domain invariant\nand domain specific information, respectively. In the coarse stage, our novel\ndomain discrimination component and domain relevance attention encourage the\nmodel to learn transferable domain general structures. In the fine stage, the\nmodel is guided to concentrate on domain related details. Experiments on a\nbenchmark dataset show that our method consistently outperforms several popular\ndomain adaptation strategies. Additionally, we show that our model can well\nexploit limited target data to capture the difference between the source and\ntarget domain, even when the target domain has far fewer training instances.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13071v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.16925v3",
    "title": "Ethical Analysis on the Application of Neurotechnology for Human Augmentation in Physicians and Surgeons",
    "authors": [
      "Soaad Hossain",
      "Syed Ishtiaque Ahmed"
    ],
    "author_ids": [],
    "abstract": "With the shortage of physicians and surgeons and increase in demand worldwide\ndue to situations such as the COVID-19 pandemic, there is a growing interest in\nfinding solutions to help address the problem. A solution to this problem would\nbe to use neurotechnology to provide them augmented cognition, senses and\naction for optimal diagnosis and treatment. Consequently, doing so can\nnegatively impact them and others. We argue that applying neurotechnology for\nhuman enhancement in physicians and surgeons can cause injustices, and harm to\nthem and patients. In this paper, we will first describe the augmentations and\nneurotechnologies that can be used to achieve the relevant augmentations for\nphysicians and surgeons. We will then review selected ethical concerns\ndiscussed within literature, discuss the neuroengineering behind using\nneurotechnology for augmentation purposes, then conclude with an analysis on\noutcomes and ethical issues of implementing human augmentation via\nneurotechnology in medical and surgical practice.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC",
      "physics.med-ph",
      "68-06",
      "J.3; K.4.1; K.4.2; K.4.3; J.7; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16925v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.12793v1",
    "title": "Lumos: A Library for Diagnosing Metric Regressions in Web-Scale Applications",
    "authors": [
      "Jamie Pool",
      "Ebrahim Beyrami",
      "Vishak Gopal",
      "Ashkan Aazami",
      "Jayant Gupchup",
      "Jeff Rowland",
      "Binlong Li",
      "Pritesh Kanani",
      "Ross Cutler",
      "Johannes Gehrke"
    ],
    "author_ids": [],
    "abstract": "Web-scale applications can ship code on a daily to weekly cadence. These\napplications rely on online metrics to monitor the health of new releases.\nRegressions in metric values need to be detected and diagnosed as early as\npossible to reduce the disruption to users and product owners. Regressions in\nmetrics can surface due to a variety of reasons: genuine product regressions,\nchanges in user population, and bias due to telemetry loss (or processing) are\namong the common causes. Diagnosing the cause of these metric regressions is\ncostly for engineering teams as they need to invest time in finding the root\ncause of the issue as soon as possible. We present Lumos, a Python library\nbuilt using the principles of AB testing to systematically diagnose metric\nregressions to automate such analysis. Lumos has been deployed across the\ncomponent teams in Microsoft's Real-Time Communication applications Skype and\nMicrosoft Teams. It has enabled engineering teams to detect 100s of real\nchanges in metrics and reject 1000s of false alarms detected by anomaly\ndetectors. The application of Lumos has resulted in freeing up as much as 95%\nof the time allocated to metric-based investigations. In this work, we open\nsource Lumos and present our results from applying it to two different\ncomponents within the RTC group over millions of sessions. This general library\ncan be coupled with any production system to manage the volume of alerting\nefficiently.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12793v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12756v1",
    "title": "A Framework for Fairness in Two-Sided Marketplaces",
    "authors": [
      "Kinjal Basu",
      "Cyrus DiCiccio",
      "Heloise Logan",
      "Noureddine El Karoui"
    ],
    "author_ids": [],
    "abstract": "Many interesting problems in the Internet industry can be framed as a\ntwo-sided marketplace problem. Examples include search applications and\nrecommender systems showing people, jobs, movies, products, restaurants, etc.\nIncorporating fairness while building such systems is crucial and can have a\ndeep social and economic impact (applications include job recommendations,\nrecruiters searching for candidates, etc.). In this paper, we propose a\ndefinition and develop an end-to-end framework for achieving fairness while\nbuilding such machine learning systems at scale. We extend prior work to\ndevelop an optimization framework that can tackle fairness constraints from\nboth the source and destination sides of the marketplace, as well as dynamic\naspects of the problem. The framework is flexible enough to adapt to different\ndefinitions of fairness and can be implemented in very large-scale settings. We\nperform simulations to show the efficacy of our approach.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "62P30, 62A01",
      "K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12756v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12732v3",
    "title": "Fair Performance Metric Elicitation",
    "authors": [
      "Gaurush Hiranandani",
      "Harikrishna Narasimhan",
      "Oluwasanmi Koyejo"
    ],
    "author_ids": [],
    "abstract": "What is a fair performance metric? We consider the choice of fairness metrics\nthrough the lens of metric elicitation -- a principled framework for selecting\nperformance metrics that best reflect implicit preferences. The use of metric\nelicitation enables a practitioner to tune the performance and fairness metrics\nto the task, context, and population at hand. Specifically, we propose a novel\nstrategy to elicit group-fair performance metrics for multiclass classification\nproblems with multiple sensitive groups that also includes selecting the\ntrade-off between predictive performance and fairness violation. The proposed\nelicitation strategy requires only relative preference feedback and is robust\nto both finite sample and feedback noise.",
    "published_date": "2020-06-23T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12732v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12632v1",
    "title": "Towards Contrastive Explanations for Comparing the Ethics of Plans",
    "authors": [
      "Benjamin Krarup",
      "Senka Krivic",
      "Felix Lindner",
      "Derek Long"
    ],
    "author_ids": [],
    "abstract": "The development of robotics and AI agents has enabled their wider usage in\nhuman surroundings. AI agents are more trusted to make increasingly important\ndecisions with potentially critical outcomes. It is essential to consider the\nethical consequences of the decisions made by these systems. In this paper, we\npresent how contrastive explanations can be used for comparing the ethics of\nplans. We build upon an existing ethical framework to allow users to make\nsuggestions to plans and receive contrastive explanations.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12632v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12589v1",
    "title": "Distributional Individual Fairness in Clustering",
    "authors": [
      "Nihesh Anderson",
      "Suman K. Bera",
      "Syamantak Das",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "In this paper, we initiate the study of fair clustering that ensures\ndistributional similarity among similar individuals. In response to improving\nfairness in machine learning, recent papers have investigated fairness in\nclustering algorithms and have focused on the paradigm of statistical\nparity/group fairness. These efforts attempt to minimize bias against some\nprotected groups in the population. However, to the best of our knowledge, the\nalternative viewpoint of individual fairness, introduced by Dwork et al. (ITCS\n2012) in the context of classification, has not been considered for clustering\nso far. Similar to Dwork et al., we adopt the individual fairness notion which\nmandates that similar individuals should be treated similarly for clustering\nproblems. We use the notion of $f$-divergence as a measure of statistical\nsimilarity that significantly generalizes the ones used by Dwork et al. We\nintroduce a framework for assigning individuals, embedded in a metric space, to\nprobability distributions over a bounded number of cluster centers. The\nobjective is to ensure (a) low cost of clustering in expectation and (b)\nindividuals that are close to each other in a given fairness space are mapped\nto statistically similar distributions.\n  We provide an algorithm for clustering with $p$-norm objective ($k$-center,\n$k$-means are special cases) and individual fairness constraints with provable\napproximation guarantee. We extend this framework to include both group\nfairness and individual fairness inside the protected groups. Finally, we\nobserve conditions under which individual fairness implies group fairness. We\npresent extensive experimental evidence that justifies the effectiveness of our\napproach.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12589v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12459v2",
    "title": "IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression",
    "authors": [
      "Rianne van den Berg",
      "Alexey A. Gritsenko",
      "Mostafa Dehghani",
      "Casper Kaae Sønderby",
      "Tim Salimans"
    ],
    "author_ids": [],
    "abstract": "In this paper we analyse and improve integer discrete flows for lossless\ncompression. Integer discrete flows are a recently proposed class of models\nthat learn invertible transformations for integer-valued random variables.\nTheir discrete nature makes them particularly suitable for lossless compression\nwith entropy coding schemes. We start by investigating a recent theoretical\nclaim that states that invertible flows for discrete random variables are less\nflexible than their continuous counterparts. We demonstrate with a proof that\nthis claim does not hold for integer discrete flows due to the embedding of\ndata with finite support into the countably infinite integer lattice.\nFurthermore, we zoom in on the effect of gradient bias due to the\nstraight-through estimator in integer discrete flows, and demonstrate that its\ninfluence is highly dependent on architecture choices and less prominent than\npreviously thought. Finally, we show how different architecture modifications\nimprove the performance of this model class for lossless compression, and that\nthey also enable more efficient compression: a model with half the number of\nflow layers performs on par with or better than the original integer discrete\nflow model.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12459v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12399v1",
    "title": "How fair can we go in machine learning? Assessing the boundaries of fairness in decision trees",
    "authors": [
      "Ana Valdivia",
      "Javier Sánchez-Monedero",
      "Jorge Casillas"
    ],
    "author_ids": [],
    "abstract": "Fair machine learning works have been focusing on the development of\nequitable algorithms that address discrimination of certain groups. Yet, many\nof these fairness-aware approaches aim to obtain a unique solution to the\nproblem, which leads to a poor understanding of the statistical limits of bias\nmitigation interventions. We present the first methodology that allows to\nexplore those limits within a multi-objective framework that seeks to optimize\nany measure of accuracy and fairness and provides a Pareto front with the best\nfeasible solutions. In this work, we focus our study on decision tree\nclassifiers since they are widely accepted in machine learning, are easy to\ninterpret and can deal with non-numerical information naturally. We conclude\nexperimentally that our method can optimize decision tree models by being\nfairer with a small cost of the classification error. We believe that our\ncontribution will help stakeholders of sociotechnical systems to assess how far\nthey can go being fair and accurate, thus serving in the support of enhanced\ndecision making where machine learning is used.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12387v3",
    "title": "Leveraging traditional ecological knowledge in ecosystem restoration projects utilizing machine learning",
    "authors": [
      "Bogdana Rakova",
      "Alexander Winter"
    ],
    "author_ids": [],
    "abstract": "Ecosystem restoration has been recognized to be critical to achieving\naccelerating progress on all of the United Nations' Sustainable Development\nGoals. Decision makers, policymakers, data scientists, earth scientists, and\nother scholars working on these projects could positively benefit from the\nexplicit consideration and inclusion of diverse perspectives. Community\nengagement throughout the stages of ecosystem restoration projects could\ncontribute to improved community well-being, the conservation of biodiversity,\necosystem functions, and the resilience of socio-ecological systems. Conceptual\nframeworks are needed for the meaningful integration of traditional ecological\nknowledge of indigenous peoples and local communities with data science and\nmachine learning work practices. Adaptive frameworks would consider and address\nthe needs and challenges of local communities and geographic locations by\nimproving community and inter-agent communication around restoration and\nconservation projects and by making relevant real-time data accessible. In this\npaper, we provide a brief analysis of existing Machine Learning (ML)\napplications for forest ecosystem restoration projects. We go on to question if\ntheir inherent limitations may prevent them from being able to adequately\naddress socio-cultural aspects of the well-being of all involved stakeholders.\nBias and unintended consequences pose significant risks of downstream negative\nimplications of ML-based solutions. We suggest that adaptive and scalable\npractices could incentivize interdisciplinary collaboration during all stages\nof ecosystemic ML restoration projects and align incentives between human and\nalgorithmic actors. Furthermore, framing ML projects as open and reiterative\nprocesses can facilitate access on various levels and create incentives that\nlead to catalytic cooperation in the scaling of restoration efforts.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12387v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12196v2",
    "title": "Estimating Properties of Social Networks via Random Walk considering Private Nodes",
    "authors": [
      "Kazuki Nakajima",
      "Kazuyuki Shudo"
    ],
    "author_ids": [],
    "abstract": "Accurately analyzing graph properties of social networks is a challenging\ntask because of access limitations to the graph data. To address this\nchallenge, several algorithms to obtain unbiased estimates of properties from\nfew samples via a random walk have been studied. However, existing algorithms\ndo not consider private nodes who hide their neighbors in real social networks,\nleading to some practical problems. Here we design random walk-based algorithms\nto accurately estimate properties without any problems caused by private nodes.\nFirst, we design a random walk-based sampling algorithm that comprises the\nneighbor selection to obtain samples having the Markov property and the\ncalculation of weights for each sample to correct the sampling bias. Further,\nfor two graph property estimators, we propose the weighting methods to reduce\nnot only the sampling bias but also estimation errors due to private nodes. The\nproposed algorithms improve the estimation accuracy of the existing algorithms\nby up to 92.6% on real-world datasets.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "68R10 (Primary), 91D30 (Primary), 05C81 (Secondary)",
      "G.3; J.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12196v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.12121v1",
    "title": "Automated machine vision enabled detection of movement disorders from hand drawn spirals",
    "authors": [
      "Nabeel Seedat",
      "Vered Aharonson",
      "Ilana Schlesinger"
    ],
    "author_ids": [],
    "abstract": "A widely used test for the diagnosis of Parkinson's disease (PD) and\nEssential tremor (ET) is hand-drawn shapes,where the analysis is\nobservationally performed by the examining neurologist. This method is\nsubjective and is prone to bias amongst different physicians. Due to the\nsimilarities in the symptoms of the two diseases, they are often\nmisdiagnosed.Studies which attempt to automate the process typically use\ndigitized input, where the tablet or specialized equipment are not affordable\nin many clinical settings. This study uses a dataset of scanned pen and paper\ndrawings and a convolutional neural network (CNN) to perform classification\nbetween PD, ET and control subjects. The discrimination accuracy of PD from\ncontrols was 98.2%. The discrimination accuracy of PD from ET and from controls\nwas 92%. An ablation study was conducted and indicated that correct hyper\nparameter optimization can increases the accuracy up to 4.33%. Finally, the\nstudy indicates the viability of using a CNN-enabled machine vision system to\nprovide robust and accurate detection of movement disorders from hand drawn\nspirals.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12121v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12094v1",
    "title": "Machine learning discrimination of Parkinson's Disease stages from walker-mounted sensors data",
    "authors": [
      "Nabeel Seedat",
      "Vered Aharonson"
    ],
    "author_ids": [],
    "abstract": "Clinical methods that assess gait in Parkinson's Disease (PD) are mostly\nqualitative. Quantitative methods necessitate costly instrumentation or\ncumbersome wearable devices, which limits their usability. Only few of these\nmethods can discriminate different stages in PD progression. This study applies\nmachine learning methods to discriminate six stages of PD. The data was\nacquired by low cost walker-mounted sensors in an experiment at a movement\ndisorders clinic and the PD stages were clinically labeled. A large set of\nfeatures, some unique to this study are extracted and three feature selection\nmethods are compared using a multi-class Random Forest (RF) classifier. The\nfeature subset selected by the Analysis of Variance (ANOVA) method provided\nperformance similar to the full feature set: 93% accuracy and had significantly\nshorter computation time. Compared to PCA, this method also enabled clinical\ninterpretability of the selected features, an essential attribute to healthcare\napplications. All selected-feature sets are dominated by information theoretic\nfeatures and statistical features and offer insights into the characteristics\nof gait deterioration in PD. The results indicate a feasibility of machine\nlearning to accurately classify PD severity stages from kinematic signals\nacquired by low-cost, walker-mounted sensors and implies a potential to aid\nmedical practitioners in the quantitative assessment of PD progression. The\nstudy presents a solution to the small and noisy data problem, which is common\nin most sensor-based healthcare assessments.",
    "published_date": "2020-06-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12094v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11858v2",
    "title": "Guaranteed Performance Nonlinear Observer for Simultaneous Localization and Mapping",
    "authors": [
      "Hashim A. Hashim"
    ],
    "author_ids": [],
    "abstract": "A geometric nonlinear observer algorithm for Simultaneous Localization and\nMapping (SLAM) developed on the Lie group of \\mathbb{SLAM}_{n}\\left(3\\right) is\nproposed. The presented novel solution estimates the vehicle's pose (i.e.\nattitude and position) with respect to landmarks simultaneously positioning the\nreference features in the global frame. The proposed estimator on manifold is\ncharacterized by predefined measures of transient and steady-state performance.\nDynamically reducing boundaries guide the error function of the system to\nreduce asymptotically to the origin from its starting position within a large\ngiven set. The proposed observer has the ability to use the available velocity\nand feature measurements directly. Also, it compensates for unknown constant\nbias attached to velocity measurements. Unit-qauternion of the proposed\nobserver is presented. Numerical results reveal effectiveness of the proposed\nobserver. Keywords: Nonlinear filter algorithm, Nonlinear observer for\nSimultaneous Localization and Mapping, Nonlinear estimator, nonlinear SLAM\nobserver on manifold, nonlinear SLAM filter on matrix Lie Group, observer\ndesign, asymptotic stability, systematic convergence, Prescribed performance\nfunction, pose estimation, attitude filter, position filter, feature filter,\nlandmark filter, gradient based SLAM observer, gradient based observer for\nSLAM, adaptive estimate, SLAM observer, observer SLAM framework, equivariant\nobserver, inertial vision unit, visual, SLAM filter, SE(3), SO(3).",
    "published_date": "2020-06-21T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11858v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.11814v1",
    "title": "A blindspot of AI ethics: anti-fragility in statistical prediction",
    "authors": [
      "Michele Loi",
      "Lonneke van der Plas"
    ],
    "author_ids": [],
    "abstract": "With this paper, we aim to put an issue on the agenda of AI ethics that in\nour view is overlooked in the current discourse. The current discussions are\ndominated by topics suchas trustworthiness and bias, whereas the issue we like\nto focuson is counter to the debate on trustworthiness. We fear that the\noveruse of currently dominant AI systems that are driven by short-term\nobjectives and optimized for avoiding error leads to a society that loses its\ndiversity and flexibility needed for true progress. We couch our concerns in\nthe discourse around the term anti-fragility and show with some examples what\nthreats current methods used for decision making pose for society.",
    "published_date": "2020-06-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "62P25",
      "K.4.2; I.2.0; K.7.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11814v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11807v1",
    "title": "Improving Image Captioning with Better Use of Captions",
    "authors": [
      "Zhan Shi",
      "Xu Zhou",
      "Xipeng Qiu",
      "Xiaodan Zhu"
    ],
    "author_ids": [],
    "abstract": "Image captioning is a multimodal problem that has drawn extensive attention\nin both the natural language processing and computer vision community. In this\npaper, we present a novel image captioning architecture to better explore\nsemantics available in captions and leverage that to enhance both image\nrepresentation and caption generation. Our models first construct\ncaption-guided visual relationship graphs that introduce beneficial inductive\nbias using weakly supervised multi-instance learning. The representation is\nthen enhanced with neighbouring and contextual nodes with their textual and\nvisual features. During generation, the model further incorporates visual\nrelationships using multi-task learning for jointly predicting word and\nobject/predicate tag sequences. We perform extensive experiments on the MSCOCO\ndataset, showing that the proposed framework significantly outperforms the\nbaselines, resulting in the state-of-the-art performance under a wide range of\nevaluation metrics.",
    "published_date": "2020-06-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11807v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11737v1",
    "title": "Verifying Individual Fairness in Machine Learning Models",
    "authors": [
      "Philips George John",
      "Deepak Vijaykeerthy",
      "Diptikalyan Saha"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of whether a given decision model, working with\nstructured data, has individual fairness. Following the work of Dwork, a model\nis individually biased (or unfair) if there is a pair of valid inputs which are\nclose to each other (according to an appropriate metric) but are treated\ndifferently by the model (different class label, or large difference in\noutput), and it is unbiased (or fair) if no such pair exists. Our objective is\nto construct verifiers for proving individual fairness of a given model, and we\ndo so by considering appropriate relaxations of the problem. We construct\nverifiers which are sound but not complete for linear classifiers, and\nkernelized polynomial/radial basis function classifiers. We also report the\nexperimental results of evaluating our proposed algorithms on publicly\navailable datasets.",
    "published_date": "2020-06-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11737v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12495v1",
    "title": "Using graph theory and social media data to assess cultural ecosystem services in coastal areas: Method development and application",
    "authors": [
      "Ana Ruiz-Frau",
      "Andres Ospina-Alvarez",
      "Sebastián Villasante",
      "Pablo Pita",
      "Isidro Maya-Jariego",
      "Silvia de Juan Mohan"
    ],
    "author_ids": [],
    "abstract": "The use of social media (SM) data has emerged as a promising tool for the\nassessment of cultural ecosystem services (CES). Most studies have focused on\nthe use of single SM platforms and on the analysis of photo content to assess\nthe demand for CES. Here, we introduce a novel methodology for the assessment\nof CES using SM data through the application of graph theory network analyses\n(GTNA) on hashtags associated to SM posts and compare it to photo content\nanalysis. We applied the proposed methodology on two SM platforms, Instagram\nand Twitter, on three worldwide known case study areas, namely Great Barrier\nReef, Galapagos Islands and Easter Island. Our results indicate that the\nanalysis of hashtags through graph theory offers similar capabilities to photo\ncontent analysis in the assessment of CES provision and the identification of\nCES providers. More importantly, GTNA provides greater capabilities at\nidentifying relational values and eudaimonic aspects associated to nature,\nelusive aspects for photo content analysis. In addition, GTNA contributes to\nthe reduction of the interpreter's bias associated to photo content analyses,\nsince GTNA is based on the tags provided by the users themselves. The study\nalso highlights the importance of considering data from different social media\nplatforms, as the type of users and the information offered by these platforms\ncan show different CES attributes. The ease of application and short computing\nprocessing times involved in the application of GTNA makes it a cost-effective\nmethod with the potential of being applied to large geographical scales.",
    "published_date": "2020-06-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "14J60 (Primary) 92F05, 91D30, 91B76 (Secondary)",
      "J.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12495v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.11642v1",
    "title": "MDR Cluster-Debias: A Nonlinear WordEmbedding Debiasing Pipeline",
    "authors": [
      "Yuhao Du",
      "Kenneth Joseph"
    ],
    "author_ids": [],
    "abstract": "Existing methods for debiasing word embeddings often do so only\nsuperficially, in that words that are stereotypically associated with, e.g., a\nparticular gender in the original embedding space can still be clustered\ntogether in the debiased space. However, there has yet to be a study that\nexplores why this residual clustering exists, and how it might be addressed.\nThe present work fills this gap. We identify two potential reasons for which\nresidual bias exists and develop a new pipeline, MDR Cluster-Debias, to\nmitigate this bias. We explore the strengths and weaknesses of our method,\nfinding that it significantly outperforms other existing debiasing approaches\non a variety of upstream bias tests but achieves limited improvement on\ndecreasing gender bias in a downstream task. This indicates that word\nembeddings encode gender bias in still other ways, not necessarily captured by\nupstream tests.",
    "published_date": "2020-06-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11642v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.06711v2",
    "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual Recognition Tasks",
    "authors": [
      "Derek S. Prijatelj",
      "Mel McCurrie",
      "Walter J. Scheirer"
    ],
    "author_ids": [],
    "abstract": "An interesting development in automatic visual recognition has been the\nemergence of tasks where it is not possible to assign objective labels to\nimages, yet still feasible to collect annotations that reflect human judgements\nabout them. Machine learning-based predictors for these tasks rely on\nsupervised training that models the behavior of the annotators, i.e., what\nwould the average person's judgement be for an image? A key open question for\nthis type of work, especially for applications where inconsistency with human\nbehavior can lead to ethical lapses, is how to evaluate the epistemic\nuncertainty of trained predictors, i.e., the uncertainty that comes from the\npredictor's model. We propose a Bayesian framework for evaluating black box\npredictors in this regime, agnostic to the predictor's internal structure. The\nframework specifies how to estimate the epistemic uncertainty that comes from\nthe predictor with respect to human labels by approximating a conditional\ndistribution and producing a credible interval for the predictions and their\nmeasures of performance. The framework is successfully applied to four image\nclassification tasks that use subjective human judgements: facial beauty\nassessment, social attribute assignment, apparent age estimation, and ambiguous\nscene labeling.",
    "published_date": "2020-06-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.06711v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.13025v2",
    "title": "Fair Active Learning",
    "authors": [
      "Hadis Anahideh",
      "Abolfazl Asudeh",
      "Saravanan Thirumuruganathan"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) is increasingly being used in high-stakes applications\nimpacting society. Therefore, it is of critical importance that ML models do\nnot propagate discrimination. Collecting accurate labeled data in societal\napplications is challenging and costly. Active learning is a promising approach\nto build an accurate classifier by interactively querying an oracle within a\nlabeling budget. We design algorithms for fair active learning that carefully\nselects data points to be labeled so as to balance model accuracy and fairness.\nSpecifically, we focus on demographic parity - a widely used measure of\nfairness. Extensive experiments over benchmark datasets demonstrate the\neffectiveness of our proposed approach.",
    "published_date": "2020-06-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.13025v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11478v1",
    "title": "Representation via Representations: Domain Generalization via Adversarially Learned Invariant Representations",
    "authors": [
      "Zhun Deng",
      "Frances Ding",
      "Cynthia Dwork",
      "Rachel Hong",
      "Giovanni Parmigiani",
      "Prasad Patil",
      "Pragya Sur"
    ],
    "author_ids": [],
    "abstract": "We investigate the power of censoring techniques, first developed for\nlearning {\\em fair representations}, to address domain generalization. We\nexamine {\\em adversarial} censoring techniques for learning invariant\nrepresentations from multiple \"studies\" (or domains), where each study is drawn\naccording to a distribution on domains. The mapping is used at test time to\nclassify instances from a new domain. In many contexts, such as medical\nforecasting, domain generalization from studies in populous areas (where data\nare plentiful), to geographically remote populations (for which no training\ndata exist) provides fairness of a different flavor, not anticipated in\nprevious work on algorithmic fairness.\n  We study an adversarial loss function for $k$ domains and precisely\ncharacterize its limiting behavior as $k$ grows, formalizing and proving the\nintuition, backed by experiments, that observing data from a larger number of\ndomains helps. The limiting results are accompanied by non-asymptotic\nlearning-theoretic bounds. Furthermore, we obtain sufficient conditions for\ngood worst-case prediction performance of our algorithm on previously unseen\ndomains. Finally, we decompose our mappings into two components and provide a\ncomplete characterization of invariance in terms of this decomposition. To our\nknowledge, our results provide the first formal guarantees of these kinds for\nadversarial invariant domain generalization.",
    "published_date": "2020-06-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11478v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12995v1",
    "title": "Mitigating Bias in Online Microfinance Platforms: A Case Study on Kiva.org",
    "authors": [
      "Soumajyoti Sarkar",
      "Hamidreza Alvari"
    ],
    "author_ids": [],
    "abstract": "Over the last couple of decades in the lending industry, financial\ndisintermediation has occurred on a global scale. Traditionally, even for small\nsupply of funds, banks would act as the conduit between the funds and the\nborrowers. It has now been possible to overcome some of the obstacles\nassociated with such supply of funds with the advent of online platforms like\nKiva, Prosper, LendingClub. Kiva for example, works with Micro Finance\nInstitutions (MFIs) in developing countries to build Internet profiles of\nborrowers with a brief biography, loan requested, loan term, and purpose. Kiva,\nin particular, allows lenders to fund projects in different sectors through\ngroup or individual funding. Traditional research studies have investigated\nvarious factors behind lender preferences purely from the perspective of loan\nattributes and only until recently have some cross-country cultural preferences\nbeen investigated. In this paper, we investigate lender perceptions of economic\nfactors of the borrower countries in relation to their preferences towards\nloans associated with different sectors. We find that the influence from\neconomic factors and loan attributes can have substantially different roles to\nplay for different sectors in achieving faster funding. We formally investigate\nand quantify the hidden biases prevalent in different loan sectors using recent\ntools from causal inference and regression models that rely on Bayesian\nvariable selection methods. We then extend these models to incorporate fairness\nconstraints based on our empirical analysis and find that such models can still\nachieve near comparable results with respect to baseline regression models.",
    "published_date": "2020-06-20T00:00:00",
    "year": 2020,
    "categories": [
      "econ.EM",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12995v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11440v5",
    "title": "Local Convolutions Cause an Implicit Bias towards High Frequency Adversarial Examples",
    "authors": [
      "Josue Ortega Caro",
      "Yilong Ju",
      "Ryan Pyle",
      "Sourav Dey",
      "Wieland Brendel",
      "Fabio Anselmi",
      "Ankit Patel"
    ],
    "author_ids": [],
    "abstract": "Adversarial Attacks are still a significant challenge for neural networks.\nRecent work has shown that adversarial perturbations typically contain\nhigh-frequency features, but the root cause of this phenomenon remains unknown.\nInspired by theoretical work on linear full-width convolutional models, we\nhypothesize that the local (i.e. bounded-width) convolutional operations\ncommonly used in current neural networks are implicitly biased to learn high\nfrequency features, and that this is one of the root causes of high frequency\nadversarial examples. To test this hypothesis, we analyzed the impact of\ndifferent choices of linear and nonlinear architectures on the implicit bias of\nthe learned features and the adversarial perturbations, in both spatial and\nfrequency domains. We find that the high-frequency adversarial perturbations\nare critically dependent on the convolution operation because the\nspatially-limited nature of local convolutions induces an implicit bias towards\nhigh frequency features. The explanation for the latter involves the Fourier\nUncertainty Principle: a spatially-limited (local in the space domain) filter\ncannot also be frequency-limited (local in the frequency domain). Furthermore,\nusing larger convolution kernel sizes or avoiding convolutions (e.g. by using\nVision Transformers architecture) significantly reduces this high frequency\nbias, but not the overall susceptibility to attacks. Looking forward, our work\nstrongly suggests that understanding and controlling the implicit bias of\narchitectures will be essential for achieving adversarial robustness.",
    "published_date": "2020-06-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11440v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11439v1",
    "title": "Two Simple Ways to Learn Individual Fairness Metrics from Data",
    "authors": [
      "Debarghya Mukherjee",
      "Mikhail Yurochkin",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "Individual fairness is an intuitive definition of algorithmic fairness that\naddresses some of the drawbacks of group fairness. Despite its benefits, it\ndepends on a task specific fair metric that encodes our intuition of what is\nfair and unfair for the ML task at hand, and the lack of a widely accepted fair\nmetric for many ML tasks is the main barrier to broader adoption of individual\nfairness. In this paper, we present two simple ways to learn fair metrics from\na variety of data types. We show empirically that fair training with the\nlearned metrics leads to improved fairness on three machine learning tasks\nsusceptible to gender and racial biases. We also provide theoretical guarantees\non the statistical performance of both approaches.",
    "published_date": "2020-06-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11439v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11385v2",
    "title": "Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution",
    "authors": [
      "Benyamin Ghojogh",
      "Fakhri Karray",
      "Mark Crowley"
    ],
    "author_ids": [],
    "abstract": "We propose a new embedding method, named Quantile-Quantile Embedding (QQE),\nfor distribution transformation and manifold embedding with the ability to\nchoose the embedding distribution. QQE, which uses the concept of\nquantile-quantile plot from visual statistical tests, can transform the\ndistribution of data to any theoretical desired distribution or empirical\nreference sample. Moreover, QQE gives the user a choice of embedding\ndistribution in embedding the manifold of data into the low dimensional\nembedding space. It can also be used for modifying the embedding distribution\nof other dimensionality reduction methods, such as PCA, t-SNE, and deep metric\nlearning, for better representation or visualization of data. We propose QQE in\nboth unsupervised and supervised forms. QQE can also transform a distribution\nto either an exact reference distribution or its shape. We show that QQE allows\nfor better discrimination of classes in some cases. Our experiments on\ndifferent synthetic and image datasets show the effectiveness of the proposed\nembedding method.",
    "published_date": "2020-06-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11385v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11350v3",
    "title": "Achieving Fairness via Post-Processing in Web-Scale Recommender Systems",
    "authors": [
      "Preetam Nandy",
      "Cyrus Diciccio",
      "Divya Venugopalan",
      "Heloise Logan",
      "Kinjal Basu",
      "Noureddine El Karoui"
    ],
    "author_ids": [],
    "abstract": "Building fair recommender systems is a challenging and crucial area of study\ndue to its immense impact on society. We extended the definitions of two\ncommonly accepted notions of fairness to recommender systems, namely equality\nof opportunity and equalized odds. These fairness measures ensure that equally\n\"qualified\" (or \"unqualified\") candidates are treated equally regardless of\ntheir protected attribute status (such as gender or race). We propose scalable\nmethods for achieving equality of opportunity and equalized odds in rankings in\nthe presence of position bias, which commonly plagues data generated from\nrecommender systems. Our algorithms are model agnostic in the sense that they\ndepend only on the final scores provided by a model, making them easily\napplicable to virtually all web-scale recommender systems. We conduct extensive\nsimulations as well as real-world experiments to show the efficacy of our\napproach.",
    "published_date": "2020-06-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME",
      "62P30, 62A01"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11350v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11009v2",
    "title": "Fair clustering via equitable group representations",
    "authors": [
      "Mohsen Abbasi",
      "Aditya Bhaskara",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "What does it mean for a clustering to be fair? One popular approach seeks to\nensure that each cluster contains groups in (roughly) the same proportion in\nwhich they exist in the population. The normative principle at play is balance:\nany cluster might act as a representative of the data, and thus should reflect\nits diversity.\n  But clustering also captures a different form of representativeness. A core\nprinciple in most clustering problems is that a cluster center should be\nrepresentative of the cluster it represents, by being \"close\" to the points\nassociated with it. This is so that we can effectively replace the points by\ntheir cluster centers without significant loss in fidelity, and indeed is a\ncommon \"use case\" for clustering. For such a clustering to be fair, the centers\nshould \"represent\" different groups equally well. We call such a clustering a\ngroup-representative clustering.\n  In this paper, we study the structure and computation of group-representative\nclusterings. We show that this notion naturally parallels the development of\nfairness notions in classification, with direct analogs of ideas like\ndemographic parity and equal opportunity. We demonstrate how these notions are\ndistinct from and cannot be captured by balance-based notions of fairness. We\npresent approximation algorithms for group representative $k$-median clustering\nand couple this with an empirical evaluation on various real-world data sets.",
    "published_date": "2020-06-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11009v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10903v1",
    "title": "Exploring Weight Importance and Hessian Bias in Model Pruning",
    "authors": [
      "Mingchen Li",
      "Yahya Sattar",
      "Christos Thrampoulidis",
      "Samet Oymak"
    ],
    "author_ids": [],
    "abstract": "Model pruning is an essential procedure for building compact and\ncomputationally-efficient machine learning models. A key feature of a good\npruning algorithm is that it accurately quantifies the relative importance of\nthe model weights. While model pruning has a rich history, we still don't have\na full grasp of the pruning mechanics even for relatively simple problems\ninvolving linear models or shallow neural nets. In this work, we provide a\nprincipled exploration of pruning by building on a natural notion of\nimportance. For linear models, we show that this notion of importance is\ncaptured by covariance scaling which connects to the well-known Hessian-based\npruning. We then derive asymptotic formulas that allow us to precisely compare\nthe performance of different pruning methods. For neural networks, we\ndemonstrate that the importance can be at odds with larger magnitudes and\nproper initialization is critical for magnitude-based pruning. Specifically, we\nidentify settings in which weights become more important despite becoming\nsmaller, which in turn leads to a catastrophic failure of magnitude-based\npruning. Our results also elucidate that implicit regularization in the form of\nHessian structure has a catalytic role in identifying the important weights,\nwhich dictate the pruning performance.",
    "published_date": "2020-06-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10903v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.11659v1",
    "title": "Proceedings of the KG-BIAS Workshop 2020 at AKBC 2020",
    "authors": [
      "Edgar Meij",
      "Tara Safavi",
      "Chenyan Xiong",
      "Gianluca Demartini",
      "Miriam Redi",
      "Fatma Özcan"
    ],
    "author_ids": [],
    "abstract": "The KG-BIAS 2020 workshop touches on biases and how they surface in knowledge\ngraphs (KGs), biases in the source data that is used to create KGs, methods for\nmeasuring or remediating bias in KGs, but also identifying other biases such as\nhow and which languages are represented in automatically constructed KGs or how\npersonal KGs might incur inherent biases. The goal of this workshop is to\nuncover how various types of biases are introduced into KGs, investigate how to\nmeasure, and propose methods to remediate them.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.11659v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.16879v1",
    "title": "Combating Anti-Blackness in the AI Community",
    "authors": [
      "Devin Guillory"
    ],
    "author_ids": [],
    "abstract": "In response to a national and international awakening on the issues of\nanti-Blackness and systemic discrimination, we have penned this piece to serve\nas a resource for allies in the AI community who are wondering how they can\nmore effectively engage with dismantling racist systems. This work aims to help\nelucidate areas where the AI community actively and passively contributes to\nanti-Blackness and offers actionable items on ways to reduce harm.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16879v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10808v1",
    "title": "Data-Driven Game Development: Ethical Considerations",
    "authors": [
      "Magy Seif El-Nasr",
      "Erica Kleinman"
    ],
    "author_ids": [],
    "abstract": "In recent years, the games industry has made a major move towards data-driven\ndevelopment, using data analytics and player modeling to inform design\ndecisions. Data-driven techniques are beneficial as they allow for the study of\nplayer behavior at scale, making them very applicable to modern digital game\ndevelopment. However, with this move towards data driven decision-making comes\na number of ethical concerns. Previous work in player modeling as well as work\nin the fields of AI and machine learning have demonstrated several ways in\nwhich algorithmic decision-making can be flawed due to data or algorithmic bias\nor lack of data from specific groups. Further, black box algorithms create a\ntrust problem due to lack of interpretability and transparency of the results\nor models developed based on the data, requiring blind faith in the results. In\nthis position paper, we discuss several factors affecting the use of game data\nin the development cycle. In addition to issues raised by previous work, we\nalso raise issues with algorithms marginalizing certain player groups and flaws\nin the resulting models due to their inability to reason about situational\nfactors affecting players' decisions. Further, we outline some work that seeks\nto address these problems and identify some open problems concerning ethics and\ngame data science.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10808v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10732v4",
    "title": "When Does Preconditioning Help or Hurt Generalization?",
    "authors": [
      "Shun-ichi Amari",
      "Jimmy Ba",
      "Roger Grosse",
      "Xuechen Li",
      "Atsushi Nitanda",
      "Taiji Suzuki",
      "Denny Wu",
      "Ji Xu"
    ],
    "author_ids": [],
    "abstract": "While second order optimizers such as natural gradient descent (NGD) often\nspeed up optimization, their effect on generalization has been called into\nquestion. This work presents a more nuanced view on how the \\textit{implicit\nbias} of first- and second-order methods affects the comparison of\ngeneralization properties. We provide an exact asymptotic bias-variance\ndecomposition of the generalization error of overparameterized ridgeless\nregression under a general class of preconditioner $\\boldsymbol{P}$, and\nconsider the inverse population Fisher information matrix (used in NGD) as a\nparticular example. We determine the optimal $\\boldsymbol{P}$ for both the bias\nand variance, and find that the relative generalization performance of\ndifferent optimizers depends on the label noise and the \"shape\" of the signal\n(true parameters): when the labels are noisy, the model is misspecified, or the\nsignal is misaligned with the features, NGD can achieve lower risk; conversely,\nGD generalizes better than NGD under clean labels, a well-specified model, or\naligned signal. Based on this analysis, we discuss several approaches to manage\nthe bias-variance tradeoff, and the potential benefit of interpolating between\nGD and NGD. We then extend our analysis to regression in the reproducing kernel\nHilbert space and demonstrate that preconditioned GD can decrease the\npopulation risk faster than GD. Lastly, we empirically compare the\ngeneralization error of first- and second-order optimizers in neural network\nexperiments, and observe robust trends matching our theoretical analysis.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10732v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10667v1",
    "title": "Towards Threshold Invariant Fair Classification",
    "authors": [
      "Mingliang Chen",
      "Min Wu"
    ],
    "author_ids": [],
    "abstract": "Effective machine learning models can automatically learn useful information\nfrom a large quantity of data and provide decisions in a high accuracy. These\nmodels may, however, lead to unfair predictions in certain sense among the\npopulation groups of interest, where the grouping is based on such sensitive\nattributes as race and gender. Various fairness definitions, such as\ndemographic parity and equalized odds, were proposed in prior art to ensure\nthat decisions guided by the machine learning models are equitable.\nUnfortunately, the \"fair\" model trained with these fairness definitions is\nthreshold sensitive, i.e., the condition of fairness may no longer hold true\nwhen tuning the decision threshold. This paper introduces the notion of\nthreshold invariant fairness, which enforces equitable performances across\ndifferent groups independent of the decision threshold. To achieve this goal,\nthis paper proposes to equalize the risk distributions among the groups via two\napproximation methods. Experimental results demonstrate that the proposed\nmethodology is effective to alleviate the threshold sensitivity in machine\nlearning models designed to achieve fairness.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10667v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10626v1",
    "title": "Use of in-the-wild images for anomaly detection in face anti-spoofing",
    "authors": [
      "Latifah Abduh",
      "Ioannis Ivrissimtzis"
    ],
    "author_ids": [],
    "abstract": "The traditional approach to face anti-spoofing sees it as a binary\nclassification problem, and binary classifiers are trained and validated on\nspecialized anti-spoofing databases. One of the drawbacks of this approach is\nthat, due to the variability of face spoofing attacks, environmental factors,\nand the typically small sample size, such classifiers do not generalize well to\npreviously unseen databases. Anomaly detection, which approaches face\nanti-spoofing as a one-class classification problem, is emerging as an\nincreasingly popular alternative approach. Nevertheless, in all existing work\non anomaly detection for face anti-spoofing, the proposed training protocols\nutilize images from specialized anti-spoofing databases only, even though only\ncommon images of real faces are needed. Here, we explore the use of in-the-wild\nimages, and images from non-specialized face databases, to train one-class\nclassifiers for face anti-spoofing. Employing a well-established technique, we\ntrain a convolutional autoencoder on real faces and compare the reconstruction\nerror of the input against a threshold to classify a face image accordingly as\neither client or imposter.\n  Our results show that the inclusion in the training set of in-the-wild images\nincreases the discriminating power of the classifier significantly on an unseen\ndatabase, as evidenced by a large increase in the value of the Area Under the\nCurve. In a limitation of our approach, we note that the problem of finding a\nsuitable operating point on the unseen database remains a challenge, as\nevidenced by the values of the Half Total Error Rate.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.NE",
      "I.5.4; I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10626v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10498v2",
    "title": "Neutralizing Self-Selection Bias in Sampling for Sortition",
    "authors": [
      "Bailey Flanigan",
      "Paul Gölz",
      "Anupam Gupta",
      "Ariel Procaccia"
    ],
    "author_ids": [],
    "abstract": "Sortition is a political system in which decisions are made by panels of\nrandomly selected citizens. The process for selecting a sortition panel is\ntraditionally thought of as uniform sampling without replacement, which has\nstrong fairness properties. In practice, however, sampling without replacement\nis not possible since only a fraction of agents is willing to participate in a\npanel when invited, and different demographic groups participate at different\nrates. In order to still produce panels whose composition resembles that of the\npopulation, we develop a sampling algorithm that restores close-to-equal\nrepresentation probabilities for all agents while satisfying meaningful\ndemographic quotas. As part of its input, our algorithm requires probabilities\nindicating how likely each volunteer in the pool was to participate. Since\nthese participation probabilities are not directly observable, we show how to\nlearn them, and demonstrate our approach using data on a real sortition panel\ncombined with information on the general population in the form of publicly\navailable survey data.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10498v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.10483v5",
    "title": "Algorithmic Decision Making with Conditional Fairness",
    "authors": [
      "Renzhe Xu",
      "Peng Cui",
      "Kun Kuang",
      "Bo Li",
      "Linjun Zhou",
      "Zheyan Shen",
      "Wei Cui"
    ],
    "author_ids": [],
    "abstract": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10483v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10460v3",
    "title": "Confident Off-Policy Evaluation and Selection through Self-Normalized Importance Weighting",
    "authors": [
      "Ilja Kuzborskij",
      "Claire Vernade",
      "András György",
      "Csaba Szepesvári"
    ],
    "author_ids": [],
    "abstract": "We consider off-policy evaluation in the contextual bandit setting for the\npurpose of obtaining a robust off-policy selection strategy, where the\nselection strategy is evaluated based on the value of the chosen policy in a\nset of proposal (target) policies. We propose a new method to compute a lower\nbound on the value of an arbitrary target policy given some logged data in\ncontextual bandits for a desired coverage. The lower bound is built around the\nso-called Self-normalized Importance Weighting (SN) estimator. It combines the\nuse of a semi-empirical Efron-Stein tail inequality to control the\nconcentration and a new multiplicative (rather than additive) control of the\nbias. The new approach is evaluated on a number of synthetic and real datasets\nand is found to be superior to its main competitors, both in terms of tightness\nof the confidence intervals and the quality of the policies chosen.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10460v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10336v1",
    "title": "Cascaded Regression Tracking: Towards Online Hard Distractor Discrimination",
    "authors": [
      "Ning Wang",
      "Wengang Zhou",
      "Qi Tian",
      "Houqiang Li"
    ],
    "author_ids": [],
    "abstract": "Visual tracking can be easily disturbed by similar surrounding objects. Such\nobjects as hard distractors, even though being the minority among negative\nsamples, increase the risk of target drift and model corruption, which deserve\nadditional attention in online tracking and model update. To enhance the\ntracking robustness, in this paper, we propose a cascaded regression tracker\nwith two sequential stages. In the first stage, we filter out abundant\neasily-identified negative candidates via an efficient convolutional\nregression. In the second stage, a discrete sampling based ridge regression is\ndesigned to double-check the remaining ambiguous hard samples, which serves as\nan alternative of fully-connected layers and benefits from the closed-form\nsolver for efficient learning. Extensive experiments are conducted on 11\nchallenging tracking benchmarks including OTB-2013, OTB-2015, VOT2018, VOT2019,\nUAV123, Temple-Color, NfS, TrackingNet, LaSOT, UAV20L, and OxUvA. The proposed\nmethod achieves state-of-the-art performance on prevalent benchmarks, while\nrunning in a real-time speed.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10336v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10297v1",
    "title": "Joint Contrastive Learning for Unsupervised Domain Adaptation",
    "authors": [
      "Changhwa Park",
      "Jonghyun Lee",
      "Jaeyoon Yoo",
      "Minhoe Hur",
      "Sungroh Yoon"
    ],
    "author_ids": [],
    "abstract": "Enhancing feature transferability by matching marginal distributions has led\nto improvements in domain adaptation, although this is at the expense of\nfeature discrimination. In particular, the ideal joint hypothesis error in the\ntarget error upper bound, which was previously considered to be minute, has\nbeen found to be significant, impairing its theoretical guarantee. In this\npaper, we propose an alternative upper bound on the target error that\nexplicitly considers the joint error to render it more manageable. With the\ntheoretical analysis, we suggest a joint optimization framework that combines\nthe source and target domains. Further, we introduce Joint Contrastive Learning\n(JCL) to find class-level discriminative features, which is essential for\nminimizing the joint error. With a solid theoretical framework, JCL employs\ncontrastive loss to maximize the mutual information between a feature and its\nlabel, which is equivalent to maximizing the Jensen-Shannon divergence between\nconditional distributions. Experiments on two real-world datasets demonstrate\nthat JCL outperforms the state-of-the-art methods.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10297v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10288v3",
    "title": "Individual Calibration with Randomized Forecasting",
    "authors": [
      "Shengjia Zhao",
      "Tengyu Ma",
      "Stefano Ermon"
    ],
    "author_ids": [],
    "abstract": "Machine learning applications often require calibrated predictions, e.g. a\n90\\% credible interval should contain the true outcome 90\\% of the times.\nHowever, typical definitions of calibration only require this to hold on\naverage, and offer no guarantees on predictions made on individual samples.\nThus, predictions can be systematically over or under confident on certain\nsubgroups, leading to issues of fairness and potential vulnerabilities. We show\nthat calibration for individual samples is possible in the regression setup if\nthe predictions are randomized, i.e. outputting randomized credible intervals.\nRandomization removes systematic bias by trading off bias with variance. We\ndesign a training objective to enforce individual calibration and use it to\ntrain randomized regression functions. The resulting models are more calibrated\nfor arbitrarily chosen subgroups of the data, and can achieve higher utility in\ndecision making against adversaries that exploit miscalibrated predictions.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10288v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12622v2",
    "title": "WD3: Taming the Estimation Bias in Deep Reinforcement Learning",
    "authors": [
      "Qiang He",
      "Xinwen Hou"
    ],
    "author_ids": [],
    "abstract": "The overestimation phenomenon caused by function approximation is a\nwell-known issue in value-based reinforcement learning algorithms such as deep\nQ-networks and DDPG, which could lead to suboptimal policies. To address this\nissue, TD3 takes the minimum value between a pair of critics. In this paper, we\nshow that the TD3 algorithm introduces underestimation bias in mild\nassumptions. To obtain a more precise estimation for value function, we unify\nthese two opposites and propose a novel algorithm \\underline{W}eighted\n\\underline{D}elayed \\underline{D}eep \\underline{D}eterministic Policy Gradient\n(WD3), which can eliminate the estimation bias and further improve the\nperformance by weighting a pair of critics. To demonstrate the effectiveness of\nWD3, we compare the learning process of value function between DDPG, TD3, and\nWD3. The results verify that our algorithm does eliminate the estimation error\nof value functions. Furthermore, we evaluate our algorithm on the continuous\ncontrol tasks. We observe that in each test task, the performance of WD3\nconsistently outperforms, or at the very least matches, that of the\nstate-of-the-art algorithms\\footnote{Our code is available\nat~\\href{https://sites.google.com/view/ictai20-wd3/}{https://sites.google.com/view/ictai20-wd3/}.}.",
    "published_date": "2020-06-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12622v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10194v5",
    "title": "Gender Inequality in Research Productivity During the COVID-19 Pandemic",
    "authors": [
      "Ruomeng Cui",
      "Hao Ding",
      "Feng Zhu"
    ],
    "author_ids": [],
    "abstract": "We study the disproportionate impact of the lockdown as a result of the\nCOVID-19 outbreak on female and male academics' research productivity in social\nscience. The lockdown has caused substantial disruptions to academic\nactivities, requiring people to work from home. How this disruption affects\nproductivity and the related gender equity is an important operations and\nsocietal question. We collect data from the largest open-access preprint\nrepository for social science on 41,858 research preprints in 18 disciplines\nproduced by 76,832 authors across 25 countries over a span of two years. We use\na difference-in-differences approach leveraging the exogenous pandemic shock.\nOur results indicate that, in the 10 weeks after the lockdown in the United\nStates, although the total research productivity increased by 35%, female\nacademics' productivity dropped by 13.9% relative to that of male academics. We\nalso show that several disciplines drive such gender inequality. Finally, we\nfind that this intensified productivity gap is more pronounced for academics in\ntop-ranked universities, and the effect exists in six other countries. Our work\npoints out the fairness issue in productivity caused by the lockdown, a finding\nthat universities will find helpful when evaluating faculty productivity. It\nalso helps organizations realize the potential unintended consequences that can\narise from telecommuting.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10194v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.12621v4",
    "title": "Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning",
    "authors": [
      "Vedant Nanda",
      "Samuel Dooley",
      "Sahil Singla",
      "Soheil Feizi",
      "John P. Dickerson"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) are increasingly used in real-world applications\n(e.g. facial recognition). This has resulted in concerns about the fairness of\ndecisions made by these models. Various notions and measures of fairness have\nbeen proposed to ensure that a decision-making system does not\ndisproportionately harm (or benefit) particular subgroups of the population. In\nthis paper, we argue that traditional notions of fairness that are only based\non models' outputs are not sufficient when the model is vulnerable to\nadversarial attacks. We argue that in some cases, it may be easier for an\nattacker to target a particular subgroup, resulting in a form of\n\\textit{robustness bias}. We show that measuring robustness bias is a\nchallenging task for DNNs and propose two methods to measure this form of bias.\nWe then conduct an empirical study on state-of-the-art neural networks on\ncommonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and\nUTKFace and show that in almost all cases there are subgroups (in some cases\nbased on sensitive attributes like race, gender, etc) which are less robust and\nare thus at a disadvantage. We argue that this kind of bias arises due to both\nthe data distribution and the highly complex nature of the learned decision\nboundary in the case of DNNs, thus making mitigation of such biases a\nnon-trivial task. Our results show that robustness bias is an important\ncriterion to consider while auditing real-world systems that rely on DNNs for\ndecision making. Code to reproduce all our results can be found here:\n\\url{https://github.com/nvedant07/Fairness-Through-Robustness}",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12621v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.10148v1",
    "title": "The Essential Role of Empirical Validation in Legislative Redistricting Simulation",
    "authors": [
      "Benjamin Fifield",
      "Kosuke Imai",
      "Jun Kawahara",
      "Christopher T. Kenny"
    ],
    "author_ids": [],
    "abstract": "As granular data about elections and voters become available, redistricting\nsimulation methods are playing an increasingly important role when legislatures\nadopt redistricting plans and courts determine their legality. These simulation\nmethods are designed to yield a representative sample of all redistricting\nplans that satisfy statutory guidelines and requirements such as contiguity,\npopulation parity, and compactness. A proposed redistricting plan can be\nconsidered gerrymandered if it constitutes an outlier relative to this sample\naccording to partisan fairness metrics. Despite their growing use, an\ninsufficient effort has been made to empirically validate the accuracy of the\nsimulation methods. We apply a recently developed computational method that can\nefficiently enumerate all possible redistricting plans and yield an independent\nuniform sample from this population. We show that this algorithm scales to a\nstate with a couple of hundred geographical units. Finally, we empirically\nexamine how existing simulation methods perform on realistic validation data\nsets.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10148v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.10085v2",
    "title": "Socially Fair k-Means Clustering",
    "authors": [
      "Mehrdad Ghadiri",
      "Samira Samadi",
      "Santosh Vempala"
    ],
    "author_ids": [],
    "abstract": "We show that the popular k-means clustering algorithm (Lloyd's heuristic),\nused for a variety of scientific data, can result in outcomes that are\nunfavorable to subgroups of data (e.g., demographic groups). Such biased\nclusterings can have deleterious implications for human-centric applications\nsuch as resource allocation. We present a fair k-means objective and algorithm\nto choose cluster centers that provide equitable costs for different groups.\nThe algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means,\ninheriting its simplicity, efficiency, and stability. In comparison with\nstandard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits\nunbiased performance by ensuring that all groups have equal costs in the output\nk-clustering, while incurring a negligible increase in running time, thus\nmaking it a viable fair option wherever k-means is currently used.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10085v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.12379v1",
    "title": "Deep Learning feature selection to unhide demographic recommender systems factors",
    "authors": [
      "Jesús Bobadilla",
      "Ángel González-Prieto",
      "Fernando Ortega",
      "Raúl Lara-Cabrera"
    ],
    "author_ids": [],
    "abstract": "Extracting demographic features from hidden factors is an innovative concept\nthat provides multiple and relevant applications. The matrix factorization\nmodel generates factors which do not incorporate semantic knowledge. This paper\nprovides a deep learning-based method: DeepUnHide, able to extract demographic\ninformation from the users and items factors in collaborative filtering\nrecommender systems. The core of the proposed method is the gradient-based\nlocalization used in the image processing literature to highlight the\nrepresentative areas of each classification class. Validation experiments make\nuse of two public datasets and current baselines. Results show the superiority\nof DeepUnHide to make feature selection and demographic classification,\ncompared to the state of art of feature selection methods. Relevant and direct\napplications include recommendations explanation, fairness in collaborative\nfiltering and recommendation to groups of users.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "stat.ML",
      "I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.12379v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2007.12269v1",
    "title": "A Comprehensive Review of Deep Learning Applications in Hydrology and Water Resources",
    "authors": [
      "Muhammed Sit",
      "Bekir Z. Demiray",
      "Zhongrun Xiang",
      "Gregory J. Ewing",
      "Yusuf Sermet",
      "Ibrahim Demir"
    ],
    "author_ids": [],
    "abstract": "The global volume of digital data is expected to reach 175 zettabytes by\n2025. The volume, variety, and velocity of water-related data are increasing\ndue to large-scale sensor networks and increased attention to topics such as\ndisaster response, water resources management, and climate change. Combined\nwith the growing availability of computational resources and popularity of deep\nlearning, these data are transformed into actionable and practical knowledge,\nrevolutionizing the water industry. In this article, a systematic review of\nliterature is conducted to identify existing research which incorporates deep\nlearning methods in the water sector, with regard to monitoring, management,\ngovernance and communication of water resources. The study provides a\ncomprehensive review of state-of-the-art deep learning approaches used in the\nwater industry for generation, prediction, enhancement, and classification\ntasks, and serves as a guide for how to utilize available deep learning methods\nfor future water resources challenges. Key issues and challenges in the\napplication of these techniques in the water domain are discussed, including\nthe ethics of these technologies for decision-making in water resources\nmanagement and governance. Finally, we provide recommendations and future\ndirections for the application of deep learning models in hydrology and water\nresources.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "physics.geo-ph",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12269v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.09903v1",
    "title": "Shape optimization for superconductors governed by H(curl)-elliptic variational inequalities",
    "authors": [
      "Antoine Laurain",
      "Malte Winckler",
      "Irwin Yousept"
    ],
    "author_ids": [],
    "abstract": "This paper is devoted to the theoretical and numerical study of an optimal\ndesign problem in high-temperature superconductivity (HTS). The shape\noptimization problem is to find an optimal superconductor shape which minimizes\na certain cost functional under a given target on the electric field over a\nspecific domain of interest. For the governing PDE-model, we consider an\nelliptic curl-curl variational inequality (VI) of the second kind with an\nL1-type nonlinearity. In particular, the non-smooth VI character and the\ninvolved H(curl)-structure make the corresponding shape sensitivity analysis\nchallenging. To tackle the non-smoothness, a penalized dual VI formulation is\nproposed, leading to the G{\\^a}teaux differentiability of the corresponding\ndual variable mapping. This property allows us to derive the distributed shape\nderivative of the cost functional through rigorous shape calculus on the basis\nof the averaged adjoint method. The developed shape derivative turns out to be\nuniformly stable with respect to the penalization parameter, and strong\nconvergence of the penalized problem is guaranteed. Based on the achieved\ntheoretical findings, we propose 3D numerical solutions, realized using a level\nset algorithm and a Newton method with the Nedelec edge element discretization.\nNumerical results indicate a favourable and efficient performance of the\nproposed approach for a specific HTS application in superconducting shielding.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09903v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.10531v1",
    "title": "LimeOut: An Ensemble Approach To Improve Process Fairness",
    "authors": [
      "Vaishnavi Bhargava",
      "Miguel Couceiro",
      "Amedeo Napoli"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence and Machine Learning are becoming increasingly\npresent in several aspects of human life, especially, those dealing with\ndecision making. Many of these algorithmic decisions are taken without human\nsupervision and through decision making processes that are not transparent.\nThis raises concerns regarding the potential bias of these processes towards\ncertain groups of society, which may entail unfair results and, possibly,\nviolations of human rights. Dealing with such biased models is one of the major\nconcerns to maintain the public trust.\n  In this paper, we address the question of process or procedural fairness.\nMore precisely, we consider the problem of making classifiers fairer by\nreducing their dependence on sensitive features while increasing (or, at least,\nmaintaining) their accuracy. To achieve both, we draw inspiration from\n\"dropout\" techniques in neural based approaches, and propose a framework that\nrelies on \"feature drop-out\" to tackle process fairness. We make use of \"LIME\nExplanations\" to assess a classifier's fairness and to determine the sensitive\nfeatures to remove. This produces a pool of classifiers (through feature\ndropout) whose ensemble is shown empirically to be less dependent on sensitive\nfeatures, and with improved or no impact on accuracy.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.10531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.09717v2",
    "title": "Neural Anisotropy Directions",
    "authors": [
      "Guillermo Ortiz-Jimenez",
      "Apostolos Modas",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pascal Frossard"
    ],
    "author_ids": [],
    "abstract": "In this work, we analyze the role of the network architecture in shaping the\ninductive bias of deep classifiers. To that end, we start by focusing on a very\nsimple problem, i.e., classifying a class of linearly separable distributions,\nand show that, depending on the direction of the discriminative feature of the\ndistribution, many state-of-the-art deep convolutional neural networks (CNNs)\nhave a surprisingly hard time solving this simple task. We then define as\nneural anisotropy directions (NADs) the vectors that encapsulate the\ndirectional inductive bias of an architecture. These vectors, which are\nspecific for each architecture and hence act as a signature, encode the\npreference of a network to separate the input data based on some particular\nfeatures. We provide an efficient method to identify NADs for several CNN\narchitectures and thus reveal their directional inductive biases. Furthermore,\nwe show that, for the CIFAR-10 dataset, NADs characterize the features used by\nCNNs to discriminate between different classes.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09717v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2008.02731v1",
    "title": "Analysing Risk of Coronary Heart Disease through Discriminative Neural Networks",
    "authors": [
      "Ayush Khaneja",
      "Siddharth Srivastava",
      "Astha Rai",
      "A S Cheema",
      "P K Srivastava"
    ],
    "author_ids": [],
    "abstract": "The application of data mining, machine learning and artificial intelligence\ntechniques in the field of diagnostics is not a new concept, and these\ntechniques have been very successfully applied in a variety of applications,\nespecially in dermatology and cancer research. But, in the case of medical\nproblems that involve tests resulting in true or false (binary classification),\nthe data generally has a class imbalance with samples majorly belonging to one\nclass (ex: a patient undergoes a regular test and the results are false). Such\ndisparity in data causes problems when trying to model predictive systems on\nthe data. In critical applications like diagnostics, this class imbalance\ncannot be overlooked and must be given extra attention. In our research, we\ndepict how we can handle this class imbalance through neural networks using a\ndiscriminative model and contrastive loss using a Siamese neural network\nstructure. Such a model does not work on a probability-based approach to\nclassify samples into labels. Instead it uses a distance-based approach to\ndifferentiate between samples classified under different labels. The code is\navailable at https://tinyurl.com/DiscriminativeCHD/",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.02731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.09663v1",
    "title": "Extending the Machine Learning Abstraction Boundary: A Complex Systems Approach to Incorporate Societal Context",
    "authors": [
      "Donald Martin Jr.",
      "Vinodkumar Prabhakaran",
      "Jill Kuhlberg",
      "Andrew Smart",
      "William S. Isaac"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) fairness research tends to focus primarily on\nmathematically-based interventions on often opaque algorithms or models and/or\ntheir immediate inputs and outputs. Such oversimplified mathematical models\nabstract away the underlying societal context where ML models are conceived,\ndeveloped, and ultimately deployed. As fairness itself is a socially\nconstructed concept that originates from that societal context along with the\nmodel inputs and the models themselves, a lack of an in-depth understanding of\nsocietal context can easily undermine the pursuit of ML fairness. In this\npaper, we outline three new tools to improve the comprehension, identification\nand representation of societal context. First, we propose a complex adaptive\nsystems (CAS) based model and definition of societal context that will help\nresearchers and product developers to expand the abstraction boundary of ML\nfairness work to include societal context. Second, we introduce collaborative\ncausal theory formation (CCTF) as a key capability for establishing a\nsociotechnical frame that incorporates diverse mental models and associated\ncausal theories in modeling the problem and solution space for ML-based\nproducts. Finally, we identify community based system dynamics (CBSD) as a\npowerful, transparent and rigorous approach for practicing CCTF during all\nphases of the ML product development process. We conclude with a discussion of\nhow these systems theoretic approaches to understand the societal context\nwithin which sociotechnical systems are embedded can improve the development of\nfair and inclusive ML-based products.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09663v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.09655v1",
    "title": "Fairness-Oriented Semi-Chaotic Genetic Algorithm-Based Channel Assignment Technique for Nodes Starvation Problem in Wireless Mesh Network",
    "authors": [
      "Fuad A. Ghaleb",
      "Bander Ali Saleh Al-rimy",
      "Maznah Kamat",
      "Mohd. Foad Rohani",
      "Shukor Abd Razak"
    ],
    "author_ids": [],
    "abstract": "Multi-Radio Multi-Channel Wireless Mesh Networks (WMNs) have emerged as a\nscalable, reliable, and agile wireless network that supports many types of\ninnovative technologies such as the Internet of Things (IoT) and vehicular\nnetworks. Due to the limited number of orthogonal channels, interference\nbetween channels adversely affects the fair distribution of bandwidth among\nmesh clients, causing node starvation in terms of insufficient bandwidth, which\nimpedes the adoption of WMN as an efficient access technology. Therefore, a\nfair channel assignment is crucial for the mesh clients to utilize the\navailable resources. However, the node starvation problem due to unfair channel\ndistribution has been vastly overlooked during channel assignment by the extant\nresearch. Instead, existing channel assignment algorithms either reduce the\ntotal network interference or maximize the total network throughput, which\nneither guarantees a fair distribution of the channels nor eliminates node\nstarvation. To this end, the Fairness-Oriented Semi-Chaotic Genetic\nAlgorithm-Based Channel Assignment Technique (FA-SCGA-CAA) was proposed in this\npaper for Nodes Starvation Problem in Wireless Mesh Networks. FA-SCGA-CAA\noptimizes fairness based on multiple-criterion using a modified version of the\nGenetic Algorithm (GA). The modification includes proposing a semi-chaotic\ntechnique for creating the primary chromosome with powerful genes. Such a\nchromosome was used to create a strong population that directs the search\ntowards the global minima in an effective and efficient way. The outcome is a\nnonlinear fairness oriented fitness function that aims at maximizing the link\nfairness while minimizing the link interference. Comparison with related work\nshows that the proposed FA_SCGA_CAA reduced the potential nodes starvation by\n22% and improved network capacity utilization by 23%.",
    "published_date": "2020-06-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09655v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.11156v1",
    "title": "Why Stake When You Can Borrow?",
    "authors": [
      "Tarun Chitra",
      "Alex Evans"
    ],
    "author_ids": [],
    "abstract": "As smart contract platforms autonomously manage billions of dollars of\ncapital, quantifying the portfolio risk that investors engender in these\nsystems is increasingly important. Recent work illustrates that Proof of Stake\n(PoS) is vulnerable to financial attacks arising from on-chain lending and has\nworse capital efficiency than Proof of Work (PoW) \\cite{fanti_pos_econ}.\nNumerous methods for improving capital efficiency have been proposed that allow\nstakers to create fungible derivative claims on their staked assets. In this\npaper, we construct a unifying model for studying the security risks of these\nproposals. This model combines birth-death P\\'olya processes and risk models\nadapted from the credit derivatives literature to assess token inequality and\nreturn profiles. We find that there is a sharp transition between 'safe' and\n'unsafe' derivative usage. Surprisingly, we find that contrary to\n\\cite{fanti2019compounding} there exist conditions where derivatives can\n\\emph{reduce} concentration of wealth in these networks. This model also\napplies to Decentralized Finance (DeFi) protocols where staked assets are used\nas insurance. Our theoretical results are validated using agent-based\nsimulation.",
    "published_date": "2020-06-16T00:00:00",
    "year": 2020,
    "categories": [
      "q-fin.GN",
      "cs.MA",
      "q-fin.TR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11156v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.09428v1",
    "title": "Response by the Montreal AI Ethics Institute to the European Commission's Whitepaper on AI",
    "authors": [
      "Abhishek Gupta",
      "Camylle Lanteigne"
    ],
    "author_ids": [],
    "abstract": "In February 2020, the European Commission (EC) published a white paper\nentitled, On Artificial Intelligence - A European approach to excellence and\ntrust. This paper outlines the EC's policy options for the promotion and\nadoption of artificial intelligence (AI) in the European Union. The Montreal AI\nEthics Institute (MAIEI) reviewed this paper and published a response\naddressing the EC's plans to build an \"ecosystem of excellence\" and an\n\"ecosystem of trust,\" as well as the safety and liability implications of AI,\nthe internet of things (IoT), and robotics.\n  MAIEI provides 15 recommendations in relation to the sections outlined above,\nincluding: 1) focus efforts on the research and innovation community, member\nstates, and the private sector; 2) create alignment between trading partners'\npolicies and EU policies; 3) analyze the gaps in the ecosystem between\ntheoretical frameworks and approaches to building trustworthy AI; 4) focus on\ncoordination and policy alignment; 5) focus on mechanisms that promote private\nand secure sharing of data; 6) create a network of AI research excellence\ncentres to strengthen the research and innovation community; 7) promote\nknowledge transfer and develop AI expertise through Digital Innovation Hubs; 8)\nadd nuance to the discussion regarding the opacity of AI systems; 9) create a\nprocess for individuals to appeal an AI system's decision or output; 10)\nimplement new rules and strengthen existing regulations; 11) ban the use of\nfacial recognition technology; 12) hold all AI systems to similar standards and\ncompulsory requirements; 13) ensure biometric identification systems fulfill\nthe purpose for which they are implemented; 14) implement a voluntary labelling\nsystem for systems that are not considered high-risk; 15) appoint individuals\nto the oversight process who understand AI systems well and are able to\ncommunicate potential risks.",
    "published_date": "2020-06-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09428v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.09373v6",
    "title": "The shape and simplicity biases of adversarially robust ImageNet-trained CNNs",
    "authors": [
      "Peijie Chen",
      "Chirag Agarwal",
      "Anh Nguyen"
    ],
    "author_ids": [],
    "abstract": "Increasingly more similarities between human vision and convolutional neural\nnetworks (CNNs) have been revealed in the past few years. Yet, vanilla CNNs\noften fall short in generalizing to adversarial or out-of-distribution (OOD)\nexamples which humans demonstrate superior performance. Adversarial training is\na leading learning algorithm for improving the robustness of CNNs on\nadversarial and OOD data; however, little is known about the properties,\nspecifically the shape bias and internal features learned inside\nadversarially-robust CNNs. In this paper, we perform a thorough, systematic\nstudy to understand the shape bias and some internal mechanisms that enable the\ngeneralizability of AlexNet, GoogLeNet, and ResNet-50 models trained via\nadversarial training. We find that while standard ImageNet classifiers have a\nstrong texture bias, their R counterparts rely heavily on shapes. Remarkably,\nadversarial training induces three simplicity biases into hidden neurons in the\nprocess of \"robustifying\" CNNs. That is, each convolutional neuron in R\nnetworks often changes to detecting (1) pixel-wise smoother patterns, i.e., a\nmechanism that blocks high-frequency noise from passing through the network;\n(2) more lower-level features i.e. textures and colors (instead of objects);and\n(3) fewer types of inputs. Our findings reveal the interesting mechanisms that\nmade networks more adversarially robust and also explain some recent findings\ne.g., why R networks benefit from a much larger capacity (Xie et al. 2020) and\ncan act as a strong image prior in image synthesis (Santurkar et al. 2019).",
    "published_date": "2020-06-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09373v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.09103v1",
    "title": "Jackson-type inequalities and widths of functional classes in the Musielak-Orlicz type spaces",
    "authors": [
      "F. G. Abdullayev",
      "S. O. Chaichenko",
      "M. Imashkyzy",
      "A. L. Shidlich"
    ],
    "author_ids": [],
    "abstract": "In the Musielak-Orlicz type spaces ${\\mathcal S}_{\\bf M}$, exact Jackson-type\ninequalities are obtained in terms of best approximations of functions and the\naveraged values of their generalized moduli of smoothness. The values of\nKolmogorov, Bernstein, linear, and projective widths in ${\\mathcal S}_{\\bf M}$\nare found for classes of periodic functions defined by certain conditions on\nthe averaged values of the generalized moduli of smoothness.",
    "published_date": "2020-06-16T00:00:00",
    "year": 2020,
    "categories": [
      "math.CA",
      "cs.NA",
      "math.NA",
      "41A17, 42A32"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.09103v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.11371v2",
    "title": "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey",
    "authors": [
      "Arun Das",
      "Paul Rad"
    ],
    "author_ids": [],
    "abstract": "Nowadays, deep neural networks are widely used in mission critical systems\nsuch as healthcare, self-driving vehicles, and military which have direct\nimpact on human lives. However, the black-box nature of deep neural networks\nchallenges its use in mission critical applications, raising ethical and\njudicial concerns inducing lack of trust. Explainable Artificial Intelligence\n(XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools,\ntechniques, and algorithms that can generate high-quality interpretable,\nintuitive, human-understandable explanations of AI decisions. In addition to\nproviding a holistic view of the current XAI landscape in deep learning, this\npaper provides mathematical summaries of seminal work. We start by proposing a\ntaxonomy and categorizing the XAI techniques based on their scope of\nexplanations, methodology behind the algorithms, and explanation level or usage\nwhich helps build trustworthy, interpretable, and self-explanatory deep\nlearning models. We then describe the main principles used in XAI research and\npresent the historical timeline for landmark studies in XAI from 2007 to 2020.\nAfter explaining each category of algorithms and approaches in detail, we then\nevaluate the explanation maps generated by eight XAI algorithms on image data,\ndiscuss the limitations of this approach, and provide potential future\ndirections to improve XAI evaluation.",
    "published_date": "2020-06-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11371v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08857v1",
    "title": "Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization",
    "authors": [
      "Chong You",
      "Zhihui Zhu",
      "Qing Qu",
      "Yi Ma"
    ],
    "author_ids": [],
    "abstract": "Recent advances have shown that implicit bias of gradient descent on\nover-parameterized models enables the recovery of low-rank matrices from linear\nmeasurements, even with no prior knowledge on the intrinsic rank. In contrast,\nfor robust low-rank matrix recovery from grossly corrupted measurements,\nover-parameterization leads to overfitting without prior knowledge on both the\nintrinsic rank and sparsity of corruption. This paper shows that with a double\nover-parameterization for both the low-rank matrix and sparse corruption,\ngradient descent with discrepant learning rates provably recovers the\nunderlying matrix even without prior knowledge on neither rank of the matrix\nnor sparsity of the corruption. We further extend our approach for the robust\nrecovery of natural images by over-parameterizing images with deep\nconvolutional networks. Experiments show that our method handles different test\nimages and varying corruption levels with a single learning pipeline where the\nnetwork width and termination conditions do not need to be adjusted on a\ncase-by-case basis. Underlying the success is again the implicit bias with\ndiscrepant learning rates on different over-parameterized parameters, which may\nbear on broader applications.",
    "published_date": "2020-06-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08857v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08828v1",
    "title": "Explainable AI for a No-Teardown Vehicle Component Cost Estimation: A Top-Down Approach",
    "authors": [
      "Ayman Moawad",
      "Ehsan Islam",
      "Namdoo Kim",
      "Ram Vijayagopal",
      "Aymeric Rousseau",
      "Wei Biao Wu"
    ],
    "author_ids": [],
    "abstract": "The broader ambition of this article is to popularize an approach for the\nfair distribution of the quantity of a system's output to its subsystems, while\nallowing for underlying complex subsystem level interactions. Particularly, we\npresent a data-driven approach to vehicle price modeling and its component\nprice estimation by leveraging a combination of concepts from machine learning\nand game theory. We show an alternative to common teardown methodologies and\nsurveying approaches for component and vehicle price estimation at the\nmanufacturer's suggested retail price (MSRP) level that has the advantage of\nbypassing the uncertainties involved in 1) the gathering of teardown data, 2)\nthe need to perform expensive and biased surveying, and 3) the need to perform\nretail price equivalent (RPE) or indirect cost multiplier (ICM) adjustments to\nmark up direct manufacturing costs to MSRP. This novel exercise not only\nprovides accurate pricing of the technologies at the customer level, but also\nshows the, a priori known, large gaps in pricing strategies between\nmanufacturers, vehicle sizes, classes, market segments, and other factors.\nThere is also clear synergism or interaction between the price of certain\ntechnologies and other specifications present in the same vehicle. Those\n(unsurprising) results are indication that old methods of manufacturer-level\ncomponent costing, aggregation, and the application of a flat and rigid RPE or\nICM adjustment factor should be carefully examined. The findings are based on\nan extensive database, developed by Argonne National Laboratory, that includes\nmore than 64,000 vehicles covering MY1990 to MY2020 over hundreds of vehicle\nspecs.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08828v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08788v1",
    "title": "Learning Smooth and Fair Representations",
    "authors": [
      "Xavier Gitiaux",
      "Huzefa Rangwala"
    ],
    "author_ids": [],
    "abstract": "Organizations that own data face increasing legal liability for its\ndiscriminatory use against protected demographic groups, extending to\ncontractual transactions involving third parties access and use of the data.\nThis is problematic, since the original data owner cannot ex-ante anticipate\nall its future uses by downstream users. This paper explores the upstream\nability to preemptively remove the correlations between features and sensitive\nattributes by mapping features to a fair representation space. Our main result\nshows that the fairness measured by the demographic parity of the\nrepresentation distribution can be certified from a finite sample if and only\nif the chi-squared mutual information between features and representations is\nfinite. Empirically, we find that smoothing the representation distribution\nprovides generalization guarantees of fairness certificates, which improves\nupon existing fair representation learning approaches. Moreover, we do not\nobserve that smoothing the representation distribution degrades the accuracy of\ndownstream tasks compared to state-of-the-art methods in fair representation\nlearning.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08688v1",
    "title": "Causal intersectionality for fair ranking",
    "authors": [
      "Ke Yang",
      "Joshua R. Loftus",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "In this paper we propose a causal modeling approach to intersectional\nfairness, and a flexible, task-specific method for computing intersectionally\nfair rankings. Rankings are used in many contexts, ranging from Web search\nresults to college admissions, but causal inference for fair rankings has\nreceived limited attention. Additionally, the growing literature on causal\nfairness has directed little attention to intersectionality. By bringing these\nissues together in a formal causal framework we make the application of\nintersectionality in fair machine learning explicit, connected to important\nreal world effects and domain knowledge, and transparent about technical\nlimitations. We experimentally evaluate our approach on real and synthetic\ndatasets, exploring its behaviour under different structural assumptions.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08688v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08680v2",
    "title": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance",
    "authors": [
      "Jeff Z. HaoChen",
      "Colin Wei",
      "Jason D. Lee",
      "Tengyu Ma"
    ],
    "author_ids": [],
    "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit\nregularization effect for training overparameterized models. Prior theoretical\nwork largely focuses on spherical Gaussian noise, whereas empirical studies\ndemonstrate the phenomenon that parameter-dependent noise -- induced by\nmini-batches or label perturbation -- is far more effective than Gaussian\nnoise. This paper theoretically characterizes this phenomenon on a\nquadratically-parameterized model introduced by Vaskevicius et el. and\nWoodworth et el. We show that in an over-parameterized setting, SGD with label\nnoise recovers the sparse ground-truth with an arbitrary initialization,\nwhereas SGD with Gaussian noise or gradient descent overfits to dense solutions\nwith large norms. Our analysis reveals that parameter-dependent noise\nintroduces a bias towards local minima with smaller noise variance, whereas\nspherical Gaussian noise does not. Code for our project is publicly available.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08680v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08669v1",
    "title": "On Adversarial Bias and the Robustness of Fair Machine Learning",
    "authors": [
      "Hongyan Chang",
      "Ta Duy Nguyen",
      "Sasi Kumar Murakonda",
      "Ehsan Kazemi",
      "Reza Shokri"
    ],
    "author_ids": [],
    "abstract": "Optimizing prediction accuracy can come at the expense of fairness. Towards\nminimizing discrimination against a group, fair machine learning algorithms\nstrive to equalize the behavior of a model across different groups, by imposing\na fairness constraint on models. However, we show that giving the same\nimportance to groups of different sizes and distributions, to counteract the\neffect of bias in training data, can be in conflict with robustness. We analyze\ndata poisoning attacks against group-based fair machine learning, with the\nfocus on equalized odds. An adversary who can control sampling or labeling for\na fraction of training data, can reduce the test accuracy significantly beyond\nwhat he can achieve on unconstrained models. Adversarial sampling and\nadversarial labeling attacks can also worsen the model's fairness gap on test\ndata, even though the model satisfies the fairness constraint on training data.\nWe analyze the robustness of fair machine learning through an empirical\nevaluation of attacks on multiple algorithms and benchmark datasets.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08669v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08564v2",
    "title": "Intra-Processing Methods for Debiasing Neural Networks",
    "authors": [
      "Yash Savani",
      "Colin White",
      "Naveen Sundar Govindarajulu"
    ],
    "author_ids": [],
    "abstract": "As deep learning models become tasked with more and more decisions that\nimpact human lives, such as criminal recidivism, loan repayment, and face\nrecognition for law enforcement, bias is becoming a growing concern. Debiasing\nalgorithms are typically split into three paradigms: pre-processing,\nin-processing, and post-processing. However, in computer vision or natural\nlanguage applications, it is common to start with a large generic model and\nthen fine-tune to a specific use-case. Pre- or in-processing methods would\nrequire retraining the entire model from scratch, while post-processing methods\nonly have black-box access to the model, so they do not leverage the weights of\nthe trained model. Creating debiasing algorithms specifically for this\nfine-tuning use-case has largely been neglected.\n  In this work, we initiate the study of a new paradigm in debiasing research,\nintra-processing, which sits between in-processing and post-processing methods.\nIntra-processing methods are designed specifically to debias large models which\nhave been trained on a generic dataset and fine-tuned on a more specific task.\nWe show how to repurpose existing in-processing methods for this use-case, and\nwe also propose three baseline algorithms: random perturbation, layerwise\noptimization, and adversarial fine-tuning. All of our techniques can be used\nfor all popular group fairness measures such as equalized odds or statistical\nparity difference. We evaluate these methods across three popular datasets from\nthe AIF360 toolkit, as well as on the CelebA faces dataset. Our code is\navailable at https://github.com/abacusai/intraprocessing_debiasing.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08564v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08350v2",
    "title": "Societal biases reinforcement through machine learning: A credit scoring perspective",
    "authors": [
      "Bertrand K. Hassani"
    ],
    "author_ids": [],
    "abstract": "Does machine learning and AI ensure that social biases thrive ? This paper\naims to analyse this issue. Indeed, as algorithms are informed by data, if\nthese are corrupted, from a social bias perspective, good machine learning\nalgorithms would learn from the data provided and reverberate the patterns\nlearnt on the predictions related to either the classification or the\nregression intended. In other words, the way society behaves whether positively\nor negatively, would necessarily be reflected by the models. In this paper, we\nanalyse how social biases are transmitted from the data into banks loan\napprovals by predicting either the gender or the ethnicity of the customers\nusing the exact same information provided by customers through their\napplications.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08350v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08315v7",
    "title": "Mitigating Gender Bias in Captioning Systems",
    "authors": [
      "Ruixiang Tang",
      "Mengnan Du",
      "Yuening Li",
      "Zirui Liu",
      "Na Zou",
      "Xia Hu"
    ],
    "author_ids": [],
    "abstract": "Image captioning has made substantial progress with huge supporting image\ncollections sourced from the web. However, recent studies have pointed out that\ncaptioning datasets, such as COCO, contain gender bias found in web corpora. As\na result, learning models could heavily rely on the learned priors and image\ncontext for gender identification, leading to incorrect or even offensive\nerrors. To encourage models to learn correct gender features, we reorganize the\nCOCO dataset and present two new splits COCO-GB V1 and V2 datasets where the\ntrain and test sets have different gender-context joint distribution. Models\nrelying on contextual cues will suffer from huge gender prediction errors on\nthe anti-stereotypical test data. Benchmarking experiments reveal that most\ncaptioning models learn gender bias, leading to high gender prediction errors,\nespecially for women. To alleviate the unwanted bias, we propose a new Guided\nAttention Image Captioning model (GAIC) which provides self-guidance on visual\nattention to encourage the model to capture correct gender visual evidence.\nExperimental results validate that GAIC can significantly reduce gender\nprediction errors with a competitive caption quality. Our codes and the\ndesigned benchmark datasets are available at\nhttps://github.com/datamllab/Mitigating_Gender_Bias_In_Captioning_System.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08315v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08292v1",
    "title": "Robust Locality-Aware Regression for Labeled Data Classification",
    "authors": [
      "Liangchen Hu",
      "Wensheng Zhang"
    ],
    "author_ids": [],
    "abstract": "With the dramatic increase of dimensions in the data representation,\nextracting latent low-dimensional features becomes of the utmost importance for\nefficient classification. Aiming at the problems of unclear margin\nrepresentation and difficulty in revealing the data manifold structure in most\nof the existing linear discriminant methods, we propose a new discriminant\nfeature extraction framework, namely Robust Locality-Aware Regression (RLAR).\nIn our model, we introduce a retargeted regression to perform the marginal\nrepresentation learning adaptively instead of using the general average\ninter-class margin. Besides, we formulate a new strategy for enhancing the\nlocal intra-class compactness of the data manifold, which can achieve the joint\nlearning of locality-aware graph structure and desirable projection matrix. To\nalleviate the disturbance of outliers and prevent overfitting, we measure the\nregression term and locality-aware term together with the regularization term\nby the L2,1 norm. Further, forcing the row sparsity on the projection matrix\nthrough the L2,1 norm achieves the cooperation of feature selection and feature\nextraction. Then, we derive an effective iterative algorithm for solving the\nproposed model. The experimental results over a range of UCI data sets and\nother benchmark databases demonstrate that the proposed RLAR outperforms some\nstate-of-the-art approaches.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08292v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08267v4",
    "title": "Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility",
    "authors": [
      "Sen Cui",
      "Weishen Pan",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "author_ids": [],
    "abstract": "Bipartite ranking, which aims to learn a scoring function that ranks positive\nindividuals higher than negative ones from labeled data, is widely adopted in\nvarious applications where sample prioritization is needed. Recently, there\nhave been rising concerns on whether the learned scoring function can cause\nsystematic disparity across different protected groups defined by sensitive\nattributes. While there could be trade-off between fairness and performance, in\nthis paper we propose a model agnostic post-processing framework for balancing\nthem in the bipartite ranking scenario. Specifically, we maximize a weighted\nsum of the utility and fairness by directly adjusting the relative ordering of\nsamples across groups. By formulating this problem as the identification of an\noptimal warping path across different protected groups, we propose a\nnon-parametric method to search for such an optimal path through a dynamic\nprogramming process. Our method is compatible with various classification\nmodels and applicable to a variety of ranking fairness metrics. Comprehensive\nexperiments on a suite of benchmark data sets and two real-world patient\nelectronic health record repositories show that our method can achieve a great\nbalance between the algorithm utility and ranking fairness. Furthermore, we\nexperimentally verify the robustness of our method when faced with the fewer\ntraining samples and the difference between training and testing ranking score\ndistributions.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08267v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08102v2",
    "title": "Hybrid Systems Differential Dynamic Programming for Whole-Body Motion Planning of Legged Robots",
    "authors": [
      "He Li",
      "Patrick M. Wensing"
    ],
    "author_ids": [],
    "abstract": "This paper presents a Differential Dynamic Programming (DDP) framework for\ntrajectory optimization (TO) of hybrid systems with state-based switching. The\nproposed Hybrid Systems DDP (HS-DDP) approach is considered for application to\nwhole-body motion planning with legged robots. Specifically, HS-DDP\nincorporates three algorithmic advances: an impact-aware DDP step addressing\nthe impact event in legged locomotion, an Augmented Lagrangian (AL) method\ndealing with the switching constraint, and a Switching Time Optimization (STO)\nalgorithm that optimizes switching times by leveraging the structure of DDP.\nFurther, a Relaxed Barrier (ReB) method is used to manage inequality\nconstraints and is integrated into HS-DDP for locomotion planning. The\nperformance of the developed algorithms is benchmarked on a simulation model of\nthe MIT Mini Cheetah executing a bounding gait. We demonstrate the\neffectiveness of AL and ReB for handling switching constraints, friction\nconstraints, and torque limits. By comparing to previous solutions, we show\nthat the STO algorithm achieves 2.3 times more reduction of total switching\ntimes, demonstrating the efficiency of our method.",
    "published_date": "2020-06-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08102v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.08040v2",
    "title": "Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs",
    "authors": [
      "Chung-Wei Lee",
      "Haipeng Luo",
      "Chen-Yu Wei",
      "Mengxiao Zhang"
    ],
    "author_ids": [],
    "abstract": "We develop a new approach to obtaining high probability regret bounds for\nonline learning with bandit feedback against an adaptive adversary. While\nexisting approaches all require carefully constructing optimistic and biased\nloss estimators, our approach uses standard unbiased estimators and relies on a\nsimple increasing learning rate schedule, together with the help of\nlogarithmically homogeneous self-concordant barriers and a strengthened\nFreedman's inequality.\n  Besides its simplicity, our approach enjoys several advantages. First, the\nobtained high-probability regret bounds are data-dependent and could be much\nsmaller than the worst-case bounds, which resolves an open problem asked by Neu\n(2015). Second, resolving another open problem of Bartlett et al. (2008) and\nAbernethy and Rakhlin (2009), our approach leads to the first general and\nefficient algorithm with a high-probability regret bound for adversarial linear\nbandits, while previous methods are either inefficient or only applicable to\nspecific action sets. Finally, our approach can also be applied to learning\nadversarial Markov Decision Processes and provides the first algorithm with a\nhigh-probability small-loss bound for this problem.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08040v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07986v2",
    "title": "Fairness Under Feature Exemptions: Counterfactual and Observational Measures",
    "authors": [
      "Sanghamitra Dutta",
      "Praveen Venkatesh",
      "Piotr Mardziel",
      "Anupam Datta",
      "Pulkit Grover"
    ],
    "author_ids": [],
    "abstract": "With the growing use of ML in highly consequential domains, quantifying\ndisparity with respect to protected attributes, e.g., gender, race, etc., is\nimportant. While quantifying disparity is essential, sometimes the needs of an\noccupation may require the use of certain features that are critical in a way\nthat any disparity that can be explained by them might need to be exempted.\nE.g., in hiring a software engineer for a safety-critical application,\ncoding-skills may be weighed strongly, whereas name, zip code, or reference\nletters may be used only to the extent that they do not add disparity. In this\nwork, we propose an information-theoretic decomposition of the total disparity\n(a quantification inspired from counterfactual fairness) into two components: a\nnon-exempt component which quantifies the part that cannot be accounted for by\nthe critical features, and an exempt component that quantifies the remaining\ndisparity. This decomposition allows one to check if the disparity arose purely\ndue to the critical features (inspired from the business necessity defense of\ndisparate impact law) and also enables selective removal of the non-exempt\ncomponent if desired. We arrive at this decomposition through canonical\nexamples that lead to a set of desirable properties (axioms) that a measure of\nnon-exempt disparity should satisfy. Our proposed measure satisfies all of\nthem. Our quantification bridges ideas of causality, Simpson's paradox, and a\nbody of work from information theory called Partial Information Decomposition.\nWe also obtain an impossibility result showing that no observational measure\ncan satisfy all the desirable properties, leading us to relax our goals and\nexamine observational measures that satisfy only some of them. We perform case\nstudies to show how one can audit/train models while reducing non-exempt\ndisparity.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.CR",
      "cs.CY",
      "cs.LG",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07986v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07906v2",
    "title": "Fair Influence Maximization: A Welfare Optimization Approach",
    "authors": [
      "Aida Rahmattalabi",
      "Shahin Jabbari",
      "Himabindu Lakkaraju",
      "Phebe Vayanos",
      "Max Izenberg",
      "Ryan Brown",
      "Eric Rice",
      "Milind Tambe"
    ],
    "author_ids": [],
    "abstract": "Several behavioral, social, and public health interventions, such as\nsuicide/HIV prevention or community preparedness against natural disasters,\nleverage social network information to maximize outreach. Algorithmic influence\nmaximization techniques have been proposed to aid with the choice of \"peer\nleaders\" or \"influencers\" in such interventions. Yet, traditional algorithms\nfor influence maximization have not been designed with these interventions in\nmind. As a result, they may disproportionately exclude minority communities\nfrom the benefits of the intervention. This has motivated research on fair\ninfluence maximization. Existing techniques come with two major drawbacks.\nFirst, they require committing to a single fairness measure. Second, these\nmeasures are typically imposed as strict constraints leading to undesirable\nproperties such as wastage of resources.\n  To address these shortcomings, we provide a principled characterization of\nthe properties that a fair influence maximization algorithm should satisfy. In\nparticular, we propose a framework based on social welfare theory, wherein the\ncardinal utilities derived by each community are aggregated using the\nisoelastic social welfare functions. Under this framework, the trade-off\nbetween fairness and efficiency can be controlled by a single inequality\naversion design parameter. We then show under what circumstances our proposed\nprinciples can be satisfied by a welfare function. The resulting optimization\nproblem is monotone and submodular and can be solved efficiently with\noptimality guarantees. Our framework encompasses as special cases leximin and\nproportional fairness. Extensive experiments on synthetic and real world\ndatasets including a case study on landslide risk management demonstrate the\nefficacy of the proposed framework.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07906v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07904v2",
    "title": "An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias",
    "authors": [
      "Lu Yu",
      "Krishnakumar Balasubramanian",
      "Stanislav Volgushev",
      "Murat A. Erdogdu"
    ],
    "author_ids": [],
    "abstract": "Structured non-convex learning problems, for which critical points have\nfavorable statistical properties, arise frequently in statistical machine\nlearning. Algorithmic convergence and statistical estimation rates are\nwell-understood for such problems. However, quantifying the uncertainty\nassociated with the underlying training algorithm is not well-studied in the\nnon-convex setting. In order to address this shortcoming, in this work, we\nestablish an asymptotic normality result for the constant step size stochastic\ngradient descent (SGD) algorithm--a widely used algorithm in practice.\nSpecifically, based on the relationship between SGD and Markov Chains [DDB19],\nwe show that the average of SGD iterates is asymptotically normally distributed\naround the expected value of their unique invariant distribution, as long as\nthe non-convex and non-smooth objective function satisfies a dissipativity\nproperty. We also characterize the bias between this expected value and the\ncritical points of the objective function under various local regularity\nconditions. Together, the above two results could be leveraged to construct\nconfidence intervals for non-convex problems that are trained using the SGD\nalgorithm.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "math.ST",
      "stat.CO",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07904v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07886v3",
    "title": "On Disentangled Representations Learned From Correlated Data",
    "authors": [
      "Frederik Träuble",
      "Elliot Creager",
      "Niki Kilbertus",
      "Francesco Locatello",
      "Andrea Dittadi",
      "Anirudh Goyal",
      "Bernhard Schölkopf",
      "Stefan Bauer"
    ],
    "author_ids": [],
    "abstract": "The focus of disentanglement approaches has been on identifying independent\nfactors of variation in data. However, the causal variables underlying\nreal-world observations are often not statistically independent. In this work,\nwe bridge the gap to real-world scenarios by analyzing the behavior of the most\nprominent disentanglement approaches on correlated data in a large-scale\nempirical study (including 4260 models). We show and quantify that\nsystematically induced correlations in the dataset are being learned and\nreflected in the latent representations, which has implications for downstream\napplications of disentanglement such as fairness. We also demonstrate how to\nresolve these latent correlations, either using weak supervision during\ntraining or by post-hoc correcting a pre-trained model with a small number of\nlabels.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07886v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07845v2",
    "title": "Towards Gender-Neutral Face Descriptors for Mitigating Bias in Face Recognition",
    "authors": [
      "Prithviraj Dhar",
      "Joshua Gleason",
      "Hossein Souri",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ],
    "author_ids": [],
    "abstract": "State-of-the-art deep networks implicitly encode gender information while\nbeing trained for face recognition. Gender is often viewed as an important\nattribute with respect to identifying faces. However, the implicit encoding of\ngender information in face descriptors has two major issues: (a.) It makes the\ndescriptors susceptible to privacy leakage, i.e. a malicious agent can be\ntrained to predict the face gender from such descriptors. (b.) It appears to\ncontribute to gender bias in face recognition, i.e. we find a significant\ndifference in the recognition accuracy of DCNNs on male and female faces.\nTherefore, we present a novel `Adversarial Gender De-biasing algorithm\n(AGENDA)' to reduce the gender information present in face descriptors obtained\nfrom previously trained face recognition networks. We show that AGENDA\nsignificantly reduces gender predictability of face descriptors. Consequently,\nwe are also able to reduce gender bias in face verification while maintaining\nreasonable recognition performance.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07845v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07832v3",
    "title": "Group Fairness for Knapsack Problems",
    "authors": [
      "Deval Patel",
      "Arindam Khan",
      "Anand Louis"
    ],
    "author_ids": [],
    "abstract": "We study the knapsack problem with group fairness constraints. The input of\nthe problem consists of a knapsack of bounded capacity and a set of items, each\nitem belongs to a particular category and has and associated weight and value.\nThe goal of this problem is to select a subset of items such that all\ncategories are fairly represented, the total weight of the selected items does\nnot exceed the capacity of the knapsack,and the total value is maximized. We\nstudy the fairness parameters such as the bounds on the total value of items\nfrom each category, the total weight of items from each category, and the total\nnumber of items from each category. We give approximation algorithms for these\nproblems. These fairness notions could also be extended to the min-knapsack\nproblem. The fair knapsack problems encompass various important problems, such\nas participatory budgeting, fair budget allocation, advertising.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07832v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.07776v2",
    "title": "Domain Adaptation and Image Classification via Deep Conditional Adaptation Network",
    "authors": [
      "Pengfei Ge",
      "Chuan-Xian Ren",
      "Dao-Qing Dai",
      "Hong Yan"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptation aims to generalize the supervised model\ntrained on a source domain to an unlabeled target domain. Marginal distribution\nalignment of feature spaces is widely used to reduce the domain discrepancy\nbetween the source and target domains. However, it assumes that the source and\ntarget domains share the same label distribution, which limits their\napplication scope. In this paper, we consider a more general application\nscenario where the label distributions of the source and target domains are not\nthe same. In this scenario, marginal distribution alignment-based methods will\nbe vulnerable to negative transfer. To address this issue, we propose a novel\nunsupervised domain adaptation method, Deep Conditional Adaptation Network\n(DCAN), based on conditional distribution alignment of feature spaces. To be\nspecific, we reduce the domain discrepancy by minimizing the Conditional\nMaximum Mean Discrepancy between the conditional distributions of deep features\non the source and target domains, and extract the discriminant information from\ntarget domain by maximizing the mutual information between samples and the\nprediction labels. In addition, DCAN can be used to address a special scenario,\nPartial unsupervised domain adaptation, where the target domain category is a\nsubset of the source domain category. Experiments on both unsupervised domain\nadaptation and Partial unsupervised domain adaptation show that DCAN achieves\nsuperior classification performance over state-of-the-art methods.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07776v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07749v2",
    "title": "Parametric Bootstrap for Differentially Private Confidence Intervals",
    "authors": [
      "Cecilia Ferrando",
      "Shufan Wang",
      "Daniel Sheldon"
    ],
    "author_ids": [],
    "abstract": "The goal of this paper is to develop a practical and general-purpose approach\nto construct confidence intervals for differentially private parametric\nestimation. We find that the parametric bootstrap is a simple and effective\nsolution. It cleanly reasons about variability of both the data sample and the\nrandomized privacy mechanism and applies \"out of the box\" to a wide class of\nprivate estimation routines. It can also help correct bias caused by clipping\ndata to limit sensitivity. We prove that the parametric bootstrap gives\nconsistent confidence intervals in two broadly relevant settings, including a\nnovel adaptation to linear regression that avoids accessing the covariate data\nmultiple times. We demonstrate its effectiveness for a variety of estimators,\nand find that it provides confidence intervals with good coverage even at\nmodest sample sizes and performs better than alternative approaches.",
    "published_date": "2020-06-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07749v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07710v2",
    "title": "The Pitfalls of Simplicity Bias in Neural Networks",
    "authors": [
      "Harshay Shah",
      "Kaustav Tamuly",
      "Aditi Raghunathan",
      "Prateek Jain",
      "Praneeth Netrapalli"
    ],
    "author_ids": [],
    "abstract": "Several works have proposed Simplicity Bias (SB)---the tendency of standard\ntraining procedures such as Stochastic Gradient Descent (SGD) to find simple\nmodels---to justify why neural networks generalize well [Arpit et al. 2017,\nNakkiran et al. 2019, Soudry et al. 2018]. However, the precise notion of\nsimplicity remains vague. Furthermore, previous settings that use SB to\ntheoretically justify why neural networks generalize well do not simultaneously\ncapture the non-robustness of neural networks---a widely observed phenomenon in\npractice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile\nSB and the superior standard generalization of neural networks with the\nnon-robustness observed in practice by designing datasets that (a) incorporate\na precise notion of simplicity, (b) comprise multiple predictive features with\nvarying levels of simplicity, and (c) capture the non-robustness of neural\nnetworks trained on real data. Through theory and empirics on these datasets,\nwe make four observations: (i) SB of SGD and variants can be extreme: neural\nnetworks can exclusively rely on the simplest feature and remain invariant to\nall predictive complex features. (ii) The extreme aspect of SB could explain\nwhy seemingly benign distribution shifts and small adversarial perturbations\nsignificantly degrade model performance. (iii) Contrary to conventional wisdom,\nSB can also hurt generalization on the same data distribution, as SB persists\neven when the simplest feature has less predictive power than the more complex\nfeatures. (iv) Common approaches to improve generalization and\nrobustness---ensembles and adversarial training---can fail in mitigating SB and\nits pitfalls. Given the role of SB in training neural networks, we hope that\nthe proposed datasets and methods serve as an effective testbed to evaluate\nnovel algorithmic approaches aimed at avoiding the pitfalls of SB.",
    "published_date": "2020-06-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07710v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07647v1",
    "title": "Quota-based debiasing can decrease representation of already underrepresented groups",
    "authors": [
      "Ivan Smirnov",
      "Florian Lemmerich",
      "Markus Strohmaier"
    ],
    "author_ids": [],
    "abstract": "Many important decisions in societies such as school admissions, hiring, or\nelections are based on the selection of top-ranking individuals from a larger\npool of candidates. This process is often subject to biases, which typically\nmanifest as an under-representation of certain groups among the selected or\naccepted individuals. The most common approach to this issue is debiasing, for\nexample via the introduction of quotas that ensure proportional representation\nof groups with respect to a certain, often binary attribute. Cases include\nquotas for women on corporate boards or ethnic quotas in elections. This,\nhowever, has the potential to induce changes in representation with respect to\nother attributes. For the case of two correlated binary attributes we show that\nquota-based debiasing based on a single attribute can worsen the representation\nof already underrepresented groups and decrease overall fairness of selection.\nWe use several data sets from a broad range of domains from recidivism risk\nassessments to scientific citations to assess this effect in real-world\nsettings. Our results demonstrate the importance of including all relevant\nattributes in debiasing procedures and that more efforts need to be put into\neliminating the root causes of inequalities as purely numerical solutions such\nas quota-based debiasing might lead to unintended consequences.",
    "published_date": "2020-06-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07647v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07576v2",
    "title": "Mitigating Face Recognition Bias via Group Adaptive Classifier",
    "authors": [
      "Sixue Gong",
      "Xiaoming Liu",
      "Anil K. Jain"
    ],
    "author_ids": [],
    "abstract": "Face recognition is known to exhibit bias - subjects in a certain demographic\ngroup can be better recognized than other groups. This work aims to learn a\nfair face representation, where faces of every group could be more equally\nrepresented. Our proposed group adaptive classifier mitigates bias by using\nadaptive convolution kernels and attention mechanisms on faces based on their\ndemographic attributes. The adaptive module comprises kernel masks and\nchannel-wise attention maps for each demographic group so as to activate\ndifferent facial regions for identification, leading to more discriminative\nfeatures pertinent to their demographics. Our introduced automated adaptation\nstrategy determines whether to apply adaptation to a certain layer by\niteratively computing the dissimilarity among demographic-adaptive parameters.\nA new de-biasing loss function is proposed to mitigate the gap of average\nintra-class distance between demographic groups. Experiments on face benchmarks\n(RFW, LFW, IJB-A, and IJB-C) show that our work is able to mitigate face\nrecognition bias across demographic groups while maintaining the competitive\naccuracy.",
    "published_date": "2020-06-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07576v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07558v1",
    "title": "Ethical Considerations for AI Researchers",
    "authors": [
      "Kyle Dent"
    ],
    "author_ids": [],
    "abstract": "Use of artificial intelligence is growing and expanding into applications\nthat impact people's lives. People trust their technology without really\nunderstanding it or its limitations. There is the potential for harm and we are\nalready seeing examples of that in the world. AI researchers have an obligation\nto consider the impact of intelligent applications they work on. While the\nethics of AI is not clear-cut, there are guidelines we can consider to minimize\nthe harm we might introduce.",
    "published_date": "2020-06-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07529v2",
    "title": "Rethinking the Value of Labels for Improving Class-Imbalanced Learning",
    "authors": [
      "Yuzhe Yang",
      "Zhi Xu"
    ],
    "author_ids": [],
    "abstract": "Real-world data often exhibits long-tailed distributions with heavy class\nimbalance, posing great challenges for deep recognition models. We identify a\npersisting dilemma on the value of labels in the context of imbalanced\nlearning: on the one hand, supervision from labels typically leads to better\nresults than its unsupervised counterparts; on the other hand, heavily\nimbalanced data naturally incurs \"label bias\" in the classifier, where the\ndecision boundary can be drastically altered by the majority classes. In this\nwork, we systematically investigate these two facets of labels. We demonstrate,\ntheoretically and empirically, that class-imbalanced learning can significantly\nbenefit in both semi-supervised and self-supervised manners. Specifically, we\nconfirm that (1) positively, imbalanced labels are valuable: given more\nunlabeled data, the original labels can be leveraged with the extra data to\nreduce label bias in a semi-supervised manner, which greatly improves the final\nclassifier; (2) negatively however, we argue that imbalanced labels are not\nuseful always: classifiers that are first pre-trained in a self-supervised\nmanner consistently outperform their corresponding baselines. Extensive\nexperiments on large-scale imbalanced datasets verify our theoretically\ngrounded strategies, showing superior performance over previous\nstate-of-the-arts. Our intriguing findings highlight the need to rethink the\nusage of imbalanced labels in realistic long-tailed tasks. Code is available at\nhttps://github.com/YyzHarry/imbalanced-semi-self.",
    "published_date": "2020-06-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07529v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07459v1",
    "title": "Consistent Estimation of Identifiable Nonparametric Mixture Models from Grouped Observations",
    "authors": [
      "Alexander Ritchie",
      "Robert A. Vandermeulen",
      "Clayton Scott"
    ],
    "author_ids": [],
    "abstract": "Recent research has established sufficient conditions for finite mixture\nmodels to be identifiable from grouped observations. These conditions allow the\nmixture components to be nonparametric and have substantial (or even total)\noverlap. This work proposes an algorithm that consistently estimates any\nidentifiable mixture model from grouped observations. Our analysis leverages an\noracle inequality for weighted kernel density estimators of the distribution on\ngroups, together with a general result showing that consistent estimation of\nthe distribution on groups implies consistent estimation of mixture components.\nA practical implementation is provided for paired observations, and the\napproach is shown to outperform existing methods, especially when mixture\ncomponents overlap significantly.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07441v5",
    "title": "On the optimal constants in the two-sided Stechkin inequalities",
    "authors": [
      "Thomas Jahn",
      "Tino Ullrich"
    ],
    "author_ids": [],
    "abstract": "We address the optimal constants in the strong and the weak Stechkin\ninequalities, both in their discrete and continuous variants. These\ninequalities appear in the characterization of approximation spaces which arise\nfrom sparse approximation or have applications to interpolation theory. An\nelementary proof of a constant in the strong discrete Stechkin inequality given\nby Bennett is provided, and we improve the constants given by Levin and\nStechkin and by Copson. Finally, the minimal constants in the weak discrete\nStechkin inequalities and both continuous Stechkin inequalities are presented.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "math.CA",
      "cs.NA",
      "math.FA",
      "math.NA",
      "41A25, 46E30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07441v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.07315v2",
    "title": "Fairness in Forecasting and Learning Linear Dynamical Systems",
    "authors": [
      "Quan Zhou",
      "Jakub Marecek",
      "Robert N. Shorten"
    ],
    "author_ids": [],
    "abstract": "In machine learning, training data often capture the behaviour of multiple\nsubgroups of some underlying human population. When the amounts of training\ndata for the subgroups are not controlled carefully, under-representation bias\narises. We introduce two natural notions of subgroup fairness and instantaneous\nfairness to address such under-representation bias in time-series forecasting\nproblems. In particular, we consider the subgroup-fair and instant-fair\nlearning of a linear dynamical system (LDS) from multiple trajectories of\nvarying lengths, and the associated forecasting problems. We provide globally\nconvergent methods for the learning problems using hierarchies of\nconvexifications of non-commutative polynomial optimisation problems. Our\nempirical results on a biased data set motivated by insurance applications and\nthe well-known COMPAS data set demonstrate both the beneficial impact of\nfairness considerations on statistical performance and encouraging effects of\nexploiting sparsity on run time.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.DS",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07315v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07286v2",
    "title": "Fair Regression with Wasserstein Barycenters",
    "authors": [
      "Evgenii Chzhen",
      "Christophe Denis",
      "Mohamed Hebiri",
      "Luca Oneto",
      "Massimiliano Pontil"
    ],
    "author_ids": [],
    "abstract": "We study the problem of learning a real-valued function that satisfies the\nDemographic Parity constraint. It demands the distribution of the predicted\noutput to be independent of the sensitive attribute. We consider the case that\nthe sensitive attribute is available for prediction. We establish a connection\nbetween fair regression and optimal transport theory, based on which we derive\na close form expression for the optimal fair predictor. Specifically, we show\nthat the distribution of this optimum is the Wasserstein barycenter of the\ndistributions induced by the standard regression function on the sensitive\ngroups. This result offers an intuitive interpretation of the optimal fair\nprediction and suggests a simple post-processing algorithm to achieve fairness.\nWe establish risk and distribution-free fairness guarantees for this procedure.\nNumerical experiments indicate that our method is very effective in learning\nfair models, with a relative increase in error rate that is inferior to the\nrelative gain in fairness.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07286v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.07281v1",
    "title": "Algorithms and Learning for Fair Portfolio Design",
    "authors": [
      "Emily Diana",
      "Travis Dick",
      "Hadi Elzayn",
      "Michael Kearns",
      "Aaron Roth",
      "Zachary Schutzman",
      "Saeed Sharifi-Malvajerdi",
      "Juba Ziani"
    ],
    "author_ids": [],
    "abstract": "We consider a variation on the classical finance problem of optimal portfolio\ndesign. In our setting, a large population of consumers is drawn from some\ndistribution over risk tolerances, and each consumer must be assigned to a\nportfolio of lower risk than her tolerance. The consumers may also belong to\nunderlying groups (for instance, of demographic properties or wealth), and the\ngoal is to design a small number of portfolios that are fair across groups in a\nparticular and natural technical sense.\n  Our main results are algorithms for optimal and near-optimal portfolio design\nfor both social welfare and fairness objectives, both with and without\nassumptions on the underlying group structure. We describe an efficient\nalgorithm based on an internal two-player zero-sum game that learns\nnear-optimal fair portfolios ex ante and show experimentally that it can be\nused to obtain a small set of fair portfolios ex post as well. For the special\nbut natural case in which group structure coincides with risk tolerances (which\nmodels the reality that wealthy consumers generally tolerate greater risk), we\ngive an efficient and optimal fair algorithm. We also provide generalization\nguarantees for the underlying risk distribution that has no dependence on the\nnumber of portfolios and illustrate the theory with simulation results.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CE",
      "cs.GT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.07281v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06983v4",
    "title": "Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data",
    "authors": [
      "Chengxu Yang",
      "Qipeng Wang",
      "Mengwei Xu",
      "Zhenpeng Chen",
      "Kaigui Bian",
      "Yunxin Liu",
      "Xuanzhe Liu"
    ],
    "author_ids": [],
    "abstract": "Federated learning (FL) is an emerging, privacy-preserving machine learning\nparadigm, drawing tremendous attention in both academia and industry. A unique\ncharacteristic of FL is heterogeneity, which resides in the various hardware\nspecifications and dynamic states across the participating devices.\nTheoretically, heterogeneity can exert a huge influence on the FL training\nprocess, e.g., causing a device unavailable for training or unable to upload\nits model updates. Unfortunately, these impacts have never been systematically\nstudied and quantified in existing FL literature.\n  In this paper, we carry out the first empirical study to characterize the\nimpacts of heterogeneity in FL. We collect large-scale data from 136k\nsmartphones that can faithfully reflect heterogeneity in real-world settings.\nWe also build a heterogeneity-aware FL platform that complies with the standard\nFL protocol but with heterogeneity in consideration. Based on the data and the\nplatform, we conduct extensive experiments to compare the performance of\nstate-of-the-art FL algorithms under heterogeneity-aware and\nheterogeneity-unaware settings. Results show that heterogeneity causes\nnon-trivial performance degradation in FL, including up to 9.2% accuracy drop,\n2.32x lengthened training time, and undermined fairness. Furthermore, we\nanalyze potential impact factors and find that device failure and participant\nbias are two potential factors for performance degradation. Our study provides\ninsightful implications for FL practitioners. On the one hand, our findings\nsuggest that FL algorithm designers consider necessary heterogeneity during the\nevaluation. On the other hand, our findings urge system providers to design\nspecific mechanisms to mitigate the impacts of heterogeneity.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06983v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06928v1",
    "title": "Characterising authors on the extent of their paper acceptance: A case study of the Journal of High Energy Physics",
    "authors": [
      "Rima Hazra",
      "Aryan",
      "Hardik Aggarwal",
      "Matteo Marsili",
      "Animesh Mukherjee"
    ],
    "author_ids": [],
    "abstract": "New researchers are usually very curious about the recipe that could\naccelerate the chances of their paper getting accepted in a reputed forum\n(journal/conference). In search of such a recipe, we investigate the profile\nand peer review text of authors whose papers almost always get accepted at a\nvenue (Journal of High Energy Physics in our current work). We find authors\nwith high acceptance rate are likely to have a high number of citations, high\n$h$-index, higher number of collaborators etc. We notice that they receive\nrelatively lengthy and positive reviews for their papers. In addition, we also\nconstruct three networks -- co-reviewer, co-citation and collaboration network\nand study the network-centric features and intra- and inter-category edge\ninteractions. We find that the authors with high acceptance rate are more\n`central' in these networks; the volume of intra- and inter-category\ninteractions are also drastically different for the authors with high\nacceptance rate compared to the other authors. Finally, using the above set of\nfeatures, we train standard machine learning models (random forest, XGBoost)\nand obtain very high class wise precision and recall. In a followup discussion\nwe also narrate how apart from the author characteristics, the peer-review\nsystem might itself have a role in propelling the distinction among the\ndifferent categories which could lead to potential discrimination and\nunfairness and calls for further investigation by the system admins.",
    "published_date": "2020-06-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06928v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06879v3",
    "title": "Active Sampling for Min-Max Fairness",
    "authors": [
      "Jacob Abernethy",
      "Pranjal Awasthi",
      "Matthäus Kleindessner",
      "Jamie Morgenstern",
      "Chris Russell",
      "Jie Zhang"
    ],
    "author_ids": [],
    "abstract": "We propose simple active sampling and reweighting strategies for optimizing\nmin-max fairness that can be applied to any classification or regression model\nlearned via loss minimization. The key intuition behind our approach is to use\nat each timestep a datapoint from the group that is worst off under the current\nmodel for updating the model. The ease of implementation and the generality of\nour robust formulation make it an attractive option for improving model\nperformance on disadvantaged groups. For convex learning problems, such as\nlinear or logistic regression, we provide a fine-grained analysis, proving the\nrate of convergence to a min-max fair solution.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06879v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06865v1",
    "title": "Exploring Algorithmic Fairness in Robust Graph Covering Problems",
    "authors": [
      "Aida Rahmattalabi",
      "Phebe Vayanos",
      "Anthony Fulginiti",
      "Eric Rice",
      "Bryan Wilder",
      "Amulya Yadav",
      "Milind Tambe"
    ],
    "author_ids": [],
    "abstract": "Fueled by algorithmic advances, AI algorithms are increasingly being deployed\nin settings subject to unanticipated challenges with complex social effects.\nMotivated by real-world deployment of AI driven, social-network based suicide\nprevention and landslide risk management interventions, this paper focuses on\nrobust graph covering problems subject to group fairness constraints. We show\nthat, in the absence of fairness constraints, state-of-the-art algorithms for\nthe robust graph covering problem result in biased node coverage: they tend to\ndiscriminate individuals (nodes) based on membership in traditionally\nmarginalized groups. To mitigate this issue, we propose a novel formulation of\nthe robust graph covering problem with group fairness constraints and a\ntractable approximation scheme applicable to real-world instances. We provide a\nformal analysis of the price of group fairness (PoF) for this problem, where we\nshow that uncertainty can lead to greater PoF. We demonstrate the effectiveness\nof our approach on several real-world social networks. Our method yields\ncompetitive node coverage while significantly improving group fairness relative\nto state-of-the-art methods.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06865v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06852v2",
    "title": "Group-Fair Online Allocation in Continuous Time",
    "authors": [
      "Semih Cayci",
      "Swati Gupta",
      "Atilla Eryilmaz"
    ],
    "author_ids": [],
    "abstract": "The theory of discrete-time online learning has been successfully applied in\nmany problems that involve sequential decision-making under uncertainty.\nHowever, in many applications including contractual hiring in online\nfreelancing platforms and server allocation in cloud computing systems, the\noutcome of each action is observed only after a random and action-dependent\ntime. Furthermore, as a consequence of certain ethical and economic concerns,\nthe controller may impose deadlines on the completion of each task, and require\nfairness across different groups in the allocation of total time budget $B$. In\norder to address these applications, we consider continuous-time online\nlearning problem with fairness considerations, and present a novel framework\nbased on continuous-time utility maximization. We show that this formulation\nrecovers reward-maximizing, max-min fair and proportionally fair allocation\nrules across different groups as special cases. We characterize the optimal\noffline policy, which allocates the total time between different actions in an\noptimally fair way (as defined by the utility function), and impose deadlines\nto maximize time-efficiency. In the absence of any statistical knowledge, we\npropose a novel online learning algorithm based on dual ascent optimization for\ntime averages, and prove that it achieves $\\tilde{O}(B^{-1/2})$ regret bound.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06852v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06747v2",
    "title": "First-Order Methods for Large-Scale Market Equilibrium Computation",
    "authors": [
      "Yuan Gao",
      "Christian Kroer"
    ],
    "author_ids": [],
    "abstract": "Market equilibrium is a solution concept with many applications such as\ndigital ad markets, fair division, and resource sharing. For many classes of\nutility functions, equilibria can be captured by convex programs. We develop\nsimple first-order methods suitable for solving these convex programs for\nlarge-scale markets. We focus on three practically-relevant utility classes:\nlinear, quasilinear, and Leontief utilities. Using structural properties of\nmarket equilibria under each utility class, we show that the corresponding\nconvex programs can be reformulated as optimization of a structured smooth\nconvex function over a polyhedral set, for which projected gradient achieves\nlinear convergence. To do so, we utilize recent linear convergence results\nunder weakened strong-convexity conditions, and further refine the relevant\nconstants in existing convergence results. Then, we show that proximal gradient\n(a generalization of projected gradient) with a practical linesearch scheme\nachieves linear convergence under the Proximal-PL condition, a recently\ndeveloped error bound condition for convex composite problems. For quasilinear\nutilities, we show that Mirror Descent applied to a new convex program achieves\nsublinear last-iterate convergence and yields a form of Proportional Response\ndynamics, an elegant, interpretable algorithm for computing market equilibria\noriginally developed for linear utilities. Numerical experiments show that\nProportional Response dynamics is highly efficient for computing approximate\nmarket equilibria, while projected gradient with linesearch can be much faster\nwhen higher-accuracy solutions are needed.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06747v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.06606v2",
    "title": "What makes instance discrimination good for transfer learning?",
    "authors": [
      "Nanxuan Zhao",
      "Zhirong Wu",
      "Rynson W. H. Lau",
      "Stephen Lin"
    ],
    "author_ids": [],
    "abstract": "Contrastive visual pretraining based on the instance discrimination pretext\ntask has made significant progress. Notably, recent work on unsupervised\npretraining has shown to surpass the supervised counterpart for finetuning\ndownstream applications such as object detection and segmentation. It comes as\na surprise that image annotations would be better left unused for transfer\nlearning. In this work, we investigate the following problems: What makes\ninstance discrimination pretraining good for transfer learning? What knowledge\nis actually learned and transferred from these models? From this understanding\nof instance discrimination, how can we better exploit human annotation labels\nfor pretraining? Our findings are threefold. First, what truly matters for the\ntransfer is low-level and mid-level representations, not high-level\nrepresentations. Second, the intra-category invariance enforced by the\ntraditional supervised model weakens transferability by increasing task\nmisalignment. Finally, supervised pretraining can be strengthened by following\nan exemplar-based approach without explicit constraints among the instances\nwithin the same category.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06606v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06508v2",
    "title": "Deep learning reconstruction of digital breast tomosynthesis images for accurate breast density and patient-specific radiation dose estimation",
    "authors": [
      "Jonas Teuwen",
      "Nikita Moriakov",
      "Christian Fedon",
      "Marco Caballo",
      "Ingrid Reiser",
      "Pedrag Bakic",
      "Eloy García",
      "Oliver Diaz",
      "Koen Michielsen",
      "Ioannis Sechopoulos"
    ],
    "author_ids": [],
    "abstract": "The two-dimensional nature of mammography makes estimation of the overall\nbreast density challenging, and estimation of the true patient-specific\nradiation dose impossible. Digital breast tomosynthesis (DBT), a pseudo-3D\ntechnique, is now commonly used in breast cancer screening and diagnostics.\nStill, the severely limited 3rd dimension information in DBT has not been used,\nuntil now, to estimate the true breast density or the patient-specific dose.\nThis study proposes a reconstruction algorithm for DBT based on deep learning\nspecifically optimized for these tasks. The algorithm, which we name DBToR, is\nbased on unrolling a proximal-dual optimization method. The proximal operators\nare replaced with convolutional neural networks and prior knowledge is included\nin the model. This extends previous work on a deep learning-based\nreconstruction model by providing both the primal and the dual blocks with\nbreast thickness information, which is available in DBT. Training and testing\nof the model were performed using virtual patient phantoms from two different\nsources. Reconstruction performance, and accuracy in estimation of breast\ndensity and radiation dose, were estimated, showing high accuracy (density\n<+/-3%; dose <+/-20%) without bias, significantly improving on the current\nstate-of-the-art. This work also lays the groundwork for developing a deep\nlearning-based reconstruction algorithm for the task of image interpretation by\nradiologists.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "physics.med-ph",
      "cs.LG",
      "eess.IV",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06508v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06494v2",
    "title": "Anti-Transfer Learning for Task Invariance in Convolutional Neural Networks for Speech Processing",
    "authors": [
      "Eric Guizzo",
      "Tillman Weyde",
      "Giacomo Tarroni"
    ],
    "author_ids": [],
    "abstract": "We introduce the novel concept of anti-transfer learning for speech\nprocessing with convolutional neural networks. While transfer learning assumes\nthat the learning process for a target task will benefit from re-using\nrepresentations learned for another task, anti-transfer avoids the learning of\nrepresentations that have been learned for an orthogonal task, i.e., one that\nis not relevant and potentially misleading for the target task, such as speaker\nidentity for speech recognition or speech content for emotion recognition. In\nanti-transfer learning, we penalize similarity between activations of a network\nbeing trained and another one previously trained on an orthogonal task, which\nyields more suitable representations. This leads to better generalization and\nprovides a degree of control over correlations that are spurious or\nundesirable, e.g. to avoid social bias. We have implemented anti-transfer for\nconvolutional neural networks in different configurations with several\nsimilarity metrics and aggregation functions, which we evaluate and analyze\nwith several speech and audio tasks and settings, using six datasets. We show\nthat anti-transfer actually leads to the intended invariance to the orthogonal\ntask and to more appropriate features for the target task at hand.\nAnti-transfer learning consistently improves classification accuracy in all\ntest cases. While anti-transfer creates computation and memory cost at training\ntime, there is relatively little computation cost when using pre-trained models\nfor orthogonal tasks. Anti-transfer is widely applicable and particularly\nuseful where a specific invariance is desirable or where trained models are\navailable and labeled data for orthogonal tasks are difficult to obtain.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.NE",
      "cs.SD",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06494v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06465v1",
    "title": "DNF-Net: A Neural Architecture for Tabular Data",
    "authors": [
      "Ami Abutbul",
      "Gal Elidan",
      "Liran Katzir",
      "Ran El-Yaniv"
    ],
    "author_ids": [],
    "abstract": "A challenging open question in deep learning is how to handle tabular data.\nUnlike domains such as image and natural language processing, where deep\narchitectures prevail, there is still no widely accepted neural architecture\nthat dominates tabular data. As a step toward bridging this gap, we present\nDNF-Net a novel generic architecture whose inductive bias elicits models whose\nstructure corresponds to logical Boolean formulas in disjunctive normal form\n(DNF) over affine soft-threshold decision terms. In addition, DNF-Net promotes\nlocalized decisions that are taken over small subsets of the features. We\npresent an extensive empirical study showing that DNF-Nets significantly and\nconsistently outperform FCNs over tabular data. With relatively few\nhyperparameters, DNF-Nets open the door to practical end-to-end handling of\ntabular data using neural networks. We present ablation studies, which justify\nthe design choices of DNF-Net including the three inductive bias elements,\nnamely, Boolean formulation, locality, and feature selection.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06465v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06355v3",
    "title": "Improved Design of Quadratic Discriminant Analysis Classifier in Unbalanced Settings",
    "authors": [
      "Amine Bejaoui",
      "Khalil Elkhalil",
      "Abla Kammoun",
      "Mohamed Slim Alouni",
      "Tarek Al-Naffouri"
    ],
    "author_ids": [],
    "abstract": "The use of quadratic discriminant analysis (QDA) or its regularized version\n(R-QDA) for classification is often not recommended, due to its\nwell-acknowledged high sensitivity to the estimation noise of the covariance\nmatrix. This becomes all the more the case in unbalanced data settings for\nwhich it has been found that R-QDA becomes equivalent to the classifier that\nassigns all observations to the same class. In this paper, we propose an\nimproved R-QDA that is based on the use of two regularization parameters and a\nmodified bias, properly chosen to avoid inappropriate behaviors of R-QDA in\nunbalanced settings and to ensure the best possible classification performance.\nThe design of the proposed classifier builds on a refined asymptotic analysis\nof its performance when the number of samples and that of features grow large\nsimultaneously, which allows to cope efficiently with the high-dimensionality\nfrequently met within the big data paradigm. The performance of the proposed\nclassifier is assessed on both real and synthetic data sets and was shown to be\nmuch better than what one would expect from a traditional R-QDA.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06355v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06332v3",
    "title": "A Variational Approach to Privacy and Fairness",
    "authors": [
      "Borja Rodríguez-Gálvez",
      "Ragnar Thobaben",
      "Mikael Skoglund"
    ],
    "author_ids": [],
    "abstract": "In this article, we propose a new variational approach to learn private\nand/or fair representations. This approach is based on the Lagrangians of a new\nformulation of the privacy and fairness optimization problems that we propose.\nIn this formulation, we aim to generate representations of the data that keep a\nprescribed level of the relevant information that is not shared by the private\nor sensitive data, while minimizing the remaining information they keep. The\nproposed approach (i) exhibits the similarities of the privacy and fairness\nproblems, (ii) allows us to control the trade-off between utility and privacy\nor fairness through the Lagrange multiplier parameter, and (iii) can be\ncomfortably incorporated to common representation learning algorithms such as\nthe VAE, the $\\beta$-VAE, the VIB, or the nonlinear IB.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06332v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06300v1",
    "title": "Montreal AI Ethics Institute's Response to Scotland's AI Strategy",
    "authors": [
      "Abhishek Gupta"
    ],
    "author_ids": [],
    "abstract": "In January and February 2020, the Scottish Government released two documents\nfor review by the public regarding their artificial intelligence (AI) strategy.\nThe Montreal AI Ethics Institute (MAIEI) reviewed these documents and published\na response on 4 June 2020. MAIEI's response examines several questions that\ntouch on the proposed definition of AI; the people-centered nature of the\nstrategy; considerations to ensure that everyone benefits from AI; the\nstrategy's overarching vision; Scotland's AI ecosystem; the proposed strategic\nthemes; and how to grow public confidence in AI by building responsible and\nethical systems.\n  In addition to examining the points above, MAIEI suggests that the strategy\nbe extended to include considerations on biometric data and how that will be\nprocessed and used in the context of AI. It also highlights the importance of\ntackling head-on the inherently stochastic nature of deep learning systems and\ndeveloping concrete guidelines to ensure that these systems are built\nresponsibly and ethically, particularly as machine learning becomes more\naccessible. Finally, it concludes that any national AI strategy must clearly\naddress the measurements of success in regards to the strategy's stated goals\nand vision to ensure that they are interpreted and applied consistently. To do\nthis, there must be inclusion and transparency between those building the\nsystems and those using them in their work.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06300v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.08328v2",
    "title": "ETHOS: an Online Hate Speech Detection Dataset",
    "authors": [
      "Ioannis Mollas",
      "Zoe Chrysopoulou",
      "Stamatis Karlos",
      "Grigorios Tsoumakas"
    ],
    "author_ids": [],
    "abstract": "Online hate speech is a recent problem in our society that is rising at a\nsteady pace by leveraging the vulnerabilities of the corresponding regimes that\ncharacterise most social media platforms. This phenomenon is primarily fostered\nby offensive comments, either during user interaction or in the form of a\nposted multimedia context. Nowadays, giant corporations own platforms where\nmillions of users log in every day, and protection from exposure to similar\nphenomena appears to be necessary in order to comply with the corresponding\nlegislation and maintain a high level of service quality. A robust and reliable\nsystem for detecting and preventing the uploading of relevant content will have\na significant impact on our digitally interconnected society. Several aspects\nof our daily lives are undeniably linked to our social profiles, making us\nvulnerable to abusive behaviours. As a result, the lack of accurate hate speech\ndetection mechanisms would severely degrade the overall user experience,\nalthough its erroneous operation would pose many ethical concerns. In this\npaper, we present 'ETHOS', a textual dataset with two variants: binary and\nmulti-label, based on YouTube and Reddit comments validated using the\nFigure-Eight crowdsourcing platform. Furthermore, we present the annotation\nprotocol used to create this dataset: an active sampling procedure for\nbalancing our data in relation to the various aspects defined. Our key\nassumption is that, even gaining a small amount of labelled data from such a\ntime-consuming process, we can guarantee hate speech occurrences in the\nexamined material.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML",
      "I.2.6; I.2.7; I.5.4; H.2.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.08328v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06137v3",
    "title": "Analysis of Trade-offs in Fair Principal Component Analysis Based on Multi-objective Optimization",
    "authors": [
      "Guilherme D. Pelegrina",
      "Renan D. B. Brotto",
      "Leonardo T. Duarte",
      "Romis Attux",
      "João M. T. Romano"
    ],
    "author_ids": [],
    "abstract": "In dimensionality reduction problems, the adopted technique may produce\ndisparities between the representation errors of different groups. For\ninstance, in the projected space, a specific class can be better represented in\ncomparison with another one. In some situations, this unfair result may\nintroduce ethical concerns. Aiming at overcoming this inconvenience, a fairness\nmeasure can be considered when performing dimensionality reduction through\nPrincipal Component Analysis. However, a solution that increases fairness tends\nto increase the overall re-construction error. In this context, this paper\nproposes to address this trade-off by means of a multi-objective-based\napproach. For this purpose, we adopt a fairness measure associated with the\ndisparity between the representation errors of different groups. Moreover, we\ninvestigate if the solution of a classical Principal Component Analysis can be\nused to find a fair projection. Numerical experiments attest that a fairer\nresult can be achieved with a very small loss in the overall reconstruction\nerror.",
    "published_date": "2020-06-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06137v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06102v2",
    "title": "Multi-index Antithetic Stochastic Gradient Algorithm",
    "authors": [
      "Mateusz B. Majka",
      "Marc Sabate-Vidales",
      "Łukasz Szpruch"
    ],
    "author_ids": [],
    "abstract": "Stochastic Gradient Algorithms (SGAs) are ubiquitous in computational\nstatistics, machine learning and optimisation. Recent years have brought an\ninflux of interest in SGAs, and the non-asymptotic analysis of their bias is by\nnow well-developed. However, relatively little is known about the optimal\nchoice of the random approximation (e.g mini-batching) of the gradient in SGAs\nas this relies on the analysis of the variance and is problem specific. While\nthere have been numerous attempts to reduce the variance of SGAs, these\ntypically exploit a particular structure of the sampled distribution by\nrequiring a priori knowledge of its density's mode. It is thus unclear how to\nadapt such algorithms to non-log-concave settings. In this paper, we construct\na Multi-index Antithetic Stochastic Gradient Algorithm (MASGA) whose\nimplementation is independent of the structure of the target measure and which\nachieves performance on par with Monte Carlo estimators that have access to\nunbiased samples from the distribution of interest. In other words, MASGA is an\noptimal estimator from the mean square error-computational cost perspective\nwithin the class of Monte Carlo estimators. We prove this fact rigorously for\nlog-concave settings and verify it numerically for some examples where the\nlog-concavity assumption is not satisfied.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST",
      "stat.CO",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06102v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06094v1",
    "title": "Robust Grouped Variable Selection Using Distributionally Robust Optimization",
    "authors": [
      "Ruidi Chen",
      "Ioannis Ch. Paschalidis"
    ],
    "author_ids": [],
    "abstract": "We propose a Distributionally Robust Optimization (DRO) formulation with a\nWasserstein-based uncertainty set for selecting grouped variables under\nperturbations on the data for both linear regression and classification\nproblems. The resulting model offers robustness explanations for Grouped Least\nAbsolute Shrinkage and Selection Operator (GLASSO) algorithms and highlights\nthe connection between robustness and regularization. We prove probabilistic\nbounds on the out-of-sample loss and the estimation bias, and establish the\ngrouping effect of our estimator, showing that coefficients in the same group\nconverge to the same value as the sample correlation between covariates\napproaches 1. Based on this result, we propose to use the spectral clustering\nalgorithm with the Gaussian similarity function to perform grouping on the\npredictors, which makes our approach applicable without knowing the grouping\nstructure a priori. We compare our approach to an array of alternatives and\nprovide extensive numerical results on both synthetic data and a real large\ndataset of surgery-related medical records, showing that our formulation\nproduces an interpretable and parsimonious model that encourages sparsity at a\ngroup level and is able to achieve better prediction and estimation performance\nin the presence of outliers.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06094v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06082v3",
    "title": "Towards Integrating Fairness Transparently in Industrial Applications",
    "authors": [
      "Emily Dodwell",
      "Cheryl Flynn",
      "Balachander Krishnamurthy",
      "Subhabrata Majumdar",
      "Ritwik Mitra"
    ],
    "author_ids": [],
    "abstract": "Numerous Machine Learning (ML) bias-related failures in recent years have led\nto scrutiny of how companies incorporate aspects of transparency and\naccountability in their ML lifecycles. Companies have a responsibility to\nmonitor ML processes for bias and mitigate any bias detected, ensure business\nproduct integrity, preserve customer loyalty, and protect brand image.\nChallenges specific to industry ML projects can be broadly categorized into\nprincipled documentation, human oversight, and need for mechanisms that enable\ninformation reuse and improve cost efficiency. We highlight specific roadblocks\nand propose conceptual solutions on a per-category basis for ML practitioners\nand organizational subject matter experts. Our systematic approach tackles\nthese challenges by integrating mechanized and human-in-the-loop components in\nbias detection, mitigation, and documentation of projects at various stages of\nthe ML lifecycle. To motivate the implementation of our system -- SIFT (System\nto Integrate Fairness Transparently) -- we present its structural primitives\nwith an example real-world use case on how it can be used to identify potential\nbiases and determine appropriate mitigation strategies in a participatory\nmanner.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06082v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06053v2",
    "title": "Causal Feature Selection for Algorithmic Fairness",
    "authors": [
      "Sainyam Galhotra",
      "Karthikeyan Shanmugam",
      "Prasanna Sattigeri",
      "Kush R. Varshney"
    ],
    "author_ids": [],
    "abstract": "The use of machine learning (ML) in high-stakes societal decisions has\nencouraged the consideration of fairness throughout the ML lifecycle. Although\ndata integration is one of the primary steps to generate high quality training\ndata, most of the fairness literature ignores this stage. In this work, we\nconsider fairness in the integration component of data management, aiming to\nidentify features that improve prediction without adding any bias to the\ndataset. We work under the causal interventional fairness paradigm. Without\nrequiring the underlying structural causal model a priori, we propose an\napproach to identify a sub-collection of features that ensure the fairness of\nthe dataset by performing conditional independence tests between different\nsubsets of features. We use group testing to improve the complexity of the\napproach. We theoretically prove the correctness of the proposed algorithm to\nidentify features that ensure interventional fairness and show that sub-linear\nconditional independence tests are sufficient to identify these variables. A\ndetailed empirical evaluation is performed on real-world datasets to\ndemonstrate the efficacy and efficiency of our technique.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DB",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06053v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05963v1",
    "title": "Balancing Fairness and Efficiency in an Optimization Model",
    "authors": [
      "Violet Xinying Chen",
      "J. N. Hooker"
    ],
    "author_ids": [],
    "abstract": "Optimization models generally aim for efficiency by maximizing total benefit\nor minimizing cost. Yet a trade-off between fairness and efficiency is an\nimportant element of many practical decisions. We propose a principled and\npractical method for balancing these two criteria in an optimization model.\nFollowing a critical assessment of existing schemes, we define a set of social\nwelfare functions (SWFs) that combine Rawlsian leximax fairness and\nutilitarianism and overcome some of the weaknesses of previous approaches. In\nparticular, we regulate the equity/efficiency trade-off with a single parameter\nthat has a meaningful interpretation in practical contexts. We formulate the\nSWFs using mixed integer constraints and sequentially maximize them subject to\nconstraints that define the problem at hand. After providing practical\nstep-by-step instructions for implementation, we demonstrate the method on\nproblems of realistic size involving healthcare resource allocation and\ndisaster preparation. The solution times are modest, ranging from a fraction of\na second to 18 seconds for a given value of the trade-off parameter.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05963v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06548v1",
    "title": "Efficient democratic decisions via nondeterministic proportional consensus",
    "authors": [
      "Jobst Heitzig",
      "Forest W. Simmons"
    ],
    "author_ids": [],
    "abstract": "Are there voting methods which (i) give everyone, including minorities, an\nequal share of effective power even if voters act strategically, (ii) promote\nconsensus rather than polarization and inequality, and (iii) do not favour the\nstatus quo or rely too much on chance?\n  We show the answer is yes by describing two nondeterministic voting methods,\none based on automatic bargaining over lotteries, the other on conditional\ncommitments to approve compromise options. Our theoretical analysis and\nagent-based simulation experiments suggest that with these, majorities cannot\nconsistently suppress minorities as with deterministic methods, proponents of\nthe status quo cannot block decisions as in consensus-based approaches, the\nresulting aggregate welfare is comparable to existing methods, and average\nrandomness is lower than for other nondeterministic methods.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "econ.GN",
      "cs.GT",
      "cs.MA",
      "q-fin.EC",
      "91B14, 91A80, 91B12",
      "J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06548v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.05868v1",
    "title": "Improving Dependability of Neuromorphic Computing With Non-Volatile Memory",
    "authors": [
      "Shihao Song",
      "Anup Das",
      "Nagarajan Kandasamy"
    ],
    "author_ids": [],
    "abstract": "As process technology continues to scale aggressively, circuit aging in a\nneuromorphic hardware due to negative bias temperature instability (NBTI) and\ntime-dependent dielectric breakdown (TDDB) is becoming a critical reliability\nissue and is expected to proliferate when using non-volatile memory (NVM) for\nsynaptic storage. This is because an NVM requires high voltage and current to\naccess its synaptic weight, which further accelerates the circuit aging in a\nneuromorphic hardware. Current methods for qualifying reliability are overly\nconservative, since they estimate circuit aging considering worst-case\noperating conditions and unnecessarily constrain performance. This paper\nproposes RENEU, a reliability-oriented approach to map machine learning\napplications to neuromorphic hardware, with the aim of improving system-wide\nreliability without compromising key performance metrics such as execution time\nof these applications on the hardware. Fundamental to RENEU is a novel\nformulation of the aging of CMOS-based circuits in a neuromorphic hardware\nconsidering different failure mechanisms. Using this formulation, RENEU\ndevelops a system-wide reliability model which can be used inside a\ndesign-space exploration framework involving the mapping of neurons and\nsynapses to the hardware. To this end, RENEU uses an instance of Particle Swarm\nOptimization (PSO) to generate mappings that are Pareto-optimal in terms of\nperformance and reliability. We evaluate RENEU using different machine learning\napplications on a state-of-the-art neuromorphic hardware with NVM synapses. Our\nresults demonstrate an average 38\\% reduction in circuit aging, leading to an\naverage 18% improvement in the lifetime of the hardware compared to current\npractices. RENEU only introduces a marginal performance overhead of 5% compared\nto a performance-oriented state-of-the-art.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.AR",
      "cs.ET"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05868v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05839v1",
    "title": "Weakly Secure Symmetric Multilevel Diversity Coding",
    "authors": [
      "Tao Guo",
      "Chao Tian",
      "Tie Liu",
      "Raymond W. Yeung"
    ],
    "author_ids": [],
    "abstract": "Multilevel diversity coding is a classical coding model where multiple\nmutually independent information messages are encoded, such that different\nreliability requirements can be afforded to different messages. It is well\nknown that {\\em superposition coding}, namely separately encoding the\nindependent messages, is optimal for symmetric multilevel diversity coding\n(SMDC) (Yeung-Zhang 1999). In the current paper, we consider weakly secure SMDC\nwhere security constraints are injected on each individual message, and provide\na complete characterization of the conditions under which superposition coding\nis sum-rate optimal. Two joint coding strategies, which lead to rate savings\ncompared to superposition coding, are proposed, where some coding components\nfor one message can be used as the encryption key for another. By applying\ndifferent variants of Han's inequality, we show that the lack of opportunity to\napply these two coding strategies directly implies the optimality of\nsuperposition coding. It is further shown that under a set of particular\nsecurity constraints, one of the proposed joint coding strategies can be used\nto construct a code that achieves the optimal rate region.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05839v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.05838v2",
    "title": "To Regularize or Not To Regularize? The Bias Variance Trade-off in Regularized AEs",
    "authors": [
      "Arnab Kumar Mondal",
      "Himanshu Asnani",
      "Parag Singla",
      "Prathosh AP"
    ],
    "author_ids": [],
    "abstract": "Regularized Auto-Encoders (RAEs) form a rich class of neural generative\nmodels. They effectively model the joint-distribution between the data and the\nlatent space using an Encoder-Decoder combination, with regularization imposed\nin terms of a prior over the latent space. Despite their advantages, such as\nstability in training, the performance of AE based models has not reached the\nsuperior standards as that of the other generative models such as Generative\nAdversarial Networks (GANs). Motivated by this, we examine the effect of the\nlatent prior on the generation quality of deterministic AE models in this\npaper. Specifically, we consider the class of RAEs with deterministic\nEncoder-Decoder pairs, Wasserstein Auto-Encoders (WAE), and show that having a\nfixed prior distribution, \\textit{a priori}, oblivious to the dimensionality of\nthe `true' latent space, will lead to the infeasibility of the optimization\nproblem considered. Further, we show that, in the finite data regime, despite\nknowing the correct latent dimensionality, there exists a bias-variance\ntrade-off with any arbitrary prior imposition. As a remedy to both the issues\nmentioned above, we introduce an additional state space in the form of flexibly\nlearnable latent priors, in the optimization objective of the WAEs. We\nimplicitly learn the distribution of the latent prior jointly with the AE\ntraining, which not only makes the learning objective feasible but also\nfacilitates operation on different points of the bias-variance curve. We show\nthe efficacy of our model, called FlexAE, through several experiments on\nmultiple datasets, and demonstrate that it is the new state-of-the-art for the\nAE based generative models.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05838v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05754v1",
    "title": "Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus",
    "authors": [
      "Luisa Bentivogli",
      "Beatrice Savoldi",
      "Matteo Negri",
      "Mattia Antonino Di Gangi",
      "Roldano Cattoni",
      "Marco Turchi"
    ],
    "author_ids": [],
    "abstract": "Translating from languages without productive grammatical gender like English\ninto gender-marked languages is a well-known difficulty for machines. This\ndifficulty is also due to the fact that the training data on which models are\nbuilt typically reflect the asymmetries of natural languages, gender bias\nincluded. Exclusively fed with textual data, machine translation is\nintrinsically constrained by the fact that the input sentence does not always\ncontain clues about the gender identity of the referred human entities. But\nwhat happens with speech translation, where the input is an audio signal? Can\naudio provide additional information to reduce gender bias? We present the\nfirst thorough investigation of gender bias in speech translation, contributing\nwith: i) the release of a benchmark useful for future studies, and ii) the\ncomparison of different technologies (cascade and end-to-end) on two language\ndirections (English-Italian/French).",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05726v2",
    "title": "Estimating semantic structure for the VQA answer space",
    "authors": [
      "Corentin Kervadec",
      "Grigory Antipov",
      "Moez Baccouche",
      "Christian Wolf"
    ],
    "author_ids": [],
    "abstract": "Since its appearance, Visual Question Answering (VQA, i.e. answering a\nquestion posed over an image), has always been treated as a classification\nproblem over a set of predefined answers. Despite its convenience, this\nclassification approach poorly reflects the semantics of the problem limiting\nthe answering to a choice between independent proposals, without taking into\naccount the similarity between them (e.g. equally penalizing for answering cat\nor German shepherd instead of dog). We address this issue by proposing (1) two\nmeasures of proximity between VQA classes, and (2) a corresponding loss which\ntakes into account the estimated proximity. This significantly improves the\ngeneralization of VQA models by reducing their language bias. In particular, we\nshow that our approach is completely model-agnostic since it allows consistent\nimprovements with three different VQA models. Finally, by combining our method\nwith a language bias reduction approach, we report SOTA-level performance on\nthe challenging VQAv2-CP dataset.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05726v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05722v1",
    "title": "Interferometric Graph Transform: a Deep Unsupervised Graph Representation",
    "authors": [
      "Edouard Oyallon"
    ],
    "author_ids": [],
    "abstract": "We propose the Interferometric Graph Transform (IGT), which is a new class of\ndeep unsupervised graph convolutional neural network for building graph\nrepresentations. Our first contribution is to propose a generic, complex-valued\nspectral graph architecture obtained from a generalization of the Euclidean\nFourier transform. We show that our learned representation consists of both\ndiscriminative and invariant features, thanks to a novel greedy concave\nobjective. From our experiments, we conclude that our learning procedure\nexploits the topology of the spectral domain, which is normally a flaw of\nspectral methods, and in particular our method can recover an analytic operator\nfor vision tasks. We test our algorithm on various and challenging tasks such\nas image classification (MNIST, CIFAR-10), community detection (Authorship,\nFacebook graph) and action recognition from 3D skeletons videos (SBU, NTU),\nexhibiting a new state-of-the-art in spectral graph unsupervised settings.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05722v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05645v3",
    "title": "Hypergraph Clustering for Finding Diverse and Experienced Groups",
    "authors": [
      "Ilya Amburg",
      "Nate Veldt",
      "Austin R. Benson"
    ],
    "author_ids": [],
    "abstract": "When forming a team or group of individuals, we often seek a balance of\nexpertise in a particular task while at the same time maintaining diversity of\nskills within each group. Here, we view the problem of finding diverse and\nexperienced groups as clustering in hypergraphs with multiple edge types. The\ninput data is a hypergraph with multiple hyperedge types -- representing\ninformation about past experiences of groups of individuals -- and the output\nis groups of nodes. In contrast to related problems on fair or balanced\nclustering, we model diversity in terms of variety of past experience (instead\nof, e.g., protected attributes), with a goal of forming groups that have both\nexperience and diversity with respect to participation in edge types. In other\nwords, both diversity and experience are measured from the types of the\nhyperedges.\n  Our clustering model is based on a regularized version of an edge-based\nhypergraph clustering objective, and we also show how naive objectives actually\nhave no diversity-experience tradeoff. Although our objective function is\nNP-hard to optimize, we design an efficient 2-approximation algorithm and also\nshow how to compute bounds for the regularization hyperparameter that lead to\nmeaningful diversity-experience tradeoffs. We demonstrate an application of\nthis framework in online review platforms, where the goal is to curate sets of\nuser reviews for a product type. In this context, \"experience\" corresponds to\nusers familiar with the type of product, and \"diversity\" to users that have\nreviewed related products.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.IR",
      "cs.LG",
      "physics.soc-ph",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05645v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05603v1",
    "title": "Using an expert deviation carrying the knowledge of climate data in usual clustering algorithms",
    "authors": [
      "Emmanuel Biabiany",
      "Vincent Page",
      "Didier Bernard",
      "Hélène Paugam-Moisy"
    ],
    "author_ids": [],
    "abstract": "In order to help physicists to expand their knowledge of the climate in the\nLesser Antilles, we aim to identify the spatio-temporal configurations using\nclustering analysis on wind speed and cumulative rainfall datasets. But we show\nthat using the L2 norm in conventional clustering methods as K-Means (KMS) and\nHierarchical Agglomerative Clustering (HAC) can induce undesirable effects. So,\nwe propose to replace Euclidean distance (L2) by a dissimilarity measure named\nExpert Deviation (ED). Based on the symmetrized Kullback-Leibler divergence,\nthe ED integrates the properties of the observed physical parameters and\nclimate knowledge. This measure helps comparing histograms of four patches,\ncorresponding to geographical zones, that are influenced by atmospheric\nstructures. The combined evaluation of the internal homogeneity and the\nseparation of the clusters obtained using ED and L2 was performed. The results,\nwhich are compared using the silhouette index, show five clusters with high\nindexes. For the two available datasets one can see that, unlike KMS-L2, KMS-ED\ndiscriminates the daily situations favorably, giving more physical meaning to\nthe clusters discovered by the algorithm. The effect of patches is observed in\nthe spatial analysis of representative elements for KMS-ED. The ED is able to\nproduce different configurations which makes the usual atmospheric structures\nclearly identifiable. Atmospheric physicists can interpret the locations of the\nimpact of each cluster on a specific zone according to atmospheric structures.\nKMS-L2 does not lead to such an interpretability, because the situations\nrepresented are spatially quite smooth. This climatological study illustrates\nthe advantage of using ED as a new approach.",
    "published_date": "2020-06-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "physics.ao-ph",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05603v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05487v2",
    "title": "Probably Approximately Correct Constrained Learning",
    "authors": [
      "Luiz F. O. Chamon",
      "Alejandro Ribeiro"
    ],
    "author_ids": [],
    "abstract": "As learning solutions reach critical applications in social, industrial, and\nmedical domains, the need to curtail their behavior has become paramount. There\nis now ample evidence that without explicit tailoring, learning can lead to\nbiased, unsafe, and prejudiced solutions. To tackle these problems, we develop\na generalization theory of constrained learning based on the probably\napproximately correct (PAC) learning framework. In particular, we show that\nimposing requirements does not make a learning problem harder in the sense that\nany PAC learnable class is also PAC constrained learnable using a constrained\ncounterpart of the empirical risk minimization (ERM) rule. For typical\nparametrized models, however, this learner involves solving a constrained\nnon-convex optimization program for which even obtaining a feasible solution is\nchallenging. To overcome this issue, we prove that under mild conditions the\nempirical dual problem of constrained learning is also a PAC constrained\nlearner that now leads to a practical constrained learning algorithm based\nsolely on solving unconstrained problems. We analyze the generalization\nproperties of this solution and use it to illustrate how constrained learning\ncan address problems in fair and robust classification.",
    "published_date": "2020-06-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05487v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05483v1",
    "title": "Blockchain in the management of science: conceptual models, promises and challenges",
    "authors": [
      "Artyom Kosmarski"
    ],
    "author_ids": [],
    "abstract": "Blockchain has received much attention recently, due to its promises of\nverifiable, permanent, decentralized, and efficient data handling. In 2017-2019\nblockchain and associated technologies such as smart contracts has progressed\nbeyond cryptocurrencies, and has been adopted in banking, retail, healthcare,\nand other fields. This study critically examines recent applications of\nblockchain in science, touching upon different stages of research cycle, from\ndata management to publishing, peer review, research evaluation and funding.\nThe paper is based upon a review of blockchain projects, relevant literature, a\nset of interviews and focus groups with startup founders, scholars, librarians,\nIT experts from the EU, USA, Russia, and Belarus. Proponents of blockchain for\nscience present this technology as a tool to make science free from bias, red\ntape, data fraud, as well as provide innovative means to secure financial\nbacking for new ideas. However, these projects face a set of challenges. One\nissue concerns introducing crypto economy, with its financial incentives, into\nscience, a field that emphasizes disinterested and non-pecuniary pursuit of\ntruth. Another source of concern relates to the ongoing conflict between the\nprinciple of decentralization inherent to blockchain and the practice of\nforcing it from above, by the state and other centralized entities.",
    "published_date": "2020-06-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05483v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.05255v1",
    "title": "DeepFair: Deep Learning for Improving Fairness in Recommender Systems",
    "authors": [
      "Jesús Bobadilla",
      "Raúl Lara-Cabrera",
      "Ángel González-Prieto",
      "Fernando Ortega"
    ],
    "author_ids": [],
    "abstract": "The lack of bias management in Recommender Systems leads to minority groups\nreceiving unfair recommendations. Moreover, the trade-off between equity and\nprecision makes it difficult to obtain recommendations that meet both criteria.\nHere we propose a Deep Learning based Collaborative Filtering algorithm that\nprovides recommendations with an optimum balance between fairness and accuracy\nwithout knowing demographic information about the users. Experimental results\nshow that it is possible to make fair recommendations without losing a\nsignificant proportion of accuracy.",
    "published_date": "2020-06-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05255v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05203v2",
    "title": "The Tragedy of the AI Commons",
    "authors": [
      "Travis LaCroix",
      "Aydin Mohseni"
    ],
    "author_ids": [],
    "abstract": "Policy and guideline proposals for ethical artificial-intelligence research\nhave proliferated in recent years. These are supposed to guide the\nsocially-responsible development of AI for the common good. However, there\ntypically exist incentives for non-cooperation (i.e., non-adherence to such\npolicies and guidelines); and, these proposals often lack effective mechanisms\nto enforce their own normative claims. The situation just described constitutes\na social dilemma; namely, a situation where no one has an individual incentive\nto cooperate, though mutual cooperation would lead to the best outcome for all\ninvolved. In this paper, we use stochastic evolutionary game dynamics to model\nthis social dilemma in the context of the ethical development of artificial\nintelligence. This formalism allows us to isolate variables that may be\nintervened upon, thus providing actionable suggestions for increased\ncooperation amongst numerous stakeholders in AI. Our results show how\nstochastic effects can help make cooperation viable in such a scenario. They\nsuggest that coordination for a common good should be attempted in smaller\ngroups in which the cost for cooperation is low, and the perceived risk of\nfailure is high. This provides insight into the conditions under which we\nshould expect such ethics proposals to be successful with regard to their\nscope, scale, and content.",
    "published_date": "2020-06-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05203v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.05109v3",
    "title": "Fair Bayesian Optimization",
    "authors": [
      "Valerio Perrone",
      "Michele Donini",
      "Muhammad Bilal Zafar",
      "Robin Schmucker",
      "Krishnaram Kenthapadi",
      "Cédric Archambeau"
    ],
    "author_ids": [],
    "abstract": "Given the increasing importance of machine learning (ML) in our lives,\nseveral algorithmic fairness techniques have been proposed to mitigate biases\nin the outcomes of the ML models. However, most of these techniques are\nspecialized to cater to a single family of ML models and a specific definition\nof fairness, limiting their adaptibility in practice. We introduce a general\nconstrained Bayesian optimization (BO) framework to optimize the performance of\nany ML model while enforcing one or multiple fairness constraints. BO is a\nmodel-agnostic optimization method that has been successfully applied to\nautomatically tune the hyperparameters of ML models. We apply BO with fairness\nconstraints to a range of popular models, including random forests, gradient\nboosting, and neural networks, showing that we can obtain accurate and fair\nsolutions by acting solely on the hyperparameters. We also show empirically\nthat our approach is competitive with specialized techniques that enforce\nmodel-specific fairness constraints, and outperforms preprocessing methods that\nlearn fair representations of the input data. Moreover, our method can be used\nin synergy with such specialized fairness techniques to tune their\nhyperparameters. Finally, we study the relationship between fairness and the\nhyperparameters selected by BO. We observe a correlation between regularization\nand unbiased models, explaining why acting on the hyperparameters leads to ML\nmodels that generalize well and are fair.",
    "published_date": "2020-06-09T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05109v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04996v1",
    "title": "Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation",
    "authors": [
      "Xiang Jiang",
      "Qicheng Lao",
      "Stan Matwin",
      "Mohammad Havaei"
    ],
    "author_ids": [],
    "abstract": "We present an approach for unsupervised domain adaptation---with a strong\nfocus on practical considerations of within-domain class imbalance and\nbetween-domain class distribution shift---from a class-conditioned domain\nalignment perspective. Current methods for class-conditioned domain alignment\naim to explicitly minimize a loss function based on pseudo-label estimations of\nthe target domain. However, these methods suffer from pseudo-label bias in the\nform of error accumulation. We propose a method that removes the need for\nexplicit optimization of model parameters from pseudo-labels directly. Instead,\nwe present a sampling-based implicit alignment approach, where the sample\nselection procedure is implicitly guided by the pseudo-labels. Theoretical\nanalysis reveals the existence of a domain-discriminator shortcut in misaligned\nclasses, which is addressed by the proposed implicit alignment approach to\nfacilitate domain-adversarial learning. Empirical results and ablation studies\nconfirm the effectiveness of the proposed approach, especially in the presence\nof within-domain class imbalance and between-domain class distribution shift.",
    "published_date": "2020-06-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML",
      "68T07"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04996v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04960v1",
    "title": "A Notion of Individual Fairness for Clustering",
    "authors": [
      "Matthäus Kleindessner",
      "Pranjal Awasthi",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "A common distinction in fair machine learning, in particular in fair\nclassification, is between group fairness and individual fairness. In the\ncontext of clustering, group fairness has been studied extensively in recent\nyears; however, individual fairness for clustering has hardly been explored. In\nthis paper, we propose a natural notion of individual fairness for clustering.\nOur notion asks that every data point, on average, is closer to the points in\nits own cluster than to the points in any other cluster. We study several\nquestions related to our proposed notion of individual fairness. On the\nnegative side, we show that deciding whether a given data set allows for such\nan individually fair clustering in general is NP-hard. On the positive side,\nfor the special case of a data set lying on the real line, we propose an\nefficient dynamic programming approach to find an individually fair clustering.\nFor general data sets, we investigate heuristics aimed at minimizing the number\nof individual fairness violations and compare them to standard clustering\napproaches on real data sets.",
    "published_date": "2020-06-08T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04960v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04778v3",
    "title": "Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees",
    "authors": [
      "L. Elisa Celis",
      "Lingxiao Huang",
      "Vijay Keswani",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "We present an optimization framework for learning a fair classifier in the\npresence of noisy perturbations in the protected attributes. Compared to prior\nwork, our framework can be employed with a very general class of linear and\nlinear-fractional fairness constraints, can handle multiple, non-binary\nprotected attributes, and outputs a classifier that comes with provable\nguarantees on both accuracy and fairness. Empirically, we show that our\nframework can be used to attain either statistical rate or false positive rate\nfairness guarantees with a minimal loss in accuracy, even when the noise is\nlarge, in two real-world datasets.",
    "published_date": "2020-06-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04778v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.06531v1",
    "title": "A Comparative Study of U-Net Topologies for Background Removal in Histopathology Images",
    "authors": [
      "Abtin Riasatian",
      "Maral Rasoolijaberi",
      "Morteza Babaei",
      "H. R. Tizhoosh"
    ],
    "author_ids": [],
    "abstract": "During the last decade, the digitization of pathology has gained considerable\nmomentum. Digital pathology offers many advantages including more efficient\nworkflows, easier collaboration as well as a powerful venue for telepathology.\nAt the same time, applying Computer-Aided Diagnosis (CAD) on Whole Slide Images\n(WSIs) has received substantial attention as a direct result of the\ndigitization. The first step in any image analysis is to extract the tissue.\nHence, background removal is an essential prerequisite for efficient and\naccurate results for many algorithms. In spite of the obvious discrimination\nfor human operators, the identification of tissue regions in WSIs could be\nchallenging for computers, mainly due to the existence of color variations and\nartifacts. Moreover, some cases such as alveolar tissue types, fatty tissues,\nand tissues with poor staining are difficult to detect. In this paper, we\nperform experiments on U-Net architecture with different network backbones\n(different topologies) to remove the background as well as artifacts from WSIs\nin order to extract the tissue regions. We compare a wide range of backbone\nnetworks including MobileNet, VGG16, EfficientNet-B3, ResNet50, ResNext101 and\nDenseNet121. We trained and evaluated the network on a manually labeled subset\nof The Cancer Genome Atlas (TCGA) Dataset. EfficientNet-B3 and MobileNet by\nalmost 99% sensitivity and specificity reached the best results.",
    "published_date": "2020-06-08T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.06531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04599v6",
    "title": "Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms",
    "authors": [
      "Akshat Pandey",
      "Aylin Caliskan"
    ],
    "author_ids": [],
    "abstract": "Ridehailing applications that collect mobility data from individuals to\ninform smart city planning predict each trip's fare pricing with automated\nalgorithms that rely on artificial intelligence (AI). This type of AI\nalgorithm, namely a price discrimination algorithm, is widely used in the\nindustry's black box systems for dynamic individualized pricing. Lacking\ntransparency, studying such AI systems for fairness and disparate impact has\nnot been possible without access to data used in generating the outcomes of\nprice discrimination algorithms. Recently, in an effort to enhance transparency\nin city planning, the city of Chicago regulation mandated that transportation\nproviders publish anonymized data on ridehailing. As a result, we present the\nfirst large-scale measurement of the disparate impact of price discrimination\nalgorithms used by ridehailing applications.\n  The application of random effects models from the meta-analysis literature\ncombines the city-level effects of AI bias on fare pricing from census tract\nattributes, aggregated from the American Community Survey. An analysis of 100\nmillion ridehailing samples from the city of Chicago indicates a significant\ndisparate impact in fare pricing of neighborhoods due to AI bias learned from\nridehailing utilization patterns associated with demographic attributes.\nNeighborhoods with larger non-white populations, higher poverty levels, younger\nresidents, and high education levels are significantly associated with higher\nfare prices, with combined effect sizes, measured in Cohen's d, of -0.32,\n-0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods\nhold promise for identifying and addressing the sources of disparate impact in\nAI algorithms learning from datasets that contain U.S. geolocations.",
    "published_date": "2020-06-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04599v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04541v2",
    "title": "Ethical Considerations and Statistical Analysis of Industry Involvement in Machine Learning Research",
    "authors": [
      "Thilo Hagendorff",
      "Kristof Meding"
    ],
    "author_ids": [],
    "abstract": "Industry involvement in the machine learning (ML) community seems to be\nincreasing. However, the quantitative scale and ethical implications of this\ninfluence are rather unknown. For this purpose, we have not only carried out an\ninformed ethical analysis of the field, but have inspected all papers of the\nmain ML conferences NeurIPS, CVPR, and ICML of the last 5 years - almost 11,000\npapers in total. Our statistical approach focuses on conflicts of interest,\ninnovation and gender equality. We have obtained four main findings: (1)\nAcademic-corporate collaborations are growing in numbers. At the same time, we\nfound that conflicts of interest are rarely disclosed. (2) Industry publishes\npapers about trending ML topics on average two years earlier than academia\ndoes. (3) Industry papers are not lagging behind academic papers in regard to\nsocial impact considerations. (4) Finally, we demonstrate that industrial\npapers fall short of their academic counterparts with respect to the ratio of\ngender diversity. We believe that this work is a starting point for an informed\ndebate within and outside of the ML community.",
    "published_date": "2020-06-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04541v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04381v1",
    "title": "Balance-Subsampled Stable Prediction",
    "authors": [
      "Kun Kuang",
      "Hengtao Zhang",
      "Fei Wu",
      "Yueting Zhuang",
      "Aijun Zhang"
    ],
    "author_ids": [],
    "abstract": "In machine learning, it is commonly assumed that training and test data share\nthe same population distribution. However, this assumption is often violated in\npractice because the sample selection bias may induce the distribution shift\nfrom training data to test data. Such a model-agnostic distribution shift\nusually leads to prediction instability across unknown test data. In this\npaper, we propose a novel balance-subsampled stable prediction (BSSP) algorithm\nbased on the theory of fractional factorial design. It isolates the clear\neffect of each predictor from the confounding variables. A design-theoretic\nanalysis shows that the proposed method can reduce the confounding effects\namong predictors induced by the distribution shift, hence improve both the\naccuracy of parameter estimation and prediction stability. Numerical\nexperiments on both synthetic and real-world data sets demonstrate that our\nBSSP algorithm significantly outperforms the baseline methods for stable\nprediction across unknown test data.",
    "published_date": "2020-06-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04381v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04282v2",
    "title": "Equality of Learning Opportunity via Individual Fairness in Personalized Recommendations",
    "authors": [
      "Mirko Marras",
      "Ludovico Boratto",
      "Guilherme Ramos",
      "Gianni Fenu"
    ],
    "author_ids": [],
    "abstract": "Online educational platforms are playing a primary role in mediating the\nsuccess of individuals' careers. Therefore, while building overlying content\nrecommendation services, it becomes essential to guarantee that learners are\nprovided with equal recommended learning opportunities, according to the\nplatform values, context, and pedagogy. Though the importance of ensuring\nequality of learning opportunities has been well investigated in traditional\ninstitutions, how this equality can be operationalized in online learning\necosystems through recommender systems is still under-explored. In this paper,\nwe formalize educational principles that model recommendations' learning\nproperties, and a novel fairness metric that combines them in order to monitor\nthe equality of recommended learning opportunities among learners. Then, we\nenvision a scenario wherein an educational platform should be arranged in such\na way that the generated recommendations meet each principle to a certain\ndegree for all learners, constrained to their individual preferences. Under\nthis view, we explore the learning opportunities provided by recommender\nsystems in a large-scale course platform, uncovering systematic inequalities.\nTo reduce this effect, we propose a novel post-processing approach that\nbalances personalization and equality of recommended opportunities. Experiments\nshow that our approach leads to higher equality, with a negligible loss in\npersonalization. Our study moves a step forward in operationalizing the ethics\nof human learning in recommendations, a core unit of intelligent educational\nsystems.",
    "published_date": "2020-06-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04282v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.04279v3",
    "title": "Interplay between Upsampling and Regularization for Provider Fairness in Recommender Systems",
    "authors": [
      "Ludovico Boratto",
      "Gianni Fenu",
      "Mirko Marras"
    ],
    "author_ids": [],
    "abstract": "Considering the impact of recommendations on item providers is one of the\nduties of multi-sided recommender systems. Item providers are key stakeholders\nin online platforms, and their earnings and plans are influenced by the\nexposure their items receive in recommended lists. Prior work showed that\ncertain minority groups of providers, characterized by a common sensitive\nattribute (e.g., gender or race), are being disproportionately affected by\nindirect and unintentional discrimination. Our study in this paper handles a\nsituation where ($i$) the same provider is associated with multiple items of a\nlist suggested to a user, ($ii$) an item is created by more than one provider\njointly, and ($iii$) predicted user-item relevance scores are biasedly\nestimated for items of provider groups. Under this scenario, we assess\ndisparities in relevance, visibility, and exposure, by simulating diverse\nrepresentations of the minority group in the catalog and the interactions.\nBased on emerged unfair outcomes, we devise a treatment that combines\nobservation upsampling and loss regularization, while learning user-item\nrelevance scores. Experiments on real-world data demonstrate that our treatment\nleads to lower disparate relevance. The resulting recommended lists show fairer\nvisibility and exposure, higher minority item coverage, and negligible loss in\nrecommendation utility.",
    "published_date": "2020-06-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04279v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.04124v1",
    "title": "On the Complexity of Branching Proofs",
    "authors": [
      "Daniel Dadush",
      "Samarth Tiwari"
    ],
    "author_ids": [],
    "abstract": "We consider the task of proving integer infeasibility of a bounded convex $K$\nin $\\mathbb{R}^n$ using a general branching proof system. In a general\nbranching proof, one constructs a branching tree by adding an integer\ndisjunction $\\mathbf{a} \\mathbf{x} \\leq b$ or $\\mathbf{a} \\mathbf{x} \\geq b+1$,\n$\\mathbf{a} \\in \\mathbb{Z}^n$, $b \\in \\mathbb{Z}$, at each node, such that the\nleaves of the tree correspond to empty sets (i.e., $K$ together with the\ninequalities picked up from the root to leaf is empty). Recently, Beame et al\n(ITCS 2018), asked whether the bit size of the coefficients in a branching\nproof, which they named stabbing planes (SP) refutations, for the case of\npolytopes derived from SAT formulas, can be assumed to be polynomial in $n$. We\nresolve this question by showing that any branching proof can be recompiled so\nthat the integer disjunctions have coefficients of size at most $(n\nR)^{O(n^2)}$, where $R \\in \\mathbb{N}$ such that $K \\in R \\mathbb{B}_1^n$,\nwhile increasing the number of nodes in the branching tree by at most a factor\n$O(n)$. As our second contribution, we show that Tseitin formulas, an important\nclass of infeasible SAT instances, have quasi-polynomial sized cutting plane\n(CP) refutations, disproving the conjecture that Tseitin formulas are\n(exponentially) hard for CP. As our final contribution, we give a simple family\nof polytopes in $[0,1]^n$ requiring branching proofs of length $2^n/n$.",
    "published_date": "2020-06-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CC",
      "cs.DM",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04124v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.04055v2",
    "title": "Green Resource Allocation and Energy Management in Heterogeneous Small Cell Networks Powered by Hybrid Energy",
    "authors": [
      "Qiaoni Han",
      "Bo Yang",
      "Nan Song",
      "Yuwei Li",
      "Ping Wei"
    ],
    "author_ids": [],
    "abstract": "In heterogeneous networks (HetNets), how to improve spectrum efficiency is a\ncrucial issue. Meanwhile increased energy consumption inspires network\noperators to deploy renewable energy sources as assistance to traditional\nelectricity. Based on above aspects, we allow base stations (BSs) to share\ntheir licensed spectrum resource with each other and adjust transmission power\nto adapt to the renewable energy level. Considering the sharing fairness among\nBSs, we formulate a multi-person bargaining problem as a stochastic\noptimization problem. We divide the optimization problem into three parts: data\nrate control, resource allocation and energy management. An online dynamic\ncontrol algorithm is proposed to control admission rate and resource allocation\nto maximize the transmission and sharing profits with the least grid energy\nconsumption. Simulation results investigate the time-varying data control and\nenergy management of BSs and demonstrate the effectiveness of the proposed\nscheme.",
    "published_date": "2020-06-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04055v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.04029v1",
    "title": "Ethics, Data Science, and Health and Human Services: Embedded Bias in Policy Approaches to Teen Pregnancy Prevention",
    "authors": [
      "Davon Woodard",
      "Huthaifa I. Ashqar",
      "Taoran Ji"
    ],
    "author_ids": [],
    "abstract": "Background: This study aims to evaluate the Chicago Teen Pregnancy Prevention\nInitiative delivery optimization outcomes given policy-neutral and\npolicy-focused approaches to deliver this program to at-risk teens across the\nCity of Chicago. Methods: We collect and compile several datasets from public\nsources including: Chicago Department of Public Health clinic locations, two\npublic health statistics datasets, census data of Chicago, list of Chicago\npublic high schools, and their Locations. Our policy-neutral approach will\nconsist of an equal distribution of funds and resources to schools and centers,\nregardless of past trends and outcomes. The policy-focused approaches will\nevaluate two models: first, a funding model based on prediction models from\nhistorical data; and second, a funding model based on economic and social\noutcomes for communities. Results: Results of this study confirms our initial\nhypothesis, that even though the models are optimized from a machine learning\nperspective, there is still possible that the models will produce wildly\ndifferent results in the real-world application. Conclusions: When ethics and\nethical considerations are extended beyond algorithmic optimization to\nencompass output and societal optimization, the foundation and philosophical\ngrounding of the decision-making process become even more critical in the\nknowledge discovery process.",
    "published_date": "2020-06-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04029v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03985v1",
    "title": "Enhancing Facial Data Diversity with Style-based Face Aging",
    "authors": [
      "Markos Georgopoulos",
      "James Oldfield",
      "Mihalis A. Nicolaou",
      "Yannis Panagakis",
      "Maja Pantic"
    ],
    "author_ids": [],
    "abstract": "A significant limiting factor in training fair classifiers relates to the\npresence of dataset bias. In particular, face datasets are typically biased in\nterms of attributes such as gender, age, and race. If not mitigated, bias leads\nto algorithms that exhibit unfair behaviour towards such groups. In this work,\nwe address the problem of increasing the diversity of face datasets with\nrespect to age. Concretely, we propose a novel, generative style-based\narchitecture for data augmentation that captures fine-grained aging patterns by\nconditioning on multi-resolution age-discriminative representations. By\nevaluating on several age-annotated datasets in both single- and cross-database\nexperiments, we show that the proposed method outperforms state-of-the-art\nalgorithms for age transfer, especially in the case of age groups that lie in\nthe tails of the label distribution. We further show significantly increased\ndiversity in the augmented datasets, outperforming all compared methods\naccording to established metrics.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03985v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03983v3",
    "title": "Advertising for Demographically Fair Outcomes",
    "authors": [
      "Lodewijk Gelauff",
      "Ashish Goel",
      "Kamesh Munagala",
      "Sravya Yandamuri"
    ],
    "author_ids": [],
    "abstract": "Online advertising on platforms such as Google or Facebook has become an\nindispensable outreach tool, including for applications where it is desirable\nto engage different demographics in an equitable fashion, such as hiring,\nhousing, civic processes, and public health outreach efforts. Somewhat\nsurprisingly, the existing online advertising ecosystem provides very little\nsupport for advertising to (and recruiting) a demographically representative\ncohort.\n  We study the problem of advertising for demographic representativeness from\nboth an empirical and algorithmic perspective. In essence, we seek fairness in\nthe outcome or conversions generated by the advertising campaigns. We first\npresent detailed empirical findings from real-world experiments for recruiting\nfor civic processes, using which we show that methods using Facebook-inferred\nfeatures are too inaccurate for achieving equity in outcomes, while targeting\nvia custom audiences based on a list of registered voters segmented on known\nattributes has much superior accuracy.\n  This motivates us to consider the algorithmic question of optimally\nsegmenting the list of individuals with known attributes into a few custom\ncampaigns and allocating budgets to them so that we cost-effectively achieve\noutcome parity with the population on the maximum possible number of\ndemographics. Under the assumption that a platform can reasonably enforce\nproportionality in spend across demographics, we present efficient exact and\napproximation algorithms for this problem. We present simulation results on our\ndatasets to show the efficacy of these algorithms in achieving demographic\nparity.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03983v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.03977v1",
    "title": "IP Geolocation Underestimates Regressive Economic Patterns in MOOC Usage",
    "authors": [
      "Daniela Ganelin",
      "Isaac Chuang"
    ],
    "author_ids": [],
    "abstract": "Massive open online courses (MOOCs) promise to make rigorous higher education\naccessible to everyone, but prior research has shown that registrants tend to\ncome from backgrounds of higher socioeconomic status. We study geographically\ngranular economic patterns in about 76,000 U.S. registrations for about 600\nHarvardX and MITx courses between 2012 and 2018, identifying registrants'\nlocations using both IP geolocation and user-reported mailing addresses. By\neither metric, we find higher registration rates among postal codes with\ngreater prosperity or population density. However, we also find evidence of\nbias in IP geolocation: it makes greater errors, both geographically and\neconomically, for users from more economically distressed areas; it\ndisproportionately places users in prosperous areas; and it underestimates the\nregressive pattern in MOOC registration. Researchers should use IP geolocation\nin MOOC studies with care, and consider the possibility of similar economic\nbiases affecting its other academic, commercial, and legal uses.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03977v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.03955v5",
    "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
    "authors": [
      "Wei Guo",
      "Aylin Caliskan"
    ],
    "author_ids": [],
    "abstract": "With the starting point that implicit human biases are reflected in the\nstatistical regularities of language, it is possible to measure biases in\nEnglish static word embeddings. State-of-the-art neural language models\ngenerate dynamic word embeddings dependent on the context in which the word\nappears. Current methods measure pre-defined social and intersectional biases\nthat appear in particular contexts defined by sentence templates. Dispensing\nwith templates, we introduce the Contextualized Embedding Association Test\n(CEAT), that can summarize the magnitude of overall bias in neural language\nmodels by incorporating a random-effects model. Experiments on social and\nintersectional biases show that CEAT finds evidence of all tested biases and\nprovides comprehensive information on the variance of effect magnitudes of the\nsame bias in different contexts. All the models trained on English corpora that\nwe study contain biased representations.\n  Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and\nEmergent Intersectional Bias Detection (EIBD), to automatically identify the\nintersectional biases and emergent intersectional biases from static word\nembeddings in addition to measuring them in contextualized word embeddings. We\npresent the first algorithmic bias detection findings on how intersectional\ngroup members are strongly associated with unique emergent biases that do not\noverlap with the biases of their constituent minority identities. IBD and EIBD\nachieve high accuracy when detecting the intersectional and emergent biases of\nAfrican American females and Mexican American females. Our results indicate\nthat biases at the intersection of race and gender associated with members of\nmultiple minority groups, such as African American females and Mexican American\nfemales, have the highest magnitude across all neural language models.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03955v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03824v1",
    "title": "Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing its Gradient Estimator Bias",
    "authors": [
      "Axel Laborieux",
      "Maxence Ernoult",
      "Benjamin Scellier",
      "Yoshua Bengio",
      "Julie Grollier",
      "Damien Querlioz"
    ],
    "author_ids": [],
    "abstract": "Equilibrium Propagation (EP) is a biologically-inspired algorithm for\nconvergent RNNs with a local learning rule that comes with strong theoretical\nguarantees. The parameter updates of the neural network during the credit\nassignment phase have been shown mathematically to approach the gradients\nprovided by Backpropagation Through Time (BPTT) when the network is\ninfinitesimally nudged toward its target. In practice, however, training a\nnetwork with the gradient estimates provided by EP does not scale to visual\ntasks harder than MNIST. In this work, we show that a bias in the gradient\nestimate of EP, inherent in the use of finite nudging, is responsible for this\nphenomenon and that cancelling it allows training deep ConvNets by EP. We show\nthat this bias can be greatly reduced by using symmetric nudging (a positive\nnudging and a negative one). We also generalize previous EP equations to the\ncase of cross-entropy loss (by opposition to squared error). As a result of\nthese advances, we are able to achieve a test error of 11.7% on CIFAR-10 by EP,\nwhich approaches the one achieved by BPTT and provides a major improvement with\nrespect to the standard EP approach with same-sign nudging that gives 86% test\nerror. We also apply these techniques to train an architecture with asymmetric\nforward and backward connections, yielding a 13.2% test error. These results\nhighlight EP as a compelling biologically-plausible approach to compute error\ngradients in deep neural networks.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03824v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03810v2",
    "title": "An Empirical Analysis of the Impact of Data Augmentation on Knowledge Distillation",
    "authors": [
      "Deepan Das",
      "Haley Massa",
      "Abhimanyu Kulkarni",
      "Theodoros Rekatsinas"
    ],
    "author_ids": [],
    "abstract": "Generalization Performance of Deep Learning models trained using Empirical\nRisk Minimization can be improved significantly by using Data Augmentation\nstrategies such as simple transformations, or using Mixed Samples. We attempt\nto empirically analyze the impact of such strategies on the transfer of\ngeneralization between teacher and student models in a distillation setup. We\nobserve that if a teacher is trained using any of the mixed sample augmentation\nstrategies, such as MixUp or CutMix, the student model distilled from it is\nimpaired in its generalization capabilities. We hypothesize that such\nstrategies limit a model's capability to learn example-specific features,\nleading to a loss in quality of the supervision signal during distillation. We\npresent a novel Class-Discrimination metric to quantitatively measure this\ndichotomy in performance and link it to the discriminative capacity induced by\nthe different strategies on a network's latent space.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03810v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03744v1",
    "title": "Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report Generation",
    "authors": [
      "Mingjie Li",
      "Fuyu Wang",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ],
    "author_ids": [],
    "abstract": "Beyond the common difficulties faced in the natural image captioning, medical\nreport generation specifically requires the model to describe a medical image\nwith a fine-grained and semantic-coherence paragraph that should satisfy both\nmedical commonsense and logic. Previous works generally extract the global\nimage features and attempt to generate a paragraph that is similar to\nreferenced reports; however, this approach has two limitations. Firstly, the\nregions of primary interest to radiologists are usually located in a small area\nof the global image, meaning that the remainder parts of the image could be\nconsidered as irrelevant noise in the training procedure. Secondly, there are\nmany similar sentences used in each medical report to describe the normal\nregions of the image, which causes serious data bias. This deviation is likely\nto teach models to generate these inessential sentences on a regular basis. To\naddress these problems, we propose an Auxiliary Signal-Guided Knowledge\nEncoder-Decoder (ASGK) to mimic radiologists' working patterns. In more detail,\nASGK integrates internal visual feature fusion and external medical linguistic\ninformation to guide medical knowledge transfer and learning. The core\nstructure of ASGK consists of a medical graph encoder and a natural language\ndecoder, inspired by advanced Generative Pre-Training (GPT). Experiments on the\nCX-CHR dataset and our COVID-19 CT Report dataset demonstrate that our proposed\nASGK is able to generate a robust and accurate report, and moreover outperforms\nstate-of-the-art methods on both medical terminology classification and\nparagraph generation metrics.",
    "published_date": "2020-06-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CL",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03744v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03423v1",
    "title": "Generation of Differentially Private Heterogeneous Electronic Health Records",
    "authors": [
      "Kieran Chin-Cheong",
      "Thomas Sutter",
      "Julia E. Vogt"
    ],
    "author_ids": [],
    "abstract": "Electronic Health Records (EHRs) are commonly used by the machine learning\ncommunity for research on problems specifically related to health care and\nmedicine. EHRs have the advantages that they can be easily distributed and\ncontain many features useful for e.g. classification problems. What makes EHR\ndata sets different from typical machine learning data sets is that they are\noften very sparse, due to their high dimensionality, and often contain\nheterogeneous (mixed) data types. Furthermore, the data sets deal with\nsensitive information, which limits the distribution of any models learned\nusing them, due to privacy concerns. For these reasons, using EHR data in\npractice presents a real challenge. In this work, we explore using Generative\nAdversarial Networks to generate synthetic, heterogeneous EHRs with the goal of\nusing these synthetic records in place of existing data sets for downstream\nclassification tasks. We will further explore applying differential privacy\n(DP) preserving optimization in order to produce DP synthetic EHR data sets,\nwhich provide rigorous privacy guarantees, and are therefore shareable and\nusable in the real world. The performance (measured by AUROC, AUPRC and\naccuracy) of our model's synthetic, heterogeneous data is very close to the\noriginal data set (within 3 - 5% of the baseline) for the non-DP model when\ntested in a binary classification task. Using strong $(1, 10^{-5})$ DP, our\nmodel still produces data useful for machine learning tasks, albeit incurring a\nroughly 17% performance penalty in our tested classification task. We\nadditionally perform a sub-population analysis and find that our model does not\nintroduce any bias into the synthetic EHR data compared to the baseline in\neither male/female populations, or the 0-18, 19-50 and 51+ age groups in terms\nof classification performance for either the non-DP or DP variant.",
    "published_date": "2020-06-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03423v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03167v1",
    "title": "Inject Machine Learning into Significance Test for Misspecified Linear Models",
    "authors": [
      "Jiaye Teng",
      "Yang Yuan"
    ],
    "author_ids": [],
    "abstract": "Due to its strong interpretability, linear regression is widely used in\nsocial science, from which significance test provides the significance level of\nmodels or coefficients in the traditional statistical inference. However,\nlinear regression methods rely on the linear assumptions of the ground truth\nfunction, which do not necessarily hold in practice. As a result, even for\nsimple non-linear cases, linear regression may fail to report the correct\nsignificance level.\n  In this paper, we present a simple and effective assumption-free method for\nlinear approximation in both linear and non-linear scenarios. First, we apply a\nmachine learning method to fit the ground truth function on the training set\nand calculate its linear approximation. Afterward, we get the estimator by\nadding adjustments based on the validation set. We prove the concentration\ninequalities and asymptotic properties of our estimator, which leads to the\ncorresponding significance test. Experimental results show that our estimator\nsignificantly outperforms linear regression for non-linear ground truth\nfunctions, indicating that our estimator might be a better tool for the\nsignificance test.",
    "published_date": "2020-06-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03167v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03121v2",
    "title": "Effects of algorithmic flagging on fairness: quasi-experimental evidence from Wikipedia",
    "authors": [
      "Nathan TeBlunthuis",
      "Benjamin Mako Hill",
      "Aaron Halfaker"
    ],
    "author_ids": [],
    "abstract": "Online community moderators often rely on social signals such as whether or\nnot a user has an account or a profile page as clues that users may cause\nproblems. Reliance on these clues can lead to overprofiling bias when\nmoderators focus on these signals but overlook the misbehavior of others. We\npropose that algorithmic flagging systems deployed to improve the efficiency of\nmoderation work can also make moderation actions more fair to these users by\nreducing reliance on social signals and making norm violations by everyone else\nmore visible. We analyze moderator behavior in Wikipedia as mediated by\nRCFilters, a system which displays social signals and algorithmic flags, and\nestimate the causal effect of being flagged on moderator actions. We show that\nalgorithmically flagged edits are reverted more often, especially those by\nestablished editors with positive social signals, and that flagging decreases\nthe likelihood that moderation actions will be undone. Our results suggest that\nalgorithmic flagging systems can lead to increased fairness in some contexts\nbut that the relationship is complex and contingent.",
    "published_date": "2020-06-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.SI",
      "K.4.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03121v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03119v1",
    "title": "How individual behaviors drive inequality in online community sizes: an agent-based simulation",
    "authors": [
      "Jeremy Foote",
      "Nathan TeBlunthuis",
      "Benjamin Mako Hill",
      "Aaron Shaw"
    ],
    "author_ids": [],
    "abstract": "Why are online community sizes so extremely unequal? Most answers to this\nquestion have pointed to general mathematical processes drawn from physics like\ncumulative advantage. These explanations provide little insight into specific\nsocial dynamics or decisions that individuals make when joining and leaving\ncommunities. In addition, explanations in terms of cumulative advantage do not\ndraw from the enormous body of social computing research that studies\nindividual behavior. Our work bridges this divide by testing whether two\ninfluential social mechanisms used to explain community joining can also\nexplain the distribution of community sizes. Using agent-based simulations, we\nevaluate how well individual-level processes of social exposure and decisions\nbased on individual expected benefits reproduce empirical community size data\nfrom Reddit. Our simulations contribute to social computing theory by providing\nevidence that both processes together---but neither alone---generate realistic\ndistributions of community sizes. Our results also illustrate the potential\nvalue of agent-based simulation to online community researchers to both\nevaluate and bridge individual and group-level theories.",
    "published_date": "2020-06-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.SI",
      "K.4.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03119v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.03051v2",
    "title": "NewB: 200,000+ Sentences for Political Bias Detection",
    "authors": [
      "Jerry Wei"
    ],
    "author_ids": [],
    "abstract": "We present the Newspaper Bias Dataset (NewB), a text corpus of more than\n200,000 sentences from eleven news sources regarding Donald Trump. While\nprevious datasets have labeled sentences as either liberal or conservative,\nNewB covers the political views of eleven popular media sources, capturing more\nnuanced political viewpoints than a traditional binary classification system\ndoes. We train two state-of-the-art deep learning models to predict the news\nsource of a given sentence from eleven newspapers and find that a recurrent\nneural network achieved top-1, top-3, and top-5 accuracies of 33.3%, 61.4%, and\n77.6%, respectively, significantly outperforming a baseline logistic regression\nmodel's accuracies of 18.3%, 42.6%, and 60.8%. Using the news source label of\nsentences, we analyze the top n-grams with our model to gain meaningful insight\ninto the portrayal of Trump by media sources.We hope that the public release of\nour dataset will encourage further research in using natural language\nprocessing to analyze more complex political biases.\n  Our dataset is posted at https://github.com/JerryWeiAI/NewB .",
    "published_date": "2020-06-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03051v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.02956v1",
    "title": "A Fair, Traceable, Auditable and Participatory Randomization Tool for Legal Systems",
    "authors": [
      "Marcos Vinicius M. Silva",
      "Marcos Antonio Simplicio Jr.",
      "Roberto Augusto Castellanos Pfeiffer",
      "Julio Michael Stern"
    ],
    "author_ids": [],
    "abstract": "Many real-world scenarios require the random selection of one or more\nindividuals from a pool of eligible candidates. One example of especial social\nrelevance refers to the legal system, in which the jurors and judges are\ncommonly picked according to some probability distribution aiming to avoid\nbiased decisions. In this scenario, ensuring auditability of the random drawing\nprocedure is imperative to promote confidence in its fairness. With this goal\nin mind, this article describes a protocol for random drawings specially\ndesigned for use in legal systems. The proposed design combines the following\nproperties: security by design, ensuring the fairness of the random draw as\nlong as at least one participant behaves honestly; auditability by any\ninterested party, even those having no technical background, using only public\ninformation; and statistical robustness, supporting drawings where candidates\nmay have distinct probability distributions. Moreover, it is capable of\ninviting and engaging as participating stakeholders the main interested parties\nof a legal process, in a way that promotes process transparency, public trust\nand institutional resilience. An open-source implementation is also provided as\nsupplementary material.",
    "published_date": "2020-06-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "stat.OT",
      "64-04 (Primary), 62D99 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02956v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.02825v1",
    "title": "SOS -- Self-Organization for Survival: Introducing fairness in emergency communication to save lives",
    "authors": [
      "Indushree Banerjee",
      "Martijn Warnier",
      "Frances M. T. Brazier",
      "Dirk Helbing"
    ],
    "author_ids": [],
    "abstract": "Communication is crucial when disasters isolate communities of people and\nrescue is delayed. Such delays force citizens to be first responders and form\nsmall rescue teams. Rescue teams require reliable communication, particularly\nin the first 72 hours, which is challenging due to damaged infrastructure and\nelectrical blackouts. We design a peer-to-peer communication network that meets\nthese challenges. We introduce the concept of participatory fairness: equal\ncommunication opportunities for all citizens regardless of initial inequality\nin phone battery charge. Our value-sensitive design approach achieves an even\nbattery charge distribution across phones over time and enables citizens to\ncommunicate over 72 hours. We apply the fairness principle to communication in\nan adapted standard Barabasi-Albert model of a scale-free network that\nautomatically (i) assigns high-battery phones as hubs, (ii) adapts the network\ntopology to the spatio-temporal battery charge distribution, and (iii)\nself-organizes to remain robust and reliable when links fail or phones leave\nthe network. While the Barabasi-Albert model has become a widespread\ndescriptive model, we demonstrate its use as a design principle to meet values\nsuch as fairness and systemic efficiency. Our results demonstrate that,\ncompared to a generic peer-to-peer mesh network, the new protocol achieves (i)\na longer network lifetime, (ii) an adaptive information flow, (iii) a fair\ndistribution of battery charge, and (iv) higher participation rates. Hence, our\nprotocol, Self-Organization for Survival ('SOS'), provides fair communication\nopportunities to all citizens during a disaster through self-organization. SOS\nenables participatory resilience and sustainability, empowering citizens to\ncommunicate when they need it most.",
    "published_date": "2020-06-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cond-mat.dis-nn",
      "nlin.AO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02825v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.02577v1",
    "title": "An optimizable scalar objective value cannot be objective and should not be the sole objective",
    "authors": [
      "Isabel Kloumann",
      "Mark Tygert"
    ],
    "author_ids": [],
    "abstract": "This paper concerns the ethics and morality of algorithms and computational\nsystems, and has been circulating internally at Facebook for the past couple\nyears. The paper reviews many Nobel laureates' work, as well as the work of\nother prominent scientists such as Richard Dawkins, Andrei Kolmogorov, Vilfredo\nPareto, and John von Neumann. The paper draws conclusions based on such works,\nas summarized in the title. The paper argues that the standard approach to\nmodern machine learning and artificial intelligence is bound to be biased and\nunfair, and that longstanding traditions in the professions of law, justice,\npolitics, and medicine should help.",
    "published_date": "2020-06-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02577v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.02575v1",
    "title": "Debiased Sinkhorn barycenters",
    "authors": [
      "Hicham Janati",
      "Marco Cuturi",
      "Alexandre Gramfort"
    ],
    "author_ids": [],
    "abstract": "Entropy regularization in optimal transport (OT) has been the driver of many\nrecent interests for Wasserstein metrics and barycenters in machine learning.\nIt allows to keep the appealing geometrical properties of the unregularized\nWasserstein distance while having a significantly lower complexity thanks to\nSinkhorn's algorithm. However, entropy brings some inherent smoothing bias,\nresulting for example in blurred barycenters. This side effect has prompted an\nincreasing temptation in the community to settle for a slower algorithm such as\nlog-domain stabilized Sinkhorn which breaks the parallel structure that can be\nleveraged on GPUs, or even go back to unregularized OT. Here we show how this\nbias is tightly linked to the reference measure that defines the entropy\nregularizer and propose debiased Wasserstein barycenters that preserve the best\nof both worlds: fast Sinkhorn-like iterations without entropy smoothing.\nTheoretically, we prove that the entropic OT barycenter of univariate Gaussians\nis a Gaussian and quantify its variance bias. This result is obtained by\nextending the differentiability and convexity of entropic OT to sub-Gaussian\nmeasures with unbounded supports. Empirically, we illustrate the reduced\nblurring and the computational advantage on various applications.",
    "published_date": "2020-06-03T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02575v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.02148v1",
    "title": "Being published successfully or getting arXived? The importance of social capital and interdisciplinary collaboration for getting printed in a high impact journal in Physics",
    "authors": [
      "Oliver J. Wieczorek",
      "Mark Wittek",
      "Raphael H. Heiberger"
    ],
    "author_ids": [],
    "abstract": "The structure of collaboration is known to be of great importance for the\nsuccess of scientific endeavors. In particular, various types of social capital\nemployed in co-authored work and projects bridging disciplinary boundaries have\nattracted researchers' interest. Almost all previous studies, however, use\nsamples with an inherent survivor bias, i.e., they focus on papers that have\nalready been published. In contrast, our article examines the chances for\ngetting a working paper published by using a unique dataset of 245,000 papers\nuploaded to arXiv. ArXiv is a popular preprint platform in Physics which allows\nus to construct a co-authorship network from which we can derive different\ntypes of social capital and interdisciplinary teamwork. To emphasize the\n'normal case' of community-specific standards of excellence, we assess\npublications in Physics' high impact journals as success. Utilizing multilevel\nevent history models, our results reveal that already a moderate number of\npersistent collaborations spanning at least two years is the most important\nsocial antecedent of getting a manuscript published successfully. In contrast,\ninter- and subdisciplinary collaborations decrease the probability of\npublishing in an eminent journal in Physics, which can only partially be\nmitigated by scientists' social capital.",
    "published_date": "2020-06-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02148v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.02046v2",
    "title": "Fairness-Aware Explainable Recommendation over Knowledge Graphs",
    "authors": [
      "Zuohui Fu",
      "Yikun Xian",
      "Ruoyuan Gao",
      "Jieyu Zhao",
      "Qiaoying Huang",
      "Yingqiang Ge",
      "Shuyuan Xu",
      "Shijie Geng",
      "Chirag Shah",
      "Yongfeng Zhang",
      "Gerard de Melo"
    ],
    "author_ids": [],
    "abstract": "There has been growing attention on fairness considerations recently,\nespecially in the context of intelligent decision making systems. Explainable\nrecommendation systems, in particular, may suffer from both explanation bias\nand performance disparity. In this paper, we analyze different groups of users\naccording to their level of activity, and find that bias exists in\nrecommendation performance between different groups. We show that inactive\nusers may be more susceptible to receiving unsatisfactory recommendations, due\nto insufficient training data for the inactive users, and that their\nrecommendations may be biased by the training records of more active users, due\nto the nature of collaborative filtering, which leads to an unfair treatment by\nthe system. We propose a fairness constrained approach via heuristic re-ranking\nto mitigate this unfairness problem in the context of explainable\nrecommendation over knowledge graphs. We experiment on several real-world\ndatasets with state-of-the-art knowledge graph-based explainable recommendation\nalgorithms. The promising results show that our algorithm is not only able to\nprovide high-quality explainable recommendations, but also reduces the\nrecommendation unfairness in several respects.",
    "published_date": "2020-06-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02046v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.01938v1",
    "title": "Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings",
    "authors": [
      "Vaibhav Kumar",
      "Tenzin Singhay Bhotia",
      "Vaibhav Kumar",
      "Tanmoy Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Word embeddings are the standard model for semantic and syntactic\nrepresentations of words. Unfortunately, these models have been shown to\nexhibit undesirable word associations resulting from gender, racial, and\nreligious biases. Existing post-processing methods for debiasing word\nembeddings are unable to mitigate gender bias hidden in the spatial arrangement\nof word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing\nmethodology which not only eliminates the bias present in a word vector but\nalso alters the spatial distribution of its neighbouring vectors, achieving a\nbias-free setting while maintaining minimal semantic offset. We also propose a\nnew bias evaluation metric - Gender-based Illicit Proximity Estimate (GIPE),\nwhich measures the extent of undue proximity in word vectors resulting from the\npresence of gender-based predilections. Experiments based on a suite of\nevaluation metrics show that RAN-Debias significantly outperforms the\nstate-of-the-art in reducing proximity bias (GIPE) by at least 42.02%. It also\nreduces direct bias, adding minimal semantic disturbance, and achieves the best\nperformance in a downstream application task (coreference resolution).",
    "published_date": "2020-06-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01938v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.01816v1",
    "title": "Age-Based Coded Computation for Bias Reduction in Distributed Learning",
    "authors": [
      "Emre Ozfatura",
      "Baturalp Buyukates",
      "Deniz Gunduz",
      "Sennur Ulukus"
    ],
    "author_ids": [],
    "abstract": "Coded computation can be used to speed up distributed learning in the\npresence of straggling workers. Partial recovery of the gradient vector can\nfurther reduce the computation time at each iteration; however, this can result\nin biased estimators, which may slow down convergence, or even cause\ndivergence. Estimator bias will be particularly prevalent when the straggling\nbehavior is correlated over time, which results in the gradient estimators\nbeing dominated by a few fast servers. To mitigate biased estimators, we design\na $timely$ dynamic encoding framework for partial recovery that includes an\nordering operator that changes the codewords and computation orders at workers\nover time. To regulate the recovery frequencies, we adopt an $age$ metric in\nthe design of the dynamic encoding scheme. We show through numerical results\nthat the proposed dynamic encoding strategy increases the timeliness of the\nrecovered computations, which as a result, reduces the bias in model updates,\nand accelerates the convergence compared to the conventional static partial\nrecovery schemes.",
    "published_date": "2020-06-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.DC",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01816v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.01784v1",
    "title": "Coordinating Multiagent Industrial Symbiosis",
    "authors": [
      "Vahid Yazdanpanah",
      "Devrim Murat Yazan",
      "W. Henk M. Zijm"
    ],
    "author_ids": [],
    "abstract": "We present a formal multiagent framework for coordinating a class of\ncollaborative industrial practices called Industrial Symbiotic Networks (ISNs)\nas cooperative games. The game-theoretic formulation of ISNs enables systematic\nreasoning about what we call the ISN implementation problem. Specifically, the\ncharacteristics of ISNs may lead to the inapplicability of standard fair and\nstable benefit allocation methods. Inspired by realistic ISN scenarios and\nfollowing the literature on normative multiagent systems, we consider\nregulations and normative socio-economic policies as coordination instruments\nthat in combination with ISN games resolve the situation. In this multiagent\nsystem, employing Marginal Contribution Nets (MC-Nets) as rule-based\ncooperative game representations foster the combination of regulations and ISN\ngames with no loss in expressiveness. We develop algorithmic methods for\ngenerating regulations that ensure the implementability of ISNs and as a policy\nsupport, present the policy requirements that guarantee the implementability of\nall the desired ISNs in a balanced-budget way.",
    "published_date": "2020-06-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01784v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.01770v2",
    "title": "What's Sex Got To Do With Fair Machine Learning?",
    "authors": [
      "Lily Hu",
      "Issa Kohler-Hausmann"
    ],
    "author_ids": [],
    "abstract": "Debate about fairness in machine learning has largely centered around\ncompeting definitions of what fairness or nondiscrimination between groups\nrequires. However, little attention has been paid to what precisely a group is.\nMany recent approaches to \"fairness\" require one to specify a causal model of\nthe data generating process. These exercises make an implicit ontological\nassumption that a racial or sex group is simply a collection of individuals who\nshare a given trait. We show this by exploring the formal assumption of\nmodularity in causal models, which holds that the dependencies captured by one\ncausal pathway are invariant to interventions on any other pathways. Causal\nmodels of sex propose two substantive claims: 1) There exists a feature,\nsex-on-its-own, that is an inherent trait of an individual that causally brings\nabout social phenomena external to it in the world; and 2) the relations\nbetween sex and its effects can be modified in whichever ways and the former\nfeature would still retain the meaning that sex has in our world. We argue that\nthis ontological picture is false. Many of the \"effects\" that sex purportedly\n\"causes\" are in fact constitutive features of sex as a social status. They give\nthe social meaning of sex features, meanings that are precisely what make sex\ndiscrimination a distinctively morally problematic type of action. Correcting\nthis conceptual error has a number of implications for how models can be used\nto detect discrimination. Formal diagrams of constitutive relations present an\nentirely different path toward reasoning about discrimination. Whereas causal\ndiagrams guide the construction of sophisticated modular counterfactuals,\nconstitutive diagrams identify a different kind of counterfactual as central to\nan inquiry on discrimination: one that asks how the social meaning of a group\nwould be changed if its non-modular features were altered.",
    "published_date": "2020-06-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01770v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.01615v1",
    "title": "A Multi-Task Comparator Framework for Kinship Verification",
    "authors": [
      "Stefan Hörmann",
      "Martin Knoche",
      "Gerhard Rigoll"
    ],
    "author_ids": [],
    "abstract": "Approaches for kinship verification often rely on cosine distances between\nface identification features. However, due to gender bias inherent in these\nfeatures, it is hard to reliably predict whether two opposite-gender pairs are\nrelated. Instead of fine tuning the feature extractor network on kinship\nverification, we propose a comparator network to cope with this bias. After\nconcatenating both features, cascaded local expert networks extract the\ninformation most relevant for their corresponding kinship relation. We\ndemonstrate that our framework is robust against this gender bias and achieves\ncomparable results on two tracks of the RFIW Challenge 2020. Moreover, we show\nhow our framework can be further extended to handle partially known or unknown\nkinship relations.",
    "published_date": "2020-06-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01615v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.02295v2",
    "title": "Improved acoustic word embeddings for zero-resource languages using multilingual transfer",
    "authors": [
      "Herman Kamper",
      "Yevgen Matusevych",
      "Sharon Goldwater"
    ],
    "author_ids": [],
    "abstract": "Acoustic word embeddings are fixed-dimensional representations of\nvariable-length speech segments. Such embeddings can form the basis for speech\nsearch, indexing and discovery systems when conventional speech recognition is\nnot possible. In zero-resource settings where unlabelled speech is the only\navailable resource, we need a method that gives robust embeddings on an\narbitrary language. Here we explore multilingual transfer: we train a single\nsupervised embedding model on labelled data from multiple well-resourced\nlanguages and then apply it to unseen zero-resource languages. We consider\nthree multilingual recurrent neural network (RNN) models: a classifier trained\non the joint vocabularies of all training languages; a Siamese RNN trained to\ndiscriminate between same and different words from multiple languages; and a\ncorrespondence autoencoder (CAE) RNN trained to reconstruct word pairs. In a\nword discrimination task on six target languages, all of these models\noutperform state-of-the-art unsupervised models trained on the zero-resource\nlanguages themselves, giving relative improvements of more than 30% in average\nprecision. When using only a few training languages, the multilingual CAE\nperforms better, but with more training languages the other multilingual models\nperform similarly. Using more training languages is generally beneficial, but\nimprovements are marginal on some languages. We present probing experiments\nwhich show that the CAE encodes more phonetic, word duration, language identity\nand speaker information than the other multilingual models.",
    "published_date": "2020-06-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.02295v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.01315v1",
    "title": "Multi-view Deep Features for Robust Facial Kinship Verification",
    "authors": [
      "Oualid Laiadi",
      "Abdelmalik Ouamane",
      "Abdelhamid Benakcha",
      "Abdelmalik Taleb-Ahmed",
      "Abdenour Hadid"
    ],
    "author_ids": [],
    "abstract": "Automatic kinship verification from facial images is an emerging research\ntopic in machine learning community. In this paper, we proposed an effective\nfacial features extraction model based on multi-view deep features. Thus, we\nused four pre-trained deep learning models using eight features layers (FC6 and\nFC7 layers of each VGG-F, VGG-M, VGG-S and VGG-Face models) to train the\nproposed Multilinear Side-Information based Discriminant Analysis integrating\nWithin Class Covariance Normalization (MSIDA+WCCN) method. Furthermore, we show\nthat how can metric learning methods based on WCCN method integration improves\nthe Simple Scoring Cosine similarity (SSC) method. We refer that we used the\nSSC method in RFIW'20 competition using the eight deep features concatenation.\nThus, the integration of WCCN in the metric learning methods decreases the\nintra-class variations effect introduced by the deep features weights. We\nevaluate our proposed method on two kinship benchmarks namely KinFaceW-I and\nKinFaceW-II databases using four Parent-Child relations (Father-Son,\nFather-Daughter, Mother-Son and Mother-Daughter). Thus, the proposed MSIDA+WCCN\nmethod improves the SSC method with 12.80% and 14.65% on KinFaceW-I and\nKinFaceW-II databases, respectively. The results obtained are positively\ncompared with some modern methods, including those that rely on deep learning.",
    "published_date": "2020-06-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01315v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.00765v2",
    "title": "Conspiracy vs science: A large-scale analysis of online discussion cascades",
    "authors": [
      "Yafei Zhang",
      "Lin Wang",
      "Jonathan J. H. Zhu",
      "Xiaofan Wang"
    ],
    "author_ids": [],
    "abstract": "With the emergence and rapid proliferation of social media platforms and\nsocial networking sites, recent years have witnessed a surge of misinformation\nspreading in our daily life. Drawing on a large-scale dataset which covers more\nthan 1.4M posts and 18M comments, we investigate the propagation of two\ndistinct narratives--(i) conspiracy information, whose claims are generally\nunsubstantiated and thus referred as misinformation to some extent, and (ii)\nscientific information, whose origins are generally readily identifiable and\nverifiable--in an online social media platform. We find that conspiracy\ncascades tend to propagate in a multigenerational branching process while\nscience cascades are more likely to grow in a breadth-first manner.\nSpecifically, conspiracy information triggers larger cascades, involves more\nusers and generations, persists longer, is more viral and bursty than science\ninformation. Content analysis reveals that conspiracy cascades contain more\nnegative words and emotional words which convey anger, fear, disgust, surprise\nand trust. We also find that conspiracy cascades are more concerned with\npolitical and controversial topics. After applying machine learning models, we\nachieve an AUC score of nearly 90% in discriminating conspiracy from science\nnarratives.\n  We find that conspiracy cascades are more likely to be controlled by a\nbroader set of users than science cascades, imposing new challenges on the\nmanagement of misinformation. Although political affinity is thought to affect\nthe consumption of misinformation, there is very little evidence that political\norientation of the information source plays a role during the propagation of\nconspiracy information. Our study provides complementing evidence to current\nmisinformation research and has practical policy implications to stem the\npropagation and mitigate the influence of misinformation online.",
    "published_date": "2020-06-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.00765v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.04944v1",
    "title": "A Machine Learning System for Retaining Patients in HIV Care",
    "authors": [
      "Avishek Kumar",
      "Arthi Ramachandran",
      "Adolfo De Unanue",
      "Christina Sung",
      "Joe Walsh",
      "John Schneider",
      "Jessica Ridgway",
      "Stephanie Masiello Schuette",
      "Jeff Lauritsen",
      "Rayid Ghani"
    ],
    "author_ids": [],
    "abstract": "Retaining persons living with HIV (PLWH) in medical care is paramount to\npreventing new transmissions of the virus and allowing PLWH to live normal and\nhealthy lifespans. Maintaining regular appointments with an HIV provider and\ntaking medication daily for a lifetime is exceedingly difficult. 51% of PLWH\nare non-adherent with their medications and eventually drop out of medical\ncare. Current methods of re-linking individuals to care are reactive (after a\npatient has dropped-out) and hence not very effective. We describe our system\nto predict who is most at risk to drop-out-of-care for use by the University of\nChicago HIV clinic and the Chicago Department of Public Health. Models were\nselected based on their predictive performance under resource constraints,\nstability over time, as well as fairness. Our system is applicable as a\npoint-of-care system in a clinical setting as well as a batch prediction system\nto support regular interventions at the city level. Our model performs 3x\nbetter than the baseline for the clinical model and 2.3x better than baseline\nfor the city-wide model. The code has been released on github and we hope this\nmethodology, particularly our focus on fairness, will be adopted by other\nclinics and public health agencies in order to curb the HIV epidemic.",
    "published_date": "2020-06-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.04944v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.00481v3",
    "title": "Fair Cake Division Under Monotone Likelihood Ratios",
    "authors": [
      "Siddharth Barman",
      "Nidhi Rathi"
    ],
    "author_ids": [],
    "abstract": "This work develops algorithmic results for the classic cake-cutting problem\nin which a divisible, heterogeneous resource (modeled as a cake) needs to be\npartitioned among agents with distinct preferences. We focus on a standard\nformulation of cake cutting wherein each agent must receive a contiguous piece\nof the cake. While multiple hardness results exist in this setup for finding\nfair/efficient cake divisions, we show that, if the value densities of the\nagents satisfy the monotone likelihood ratio property (MLRP), then strong\nalgorithmic results hold for various notions of fairness and economic\nefficiency.\n  Addressing cake-cutting instances with MLRP, first we develop an algorithm\nthat finds cake divisions (with connected pieces) that are envy-free, up to an\narbitrary precision. The time complexity of our algorithm is polynomial in the\nnumber of agents and the bit complexity of an underlying Lipschitz constant. We\nobtain similar positive results for maximizing social (utilitarian) and\negalitarian welfare. In addition, we show that, under MLRP, the problem of\nmaximizing Nash social welfare admits a fully polynomial-time approximation\nscheme (FPTAS).\n  Many distribution families bear MLRP. In particular, this property holds if\nall the value densities belong to any one of the following families: Gaussian\n(with the same variance), linear, binomial, Poisson, and exponential\ndistributions. Furthermore, it is known that linear translations of any\nlog-concave function satisfy MLRP. Therefore, our results also hold when the\nvalue densities of the agents are linear translations of the following\n(log-concave) distributions: Laplace, gamma, beta, Subbotin, chi-square,\nDirichlet, and logistic. Hence, through MLRP, the current work obtains novel\ncake-cutting algorithms for multiple distribution families.",
    "published_date": "2020-05-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "F.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.00481v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.00412v1",
    "title": "Attribute-Induced Bias Eliminating for Transductive Zero-Shot Learning",
    "authors": [
      "Hantao Yao",
      "Shaobo Min",
      "Yongdong Zhang",
      "Changsheng Xu"
    ],
    "author_ids": [],
    "abstract": "Transductive Zero-shot learning (ZSL) targets to recognize the unseen\ncategories by aligning the visual and semantic information in a joint embedding\nspace. There exist four kinds of domain biases in Transductive ZSL, i.e.,\nvisual bias and semantic bias between two domains and two visual-semantic\nbiases in respective seen and unseen domains, but existing work only focuses on\nthe part of them, which leads to severe semantic ambiguity during the knowledge\ntransfer. To solve the above problem, we propose a novel Attribute-Induced Bias\nEliminating (AIBE) module for Transductive ZSL. Specifically, for the visual\nbias between two domains, the Mean-Teacher module is first leveraged to bridge\nthe visual representation discrepancy between two domains with unsupervised\nlearning and unlabelled images. Then, an attentional graph attribute embedding\nis proposed to reduce the semantic bias between seen and unseen categories,\nwhich utilizes the graph operation to capture the semantic relationship between\ncategories. Besides, to reduce the semantic-visual bias in the seen domain, we\nalign the visual center of each category, instead of the individual visual data\npoint, with the corresponding semantic attributes, which further preserves the\nsemantic relationship in the embedding space. Finally, for the semantic-visual\nbias in the unseen domain, an unseen semantic alignment constraint is designed\nto align visual and semantic space in an unsupervised manner. The evaluations\non several benchmarks demonstrate the effectiveness of the proposed method,\ne.g., obtaining the 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for\nConventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets,\nrespectively.",
    "published_date": "2020-05-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.00412v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.03498v1",
    "title": "Commuting Variability by Wage Groups in Baton Rouge 1990-2010",
    "authors": [
      "Yujie Hu",
      "Fahui Wang",
      "Chester Wilmot"
    ],
    "author_ids": [],
    "abstract": "Residential segregation recently has shifted to more class or income-based in\nthe United States, and neighborhoods are undergoing significant changes such as\ncommuting patterns over time. To better understand the commuting inequality\nacross neighborhoods of different income levels, this research analyzes\ncommuting variability (in both distance and time) across wage groups as well as\nstability over time using the CTPP data 1990-2010 in Baton Rouge. In comparison\nto previous work, commuting distance is estimated more accurately by Monte\nCarlo simulation of individual trips to mitigate aggregation error and scale\neffect. The results based on neighborhoods mean wage rate indicate that\ncommuting behaviors vary across areas of different wage rates and such\nvariability is captured by a convex shape. Affluent neighborhoods tended to\ncommute more but highest-wage neighborhoods retreated for less commuting. This\ntrend remains relatively stable over time despite an overall transportation\nimprovement in general. A complementary analysis based on the distribution of\nwage groups is conducted to gain more detailed insights and uncovers the\nlasting poor mobility (e.g., fewer location and transport options) of the\nlowest-wage workers in 1990-2010.",
    "published_date": "2020-05-30T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.CY",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03498v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2006.00202v2",
    "title": "Attention-Guided Discriminative Region Localization and Label Distribution Learning for Bone Age Assessment",
    "authors": [
      "Chao Chen",
      "Zhihong Chen",
      "Xinyu Jin",
      "Lanjuan Li",
      "William Speier",
      "Corey W. Arnold"
    ],
    "author_ids": [],
    "abstract": "Bone age assessment (BAA) is clinically important as it can be used to\ndiagnose endocrine and metabolic disorders during child development. Existing\ndeep learning based methods for classifying bone age use the global image as\ninput, or exploit local information by annotating extra bounding boxes or key\npoints. However, training with the global image underutilizes discriminative\nlocal information, while providing extra annotations is expensive and\nsubjective. In this paper, we propose an attention-guided approach to\nautomatically localize the discriminative regions for BAA without any extra\nannotations. Specifically, we first train a classification model to learn the\nattention maps of the discriminative regions, finding the hand region, the most\ndiscriminative region (the carpal bones), and the next most discriminative\nregion (the metacarpal bones). Guided by those attention maps, we then crop the\ninformative local regions from the original image and aggregate different\nregions for BAA. Instead of taking BAA as a general regression task, which is\nsuboptimal due to the label ambiguity problem in the age label space, we\npropose using joint age distribution learning and expectation regression, which\nmakes use of the ordinal relationship among hand images with different\nindividual ages and leads to more robust age estimation. Extensive experiments\nare conducted on the RSNA pediatric bone age data set. Using no training\nannotations, our method achieves competitive results compared with existing\nstate-of-the-art semi-automatic deep learning-based methods that require manual\nannotation. Code is available at https:\n//github.com/chenchao666/Bone-Age-Assessment.",
    "published_date": "2020-05-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.00202v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2006.00115v1",
    "title": "Overview of Scanner Invariant Representations",
    "authors": [
      "Daniel Moyer",
      "Greg Ver Steeg",
      "Paul M. Thompson"
    ],
    "author_ids": [],
    "abstract": "Pooled imaging data from multiple sources is subject to bias from each\nsource. Studies that do not correct for these scanner/site biases at best lose\nstatistical power, and at worst leave spurious correlations in their data.\nEstimation of the bias effects is non-trivial due to the paucity of data with\ncorrespondence across sites, so called \"traveling phantom\" data, which is\nexpensive to collect. Nevertheless, numerous solutions leveraging direct\ncorrespondence have been proposed. In contrast to this, Moyer et al. (2019)\nproposes an unsupervised solution using invariant representations, one which\ndoes not require correspondence and thus does not require paired images. By\nleveraging the data processing inequality, an invariant representation can then\nbe used to create an image reconstruction that is uninformative of its original\nsource, yet still faithful to the underlying structure. In the present abstract\nwe provide an overview of this method.",
    "published_date": "2020-05-29T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.00115v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14713v1",
    "title": "Controlling Fairness and Bias in Dynamic Learning-to-Rank",
    "authors": [
      "Marco Morik",
      "Ashudeep Singh",
      "Jessica Hong",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "Rankings are the primary interface through which many online platforms match\nusers to items (e.g. news, products, music, video). In these two-sided markets,\nnot only the users draw utility from the rankings, but the rankings also\ndetermine the utility (e.g. exposure, revenue) for the item providers (e.g.\npublishers, sellers, artists, studios). It has already been noted that\nmyopically optimizing utility to the users, as done by virtually all\nlearning-to-rank algorithms, can be unfair to the item providers. We,\ntherefore, present a learning-to-rank approach for explicitly enforcing\nmerit-based fairness guarantees to groups of items (e.g. articles by the same\npublisher, tracks by the same artist). In particular, we propose a learning\nalgorithm that ensures notions of amortized group fairness, while\nsimultaneously learning the ranking function from implicit feedback data. The\nalgorithm takes the form of a controller that integrates unbiased estimators\nfor both fairness and utility, dynamically adapting both as more data becomes\navailable. In addition to its rigorous theoretical foundation and convergence\nguarantees, we find empirically that the algorithm is highly practical and\nrobust.",
    "published_date": "2020-05-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14713v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.14707v3",
    "title": "Towards Context-Agnostic Learning Using Synthetic Data",
    "authors": [
      "Charles Jin",
      "Martin Rinard"
    ],
    "author_ids": [],
    "abstract": "We propose a novel setting for learning, where the input domain is the image\nof a map defined on the product of two sets, one of which completely determines\nthe labels. We derive a new risk bound for this setting that decomposes into a\nbias and an error term, and exhibits a surprisingly weak dependence on the true\nlabels. Inspired by these results, we present an algorithm aimed at minimizing\nthe bias term by exploiting the ability to sample from each set independently.\nWe apply our setting to visual classification tasks, where our approach enables\nus to train classifiers on datasets that consist entirely of a single synthetic\nexample of each class. On several standard benchmarks for real-world image\nclassification, we achieve robust performance in the context-agnostic setting,\nwith good generalization to real world domains, whereas training directly on\nreal world data without our techniques yields classifiers that are brittle to\nperturbations of the background.",
    "published_date": "2020-05-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14707v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14431v2",
    "title": "Fairness-Aware PageRank",
    "authors": [
      "Sotiris Tsioutsiouliklis",
      "Evaggelia Pitoura",
      "Panayiotis Tsaparas",
      "Ilias Kleftakis",
      "Nikos Mamoulis"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness has attracted significant attention in the past years.\nSurprisingly, there is little work on fairness in networks. In this work, we\nconsider fairness for link analysis algorithms and in particular for the\ncelebrated PageRank algorithm. We provide definitions for fairness, and propose\ntwo approaches for achieving fairness. The first modifies the jump vector of\nthe Pagerank algorithm to enfonce fairness, and the second imposes a fair\nbehavior per node. We also consider the problem of achieving fairness while\nminimizing the utility loss with respect to the original algorithm. We present\nexperiments with real and synthetic graphs that examine the fairness of\nPagerank and demonstrate qualitatively and quantitatively the properties of our\nalgorithms.",
    "published_date": "2020-05-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "H.3.1; H.3.3; H.3.5; I.6.4; F.2.1; F.3.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14431v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.14263v1",
    "title": "Estimating the Prediction Performance of Spatial Models via Spatial k-Fold Cross Validation",
    "authors": [
      "Jonne Pohjankukka",
      "Tapio Pahikkala",
      "Paavo Nevalainen",
      "Jukka Heikkonen"
    ],
    "author_ids": [],
    "abstract": "In machine learning one often assumes the data are independent when\nevaluating model performance. However, this rarely holds in practise.\nGeographic information data sets are an example where the data points have\nstronger dependencies among each other the closer they are geographically. This\nphenomenon known as spatial autocorrelation (SAC) causes the standard cross\nvalidation (CV) methods to produce optimistically biased prediction performance\nestimates for spatial models, which can result in increased costs and accidents\nin practical applications. To overcome this problem we propose a modified\nversion of the CV method called spatial k-fold cross validation (SKCV), which\nprovides a useful estimate for model prediction performance without optimistic\nbias due to SAC. We test SKCV with three real world cases involving open\nnatural data showing that the estimates produced by the ordinary CV are up to\n40% more optimistic than those of SKCV. Both regression and classification\ncases are considered in our experiments. In addition, we will show how the SKCV\nmethod can be applied as a criterion for selecting data sampling density for\nnew research area.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14263v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14238v1",
    "title": "Human Recognition Using Face in Computed Tomography",
    "authors": [
      "Jiuwen Zhu",
      "Hu Han",
      "S. Kevin Zhou"
    ],
    "author_ids": [],
    "abstract": "With the mushrooming use of computed tomography (CT) images in clinical\ndecision making, management of CT data becomes increasingly difficult. From the\npatient identification perspective, using the standard DICOM tag to track\npatient information is challenged by issues such as misspelling, lost file,\nsite variation, etc. In this paper, we explore the feasibility of leveraging\nthe faces in 3D CT images as biometric features. Specifically, we propose an\nautomatic processing pipeline that first detects facial landmarks in 3D for ROI\nextraction and then generates aligned 2D depth images, which are used for\nautomatic recognition. To boost the recognition performance, we employ transfer\nlearning to reduce the data sparsity issue and to introduce a group sampling\nstrategy to increase inter-class discrimination when training the recognition\nnetwork. Our proposed method is capable of capturing underlying identity\ncharacteristics in medical images while reducing memory consumption. To test\nits effectiveness, we curate 600 3D CT images of 280 patients from multiple\nsources for performance evaluation. Experimental results demonstrate that our\nmethod achieves a 1:56 identification accuracy of 92.53% and a 1:1 verification\naccuracy of 96.12%, outperforming other competing approaches.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14238v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14236v1",
    "title": "Fuzziness-based Spatial-Spectral Class Discriminant Information Preserving Active Learning for Hyperspectral Image Classification",
    "authors": [
      "Muhammad Ahmad"
    ],
    "author_ids": [],
    "abstract": "Traditional Active/Self/Interactive Learning for Hyperspectral Image\nClassification (HSIC) increases the size of the training set without\nconsidering the class scatters and randomness among the existing and new\nsamples. Second, very limited research has been carried out on joint\nspectral-spatial information and finally, a minor but still worth mentioning is\nthe stopping criteria which not being much considered by the community.\nTherefore, this work proposes a novel fuzziness-based spatial-spectral within\nand between for both local and global class discriminant information preserving\n(FLG) method. We first investigate a spatial prior fuzziness-based\nmisclassified sample information. We then compute the total local and global\nfor both within and between class information and formulate it in a\nfine-grained manner. Later this information is fed to a discriminative\nobjective function to query the heterogeneous samples which eliminate the\nrandomness among the training samples. Experimental results on benchmark HSI\ndatasets demonstrate the effectiveness of the FLG method on Generative, Extreme\nLearning Machine and Sparse Multinomial Logistic Regression (SMLR)-LORSAL\nclassifiers.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14236v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14122v1",
    "title": "Best of Both Worlds: Ex-Ante and Ex-Post Fairness in Resource Allocation",
    "authors": [
      "Rupert Freeman",
      "Nisarg Shah",
      "Rohit Vaish"
    ],
    "author_ids": [],
    "abstract": "We study the problem of allocating indivisible goods among agents with\nadditive valuations. When randomization is allowed, it is possible to achieve\ncompelling notions of fairness such as envy-freeness, which states that no\nagent should prefer any other agent's allocation to her own. When allocations\nmust be deterministic, achieving exact fairness is impossible but approximate\nnotions such as envy-freeness up to one good can be guaranteed. Our goal in\nthis work is to achieve both simultaneously, by constructing a randomized\nallocation that is exactly fair ex-ante and approximately fair ex-post. The key\nquestion we address is whether ex-ante envy-freeness can be achieved in\ncombination with ex-post envy-freeness up to one good. We settle this\npositively by designing an efficient algorithm that achieves both properties\nsimultaneously. If we additionally require economic efficiency, we obtain an\nimpossibility result. However, we show that economic efficiency and ex-ante\nenvy-freeness can be simultaneously achieved if we slightly relax our ex-post\nfairness guarantee. On our way, we characterize the well-known Maximum Nash\nWelfare allocation rule in terms of a recently introduced fairness guarantee\nthat applies to groups of agents, not just individuals.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14122v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.14050v2",
    "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
    "authors": [
      "Su Lin Blodgett",
      "Solon Barocas",
      "Hal Daumé III",
      "Hanna Wallach"
    ],
    "author_ids": [],
    "abstract": "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their\nmotivations are often vague, inconsistent, and lacking in normative reasoning,\ndespite the fact that analyzing \"bias\" is an inherently normative process. We\nfurther find that these papers' proposed quantitative techniques for measuring\nor mitigating \"bias\" are poorly matched to their motivations and do not engage\nwith the relevant literature outside of NLP. Based on these findings, we\ndescribe the beginnings of a path forward by proposing three recommendations\nthat should guide work analyzing \"bias\" in NLP systems. These recommendations\nrest on a greater recognition of the relationships between language and social\nhierarchies, encouraging researchers and practitioners to articulate their\nconceptualizations of \"bias\"---i.e., what kinds of system behaviors are\nharmful, in what ways, to whom, and why, as well as the normative reasoning\nunderlying these statements---and to center work around the lived experiences\nof members of communities affected by NLP systems, while interrogating and\nreimagining the power relations between technologists and such communities.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14050v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13956v2",
    "title": "Improving Generalized Zero-Shot Learning by Semantic Discriminator",
    "authors": [
      "Xinpeng Li"
    ],
    "author_ids": [],
    "abstract": "It is a recognized fact that the classification accuracy of unseen classes in\nthe setting of Generalized Zero-Shot Learning (GZSL) is much lower than that of\ntraditional Zero-Shot Leaning (ZSL). One of the reasons is that an instance is\nalways misclassified to the wrong domain. Here we refer to the seen and unseen\nclasses as two domains respectively. We propose a new approach to distinguish\nwhether the instances come from the seen or unseen classes. First the visual\nfeature of instance is projected into the semantic space. Then the absolute\nnorm difference between the projected semantic vector and the class semantic\nembedding vector, and the minimum distance between the projected semantic\nvectors and the semantic embedding vectors of the seen classes are used as\ndiscrimination basis. This approach is termed as SD (Semantic Discriminator)\nbecause domain judgement of instance is performed in the semantic space. Our\napproach can be combined with any existing ZSL method and fully supervision\nclassification model to form a new GZSL method. Furthermore, our approach is\nvery simple and does not need any fixed parameters.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13956v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13947v1",
    "title": "Disentanglement Then Reconstruction: Learning Compact Features for Unsupervised Domain Adaptation",
    "authors": [
      "Lihua Zhou",
      "Mao Ye",
      "Xinpeng Li",
      "Ce Zhu",
      "Yiguang Liu",
      "Xue Li"
    ],
    "author_ids": [],
    "abstract": "Recent works in domain adaptation always learn domain invariant features to\nmitigate the gap between the source and target domains by adversarial methods.\nThe category information are not sufficiently used which causes the learned\ndomain invariant features are not enough discriminative. We propose a new\ndomain adaptation method based on prototype construction which likes capturing\ndata cluster centers. Specifically, it consists of two parts: disentanglement\nand reconstruction. First, the domain specific features and domain invariant\nfeatures are disentangled from the original features. At the same time, the\ndomain prototypes and class prototypes of both domains are estimated. Then, a\nreconstructor is trained by reconstructing the original features from the\ndisentangled domain invariant features and domain specific features. By this\nreconstructor, we can construct prototypes for the original features using\nclass prototypes and domain prototypes correspondingly. In the end, the feature\nextraction network is forced to extract features close to these prototypes. Our\ncontribution lies in the technical use of the reconstructor to obtain the\noriginal feature prototypes which helps to learn compact and discriminant\nfeatures. As far as we know, this idea is proposed for the first time.\nExperiment results on several public datasets confirm the state-of-the-art\nperformance of our method.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13947v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13820v2",
    "title": "TOAN: Target-Oriented Alignment Network for Fine-Grained Image Categorization with Few Labeled Samples",
    "authors": [
      "Huaxi Huang",
      "Junjie Zhang",
      "Jian Zhang",
      "Qiang Wu",
      "Chang Xu"
    ],
    "author_ids": [],
    "abstract": "The challenges of high intra-class variance yet low inter-class fluctuations\nin fine-grained visual categorization are more severe with few labeled samples,\n\\textit{i.e.,} Fine-Grained categorization problems under the Few-Shot setting\n(FGFS). High-order features are usually developed to uncover subtle differences\nbetween sub-categories in FGFS, but they are less effective in handling the\nhigh intra-class variance. In this paper, we propose a Target-Oriented\nAlignment Network (TOAN) to investigate the fine-grained relation between the\ntarget query image and support classes. The feature of each support image is\ntransformed to match the query ones in the embedding feature space, which\nreduces the disparity explicitly within each category. Moreover, different from\nexisting FGFS approaches devise the high-order features over the global image\nwith less explicit consideration of discriminative parts, we generate\ndiscriminative fine-grained features by integrating compositional concept\nrepresentations to global second-order pooling. Extensive experiments are\nconducted on four fine-grained benchmarks to demonstrate the effectiveness of\nTOAN compared with the state-of-the-art models.",
    "published_date": "2020-05-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13820v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13691v1",
    "title": "Challenges in Combating COVID-19 Infodemic -- Data, Tools, and Ethics",
    "authors": [
      "Kaize Ding",
      "Kai Shu",
      "Yichuan Li",
      "Amrita Bhattacharjee",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "While the COVID-19 pandemic continues its global devastation, numerous\naccompanying challenges emerge. One important challenge we face is to\nefficiently and effectively use recently gathered data and find computational\ntools to combat the COVID-19 infodemic, a typical information overloading\nproblem. Novel coronavirus presents many questions without ready answers; its\nuncertainty and our eagerness in search of solutions offer a fertile\nenvironment for infodemic. It is thus necessary to combat the infodemic and\nmake a concerted effort to confront COVID-19 and mitigate its negative impact\nin all walks of life when saving lives and maintaining normal orders during\ntrying times. In this position paper of combating the COVID-19 infodemic, we\nillustrate its need by providing real-world examples of rampant conspiracy\ntheories, misinformation, and various types of scams that take advantage of\nhuman kindness, fear, and ignorance. We present three key challenges in this\nfight against the COVID-19 infodemic where researchers and practitioners\ninstinctively want to contribute and help. We demonstrate that these three\nchallenges can and will be effectively addressed by collective wisdom,\ncrowdsourcing, and collaborative research.",
    "published_date": "2020-05-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13691v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.13407v5",
    "title": "CausaLM: Causal Model Explanation Through Counterfactual Language Models",
    "authors": [
      "Amir Feder",
      "Nadav Oved",
      "Uri Shalit",
      "Roi Reichart"
    ],
    "author_ids": [],
    "abstract": "Understanding predictions made by deep neural networks is notoriously\ndifficult, but also crucial to their dissemination. As all machine learning\nbased methods, they are as good as their training data, and can also capture\nunwanted biases. While there are tools that can help understand whether such\nbiases exist, they do not distinguish between correlation and causation, and\nmight be ill-suited for text-based models and for reasoning about high level\nlanguage concepts. A key problem of estimating the causal effect of a concept\nof interest on a given model is that this estimation requires the generation of\ncounterfactual examples, which is challenging with existing generation\ntechnology. To bridge that gap, we propose CausaLM, a framework for producing\ncausal model explanations using counterfactual language representation models.\nOur approach is based on fine-tuning of deep contextualized embedding models\nwith auxiliary adversarial tasks derived from the causal graph of the problem.\nConcretely, we show that by carefully choosing auxiliary adversarial\npre-training tasks, language representation models such as BERT can effectively\nlearn a counterfactual representation for a given concept of interest, and be\nused to estimate its true causal effect on model performance. A byproduct of\nour method is a language representation model that is unaffected by the tested\nconcept, which can be useful in mitigating unwanted bias ingrained in the data.",
    "published_date": "2020-05-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13407v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14024v1",
    "title": "The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and Outlet Popularity",
    "authors": [
      "Lukas Gebhard",
      "Felix Hamborg"
    ],
    "author_ids": [],
    "abstract": "News articles covering policy issues are an essential source of information\nin the social sciences and are also frequently used for other use cases, e.g.,\nto train NLP language models. To derive meaningful insights from the analysis\nof news, large datasets are required that represent real-world distributions,\ne.g., with respect to the contained outlets' popularity, topically, or across\ntime. Information on the political leanings of media publishers is often\nneeded, e.g., to study differences in news reporting across the political\nspectrum, which is one of the prime use cases in the social sciences when\nstudying media bias and related societal issues. Concerning these requirements,\nexisting datasets have major flaws, resulting in redundant and cumbersome\neffort in the research community for dataset creation. To fill this gap, we\npresent POLUSA, a dataset that represents the online media landscape as\nperceived by an average US news consumer. The dataset contains 0.9M articles\ncovering policy topics published between Jan. 2017 and Aug. 2019 by 18 news\noutlets representing the political spectrum. Each outlet is labeled by its\npolitical leaning, which we derive using a systematic aggregation of eight data\nsources. The news dataset is balanced with respect to publication date and\noutlet popularity. POLUSA enables studying a variety of subjects, e.g., media\neffects and political partisanship. Due to its size, the dataset allows to\nutilize data-intense deep learning methods.",
    "published_date": "2020-05-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.IR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14024v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13284v3",
    "title": "Convergence Analysis of Riemannian Stochastic Approximation Schemes",
    "authors": [
      "Alain Durmus",
      "Pablo Jiménez",
      "Éric Moulines",
      "Salem Said",
      "Hoi-To Wai"
    ],
    "author_ids": [],
    "abstract": "This paper analyzes the convergence for a large class of Riemannian\nstochastic approximation (SA) schemes, which aim at tackling stochastic\noptimization problems. In particular, the recursions we study use either the\nexponential map of the considered manifold (geodesic schemes) or more general\nretraction functions (retraction schemes) used as a proxy for the exponential\nmap. Such approximations are of great interest since they are low complexity\nalternatives to geodesic schemes. Under the assumption that the mean field of\nthe SA is correlated with the gradient of a smooth Lyapunov function (possibly\nnon-convex), we show that the above Riemannian SA schemes find an\n${\\mathcal{O}}(b_\\infty + \\log n / \\sqrt{n})$-stationary point (in expectation)\nwithin ${\\mathcal{O}}(n)$ iterations, where $b_\\infty \\geq 0$ is the asymptotic\nbias. Compared to previous works, the conditions we derive are considerably\nmilder. First, all our analysis are global as we do not assume iterates to be\na-priori bounded. Second, we study biased SA schemes. To be more specific, we\nconsider the case where the mean-field function can only be estimated up to a\nsmall bias, and/or the case in which the samples are drawn from a controlled\nMarkov chain. Third, the conditions on retractions required to ensure\nconvergence of the related SA schemes are weak and hold for well-known\nexamples. We illustrate our results on three machine learning problems.",
    "published_date": "2020-05-27T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "60F05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13284v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13222v1",
    "title": "Zoom in to the details of human-centric videos",
    "authors": [
      "Guanghan Li",
      "Yaping Zhao",
      "Mengqi Ji",
      "Xiaoyun Yuan",
      "Lu Fang"
    ],
    "author_ids": [],
    "abstract": "Presenting high-resolution (HR) human appearance is always critical for the\nhuman-centric videos. However, current imagery equipment can hardly capture HR\ndetails all the time. Existing super-resolution algorithms barely mitigate the\nproblem by only considering universal and low-level priors of im-age patches.\nIn contrast, our algorithm is under bias towards the human body\nsuper-resolution by taking advantage of high-level prior defined by HR human\nappearance. Firstly, a motion analysis module extracts inherent motion pattern\nfrom the HR reference video to refine the pose estimation of the low-resolution\n(LR) sequence. Furthermore, a human body reconstruction module maps the HR\ntexture in the reference frames onto a 3D mesh model. Consequently, the input\nLR videos get super-resolved HR human sequences are generated conditioned on\nthe original LR videos as well as few HR reference frames. Experiments on an\nexisting dataset and real-world data captured by hybrid cameras show that our\napproach generates superior visual quality of human body compared with the\ntraditional method.",
    "published_date": "2020-05-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13222v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13041v1",
    "title": "Examining Racial Bias in an Online Abuse Corpus with Structural Topic Modeling",
    "authors": [
      "Thomas Davidson",
      "Debasmita Bhattacharya"
    ],
    "author_ids": [],
    "abstract": "We use structural topic modeling to examine racial bias in data collected to\ntrain models to detect hate speech and abusive language in social media posts.\nWe augment the abusive language dataset by adding an additional feature\nindicating the predicted probability of the tweet being written in\nAfrican-American English. We then use structural topic modeling to examine the\ncontent of the tweets and how the prevalence of different topics is related to\nboth abusiveness annotation and dialect prediction. We find that certain topics\nare disproportionately racialized and considered abusive. We discuss how topic\nmodeling may be a useful approach for identifying bias in annotated data.",
    "published_date": "2020-05-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13755v1",
    "title": "Review of Mathematical frameworks for Fairness in Machine Learning",
    "authors": [
      "Eustasio del Barrio",
      "Paula Gordaliza",
      "Jean-Michel Loubes"
    ],
    "author_ids": [],
    "abstract": "A review of the main fairness definitions and fair learning methodologies\nproposed in the literature over the last years is presented from a mathematical\npoint of view. Following our independence-based approach, we consider how to\nbuild fair algorithms and the consequences on the degradation of their\nperformance compared to the possibly unfair case. This corresponds to the price\nfor fairness given by the criteria $\\textit{statistical parity}$ or\n$\\textit{equality of odds}$. Novel results giving the expressions of the\noptimal fair classifier and the optimal fair predictor (under a linear\nregression gaussian model) in the sense of $\\textit{equality of odds}$ are\npresented.",
    "published_date": "2020-05-26T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13755v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.12544v1",
    "title": "Unsupervised Domain Expansion from Multiple Sources",
    "authors": [
      "Jing Zhang",
      "Wanqing Li",
      "Lu sheng",
      "Chang Tang",
      "Philip Ogunbona"
    ],
    "author_ids": [],
    "abstract": "Given an existing system learned from previous source domains, it is\ndesirable to adapt the system to new domains without accessing and forgetting\nall the previous domains in some applications. This problem is known as domain\nexpansion. Unlike traditional domain adaptation in which the target domain is\nthe domain defined by new data, in domain expansion the target domain is formed\njointly by the source domains and the new domain (hence, domain expansion) and\nthe label function to be learned must work for the expanded domain.\nSpecifically, this paper presents a method for unsupervised multi-source domain\nexpansion (UMSDE) where only the pre-learned models of the source domains and\nunlabelled new domain data are available. We propose to use the predicted class\nprobability of the unlabelled data in the new domain produced by different\nsource models to jointly mitigate the biases among domains, exploit the\ndiscriminative information in the new domain, and preserve the performance in\nthe source domains. Experimental results on the VLCS, ImageCLEF_DA and PACS\ndatasets have verified the effectiveness of the proposed method.",
    "published_date": "2020-05-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12544v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.12503v1",
    "title": "BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection",
    "authors": [
      "Jihyung Moon",
      "Won Ik Cho",
      "Junbum Lee"
    ],
    "author_ids": [],
    "abstract": "Toxic comments in online platforms are an unavoidable social issue under the\ncloak of anonymity. Hate speech detection has been actively done for languages\nsuch as English, German, or Italian, where manually labeled corpus has been\nreleased. In this work, we first present 9.4K manually labeled entertainment\nnews comments for identifying Korean toxic speech, collected from a widely used\nonline news platform in Korea. The comments are annotated regarding social bias\nand hate speech since both aspects are correlated. The inter-annotator\nagreement Krippendorff's alpha score is 0.492 and 0.496, respectively. We\nprovide benchmarks using CharCNN, BiLSTM, and BERT, where BERT achieves the\nhighest score on all tasks. The models generally display better performance on\nbias identification, since the hate speech detection is a more subjective\nissue. Additionally, when BERT is trained with bias label for hate speech\ndetection, the prediction score increases, implying that bias and hate are\nintertwined. We make our dataset publicly available and open competitions with\nthe corpus and benchmarks.",
    "published_date": "2020-05-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12503v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.12495v2",
    "title": "Exploiting Local and Cloud Sensor Fusion in Intermittently Connected Sensor Networks",
    "authors": [
      "Michal Yemini",
      "Stephanie Gil",
      "Andrea Goldsmith"
    ],
    "author_ids": [],
    "abstract": "We consider a detection problem where sensors experience noisy measurements\nand intermittent communication opportunities to a centralized fusion center (or\ncloud). The objective of the problem is to arrive at the correct estimate of\nevent detection in the environment. The sensors may communicate locally with\nother sensors (local clusters) where they fuse their noisy sensor data to\nestimate the detection of an event locally. In addition, each sensor cluster\ncan intermittently communicate to the cloud, where a centralized fusion center\nfuses estimates from all sensor clusters to make a final determination\nregarding the occurrence of the event across the deployment area. We refer to\nthis hybrid communication scheme as a cloud-cluster architecture. Minimizing\nthe expected loss function of networks where noisy sensors are intermittently\nconnected to the cloud, as in our hybrid communication scheme, has not been\ninvestigated to our knowledge. We leverage recently improved concentration\ninequalities to arrive at an optimized decision rule for each cluster and we\nanalyze the expected detection performance resulting from our hybrid scheme.\nOur analysis shows that clustering the sensors provides resilience to noise in\nthe case of low communication probability with the cloud. For larger clusters,\na steep improvement in detection performance is possible even for a low\ncommunication probability by using our cloud-cluster architecture.",
    "published_date": "2020-05-26T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12495v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.12246v1",
    "title": "Demoting Racial Bias in Hate Speech Detection",
    "authors": [
      "Mengzhou Xia",
      "Anjalie Field",
      "Yulia Tsvetkov"
    ],
    "author_ids": [],
    "abstract": "In current hate speech datasets, there exists a high correlation between\nannotators' perceptions of toxicity and signals of African American English\n(AAE). This bias in annotated training data and the tendency of machine\nlearning models to amplify it cause AAE text to often be mislabeled as\nabusive/offensive/hate speech with a high false positive rate by current hate\nspeech classifiers. In this paper, we use adversarial training to mitigate this\nbias, introducing a hate speech classifier that learns to detect toxic\nsentences while demoting confounds corresponding to AAE texts. Experimental\nresults on a hate speech dataset and an AAE dataset suggest that our method is\nable to substantially reduce the false positive rate for AAE text while only\nminimally affecting the performance of hate speech classification.",
    "published_date": "2020-05-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12246v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.12087v1",
    "title": "Experimental, ad hoc, online, inter-university student e-contest during the pandemic: Lessons learned",
    "authors": [
      "Horia-Nicolai Teodorescu"
    ],
    "author_ids": [],
    "abstract": "We are reporting on lessons learned from an e-contest for students held\nduring the current pandemic. We compare the e-contest with the 10 previous\neditions of the same but face-to-face contest. While apparently the competition\ndid not suffer because of being a virtual one, some disadvantages were noted.\nThe main conclusions are: the basic interconnectivity means arise no serious\ntechnical issue, but the interconnectivity is more limited than the\nface-to-face one; online jury-competitors interactivity is poorer than\nface-to-face interactivity; human factors, higher uncertainties in the\norganization process, and less time to spend in the process for the local\norganizers are major limiting factors; concerns on the participation and\nevaluation fairness are higher; involuntary gender discrimination seems lower,\nbut persists; there are serious concerns related to privacy, including\ndifferential privacy; some peculiarities of the presented topics and of the\nevaluation process emerged, but it is unclear if they are related to the online\nnature of the competition, to the extra stress on the participants during the\npandemic, to other factors, or are random. While some conclusions may be\nintimately related to the analyzed case, some are general enough for being\nworth to other online competitions.",
    "published_date": "2020-05-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "K.0; K.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12087v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.11882v2",
    "title": "Sentiment Analysis: Automatically Detecting Valence, Emotions, and Other Affectual States from Text",
    "authors": [
      "Saif M. Mohammad"
    ],
    "author_ids": [],
    "abstract": "Recent advances in machine learning have led to computer systems that are\nhuman-like in behaviour. Sentiment analysis, the automatic determination of\nemotions in text, is allowing us to capitalize on substantial previously\nunattainable opportunities in commerce, public health, government policy,\nsocial sciences, and art. Further, analysis of emotions in text, from news to\nsocial media posts, is improving our understanding of not just how people\nconvey emotions through language but also how emotions shape our behaviour.\nThis article presents a sweeping overview of sentiment analysis research that\nincludes: the origins of the field, the rich landscape of tasks, challenges, a\nsurvey of the methods and resources used, and applications. We also discuss\ndiscuss how, without careful fore-thought, sentiment analysis has the potential\nfor harmful outcomes. We outline the latest lines of research in pursuit of\nfairness in sentiment analysis.",
    "published_date": "2020-05-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11882v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.11833v5",
    "title": "SecureABC: Secure AntiBody Certificates for COVID-19",
    "authors": [
      "Chris Hicks",
      "David Butler",
      "Carsten Maple",
      "Jon Crowcroft"
    ],
    "author_ids": [],
    "abstract": "COVID-19 has resulted in unprecedented social distancing policies being\nenforced worldwide. As governments seek to restore their economies, open\nworkplaces and permit travel there is a demand for technologies that may\nalleviate the requirement for social distancing whilst also protecting\nhealthcare services. In this work we explore the controversial technique of\nso-called immunity passports and present SecureABC: a decentralised,\nprivacy-preserving protocol for issuing and verifying antibody certificates. We\nconsider the implications of antibody certificate systems, develop a set of\nrisk-minimising principles and a security framework for their evaluation, and\nshow that these may be satisfied in practice. Finally, we also develop two\nadditional protocols that minimise individual discrimination but which still\nallow for collective transmission risk to be estimated. We use these two\nprotocols to illustrate the utility-privacy trade-offs of antibody certificates\nand their alternatives.",
    "published_date": "2020-05-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11833v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.11720v4",
    "title": "Projection to Fairness in Statistical Learning",
    "authors": [
      "Thibaut Le Gouic",
      "Jean-Michel Loubes",
      "Philippe Rigollet"
    ],
    "author_ids": [],
    "abstract": "In the context of regression, we consider the fundamental question of making\nan estimator fair while preserving its prediction accuracy as much as possible.\nTo that end, we define its projection to fairness as its closest fair estimator\nin a sense that reflects prediction accuracy. Our methodology leverages tools\nfrom optimal transport to construct efficiently the projection to fairness of\nany given estimator as a simple post-processing step. Moreover, our approach\nprecisely quantifies the cost of fairness, measured in terms of prediction\naccuracy.",
    "published_date": "2020-05-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "62J02",
      "I.2.0; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11720v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.11718v2",
    "title": "An inequality for the number of periods in a word",
    "authors": [
      "Daniel Gabric",
      "Narad Rampersad",
      "Jeffrey Shallit"
    ],
    "author_ids": [],
    "abstract": "We prove an inequality for the number of periods in a word x in terms of the\nlength of x and its initial critical exponent. Next, we characterize all\nperiods of the length-n prefix of a characteristic Sturmian word in terms of\nthe lazy Ostrowski representation of n, and use this result to show that our\ninequality is tight for infinitely many words x. We propose two related\nmeasures of periodicity for infinite words. Finally, we also consider special\ncases where x is overlap-free or squarefree.",
    "published_date": "2020-05-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DM",
      "cs.FL",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11718v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.11534v1",
    "title": "Token-curated registry (TCR) in a scholarly journal: blockchain meets invisible colleges",
    "authors": [
      "Artyom Kosmarski",
      "Nikolay Gordiychuk"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a novel framework for a scholarly journal, a\ntoken-curated registry (TCR). This model originates in the field of blockchain\nand cryptoeconomics and is essentially a decentralized system where tokens\n(digital currency) are used to incentivize quality curation of information. TCR\nis an automated way to create lists of any kind where decisions (whether to\ninclude N or not) are made through voting that brings benefit or loss to\nvoters. In an academic journal, TCR could act as a tool to introduce\ncommunity-driven decisions on papers to be published, thus encouraging more\nactive participation of authors and reviewers in editorial policy and\nelaborating the idea of a journal as a club. TCR could also provide a novel\nsolution to the problems of editorial bias and the lack of rewards/incentives\nfor reviewers. In the paper, we discuss core principles of TCR, its\ntechnological and cultural foundations, and finally analyze the risks and\nchallenges it could bring to scholarly publishing.",
    "published_date": "2020-05-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11534v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.11478v2",
    "title": "Short-term Load Forecasting Based on Hybrid Strategy Using Warm-start Gradient Tree Boosting",
    "authors": [
      "Yuexin Zhang",
      "Jiahong Wang"
    ],
    "author_ids": [],
    "abstract": "A deep-learning-based hybrid strategy for short-term load forecasting is\npresented. The strategy proposes a novel tree-based ensemble method Warm-start\nGradient Tree Boosting (WGTB). Current strategies either ensemble submodels of\na single type, which fail to take advantage of the statistical strengths of\ndifferent inference models. Or they simply sum the outputs from completely\ndifferent inference models, which doesn't maximize the potential of ensemble.\nInspired by the bias-variance trade-off, WGTB is proposed and tailored to the\ngreat disparity among different inference models on accuracy, volatility and\nlinearity. The complete strategy integrates four different inference models of\ndifferent capacities. WGTB then ensembles their outputs by a warm-start and a\nhybrid of bagging and boosting, which lowers bias and variance concurrently. It\nis validated on two real datasets from State Grid Corporation of China of\nhourly resolution. The result demonstrates the effectiveness of the proposed\nstrategy that hybridizes the statistical strengths of both low-bias and\nlow-variance inference models.",
    "published_date": "2020-05-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11478v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.11282v1",
    "title": "PruneNet: Channel Pruning via Global Importance",
    "authors": [
      "Ashish Khetan",
      "Zohar Karnin"
    ],
    "author_ids": [],
    "abstract": "Channel pruning is one of the predominant approaches for accelerating deep\nneural networks. Most existing pruning methods either train from scratch with a\nsparsity inducing term such as group lasso, or prune redundant channels in a\npretrained network and then fine tune the network. Both strategies suffer from\nsome limitations: the use of group lasso is computationally expensive,\ndifficult to converge and often suffers from worse behavior due to the\nregularization bias. The methods that start with a pretrained network either\nprune channels uniformly across the layers or prune channels based on the basic\nstatistics of the network parameters. These approaches either ignore the fact\nthat some CNN layers are more redundant than others or fail to adequately\nidentify the level of redundancy in different layers. In this work, we\ninvestigate a simple-yet-effective method for pruning channels based on a\ncomputationally light-weight yet effective data driven optimization step that\ndiscovers the necessary width per layer. Experiments conducted on ILSVRC-$12$\nconfirm effectiveness of our approach. With non-uniform pruning across the\nlayers on ResNet-$50$, we are able to match the FLOP reduction of\nstate-of-the-art channel pruning results while achieving a $0.98\\%$ higher\naccuracy. Further, we show that our pruned ResNet-$50$ network outperforms\nResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$\noutperforms ResNet-$50$.",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11282v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.11186v1",
    "title": "EEG source localization analysis in epileptic children during a visual working-memory task",
    "authors": [
      "Evangelos Galaris",
      "Ioannis Gallos",
      "Ivan Myatchin",
      "Lieven Lagae",
      "Constantinos Siettos"
    ],
    "author_ids": [],
    "abstract": "We localize the sources of brain activity of children with epilepsy based on\nEEG recordings acquired during a visual discrimination working memory task. For\nthe numerical solution of the inverse problem, with the aid of age-specific MRI\nscans processed from a publicly available database, we use and compare three\nregularization numerical methods, namely the standarized Low Resolution\nElectromagnetic Tomography (sLORETA), the weighted Minimum Norm Estimation\n(wMNE) and the dynamic Statistical Parametric Mapping (dSPM). We show that all\nthree methods provide the same spatio-temporal patterns of differences between\nepileptic and control children. In particular, our analysis reveals\nstatistically significant differences between the two groups in regions of the\nParietal Cortex indicating that these may serve as \"biomarkers\" for diagnostic\npurposes and ultimately localized treatment.",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.NC",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11186v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.11082v2",
    "title": "Tractometry-based Anomaly Detection for Single-subject White Matter Analysis",
    "authors": [
      "Maxime Chamberland",
      "Sila Genc",
      "Erika P. Raven",
      "Greg D. Parker",
      "Adam Cunningham",
      "Joanne Doherty",
      "Marianne van den Bree",
      "Chantal M. W. Tax",
      "Derek K. Jones"
    ],
    "author_ids": [],
    "abstract": "There is an urgent need for a paradigm shift from group-wise comparisons to\nindividual diagnosis in diffusion MRI (dMRI) to enable the analysis of rare\ncases and clinically-heterogeneous groups. Deep autoencoders have shown great\npotential to detect anomalies in neuroimaging data. We present a framework that\noperates on the manifold of white matter (WM) pathways to learn normative\nmicrostructural features, and discriminate those at genetic risk from controls\nin a paediatric population.",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11082v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.11037v1",
    "title": "Style Normalization and Restitution for Generalizable Person Re-identification",
    "authors": [
      "Xin Jin",
      "Cuiling Lan",
      "Wenjun Zeng",
      "Zhibo Chen",
      "Li Zhang"
    ],
    "author_ids": [],
    "abstract": "Existing fully-supervised person re-identification (ReID) methods usually\nsuffer from poor generalization capability caused by domain gaps. The key to\nsolving this problem lies in filtering out identity-irrelevant interference and\nlearning domain-invariant person representations. In this paper, we aim to\ndesign a generalizable person ReID framework which trains a model on source\ndomains yet is able to generalize/perform well on target domains. To achieve\nthis goal, we propose a simple yet effective Style Normalization and\nRestitution (SNR) module. Specifically, we filter out style variations (e.g.,\nillumination, color contrast) by Instance Normalization (IN). However, such a\nprocess inevitably removes discriminative information. We propose to distill\nidentity-relevant feature from the removed information and restitute it to the\nnetwork to ensure high discrimination. For better disentanglement, we enforce a\ndual causal loss constraint in SNR to encourage the separation of\nidentity-relevant features and identity-irrelevant features. Extensive\nexperiments demonstrate the strong generalization capability of our framework.\nOur models empowered by the SNR modules significantly outperform the\nstate-of-the-art domain generalization approaches on multiple widely-used\nperson ReID benchmarks, and also show superiority on unsupervised domain\nadaptation.",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11037v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.11029v1",
    "title": "Economic Valuation and Pricing of Inertia in Inverter-Dominated Power Systems",
    "authors": [
      "Matthieu Paturet",
      "Uros Markovic",
      "Stefanos Delikaraoglou",
      "Evangelos Vrettos",
      "Petros Aristidou",
      "Garbiela Hug"
    ],
    "author_ids": [],
    "abstract": "This paper studies the procurement and pricing of inertial response using a\nfrequency-constrained unit commitment formulation, which co-optimizes the\nprovision of energy and inertia services while accounting for their\ncomplementary properties. The proposed approach builds on a two-step process\nthat allows to differentiate between the units being online for energy purposes\nand the ones committed additionally solely for inertia provision. Subsequently,\nthree novel pricing and payment schemes that reimburse inertia providers in a\ntransparent and fair manner according to their individual participation are\nproposed. The analysis considers both synchronous and converter-based\ngenerators and provides insights regarding the impact of each pricing scheme on\ntotal system cost, as well as on the investment signals for technologies\ncapable of offering affordable inertial response. The results show that all\nthree methods will have a beneficial impact on frequency stability and aid the\noperator in ensuring system reliability, whereas the policy implications to\ndifferent inertia providing units will vary between the payment schemes.",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11029v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.11009v1",
    "title": "Investigating Label Bias in Beam Search for Open-ended Text Generation",
    "authors": [
      "Liang Wang",
      "Jinlong Liu",
      "Jingming Liu"
    ],
    "author_ids": [],
    "abstract": "Beam search is an effective and widely used decoding algorithm in many\nsequence-to-sequence (seq2seq) text generation tasks. However, in open-ended\ntext generation, beam search is often found to produce repetitive and generic\ntexts, sampling-based decoding algorithms like top-k sampling and nucleus\nsampling are more preferred. Standard seq2seq models suffer from label bias due\nto its locally normalized probability formulation. This paper provides a series\nof empirical evidence that label bias is a major reason for such degenerate\nbehaviors of beam search. By combining locally normalized maximum likelihood\nestimation and globally normalized sequence-level training, label bias can be\nreduced with almost no sacrifice in perplexity. To quantitatively measure label\nbias, we test the model's ability to discriminate the groundtruth text and a\nset of context-agnostic distractors. We conduct experiments on large-scale\nresponse generation datasets. Results show that beam search can produce more\ndiverse and meaningful texts with our approach, in terms of both automatic and\nhuman evaluation metrics. Our analysis also suggests several future working\ndirections towards the grand challenge of open-ended text generation.",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.11009v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10979v1",
    "title": "Focus Longer to See Better:Recursively Refined Attention for Fine-Grained Image Classification",
    "authors": [
      "Prateek Shroff",
      "Tianlong Chen",
      "Yunchao Wei",
      "Zhangyang Wang"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Network has shown great strides in the coarse-grained image\nclassification task. It was in part due to its strong ability to extract\ndiscriminative feature representations from the images. However, the marginal\nvisual difference between different classes in fine-grained images makes this\nvery task harder. In this paper, we tried to focus on these marginal\ndifferences to extract more representative features. Similar to human vision,\nour network repetitively focuses on parts of images to spot small\ndiscriminative parts among the classes. Moreover, we show through\ninterpretability techniques how our network focus changes from coarse to fine\ndetails. Through our experiments, we also show that a simple attention model\ncan aggregate (weighted) these finer details to focus on the most dominant\ndiscriminative part of the image. Our network uses only image-level labels and\ndoes not need bounding box/part annotation information. Further, the simplicity\nof our network makes it an easy plug-n-play module. Apart from providing\ninterpretability, our network boosts the performance (up to 2%) when compared\nto its baseline counterparts. Our codebase is available at\nhttps://github.com/TAMU-VITA/Focus-Longer-to-See-Better",
    "published_date": "2020-05-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10979v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.12379v2",
    "title": "Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness",
    "authors": [
      "Sumon Biswas",
      "Hridesh Rajan"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are increasingly being used in important\ndecision-making software such as approving bank loans, recommending criminal\nsentencing, hiring employees, and so on. It is important to ensure the fairness\nof these models so that no discrimination is made based on protected attribute\n(e.g., race, sex, age) while decision making. Algorithms have been developed to\nmeasure unfairness and mitigate them to a certain extent. In this paper, we\nhave focused on the empirical evaluation of fairness and mitigations on\nreal-world machine learning models. We have created a benchmark of 40 top-rated\nmodels from Kaggle used for 5 different tasks, and then using a comprehensive\nset of fairness metrics, evaluated their fairness. Then, we have applied 7\nmitigation techniques on these models and analyzed the fairness, mitigation\nresults, and impacts on performance. We have found that some model optimization\ntechniques result in inducing unfairness in the models. On the other hand,\nalthough there are some fairness control mechanisms in machine learning\nlibraries, they are not documented. The mitigation algorithm also exhibit\ncommon patterns such as mitigation in the post-processing is often costly (in\nterms of performance) and mitigation in the pre-processing stage is preferred\nin most cases. We have also presented different trade-off choices of fairness\nmitigation decisions. Our study suggests future research directions to reduce\nthe gap between theoretical fairness aware algorithms and the software\nengineering methods to leverage them in practice.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.SE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12379v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10930v1",
    "title": "Reversals of Rényi Entropy Inequalities under Log-Concavity",
    "authors": [
      "James Melbourne",
      "Tomasz Tkocz"
    ],
    "author_ids": [],
    "abstract": "We establish a discrete analog of the R\\'enyi entropy comparison due to\nBobkov and Madiman. For log-concave variables on the integers, the min entropy\nis within log e of the usual Shannon entropy. Additionally we investigate the\nentropic Rogers-Shephard inequality studied by Madiman and Kontoyannis, and\nestablish a sharp R\\'enyi version for certain parameters in both the continuous\nand discrete cases",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10930v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.10716v2",
    "title": "Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation",
    "authors": [
      "Weixin Liang",
      "James Zou",
      "Zhou Yu"
    ],
    "author_ids": [],
    "abstract": "Open Domain dialog system evaluation is one of the most important challenges\nin dialog research. Existing automatic evaluation metrics, such as BLEU are\nmostly reference-based. They calculate the difference between the generated\nresponse and a limited number of available references. Likert-score based\nself-reported user rating is widely adopted by social conversational systems,\nsuch as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\nfrom bias and variance among different users. To alleviate this problem, we\nformulate dialog evaluation as a comparison task. We also propose an automatic\nevaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\nautomatically cleans self-reported user ratings as it trains on them.\nSpecifically, we first use a self-supervised method to learn better dialog\nfeature representation, and then use KNN and Shapley to remove confusing\nsamples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\ncomparison task.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10716v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.13521v1",
    "title": "Q-NAV: NAV Setting Method based on Reinforcement Learning in Underwater Wireless Networks",
    "authors": [
      "Seok-Hyeon Park",
      "Ohyun Jo"
    ],
    "author_ids": [],
    "abstract": "The demand on the underwater communications is extremely increasing in\nsearching for underwater resources, marine expedition, or environmental\nresearches, yet there are many problems with the wireless communications\nbecause of the characteristics of the underwater environments. Especially, with\nthe underwater wireless networks, there happen inevitable delay time and\nspacial inequality due to the distances between the nodes. To solve these\nproblems, this paper suggests a new solution based on ALOHA-Q. The suggested\nmethod use random NAV value. and Environments take reward through\ncommunications success or fail. After then, The environments setting NAV value\nfrom reward. This model minimizes usage of energy and computing resources under\nthe underwater wireless networks, and learns and setting NAV values through\nintense learning. The results of the simulations show that NAV values can be\nenvironmentally adopted and select best value to the circumstances, so the\nproblems which are unnecessary delay times and spacial inequality can be\nsolved. Result of simulations, NAV time decreasing 17.5% compared with original\nNAV.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13521v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.14621v1",
    "title": "Fair Classification via Unconstrained Optimization",
    "authors": [
      "Ibrahim Alabdulmohsin"
    ],
    "author_ids": [],
    "abstract": "Achieving the Bayes optimal binary classification rule subject to group\nfairness constraints is known to be reducible, in some cases, to learning a\ngroup-wise thresholding rule over the Bayes regressor. In this paper, we extend\nthis result by proving that, in a broader setting, the Bayes optimal fair\nlearning rule remains a group-wise thresholding rule over the Bayes regressor\nbut with a (possible) randomization at the thresholds. This provides a stronger\njustification to the post-processing approach in fair classification, in which\n(1) a predictor is learned first, after which (2) its output is adjusted to\nremove bias. We show how the post-processing rule in this two-stage approach\ncan be learned quite efficiently by solving an unconstrained optimization\nproblem. The proposed algorithm can be applied to any black-box machine\nlearning model, such as deep neural networks, random forests and support vector\nmachines. In addition, it can accommodate many fairness criteria that have been\npreviously proposed in the literature, such as equalized odds and statistical\nparity. We prove that the algorithm is Bayes consistent and motivate it,\nfurthermore, via an impossibility result that quantifies the tradeoff between\naccuracy and fairness across multiple demographic groups. Finally, we conclude\nby validating the algorithm on the Adult benchmark dataset.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML",
      "68T05",
      "I.2.6; I.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.14621v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.12974v1",
    "title": "Opportunistic Multi-aspect Fairness through Personalized Re-ranking",
    "authors": [
      "Nasim Sonboli",
      "Farzad Eskandanian",
      "Robin Burke",
      "Weiwen Liu",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "As recommender systems have become more widespread and moved into areas with\ngreater social impact, such as employment and housing, researchers have begun\nto seek ways to ensure fairness in the results that such systems produce. This\nwork has primarily focused on developing recommendation approaches in which\nfairness metrics are jointly optimized along with recommendation accuracy.\nHowever, the previous work had largely ignored how individual preferences may\nlimit the ability of an algorithm to produce fair recommendations. Furthermore,\nwith few exceptions, researchers have only considered scenarios in which\nfairness is measured relative to a single sensitive feature or attribute (such\nas race or gender). In this paper, we present a re-ranking approach to\nfairness-aware recommendation that learns individual preferences across\nmultiple fairness dimensions and uses them to enhance provider fairness in\nrecommendation results. Specifically, we show that our opportunistic and\nmetric-agnostic approach achieves a better trade-off between accuracy and\nfairness than prior re-ranking approaches and does so across multiple fairness\ndimensions.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12974v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10430v1",
    "title": "Gender Slopes: Counterfactual Fairness for Computer Vision Models by Attribute Manipulation",
    "authors": [
      "Jungseock Joo",
      "Kimmo Kärkkäinen"
    ],
    "author_ids": [],
    "abstract": "Automated computer vision systems have been applied in many domains including\nsecurity, law enforcement, and personal devices, but recent reports suggest\nthat these systems may produce biased results, discriminating against people in\ncertain demographic groups. Diagnosing and understanding the underlying true\ncauses of model biases, however, are challenging tasks because modern computer\nvision systems rely on complex black-box models whose behaviors are hard to\ndecode. We propose to use an encoder-decoder network developed for image\nattribute manipulation to synthesize facial images varying in the dimensions of\ngender and race while keeping other signals intact. We use these synthesized\nimages to measure counterfactual fairness of commercial computer vision\nclassifiers by examining the degree to which these classifiers are affected by\ngender and racial cues controlled in the images, e.g., feminine faces may\nelicit higher scores for the concept of nurse and lower scores for STEM-related\nconcepts. We also report the skewed gender representations in an online search\nservice on profession-related keywords, which may explain the origin of the\nbiases encoded in the models.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10419v1",
    "title": "Why distillation helps: a statistical perspective",
    "authors": [
      "Aditya Krishna Menon",
      "Ankit Singh Rawat",
      "Sashank J. Reddi",
      "Seungyeon Kim",
      "Sanjiv Kumar"
    ],
    "author_ids": [],
    "abstract": "Knowledge distillation is a technique for improving the performance of a\nsimple \"student\" model by replacing its one-hot training labels with a\ndistribution over labels obtained from a complex \"teacher\" model. While this\nsimple approach has proven widely effective, a basic question remains\nunresolved: why does distillation help? In this paper, we present a statistical\nperspective on distillation which addresses this question, and provides a novel\nconnection to extreme multiclass retrieval techniques. Our core observation is\nthat the teacher seeks to estimate the underlying (Bayes) class-probability\nfunction. Building on this, we establish a fundamental bias-variance tradeoff\nin the student's objective: this quantifies how approximate knowledge of these\nclass-probabilities can significantly aid learning. Finally, we show how\ndistillation complements existing negative mining techniques for extreme\nmulticlass retrieval, and propose a unified objective which combines these\nideas.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10419v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10400v5",
    "title": "Principal Fairness for Human and Algorithmic Decision-Making",
    "authors": [
      "Kosuke Imai",
      "Zhichao Jiang"
    ],
    "author_ids": [],
    "abstract": "Using the concept of principal stratification from the causal inference\nliterature, we introduce a new notion of fairness, called principal fairness,\nfor human and algorithmic decision-making. The key idea is that one should not\ndiscriminate among individuals who would be similarly affected by the decision.\nUnlike the existing statistical definitions of fairness, principal fairness\nexplicitly accounts for the fact that individuals can be impacted by the\ndecision. Furthermore, we explain how principal fairness differs from the\nexisting causality-based fairness criteria. In contrast to the counterfactual\nfairness criteria, for example, principal fairness considers the effects of\ndecision in question rather than those of protected attributes of interest. We\nbriefly discuss how to approach empirical evaluation and policy learning\nproblems under the proposed principal fairness criterion.",
    "published_date": "2020-05-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10400v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10388v3",
    "title": "Why are many businesses instilling a DevOps culture into their organization?",
    "authors": [
      "Jessica Diaz",
      "Daniel López-Fernández",
      "Jorge Perez",
      "Ángel González-Prieto"
    ],
    "author_ids": [],
    "abstract": "Context: DevOps can be defined as a cultural movement to improve and\naccelerate the delivery of business value by making the collaboration between\ndevelopment and operations effective. Although this movement is relatively\nrecent, there exist an intensive research around DevOps. However, the real\nreasons why companies move to DevOps and the results they expect to obtain have\nbeen paid little attention in real contexts. Objective: This paper aims to help\npractitioners and researchers to better understand the context and the problems\nthat many companies face day to day in their organizations when they try to\naccelerate software delivery and the main drivers that move these companies to\nadopting DevOps. Method: We conducted an exploratory study by leveraging in\ndepth, semi-structured interviews to relevant stakeholders of 30 multinational\nsoftware-intensive companies, together industrial workshops and observations at\norganizations' facilities that supported triangulation. Additionally, we\nconducted an inter-coder agreement analysis, which is not usually addressed in\nqualitative studies in software engineering, to increase reliability and reduce\nauthors bias of the drawn findings. Results: The research explores the problems\nand expected outcomes that moved companies to adopt DevOps and reveals a set of\npatterns and anti-patterns about the reasons why companies are instilling a\nDevOps culture. Conclusions: This study aims to strengthen evidence and support\npractitioners in making better informed about which problems trigger a DevOps\ntransition and most common expected results.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10388v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.10266v4",
    "title": "Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation",
    "authors": [
      "Liang-Chieh Chen",
      "Raphael Gontijo Lopes",
      "Bowen Cheng",
      "Maxwell D. Collins",
      "Ekin D. Cubuk",
      "Barret Zoph",
      "Hartwig Adam",
      "Jonathon Shlens"
    ],
    "author_ids": [],
    "abstract": "Supervised learning in large discriminative models is a mainstay for modern\ncomputer vision. Such an approach necessitates investing in large-scale\nhuman-annotated datasets for achieving state-of-the-art results. In turn, the\nefficacy of supervised learning may be limited by the size of the human\nannotated dataset. This limitation is particularly notable for image\nsegmentation tasks, where the expense of human annotation is especially large,\nyet large amounts of unlabeled data may exist. In this work, we ask if we may\nleverage semi-supervised learning in unlabeled video sequences and extra images\nto improve the performance on urban scene segmentation, simultaneously tackling\nsemantic, instance, and panoptic segmentation. The goal of this work is to\navoid the construction of sophisticated, learned architectures specific to\nlabel propagation (e.g., patch matching and optical flow). Instead, we simply\npredict pseudo-labels for the unlabeled data and train subsequent models with\nboth human-annotated and pseudo-labeled data. The procedure is iterated for\nseveral times. As a result, our Naive-Student model, trained with such simple\nyet effective iterative semi-supervised learning, attains state-of-the-art\nresults at all three Cityscapes benchmarks, reaching the performance of 67.8%\nPQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable\nstep towards building a simple procedure to harness unlabeled video sequences\nand extra images to surpass state-of-the-art performance on core computer\nvision tasks.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10266v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10150v1",
    "title": "GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection",
    "authors": [
      "Shijie Zhang",
      "Hongzhi Yin",
      "Tong Chen",
      "Quoc Viet Nguyen Hung",
      "Zi Huang",
      "Lizhen Cui"
    ],
    "author_ids": [],
    "abstract": "In recent years, recommender system has become an indispensable function in\nall e-commerce platforms. The review rating data for a recommender system\ntypically comes from open platforms, which may attract a group of malicious\nusers to deliberately insert fake feedback in an attempt to bias the\nrecommender system to their favour. The presence of such attacks may violate\nmodeling assumptions that high-quality data is always available and these data\ntruly reflect users' interests and preferences. Therefore, it is of great\npractical significance to construct a robust recommender system that is able to\ngenerate stable recommendations even in the presence of shilling attacks. In\nthis paper, we propose GraphRfi - a GCN-based user representation learning\nframework to perform robust recommendation and fraudster detection in a unified\nway. In its end-to-end learning process, the probability of a user being\nidentified as a fraudster in the fraudster detection component automatically\ndetermines the contribution of this user's rating data in the recommendation\ncomponent; while the prediction error outputted in the recommendation component\nacts as an important feature in the fraudster detection component. Thus, these\ntwo components can mutually enhance each other. Extensive experiments have been\nconducted and the experimental results show the superiority of our GraphRfi in\nthe two tasks - robust rating prediction and fraudster detection. Furthermore,\nthe proposed GraphRfi is validated to be more robust to the various types of\nshilling attacks over the state-of-the-art recommender systems.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10150v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.10089v2",
    "title": "Investigation of Large-Margin Softmax in Neural Language Modeling",
    "authors": [
      "Jingjing Huo",
      "Yingbo Gao",
      "Weiyue Wang",
      "Ralf Schlüter",
      "Hermann Ney"
    ],
    "author_ids": [],
    "abstract": "To encourage intra-class compactness and inter-class separability among\ntrainable feature vectors, large-margin softmax methods are developed and\nwidely applied in the face recognition community. The introduction of the\nlarge-margin concept into the softmax is reported to have good properties such\nas enhanced discriminative power, less overfitting and well-defined geometric\nintuitions. Nowadays, language modeling is commonly approached with neural\nnetworks using softmax and cross entropy. In this work, we are curious to see\nif introducing large-margins to neural language models would improve the\nperplexity and consequently word error rate in automatic speech recognition.\nSpecifically, we first implement and test various types of conventional margins\nfollowing the previous works in face recognition. To address the distribution\nof natural language data, we then compare different strategies for word vector\nnorm-scaling. After that, we apply the best norm-scaling setup in combination\nwith various margins and conduct neural language models rescoring experiments\nin automatic speech recognition. We find that although perplexity is slightly\ndeteriorated, neural language models with large-margin softmax can yield word\nerror rate similar to that of the standard softmax baseline. Finally, expected\nmargins are analyzed through visualization of word vectors, showing that the\nsyntactic and semantic relationships are also preserved.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10089v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.10050v2",
    "title": "Risk of Training Diagnostic Algorithms on Data with Demographic Bias",
    "authors": [
      "Samaneh Abbasi-Sureshjani",
      "Ralf Raumanns",
      "Britt E. J. Michels",
      "Gerard Schouten",
      "Veronika Cheplygina"
    ],
    "author_ids": [],
    "abstract": "One of the critical challenges in machine learning applications is to have\nfair predictions. There are numerous recent examples in various domains that\nconvincingly show that algorithms trained with biased datasets can easily lead\nto erroneous or discriminatory conclusions. This is even more crucial in\nclinical applications where the predictive algorithms are designed mainly based\non a limited or given set of medical images and demographic variables such as\nage, sex and race are not taken into account. In this work, we conduct a survey\nof the MICCAI 2018 proceedings to investigate the common practice in medical\nimage analysis applications. Surprisingly, we found that papers focusing on\ndiagnosis rarely describe the demographics of the datasets used, and the\ndiagnosis is purely based on images. In order to highlight the importance of\nconsidering the demographics in diagnosis tasks, we used a publicly available\ndataset of skin lesions. We then demonstrate that a classifier with an overall\narea under the curve (AUC) of 0.83 has variable performance between 0.76 and\n0.91 on subgroups based on age and sex, even though the training set was\nrelatively balanced. Moreover, we show that it is possible to learn unbiased\nfeatures by explicitly using demographic variables in an adversarial training\nsetup, which leads to balanced scores per subgroups. Finally, we discuss the\nimplications of these results and provide recommendations for further research.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10050v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09959v4",
    "title": "Psychometrics in Behavioral Software Engineering: A Methodological Introduction with Guidelines",
    "authors": [
      "Daniel Graziotin",
      "Per Lenberg",
      "Robert Feldt",
      "Stefan Wagner"
    ],
    "author_ids": [],
    "abstract": "A meaningful and deep understanding of the human aspects of software\nengineering (SE) requires psychological constructs to be considered. Psychology\ntheory can facilitate the systematic and sound development as well as the\nadoption of instruments (e.g., psychological tests, questionnaires) to assess\nthese constructs. In particular, to ensure high quality, the psychometric\nproperties of instruments need evaluation. In this paper, we provide an\nintroduction to psychometric theory for the evaluation of measurement\ninstruments for SE researchers. We present guidelines that enable using\nexisting instruments and developing new ones adequately. We conducted a\ncomprehensive review of the psychology literature framed by the Standards for\nEducational and Psychological Testing. We detail activities used when\noperationalizing new psychological constructs, such as item pooling, item\nreview, pilot testing, item analysis, factor analysis, statistical property of\nitems, reliability, validity, and fairness in testing and test bias. We provide\nan openly available example of a psychometric evaluation based on our\nguideline. We hope to encourage a culture change in SE research towards the\nadoption of established methods from psychology. To improve the quality of\nbehavioral research in SE, studies focusing on introducing, validating, and\nthen using psychometric instruments need to be more common.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09959v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.12964v9",
    "title": "Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems",
    "authors": [
      "Chang Zhou",
      "Jianxin Ma",
      "Jianwei Zhang",
      "Jingren Zhou",
      "Hongxia Yang"
    ],
    "author_ids": [],
    "abstract": "Deep candidate generation (DCG) that narrows down the collection of relevant\nitems from billions to hundreds via representation learning has become\nprevalent in industrial recommender systems. Standard approaches approximate\nmaximum likelihood estimation (MLE) through sampling for better scalability and\naddress the problem of DCG in a way similar to language modeling. However, live\nrecommender systems face severe exposure bias and have a vocabulary several\norders of magnitude larger than that of natural language, implying that MLE\nwill preserve and even exacerbate the exposure bias in the long run in order to\nfaithfully fit the observed samples. In this paper, we theoretically prove that\na popular choice of contrastive loss is equivalent to reducing the exposure\nbias via inverse propensity weighting, which provides a new perspective for\nunderstanding the effectiveness of contrastive learning. Based on the\ntheoretical discovery, we design CLRec, a contrastive learning method to\nimprove DCG in terms of fairness, effectiveness and efficiency in recommender\nsystems with extremely large candidate size. We further improve upon CLRec and\npropose Multi-CLRec, for accurate multi-intention aware bias reduction. Our\nmethods have been successfully deployed in Taobao, where at least four-month\nonline A/B tests and offline analyses demonstrate its substantial improvements,\nincluding a dramatic reduction in the Matthew effect.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12964v9",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09900v2",
    "title": "Fair Outlier Detection",
    "authors": [
      "Deepak P",
      "Savitha Sam Abraham"
    ],
    "author_ids": [],
    "abstract": "An outlier detection method may be considered fair over specified sensitive\nattributes if the results of outlier detection are not skewed towards\nparticular groups defined on such sensitive attributes. In this task, we\nconsider, for the first time to our best knowledge, the task of fair outlier\ndetection. In this work, we consider the task of fair outlier detection over\nmultiple multi-valued sensitive attributes (e.g., gender, race, religion,\nnationality, marital status etc.). We propose a fair outlier detection method,\nFairLOF, that is inspired by the popular LOF formulation for neighborhood-based\noutlier detection. We outline ways in which unfairness could be induced within\nLOF and develop three heuristic principles to enhance fairness, which form the\nbasis of the FairLOF method. Being a novel task, we develop an evaluation\nframework for fair outlier detection, and use that to benchmark FairLOF on\nquality and fairness of results. Through an extensive empirical evaluation over\nreal-world datasets, we illustrate that FairLOF is able to achieve significant\nimprovements in fairness at sometimes marginal degradations on result quality\nas measured against the fairness-agnostic LOF method.",
    "published_date": "2020-05-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09900v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09755v1",
    "title": "Adapting a Kidney Exchange Algorithm to Align with Human Values",
    "authors": [
      "Rachel Freedman",
      "Jana Schaich Borg",
      "Walter Sinnott-Armstrong",
      "John P. Dickerson",
      "Vincent Conitzer"
    ],
    "author_ids": [],
    "abstract": "The efficient and fair allocation of limited resources is a classical problem\nin economics and computer science. In kidney exchanges, a central market maker\nallocates living kidney donors to patients in need of an organ. Patients and\ndonors in kidney exchanges are prioritized using ad-hoc weights decided on by\ncommittee and then fed into an allocation algorithm that determines who gets\nwhat--and who does not. In this paper, we provide an end-to-end methodology for\nestimating weights of individual participant profiles in a kidney exchange. We\nfirst elicit from human subjects a list of patient attributes they consider\nacceptable for the purpose of prioritizing patients (e.g., medical\ncharacteristics, lifestyle choices, and so on). Then, we ask subjects\ncomparison queries between patient profiles and estimate weights in a\nprincipled way from their responses. We show how to use these weights in kidney\nexchange market clearing algorithms. We then evaluate the impact of the weights\nin simulations and find that the precise numerical values of the weights we\ncomputed matter little, other than the ordering of profiles that they imply.\nHowever, compared to not prioritizing patients at all, there is a significant\neffect, with certain classes of patients being (de)prioritized based on the\nhuman-elicited value judgments.",
    "published_date": "2020-05-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09755v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09619v2",
    "title": "Identifying Statistical Bias in Dataset Replication",
    "authors": [
      "Logan Engstrom",
      "Andrew Ilyas",
      "Shibani Santurkar",
      "Dimitris Tsipras",
      "Jacob Steinhardt",
      "Aleksander Madry"
    ],
    "author_ids": [],
    "abstract": "Dataset replication is a useful tool for assessing whether improvements in\ntest accuracy on a specific benchmark correspond to improvements in models'\nability to generalize reliably. In this work, we present unintuitive yet\nsignificant ways in which standard approaches to dataset replication introduce\nstatistical bias, skewing the resulting observations. We study ImageNet-v2, a\nreplication of the ImageNet dataset on which models exhibit a significant\n(11-14%) drop in accuracy, even after controlling for a standard\nhuman-in-the-loop measure of data quality. We show that after correcting for\nthe identified statistical bias, only an estimated $3.6\\% \\pm 1.5\\%$ of the\noriginal $11.7\\% \\pm 1.0\\%$ accuracy drop remains unaccounted for. We conclude\nwith concrete recommendations for recognizing and avoiding bias in dataset\nreplication. Code for our study is publicly available at\nhttp://github.com/MadryLab/dataset-replication-analysis .",
    "published_date": "2020-05-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09619v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09420v2",
    "title": "Probabilistic feasibility guarantees for solution sets to uncertain variational inequalities",
    "authors": [
      "Filippo Fabiani",
      "Kostas Margellos",
      "Paul J. Goulart"
    ],
    "author_ids": [],
    "abstract": "We develop a data-driven approach to the computation of a-posteriori\nfeasibility certificates to the solution sets of variational inequalities\naffected by uncertainty. Specifically, we focus on instances of variational\ninequalities with a deterministic mapping and an uncertain feasibility set, and\nrepresent uncertainty by means of scenarios. Building upon recent advances in\nthe scenario approach literature, we quantify the robustness properties of the\nentire set of solutions of a variational inequality, with feasibility set\nconstructed using the scenario approach, against a new unseen realization of\nthe uncertainty. Our results extend existing results that typically impose an\nassumption that the solution set is a singleton and require certain\nnon-degeneracy properties, and thereby offer probabilistic feasibility\nguarantees to any feasible solution. We show that assessing the violation\nprobability of an entire set of solutions, rather than of a singleton, requires\nenumeration of the support constraints that \"shape\" this set. Additionally, we\npropose a general procedure to enumerate the support constraints that does not\nrequire a closed form description of the solution set, which is unlikely to be\navailable. We show that robust game theory problems can be modelling via\nuncertain variational inequalities, and illustrate our theoretical results\nthrough extensive numerical simulations on a case study involving an electric\nvehicle charging coordination problem.",
    "published_date": "2020-05-19T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09420v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.09272v2",
    "title": "Addressing Class-Imbalance Problem in Personalized Ranking",
    "authors": [
      "Lu Yu",
      "Shichao Pei",
      "Chuxu Zhang",
      "Shangsong Liang",
      "Xiao Bai",
      "Nitesh Chawla",
      "Xiangliang Zhang"
    ],
    "author_ids": [],
    "abstract": "Pairwise ranking models have been widely used to address recommendation\nproblems. The basic idea is to learn the rank of users' preferred items through\nseparating items into \\emph{positive} samples if user-item interactions exist,\nand \\emph{negative} samples otherwise. Due to the limited number of observable\ninteractions, pairwise ranking models face serious \\emph{class-imbalance}\nissues. Our theoretical analysis shows that current sampling-based methods\ncause the vertex-level imbalance problem, which makes the norm of learned item\nembeddings towards infinite after a certain training iterations, and\nconsequently results in vanishing gradient and affects the model inference\nresults. We thus propose an efficient \\emph{\\underline{Vi}tal\n\\underline{N}egative \\underline{S}ampler} (VINS) to alleviate the\nclass-imbalance issue for pairwise ranking model, in particular for deep\nlearning models optimized by gradient methods. The core of VINS is a bias\nsampler with reject probability that will tend to accept a negative candidate\nwith a larger degree weight than the given positive item. Evaluation results on\nseveral real datasets demonstrate that the proposed sampling method speeds up\nthe training procedure 30\\% to 50\\% for ranking models ranging from shallow to\ndeep, while maintaining and even improving the quality of ranking results in\ntop-N item recommendation.",
    "published_date": "2020-05-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09272v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09257v3",
    "title": "Bias-based Universal Adversarial Patch Attack for Automatic Check-out",
    "authors": [
      "Aishan Liu",
      "Jiakai Wang",
      "Xianglong Liu",
      "Bowen Cao",
      "Chongzhi Zhang",
      "Hang Yu"
    ],
    "author_ids": [],
    "abstract": "Adversarial examples are inputs with imperceptible perturbations that easily\nmisleading deep neural networks(DNNs). Recently, adversarial patch, with noise\nconfined to a small and localized patch, has emerged for its easy feasibility\nin real-world scenarios. However, existing strategies failed to generate\nadversarial patches with strong generalization ability. In other words, the\nadversarial patches were input-specific and failed to attack images from all\nclasses, especially unseen ones during training. To address the problem, this\npaper proposes a bias-based framework to generate class-agnostic universal\nadversarial patches with strong generalization ability, which exploits both the\nperceptual and semantic bias of models. Regarding the perceptual bias, since\nDNNs are strongly biased towards textures, we exploit the hard examples which\nconvey strong model uncertainties and extract a textural patch prior from them\nby adopting the style similarities. The patch prior is more close to decision\nboundaries and would promote attacks. To further alleviate the heavy dependency\non large amounts of data in training universal attacks, we further exploit the\nsemantic bias. As the class-wise preference, prototypes are introduced and\npursued by maximizing the multi-class margin to help universal training. Taking\nAutomaticCheck-out (ACO) as the typical scenario, extensive experiments\nincluding white-box and black-box settings in both digital-world(RPC, the\nlargest ACO related dataset) and physical-world scenario(Taobao and JD, the\nworld' s largest online shopping platforms) are conducted. Experimental results\ndemonstrate that our proposed framework outperforms state-of-the-art\nadversarial patch attack methods.",
    "published_date": "2020-05-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09257v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09209v3",
    "title": "Fair Inputs and Fair Outputs: The Incompatibility of Fairness in Privacy and Accuracy",
    "authors": [
      "Bashir Rastegarpanah",
      "Mark Crovella",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "Fairness concerns about algorithmic decision-making systems have been mainly\nfocused on the outputs (e.g., the accuracy of a classifier across individuals\nor groups). However, one may additionally be concerned with fairness in the\ninputs. In this paper, we propose and formulate two properties regarding the\ninputs of (features used by) a classifier. In particular, we claim that fair\nprivacy (whether individuals are all asked to reveal the same information) and\nneed-to-know (whether users are only asked for the minimal information required\nfor the task at hand) are desirable properties of a decision system. We explore\nthe interaction between these properties and fairness in the outputs (fair\nprediction accuracy). We show that for an optimal classifier these three\nproperties are in general incompatible, and we explain what common properties\nof data make them incompatible. Finally we provide an algorithm to verify if\nthe trade-off between the three properties exists in a given dataset, and use\nthe algorithm to show that this trade-off is common in real data.",
    "published_date": "2020-05-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09209v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.09052v3",
    "title": "Underestimation Bias and Underfitting in Machine Learning",
    "authors": [
      "Padraig Cunningham",
      "Sarah Jane Delany"
    ],
    "author_ids": [],
    "abstract": "Often, what is termed algorithmic bias in machine learning will be due to\nhistoric bias in the training data. But sometimes the bias may be introduced\n(or at least exacerbated) by the algorithm itself. The ways in which algorithms\ncan actually accentuate bias has not received a lot of attention with\nresearchers focusing directly on methods to eliminate bias - no matter the\nsource. In this paper we report on initial research to understand the factors\nthat contribute to bias in classification algorithms. We believe this is\nimportant because underestimation bias is inextricably tied to regularization,\ni.e. measures to address overfitting can accentuate bias.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.09052v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08864v1",
    "title": "Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings",
    "authors": [
      "Katherine McCurdy",
      "Oguz Serbetci"
    ],
    "author_ids": [],
    "abstract": "Recent research has demonstrated that vector space models of semantics can\nreflect undesirable biases in human culture. Our investigation of\ncrosslinguistic word embeddings reveals that topical gender bias interacts\nwith, and is surpassed in magnitude by, the effect of grammatical gender\nassociations, and both may be attenuated by corpus lemmatization. This finding\nhas implications for downstream applications such as machine translation.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08864v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08788v1",
    "title": "Entropy conservation property and entropy stabilization of high-order continuous Galerkin approximations to scalar conservation laws",
    "authors": [
      "Dmitri Kuzmin",
      "Manuel Quezada de Luna"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the design of linear and nonlinear stabilization\nprocedures for high-order continuous Galerkin (CG) finite element\ndiscretizations of scalar conservation laws. We prove that the standard CG\nmethod is entropy conservative for the square entropy. In general, the rate of\nentropy production/dissipation depends on the residual of the governing\nequation and on the accuracy of the finite element approximation to the entropy\nvariable. The inclusion of linear high-order stabilization generates an\nadditional source/sink in the entropy budget equation. To balance the amount of\nentropy production in each cell, we construct entropy-dissipative element\ncontributions using a coercive bilinear form and a parameter-free entropy\nviscosity coefficient. The entropy stabilization term is high-order consistent,\nand optimal convergence behavior is achieved in practice. To enforce\npreservation of local bounds in addition to entropy stability, we use the\nBernstein basis representation of the finite element solution and a new subcell\nflux limiting procedure. The underlying inequality constraints ensure the\nvalidity of localized entropy conditions and local maximum principles. The\nbenefits of the proposed modifications are illustrated by numerical results for\nlinear and nonlinear test problems.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08788v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.08749v2",
    "title": "Learning Adjustment Sets from Observational and Limited Experimental Data",
    "authors": [
      "Sofia Triantafillou",
      "Gregory Cooper"
    ],
    "author_ids": [],
    "abstract": "Estimating causal effects from observational data is not always possible due\nto confounding. Identifying a set of appropriate covariates (adjustment set)\nand adjusting for their influence can remove confounding bias; however, such a\nset is typically not identifiable from observational data alone. Experimental\ndata do not have confounding bias, but are typically limited in sample size and\ncan therefore yield imprecise estimates. Furthermore, experimental data often\ninclude a limited set of covariates, and therefore provide limited insight into\nthe causal structure of the underlying system. In this work we introduce a\nmethod that combines large observational and limited experimental data to\nidentify adjustment sets and improve the estimation of causal effects. The\nmethod identifies an adjustment set (if possible) by calculating the marginal\nlikelihood for the experimental data given observationally-derived prior\nprobabilities of potential adjustmen sets. In this way, the method can make\ninferences that are not possible using only the conditional dependencies and\nindependencies in all the observational and experimental data. We show that the\nmethod successfully identifies adjustment sets and improves causal effect\nestimation in simulated data, and it can sometimes make additional inferences\nwhen compared to state-of-the-art methods for combining experimental and\nobservational data.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08749v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08663v1",
    "title": "Ethical Issues Regarding the Use of AI Profiling Services for Recruiting: The Japanese Rikunabi Data Scandal",
    "authors": [
      "Kudo Fumiko",
      "Hiromi Arai",
      "Arisa Ema"
    ],
    "author_ids": [],
    "abstract": "The ethical, legal, and social challenges involved in the use of profiling\nservices for recruitment are the focus of many previous studies; however, the\nprocesses vary depending on the social system and cultural practices. In August\n2019, a scandal occurred in Japan in which a recruitment management company was\nfound to have breached users' and students' trust by selling their data to\nclients. By sharing the Japanese recruitment context and associated laws, this\narticle contributes to our understanding of the ethical issues involved in\nartificial intelligence profiling and in handling sensitive personal\ninformation.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08663v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08531v1",
    "title": "Meta-learning with Stochastic Linear Bandits",
    "authors": [
      "Leonardo Cella",
      "Alessandro Lazaric",
      "Massimiliano Pontil"
    ],
    "author_ids": [],
    "abstract": "We investigate meta-learning procedures in the setting of stochastic linear\nbandits tasks. The goal is to select a learning algorithm which works well on\naverage over a class of bandits tasks, that are sampled from a\ntask-distribution. Inspired by recent work on learning-to-learn linear\nregression, we consider a class of bandit algorithms that implement a\nregularized version of the well-known OFUL algorithm, where the regularization\nis a square euclidean distance to a bias vector. We first study the benefit of\nthe biased OFUL algorithm in terms of regret minimization. We then propose two\nstrategies to estimate the bias within the learning-to-learn setting. We show\nboth theoretically and experimentally, that when the number of tasks grows and\nthe variance of the task-distribution is small, our strategies have a\nsignificant advantage over learning the tasks in isolation.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08502v2",
    "title": "COVI White Paper",
    "authors": [
      "Hannah Alsdurf",
      "Edmond Belliveau",
      "Yoshua Bengio",
      "Tristan Deleu",
      "Prateek Gupta",
      "Daphne Ippolito",
      "Richard Janda",
      "Max Jarvie",
      "Tyler Kolody",
      "Sekoul Krastev",
      "Tegan Maharaj",
      "Robert Obryk",
      "Dan Pilat",
      "Valerie Pisano",
      "Benjamin Prud'homme",
      "Meng Qu",
      "Nasim Rahaman",
      "Irina Rish",
      "Jean-Francois Rousseau",
      "Abhinav Sharma",
      "Brooke Struck",
      "Jian Tang",
      "Martin Weiss",
      "Yun William Yu"
    ],
    "author_ids": [],
    "abstract": "The SARS-CoV-2 (Covid-19) pandemic has caused significant strain on public\nhealth institutions around the world. Contact tracing is an essential tool to\nchange the course of the Covid-19 pandemic. Manual contact tracing of Covid-19\ncases has significant challenges that limit the ability of public health\nauthorities to minimize community infections. Personalized peer-to-peer contact\ntracing through the use of mobile apps has the potential to shift the paradigm.\nSome countries have deployed centralized tracking systems, but more\nprivacy-protecting decentralized systems offer much of the same benefit without\nconcentrating data in the hands of a state authority or for-profit\ncorporations. Machine learning methods can circumvent some of the limitations\nof standard digital tracing by incorporating many clues and their uncertainty\ninto a more graded and precise estimation of infection risk. The estimated risk\ncan provide early risk awareness, personalized recommendations and relevant\ninformation to the user. Finally, non-identifying risk data can inform\nepidemiological models trained jointly with the machine learning predictor.\nThese models can provide statistical evidence for the importance of factors\ninvolved in disease transmission. They can also be used to monitor, evaluate\nand optimize health policy and (de)confinement scenarios according to medical\nand economic productivity indicators. However, such a strategy based on mobile\napps and machine learning should proactively mitigate potential ethical and\nprivacy risks, which could have substantial impacts on society (not only\nimpacts on health but also impacts such as stigmatization and abuse of personal\ndata). Here, we present an overview of the rationale, design, ethical\nconsiderations and privacy strategy of `COVI,' a Covid-19 public peer-to-peer\ncontact tracing and risk awareness mobile application developed in Canada.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08502v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08482v2",
    "title": "Variational Hyper-Encoding Networks",
    "authors": [
      "Phuoc Nguyen",
      "Truyen Tran",
      "Sunil Gupta",
      "Santu Rana",
      "Hieu-Chi Dam",
      "Svetha Venkatesh"
    ],
    "author_ids": [],
    "abstract": "We propose a framework called HyperVAE for encoding distributions of\ndistributions. When a target distribution is modeled by a VAE, its neural\nnetwork parameters \\theta is drawn from a distribution p(\\theta) which is\nmodeled by a hyper-level VAE. We propose a variational inference using Gaussian\nmixture models to implicitly encode the parameters \\theta into a low\ndimensional Gaussian distribution. Given a target distribution, we predict the\nposterior distribution of the latent code, then use a matrix-network decoder to\ngenerate a posterior distribution q(\\theta). HyperVAE can encode the parameters\n\\theta in full in contrast to common hyper-networks practices, which generate\nonly the scale and bias vectors as target-network parameters. Thus HyperVAE\npreserves much more information about the model for each task in the latent\nspace. We discuss HyperVAE using the minimum description length (MDL) principle\nand show that it helps HyperVAE to generalize. We evaluate HyperVAE in density\nestimation tasks, outlier detection and discovery of novel design classes,\ndemonstrating its efficacy.",
    "published_date": "2020-05-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08482v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08275v2",
    "title": "Variable Splitting Methods for Constrained State Estimation in Partially Observed Markov Processes",
    "authors": [
      "Rui Gao",
      "Filip Tronarp",
      "Simo Särkkä"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a class of efficient, accurate, and general methods\nfor solving state-estimation problems with equality and inequality constraints.\nThe methods are based on recent developments in variable splitting and\npartially observed Markov processes. We first present the generalized framework\nbased on variable splitting, then develop efficient methods to solve the\nstate-estimation subproblems arising in the framework. The solutions to these\nsubproblems can be made efficient by leveraging the Markovian structure of the\nmodel as is classically done in so-called Bayesian filtering and smoothing\nmethods. The numerical experiments demonstrate that our methods outperform\nconventional optimization methods in computation cost as well as the estimation\nperformance.",
    "published_date": "2020-05-17T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08275v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.08231v2",
    "title": "Studying the Transfer of Biases from Programmers to Programs",
    "authors": [
      "Johanna Johansen",
      "Tore Pedersen",
      "Christian Johansen"
    ],
    "author_ids": [],
    "abstract": "It is generally agreed that one origin of machine bias is resulting from\ncharacteristics within the dataset on which the algorithms are trained, i.e.,\nthe data does not warrant a generalized inference. We, however, hypothesize\nthat a different `mechanism', hitherto not articulated in the literature, may\nalso be responsible for machine's bias, namely that biases may originate from\n(i) the programmers' cultural background, such as education or line of work, or\n(ii) the contextual programming environment, such as software requirements or\ndeveloper tools. Combining an experimental and comparative design, we studied\nthe effects of cultural metaphors and contextual metaphors, and tested whether\neach of these would `transfer' from the programmer to program, thus\nconstituting a machine bias. The results show (i) that cultural metaphors\ninfluence the programmer's choices and (ii) that `induced' contextual metaphors\ncan be used to moderate or exacerbate the effects of the cultural metaphors.\nThis supports our hypothesis that biases in automated systems do not always\noriginate from within the machine's training data. Instead, machines may also\n`replicate' and `reproduce' biases from the programmers' cultural background by\nthe transfer of cultural metaphors into the programming process. Implications\nfor academia and professional practice range from the micro programming-level\nto the macro national-regulations or educational level, and span across all\nsocietal domains where software-based systems are operating such as the popular\nAI-based automated decision support systems.",
    "published_date": "2020-05-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08231v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08146v2",
    "title": "Semi-Automating Knowledge Base Construction for Cancer Genetics",
    "authors": [
      "Somin Wadhwa",
      "Kanhua Yin",
      "Kevin S. Hughes",
      "Byron C. Wallace"
    ],
    "author_ids": [],
    "abstract": "In this work, we consider the exponentially growing subarea of genetics in\ncancer. The need to synthesize and centralize this evidence for dissemination\nhas motivated a team of physicians to manually construct and maintain a\nknowledge base that distills key results reported in the literature. This is a\nlaborious process that entails reading through full-text articles to understand\nthe study design, assess study quality, and extract the reported cancer risk\nestimates associated with particular hereditary cancer genes (i.e.,\npenetrance). In this work, we propose models to automatically surface key\nelements from full-text cancer genetics articles, with the ultimate aim of\nexpediting the manual workflow currently in place.\n  We propose two challenging tasks that are critical for characterizing the\nfindings reported cancer genetics studies: (i) Extracting snippets of text that\ndescribe \\emph{ascertainment mechanisms}, which in turn inform whether the\npopulation studied may introduce bias owing to deviations from the target\npopulation; (ii) Extracting reported risk estimates (e.g., odds or hazard\nratios) associated with specific germline mutations. The latter task may be\nviewed as a joint entity tagging and relation extraction problem. To train\nmodels for these tasks, we induce distant supervision over tokens and snippets\nin full-text articles using the manually constructed knowledge base. We propose\nand evaluate several model variants, including a transformer-based joint entity\nand relation extraction model to extract <germline mutation, risk-estimate>}\npairs. We observe strong empirical performance, highlighting the practical\npotential for such models to aid KB construction in this space. We ablate\ncomponents of our model, observing, e.g., that a joint model for <germline\nmutation, risk-estimate> fares substantially better than a pipelined approach.",
    "published_date": "2020-05-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08146v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.08141v4",
    "title": "Neutral bots probe political bias on social media",
    "authors": [
      "Wen Chen",
      "Diogo Pacheco",
      "Kai-Cheng Yang",
      "Filippo Menczer"
    ],
    "author_ids": [],
    "abstract": "Social media platforms attempting to curb abuse and misinformation have been\naccused of political bias. We deploy neutral social bots who start following\ndifferent news sources on Twitter, and track them to probe distinct biases\nemerging from platform mechanisms versus user interactions. We find no strong\nor consistent evidence of political bias in the news feed. Despite this, the\nnews and information to which U.S. Twitter users are exposed depend strongly on\nthe political leaning of their early connections. The interactions of\nconservative accounts are skewed toward the right, whereas liberal accounts are\nexposed to moderate content shifting their experience toward the political\ncenter. Partisan accounts, especially conservative ones, tend to receive more\nfollowers and follow more automated accounts. Conservative accounts also find\nthemselves in denser communities and are exposed to more low-credibility\ncontent.",
    "published_date": "2020-05-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08141v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.08065v1",
    "title": "How Biased is the Population of Facebook Users? Comparing the Demographics of Facebook Users with Census Data to Generate Correction Factors",
    "authors": [
      "Filipe N. Ribeiro",
      "Fabrício Benevenuto",
      "Emilio Zagheni"
    ],
    "author_ids": [],
    "abstract": "Censuses around the world are key sources of data to guide government\ninvestments and public policies. However, these sources are very expensive to\nobtain and are collected relatively infrequently. Over the last decade, there\nhas been growing interest in the use of data from social media to complement\ntraditional data sources. However, social media users are not representative of\nthe general population. Thus, analyses based on social media data require\nstatistical adjustments, like post-stratification, in order to remove the bias\nand make solid statistical claims. These adjustments are possible only when we\nhave information about the frequency of demographic groups using social media.\nThese data, when compared with official statistics, enable researchers to\nproduce appropriate statistical correction factors. In this paper, we leverage\nthe Facebook advertising platform to compile the equivalent of an\naggregate-level census of Facebook users. Our compilation includes the\npopulation distribution for seven demographic attributes such as gender and age\nat different geographic levels for the US. By comparing the Facebook counts\nwith official reports provided by the US Census and Gallup, we found very high\ncorrelations, especially for political leaning and race. We also identified\ninstances where official statistics may be underestimating population counts as\nin the case of immigration. We use the information collected to calculate bias\ncorrection factors for all computed attributes in order to evaluate the extent\nto which different demographic groups are more or less represented on Facebook.\nWe provide the first comprehensive analysis for assessing biases in Facebook\nusers across several dimensions. This information can be used to generate\nbias-adjusted population estimates and demographic counts in a timely way and\nat fine geographic granularity in between data releases of official statistics",
    "published_date": "2020-05-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08065v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.08033v1",
    "title": "Towards classification parity across cohorts",
    "authors": [
      "Aarsh Patel",
      "Rahul Gupta",
      "Mukund Harakere",
      "Satyapriya Krishna",
      "Aman Alok",
      "Peng Liu"
    ],
    "author_ids": [],
    "abstract": "Recently, there has been a lot of interest in ensuring algorithmic fairness\nin machine learning where the central question is how to prevent sensitive\ninformation (e.g. knowledge about the ethnic group of an individual) from\nadding \"unfair\" bias to a learning algorithm (Feldman et al. (2015), Zemel et\nal. (2013)). This has led to several debiasing algorithms on word embeddings\n(Qian et al. (2019) , Bolukbasi et al. (2016)), coreference resolution (Zhao et\nal. (2018a)), semantic role labeling (Zhao et al. (2017)), etc. Most of these\nexisting work deals with explicit sensitive features such as gender,\noccupations or race which doesn't work with data where such features are not\ncaptured due to privacy concerns. In this research work, we aim to achieve\nclassification parity across explicit as well as implicit sensitive features.\nWe define explicit cohorts as groups of people based on explicit sensitive\nattributes provided in the data (age, gender, race) whereas implicit cohorts\nare defined as groups of people with similar language usage. We obtain implicit\ncohorts by clustering embeddings of each individual trained on the language\ngenerated by them using a language model. We achieve two primary objectives in\nthis work : [1.] We experimented and discovered classification performance\ndifferences across cohorts based on implicit and explicit features , [2] We\nimproved classification parity by introducing modification to the loss function\naimed to minimize the range of model performances across cohorts.",
    "published_date": "2020-05-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.08033v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.07778v3",
    "title": "Access Control for Distributed Ledgers in the Internet of Things: A Networking Approach",
    "authors": [
      "Andrew Cullen",
      "Pietro Ferraro",
      "William Sanders",
      "Luigi Vigneri",
      "Robert Shorten"
    ],
    "author_ids": [],
    "abstract": "In the Internet of Things (IoT) domain, devices need a platform to transact\nseamlessly without a trusted intermediary. Although Distributed Ledger\nTechnologies (DLTs) could provide such a platform, blockchains, such as\nBitcoin, were not designed with IoT networks in mind, hence are often\nunsuitable for such applications: they offer poor transaction throughput and\nconfirmation times, put stress on constrained computing and storage resources,\nand require high transaction fees. In this work, we consider a class of\nIoT-friendly DLTs based on directed acyclic graphs, rather than a blockchain,\nand with a reputation system in the place of Proof of Work (PoW). However,\nwithout PoW, implementation of these DLTs requires an access control algorithm\nto manage the rate at which nodes can add new transactions to the ledger. We\nmodel the access control problem and present an algorithm that is fair,\nefficient and secure. Our algorithm represents a new design paradigm for DLTs\nin which concepts from networking are applied to the DLT setting for the first\ntime. For example, our algorithm uses distributed rate setting which is similar\nin nature to transmission control used in the Internet. However, our solution\nfeatures novel adaptations to cope with the adversarial environment of DLTs in\nwhich no individual agent can be trusted. Our algorithm guarantees utilisation\nof resources, consistency, fairness, and resilience against attackers. All of\nthis is achieved efficiently and with regard for the limitations of IoT\ndevices. We perform extensive simulations to validate these claims.",
    "published_date": "2020-05-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07778v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.07734v1",
    "title": "Uncovering Gender Bias in Media Coverage of Politicians with Machine Learning",
    "authors": [
      "Susan Leavy"
    ],
    "author_ids": [],
    "abstract": "This paper presents research uncovering systematic gender bias in the\nrepresentation of political leaders in the media, using artificial\nintelligence. Newspaper coverage of Irish ministers over a fifteen year period\nwas gathered and analysed with natural language processing techniques and\nmachine learning. Findings demonstrate evidence of gender bias in the portrayal\nof female politicians, the kind of policies they were associated with and how\nthey were evaluated in terms of their performance as political leaders. This\npaper also sets out a methodology whereby media content may be analysed on a\nlarge scale utilising techniques from artificial intelligence within a\ntheoretical framework founded in gender theory and feminist linguistics.",
    "published_date": "2020-05-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07734v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.07572v3",
    "title": "Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics",
    "authors": [
      "Donald Martin Jr.",
      "Vinodkumar Prabhakaran",
      "Jill Kuhlberg",
      "Andrew Smart",
      "William S. Isaac"
    ],
    "author_ids": [],
    "abstract": "Recent research on algorithmic fairness has highlighted that the problem\nformulation phase of ML system development can be a key source of bias that has\nsignificant downstream impacts on ML system fairness outcomes. However, very\nlittle attention has been paid to methods for improving the fairness efficacy\nof this critical phase of ML system development. Current practice neither\naccounts for the dynamic complexity of high-stakes domains nor incorporates the\nperspectives of vulnerable stakeholders. In this paper we introduce community\nbased system dynamics (CBSD) as an approach to enable the participation of\ntypically excluded stakeholders in the problem formulation phase of the ML\nsystem development process and facilitate the deep problem understanding\nrequired to mitigate bias during this crucial stage.",
    "published_date": "2020-05-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07572v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.07423v1",
    "title": "Phase Transition of a Non-Linear Opinion Dynamics with Noisy Interactions",
    "authors": [
      "Francesco d'Amore",
      "Andrea Clementi",
      "Emanuele Natale"
    ],
    "author_ids": [],
    "abstract": "In several real \\emph{Multi-Agent Systems} (MAS), it has been observed that\nonly weaker forms of\\emph{metastable consensus} are achieved, in which a large\nmajority of agents agree on some opinion while other opinions continue to be\nsupported by a (small) minority of agents. In this work, we take a step towards\nthe investigation of metastable consensus for complex (non-linear)\n\\emph{opinion dynamics} by considering the famous \\undecided dynamics in the\nbinary setting, which is known to reach consensus exponentially faster than the\n\\voter dynamics. We propose a simple form of uniform noise in which each\nmessage can change to another one with probability $p$ and we prove that the\npersistence of a \\emph{metastable consensus} undergoes a \\emph{phase\ntransition} for $p=\\frac 16$. In detail, below this threshold, we prove the\nsystem reaches with high probability a metastable regime where a large majority\nof agents keeps supporting the same opinion for polynomial time. Moreover, this\nopinion turns out to be the initial majority opinion, whenever the initial bias\nis slightly larger than its standard deviation.On the contrary, above the\nthreshold, we show that the information about the initial majority opinion is\n\"lost\" within logarithmic time even when the initial bias is\nmaximum.Interestingly, using a simple coupling argument, we show the\nequivalence between our noisy model above and the model where a subset of\nagents behave in a \\emph{stubborn} way.",
    "published_date": "2020-05-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC",
      "cs.CC",
      "cs.SI",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07423v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.07383v2",
    "title": "On Bottleneck Features for Text-Dependent Speaker Verification Using X-vectors",
    "authors": [
      "Achintya Kumar Sarkar",
      "Zheng-Hua Tan"
    ],
    "author_ids": [],
    "abstract": "Applying x-vectors for speaker verification has recently attracted great\ninterest, with the focus being on text-independent speaker verification. In\nthis paper, we study x-vectors for text-dependent speaker verification (TD-SV),\nwhich remains unexplored. We further investigate the impact of the different\nbottleneck (BN) features on the performance of x-vectors, including the\nrecently-introduced time-contrastive-learning (TCL) BN features and\nphone-discriminant BN features. TCL is a weakly supervised learning approach\nthat constructs training data by uniformly partitioning each utterance into a\npredefined number of segments and then assigning each segment a class label\ndepending on their position in the utterance. We also compare TD-SV performance\nfor different modeling techniques, including the Gaussian mixture\nmodels-universal background model (GMM-UBM), i-vector, and x-vector.\nExperiments are conducted on the RedDots 2016 challenge database. It is found\nthat the type of features has a marginal impact on the performance of x-vectors\nwith the TCL BN feature achieving the lowest equal error rate, while the impact\nof features is significant for i-vector and GMM-UBM. The fusion of x-vector and\ni-vector systems gives a large gain in performance. The GMM-UBM technique shows\nits advantage for TD-SV using short utterances.",
    "published_date": "2020-05-15T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07383v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.07302v2",
    "title": "Investigating Bias in Deep Face Analysis: The KANFace Dataset and Empirical Study",
    "authors": [
      "Markos Georgopoulos",
      "Yannis Panagakis",
      "Maja Pantic"
    ],
    "author_ids": [],
    "abstract": "Deep learning-based methods have pushed the limits of the state-of-the-art in\nface analysis. However, despite their success, these models have raised\nconcerns regarding their bias towards certain demographics. This bias is\ninflicted both by limited diversity across demographics in the training set, as\nwell as the design of the algorithms. In this work, we investigate the\ndemographic bias of deep learning models in face recognition, age estimation,\ngender recognition and kinship verification. To this end, we introduce the most\ncomprehensive, large-scale dataset of facial images and videos to date. It\nconsists of 40K still images and 44K sequences (14.5M video frames in total)\ncaptured in unconstrained, real-world conditions from 1,045 subjects. The data\nare manually annotated in terms of identity, exact age, gender and kinship. The\nperformance of state-of-the-art models is scrutinized and demographic bias is\nexposed by conducting a series of experiments. Lastly, a method to debias\nnetwork embeddings is introduced and tested on the proposed benchmarks.",
    "published_date": "2020-05-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07302v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.07293v1",
    "title": "Statistical Equity: A Fairness Classification Objective",
    "authors": [
      "Ninareh Mehrabi",
      "Yuzhong Huang",
      "Fred Morstatter"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems have been shown to propagate the societal errors of\nthe past. In light of this, a wealth of research focuses on designing solutions\nthat are \"fair.\" Even with this abundance of work, there is no singular\ndefinition of fairness, mainly because fairness is subjective and context\ndependent. We propose a new fairness definition, motivated by the principle of\nequity, that considers existing biases in the data and attempts to make\nequitable decisions that account for these previous historical biases. We\nformalize our definition of fairness, and motivate it with its appropriate\ncontexts. Next, we operationalize it for equitable classification. We perform\nmultiple automatic and human evaluations to show the effectiveness of our\ndefinition and demonstrate its utility for aspects of fairness, such as the\nfeedback loop.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07293v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.07253v3",
    "title": "Information Design for Congested Social Services: Optimal Need-Based Persuasion",
    "authors": [
      "Jerry Anunrojwong",
      "Krishnamurthy Iyer",
      "Vahideh Manshadi"
    ],
    "author_ids": [],
    "abstract": "We study the effectiveness of information design in reducing congestion in\nsocial services catering to users with varied levels of need. In the absence of\nprice discrimination and centralized admission, the provider relies on sharing\ninformation about wait times to improve welfare. We consider a stylized model\nwith heterogeneous users who differ in their private outside options: low-need\nusers have an acceptable outside option to the social service, whereas\nhigh-need users have no viable outside option. Upon arrival, a user decides to\nwait for the service by joining an unobservable first-come-first-serve queue,\nor leave and seek her outside option. To reduce congestion and improve social\noutcomes, the service provider seeks to persuade more low-need users to avail\ntheir outside option, and thus better serve high-need users. We characterize\nthe Pareto-optimal signaling mechanisms and compare their welfare outcomes\nagainst several benchmarks. We show that if either type is the overwhelming\nmajority of the population, information design does not provide improvement\nover sharing full information or no information. On the other hand, when the\npopulation is a mixture of the two types, information design not only Pareto\ndominates full-information and no-information mechanisms, in some regimes it\nalso achieves the same welfare as the \"first-best\", i.e., the Pareto-optimal\ncentralized admission policy with knowledge of users' types.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.TH",
      "90B22, 91A80",
      "G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.07253v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.06618v2",
    "title": "Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective Learning",
    "authors": [
      "Procheta Sen",
      "Debasis Ganguly"
    ],
    "author_ids": [],
    "abstract": "Human society had a long history of suffering from cognitive biases leading\nto social prejudices and mass injustice. The prevalent existence of cognitive\nbiases in large volumes of historical data can pose a threat of being\nmanifested as unethical and seemingly inhuman predictions as outputs of AI\nsystems trained on such data. To alleviate this problem, we propose a\nbias-aware multi-objective learning framework that given a set of identity\nattributes (e.g. gender, ethnicity etc.) and a subset of sensitive categories\nof the possible classes of prediction outputs, learns to reduce the frequency\nof predicting certain combinations of them, e.g. predicting stereotypes such as\n`most blacks use abusive language', or `fear is a virtue of women'. Our\nexperiments conducted on an emotion prediction task with balanced class priors\nshows that a set of baseline bias-agnostic models exhibit cognitive biases with\nrespect to gender, such as women are prone to be afraid whereas men are more\nprone to be angry. In contrast, our proposed bias-aware multi-objective\nlearning methodology is shown to reduce such biases in the predictied emotions.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06618v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06935v1",
    "title": "Simultaneous imputation and disease classification in incomplete medical datasets using Multigraph Geometric Matrix Completion (MGMC)",
    "authors": [
      "Gerome Vivar",
      "Anees Kazi",
      "Hendrik Burwinkel",
      "Andreas Zwergal",
      "Nassir Navab",
      "Seyed-Ahmad Ahmadi"
    ],
    "author_ids": [],
    "abstract": "Large-scale population-based studies in medicine are a key resource towards\nbetter diagnosis, monitoring, and treatment of diseases. They also serve as\nenablers of clinical decision support systems, in particular Computer Aided\nDiagnosis (CADx) using machine learning (ML). Numerous ML approaches for CADx\nhave been proposed in literature. However, these approaches assume full data\navailability, which is not always feasible in clinical data. To account for\nmissing data, incomplete data samples are either removed or imputed, which\ncould lead to data bias and may negatively affect classification performance.\nAs a solution, we propose an end-to-end learning of imputation and disease\nprediction of incomplete medical datasets via Multigraph Geometric Matrix\nCompletion (MGMC). MGMC uses multiple recurrent graph convolutional networks,\nwhere each graph represents an independent population model based on a key\nclinical meta-feature like age, sex, or cognitive function. Graph signal\naggregation from local patient neighborhoods, combined with multigraph signal\nfusion via self-attention, has a regularizing effect on both matrix\nreconstruction and classification performance. Our proposed approach is able to\nimpute class relevant features as well as perform accurate classification on\ntwo publicly available medical datasets. We empirically show the superiority of\nour proposed approach in terms of classification and imputation performance\nwhen compared with state-of-the-art approaches. MGMC enables disease prediction\nin multimodal and incomplete medical datasets. These findings could serve as\nbaseline for future CADx approaches which utilize incomplete datasets.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06935v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06915v3",
    "title": "Can The Crowd Identify Misinformation Objectively? The Effects of Judgment Scale and Assessor's Background",
    "authors": [
      "Kevin Roitero",
      "Michael Soprano",
      "Shaoyang Fan",
      "Damiano Spina",
      "Stefano Mizzaro",
      "Gianluca Demartini"
    ],
    "author_ids": [],
    "abstract": "Truthfulness judgments are a fundamental step in the process of fighting\nmisinformation, as they are crucial to train and evaluate classifiers that\nautomatically distinguish true and false statements. Usually such judgments are\nmade by experts, like journalists for political statements or medical doctors\nfor medical statements. In this paper, we follow a different approach and rely\non (non-expert) crowd workers. This of course leads to the following research\nquestion: Can crowdsourcing be reliably used to assess the truthfulness of\ninformation and to create large-scale labeled collections for information\ncredibility systems? To address this issue, we present the results of an\nextensive study based on crowdsourcing: we collect thousands of truthfulness\nassessments over two datasets, and we compare expert judgments with crowd\njudgments, expressed on scales with various granularity levels. We also measure\nthe political bias and the cognitive background of the workers, and quantify\ntheir effect on the reliability of the data provided by the crowd.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.SI",
      "68P20",
      "H.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06915v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.06898v2",
    "title": "Mitigating Gender Bias in Machine Learning Data Sets",
    "authors": [
      "Susan Leavy",
      "Gerardine Meaney",
      "Karen Wade",
      "Derek Greene"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence has the capacity to amplify and perpetuate societal\nbiases and presents profound ethical implications for society. Gender bias has\nbeen identified in the context of employment advertising and recruitment tools,\ndue to their reliance on underlying language processing and recommendation\nalgorithms. Attempts to address such issues have involved testing learned\nassociations, integrating concepts of fairness to machine learning and\nperforming more rigorous analysis of training data. Mitigating bias when\nalgorithms are trained on textual data is particularly challenging given the\ncomplex way gender ideology is embedded in language. This paper proposes a\nframework for the identification of gender bias in training data for machine\nlearning.The work draws upon gender theory and sociolinguistics to\nsystematically indicate levels of bias in textual training data and associated\nneural word embedding models, thus highlighting pathways for both removing bias\nfrom training data and critically assessing its impact.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06898v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06852v2",
    "title": "Ethical Adversaries: Towards Mitigating Unfairness with Adversarial Machine Learning",
    "authors": [
      "Pieter Delobelle",
      "Paul Temple",
      "Gilles Perrouin",
      "Benoît Frénay",
      "Patrick Heymans",
      "Bettina Berendt"
    ],
    "author_ids": [],
    "abstract": "Machine learning is being integrated into a growing number of critical\nsystems with far-reaching impacts on society. Unexpected behaviour and unfair\ndecision processes are coming under increasing scrutiny due to this widespread\nuse and its theoretical considerations. Individuals, as well as organisations,\nnotice, test, and criticize unfair results to hold model designers and\ndeployers accountable. We offer a framework that assists these groups in\nmitigating unfair representations stemming from the training datasets. Our\nframework relies on two inter-operating adversaries to improve fairness. First,\na model is trained with the goal of preventing the guessing of protected\nattributes' values while limiting utility losses. This first step optimizes the\nmodel's parameters for fairness. Second, the framework leverages evasion\nattacks from adversarial machine learning to generate new examples that will be\nmisclassified. These new examples are then used to retrain and improve the\nmodel in the first step. These two steps are iteratively applied until a\nsignificant improvement in fairness is obtained. We evaluated our framework on\nwell-studied datasets in the fairness literature -- including COMPAS -- where\nit can surpass other approaches concerning demographic parity, equality of\nopportunity and also the model's utility. We also illustrate our findings on\nthe subtle difficulties when mitigating unfairness and highlight how our\nframework can assist model designers.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06852v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06725v1",
    "title": "Thompson Sampling for Combinatorial Semi-bandits with Sleeping Arms and Long-Term Fairness Constraints",
    "authors": [
      "Zhiming Huang",
      "Yifan Xu",
      "Bingshan Hu",
      "Qipeng Wang",
      "Jianping Pan"
    ],
    "author_ids": [],
    "abstract": "We study the combinatorial sleeping multi-armed semi-bandit problem with\nlong-term fairness constraints~(CSMAB-F). To address the problem, we adopt\nThompson Sampling~(TS) to maximize the total rewards and use virtual queue\ntechniques to handle the fairness constraints, and design an algorithm called\n\\emph{TS with beta priors and Bernoulli likelihoods for CSMAB-F~(TSCSF-B)}.\nFurther, we prove TSCSF-B can satisfy the fairness constraints, and the\ntime-averaged regret is upper bounded by $\\frac{N}{2\\eta} +\nO\\left(\\frac{\\sqrt{mNT\\ln T}}{T}\\right)$, where $N$ is the total number of\narms, $m$ is the maximum number of arms that can be pulled simultaneously in\neach round~(the cardinality constraint) and $\\eta$ is the parameter trading off\nfairness for rewards. By relaxing the fairness constraints (i.e., let $\\eta\n\\rightarrow \\infty$), the bound boils down to the first problem-independent\nbound of TS algorithms for combinatorial sleeping multi-armed semi-bandit\nproblems. Finally, we perform numerical experiments and use a high-rating movie\nrecommendation application to show the effectiveness and efficiency of the\nproposed algorithm.",
    "published_date": "2020-05-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06725v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06251v1",
    "title": "Mitigating Gender Bias Amplification in Distribution by Posterior Regularization",
    "authors": [
      "Shengyu Jia",
      "Tao Meng",
      "Jieyu Zhao",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Advanced machine learning techniques have boosted the performance of natural\nlanguage processing. Nevertheless, recent studies, e.g., Zhao et al. (2017)\nshow that these techniques inadvertently capture the societal bias hidden in\nthe corpus and further amplify it. However, their analysis is conducted only on\nmodels' top predictions. In this paper, we investigate the gender bias\namplification issue from the distribution perspective and demonstrate that the\nbias is amplified in the view of predicted probability distribution over\nlabels. We further propose a bias mitigation approach based on posterior\nregularization. With little performance loss, our method can almost remove the\nbias amplification in the distribution. Our study sheds the light on\nunderstanding the bias amplification.",
    "published_date": "2020-05-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06251v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.05921v3",
    "title": "Intersectional Bias in Hate Speech and Abusive Language Datasets",
    "authors": [
      "Jae Yeon Kim",
      "Carlos Ortiz",
      "Sarah Nam",
      "Sarah Santiago",
      "Vivek Datta"
    ],
    "author_ids": [],
    "abstract": "Algorithms are widely applied to detect hate speech and abusive language in\nsocial media. We investigated whether the human-annotated data used to train\nthese algorithms are biased. We utilized a publicly available annotated Twitter\ndataset (Founta et al. 2018) and classified the racial, gender, and party\nidentification dimensions of 99,996 tweets. The results showed that African\nAmerican tweets were up to 3.7 times more likely to be labeled as abusive, and\nAfrican American male tweets were up to 77% more likely to be labeled as\nhateful compared to the others. These patterns were statistically significant\nand robust even when party identification was added as a control variable. This\nstudy provides the first systematic evidence on intersectional bias in datasets\nof hate speech and abusive language.",
    "published_date": "2020-05-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.05921v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.05906v1",
    "title": "Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI",
    "authors": [
      "Sandra Wachter",
      "Brent Mittelstadt",
      "Chris Russell"
    ],
    "author_ids": [],
    "abstract": "This article identifies a critical incompatibility between European notions\nof discrimination and existing statistical measures of fairness. First, we\nreview the evidential requirements to bring a claim under EU non-discrimination\nlaw. Due to the disparate nature of algorithmic and human discrimination, the\nEU's current requirements are too contextual, reliant on intuition, and open to\njudicial interpretation to be automated. Second, we show how the legal\nprotection offered by non-discrimination law is challenged when AI, not humans,\ndiscriminate. Humans discriminate due to negative attitudes (e.g. stereotypes,\nprejudice) and unintentional biases (e.g. organisational practices or\ninternalised stereotypes) which can act as a signal to victims that\ndiscrimination has occurred. Finally, we examine how existing work on fairness\nin machine learning lines up with procedures for assessing cases under EU\nnon-discrimination law. We propose \"conditional demographic disparity\" (CDD) as\na standard baseline statistical measurement that aligns with the European Court\nof Justice's \"gold standard.\" Establishing a standard set of statistical\nevidence for automated discrimination cases can help ensure consistent\nprocedures for assessment, but not judicial interpretation, of cases involving\nAI and automated systems. Through this proposal for procedural regularity in\nthe identification and assessment of automated discrimination, we clarify how\nto build considerations of fairness into automated systems as far as possible\nwhile still respecting and enabling the contextual approach to judicial\ninterpretation practiced under EU non-discrimination law.\n  N.B. Abridged abstract",
    "published_date": "2020-05-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.05906v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.05874v4",
    "title": "Fair Resource Allocation in Optical Networks under Tidal Traffic",
    "authors": [
      "Tania Panayiotou",
      "Georgios Ellinas"
    ],
    "author_ids": [],
    "abstract": "We propose an alpha-fair routing and spectrum allocation (RSA) framework for\nreconfigurable elastic optical networks under modeled tidal traffic, that is\nbased on the maximization of the social welfare function parameterized by a\nscalar alpha (the inequality aversion parameter). The objective is to\napproximate an egalitarian spectrum allocation (SA) that maximizes the minimum\npossible SA over all connections contending for the network resources, shifting\nfrom the widely used utilitarian SA that merely maximizes the network\nefficiency. A set of existing metrics are examined (i.e., connection blocking,\nresource utilization, coefficient of variation (CV) of utilities), and a set of\nnew measures are also introduced (i.e., improvement on connection over- (COP)\nand under-provisioning (CUP), CV of unserved traffic), allowing a network\noperator to derive and evaluate in advance a set of alpha-fair RSA solutions\nand select the one that best fits the performance requirements of both the\nindividual connections and the overall network. We show that an egalitarian SA\nbetter utilizes the network resources by significantly improving both COP (up\nto 20%) and CUP (up to 80%), compared to the utilitarian allocation, while\nattaining zero blocking. Importantly, the CVs of utilities and unserved traffic\nindicate that a SA that is fairest with respect to the amount of utilities\nallocated to the connections does not imply that the SA is also fairest with\nrespect to the achievable QoS of the connections, while an egalitarian SA\nbetter approximates a fairest QoS-based SA.",
    "published_date": "2020-05-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI",
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.05874v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.05597v1",
    "title": "Widths of functional classes defined by majorants of generalized moduli of smoothness in the spaces ${\\mathcal S}^p$",
    "authors": [
      "Fahreddin Abdullayev",
      "Anatolii Serdyuk",
      "Andrii Shidlich"
    ],
    "author_ids": [],
    "abstract": "Exact Jackson-type inequalities are obtained in terms of best approximations\nand averaged values of generalized moduli of smoothness in the spaces\n${\\mathcal S}^p$. The values of Kolmogorov, Bernstein, linear, and projective\nwidths in the spaces ${\\mathcal S}^p$ are found for classes of periodic\nfunctions defined by certain conditions on the averaged values of the\ngeneralized moduli of smoothness.",
    "published_date": "2020-05-12T00:00:00",
    "year": 2020,
    "categories": [
      "math.CA",
      "cs.NA",
      "math.NA",
      "41A17, 42A32"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.05597v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.05546v1",
    "title": "The Geometry of Nonlinear Embeddings in Kernel Discriminant Analysis",
    "authors": [
      "Jiae Kim",
      "Yoonkyung Lee",
      "Zhiyu Liang"
    ],
    "author_ids": [],
    "abstract": "Fisher's linear discriminant analysis is a classical method for\nclassification, yet it is limited to capturing linear features only. Kernel\ndiscriminant analysis as an extension is known to successfully alleviate the\nlimitation through a nonlinear feature mapping. We study the geometry of\nnonlinear embeddings in discriminant analysis with polynomial kernels and\nGaussian kernel by identifying the population-level discriminant function that\ndepends on the data distribution and the kernel. In order to obtain the\ndiscriminant function, we solve a generalized eigenvalue problem with\nbetween-class and within-class covariance operators. The polynomial\ndiscriminants are shown to capture the class difference through the population\nmoments explicitly. For approximation of the Gaussian discriminant, we use a\nparticular representation of the Gaussian kernel by utilizing the exponential\ngenerating function for Hermite polynomials. We also show that the Gaussian\ndiscriminant can be approximated using randomized projections of the data. Our\nresults illuminate how the data distribution and the kernel interact in\ndetermination of the nonlinear embedding for discrimination, and provide a\nguideline for choice of the kernel and its parameters.",
    "published_date": "2020-05-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.05546v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.05129v2",
    "title": "SPADE: Sequential-clustering Particle Annihilation via Discrepancy Estimation",
    "authors": [
      "Sihong Shao",
      "Yunfeng Xiong"
    ],
    "author_ids": [],
    "abstract": "For an empirical signed measure $\\mu = \\frac{1}{N} \\left(\\sum_{i=1}^P\n\\delta_{x_i} - \\sum_{i=1}^M \\delta_{y_i}\\right)$, particle annihilation (PA)\nremoves $N_A$ particles from both $\\{x_i\\}_{i=1}^P$ and $\\{y_i\\}_{i=1}^M$\nsimultaneously, yielding another empirical signed measure $\\nu$ such that $\\int\nf d \\nu$ approximates to $\\int f d \\mu$ within an acceptable accuracy for\nsuitable test functions $f$. Such annihilation of particles carrying opposite\nimportance weights has been extensively utilized for alleviating the numerical\nsign problem in particle simulations. In this paper, we propose an algorithm\nfor PA in high-dimensional Euclidean space based on hybrid of clustering and\nmatching, dubbed the Sequential-clustering Particle Annihilation via\nDiscrepancy Estimation (SPADE). It consists of two steps: Adaptive clustering\nof particles via controlling their number-theoretic discrepancies, and\nindependent random matching among positive and negative particles in each\ncluster. Both deterministic error bounds by the Koksma-Hlawka inequality and\nnon-asymptotic random error bounds by concentration inequalities are proved to\nbe affected by two factors. One factor measures the irregularity of point\ndistributions and reflects their discrete nature. The other relies on the\nvariation of test function and is influenced by the continuity. Only the latter\nimplicitly depends on dimensionality $d$, implying that SPADE can be immune to\nthe curse of dimensionality for a wide class of test functions. Numerical\nexperiments up to $d=1080$ validate our theoretical discoveries.",
    "published_date": "2020-05-11T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.ST",
      "physics.comp-ph",
      "stat.TH",
      "62G09, 11K38, 65D40, 62G07, 62D05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.05129v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.04949v2",
    "title": "Designing for Human Rights in AI",
    "authors": [
      "Evgeni Aizenberg",
      "Jeroen van den Hoven"
    ],
    "author_ids": [],
    "abstract": "In the age of big data, companies and governments are increasingly using\nalgorithms to inform hiring decisions, employee management, policing, credit\nscoring, insurance pricing, and many more aspects of our lives. AI systems can\nhelp us make evidence-driven, efficient decisions, but can also confront us\nwith unjustified, discriminatory decisions wrongly assumed to be accurate\nbecause they are made automatically and quantitatively. It is becoming evident\nthat these technological developments are consequential to people's fundamental\nhuman rights. Despite increasing attention to these urgent challenges in recent\nyears, technical solutions to these complex socio-ethical problems are often\ndeveloped without empirical study of societal context and the critical input of\nsocietal stakeholders who are impacted by the technology. On the other hand,\ncalls for more ethically- and socially-aware AI often fail to provide answers\nfor how to proceed beyond stressing the importance of transparency,\nexplainability, and fairness. Bridging these socio-technical gaps and the deep\ndivide between abstract value language and design requirements is essential to\nfacilitate nuanced, context-dependent design choices that will support moral\nand social values. In this paper, we bridge this divide through the framework\nof Design for Values, drawing on methodologies of Value Sensitive Design and\nParticipatory Design to present a roadmap for proactively engaging societal\nstakeholders to translate fundamental human rights into context-dependent\ndesign requirements through a structured, inclusive, and transparent process.",
    "published_date": "2020-05-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04949v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04864v1",
    "title": "The Fairness of Leximin in Allocation of Indivisible Chores",
    "authors": [
      "Xingyu Chen",
      "Zijie Liu"
    ],
    "author_ids": [],
    "abstract": "The leximin solution -- which selects an allocation that maximizes the\nminimum utility, then the second minimum utility, and so forth -- is known to\nprovide EFX (envy-free up to any good) fairness guarantee in some contexts when\nallocating indivisible goods. However, it remains unknown how fair the leximin\nsolution is when used to allocate indivisible chores. In this paper, we\ndemonstrate that the leximin solution can be modified to also provide\ncompelling fairness guarantees for the allocation of indivisible chores. First,\nwe generalize the definition of the leximin solution. Then, we show that the\nleximin solution finds a PROP1 (proportional up to one good) and PO\n(Pareto-optimal) allocation for 3 or 4 agents in the context of chores\nallocation with additive distinct valuations. Additionally, we prove that the\nleximin solution is EFX for combinations of goods and chores for agents with\ngeneral but identical valuations.",
    "published_date": "2020-05-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04864v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.04855v1",
    "title": "Fair Division: The Computer Scientist's Perspective",
    "authors": [
      "Toby Walsh"
    ],
    "author_ids": [],
    "abstract": "I survey recent progress on a classic and challenging problem in social\nchoice: the fair division of indivisible items. I discuss how a computational\nperspective has provided interesting insights into and understanding of how to\ndivide items fairly and efficiently. This has involved bringing to bear tools\nsuch as those used in knowledge representation, computational complexity,\napproximation methods, game theory, online analysis and communication\ncomplexity",
    "published_date": "2020-05-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "91B14",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04855v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04813v1",
    "title": "The Visual Social Distancing Problem",
    "authors": [
      "Marco Cristani",
      "Alessio Del Bue",
      "Vittorio Murino",
      "Francesco Setti",
      "Alessandro Vinciarelli"
    ],
    "author_ids": [],
    "abstract": "One of the main and most effective measures to contain the recent viral\noutbreak is the maintenance of the so-called Social Distancing (SD). To comply\nwith this constraint, workplaces, public institutions, transports and schools\nwill likely adopt restrictions over the minimum inter-personal distance between\npeople. Given this actual scenario, it is crucial to massively measure the\ncompliance to such physical constraint in our life, in order to figure out the\nreasons of the possible breaks of such distance limitations, and understand if\nthis implies a possible threat given the scene context. All of this, complying\nwith privacy policies and making the measurement acceptable. To this end, we\nintroduce the Visual Social Distancing (VSD) problem, defined as the automatic\nestimation of the inter-personal distance from an image, and the\ncharacterization of the related people aggregations. VSD is pivotal for a\nnon-invasive analysis to whether people comply with the SD restriction, and to\nprovide statistics about the level of safety of specific areas whenever this\nconstraint is violated. We then discuss how VSD relates with previous\nliterature in Social Signal Processing and indicate which existing Computer\nVision methods can be used to manage such problem. We conclude with future\nchallenges related to the effectiveness of VSD systems, ethical implications\nand future application scenarios.",
    "published_date": "2020-05-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04813v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04732v2",
    "title": "Towards Robustifying NLI Models Against Lexical Dataset Biases",
    "authors": [
      "Xiang Zhou",
      "Mohit Bansal"
    ],
    "author_ids": [],
    "abstract": "While deep learning models are making fast progress on the task of Natural\nLanguage Inference, recent studies have also shown that these models achieve\nhigh accuracy by exploiting several dataset biases, and without deep\nunderstanding of the language semantics. Using contradiction-word bias and\nword-overlapping bias as our two bias examples, this paper explores both\ndata-level and model-level debiasing methods to robustify models against\nlexical dataset biases. First, we debias the dataset through data augmentation\nand enhancement, but show that the model bias cannot be fully removed via this\nmethod. Next, we also compare two ways of directly debiasing the model without\nknowing what the dataset biases are in advance. The first approach aims to\nremove the label bias at the embedding level. The second approach employs a\nbag-of-words sub-model to capture the features that are likely to exploit the\nbias and prevents the original model from learning these biased features by\nforcing orthogonality between these two sub-models. We performed evaluations on\nnew balanced datasets extracted from the original MNLI dataset as well as the\nNLI stress tests, and show that the orthogonality approach is better at\ndebiasing the model while maintaining competitive overall accuracy. Our code\nand data are available at: https://github.com/owenzx/LexicalDebias-ACL2020",
    "published_date": "2020-05-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04732v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04564v1",
    "title": "Class-Aware Domain Adaptation for Improving Adversarial Robustness",
    "authors": [
      "Xianxu Hou",
      "Jingxin Liu",
      "Bolei Xu",
      "Xiaolong Wang",
      "Bozhi Liu",
      "Guoping Qiu"
    ],
    "author_ids": [],
    "abstract": "Recent works have demonstrated convolutional neural networks are vulnerable\nto adversarial examples, i.e., inputs to machine learning models that an\nattacker has intentionally designed to cause the models to make a mistake. To\nimprove the adversarial robustness of neural networks, adversarial training has\nbeen proposed to train networks by injecting adversarial examples into the\ntraining data. However, adversarial training could overfit to a specific type\nof adversarial attack and also lead to standard accuracy drop on clean images.\nTo this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for\nadversarial defense without directly applying adversarial training.\nSpecifically, we propose to learn domain-invariant features for adversarial\nexamples and clean images via a domain discriminator. Furthermore, we introduce\na class-aware component into the discriminator to increase the discriminative\npower of the network for adversarial examples. We evaluate our newly proposed\napproach using multiple benchmark datasets. The results demonstrate that our\nmethod can significantly improve the state-of-the-art of adversarial robustness\nfor various attacks and maintain high performances on clean images.",
    "published_date": "2020-05-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04564v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04518v1",
    "title": "What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context",
    "authors": [
      "Ramy Baly",
      "Georgi Karadzhov",
      "Jisun An",
      "Haewoon Kwak",
      "Yoan Dinkov",
      "Ahmed Ali",
      "James Glass",
      "Preslav Nakov"
    ],
    "author_ids": [],
    "abstract": "Predicting the political bias and the factuality of reporting of entire news\noutlets are critical elements of media profiling, which is an understudied but\nan increasingly important research direction. The present level of\nproliferation of fake, biased, and propagandistic content online, has made it\nimpossible to fact-check every single suspicious claim, either manually or\nautomatically. Alternatively, we can profile entire news outlets and look for\nthose that are likely to publish fake or biased content. This approach makes it\npossible to detect likely \"fake news\" the moment they are published, by simply\nchecking the reliability of their source. From a practical perspective,\npolitical bias and factuality of reporting have a linguistic aspect but also a\nsocial context. Here, we study the impact of both, namely (i) what was written\n(i.e., what was published by the target medium, and how it describes itself on\nTwitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium\non Facebook, Twitter, and YouTube). We further study (iii) what was written\nabout the target medium on Wikipedia. The evaluation results show that what was\nwritten matters most, and that putting all information sources together yields\nhuge improvements over the current state-of-the-art.",
    "published_date": "2020-05-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04518v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06625v2",
    "title": "Cyberbullying Detection with Fairness Constraints",
    "authors": [
      "Oguzhan Gencoglu"
    ],
    "author_ids": [],
    "abstract": "Cyberbullying is a widespread adverse phenomenon among online social\ninteractions in today's digital society. While numerous computational studies\nfocus on enhancing the cyberbullying detection performance of machine learning\nalgorithms, proposed models tend to carry and reinforce unintended social\nbiases. In this study, we try to answer the research question of \"Can we\nmitigate the unintended bias of cyberbullying detection models by guiding the\nmodel training with fairness constraints?\". For this purpose, we propose a\nmodel training scheme that can employ fairness constraints and validate our\napproach with different datasets. We demonstrate that various types of\nunintended biases can be successfully mitigated without impairing the model\nquality. We believe our work contributes to the pursuit of unbiased,\ntransparent, and ethical machine learning solutions for cyber-social health.",
    "published_date": "2020-05-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06625v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04372v4",
    "title": "Information-Theoretic Generalization Bounds for Meta-Learning and Applications",
    "authors": [
      "Sharu Theresa Jose",
      "Osvaldo Simeone"
    ],
    "author_ids": [],
    "abstract": "Meta-learning, or \"learning to learn\", refers to techniques that infer an\ninductive bias from data corresponding to multiple related tasks with the goal\nof improving the sample efficiency for new, previously unobserved, tasks. A key\nperformance measure for meta-learning is the meta-generalization gap, that is,\nthe difference between the average loss measured on the meta-training data and\non a new, randomly selected task. This paper presents novel\ninformation-theoretic upper bounds on the meta-generalization gap. Two broad\nclasses of meta-learning algorithms are considered that uses either separate\nwithin-task training and test sets, like MAML, or joint within-task training\nand test sets, like Reptile. Extending the existing work for conventional\nlearning, an upper bound on the meta-generalization gap is derived for the\nformer class that depends on the mutual information (MI) between the output of\nthe meta-learning algorithm and its input meta-training data. For the latter,\nthe derived bound includes an additional MI between the output of the per-task\nlearning procedure and corresponding data set to capture within-task\nuncertainty. Tighter bounds are then developed, under given technical\nconditions, for the two classes via novel Individual Task MI (ITMI) bounds.\nApplications of the derived bounds are finally discussed, including a broad\nclass of noisy iterative algorithms for meta-learning.",
    "published_date": "2020-05-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IT",
      "eess.SP",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04372v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04364v1",
    "title": "It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations",
    "authors": [
      "Samson Tan",
      "Shafiq Joty",
      "Min-Yen Kan",
      "Richard Socher"
    ],
    "author_ids": [],
    "abstract": "Training on only perfect Standard English corpora predisposes pre-trained\nneural networks to discriminate against minorities from non-standard linguistic\nbackgrounds (e.g., African American Vernacular English, Colloquial Singapore\nEnglish, etc.). We perturb the inflectional morphology of words to craft\nplausible and semantically similar adversarial examples that expose these\nbiases in popular NLP models, e.g., BERT and Transformer, and show that\nadversarially fine-tuning them for a single epoch significantly improves\nrobustness without sacrificing performance on clean data.",
    "published_date": "2020-05-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04364v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04345v3",
    "title": "An Investigation of Why Overparameterization Exacerbates Spurious Correlations",
    "authors": [
      "Shiori Sagawa",
      "Aditi Raghunathan",
      "Pang Wei Koh",
      "Percy Liang"
    ],
    "author_ids": [],
    "abstract": "We study why overparameterization -- increasing model size well beyond the\npoint of zero training error -- can hurt test error on minority groups despite\nimproving average test error when there are spurious correlations in the data.\nThrough simulations and experiments on two image datasets, we identify two key\nproperties of the training data that drive this behavior: the proportions of\nmajority versus minority groups, and the signal-to-noise ratio of the spurious\ncorrelations. We then analyze a linear setting and theoretically show how the\ninductive bias of models towards \"memorizing\" fewer examples can cause\noverparameterization to hurt. Our analysis leads to a counterintuitive approach\nof subsampling the majority group, which empirically achieves low minority\nerror in the overparameterized regime, even though the standard approach of\nupweighting the minority fails. Overall, our results suggest a tension between\nusing overparameterized models versus using all the training data for achieving\nlow worst-group error.",
    "published_date": "2020-05-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04345v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04343v4",
    "title": "How good is good enough for COVID19 apps? The influence of benefits, accuracy, and privacy on willingness to adopt",
    "authors": [
      "Gabriel Kaptchuk",
      "Daniel G. Goldstein",
      "Eszter Hargittai",
      "Jake Hofman",
      "Elissa M. Redmiles"
    ],
    "author_ids": [],
    "abstract": "A growing number of contact tracing apps are being developed to complement\nmanual contact tracing. A key question is whether users will be willing to\nadopt these contact tracing apps. In this work, we survey over 4,500 Americans\nto evaluate (1) the effect of both accuracy and privacy concerns on reported\nwillingness to install COVID19 contact tracing apps and (2) how different\ngroups of users weight accuracy vs. privacy. Drawing on our findings from these\nfirst two research questions, we (3) quantitatively model how the amount of\npublic health benefit (reduction in infection rate), amount of individual\nbenefit (true-positive detection of exposures to COVID), and degree of privacy\nrisk in a hypothetical contact tracing app may influence American's willingness\nto install. Our work takes a descriptive ethics approach toward offering\nimplications for the development of policy and app designs related to COVID19.",
    "published_date": "2020-05-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04343v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04269v1",
    "title": "Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics",
    "authors": [
      "Arsenii Kuznetsov",
      "Pavel Shvechikov",
      "Alexander Grishin",
      "Dmitry Vetrov"
    ],
    "author_ids": [],
    "abstract": "The overestimation bias is one of the major impediments to accurate\noff-policy learning. This paper investigates a novel way to alleviate the\noverestimation bias in a continuous control setting. Our method---Truncated\nQuantile Critics, TQC,---blends three ideas: distributional representation of a\ncritic, truncation of critics prediction, and ensembling of multiple critics.\nDistributional representation and truncation allow for arbitrary granular\noverestimation control, while ensembling provides additional score\nimprovements. TQC outperforms the current state of the art on all environments\nfrom the continuous control benchmark suite, demonstrating 25% improvement on\nthe most challenging Humanoid environment.",
    "published_date": "2020-05-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04269v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04176v3",
    "title": "In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction",
    "authors": [
      "Caroline Wang",
      "Bin Han",
      "Bhrij Patel",
      "Cynthia Rudin"
    ],
    "author_ids": [],
    "abstract": "Objectives: We study interpretable recidivism prediction using machine\nlearning (ML) models and analyze performance in terms of prediction ability,\nsparsity, and fairness. Unlike previous works, this study trains interpretable\nmodels that output probabilities rather than binary predictions, and uses\nquantitative fairness definitions to assess the models. This study also\nexamines whether models can generalize across geographic locations. Methods: We\ngenerated black-box and interpretable ML models on two different criminal\nrecidivism datasets from Florida and Kentucky. We compared predictive\nperformance and fairness of these models against two methods that are currently\nused in the justice system to predict pretrial recidivism: the Arnold PSA and\nCOMPAS. We evaluated predictive performance of all models on predicting six\ndifferent types of crime over two time spans. Results: Several interpretable ML\nmodels can predict recidivism as well as black-box ML models and are more\naccurate than COMPAS or the Arnold PSA. These models are potentially useful in\npractice. Similar to the Arnold PSA, some of these interpretable models can be\nwritten down as a simple table. Others can be displayed using a set of\nvisualizations. Our geographic analysis indicates that ML models should be\ntrained separately for separate locations and updated over time. We also\npresent a fairness analysis for the interpretable models. Conclusions:\nInterpretable machine learning models can perform just as well as\nnon-interpretable methods and currently-used risk assessment scales, in terms\nof both prediction accuracy and fairness. Machine learning models might be more\naccurate when trained separately for distinct locations and kept up-to-date.",
    "published_date": "2020-05-08T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04176v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.04074v2",
    "title": "Adversarial Graph Embeddings for Fair Influence Maximization over Social Networks",
    "authors": [
      "Moein Khajehnejad",
      "Ahmad Asgharian Rezaei",
      "Mahmoudreza Babaei",
      "Jessica Hoffmann",
      "Mahdi Jalili",
      "Adrian Weller"
    ],
    "author_ids": [],
    "abstract": "Influence maximization is a widely studied topic in network science, where\nthe aim is to reach the maximum possible number of nodes, while only targeting\na small initial set of individuals. It has critical applications in many\nfields, including viral marketing, information propagation, news dissemination,\nand vaccinations. However, the objective does not usually take into account\nwhether the final set of influenced nodes is fair with respect to sensitive\nattributes, such as race or gender. Here we address fair influence\nmaximization, aiming to reach minorities more equitably. We introduce\nAdversarial Graph Embeddings: we co-train an auto-encoder for graph embedding\nand a discriminator to discern sensitive attributes. This leads to embeddings\nwhich are similarly distributed across sensitive attributes. We then find a\ngood initial set by clustering the embeddings. We believe we are the first to\nuse embeddings for the task of fair influence maximization. While there are\ntypically trade-offs between fairness and influence maximization objectives,\nour experiments on synthetic and real-world datasets show that our approach\ndramatically reduces disparity while remaining competitive with\nstate-of-the-art influence maximization methods.",
    "published_date": "2020-05-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04074v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03912v1",
    "title": "An Extensive Study on Cross-Dataset Bias and Evaluation Metrics Interpretation for Machine Learning applied to Gastrointestinal Tract Abnormality Classification",
    "authors": [
      "Vajira Thambawita",
      "Debesh Jha",
      "Hugo Lewi Hammer",
      "Håvard D. Johansen",
      "Dag Johansen",
      "Pål Halvorsen",
      "Michael A. Riegler"
    ],
    "author_ids": [],
    "abstract": "Precise and efficient automated identification of Gastrointestinal (GI) tract\ndiseases can help doctors treat more patients and improve the rate of disease\ndetection and identification. Currently, automatic analysis of diseases in the\nGI tract is a hot topic in both computer science and medical-related journals.\nNevertheless, the evaluation of such an automatic analysis is often incomplete\nor simply wrong. Algorithms are often only tested on small and biased datasets,\nand cross-dataset evaluations are rarely performed. A clear understanding of\nevaluation metrics and machine learning models with cross datasets is crucial\nto bring research in the field to a new quality level. Towards this goal, we\npresent comprehensive evaluations of five distinct machine learning models\nusing Global Features and Deep Neural Networks that can classify 16 different\nkey types of GI tract conditions, including pathological findings, anatomical\nlandmarks, polyp removal conditions, and normal findings from images captured\nby common GI tract examination instruments. In our evaluation, we introduce\nperformance hexagons using six performance metrics such as recall, precision,\nspecificity, accuracy, F1-score, and Matthews Correlation Coefficient to\ndemonstrate how to determine the real capabilities of models rather than\nevaluating them shallowly. Furthermore, we perform cross-dataset evaluations\nusing different datasets for training and testing. With these cross-dataset\nevaluations, we demonstrate the challenge of actually building a generalizable\nmodel that could be used across different hospitals. Our experiments clearly\nshow that more sophisticated performance metrics and evaluation methods need to\nbe applied to get reliable models rather than depending on evaluations of the\nsplits of the same dataset, i.e., the performance metrics should always be\ninterpreted together rather than relying on a single metric.",
    "published_date": "2020-05-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.MM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03912v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03645v5",
    "title": "XEM: An Explainable-by-Design Ensemble Method for Multivariate Time Series Classification",
    "authors": [
      "Kevin Fauvel",
      "Élisa Fromont",
      "Véronique Masson",
      "Philippe Faverdin",
      "Alexandre Termier"
    ],
    "author_ids": [],
    "abstract": "We present XEM, an eXplainable-by-design Ensemble method for Multivariate\ntime series classification. XEM relies on a new hybrid ensemble method that\ncombines an explicit boosting-bagging approach to handle the bias-variance\ntrade-off faced by machine learning models and an implicit divide-and-conquer\napproach to individualize classifier errors on different parts of the training\ndata. Our evaluation shows that XEM outperforms the state-of-the-art MTS\nclassifiers on the public UEA datasets. Furthermore, XEM provides faithful\nexplainability-by-design and manifests robust performance when faced with\nchallenges arising from continuous data collection (different MTS length,\nmissing data and noise).",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03645v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03642v1",
    "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation",
    "authors": [
      "Chaojun Wang",
      "Rico Sennrich"
    ],
    "author_ids": [],
    "abstract": "The standard training algorithm in neural machine translation (NMT) suffers\nfrom exposure bias, and alternative algorithms have been proposed to mitigate\nthis. However, the practical impact of exposure bias is under debate. In this\npaper, we link exposure bias to another well-known problem in NMT, namely the\ntendency to generate hallucinations under domain shift. In experiments on three\ndatasets with multiple test domains, we show that exposure bias is partially to\nblame for hallucinations, and that training with Minimum Risk Training, which\navoids exposure bias, can mitigate this. Our analysis explains why exposure\nbias is more problematic under domain shift, and also links exposure bias to\nthe beam search problem, i.e. performance deterioration with increasing beam\nsize. Our results provide a new justification for methods that reduce exposure\nbias: even if they do not increase performance on in-domain test sets, they can\nincrease model robustness to domain shift.",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03642v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03632v2",
    "title": "Visualisation and knowledge discovery from interpretable models",
    "authors": [
      "Sreejita Ghosh",
      "Peter Tino",
      "Kerstin Bunte"
    ],
    "author_ids": [],
    "abstract": "Increasing number of sectors which affect human lives, are using Machine\nLearning (ML) tools. Hence the need for understanding their working mechanism\nand evaluating their fairness in decision-making, are becoming paramount,\nushering in the era of Explainable AI (XAI). In this contribution we introduced\na few intrinsically interpretable models which are also capable of dealing with\nmissing values, in addition to extracting knowledge from the dataset and about\nthe problem. These models are also capable of visualisation of the classifier\nand decision boundaries: they are the angle based variants of Learning Vector\nQuantization. We have demonstrated the algorithms on a synthetic dataset and a\nreal-world one (heart disease dataset from the UCI repository). The newly\ndeveloped classifiers helped in investigating the complexities of the UCI\ndataset as a multiclass problem. The performance of the developed classifiers\nwere comparable to those reported in literature for this dataset, with\nadditional value of interpretability, when the dataset was treated as a binary\nclass problem.",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03632v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06620v3",
    "title": "IEEE 7010: A New Standard for Assessing the Well-being Implications of Artificial Intelligence",
    "authors": [
      "Daniel S. Schiff",
      "Aladdin Ayesh",
      "Laura Musikanski",
      "John C. Havens"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) enabled products and services are becoming a\nstaple of everyday life. While governments and businesses are eager to enjoy\nthe benefits of AI innovations, the mixed impact of these autonomous and\nintelligent systems on human well-being has become a pressing issue. This\narticle introduces one of the first international standards focused on the\nsocial and ethical implications of AI: The Institute of Electrical and\nElectronics Engineering (IEEE) Standard (Std) 7010-2020 Recommended Practice\nfor Assessing the Impact of Autonomous and Intelligent Systems on Human\nWell-being. Incorporating well-being factors throughout the lifecycle of AI is\nboth challenging and urgent and IEEE 7010 provides key guidance for those who\ndesign, deploy, and procure these technologies. We begin by articulating the\nbenefits of an approach for AI centered around well-being and the measurement\nof well-being data. Next, we provide an overview of IEEE 7010, including its\nkey principles and how the standard relates to approaches and perspectives in\nplace in the AI community. Finally, we indicate where future efforts are\nneeded.",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06620v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03557v2",
    "title": "Non-asymptotic Convergence Analysis of Two Time-scale (Natural) Actor-Critic Algorithms",
    "authors": [
      "Tengyu Xu",
      "Zhe Wang",
      "Yingbin Liang"
    ],
    "author_ids": [],
    "abstract": "As an important type of reinforcement learning algorithms, actor-critic (AC)\nand natural actor-critic (NAC) algorithms are often executed in two ways for\nfinding optimal policies. In the first nested-loop design, actor's one update\nof policy is followed by an entire loop of critic's updates of the value\nfunction, and the finite-sample analysis of such AC and NAC algorithms have\nbeen recently well established. The second two time-scale design, in which\nactor and critic update simultaneously but with different learning rates, has\nmuch fewer tuning parameters than the nested-loop design and is hence\nsubstantially easier to implement. Although two time-scale AC and NAC have been\nshown to converge in the literature, the finite-sample convergence rate has not\nbeen established. In this paper, we provide the first such non-asymptotic\nconvergence rate for two time-scale AC and NAC under Markovian sampling and\nwith actor having general policy class approximation. We show that two\ntime-scale AC requires the overall sample complexity at the order of\n$\\mathcal{O}(\\epsilon^{-2.5}\\log^3(\\epsilon^{-1}))$ to attain an\n$\\epsilon$-accurate stationary point, and two time-scale NAC requires the\noverall sample complexity at the order of\n$\\mathcal{O}(\\epsilon^{-4}\\log^2(\\epsilon^{-1}))$ to attain an\n$\\epsilon$-accurate global optimal point. We develop novel techniques for\nbounding the bias error of the actor due to dynamically changing Markovian\nsampling and for analyzing the convergence rate of the linear critic with\ndynamically changing base functions and transition kernel.",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03557v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03468v2",
    "title": "Indexing Metric Spaces for Exact Similarity Search",
    "authors": [
      "Lu Chen",
      "Yunjun Gao",
      "Xuan Song",
      "Zheng Li",
      "Yifan Zhu",
      "Xiaoye Miao",
      "Christian S. Jensen"
    ],
    "author_ids": [],
    "abstract": "With the continued digitization of societal processes, we are seeing an\nexplosion in available data. This is referred to as big data. In a research\nsetting, three aspects of the data are often viewed as the main sources of\nchallenges when attempting to enable value creation from big data: volume,\nvelocity, and variety. Many studies address volume or velocity, while fewer\nstudies concern the variety. Metric spaces are ideal for addressing variety\nbecause they can accommodate any data as long as it can be equipped with a\ndistance notion that satisfies the triangle inequality. To accelerate search in\nmetric spaces, a collection of indexing techniques for metric data have been\nproposed. However, existing surveys offer limited coverage, and a comprehensive\nempirical study exists has yet to be reported. We offer a comprehensive survey\nof existing metric indexes that support exact similarity search: we summarize\nexisting partitioning, pruning, and validation techniques used by metric\nindexes to support exact similarity search; we provide the time and space\ncomplexity analyses of index construction; and we offer an empirical comparison\nof their query processing performance. Empirical studies are important when\nevaluating metric indexing performance, because performance can depend highly\non the effectiveness of available pruning and validation as well as on the data\ndistribution, which means that complexity analyses often offer limited\ninsights. This article aims at revealing strengths and weaknesses of different\nindexing techniques to offer guidance on selecting an appropriate indexing\ntechnique for a given setting, and to provide directions for future research on\nmetric indexing.",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03468v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.03197v4",
    "title": "Fair Algorithms for Hierarchical Agglomerative Clustering",
    "authors": [
      "Anshuman Chhabra",
      "Prasant Mohapatra"
    ],
    "author_ids": [],
    "abstract": "Hierarchical Agglomerative Clustering (HAC) algorithms are extensively\nutilized in modern data science, and seek to partition the dataset into\nclusters while generating a hierarchical relationship between the data samples.\nHAC algorithms are employed in many applications, such as biology, natural\nlanguage processing, and recommender systems. Thus, it is imperative to ensure\nthat these algorithms are fair -- even if the dataset contains biases against\ncertain protected groups, the cluster outputs generated should not discriminate\nagainst samples from any of these groups. However, recent work in clustering\nfairness has mostly focused on center-based clustering algorithms, such as\nk-median and k-means clustering. In this paper, we propose fair algorithms for\nperforming HAC that enforce fairness constraints 1) irrespective of the\ndistance linkage criteria used, 2) generalize to any natural measures of\nclustering fairness for HAC, 3) work for multiple protected groups, and 4) have\ncompetitive running times to vanilla HAC. Through extensive experiments on\nmultiple real-world UCI datasets, we show that our proposed algorithm finds\nfairer clusterings compared to vanilla HAC as well as other state-of-the-art\nfair clustering approaches.",
    "published_date": "2020-05-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03197v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03086v1",
    "title": "Diagnosing the Environment Bias in Vision-and-Language Navigation",
    "authors": [
      "Yubo Zhang",
      "Hao Tan",
      "Mohit Bansal"
    ],
    "author_ids": [],
    "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow\nnatural-language instructions, explore the given environments, and reach the\ndesired target locations. These step-by-step navigational instructions are\ncrucial when the agent is navigating new environments about which it has no\nprior knowledge. Most recent works that study VLN observe a significant\nperformance drop when tested on unseen environments (i.e., environments not\nused in training), indicating that the neural agent models are highly biased\ntowards training environments. Although this issue is considered as one of the\nmajor challenges in VLN research, it is still under-studied and needs a clearer\nexplanation. In this work, we design novel diagnosis experiments via\nenvironment re-splitting and feature replacement, looking into possible reasons\nfor this environment bias. We observe that neither the language nor the\nunderlying navigational graph, but the low-level visual appearance conveyed by\nResNet features directly affects the agent model and contributes to this\nenvironment bias in results. According to this observation, we explore several\nkinds of semantic representations that contain less low-level visual\ninformation, hence the agent learned with these features could be better\ngeneralized to unseen testing environments. Without modifying the baseline\nagent model and its training method, our explored semantic features\nsignificantly decrease the performance gaps between seen and unseen on multiple\ndatasets (i.e. R2R, R4R, and CVDN) and achieve competitive unseen results to\nprevious state-of-the-art models. Our code and features are available at:\nhttps://github.com/zhangybzbo/EnvBiasVLN",
    "published_date": "2020-05-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03086v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.03474v1",
    "title": "Ensuring Fairness under Prior Probability Shifts",
    "authors": [
      "Arpita Biswas",
      "Suvam Mukherjee"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the problem of fair classification in the presence of\nprior probability shifts, where the training set distribution differs from the\ntest set. This phenomenon can be observed in the yearly records of several\nreal-world datasets, such as recidivism records and medical expenditure\nsurveys. If unaccounted for, such shifts can cause the predictions of a\nclassifier to become unfair towards specific population subgroups. While the\nfairness notion called Proportional Equality (PE) accounts for such shifts, a\nprocedure to ensure PE-fairness was unknown.\n  In this work, we propose a method, called CAPE, which provides a\ncomprehensive solution to the aforementioned problem. CAPE makes novel use of\nprevalence estimation techniques, sampling and an ensemble of classifiers to\nensure fair predictions under prior probability shifts. We introduce a metric,\ncalled prevalence difference (PD), which CAPE attempts to minimize in order to\nensure PE-fairness. We theoretically establish that this metric exhibits\nseveral desirable properties.\n  We evaluate the efficacy of CAPE via a thorough empirical evaluation on\nsynthetic datasets. We also compare the performance of CAPE with several\npopular fair classifiers on real-world datasets like COMPAS (criminal risk\nassessment) and MEPS (medical expenditure panel survey). The results indicate\nthat CAPE ensures PE-fair predictions, while performing well on other\nperformance metrics.",
    "published_date": "2020-05-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03474v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02517v1",
    "title": "Phonetic and Visual Priors for Decipherment of Informal Romanization",
    "authors": [
      "Maria Ryskina",
      "Matthew R. Gormley",
      "Taylor Berg-Kirkpatrick"
    ],
    "author_ids": [],
    "abstract": "Informal romanization is an idiosyncratic process used by humans in informal\ndigital communication to encode non-Latin script languages into Latin character\nsets found on common keyboards. Character substitution choices differ between\nusers but have been shown to be governed by the same main principles observed\nacross a variety of languages---namely, character pairs are often associated\nthrough phonetic or visual similarity. We propose a noisy-channel WFST cascade\nmodel for deciphering the original non-Latin script from observed romanized\ntext in an unsupervised fashion. We train our model directly on romanized data\nfrom two languages: Egyptian Arabic and Russian. We demonstrate that adding\ninductive bias through phonetic and visual priors on character mappings\nsubstantially improves the model's performance on both languages, yielding\nresults much closer to the supervised skyline. Finally, we introduce a new\ndataset of romanized Russian, collected from a Russian social network website\nand partially annotated for our experiments.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02517v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02439v3",
    "title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation",
    "authors": [
      "Brendan Kennedy",
      "Xisen Jin",
      "Aida Mostafazadeh Davani",
      "Morteza Dehghani",
      "Xiang Ren"
    ],
    "author_ids": [],
    "abstract": "Hate speech classifiers trained on imbalanced datasets struggle to determine\nif group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced\nways. Such biases manifest in false positives when these identifiers are\npresent, due to models' inability to learn the contexts which constitute a\nhateful usage of identifiers. We extract SOC post-hoc explanations from\nfine-tuned BERT classifiers to efficiently detect bias towards identity terms.\nThen, we propose a novel regularization technique based on these explanations\nthat encourages models to learn from the context of group identifiers in\naddition to the identifiers themselves. Our approach improved over baselines in\nlimiting false positives on out-of-domain data while maintaining or improving\nin-domain performance. Project page:\nhttps://inklab.usc.edu/contextualize-hate-speech/.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02439v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02269v2",
    "title": "Towards explainable classifiers using the counterfactual approach -- global explanations for discovering bias in data",
    "authors": [
      "Agnieszka Mikołajczyk",
      "Michał Grochowski",
      "Arkadiusz Kwasigroch"
    ],
    "author_ids": [],
    "abstract": "The paper proposes summarized attribution-based post-hoc explanations for the\ndetection and identification of bias in data. A global explanation is proposed,\nand a step-by-step framework on how to detect and test bias is introduced.\nSince removing unwanted bias is often a complicated and tremendous task, it is\nautomatically inserted, instead. Then, the bias is evaluated with the proposed\ncounterfactual approach. The obtained results are validated on a sample skin\nlesion dataset. Using the proposed method, a number of possible bias causing\nartifacts are successfully identified and confirmed in dermoscopy images. In\nparticular, it is confirmed that black frames have a strong influence on\nConvolutional Neural Network's prediction: 22% of them changed the prediction\nfrom benign to malignant.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02269v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02390v2",
    "title": "Mechanism Design and Blockchains",
    "authors": [
      "Akaki Mamageishvili",
      "Jan Christoph Schlegel"
    ],
    "author_ids": [],
    "abstract": "Game theory is often used as a tool to analyze decentralized systems and\ntheir properties, in particular, blockchains. In this note, we take the\nopposite view. We argue that blockchains can and should be used to implement\neconomic mechanisms because they can help to overcome problems that occur if\ntrust in the mechanism designer cannot be assumed. Mechanism design deals with\nthe allocation of resources to agents, often by extracting private information\nfrom them. Some mechanisms are immune to early information disclosure, while\nothers may heavily depend on it. Some mechanisms have to randomize to achieve\nfairness and efficiency. Both issues, information disclosure, and randomness\nrequire trust in the mechanism designer. If there is no trust, mechanisms can\nbe manipulated. We claim that mechanisms that use randomness or sequential\ninformation disclosure are much harder, if not impossible, to audit. Therefore,\ncentralized implementation is often not a good solution. We consider some of\nthe most frequently used mechanisms in practice and identify circumstances\nunder which manipulation is possible. We propose a decentralized implementation\nof such mechanisms, that can be, in practical terms, realized by blockchain\ntechnology. Moreover, we argue in which environments a decentralized\nimplementation of a mechanism brings a significant advantage.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02390v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.02258v3",
    "title": "AGE Challenge: Angle Closure Glaucoma Evaluation in Anterior Segment Optical Coherence Tomography",
    "authors": [
      "Huazhu Fu",
      "Fei Li",
      "Xu Sun",
      "Xingxing Cao",
      "Jingan Liao",
      "Jose Ignacio Orlando",
      "Xing Tao",
      "Yuexiang Li",
      "Shihao Zhang",
      "Mingkui Tan",
      "Chenglang Yuan",
      "Cheng Bian",
      "Ruitao Xie",
      "Jiongcheng Li",
      "Xiaomeng Li",
      "Jing Wang",
      "Le Geng",
      "Panming Li",
      "Huaying Hao",
      "Jiang Liu",
      "Yan Kong",
      "Yongyong Ren",
      "Hrvoje Bogunovic",
      "Xiulan Zhang",
      "Yanwu Xu"
    ],
    "author_ids": [],
    "abstract": "Angle closure glaucoma (ACG) is a more aggressive disease than open-angle\nglaucoma, where the abnormal anatomical structures of the anterior chamber\nangle (ACA) may cause an elevated intraocular pressure and gradually lead to\nglaucomatous optic neuropathy and eventually to visual impairment and\nblindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging\nprovides a fast and contactless way to discriminate angle closure from open\nangle. Although many medical image analysis algorithms have been developed for\nglaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In\nparticular, there is no public AS-OCT dataset available for evaluating the\nexisting methods in a uniform way, which limits progress in the development of\nautomated techniques for angle closure detection and assessment. To address\nthis, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held\nin conjunction with MICCAI 2019. The AGE challenge consisted of two tasks:\nscleral spur localization and angle closure classification. For this challenge,\nwe released a large dataset of 4800 annotated AS-OCT images from 199 patients,\nand also proposed an evaluation framework to benchmark and compare different\nmodels. During the AGE challenge, over 200 teams registered online, and more\nthan 1100 results were submitted for online evaluation. Finally, eight teams\nparticipated in the onsite challenge. In this paper, we summarize these eight\nonsite challenge methods and analyze their corresponding results for the two\ntasks. We further discuss limitations and future directions. In the AGE\nchallenge, the top-performing approach had an average Euclidean Distance of 10\npixels (10um) in scleral spur localization, while in the task of angle closure\nclassification, all the algorithms achieved satisfactory performances, with two\nbest obtaining an accuracy rate of 100%.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02258v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02111v1",
    "title": "Mechanisms of intermediary platforms",
    "authors": [
      "Tobias Kölbel",
      "Daniel Kunz"
    ],
    "author_ids": [],
    "abstract": "In the current digital age of the Internet, with ever-growing networks and\ndata-driven business models, digital platforms and especially marketplaces are\nbecoming increasingly important. These platforms focus primarily on digital\nbusinesses by offering services that bring together consumers and producers.\nDue to added value created for consumers, the profit-driven operators of these\nplatforms Matchmakers are extremely successful and have come to dominate their\nrespective markets. The aim of this article is to understand how Matchmakers\nand coordination networks gain market dominance. The following sections will\ntake a closer look at network and coordination effects as well as intermediary\nplatform mechanisms and entailing disadvantages for users. Considering\nstrategic and business challenges, we suggest a possible solution and strategy\nto avoid dependencies on individual players in the digital economy. We present\na cooperative approach towards a fair and open Economy of Things (EoT) based on\ndecentralized technologies.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02111v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.01980v1",
    "title": "Can gender inequality be created without inter-group discrimination?",
    "authors": [
      "Sylvie Huet1",
      "Floriana Gargiulo",
      "Felicia Pratto"
    ],
    "author_ids": [],
    "abstract": "Understanding human societies requires knowing how they develop gender\nhierarchies which are ubiquitous. We test whether a simple agent-based dynamic\nprocess could create gender inequality. Relying on evidence of gendered status\nconcerns, self-construals, and cognitive habits, our model included a gender\ndifference in how responsive male-like and female-like agents are to others'\nopinions about the level of esteem for someone. We simulate a population who\ninteract in pairs of randomly selected agents to influence each other about\ntheir esteem judgments of self and others. Half the agents are more influenced\nby their relative status rank during the interaction than the others. Without\nprejudice, stereotypes, segregation, or categorization, our model produces\ninter-group inequality of self-esteem and status that is stable, consensual,\nand exhibits characteristics of glass ceiling effects. Outcomes are not\naffected by relative group size. We discuss implications for group orientation\nto dominance and individuals' motivations to exchange.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01980v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.01927v1",
    "title": "StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching",
    "authors": [
      "Rui Liu",
      "Chengxi Yang",
      "Wenxiu Sun",
      "Xiaogang Wang",
      "Hongsheng Li"
    ],
    "author_ids": [],
    "abstract": "Large-scale synthetic datasets are beneficial to stereo matching but usually\nintroduce known domain bias. Although unsupervised image-to-image translation\nnetworks represented by CycleGAN show great potential in dealing with domain\ngap, it is non-trivial to generalize this method to stereo matching due to the\nproblem of pixel distortion and stereo mismatch after translation. In this\npaper, we propose an end-to-end training framework with domain translation and\nstereo matching networks to tackle this challenge. First, joint optimization\nbetween domain translation and stereo matching networks in our end-to-end\nframework makes the former facilitate the latter one to the maximum extent.\nSecond, this framework introduces two novel losses, i.e., bidirectional\nmulti-scale feature re-projection loss and correlation consistency loss, to\nhelp translate all synthetic stereo images into realistic ones as well as\nmaintain epipolar constraints. The effective combination of above two\ncontributions leads to impressive stereo-consistent translation and disparity\nestimation accuracy. In addition, a mode seeking regularization term is added\nto endow the synthetic-to-real translation results with higher fine-grained\ndiversity. Extensive experiments demonstrate the effectiveness of the proposed\nframework on bridging the synthetic-to-real domain gap on stereo matching.",
    "published_date": "2020-05-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01927v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06604v1",
    "title": "Essential requirements for establishing and operating data trusts: practical guidance based on a working meeting of fifteen Canadian organizations and initiatives",
    "authors": [
      "P. Alison Paprica",
      "Eric Sutherland",
      "Andrea Smith",
      "Michael Brudno",
      "Rosario G. Cartagena",
      "Monique Crichlow",
      "Brian K Courtney",
      "Chris Loken",
      "Kimberlyn M. McGrail",
      "Alex Ryan",
      "Michael J Schull",
      "Adrian Thorogood",
      "Carl Virtanen",
      "Kathleen Yang"
    ],
    "author_ids": [],
    "abstract": "Introduction: Increasingly, the label data trust is being applied to\nrepeatable mechanisms or approaches to sharing data in a timely, fair, safe and\nequitable way. However, there is a gap in terms of practical guidance about how\nto establish and operate a data trust.\n  Aim and Approach: In December 2019, the Canadian Institute for Health\nInformation and the Vector Institute for Artificial Intelligence convened a\nworking meeting of 19 people representing 15 Canadian organizations/initiatives\ninvolved in data sharing, most of which focus on public sector health data. The\nobjective was to identify essential requirements for the establishment and\noperation of data trusts. Preliminary findings were presented during the\nmeeting then refined as participants and co-authors identified relevant\nliterature and contributed to this manuscript.\n  Results: Twelve (12) minimum specification requirements (min specs) for data\ntrusts were identified. The foundational min spec is that data trusts must meet\nall legal requirements, including legal authority to collect, hold or share\ndata. In addition, there was agreement that data trusts must have (i) an\naccountable governing body which ensures the data trust advances its stated\npurpose and is transparent, (ii) comprehensive data management including\nresponsible parties and clear processes for the collection, storage, access,\ndisclosure and use of data, (iii) training and accountability requirements for\nall data users and (iv) ongoing public and stakeholder engagement.\n  Conclusion / Implications: Based on a review of the literature and advice\nfrom participants from 15 Canadian organizations/initiatives, practical\nguidance in the form of twelve min specs for data trusts were agreed on. Public\nengagement and continued exchange of insights and experience is recommended on\nthis evolving topic.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06604v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.01800v1",
    "title": "Mind the Gap: On Bridging the Semantic Gap between Machine Learning and Information Security",
    "authors": [
      "Michael R. Smith",
      "Nicholas T. Johnson",
      "Joe B. Ingram",
      "Armida J. Carbajal",
      "Ramyaa Ramyaa",
      "Evelyn Domschot",
      "Christopher C. Lamb",
      "Stephen J. Verzi",
      "W. Philip Kegelmeyer"
    ],
    "author_ids": [],
    "abstract": "Despite the potential of Machine learning (ML) to learn the behavior of\nmalware, detect novel malware samples, and significantly improve information\nsecurity (InfoSec) we see few, if any, high-impact ML techniques in deployed\nsystems, notwithstanding multiple reported successes in open literature. We\nhypothesize that the failure of ML in making high-impacts in InfoSec are rooted\nin a disconnect between the two communities as evidenced by a semantic gap---a\ndifference in how executables are described (e.g. the data and features\nextracted from the data). Specifically, current datasets and representations\nused by ML are not suitable for learning the behaviors of an executable and\ndiffer significantly from those used by the InfoSec community. In this paper,\nwe survey existing datasets used for classifying malware by ML algorithms and\nthe features that are extracted from the data. We observe that: 1) the current\nset of extracted features are primarily syntactic, not behavioral, 2) datasets\ngenerally contain extreme exemplars producing a dataset in which it is easy to\ndiscriminate classes, and 3) the datasets provide significantly different\nrepresentations of the data encountered in real-world systems. For ML to make\nmore of an impact in the InfoSec community requires a change in the data\n(including the features and labels) that is used to bridge the current semantic\ngap. As a first step in enabling more behavioral analyses, we label existing\nmalware datasets with behavioral features using open-source threat reports\nassociated with malware families. This behavioral labeling alters the analysis\nfrom identifying intent (e.g. good vs bad) or malware family membership to an\nanalysis of which behaviors are exhibited by an executable. We offer the\nannotations with the hope of inspiring future improvements in the data that\nwill further bridge the semantic gap between the ML and InfoSec communities.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01800v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.01757v2",
    "title": "Sample Complexity of Uniform Convergence for Multicalibration",
    "authors": [
      "Eliran Shabat",
      "Lee Cohen",
      "Yishay Mansour"
    ],
    "author_ids": [],
    "abstract": "There is a growing interest in societal concerns in machine learning systems,\nespecially in fairness. Multicalibration gives a comprehensive methodology to\naddress group fairness. In this work, we address the multicalibration error and\ndecouple it from the prediction error. The importance of decoupling the\nfairness metric (multicalibration) and the accuracy (prediction error) is due\nto the inherent trade-off between the two, and the societal decision regarding\nthe \"right tradeoff\" (as imposed many times by regulators). Our work gives\nsample complexity bounds for uniform convergence guarantees of multicalibration\nerror, which implies that regardless of the accuracy, we can guarantee that the\nempirical and (true) multicalibration errors are close. We emphasize that our\nresults: (1) are more general than previous bounds, as they apply to both\nagnostic and realizable settings, and do not rely on a specific type of\nalgorithm (such as deferentially private), (2) improve over previous\nmulticalibration sample complexity bounds and (3) implies uniform convergence\nguarantees for the classical calibration error.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML",
      "68Q32",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01757v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.01703v2",
    "title": "Transforming and Projecting Images into Class-conditional Generative Networks",
    "authors": [
      "Minyoung Huh",
      "Richard Zhang",
      "Jun-Yan Zhu",
      "Sylvain Paris",
      "Aaron Hertzmann"
    ],
    "author_ids": [],
    "abstract": "We present a method for projecting an input image into the space of a\nclass-conditional generative neural network. We propose a method that optimizes\nfor transformation to counteract the model biases in generative neural\nnetworks. Specifically, we demonstrate that one can solve for image\ntranslation, scale, and global color transformation, during the projection\noptimization to address the object-center bias and color bias of a Generative\nAdversarial Network. This projection process poses a difficult optimization\nproblem, and purely gradient-based optimizations fail to find good solutions.\nWe describe a hybrid optimization strategy that finds good projections by\nestimating transformations and class parameters. We show the effectiveness of\nour method on real images and further demonstrate how the corresponding\nprojections lead to better editability of these images.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01703v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.01683v2",
    "title": "Group Equivariant Generative Adversarial Networks",
    "authors": [
      "Neel Dey",
      "Antong Chen",
      "Soheil Ghafurian"
    ],
    "author_ids": [],
    "abstract": "Recent improvements in generative adversarial visual synthesis incorporate\nreal and fake image transformation in a self-supervised setting, leading to\nincreased stability and perceptual fidelity. However, these approaches\ntypically involve image augmentations via additional regularizers in the GAN\nobjective and thus spend valuable network capacity towards approximating\ntransformation equivariance instead of their desired task. In this work, we\nexplicitly incorporate inductive symmetry priors into the network architectures\nvia group-equivariant convolutional networks. Group-convolutions have higher\nexpressive power with fewer samples and lead to better gradient feedback\nbetween generator and discriminator. We show that group-equivariance integrates\nseamlessly with recent techniques for GAN training across regularizers,\narchitectures, and loss functions. We demonstrate the utility of our methods\nfor conditional synthesis by improving generation in the limited data regime\nacross symmetric imaging datasets and even find benefits for natural images\nwith preferred orientation.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01683v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.01677v1",
    "title": "Fast and Robust Unsupervised Contextual Biasing for Speech Recognition",
    "authors": [
      "Young Mo Kang",
      "Yingbo Zhou"
    ],
    "author_ids": [],
    "abstract": "Automatic speech recognition (ASR) system is becoming a ubiquitous\ntechnology. Although its accuracy is closing the gap with that of human level\nunder certain settings, one area that can further improve is to incorporate\nuser-specific information or context to bias its prediction. A common framework\nis to dynamically construct a small language model from the provided contextual\nmini corpus and interpolate its score with the main language model during the\ndecoding process.\n  Here we propose an alternative approach that does not entail explicit\ncontextual language model. Instead, we derive the bias score for every word in\nthe system vocabulary from the training corpus. The method is unique in that 1)\nit does not require meta-data or class-label annotation for the context or the\ntraining corpus. 2) The bias score is proportional to the word's\nlog-probability, thus not only would it bias the provided context, but also\nrobust against irrelevant context (e.g. user mis-specified or in case where it\nis hard to quantify a tight scope). 3) The bias score for the entire vocabulary\nis pre-determined during the training stage, thereby eliminating\ncomputationally expensive language model construction during inference.\n  We show significant improvement in recognition accuracy when the relevant\ncontext is available. Additionally, we also demonstrate that the proposed\nmethod exhibits high tolerance to false-triggering errors in the presence of\nirrelevant context.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.01536v1",
    "title": "Flow-Partitionable Signed Graphs",
    "authors": [
      "Jan-Hendrik Lange"
    ],
    "author_ids": [],
    "abstract": "The NP-hard problem of correlation clustering is to partition a signed graph\nsuch that the number of conflicts between the partition and the signature of\nthe graph is minimized. This paper studies graph signatures that allow the\noptimal partition to be found efficiently. We define the class of\nflow-partitionable signed graphs, which have the property that the standard\nlinear programming relaxation based on so-called cycle inequalities is tight.\nIn other words, flow-partitionable signed graphs satisfy an exact\nmax-multiflow-min-multicut relation in the associated instances of minimum\nmulticut. In this work we propose to characterize flow-partitionable signed\ngraphs in terms of forbidden minors. Our initial results include two infinite\nclasses of forbidden minors, which are sufficient if the positive subgraph is a\ncircuit or a tree. For the general case we present another forbidden minor and\npoint out a connection to open problems in the theory of ideal clutters.",
    "published_date": "2020-05-04T00:00:00",
    "year": 2020,
    "categories": [
      "math.CO",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.01536v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.00965v1",
    "title": "Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation",
    "authors": [
      "Tianlu Wang",
      "Xi Victoria Lin",
      "Nazneen Fatema Rajani",
      "Bryan McCann",
      "Vicente Ordonez",
      "Caiming Xiong"
    ],
    "author_ids": [],
    "abstract": "Word embeddings derived from human-generated corpora inherit strong gender\nbias which can be further amplified by downstream models. Some commonly adopted\ndebiasing approaches, including the seminal Hard Debias algorithm, apply\npost-processing procedures that project pre-trained word embeddings into a\nsubspace orthogonal to an inferred gender subspace. We discover that\nsemantic-agnostic corpus regularities such as word frequency captured by the\nword embeddings negatively impact the performance of these algorithms. We\npropose a simple but effective technique, Double Hard Debias, which purifies\nthe word embeddings against such corpus regularities prior to inferring and\nremoving the gender subspace. Experiments on three bias mitigation benchmarks\nshow that our approach preserves the distributional semantics of the\npre-trained word embeddings while reducing gender bias to a significantly\nlarger degree than prior approaches.",
    "published_date": "2020-05-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00965v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00962v2",
    "title": "Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations",
    "authors": [
      "Saif M. Mohammad"
    ],
    "author_ids": [],
    "abstract": "Disparities in authorship and citations across gender can have substantial\nadverse consequences not just on the disadvantaged genders, but also on the\nfield of study as a whole. Measuring gender gaps is a crucial step towards\naddressing them. In this work, we examine female first author percentages and\nthe citations to their papers in Natural Language Processing (1965 to 2019). We\ndetermine aggregate-level statistics using an existing manually curated\nauthor--gender list as well as first names strongly associated with a gender.\nWe find that only about 29% of first authors are female and only about 25% of\nlast authors are female. Notably, this percentage has not improved since the\nmid 2000s. We also show that, on average, female first authors are cited less\nthan male first authors, even when controlling for experience and area of\nresearch. Finally, we discuss the ethical considerations involved in automatic\ndemographic analysis.",
    "published_date": "2020-05-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00962v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00955v1",
    "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    "authors": [
      "Tal Linzen"
    ],
    "author_ids": [],
    "abstract": "This position paper describes and critiques the Pretraining-Agnostic\nIdentically Distributed (PAID) evaluation paradigm, which has become a central\ntool for measuring progress in natural language understanding. This paradigm\nconsists of three stages: (1) pre-training of a word prediction model on a\ncorpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set\nrepresenting a classification task; (3) evaluation on a test set drawn from the\nsame distribution as that training set. This paradigm favors simple, low-bias\narchitectures, which, first, can be scaled to process vast amounts of data, and\nsecond, can capture the fine-grained statistical properties of a particular\ndata set, regardless of whether those properties are likely to generalize to\nexamples of the task outside the data set. This contrasts with humans, who\nlearn language from several orders of magnitude less data than the systems\nfavored by this evaluation paradigm, and generalize to new tasks in a\nconsistent way. We advocate for supplementing or replacing PAID with paradigms\nthat reward architectures that generalize as quickly and robustly as humans.",
    "published_date": "2020-05-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00955v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.06605v2",
    "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship Analysis",
    "authors": [
      "Oren Halvani",
      "Lukas Graner"
    ],
    "author_ids": [],
    "abstract": "Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.",
    "published_date": "2020-05-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.DL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.06605v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00808v3",
    "title": "Dimensions of Diversity in Human Perceptions of Algorithmic Fairness",
    "authors": [
      "Nina Grgić-Hlača",
      "Gabriel Lima",
      "Adrian Weller",
      "Elissa M. Redmiles"
    ],
    "author_ids": [],
    "abstract": "A growing number of oversight boards and regulatory bodies seek to monitor\nand govern algorithms that make decisions about people's lives. Prior work has\nexplored how people believe algorithmic decisions should be made, but there is\nlittle understanding of how individual factors like sociodemographics or direct\nexperience with a decision-making scenario may affect their ethical views. We\ntake a step toward filling this gap by exploring how people's perceptions of\none aspect of procedural algorithmic fairness (the fairness of using particular\nfeatures in an algorithmic decision) relate to their (i) demographics (age,\neducation, gender, race, political views) and (ii) personal experiences with\nthe algorithmic decision-making scenario. We find that political views and\npersonal experience with the algorithmic decision context significantly\ninfluence perceptions about the fairness of using different features for bail\ndecision-making. Drawing on our results, we discuss the implications for\nstakeholder engagement and algorithmic oversight including the need to consider\nmultiple dimensions of diversity in composing oversight and regulatory bodies.",
    "published_date": "2020-05-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00808v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02183v1",
    "title": "Comparing SNNs and RNNs on Neuromorphic Vision Datasets: Similarities and Differences",
    "authors": [
      "Weihua He",
      "YuJie Wu",
      "Lei Deng",
      "Guoqi Li",
      "Haoyu Wang",
      "Yang Tian",
      "Wei Ding",
      "Wenhui Wang",
      "Yuan Xie"
    ],
    "author_ids": [],
    "abstract": "Neuromorphic data, recording frameless spike events, have attracted\nconsiderable attention for the spatiotemporal information components and the\nevent-driven processing fashion. Spiking neural networks (SNNs) represent a\nfamily of event-driven models with spatiotemporal dynamics for neuromorphic\ncomputing, which are widely benchmarked on neuromorphic data. Interestingly,\nresearchers in the machine learning community can argue that recurrent\n(artificial) neural networks (RNNs) also have the capability to extract\nspatiotemporal features although they are not event-driven. Thus, the question\nof \"what will happen if we benchmark these two kinds of models together on\nneuromorphic data\" comes out but remains unclear. In this work, we make a\nsystematic study to compare SNNs and RNNs on neuromorphic data, taking the\nvision datasets as a case study. First, we identify the similarities and\ndifferences between SNNs and RNNs (including the vanilla RNNs and LSTM) from\nthe modeling and learning perspectives. To improve comparability and fairness,\nwe unify the supervised learning algorithm based on backpropagation through\ntime (BPTT), the loss function exploiting the outputs at all timesteps, the\nnetwork structure with stacked fully-connected or convolutional layers, and the\nhyper-parameters during training. Especially, given the mainstream loss\nfunction used in RNNs, we modify it inspired by the rate coding scheme to\napproach that of SNNs. Furthermore, we tune the temporal resolution of datasets\nto test model robustness and generalization. At last, a series of contrast\nexperiments are conducted on two types of neuromorphic datasets: DVS-converted\n(N-MNIST) and DVS-captured (DVS Gesture).",
    "published_date": "2020-05-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.NE",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02183v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00731v1",
    "title": "Sentiment Paradoxes in Social Networks: Why Your Friends Are More Positive Than You?",
    "authors": [
      "Xinyi Zhou",
      "Shengmin Jin",
      "Reza Zafarani"
    ],
    "author_ids": [],
    "abstract": "Most people consider their friends to be more positive than themselves,\nexhibiting a Sentiment Paradox. Psychology research attributes this paradox to\nhuman cognition bias. With the goal to understand this phenomenon, we study\nsentiment paradoxes in social networks. Our work shows that social connections\n(friends, followees, or followers) of users are indeed (not just illusively)\nmore positive than the users themselves. This is mostly due to positive users\nhaving more friends. We identify five sentiment paradoxes at different network\nlevels ranging from triads to large-scale communities. Empirical and\ntheoretical evidence are provided to validate the existence of such sentiment\nparadoxes. By investigating the relationships between the sentiment paradox and\nother well-developed network paradoxes, i.e., friendship paradox and activity\nparadox, we find that user sentiments are positively correlated to their number\nof friends but rarely to their social activity. Finally, we demonstrate how\nsentiment paradoxes can be used to predict user sentiments.",
    "published_date": "2020-05-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00731v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.00699v1",
    "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
    "authors": [
      "Jieyu Zhao",
      "Subhabrata Mukherjee",
      "Saghar Hosseini",
      "Kai-Wei Chang",
      "Ahmed Hassan Awadallah"
    ],
    "author_ids": [],
    "abstract": "Multilingual representations embed words from many languages into a single\nsemantic space such that words with similar meanings are close to each other\nregardless of the language. These embeddings have been widely used in various\nsettings, such as cross-lingual transfer, where a natural language processing\n(NLP) model trained on one language is deployed to another language. While the\ncross-lingual transfer techniques are powerful, they carry gender bias from the\nsource to target languages. In this paper, we study gender bias in multilingual\nembeddings and how it affects transfer learning for NLP applications. We create\na multilingual dataset for bias analysis and propose several ways for\nquantifying bias in multilingual representations from both the intrinsic and\nextrinsic perspectives. Experimental results show that the magnitude of bias in\nthe multilingual representations changes differently when we align the\nembeddings to different target spaces and that the alignment direction can also\nhave an influence on the bias in transfer learning. We further provide\nrecommendations for using the multilingual word representations for downstream\ntasks.",
    "published_date": "2020-05-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00699v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00649v1",
    "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates",
    "authors": [
      "Katherine A. Keith",
      "David Jensen",
      "Brendan O'Connor"
    ],
    "author_ids": [],
    "abstract": "Many applications of computational social science aim to infer causal\nconclusions from non-experimental data. Such observational data often contains\nconfounders, variables that influence both potential causes and potential\neffects. Unmeasured or latent confounders can bias causal estimates, and this\nhas motivated interest in measuring potential confounders from observed text.\nFor example, an individual's entire history of social media posts or the\ncontent of a news article could provide a rich measurement of multiple\nconfounders. Yet, methods and applications for this problem are scattered\nacross different communities and evaluation practices are inconsistent. This\nreview is the first to gather and categorize these examples and provide a guide\nto data-processing and evaluation decisions. Despite increased attention on\nadjusting for confounding using text, there are still many open problems, which\nwe highlight in this paper.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00614v1",
    "title": "Multi-Dimensional Gender Bias Classification",
    "authors": [
      "Emily Dinan",
      "Angela Fan",
      "Ledell Wu",
      "Jason Weston",
      "Douwe Kiela",
      "Adina Williams"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are trained to find patterns in data. NLP models can\ninadvertently learn socially undesirable patterns when training on gender\nbiased text. In this work, we propose a general framework that decomposes\ngender bias in text along several pragmatic and semantic dimensions: bias from\nthe gender of the person being spoken about, bias from the gender of the person\nbeing spoken to, and bias from the gender of the speaker. Using this\nfine-grained framework, we automatically annotate eight large scale datasets\nwith gender information. In addition, we collect a novel, crowdsourced\nevaluation benchmark of utterance-level gender rewrites. Distinguishing between\ngender bias along multiple dimensions is important, as it enables us to train\nfiner-grained gender bias classifiers. We show our classifiers prove valuable\nfor a variety of important applications, such as controlling for gender bias in\ngenerative models, detecting gender bias in arbitrary text, and shed light on\noffensive language in terms of genderedness.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00614v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00568v2",
    "title": "Adversarial domain adaptation to reduce sample bias of a high energy physics classifier",
    "authors": [
      "Jose M. Clavijo",
      "Paul Glaysher",
      "Judith M. Katzy",
      "Jenia Jitsev"
    ],
    "author_ids": [],
    "abstract": "We apply adversarial domain adaptation in unsupervised setting to reduce\nsample bias in a supervised high energy physics events classifier training. We\nmake use of a neural network containing event and domain classifier with a\ngradient reversal layer to simultaneously enable signal versus background\nevents classification on the one hand, while on the other hand minimising the\ndifference in response of the network to background samples originating from\ndifferent MC models via adversarial domain classification loss. We show the\nsuccessful bias removal on the example of simulated events at the LHC with\n$t\\bar{t}H$ signal versus $t\\bar{t}b\\bar{b}$ background classification and\ndiscuss implications and limitations of the method",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "hep-ex",
      "hep-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00568v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00504v1",
    "title": "Uniform Welfare Guarantees Under Identical Subadditive Valuations",
    "authors": [
      "Siddharth Barman",
      "Ranjani G. Sundaram"
    ],
    "author_ids": [],
    "abstract": "We study the problem of allocating indivisible goods among agents that have\nan identical subadditive valuation over the goods. The extent of fairness and\nefficiency of allocations is measured by the generalized means of the values\nthat the allocations generate among the agents. Parameterized by an exponent\nterm $p$, generalized-mean welfares encompass multiple well-studied objectives,\nsuch as social welfare, Nash social welfare, and egalitarian welfare.\n  We establish that, under identical subadditive valuations and in the demand\noracle model, one can efficiently find a single allocation that approximates\nthe optimal generalized-mean welfare---to within a factor of $40$---uniformly\nfor all $p \\in (-\\infty, 1]$. Hence, by way of a constant-factor approximation\nalgorithm, we obtain novel results for maximizing Nash social welfare and\negalitarian welfare for identical subadditive valuations.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00504v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.00372v3",
    "title": "Do Neural Ranking Models Intensify Gender Bias?",
    "authors": [
      "Navid Rekabsaz",
      "Markus Schedl"
    ],
    "author_ids": [],
    "abstract": "Concerns regarding the footprint of societal biases in information retrieval\n(IR) systems have been raised in several previous studies. In this work, we\nexamine various recent IR models from the perspective of the degree of gender\nbias in their retrieval results. To this end, we first provide a bias\nmeasurement framework which includes two metrics to quantify the degree of the\nunbalanced presence of gender-related concepts in a given IR model's ranking\nlist. To examine IR models by means of the framework, we create a dataset of\nnon-gendered queries, selected by human annotators. Applying these queries to\nthe MS MARCO Passage retrieval collection, we then measure the gender bias of a\nBM25 model and several recent neural ranking models. The results show that\nwhile all models are strongly biased toward male, the neural models, and in\nparticular the ones based on contextualized embedding models, significantly\nintensify gender bias. Our experiments also show an overall increase in the\ngender bias of neural models when they exploit transfer learning, namely when\nthey use (already biased) pre-trained embeddings.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00372v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00306v2",
    "title": "PCA-SRGAN: Incremental Orthogonal Projection Discrimination for Face Super-resolution",
    "authors": [
      "Hao Dou",
      "Chen Chen",
      "Xiyuan Hu",
      "Zuxing Xuan",
      "Zhisen Hu",
      "Silong Peng"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GAN) have been employed for face super\nresolution but they bring distorted facial details easily and still have\nweakness on recovering realistic texture. To further improve the performance of\nGAN based models on super-resolving face images, we propose PCA-SRGAN which\npays attention to the cumulative discrimination in the orthogonal projection\nspace spanned by PCA projection matrix of face data. By feeding the principal\ncomponent projections ranging from structure to details into the discriminator,\nthe discrimination difficulty will be greatly alleviated and the generator can\nbe enhanced to reconstruct clearer contour and finer texture, helpful to\nachieve the high perception and low distortion eventually. This incremental\northogonal projection discrimination has ensured a precise optimization\nprocedure from coarse to fine and avoids the dependence on the perceptual\nregularization. We conduct experiments on CelebA and FFHQ face datasets. The\nqualitative visual effect and quantitative evaluation have demonstrated the\noverwhelming performance of our model over related works.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00306v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.00284v1",
    "title": "Strangers in the Room: Unpacking Perceptions of 'Smartness' and Related Ethical Concerns in the Home",
    "authors": [
      "William Seymour",
      "Reuben Binns",
      "Petr Slovak",
      "Max Van Kleek",
      "Nigel Shadbolt"
    ],
    "author_ids": [],
    "abstract": "The increasingly widespread use of 'smart' devices has raised multifarious\nethical concerns regarding their use in domestic spaces. Previous work\nexamining such ethical dimensions has typically either involved empirical\nstudies of concerns raised by specific devices and use contexts, or\nalternatively expounded on abstract concepts like autonomy, privacy or trust in\nrelation to 'smart homes' in general. This paper attempts to bridge these\napproaches by asking what features of smart devices users consider as rendering\nthem 'smart' and how these relate to ethical concerns. Through a multimethod\ninvestigation including surveys with smart device users (n=120) and\nsemi-structured interviews (n=15), we identify and describe eight types of\nsmartness and explore how they engender a variety of ethical concerns including\nprivacy, autonomy, and disruption of the social order. We argue that this\nmiddle ground, between concerns arising from particular devices and more\nabstract ethical concepts, can better anticipate potential ethical concerns\nregarding smart devices.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00284v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2005.00268v2",
    "title": "Towards Controllable Biases in Language Generation",
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Premkumar Natarajan",
      "Nanyun Peng"
    ],
    "author_ids": [],
    "abstract": "We present a general approach towards controllable societal biases in natural\nlanguage generation (NLG). Building upon the idea of adversarial triggers, we\ndevelop a method to induce societal biases in generated text when input prompts\ncontain mentions of specific demographic groups. We then analyze two scenarios:\n1) inducing negative biases for one demographic and positive biases for another\ndemographic, and 2) equalizing biases between demographics. The former scenario\nenables us to detect the types of biases present in the model. Specifically, we\nshow the effectiveness of our approach at facilitating bias analysis by finding\ntopics that correspond to demographic inequalities in generated text and\ncomparing the relative effectiveness of inducing biases for different\ndemographics. The second scenario is useful for mitigating biases in downstream\napplications such as dialogue generation. In our experiments, the mitigation\ntechnique proves to be effective at equalizing the amount of biases across\ndemographics while simultaneously generating less negatively biased text\noverall.",
    "published_date": "2020-05-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00268v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14936v2",
    "title": "Generative Adversarial Networks in Digital Pathology: A Survey on Trends and Future Potential",
    "authors": [
      "Maximilian Ernst Tschuchnig",
      "Gertie Janneke Oostingh",
      "Michael Gadermayr"
    ],
    "author_ids": [],
    "abstract": "Image analysis in the field of digital pathology has recently gained\nincreased popularity. The use of high-quality whole slide scanners enables the\nfast acquisition of large amounts of image data, showing extensive context and\nmicroscopic detail at the same time. Simultaneously, novel machine learning\nalgorithms have boosted the performance of image analysis approaches. In this\npaper, we focus on a particularly powerful class of architectures, called\nGenerative Adversarial Networks (GANs), applied to histological image data.\nBesides improving performance, GANs also enable application scenarios in this\nfield, which were previously intractable. However, GANs could exhibit a\npotential for introducing bias. Hereby, we summarize the recent\nstate-of-the-art developments in a generalizing notation, present the main\napplications of GANs and give an outlook of some chosen promising approaches\nand their possible future applications. In addition, we identify currently\nunavailable methods with potential for future applications.",
    "published_date": "2020-04-30T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14936v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14879v1",
    "title": "Effectiveness of dismantling strategies on moderated vs. unmoderated online social platforms",
    "authors": [
      "Oriol Artime",
      "Valeria d'Andrea",
      "Riccardo Gallotti",
      "Pier Luigi Sacco",
      "Manlio De Domenico"
    ],
    "author_ids": [],
    "abstract": "Online social networks are the perfect test bed to better understand\nlarge-scale human behavior in interacting contexts. Although they are broadly\nused and studied, little is known about how their terms of service and posting\nrules affect the way users interact and information spreads. Acknowledging the\nrelation between network connectivity and functionality, we compare the\nrobustness of two different online social platforms, Twitter and Gab, with\nrespect to dismantling strategies based on the recursive censor of users\ncharacterized by social prominence (degree) or intensity of inflammatory\ncontent (sentiment). We find that the moderated (Twitter) vs unmoderated (Gab)\ncharacter of the network is not a discriminating factor for intervention\neffectiveness. We find, however, that more complex strategies based upon the\ncombination of topological and content features may be effective for network\ndismantling. Our results provide useful indications to design better strategies\nfor countervailing the production and dissemination of anti-social content in\nonline social platforms.",
    "published_date": "2020-04-30T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14879v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.14602v4",
    "title": "Look at the First Sentence: Position Bias in Question Answering",
    "authors": [
      "Miyoung Ko",
      "Jinhyuk Lee",
      "Hyunjae Kim",
      "Gangwoo Kim",
      "Jaewoo Kang"
    ],
    "author_ids": [],
    "abstract": "Many extractive question answering models are trained to predict start and\nend positions of answers. The choice of predicting answers as positions is\nmainly due to its simplicity and effectiveness. In this study, we hypothesize\nthat when the distribution of the answer positions is highly skewed in the\ntraining set (e.g., answers lie only in the k-th sentence of each passage), QA\nmodels predicting answers as positions can learn spurious positional cues and\nfail to give answers in different positions. We first illustrate this position\nbias in popular extractive QA models such as BiDAF and BERT and thoroughly\nexamine how position bias propagates through each layer of BERT. To safely\ndeliver position information without position bias, we train models with\nvarious de-biasing methods including entropy regularization and bias\nensembling. Among them, we found that using the prior distribution of answer\npositions as a bias model is very effective at reducing position bias,\nrecovering the performance of BERT from 37.48% to 81.64% when trained on a\nbiased SQuAD dataset.",
    "published_date": "2020-04-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14602v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14528v1",
    "title": "Bias-corrected estimator for intrinsic dimension and differential entropy--a visual multiscale approach",
    "authors": [
      "Jugurta Montalvão",
      "Jânio Canuto",
      "Luiz Miranda"
    ],
    "author_ids": [],
    "abstract": "Intrinsic dimension and differential entropy estimators are studied in this\npaper, including their systematic bias. A pragmatic approach for joint\nestimation and bias correction of these two fundamental measures is proposed.\nShared steps on both estimators are highlighted, along with their useful\nconsequences to data analysis. It is shown that both estimators can be\ncomplementary parts of a single approach, and that the simultaneous estimation\nof differential entropy and intrinsic dimension give meaning to each other,\nwhere estimates at different observation scales convey different perspectives\nof underlying manifolds. Experiments with synthetic and real datasets are\npresented to illustrate how to extract meaning from visual inspections, and how\nto compensate for biases.",
    "published_date": "2020-04-30T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14528v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14492v1",
    "title": "Rethinking Class-Discrimination Based CNN Channel Pruning",
    "authors": [
      "Yuchen Liu",
      "David Wentzlaff",
      "S. Y. Kung"
    ],
    "author_ids": [],
    "abstract": "Channel pruning has received ever-increasing focus on network compression. In\nparticular, class-discrimination based channel pruning has made major headway,\nas it fits seamlessly with the classification objective of CNNs and provides\ngood explainability. Prior works singly propose and evaluate their discriminant\nfunctions, while further study on the effectiveness of the adopted metrics is\nabsent. To this end, we initiate the first study on the effectiveness of a\nbroad range of discriminant functions on channel pruning. Conventional\nsingle-variate binary-class statistics like Student's T-Test are also included\nin our study via an intuitive generalization. The winning metric of our study\nhas a greater ability to select informative channels over other\nstate-of-the-art methods, which is substantiated by our qualitative and\nquantitative analysis. Moreover, we develop a FLOP-normalized sensitivity\nanalysis scheme to automate the structural pruning procedure. On CIFAR-10,\nCIFAR-100, and ILSVRC-2012 datasets, our pruned models achieve higher accuracy\nwith less inference cost compared to state-of-the-art results. For example, on\nILSVRC-2012, our 44.3% FLOPs-pruned ResNet-50 has only a 0.3% top-1 accuracy\ndrop, which significantly outperforms the state of the art.",
    "published_date": "2020-04-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14492v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14366v2",
    "title": "Elastic weight consolidation for better bias inoculation",
    "authors": [
      "James Thorne",
      "Andreas Vlachos"
    ],
    "author_ids": [],
    "abstract": "The biases present in training datasets have been shown to affect models for\nsentence pair classification tasks such as natural language inference (NLI) and\nfact verification. While fine-tuning models on additional data has been used to\nmitigate them, a common issue is that of catastrophic forgetting of the\noriginal training dataset. In this paper, we show that elastic weight\nconsolidation (EWC) allows fine-tuning of models to mitigate biases while being\nless susceptible to catastrophic forgetting. In our evaluation on fact\nverification and NLI stress tests, we show that fine-tuning with EWC dominates\nstandard fine-tuning, yielding models with lower levels of forgetting on the\noriginal (biased) dataset for equivalent gains in accuracy on the fine-tuning\n(unbiased) dataset.",
    "published_date": "2020-04-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14366v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14299v1",
    "title": "Detecting Perceived Emotions in Hurricane Disasters",
    "authors": [
      "Shrey Desai",
      "Cornelia Caragea",
      "Junyi Jessy Li"
    ],
    "author_ids": [],
    "abstract": "Natural disasters (e.g., hurricanes) affect millions of people each year,\ncausing widespread destruction in their wake. People have recently taken to\nsocial media websites (e.g., Twitter) to share their sentiments and feelings\nwith the larger community. Consequently, these platforms have become\ninstrumental in understanding and perceiving emotions at scale. In this paper,\nwe introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning\nthree hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of\nfine-grained emotions and propose classification tasks to discriminate between\ncoarse-grained emotion groups. Our best BERT model, even after task-guided\npre-training which leverages unlabeled Twitter data, achieves only 68% accuracy\n(averaged across all groups). HurricaneEmo serves not only as a challenging\nbenchmark for models but also as a valuable resource for analyzing emotions in\ndisaster-centric domains.",
    "published_date": "2020-04-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14299v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.14088v3",
    "title": "Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting",
    "authors": [
      "Guanhua Zhang",
      "Bing Bai",
      "Junqi Zhang",
      "Kun Bai",
      "Conghui Zhu",
      "Tiejun Zhao"
    ],
    "author_ids": [],
    "abstract": "With the recent proliferation of the use of text classifications, researchers\nhave found that there are certain unintended biases in text classification\ndatasets. For example, texts containing some demographic identity-terms (e.g.,\n\"gay\", \"black\") are more likely to be abusive in existing abusive language\ndetection datasets. As a result, models trained with these datasets may\nconsider sentences like \"She makes me happy to be gay\" as abusive simply\nbecause of the word \"gay.\" In this paper, we formalize the unintended biases in\ntext classification datasets as a kind of selection bias from the\nnon-discrimination distribution to the discrimination distribution. Based on\nthis formalization, we further propose a model-agnostic debiasing training\nframework by recovering the non-discrimination distribution using instance\nweighting, which does not require any extra resources or annotations apart from\na pre-defined set of demographic identity-terms. Experiments demonstrate that\nour method can effectively alleviate the impacts of the unintended biases\nwithout significantly hurting models' generalization ability.",
    "published_date": "2020-04-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14088v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2005.02157v1",
    "title": "Stereotype-Free Classification of Fictitious Faces",
    "authors": [
      "Mohammadhossein Toutiaee",
      "Soheyla Amirian",
      "John A. Miller",
      "Sheng Li"
    ],
    "author_ids": [],
    "abstract": "Equal Opportunity and Fairness are receiving increasing attention in\nartificial intelligence. Stereotyping is another source of discrimination,\nwhich yet has been unstudied in literature. GAN-made faces would be exposed to\nsuch discrimination, if they are classified by human perception. It is possible\nto eliminate the human impact on fictitious faces classification task by the\nuse of statistical approaches. We present a novel approach through penalized\nregression to label stereotype-free GAN-generated synthetic unlabeled images.\nThe proposed approach aids labeling new data (fictitious output images) by\nminimizing a penalized version of the least squares cost function between\nrealistic pictures and target pictures.",
    "published_date": "2020-04-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.02157v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.13954v2",
    "title": "Rethink the Connections among Generalization, Memorization and the Spectral Bias of DNNs",
    "authors": [
      "Xiao Zhang",
      "Haoyi Xiong",
      "Dongrui Wu"
    ],
    "author_ids": [],
    "abstract": "Over-parameterized deep neural networks (DNNs) with sufficient capacity to\nmemorize random noise can achieve excellent generalization performance,\nchallenging the bias-variance trade-off in classical learning theory. Recent\nstudies claimed that DNNs first learn simple patterns and then memorize noise;\nsome other works showed a phenomenon that DNNs have a spectral bias to learn\ntarget functions from low to high frequencies during training. However, we show\nthat the monotonicity of the learning bias does not always hold: under the\nexperimental setup of deep double descent, the high-frequency components of\nDNNs diminish in the late stage of training, leading to the second descent of\nthe test error. Besides, we find that the spectrum of DNNs can be applied to\nindicating the second descent of the test error, even though it is calculated\nfrom the training set only.",
    "published_date": "2020-04-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13954v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.13866v1",
    "title": "Deflating Dataset Bias Using Synthetic Data Augmentation",
    "authors": [
      "Nikita Jaipuria",
      "Xianling Zhang",
      "Rohan Bhasin",
      "Mayar Arafa",
      "Punarjay Chakravarty",
      "Shubham Shrivastava",
      "Sagar Manglani",
      "Vidya N. Murali"
    ],
    "author_ids": [],
    "abstract": "Deep Learning has seen an unprecedented increase in vision applications since\nthe publication of large-scale object recognition datasets and introduction of\nscalable compute hardware. State-of-the-art methods for most vision tasks for\nAutonomous Vehicles (AVs) rely on supervised learning and often fail to\ngeneralize to domain shifts and/or outliers. Dataset diversity is thus key to\nsuccessful real-world deployment. No matter how big the size of the dataset,\ncapturing long tails of the distribution pertaining to task-specific\nenvironmental factors is impractical. The goal of this paper is to investigate\nthe use of targeted synthetic data augmentation - combining the benefits of\ngaming engine simulations and sim2real style transfer techniques - for filling\ngaps in real datasets for vision tasks. Empirical studies on three different\ncomputer vision tasks of practical use to AVs - parking slot detection, lane\ndetection and monocular depth estimation - consistently show that having\nsynthetic data in the training mix provides a significant boost in\ncross-dataset generalization performance as compared to training on real data\nonly, for the same size of the training set.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13866v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.13676v2",
    "title": "Value-based Engineering for Ethics by Design",
    "authors": [
      "Sarah Spiekermann",
      "Till Winkler"
    ],
    "author_ids": [],
    "abstract": "This article gives a methodological overview of Value-based Engineering for\nethics by design. It discusses key challenges and measures involved in\neliciting, conceptualizing, prioritizing and respecting values in system\ndesign. Thereby it draws from software engineering, value sensitive design,\ndesign thinking and participatory design as well as from philosophical sources,\nespecially Material Ethics of Value. The article recognizes timely challenges\nfor Value-based Engineering, such as compatibility with agile forms of system\ndevelopment, responsibility in hardly controllable ecosystems of interconnected\nservices, fearless integration of external stakeholders and the difficulty in\nmeasuring the ethicality of a system. Finally, the Value-based Engineering\nmethodology presented here benefits from learnings collected in the IEEE P7000\nstandardization process as well as from a case study. P7000 has been set up by\nIEEE to establish a process model, which addresses ethical considerations\nthroughout the various stages of system initiation, analysis and design.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.GL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13676v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.13567v1",
    "title": "Hybrid Attention for Automatic Segmentation of Whole Fetal Head in Prenatal Ultrasound Volumes",
    "authors": [
      "Xin Yang",
      "Xu Wang",
      "Yi Wang",
      "Haoran Dou",
      "Shengli Li",
      "Huaxuan Wen",
      "Yi Lin",
      "Pheng-Ann Heng",
      "Dong Ni"
    ],
    "author_ids": [],
    "abstract": "Background and Objective: Biometric measurements of fetal head are important\nindicators for maternal and fetal health monitoring during pregnancy. 3D\nultrasound (US) has unique advantages over 2D scan in covering the whole fetal\nhead and may promote the diagnoses. However, automatically segmenting the whole\nfetal head in US volumes still pends as an emerging and unsolved problem. The\nchallenges that automated solutions need to tackle include the poor image\nquality, boundary ambiguity, long-span occlusion, and the appearance\nvariability across different fetal poses and gestational ages. In this paper,\nwe propose the first fully-automated solution to segment the whole fetal head\nin US volumes.\n  Methods: The segmentation task is firstly formulated as an end-to-end\nvolumetric mapping under an encoder-decoder deep architecture. We then combine\nthe segmentor with a proposed hybrid attention scheme (HAS) to select\ndiscriminative features and suppress the non-informative volumetric features in\na composite and hierarchical way. With little computation overhead, HAS proves\nto be effective in addressing boundary ambiguity and deficiency. To enhance the\nspatial consistency in segmentation, we further organize multiple segmentors in\na cascaded fashion to refine the results by revisiting context in the\nprediction of predecessors.\n  Results: Validated on a large dataset collected from 100 healthy volunteers,\nour method presents superior segmentation performance (DSC (Dice Similarity\nCoefficient), 96.05%), remarkable agreements with experts. With another 156\nvolumes collected from 52 volunteers, we ahieve high reproducibilities (mean\nstandard deviation 11.524 mL) against scan variations.\n  Conclusion: This is the first investigation about whole fetal head\nsegmentation in 3D US. Our method is promising to be a feasible solution in\nassisting the volumetric US-based prenatal studies.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13567v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.13515v4",
    "title": "Addressing Artificial Intelligence Bias in Retinal Disease Diagnostics",
    "authors": [
      "Philippe Burlina",
      "Neil Joshi",
      "William Paul",
      "Katia D. Pacheco",
      "Neil M. Bressler"
    ],
    "author_ids": [],
    "abstract": "This study evaluated generative methods to potentially mitigate AI bias when\ndiagnosing diabetic retinopathy (DR) resulting from training data imbalance, or\ndomain generalization which occurs when deep learning systems (DLS) face\nconcepts at test/inference time they were not initially trained on. The public\ndomain Kaggle-EyePACS dataset (88,692 fundi and 44,346 individuals, originally\ndiverse for ethnicity) was modified by adding clinician-annotated labels and\nconstructing an artificial scenario of data imbalance and domain generalization\nby disallowing training (but not testing) exemplars for images of retinas with\nDR warranting referral (DR-referable) and from darker-skin individuals, who\npresumably have greater concentration of melanin within uveal melanocytes, on\naverage, contributing to retinal image pigmentation. A traditional/baseline\ndiagnostic DLS was compared against new DLSs that would use training data\naugmented via generative models for debiasing. Accuracy (95% confidence\nintervals [CI]) of the baseline diagnostics DLS for fundus images of\nlighter-skin individuals was 73.0% (66.9%, 79.2%) vs. darker-skin of 60.5%\n(53.5%, 67.3%), demonstrating bias/disparity (delta=12.5%) (Welch t-test\nt=2.670, P=.008) in AI performance across protected subpopulations. Using novel\ngenerative methods for addressing missing subpopulation training data\n(DR-referable darker-skin) achieved instead accuracy, for lighter-skin, of\n72.0% (65.8%, 78.2%), and for darker-skin, of 71.5% (65.2%,77.8%),\ndemonstrating closer parity (delta=0.5%) in accuracy across subpopulations\n(Welch t-test t=0.111, P=.912). Findings illustrate how data imbalance and\ndomain generalization can lead to disparity of accuracy across subpopulations,\nand show that novel generative methods of synthetic fundus images may play a\nrole for debiasing AI.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13515v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.13463v1",
    "title": "How do online consumers review negatively?",
    "authors": [
      "Menghan Sun",
      "Jichang Zhao"
    ],
    "author_ids": [],
    "abstract": "Negative reviews on e-commerce platforms, mainly in the form of texts, are\nposted by online consumers to express complaints about unsatisfactory\nexperiences, providing a proxy of big data for sellers to consider\nimprovements. However, the exact knowledge that lies beyond the negative\nreviewing still remains unknown. Aimed at a systemic understanding of how\nonline consumers post negative reviews, using 1, 450, 000 negative reviews from\nJD.com, the largest B2C platform in China, the behavioral patterns from\ntemporal, perceptional and emotional perspectives are comprehensively explored\nin the present study. Massive consumers behind these reviews across four\nsectors in the most recent 10 years are further split into five levels to\nreveal group discriminations at a fine resolution. Circadian rhythms of\nnegative reviewing after making purchases were found, and the periodic\nintervals suggest stable habits in online consumption and that consumers tend\nto negatively review at the same hour of the purchase. Consumers from lower\nlevels express more intensive negative feelings, especially on product pricing\nand seller attitudes, while those from upper levels demonstrate a stronger\nmomentum of negative emotion. The value of negative reviews from higher-level\nconsumers is thus unexpectedly highlighted because of less emotionalization and\nless biased narration, while the longer-lasting characteristic of these\nconsumers' negative responses also stresses the need for more attention from\nsellers. Our results shed light on implementing distinguished proactive\nstrategies in different buyer groups to help mitigate the negative impact due\nto negative reviews.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13463v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.14111v2",
    "title": "A System for Generating Non-Uniform Random Variates using Graphene Field-Effect Transistors",
    "authors": [
      "Nathaniel Joseph Tye",
      "James Timothy Meech",
      "Bilgesu Arif Bilgin",
      "Phillip Stanley-Marbell"
    ],
    "author_ids": [],
    "abstract": "We introduce a new method for hardware non-uniform random number generation\nbased on the transfer characteristics of graphene field-effect transistors\n(GFETs) which requires as few as two transistors and a resistor (or\ntransimpedance amplifier). The method could be integrated into a custom\ncomputing system to provide samples from arbitrary univariate distributions. We\nalso demonstrate the use of wavelet decomposition of the target distribution to\ndetermine GFET bias voltages in a multi-GFET array.\n  We implement the method by fabricating multiple GFETs and experimentally\nvalidating that their transfer characteristics exhibit the nonlinearity on\nwhich our method depends. We use the characterization data in simulations of a\nproposed architecture for generating samples from dynamically-selectable\nnon-uniform probability distributions.\n  Using a combination of experimental measurements of GFETs under a range of\nbiasing conditions and simulation of the GFET-based non-uniform random variate\ngenerator architecture, we demonstrate a speedup of Monte Carlo integration by\na factor of up to 2$\\times$. This speedup assumes the analog-to-digital\nconverters reading the outputs from the circuit can produce samples in the same\namount of time that it takes to perform memory accesses.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.ET",
      "physics.app-ph",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.14111v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.13282v1",
    "title": "Genetic programming approaches to learning fair classifiers",
    "authors": [
      "William La Cava",
      "Jason H. Moore"
    ],
    "author_ids": [],
    "abstract": "Society has come to rely on algorithms like classifiers for important\ndecision making, giving rise to the need for ethical guarantees such as\nfairness. Fairness is typically defined by asking that some statistic of a\nclassifier be approximately equal over protected groups within a population. In\nthis paper, current approaches to fairness are discussed and used to motivate\nalgorithmic proposals that incorporate fairness into genetic programming for\nclassification. We propose two ideas. The first is to incorporate a fairness\nobjective into multi-objective optimization. The second is to adapt lexicase\nselection to define cases dynamically over intersections of protected groups.\nWe describe why lexicase selection is well suited to pressure models to perform\nwell across the potentially infinitely many subgroups over which fairness is\ndesired. We use a recent genetic programming approach to construct models on\nfour datasets for which fairness constraints are necessary, and empirically\ncompare performance to prior methods utilizing game-theoretic solutions.\nMethods are assessed based on their ability to generate trade-offs of subgroup\nfairness and accuracy that are Pareto optimal. The result show that genetic\nprogramming methods in general, and random search in particular, are well\nsuited to this task.",
    "published_date": "2020-04-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13282v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.13142v1",
    "title": "Quantifying Latent Moral Foundations in Twitter Narratives: The Case of the Syrian White Helmets Misinformation",
    "authors": [
      "Ece Çiğdem Mutlu",
      "Toktam Oghaz",
      "Ege Tütüncüler",
      "Jasser Jasser",
      "Ivan Garibay"
    ],
    "author_ids": [],
    "abstract": "For years, many studies employed sentiment analysis to understand the\nreasoning behind people's choices and feelings, their communication styles, and\nthe communities which they belong to. We argue that gaining more in-depth\ninsight into moral dimensions coupled with sentiment analysis can potentially\nprovide superior results. Understanding moral foundations can yield powerful\nresults in terms of perceiving the intended meaning of the text data, as the\nconcept of morality provides additional information on the unobservable\ncharacteristics of information processing and non-conscious cognitive\nprocesses. Therefore, we studied latent moral loadings of Syrian White\nHelmets-related tweets of Twitter users from April 1st, 2018 to April 30th,\n2019. For the operationalization and quantification of moral rhetoric in\ntweets, we use Extended Moral Foundations Dictionary in which five\npsychological dimensions (Harm/Care, Fairness/Reciprocity, In-group/Loyalty,\nAuthority/Respect and Purity/Sanctity) are considered. We show that people tend\nto share more tweets involving the virtue moral rhetoric than the tweets\ninvolving the vice rhetoric. We observe that the pattern of the moral rhetoric\nof tweets among these five dimensions are very similar during different time\nperiods, while the strength of the five dimension is time-variant. Even though\nthere is no significant difference between the use of Fairness/Reciprocity,\nIn-group/Loyalty or Purity/Sanctity rhetoric, the less use of Harm/Care\nrhetoric is significant and remarkable. Besides, the strength of the moral\nrhetoric and the polarization in morality across people are mostly observed in\ntweets involving Harm/Care rhetoric despite the number of tweets involving the\nHarm/Care dimension is low.",
    "published_date": "2020-04-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13142v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.12943v3",
    "title": "Audio-Visual Instance Discrimination with Cross-Modal Agreement",
    "authors": [
      "Pedro Morgado",
      "Nuno Vasconcelos",
      "Ishan Misra"
    ],
    "author_ids": [],
    "abstract": "We present a self-supervised learning approach to learn audio-visual\nrepresentations from video and audio. Our method uses contrastive learning for\ncross-modal discrimination of video from audio and vice-versa. We show that\noptimizing for cross-modal discrimination, rather than within-modal\ndiscrimination, is important to learn good representations from video and\naudio. With this simple but powerful insight, our method achieves highly\ncompetitive performance when finetuned on action recognition tasks.\nFurthermore, while recent work in contrastive learning defines positive and\nnegative samples as individual instances, we generalize this definition by\nexploring cross-modal agreement. We group together multiple instances as\npositives by measuring their similarity in both the video and audio feature\nspaces. Cross-modal agreement creates better positive and negative sets, which\nallows us to calibrate visual similarities by seeking within-modal\ndiscrimination of positive instances, and achieve significant gains on\ndownstream tasks.",
    "published_date": "2020-04-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12943v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12906v1",
    "title": "Towards causal generative scene models via competition of experts",
    "authors": [
      "Julius von Kügelgen",
      "Ivan Ustyuzhaninov",
      "Peter Gehler",
      "Matthias Bethge",
      "Bernhard Schölkopf"
    ],
    "author_ids": [],
    "abstract": "Learning how to model complex scenes in a modular way with recombinable\ncomponents is a pre-requisite for higher-order reasoning and acting in the\nphysical world. However, current generative models lack the ability to capture\nthe inherently compositional and layered nature of visual scenes. While recent\nwork has made progress towards unsupervised learning of object-based scene\nrepresentations, most models still maintain a global representation space\n(i.e., objects are not explicitly separated), and cannot generate scenes with\nnovel object arrangement and depth ordering. Here, we present an alternative\napproach which uses an inductive bias encouraging modularity by training an\nensemble of generative models (experts). During training, experts compete for\nexplaining parts of a scene, and thus specialise on different object classes,\nwith objects being identified as parts that re-occur across multiple scenes.\nOur model allows for controllable sampling of individual objects and\nrecombination of experts in physically plausible ways. In contrast to other\nmethods, depth layering and occlusion are handled correctly, moving this\napproach closer to a causal generative scene model. Experiments on simple toy\ndata qualitatively demonstrate the conceptual advantages of the proposed\napproach.",
    "published_date": "2020-04-27T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12906v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12891v1",
    "title": "The Benefit of Being Non-Lazy in Probabilistic λ-calculus",
    "authors": [
      "Gianluca Curzi",
      "Michele Pagani"
    ],
    "author_ids": [],
    "abstract": "We consider the probabilistic applicative bisimilarity (PAB), a coinductive\nrelation comparing the applicative behaviour of probabilistic untyped lambda\nterms according to a specific operational semantics. This notion has been\nstudied with respect to the two standard parameter passing policies,\ncall-by-value (cbv) and call-by-name (cbn), using a lazy reduction strategy not\nreducing within the body of a function. In particular, PAB has been proven to\nbe fully abstract with respect to the contextual equivalence in cbv but not in\nlazy cbn. We overcome this issue of cbn by relaxing the laziness constraint: we\nprove that PAB is fully abstract with respect to the standard head reduction\ncontextual equivalence. Our proof is based on the Leventis Separation Theorem,\nusing probabilistic Nakajima trees as a tree-like representation of the\ncontextual equivalence classes. Finally, we prove also that the inequality full\nabstraction fails, showing that the probabilistic applicative similarity is\nstrictly contained in the contextual preorder.",
    "published_date": "2020-04-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12891v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.12724v1",
    "title": "Unsupervised Domain Adaptation with Multiple Domain Discriminators and Adaptive Self-Training",
    "authors": [
      "Teo Spadotto",
      "Marco Toldo",
      "Umberto Michieli",
      "Pietro Zanuttigh"
    ],
    "author_ids": [],
    "abstract": "Unsupervised Domain Adaptation (UDA) aims at improving the generalization\ncapability of a model trained on a source domain to perform well on a target\ndomain for which no labeled data is available. In this paper, we consider the\nsemantic segmentation of urban scenes and we propose an approach to adapt a\ndeep neural network trained on synthetic data to real scenes addressing the\ndomain shift between the two different data distributions. We introduce a novel\nUDA framework where a standard supervised loss on labeled synthetic data is\nsupported by an adversarial module and a self-training strategy aiming at\naligning the two domain distributions. The adversarial module is driven by a\ncouple of fully convolutional discriminators dealing with different domains:\nthe first discriminates between ground truth and generated maps, while the\nsecond between segmentation maps coming from synthetic or real world data. The\nself-training module exploits the confidence estimated by the discriminators on\nunlabeled data to select the regions used to reinforce the learning process.\nFurthermore, the confidence is thresholded with an adaptive mechanism based on\nthe per-class overall confidence. Experimental results prove the effectiveness\nof the proposed strategy in adapting a segmentation network trained on\nsynthetic datasets like GTA5 and SYNTHIA, to real world datasets like\nCityscapes and Mapillary.",
    "published_date": "2020-04-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12724v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12492v1",
    "title": "Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks",
    "authors": [
      "Kang Liu",
      "Benjamin Tan",
      "Gaurav Rajavendra Reddy",
      "Siddharth Garg",
      "Yiorgos Makris",
      "Ramesh Karri"
    ],
    "author_ids": [],
    "abstract": "Deep learning (DL) offers potential improvements throughout the CAD\ntool-flow, one promising application being lithographic hotspot detection.\nHowever, DL techniques have been shown to be especially vulnerable to inference\nand training time adversarial attacks. Recent work has demonstrated that a\nsmall fraction of malicious physical designers can stealthily \"backdoor\" a\nDL-based hotspot detector during its training phase such that it accurately\nclassifies regular layout clips but predicts hotspots containing a specially\ncrafted trigger shape as non-hotspots. We propose a novel training data\naugmentation strategy as a powerful defense against such backdooring attacks.\nThe defense works by eliminating the intentional biases introduced in the\ntraining data but does not require knowledge of which training samples are\npoisoned or the nature of the backdoor trigger. Our results show that the\ndefense can drastically reduce the attack success rate from 84% to ~0%.",
    "published_date": "2020-04-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12492v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12388v3",
    "title": "The Impact of Presentation Style on Human-In-The-Loop Detection of Algorithmic Bias",
    "authors": [
      "Po-Ming Law",
      "Sana Malik",
      "Fan Du",
      "Moumita Sinha"
    ],
    "author_ids": [],
    "abstract": "While decision makers have begun to employ machine learning, machine learning\nmodels may make predictions that bias against certain demographic groups.\nSemi-automated bias detection tools often present reports of\nautomatically-detected biases using a recommendation list or visual cues.\nHowever, there is a lack of guidance concerning which presentation style to use\nin what scenarios. We conducted a small lab study with 16 participants to\ninvestigate how presentation style might affect user behaviors in reviewing\nbias reports. Participants used both a prototype with a recommendation list and\na prototype with visual cues for bias detection. We found that participants\noften wanted to investigate the performance measures that were not\nautomatically detected as biases. Yet, when using the prototype with a\nrecommendation list, they tended to give less consideration to such measures.\nGrounded in the findings, we propose information load and comprehensiveness as\ntwo axes for characterizing bias detection tasks and illustrate how the two\naxes could be adopted to reason about when to use a recommendation list or\nvisual cues.",
    "published_date": "2020-04-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12388v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12353v1",
    "title": "Improved User Fairness in Decode-Forward Relaying Non-orthogonal Multiple Access Schemes with Imperfect SIC",
    "authors": [
      "Ferdi Kara",
      "Hakan Kaya"
    ],
    "author_ids": [],
    "abstract": "Non-orthogonal multiple access (NOMA) is one of the key technologies to serve\nin ultra-dense networks with massive connections which is crucial for Internet\nof Things (IoT) applications. Besides, NOMA provides better spectral efficiency\ncompared to orthogonal multiple access (OMA) schemes. However, in NOMA,\nsuccessive interference canceler (SIC) should be implemented for interference\nmitigation and mostly in the literature, perfect SIC is assumed for NOMA\ninvolved systems. Unfortunately, this is not the case for practical scenarios\nand this imperfect SIC effect limits the performance of NOMA involved systems.\nIn addition, it causes unfairness between users. In this paper, we introduce\nreversed decode-forward relaying NOMA (R-DFNOMA) to improve user fairness\ncompared to conventional DFNOMA (C-DFNOMA) which is widely analyzed in\nliterature. In the analysis, we define imperfect SIC effect dependant to\nchannel fading and with this imperfect SIC, we derive exact expressions for\nergodic capacity (EC) and outage probability (OP). Moreover, we evaluate bit\nerror performance of proposed R-DFNOMA and derive bit error probability (BEP)\nin closed-form which has not been also studied well in literature. Then, we\ndefine user fairness index in terms of all key performance indicators (KPIs)\n(i.e., EC, OP and BEP). Based on extensive simulations, all derived expressions\nare validated, and it is proved that proposed R-DFNOMA provides better user\nfairness than C-DFNOMA in terms of all KPIs. Finally, we discuss the effect of\npower allocations at the both source and relay on the performance metrics and\nuser fairness",
    "published_date": "2020-04-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12353v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.12332v1",
    "title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds",
    "authors": [
      "Kawin Ethayarajh"
    ],
    "author_ids": [],
    "abstract": "Most NLP datasets are not annotated with protected attributes such as gender,\nmaking it difficult to measure classification bias using standard measures of\nfairness (e.g., equal opportunity). However, manually annotating a large\ndataset with a protected attribute is slow and expensive. Instead of annotating\nall the examples, can we annotate a subset of them and use that sample to\nestimate the bias? While it is possible to do so, the smaller this annotated\nsample is, the less certain we are that the estimate is close to the true bias.\nIn this work, we propose using Bernstein bounds to represent this uncertainty\nabout the bias estimate as a confidence interval. We provide empirical evidence\nthat a 95% confidence interval derived this way consistently bounds the true\nbias. In quantifying this uncertainty, our method, which we call\nBernstein-bounded unfairness, helps prevent classifiers from being deemed\nbiased or unbiased when there is insufficient evidence to make either claim.\nOur findings suggest that the datasets currently used to measure specific\nbiases are too small to conclusively identify bias except in the most egregious\ncases. For example, consider a co-reference resolution system that is 5% more\naccurate on gender-stereotypical sentences -- to claim it is biased with 95%\nconfidence, we need a bias-specific dataset that is 3.8 times larger than\nWinoBias, the largest available.",
    "published_date": "2020-04-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12332v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12265v2",
    "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias",
    "authors": [
      "Jesse Vig",
      "Sebastian Gehrmann",
      "Yonatan Belinkov",
      "Sharon Qian",
      "Daniel Nevo",
      "Simas Sakenis",
      "Jason Huang",
      "Yaron Singer",
      "Stuart Shieber"
    ],
    "author_ids": [],
    "abstract": "Common methods for interpreting neural models in natural language processing\ntypically examine either their structure or their behavior, but not both. We\npropose a methodology grounded in the theory of causal mediation analysis for\ninterpreting which parts of a model are causally implicated in its behavior. It\nenables us to analyze the mechanisms by which information flows from input to\noutput through various model components, known as mediators. We apply this\nmethodology to analyze gender bias in pre-trained Transformer language models.\nWe study the role of individual neurons and attention heads in mediating gender\nbias across three datasets designed to gauge a model's sensitivity to gender\nbias. Our mediation analysis reveals that gender bias effects are (i) sparse,\nconcentrated in a small part of the network; (ii) synergistic, amplified or\nrepressed by different components; and (iii) decomposable into effects flowing\ndirectly from the input and indirectly through the mediators.",
    "published_date": "2020-04-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12265v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12207v1",
    "title": "Internet-human infrastructures: Lessons from Havana's StreetNet",
    "authors": [
      "Abigail Z. Jacobs",
      "Michaelanne Dye"
    ],
    "author_ids": [],
    "abstract": "We propose a mixed-methods approach to understanding the human infrastructure\nunderlying StreetNet (SNET), a distributed, community-run intranet that serves\nas the primary 'Internet' in Havana, Cuba. We bridge ethnographic studies and\nthe study of social networks and organizations to understand the way that power\nis embedded in the structure of Havana's SNET. By quantitatively and\nqualitatively unpacking the human infrastructure of SNET, this work reveals how\ndistributed infrastructure necessarily embeds the structural aspects of\ninequality distributed within that infrastructure. While traditional technical\nmeasurements of networks reflect the social, organizational, spatial, and\ntechnical constraints that shape the resulting network, ethnographies can help\nuncover the texture and role of these hidden supporting relationships. By\nmerging these perspectives, this work contributes to our understanding of\nnetwork roles in growing and maintaining distributed infrastructures, revealing\nnew approaches to understanding larger, more complex Internet-human\ninfrastructures---including the Internet and the WWW.",
    "published_date": "2020-04-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12207v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.12064v2",
    "title": "CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for Skin Lesion Classification",
    "authors": [
      "Di Zhuang",
      "Keyu Chen",
      "J. Morris Chang"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks (CNNs) have achieved the state-of-the-art\nperformance in skin lesion analysis. Compared with single CNN classifier,\ncombining the results of multiple classifiers via fusion approaches shows to be\nmore effective and robust. Since the skin lesion datasets are usually limited\nand statistically biased, while designing an effective fusion approach, it is\nimportant to consider not only the performance of each classifier on the\ntraining/validation dataset, but also the relative discriminative power (e.g.,\nconfidence) of each classifier regarding an individual sample in the testing\nphase, which calls for an active fusion approach. Furthermore, in skin lesion\nanalysis, the data of certain classes (e.g., the benign lesions) is usually\nabundant making them an over-represented majority, while the data of some other\nclasses (e.g., the cancerous lesions) is deficient, making them an\nunderrepresented minority. It is more crucial to precisely identify the samples\nfrom an underrepresented (i.e., in terms of the amount of data) but more\nimportant minority class (e.g., certain cancerous lesion). In other words,\nmisclassifying a more severe lesion to a benign or less severe lesion should\nhave relative more cost (e.g., money, time and even lives). To address such\nchallenges, we present CS-AF, a cost-sensitive multi-classifier active fusion\nframework for skin lesion classification. In the experimental evaluation, we\nprepared 96 base classifiers (of 12 CNN architectures) on the ISIC research\ndatasets. Our experimental results show that our framework consistently\noutperforms the static fusion competitors.",
    "published_date": "2020-04-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12064v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.11981v1",
    "title": "DeepMerge: Classifying High-redshift Merging Galaxies with Deep Neural Networks",
    "authors": [
      "A. Ćiprijanović",
      "G. F. Snyder",
      "B. Nord",
      "J. E. G. Peek"
    ],
    "author_ids": [],
    "abstract": "We investigate and demonstrate the use of convolutional neural networks\n(CNNs) for the task of distinguishing between merging and non-merging galaxies\nin simulated images, and for the first time at high redshifts (i.e. $z=2$). We\nextract images of merging and non-merging galaxies from the Illustris-1\ncosmological simulation and apply observational and experimental noise that\nmimics that from the Hubble Space Telescope; the data without noise form a\n\"pristine\" data set and that with noise form a \"noisy\" data set. The test set\nclassification accuracy of the CNN is $79\\%$ for pristine and $76\\%$ for noisy.\nThe CNN outperforms a Random Forest classifier, which was shown to be superior\nto conventional one- or two-dimensional statistical methods (Concentration,\nAsymmetry, the Gini, $M_{20}$ statistics etc.), which are commonly used when\nclassifying merging galaxies. We also investigate the selection effects of the\nclassifier with respect to merger state and star formation rate, finding no\nbias. Finally, we extract Grad-CAMs (Gradient-weighted Class Activation\nMapping) from the results to further assess and interrogate the fidelity of the\nclassification model.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "astro-ph.GA",
      "astro-ph.IM",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11981v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.11858v2",
    "title": "Impact of different belief facets on agents' decision -- a refined cognitive architecture to model the interaction between organisations' institutional characteristics and agents' behaviour",
    "authors": [
      "Amir Hosein Afshar Sedigh",
      "Martin K. Purvis",
      "Bastin Tony Roy Savarimuthu",
      "Christopher K Frantz",
      "Maryam A. Purvis"
    ],
    "author_ids": [],
    "abstract": "This paper presents a conceptual refinement of agent cognitive architecture\ninspired from the beliefs-desires-intentions (BDI) and the theory of planned\nbehaviour (TPB) models, with an emphasis on different belief facets. This\nenables us to investigate the impact of personality and the way that an agent\nweights its internal beliefs and social sanctions on an agent's actions. The\nstudy also uses the concept of cognitive dissonance associated with the\nfairness of institutions to investigate the agents' behaviour. To showcase our\nmodel, we simulate two historical long-distance trading societies, namely\nArmenian merchants of New-Julfa and the English East India Company. The results\ndemonstrate the importance of internal beliefs of agents as a pivotal aspect\nfor following institutional rules.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11858v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.11759v3",
    "title": "Learning Term Discrimination",
    "authors": [
      "Jibril Frej",
      "Phillipe Mulhem",
      "Didier Schwab",
      "Jean-Pierre Chevallet"
    ],
    "author_ids": [],
    "abstract": "Document indexing is a key component for efficient information retrieval\n(IR). After preprocessing steps such as stemming and stop-word removal,\ndocument indexes usually store term-frequencies (tf). Along with tf (that only\nreflects the importance of a term in a document), traditional IR models use\nterm discrimination values (TDVs) such as inverse document frequency (idf) to\nfavor discriminative terms during retrieval. In this work, we propose to learn\nTDVs for document indexing with shallow neural networks that approximate\ntraditional IR ranking functions such as TF-IDF and BM25. Our proposal\noutperforms, both in terms of nDCG and recall, traditional approaches, even\nwith few positively labelled query-document pairs as learning data. Our learned\nTDVs, when used to filter out terms of the vocabulary that have zero\ndiscrimination value, allow to both significantly lower the memory footprint of\nthe inverted index and speed up the retrieval process (BM25 is up to 3~times\nfaster), without degrading retrieval quality.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11759v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.11665v2",
    "title": "6G White paper: Research challenges for Trust, Security and Privacy",
    "authors": [
      "Mika Ylianttila",
      "Raimo Kantola",
      "Andrei Gurtov",
      "Lozenzo Mucchi",
      "Ian Oppermann",
      "Zheng Yan",
      "Tri Hong Nguyen",
      "Fei Liu",
      "Tharaka Hewa",
      "Madhusanka Liyanage",
      "Ahmad Ijaz",
      "Juha Partala",
      "Robert Abbas",
      "Artur Hecker",
      "Sara Jayousi",
      "Alessio Martinelli",
      "Stefano Caputo",
      "Jonathan Bechtold",
      "Ivan Morales",
      "Andrei Stoica",
      "Giuseppe Abreu",
      "Shahriar Shahabuddin",
      "Erdal Panayirci",
      "Harald Haas",
      "Tanesh Kumar",
      "Basak Ozan Ozparlak",
      "Juha Röning"
    ],
    "author_ids": [],
    "abstract": "The roles of trust, security and privacy are somewhat interconnected, but\ndifferent facets of next generation networks. The challenges in creating a\ntrustworthy 6G are multidisciplinary spanning technology, regulation,\ntechno-economics, politics and ethics. This white paper addresses their\nfundamental research challenges in three key areas. Trust: Under the current\n\"open internet\" regulation, the telco cloud can be used for trust services only\nequally for all users. 6G network must support embedded trust for increased\nlevel of information security in 6G. Trust modeling, trust policies and trust\nmechanisms need to be defined. 6G interlinks physical and digital worlds making\nsafety dependent on information security. Therefore, we need trustworthy 6G.\nSecurity: In 6G era, the dependence of the economy and societies on IT and the\nnetworks will deepen. The role of IT and the networks in national security\nkeeps rising - a continuation of what we see in 5G. The development towards\ncloud and edge native infrastructures is expected to continue in 6G networks,\nand we need holistic 6G network security architecture planning. Security\nautomation opens new questions: machine learning can be used to make safer\nsystems, but also more dangerous attacks. Physical layer security techniques\ncan also represent efficient solutions for securing less investigated network\nsegments as first line of defense. Privacy: There is currently no way to\nunambiguously determine when linked, deidentified datasets cross the threshold\nto become personally identifiable. Courts in different parts of the world are\nmaking decisions about whether privacy is being infringed, while companies are\nseeking new ways to exploit private data to create new business revenues. As\nsolution alternatives, we may consider blockchain, distributed ledger\ntechnologies and differential privacy approaches.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11665v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.11622v2",
    "title": "Learning the grammar of drug prescription: recurrent neural network grammars for medication information extraction in clinical texts",
    "authors": [
      "Ivan Lerner",
      "Jordan Jouffroy",
      "Anita Burgun",
      "Antoine Neuraz"
    ],
    "author_ids": [],
    "abstract": "In this study, we evaluated the RNNG, a neural top-down transition based\nparser, for medication information extraction in clinical texts. We evaluated\nthis model on a French clinical corpus. The task was to extract the name of a\ndrug (or a drug class), as well as attributes informing its administration:\nfrequency, dosage, duration, condition and route of administration. We compared\nthe RNNG model that jointly identifies entities, events and their relations\nwith separate BiLSTMs models for entities, events and relations as baselines.\nWe call seq-BiLSTMs the baseline models for relations extraction that takes as\nextra-input the output of the BiLSTMs for entities and events. Similarly, we\nevaluated seq-RNNG, a hybrid RNNG model that takes as extra-input the output of\nthe BiLSTMs for entities and events. RNNG outperforms seq-BiLSTM for\nidentifying complex relations, with on average 88.1 [84.4-91.6] % versus 69.9\n[64.0-75.4] F-measure. However, RNNG tends to be weaker than the baseline\nBiLSTM on detecting entities, with on average 82.4 [80.8-83.8] versus 84.1\n[82.7-85.6] % F- measure. RNNG trained only for detecting relations tends to be\nweaker than RNNG with the joint modelling objective, 87.4% [85.8-88.8] versus\n88.5% [87.2-89.8]. Seq-RNNG is on par with BiLSTM for entities (84.0\n[82.6-85.4] % F-measure) and with RNNG for relations (88.7 [87.4-90.0] %\nF-measure). The performance of RNNG on relations can be explained both by the\nmodel architecture, which provides inductive bias to capture the hierarchy in\nthe targets, and the joint modeling objective which allows the RNNG to learn\nricher representations. RNNG is efficient for modeling relations between\nentities or/and events in medical texts and its performances are close to those\nof a BiLSTM for entity and event detection.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11622v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.12837v1",
    "title": "A Light CNN for detecting COVID-19 from CT scans of the chest",
    "authors": [
      "Matteo Polsinelli",
      "Luigi Cinque",
      "Giuseppe Placidi"
    ],
    "author_ids": [],
    "abstract": "OVID-19 is a world-wide disease that has been declared as a pandemic by the\nWorld Health Organization. Computer Tomography (CT) imaging of the chest seems\nto be a valid diagnosis tool to detect COVID-19 promptly and to control the\nspread of the disease. Deep Learning has been extensively used in medical\nimaging and convolutional neural networks (CNNs) have been also used for\nclassification of CT images. We propose a light CNN design based on the model\nof the SqueezeNet, for the efficient discrimination of COVID-19 CT images with\nother CT images (community-acquired pneumonia and/or healthy images). On the\ntested datasets, the proposed modified SqueezeNet CNN achieved 83.00\\% of\naccuracy, 85.00\\% of sensitivity, 81.00\\% of specificity, 81.73\\% of precision\nand 0.8333 of F1Score in a very efficient way (7.81 seconds medium-end laptot\nwithout GPU acceleration). Besides performance, the average classification time\nis very competitive with respect to more complex CNN designs, thus allowing its\nusability also on medium power computers. In the next future we aim at\nimproving the performances of the method along two directions: 1) by increasing\nthe training dataset (as soon as other CT images will be available); 2) by\nintroducing an efficient pre-processing strategy.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.12837v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.11531v2",
    "title": "Statistical Discrimination in Ratings-Guided Markets",
    "authors": [
      "Yeon-Koo Che",
      "Kyungmin Kim",
      "Weijie Zhong"
    ],
    "author_ids": [],
    "abstract": "We study statistical discrimination of individuals based on payoff-irrelevant\nsocial identities in markets that utilize ratings and recommendations for\nsocial learning. Even though rating/recommendation algorithms can be designed\nto be fair and unbiased, ratings-based social learning can still lead to\ndiscriminatory outcomes. Our model demonstrates how users' attention choices\ncan result in asymmetric data sampling across social groups, leading to\ndiscriminatory inferences and potential discrimination based on group\nidentities.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11531v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.11502v1",
    "title": "Having our omic cake and eating it too: Evaluating User Response to using Blockchain Technology for Private & Secure Health Data Management and Sharing",
    "authors": [
      "Victoria L. Lemieux",
      "Darra Hofman",
      "Hoda Hamouda",
      "Danielle Batista",
      "Ravneet Kaur",
      "Wen Pan",
      "Ian Costanzo",
      "Dean Regier",
      "Samantha Pollard",
      "Deirdre Weymann",
      "Rob Fraser"
    ],
    "author_ids": [],
    "abstract": "This paper reports on the development and evaluation of a prototype\nblockchain solution for private and secure individual omics health data\nmanagement and sharing. This solution is one output of a multidisciplinary\nproject investigating the social, data and technical issues surrounding\napplication of blockchain technology in the context of personalized healthcare\nresearch. The project studies potential ethical, legal, social and cognitive\nconstraints of self-sovereign healthcare data management and sharing, and\nwhether such constraints can be addressed through careful user interface design\nof a blockchain solution.",
    "published_date": "2020-04-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11502v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.11331v1",
    "title": "Tip the Balance: Improving Exploration of Balanced Crossover Operators by Adaptive Bias",
    "authors": [
      "Luca Manzoni",
      "Luca Mariot",
      "Eva Tuba"
    ],
    "author_ids": [],
    "abstract": "The use of balanced crossover operators in Genetic Algorithms (GA) ensures\nthat the binary strings generated as offsprings have the same Hamming weight of\nthe parents, a constraint which is sought in certain discrete optimization\nproblems. Although this method reduces the size of the search space, the\nresulting fitness landscape often becomes more difficult for the GA to explore\nand to discover optimal solutions. This issue has been studied in this paper by\napplying an adaptive bias strategy to a counter-based crossover operator that\nintroduces unbalancedness in the offspring with a certain probability, which is\ndecreased throughout the evolutionary process. Experiments show that improving\nthe exploration of the search space with this adaptive bias strategy is\nbeneficial for the GA performances in terms of the number of optimal solutions\nfound for the balanced nonlinear Boolean functions problem.",
    "published_date": "2020-04-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11331v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.11197v3",
    "title": "On Relations Between the Relative entropy and $χ^2$-Divergence, Generalizations and Applications",
    "authors": [
      "Tomohiro Nishiyama",
      "Igal Sason"
    ],
    "author_ids": [],
    "abstract": "The relative entropy and chi-squared divergence are fundamental divergence\nmeasures in information theory and statistics. This paper is focused on a study\nof integral relations between the two divergences, the implications of these\nrelations, their information-theoretic applications, and some generalizations\npertaining to the rich class of $f$-divergences. Applications that are studied\nin this paper refer to lossless compression, the method of types and large\ndeviations, strong~data-processing inequalities, bounds on contraction\ncoefficients and maximal correlation, and the convergence rate to stationarity\nof a type of discrete-time Markov chains.",
    "published_date": "2020-04-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11197v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.11171v1",
    "title": "Multi-task closed-loop inverse kinematics stability through semidefinite programming",
    "authors": [
      "Josep Marti-Saumell",
      "Angel Santamaria-Navarro",
      "Carlos Ocampo-Martinez",
      "Juan Andrade-Cetto"
    ],
    "author_ids": [],
    "abstract": "Today's complex robotic designs comprise in some cases a large number of\ndegrees of freedom, enabling for multi-objective task resolution (e.g.,\nhumanoid robots or aerial manipulators). This paper tackles the stability\nproblem of a hierarchical losed-loop inverse kinematics algorithm for such\nhighly redundant robots. We present a method to guarantee system stability by\nperforming an online tuning of the closedloop control gains. We define a\nsemi-definite programming problem (SDP) with these gains as decision variables\nand a discrete-time Lyapunov stability condition as a linear matrix inequality,\nconstraining the SDP optimization problem and guaranteeing the stability of the\nprioritized tasks. To the best of authors' knowledge, this work represents the\nfirst mathematical development of an SDP formulation that introduces stability\nconditions for a multi-objective closed-loop inverse kinematic problem for\nhighly redundant robots. The validity of the proposed approach is demonstrated\nthrough simulation case studies, including didactic examples and a Matlab\ntoolbox for the benefit of the community.",
    "published_date": "2020-04-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11171v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.10998v1",
    "title": "Cross-ethnicity Face Anti-spoofing Recognition Challenge: A Review",
    "authors": [
      "Ajian Liu",
      "Xuan Li",
      "Jun Wan",
      "Sergio Escalera",
      "Hugo Jair Escalante",
      "Meysam Madadi",
      "Yi Jin",
      "Zhuoyuan Wu",
      "Xiaogang Yu",
      "Zichang Tan",
      "Qi Yuan",
      "Ruikun Yang",
      "Benjia Zhou",
      "Guodong Guo",
      "Stan Z. Li"
    ],
    "author_ids": [],
    "abstract": "Face anti-spoofing is critical to prevent face recognition systems from a\nsecurity breach. The biometrics community has %possessed achieved impressive\nprogress recently due the excellent performance of deep neural networks and the\navailability of large datasets. Although ethnic bias has been verified to\nseverely affect the performance of face recognition systems, it still remains\nan open research problem in face anti-spoofing. Recently, a multi-ethnic face\nanti-spoofing dataset, CASIA-SURF CeFA, has been released with the goal of\nmeasuring the ethnic bias. It is the largest up to date cross-ethnicity face\nanti-spoofing dataset covering $3$ ethnicities, $3$ modalities, $1,607$\nsubjects, 2D plus 3D attack types, and the first dataset including explicit\nethnic labels among the recently released datasets for face anti-spoofing. We\norganized the Chalearn Face Anti-spoofing Attack Detection Challenge which\nconsists of single-modal (e.g., RGB) and multi-modal (e.g., RGB, Depth,\nInfrared (IR)) tracks around this novel resource to boost research aiming to\nalleviate the ethnic bias. Both tracks have attracted $340$ teams in the\ndevelopment stage, and finally 11 and 8 teams have submitted their codes in the\nsingle-modal and multi-modal face anti-spoofing recognition challenges,\nrespectively. All the results were verified and re-ran by the organizing team,\nand the results were used for the final ranking. This paper presents an\noverview of the challenge, including its design, evaluation protocol and a\nsummary of results. We analyze the top ranked solutions and draw conclusions\nderived from the competition. In addition we outline future work directions.",
    "published_date": "2020-04-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10998v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.10846v4",
    "title": "Reducing the Filtering Effect in Public School Admissions: A Bias-aware Analysis for Targeted Interventions",
    "authors": [
      "Yuri Faenza",
      "Swati Gupta",
      "Aapeli Vuorinen",
      "Xuan Zhang"
    ],
    "author_ids": [],
    "abstract": "Problem definition: Traditionally, New York City's top 8 public schools have\nselected candidates solely based on their scores in the Specialized High School\nAdmissions Test (SHSAT). These scores are known to be impacted by socioeconomic\nstatus of students and test preparation received in middle schools, leading to\na massive filtering effect in the education pipeline. The classical mechanisms\nfor assigning students to schools do not naturally address problems like school\nsegregation and class diversity, which have worsened over the years. The\nscientific community, including policymakers, have reacted by incorporating\ngroup-specific quotas and proportionality constraints, with mixed results. The\nproblem of finding effective and fair methods for broadening access to\ntop-notch education is still unsolved.\n  Methodology/results: We take an operations approach to the problem different\nfrom most established literature, with the goal of increasing opportunities for\nstudents with high economic needs. Using data from the Department of Education\n(DOE) in New York City, we show that there is a shift in the distribution of\nscores obtained by students that the DOE classifies as \"disadvantaged\"\n(following criteria mostly based on economic factors). We model this shift as a\n\"bias\" that results from an underestimation of the true potential of\ndisadvantaged students. We analyze the impact this bias has on an assortative\nmatching market. We show that centrally planned interventions can significantly\nreduce the impact of bias through scholarships or training, when they target\nthe segment of disadvantaged students with average performance.",
    "published_date": "2020-04-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10846v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.10708v2",
    "title": "Geometric distinguishability measures limit quantum channel estimation and discrimination",
    "authors": [
      "Vishal Katariya",
      "Mark M. Wilde"
    ],
    "author_ids": [],
    "abstract": "Quantum channel estimation and discrimination are fundamentally related\ninformation processing tasks of interest in quantum information science. In\nthis paper, we analyze these tasks by employing the right logarithmic\nderivative Fisher information and the geometric R\\'enyi relative entropy,\nrespectively, and we also identify connections between these distinguishability\nmeasures. A key result of our paper is that a chain-rule property holds for the\nright logarithmic derivative Fisher information and the geometric R\\'enyi\nrelative entropy for the interval $\\alpha\\in(0,1) $ of the R\\'enyi parameter\n$\\alpha$. In channel estimation, these results imply a condition for the\nunattainability of Heisenberg scaling, while in channel discrimination, they\nlead to improved bounds on error rates in the Chernoff and Hoeffding error\nexponent settings. More generally, we introduce the amortized quantum Fisher\ninformation as a conceptual framework for analyzing general sequential\nprotocols that estimate a parameter encoded in a quantum channel, and we use\nthis framework, beyond the aforementioned application, to show that Heisenberg\nscaling is not possible when a parameter is encoded in a classical-quantum\nchannel. We then identify a number of other conceptual and technical\nconnections between the tasks of estimation and discrimination and the\ndistinguishability measures involved in analyzing each. As part of this work,\nwe present a detailed overview of the geometric R\\'enyi relative entropy of\nquantum states and channels, as well as its properties, which may be of\nindependent interest.",
    "published_date": "2020-04-22T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10708v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.11246v2",
    "title": "SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning",
    "authors": [
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez",
      "Manuel Cebrian",
      "Nick Obradovich",
      "Iyad Rahwan"
    ],
    "author_ids": [],
    "abstract": "We propose a discrimination-aware learning method to improve both accuracy\nand fairness of biased face recognition algorithms. The most popular face\nrecognition benchmarks assume a distribution of subjects without paying much\nattention to their demographic attributes. In this work, we perform a\ncomprehensive discrimination-aware experimentation of deep learning-based face\nrecognition. We also propose a general formulation of algorithmic\ndiscrimination with application to face biometrics. The experiments include\ntree popular face recognition models and three public databases composed of\n64,000 identities from different demographic groups characterized by gender and\nethnicity. We experimentally show that learning processes based on the most\nused face databases have led to popular pre-trained deep face models that\npresent a strong algorithmic discrimination. We finally propose a\ndiscrimination-aware learning method, Sensitive Loss, based on the popular\ntriplet loss function and a sensitive triplet generator. Our approach works as\nan add-on to pre-trained networks and is used to improve their performance in\nterms of average accuracy and fairness. The method shows results comparable to\nstate-of-the-art de-biasing networks and represents a step forward to prevent\ndiscriminatory effects by automatic systems.",
    "published_date": "2020-04-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11246v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.10393v1",
    "title": "Alleviating the recommendation bias via rank aggregation",
    "authors": [
      "Qiang Dong",
      "Quan Yuan",
      "Yang-Bo Shi"
    ],
    "author_ids": [],
    "abstract": "The primary goal of a recommender system is often known as \"helping users\nfind relevant items\", and a lot of recommendation algorithms are proposed\naccordingly. However, these accuracy-oriented methods usually suffer the\nproblem of recommendation bias on popular items, which is not welcome to not\nonly users but also item providers. To alleviate the recommendation bias\nproblem, we propose a generic rank aggregation framework for the recommendation\nresults of an existing algorithm, in which the user- and item-oriented ranking\nresults are linearly aggregated together, with a parameter controlling the\nweight of the latter ranking process. Experiment results of a typical algorithm\non two real-world data sets show that, this framework is effective to improve\nthe recommendation fairness of any existing accuracy-oriented algorithms, while\navoiding significant accuracy loss.",
    "published_date": "2020-04-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10393v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.10236v1",
    "title": "Digital tools against COVID-19: Framing the ethical challenges and how to address them",
    "authors": [
      "Urs Gasser",
      "Marcello Ienca",
      "James Scheibner",
      "Joanna Sleigh",
      "Effy Vayena"
    ],
    "author_ids": [],
    "abstract": "Data collection and processing via digital public health technologies are\nbeing promoted worldwide by governments and private companies as strategic\nremedies for mitigating the COVID-19 pandemic and loosening lockdown measures.\nHowever, the ethical and legal boundaries of deploying digital tools for\ndisease surveillance and control purposes are unclear, and a rapidly evolving\ndebate has emerged globally around the promises and risks of mobilizing digital\ntools for public health. To help scientists and policymakers navigate\ntechnological and ethical uncertainty, we present a typology of the primary\ndigital public health applications currently in use. Namely: proximity and\ncontact tracing, symptom monitoring, quarantine control, and flow modeling. For\neach, we discuss context-specific risks, cross-sectional issues, and ethical\nconcerns. Finally, in recognition of the need for practical guidance, we\npropose a navigation aid for policymakers made up of ten steps for the ethical\nuse of digital public health tools.",
    "published_date": "2020-04-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10236v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.10191v1",
    "title": "Implementing AI Ethics in Practice: An Empirical Evaluation of the RESOLVEDD Strategy",
    "authors": [
      "Ville Vakkuri",
      "Kai-Kristian Kemell"
    ],
    "author_ids": [],
    "abstract": "As Artificial Intelligence (AI) systems exert a growing influence on society,\nreal-life incidents begin to underline the importance of AI Ethics. Though\ncalls for more ethical AI systems have been voiced by scholars and the general\npublic alike, few empirical studies on the topic exist. Similarly, few tools\nand methods designed for implementing AI ethics into practice currently exist.\nTo provide empirical data into this on-going discussion, we empirically\nevaluate an existing method from the field of business ethics, the RESOLVEDD\nstrategy, in the context of ethical system development. We evaluated RESOLVEDD\nby means of a multiple case study of five student projects where its use was\ngiven as one of the design requirements for the projects. One of our key\nfindings is that, even though the use of the ethical method was forced upon the\nparticipants, its utilization nonetheless facilitated of ethical consideration\nin the projects. Specifically, it resulted in the developers displaying more\nresponsibility, even though the use of the tool did not stem from intrinsic\nmotivation.",
    "published_date": "2020-04-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10191v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.10163v2",
    "title": "Variable Decomposition for Prophet Inequalities and Optimal Ordering",
    "authors": [
      "Allen Liu",
      "Renato Paes Leme",
      "Martin Pal",
      "Jon Schneider",
      "Balasubramanian Sivan"
    ],
    "author_ids": [],
    "abstract": "We introduce a new decomposition technique for random variables that maps a\ngeneric instance of the prophet inequalities problem to a new instance where\nall but a constant number of variables have a tractable structure that we refer\nto as $(\\varepsilon, \\delta)$-smallness. Using this technique, we make progress\non several outstanding problems in the area:\n  - We show that, even in the case of non-identical distributions, it is\npossible to achieve (arbitrarily close to) the optimal approximation ratio of\n$\\beta \\approx 0.745$ as long as we are allowed to remove a small constant\nnumber of distributions.\n  - We show that for frequent instances of prophet inequalities (where each\ndistribution reoccurs some number of times), it is possible to achieve the\noptimal approximation ratio of $\\beta$ (improving over the previous best-known\nbound of $0.738$).\n  - We give a new, simpler proof of Kertz's optimal approximation guarantee of\n$\\beta \\approx 0.745$ for prophet inequalities with i.i.d. distributions. The\nproof is primal-dual and simultaneously produces upper and lower bounds.\n  - Using this decomposition in combination with a novel convex programming\nformulation, we construct the first Efficient PTAS for the Optimal Ordering\nproblem.",
    "published_date": "2020-04-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10163v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09805v1",
    "title": "AMC-Loss: Angular Margin Contrastive Loss for Improved Explainability in Image Classification",
    "authors": [
      "Hongjun Choi",
      "Anirudh Som",
      "Pavan Turaga"
    ],
    "author_ids": [],
    "abstract": "Deep-learning architectures for classification problems involve the\ncross-entropy loss sometimes assisted with auxiliary loss functions like center\nloss, contrastive loss and triplet loss. These auxiliary loss functions\nfacilitate better discrimination between the different classes of interest.\nHowever, recent studies hint at the fact that these loss functions do not take\ninto account the intrinsic angular distribution exhibited by the low-level and\nhigh-level feature representations. This results in less compactness between\nsamples from the same class and unclear boundary separations between data\nclusters of different classes. In this paper, we address this issue by\nproposing the use of geometric constraints, rooted in Riemannian geometry.\nSpecifically, we propose Angular Margin Contrastive Loss (AMC-Loss), a new loss\nfunction to be used along with the traditional cross-entropy loss. The AMC-Loss\nemploys the discriminative angular distance metric that is equivalent to\ngeodesic distance on a hypersphere manifold such that it can serve a clear\ngeometric interpretation. We demonstrate the effectiveness of AMC-Loss by\nproviding quantitative and qualitative results. We find that although the\nproposed geometrically constrained loss-function improves quantitative results\nmodestly, it has a qualitatively surprisingly beneficial effect on increasing\nthe interpretability of deep-net decisions as seen by the visual explanations\ngenerated by techniques such as the Grad-CAM. Our code is available at\nhttps://github.com/hchoi71/AMC-Loss.",
    "published_date": "2020-04-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09805v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09784v1",
    "title": "An O(log log m) Prophet Inequality for Subadditive Combinatorial Auctions",
    "authors": [
      "Paul Dütting",
      "Thomas Kesselheim",
      "Brendan Lucier"
    ],
    "author_ids": [],
    "abstract": "Prophet inequalities compare the expected performance of an online algorithm\nfor a stochastic optimization problem to the expected optimal solution in\nhindsight. They are a major alternative to classic worst-case competitive\nanalysis, of particular importance in the design and analysis of simple\n(posted-price) incentive compatible mechanisms with provable approximation\nguarantees.\n  A central open problem in this area concerns subadditive combinatorial\nauctions. Here $n$ agents with subadditive valuation functions compete for the\nassignment of $m$ items. The goal is to find an allocation of the items that\nmaximizes the total value of the assignment. The question is whether there\nexists a prophet inequality for this problem that significantly beats the best\nknown approximation factor of $O(\\log m)$.\n  We make major progress on this question by providing an $O(\\log \\log m)$\nprophet inequality. Our proof goes through a novel primal-dual approach. It is\nalso constructive, resulting in an online policy that takes the form of static\nand anonymous item prices that can be computed in polynomial time given\nappropriate query access to the valuations. As an application of our approach,\nwe construct a simple and incentive compatible mechanism based on posted prices\nthat achieves an $O(\\log \\log m)$ approximation to the optimal revenue for\nsubadditive valuations under an item-independence assumption.",
    "published_date": "2020-04-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09784v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09720v2",
    "title": "Discovering Urban Functional Zones from Biased and Sparse Points of Interests and Sparse Human Activities",
    "authors": [
      "Wen Tang",
      "Alireza Chakeri",
      "Hamid Krim"
    ],
    "author_ids": [],
    "abstract": "With rapid development of socio-economics, the task of discovering functional\nzones becomes critical to better understand the interactions between social\nactivities and spatial locations. In this paper, we propose a framework to\ndiscover the real functional zones from the biased and extremely sparse Point\nof Interests (POIs). To cope with the bias and sparsity of POIs, the unbiased\ninner influences between spatial locations and human activities are introduced\nto learn a balanced and dense latent region representation. In addition, a\nspatial location based clustering method is also included to enrich the spatial\ninformation for latent region representation and enhance the region\nfunctionality consistency for the fine-grained region segmentation. Moreover,\nto properly annotate the various and fine-grained region functionalities, we\nestimate the functionality of the regions and rank them by the differences\nbetween the normalized POI distributions to reduce the inconsistency caused by\nthe fine-grained segmentation. Thus, our whole framework is able to properly\naddress the biased categories in sparse POI data and explore the true\nfunctional zones with a fine-grained level. To validate the proposed framework,\na case study is evaluated by using very large real-world users GPS and POIs\ndata from city of Raleigh. The results demonstrate that the proposed framework\ncan better identify functional zones than the benchmarks, and, therefore,\nenhance understanding of urban structures with a finer granularity under\npractical conditions.",
    "published_date": "2020-04-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09720v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09665v2",
    "title": "Local Clustering with Mean Teacher for Semi-supervised Learning",
    "authors": [
      "Zexi Chen",
      "Benjamin Dutton",
      "Bharathkumar Ramachandra",
      "Tianfu Wu",
      "Ranga Raju Vatsavai"
    ],
    "author_ids": [],
    "abstract": "The Mean Teacher (MT) model of Tarvainen and Valpola has shown favorable\nperformance on several semi-supervised benchmark datasets. MT maintains a\nteacher model's weights as the exponential moving average of a student model's\nweights and minimizes the divergence between their probability predictions\nunder diverse perturbations of the inputs. However, MT is known to suffer from\nconfirmation bias, that is, reinforcing incorrect teacher model predictions. In\nthis work, we propose a simple yet effective method called Local Clustering\n(LC) to mitigate the effect of confirmation bias. In MT, each data point is\nconsidered independent of other points during training; however, data points\nare likely to be close to each other in feature space if they share similar\nfeatures. Motivated by this, we cluster data points locally by minimizing the\npairwise distance between neighboring data points in feature space. Combined\nwith a standard classification cross-entropy objective on labeled data points,\nthe misclassified unlabeled data points are pulled towards high-density regions\nof their correct class with the help of their neighbors, thus improving model\nperformance. We demonstrate on semi-supervised benchmark datasets SVHN and\nCIFAR-10 that adding our LC loss to MT yields significant improvements compared\nto MT and performance comparable to the state of the art in semi-supervised\nlearning.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09665v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09603v1",
    "title": "Echo Chambers on Social Media: A comparative analysis",
    "authors": [
      "Matteo Cinelli",
      "Gianmarco De Francisci Morales",
      "Alessandro Galeazzi",
      "Walter Quattrociocchi",
      "Michele Starnini"
    ],
    "author_ids": [],
    "abstract": "Recent studies have shown that online users tend to select information\nadhering to their system of beliefs, ignore information that does not, and join\ngroups - i.e., echo chambers - around a shared narrative. Although a\nquantitative methodology for their identification is still missing, the\nphenomenon of echo chambers is widely debated both at scientific and political\nlevel. To shed light on this issue, we introduce an operational definition of\necho chambers and perform a massive comparative analysis on more than 1B pieces\nof contents produced by 1M users on four social media platforms: Facebook,\nTwitter, Reddit, and Gab. We infer the leaning of users about controversial\ntopics - ranging from vaccines to abortion - and reconstruct their interaction\nnetworks by analyzing different features, such as shared links domain, followed\npages, follower relationship and commented posts. Our method quantifies the\nexistence of echo-chambers along two main dimensions: homophily in the\ninteraction networks and bias in the information diffusion toward likely-minded\npeers. We find peculiar differences across social media. Indeed, while Facebook\nand Twitter present clear-cut echo chambers in all the observed dataset, Reddit\nand Gab do not. Finally, we test the role of the social media platform on news\nconsumption by comparing Reddit and Facebook. Again, we find support for the\nhypothesis that platforms implementing news feed algorithms like Facebook may\nelicit the emergence of echo-chambers.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09603v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09581v1",
    "title": "Human-Collective Collaborative Site Selection",
    "authors": [
      "Jason R. Cody",
      "Karina A. Roundtree",
      "Julie A. Adams"
    ],
    "author_ids": [],
    "abstract": "Robotic collectives are large groups (at least 50) of locally sensing and\ncommunicating robots that encompass characteristics of swarms and colonies,\nwhose emergent behaviors accomplish complex tasks. Future human-collective\nteams will extend the ability of operators to monitor, respond, and make\ndecisions in disaster response, search and rescue, and environmental monitoring\nproblems. This manuscript evaluates two collective best-of-n decision models\nfor enabling collectives to identify and choose the highest valued target from\na finite set of n targets. Two challenges impede the future use of\nhuman-collective shared decisions: 1) environmental bias reduces collective\ndecision accuracy when poorer targets are easier to evaluate than higher\nquality targets, and 2) little is understood about shared human-collective\ndecision making interaction strategies. The two evaluated collective best-of-n\nmodels include an existing insect colony decision model and an extended\nbias-reducing model that attempts to reduce environmental bias in order to\nimprove accuracy. Collectives using these two strategies are compared\nindependently and as members of human-collective teams. Independently, the\nextended model is slower than the original model, but the extended algorithm is\n57% more accurate in decisions where the optimal option is more difficult to\nevaluate. Human-collective teams using the bias-reducing model require less\noperator influence and achieve 25% higher accuracy with difficult decisions,\nthan the human-collective teams using the original model. Further, a novel\nhuman-collective interaction strategy enables operators to adjust collective\nautonomy while making multiple simultaneous decisions.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09581v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09551v1",
    "title": "Games for Fairness and Interpretability",
    "authors": [
      "Eric Chu",
      "Nabeel Gillani",
      "Sneha Priscilla Makini"
    ],
    "author_ids": [],
    "abstract": "As Machine Learning (ML) systems becomes more ubiquitous, ensuring the fair\nand equitable application of their underlying algorithms is of paramount\nimportance. We argue that one way to achieve this is to proactively cultivate\npublic pressure for ML developers to design and develop fairer algorithms --\nand that one way to cultivate public pressure while simultaneously serving the\ninterests and objectives of algorithm developers is through gameplay. We\npropose a new class of games -- ``games for fairness and interpretability'' --\nas one example of an incentive-aligned approach for producing fairer and more\nequitable algorithms. Games for fairness and interpretability are\ncarefully-designed games with mass appeal. They are inherently engaging,\nprovide insights into how machine learning models work, and ultimately produce\ndata that helps researchers and developers improve their algorithms. We\nhighlight several possible examples of games, their implications for fairness\nand interpretability, how their proliferation could creative positive public\npressure by narrowing the gap between algorithm developers and the general\npublic, and why the machine learning community could benefit from them.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09551v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09456v1",
    "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
    "authors": [
      "Moin Nadeem",
      "Anna Bethke",
      "Siva Reddy"
    ],
    "author_ids": [],
    "abstract": "A stereotype is an over-generalized belief about a particular group of\npeople, e.g., Asians are good at math or Asians are bad drivers. Such beliefs\n(biases) are known to hurt target groups. Since pretrained language models are\ntrained on large real world data, they are known to capture stereotypical\nbiases. In order to assess the adverse effects of these models, it is important\nto quantify the bias captured in them. Existing literature on quantifying bias\nevaluates pretrained language models on a small set of artificially constructed\nbias-assessing sentences. We present StereoSet, a large-scale natural dataset\nin English to measure stereotypical biases in four domains: gender, profession,\nrace, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and\nXLNet on our dataset and show that these models exhibit strong stereotypical\nbiases. We also present a leaderboard with a hidden test set to track the bias\nof future language models at https://stereoset.mit.edu",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09456v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09403v1",
    "title": "Class Distribution Alignment for Adversarial Domain Adaptation",
    "authors": [
      "Wanqi Yang",
      "Tong Ling",
      "Chengmei Yang",
      "Lei Wang",
      "Yinghuan Shi",
      "Luping Zhou",
      "Ming Yang"
    ],
    "author_ids": [],
    "abstract": "Most existing unsupervised domain adaptation methods mainly focused on\naligning the marginal distributions of samples between the source and target\ndomains. This setting does not sufficiently consider the class distribution\ninformation between the two domains, which could adversely affect the reduction\nof domain gap. To address this issue, we propose a novel approach called\nConditional ADversarial Image Translation (CADIT) to explicitly align the class\ndistributions given samples between the two domains. It integrates a\ndiscriminative structure-preserving loss and a joint adversarial generation\nloss. The former effectively prevents undesired label-flipping during the whole\nprocess of image translation, while the latter maintains the joint distribution\nalignment of images and labels. Furthermore, our approach enforces the\nclassification consistence of target domain images before and after adaptation\nto aid the classifier training in both domains. Extensive experiments were\nconducted on multiple benchmark datasets including Digits, Faces, Scenes and\nOffice31, showing that our approach achieved superior classification in the\ntarget domain when compared to the state-of-the-art methods. Also, both\nqualitative and quantitative results well supported our motivation that\naligning the class distributions can indeed improve domain adaptation.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09403v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09293v4",
    "title": "A Social Network Analysis of Occupational Segregation",
    "authors": [
      "I. Sebastian Buhai",
      "Marco J. van der Leij"
    ],
    "author_ids": [],
    "abstract": "We propose an equilibrium interaction model of occupational segregation and\nlabor market inequality between two social groups, generated exclusively\nthrough the documented tendency to refer informal job seekers of identical\n\"social color\". The expected social color homophily in job referrals\nstrategically induces distinct career choices for individuals from different\nsocial groups, which further translates into stable partial occupational\nsegregation equilibria with sustained wage and employment inequality -- in line\nwith observed patterns of racial or gender labor market disparities. Supporting\nthe qualitative analysis with a calibration and simulation exercise, we\nfurthermore show that both first and second best utilitarian social optima\nentail segregation, any integration policy requiring explicit distributional\nconcerns. Our framework highlights that the mere social interaction through\nhomophilous contact networks can be a pivotal channel for the propagation and\npersistence of gender and racial labor market gaps, complementary to long\nstudied mechanisms such as taste or statistical discrimination.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "econ.TH",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09293v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09225v5",
    "title": "Fairness in penalty shootouts: Is it worth using dynamic sequences?",
    "authors": [
      "László Csató",
      "Dóra Gréta Petróczy"
    ],
    "author_ids": [],
    "abstract": "The sequence of moves in a dynamic team tournament may distort the ex-ante\nwinning probabilities and harm efficiency. This paper compares seven soccer\npenalty shootout rules that determine the kicking order, from a theoretical\nperspective. Their fairness is evaluated in a reasonable model of First Mover\nAdvantage. We also discuss the probability of reaching the sudden death stage.\nIn the case of stationary scoring probabilities, dynamic mechanisms are not\nbetter than static rules. However, it is worth compensating the second-mover by\nmaking it the first-mover in the sudden death stage. Our work has the potential\nto impact decision-makers who can guarantee fairer outcomes in dynamic\ntournaments by a carefully chosen sequence of actions.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.GN",
      "q-fin.EC",
      "60J20, 91A80"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09225v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.09199v1",
    "title": "Generative Feature Replay For Class-Incremental Learning",
    "authors": [
      "Xialei Liu",
      "Chenshen Wu",
      "Mikel Menta",
      "Luis Herranz",
      "Bogdan Raducanu",
      "Andrew D. Bagdanov",
      "Shangling Jui",
      "Joost van de Weijer"
    ],
    "author_ids": [],
    "abstract": "Humans are capable of learning new tasks without forgetting previous ones,\nwhile neural networks fail due to catastrophic forgetting between new and\npreviously-learned tasks. We consider a class-incremental setting which means\nthat the task-ID is unknown at inference time. The imbalance between old and\nnew classes typically results in a bias of the network towards the newest ones.\nThis imbalance problem can either be addressed by storing exemplars from\nprevious tasks, or by using image replay methods. However, the latter can only\nbe applied to toy datasets since image generation for complex datasets is a\nhard problem.\n  We propose a solution to the imbalance problem based on generative feature\nreplay which does not require any exemplars. To do this, we split the network\ninto two parts: a feature extractor and a classifier. To prevent forgetting, we\ncombine generative feature replay in the classifier with feature distillation\nin the feature extractor. Through feature generation, our method reduces the\ncomplexity of generative replay and prevents the imbalance problem. Our\napproach is computationally efficient and scalable to large datasets.\nExperiments confirm that our approach achieves state-of-the-art results on\nCIFAR-100 and ImageNet, while requiring only a fraction of the storage needed\nfor exemplar-based continual learning. Code available at\n\\url{https://github.com/xialeiliu/GFR-IL}.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09199v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09164v2",
    "title": "VOC-ReID: Vehicle Re-identification based on Vehicle-Orientation-Camera",
    "authors": [
      "Xiangyu Zhu",
      "Zhenbo Luo",
      "Pei Fu",
      "Xiang Ji"
    ],
    "author_ids": [],
    "abstract": "Vehicle re-identification is a challenging task due to high intra-class\nvariances and small inter-class variances. In this work, we focus on the\nfailure cases caused by similar background and shape. They pose serve bias on\nsimilarity, making it easier to neglect fine-grained information. To reduce the\nbias, we propose an approach named VOC-ReID, taking the triplet\nvehicle-orientation-camera as a whole and reforming background/shape similarity\nas camera/orientation re-identification. At first, we train models for vehicle,\norientation and camera re-identification respectively. Then we use orientation\nand camera similarity as penalty to get final similarity. Besides, we propose a\nhigh performance baseline boosted by bag of tricks and weakly supervised data\naugmentation. Our algorithm achieves the second place in vehicle\nre-identification at the NVIDIA AI City Challenge 2020.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09164v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09079v1",
    "title": "Isotropy and Log-Concave Polynomials: Accelerated Sampling and High-Precision Counting of Matroid Bases",
    "authors": [
      "Nima Anari",
      "Michał Dereziński"
    ],
    "author_ids": [],
    "abstract": "We define a notion of isotropy for discrete set distributions. If $\\mu$ is a\ndistribution over subsets $S$ of a ground set $[n]$, we say that $\\mu$ is in\nisotropic position if $P[e \\in S]$ is the same for all $e\\in [n]$. We design a\nnew approximate sampling algorithm that leverages isotropy for the class of\ndistributions $\\mu$ that have a log-concave generating polynomial; this class\nincludes determinantal point processes, strongly Rayleigh distributions, and\nuniform distributions over matroid bases. We show that when $\\mu$ is in\napproximately isotropic position, the running time of our algorithm depends\npolynomially on the size of the set $S$, and only logarithmically on $n$. When\n$n$ is much larger than the size of $S$, this is significantly faster than\nprior algorithms, and can even be sublinear in $n$. We then show how to\ntransform a non-isotropic $\\mu$ into an equivalent approximately isotropic form\nwith a polynomial-time preprocessing step, accelerating subsequent sampling\ntimes. The main new ingredient enabling our algorithms is a class of negative\ndependence inequalities that may be of independent interest.\n  As an application of our results, we show how to approximately count bases of\na matroid of rank $k$ over a ground set of $n$ elements to within a factor of\n$1+\\epsilon$ in time $ O((n+1/\\epsilon^2)\\cdot poly(k, \\log n))$. This is the\nfirst algorithm that runs in nearly linear time for fixed rank $k$, and\nachieves an inverse polynomially low approximation error.",
    "published_date": "2020-04-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09079v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.08945v1",
    "title": "Exploring Racial Bias within Face Recognition via per-subject Adversarially-Enabled Data Augmentation",
    "authors": [
      "Seyma Yucer",
      "Samet Akçay",
      "Noura Al-Moubayed",
      "Toby P. Breckon"
    ],
    "author_ids": [],
    "abstract": "Whilst face recognition applications are becoming increasingly prevalent\nwithin our daily lives, leading approaches in the field still suffer from\nperformance bias to the detriment of some racial profiles within society. In\nthis study, we propose a novel adversarial derived data augmentation\nmethodology that aims to enable dataset balance at a per-subject level via the\nuse of image-to-image transformation for the transfer of sensitive racial\ncharacteristic facial features. Our aim is to automatically construct a\nsynthesised dataset by transforming facial images across varying racial\ndomains, while still preserving identity-related features, such that racially\ndependant features subsequently become irrelevant within the determination of\nsubject identity. We construct our experiments on three significant face\nrecognition variants: Softmax, CosFace and ArcFace loss over a common\nconvolutional neural network backbone. In a side-by-side comparison, we show\nthe positive impact our proposed technique can have on the recognition\nperformance for (racial) minority groups within an originally imbalanced\ntraining dataset by reducing the pre-race variance in performance.",
    "published_date": "2020-04-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08945v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08892v2",
    "title": "The Moral Burden of Ambiguity Aversion",
    "authors": [
      "Brian Jabarian"
    ],
    "author_ids": [],
    "abstract": "In their article, \"Egalitarianism under Severe Uncertainty\", Philosophy and\nPublic Affairs, 46:3, 2018, Thomas Rowe and Alex Voorhoeve develop an original\nmoral decision theory for cases under uncertainty, called \"pluralist\negalitarianism under uncertainty\". In this paper, I firstly sketch their views\nand arguments. I then elaborate on their moral decision theory by discussing\nhow it applies to choice scenarios in health ethics. Finally, I suggest a new\ntwo-stage Ellsberg thought experiment challenging the core of the principle of\ntheir theory. In such an experiment pluralist egalitarianism seems to suggest\nthe wrong, morally and rationally speaking, course of action -- no matter\nwhether I consider my thought experiment in a simultaneous or a sequential\nsetting.",
    "published_date": "2020-04-19T00:00:00",
    "year": 2020,
    "categories": [
      "econ.TH",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08892v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08871v1",
    "title": "A Dimension-Reduction Model for Brittle Fractures on Thin Shells with Mesh Adaptivity",
    "authors": [
      "Stefano Almi",
      "Sandro Belz",
      "Stefano Micheletti",
      "Simona Perotto"
    ],
    "author_ids": [],
    "abstract": "In this paper we derive a new two-dimensional brittle fracture model for thin\nshells via dimension reduction, where the admissible displacements are only\nnormal to the shell surface. The main steps include to endow the shell with a\nsmall thickness, to express the three-dimensional energy in terms of the\nvariational model of brittle fracture in linear elasticity, and to study the\n$\\Gamma$-limit of the functional as the thickness tends to zero. The numerical\ndiscretization is tackled by first approximating the fracture through a phase\nfield, following an Ambrosio-Tortorelli like approach, and then resorting to an\nalternating minimization procedure, where the irreversibility of the crack\npropagation is rigorously imposed via an inequality constraint. The\nminimization is enriched with an anisotropic mesh adaptation driven by an a\nposteriori error estimator, which allows us to sharply track the whole crack\npath by optimizing the shape, the size, and the orientation of the mesh\nelements. Finally, the overall algorithm is successfully assessed on two\nRiemannian settings and proves not to bias the crack propagation.",
    "published_date": "2020-04-19T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08871v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.08634v4",
    "title": "An Accelerated Newton-Dinkelbach Method and its Application to Two Variables Per Inequality Systems",
    "authors": [
      "Daniel Dadush",
      "Zhuan Khye Koh",
      "Bento Natura",
      "László A. Végh"
    ],
    "author_ids": [],
    "abstract": "We present an accelerated, or 'look-ahead' version of the Newton-Dinkelbach\nmethod, a well-known technique for solving fractional and parametric\noptimization problems. This acceleration halves the Bregman divergence between\nthe current iterate and the optimal solution within every two iterations. Using\nthe Bregman divergence as a potential in conjunction with combinatorial\narguments, we obtain strongly polynomial algorithms in three applications\ndomains: (i) For linear fractional combinatorial optimization, we show a\nconvergence bound of $O(m \\log m)$ iterations; the previous best bound was\n$O(m^2 \\log m)$ by Wang et al. (2006). (ii) We obtain a strongly polynomial\nlabel-correcting algorithm for solving linear feasibility systems with two\nvariables per inequality (2VPI). For a 2VPI system with $n$ variables and $m$\nconstraints, our algorithm runs in $O(mn)$ iterations. Every iteration takes\n$O(mn)$ time for general 2VPI systems, and $O(m + n \\log n)$ time for the\nspecial case of deterministic Markov Decision Processes (DMDPs). This extends\nand strengthens a previous result by Madani (2002) that showed a weakly\npolynomial bound for a variant of the Newton-Dinkelbach method for solving\nDMDPs. (iii) We give a simplified variant of the parametric submodular function\nminimization result by Goemans et al. (2017).",
    "published_date": "2020-04-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08634v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.08581v1",
    "title": "Are You A Risk Taker? Adversarial Learning of Asymmetric Cross-Domain Alignment for Risk Tolerance Prediction",
    "authors": [
      "Zhe Liu",
      "Lina Yao",
      "Xianzhi Wang",
      "Lei Bai",
      "Jake An"
    ],
    "author_ids": [],
    "abstract": "Most current studies on survey analysis and risk tolerance modelling lack\nprofessional knowledge and domain-specific models. Given the effectiveness of\ngenerative adversarial learning in cross-domain information, we design an\nAsymmetric cross-Domain Generative Adversarial Network (ADGAN) for domain scale\ninequality. ADGAN utilizes the information-sufficient domain to provide extra\ninformation to improve the representation learning on the\ninformation-insufficient domain via domain alignment. We provide data analysis\nand user model on two data sources: Consumer Consumption Information and Survey\nInformation. We further test ADGAN on a real-world dataset with view embedding\nstructures and show ADGAN can better deal with the class imbalance and\nunqualified data space than state-of-the-art, demonstrating the effectiveness\nof leveraging asymmetrical domain information.",
    "published_date": "2020-04-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08581v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08543v1",
    "title": "Prove Costa's Entropy Power Inequality and High Order Inequality for Differential Entropy with Semidefinite Programming",
    "authors": [
      "Laigang Guo",
      "Chun-Ming Yuan",
      "Xiao-Shan Gao"
    ],
    "author_ids": [],
    "abstract": "Costa's entropy power inequality is an important generalization of Shannon's\nentropy power inequality. Related with Costa's entropy power inequality and a\nconjecture proposed by McKean in 1966, Cheng-Geng recently conjectured that\n$D(m,n): (-1)^{m+1}(\\partial^m/\\partial^m t)H(X_t)\\ge0$, where $X_t$ is the\n$n$-dimensional random variable in Costa's entropy power inequality and\n$H(X_t)$ the differential entropy of $X_t$. $D(1,n)$ and $D(2,n)$ were proved\nby Costa as consequences of Costa's entropy power inequality. Cheng-Geng proved\n$D(3,1)$ and $D(4,1)$. In this paper, we propose a systematical procedure to\nprove $D(m,n)$ and Costa's entropy power inequality based on semidefinite\nprogramming. Using software packages based on this procedure, we prove $D(3,n)$\nfor $n=2,3,4$ and give a new proof for Costa's entropy power inequality. We\nalso show that with the currently known constraints, $D(5,1)$ and $D(4,2)$\ncannot be proved with the procedure.",
    "published_date": "2020-04-18T00:00:00",
    "year": 2020,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08543v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.08520v1",
    "title": "Wireless Power Transmitter Deployment for Balancing Fairness and Charging Service Quality",
    "authors": [
      "Mingqing Liu",
      "Gang Wang",
      "Georgios B. Giannakis",
      "Mingliang Xiong",
      "Qingwen Liu",
      "Hao Deng"
    ],
    "author_ids": [],
    "abstract": "Wireless Energy Transfer (WET) has recently emerged as an appealing solution\nfor power supplying mobile / Internet of Things (IoT) devices. As an enabling\nWET technology, Resonant Beam Charging (RBC) is well-documented for its\nlong-range, high-power, and safe \"WiFi-like\" mobile power supply. To provide\nhigh-quality wireless charging services for multi-user in a given region, we\nformulate a deployment problem of multiple RBC transmitters for balancing the\ncharging fairness and quality of charging service. Based on the RBC\ntransmitter's coverage model and receiver's charging / discharging model, a\nGenetic Algorithm (GA)-based and a Particle Swarm Optimization (PSO)-based\nscheme are put forth to resolve the above issue. Moreover, we present a\nscheduling method to evaluate the performance of the proposed algorithms.\nNumerical results corroborate that the optimized deployment schemes outperform\nuniform and random deployment in 10%-20% charging efficiency improvement.",
    "published_date": "2020-04-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.ET",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08520v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.08377v2",
    "title": "ECCOLA -- a Method for Implementing Ethically Aligned AI Systems",
    "authors": [
      "Ville Vakkuri",
      "Kai-Kristian Kemell",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "Various recent Artificial Intelligence (AI) system failures, some of which\nhave made the global headlines, have highlighted issues in these systems. These\nfailures have resulted in calls for more ethical AI systems that better take\ninto account their effects on various stakeholders. However, implementing AI\nethics into practice is still an on-going challenge. High-level guidelines for\ndoing so exist, devised by governments and private organizations alike, but\nlack practicality for developers. To address this issue, in this paper, we\npresent a method for implementing AI ethics. The method, ECCOLA, has been\niteratively developed using a cyclical action design research approach. The\nmethod aims at making the high-level AI ethics principles more practical,\nmaking it possible for developers to more easily implement them in practice.",
    "published_date": "2020-04-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08377v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08361v2",
    "title": "Unsupervised Discovery of Implicit Gender Bias",
    "authors": [
      "Anjalie Field",
      "Yulia Tsvetkov"
    ],
    "author_ids": [],
    "abstract": "Despite their prevalence in society, social biases are difficult to identify,\nprimarily because human judgements in this domain can be unreliable. We take an\nunsupervised approach to identifying gender bias against women at a comment\nlevel and present a model that can surface text likely to contain bias. Our\nmain challenge is forcing the model to focus on signs of implicit bias, rather\nthan other artifacts in the data. Thus, our methodology involves reducing the\ninfluence of confounds through propensity matching and adversarial learning.\nOur analysis shows how biased comments directed towards female politicians\ncontain mixed criticisms, while comments directed towards other female public\nfigures focus on appearance and sexualization. Ultimately, our work offers a\nway to capture subtle biases in various domains without relying on subjective\nhuman judgements.",
    "published_date": "2020-04-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08361v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08352v2",
    "title": "Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging",
    "authors": [
      "Vineet Mehta",
      "Abhinav Dhall",
      "Sujata Pal",
      "Shehroz S. Khan"
    ],
    "author_ids": [],
    "abstract": "Automatic fall detection is a vital technology for ensuring the health and\nsafety of people. Home-based camera systems for fall detection often put\npeople's privacy at risk. Thermal cameras can partially or fully obfuscate\nfacial features, thus preserving the privacy of a person. Another challenge is\nthe less occurrence of falls in comparison to the normal activities of daily\nliving. As fall occurs rarely, it is non-trivial to learn algorithms due to\nclass imbalance. To handle these problems, we formulate fall detection as an\nanomaly detection within an adversarial framework using thermal imaging. We\npresent a novel adversarial network that comprises of two-channel 3D\nconvolutional autoencoders which reconstructs the thermal data and the optical\nflow input sequences respectively. We introduce a technique to track the region\nof interest, a region-based difference constraint, and a joint discriminator to\ncompute the reconstruction error. A larger reconstruction error indicates the\noccurrence of a fall. The experiments on a publicly available thermal fall\ndataset show the superior results obtained compared to the standard baseline.",
    "published_date": "2020-04-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08352v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08134v1",
    "title": "Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction",
    "authors": [
      "Christoph Alt",
      "Aleksandra Gabryszak",
      "Leonhard Hennig"
    ],
    "author_ids": [],
    "abstract": "Despite the recent progress, little is known about the features captured by\nstate-of-the-art neural relation extraction (RE) models. Common methods encode\nthe source sentence, conditioned on the entity mentions, before classifying the\nrelation. However, the complexity of the task makes it difficult to understand\nhow encoder architecture and supporting linguistic knowledge affect the\nfeatures learned by the encoder. We introduce 14 probing tasks targeting\nlinguistic properties relevant to RE, and we use them to study representations\nlearned by more than 40 different encoder architecture and linguistic feature\ncombinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find\nthat the bias induced by the architecture and the inclusion of linguistic\nfeatures are clearly expressed in the probing task performance. For example,\nadding contextualized word representations greatly increases performance on\nprobing tasks with a focus on named entity and part-of-speech information, and\nyields better results in RE. In contrast, entity masking improves RE, but\nconsiderably lowers performance on entity type related probing tasks.",
    "published_date": "2020-04-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08134v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.08083v4",
    "title": "Meta-Meta Classification for One-Shot Learning",
    "authors": [
      "Arkabandhu Chowdhury",
      "Dipak Chaudhari",
      "Swarat Chaudhuri",
      "Chris Jermaine"
    ],
    "author_ids": [],
    "abstract": "We present a new approach, called meta-meta classification, to learning in\nsmall-data settings. In this approach, one uses a large set of learning\nproblems to design an ensemble of learners, where each learner has high bias\nand low variance and is skilled at solving a specific type of learning problem.\nThe meta-meta classifier learns how to examine a given learning problem and\ncombine the various learners to solve the problem. The meta-meta learning\napproach is especially suited to solving few-shot learning tasks, as it is\neasier to learn to classify a new learning problem with little data than it is\nto apply a learning algorithm to a small data set. We evaluate the approach on\na one-shot, one-class-versus-all classification task and show that it is able\nto outperform traditional meta-learning as well as ensembling approaches.",
    "published_date": "2020-04-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.08083v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07999v4",
    "title": "REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets",
    "authors": [
      "Angelina Wang",
      "Alexander Liu",
      "Ryan Zhang",
      "Anat Kleiman",
      "Leslie Kim",
      "Dora Zhao",
      "Iroha Shirai",
      "Arvind Narayanan",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are known to perpetuate and even amplify the biases\npresent in the data. However, these data biases frequently do not become\napparent until after the models are deployed. Our work tackles this issue and\nenables the preemptive analysis of large-scale datasets. REVISE (REvealing\nVIsual biaSEs) is a tool that assists in the investigation of a visual dataset,\nsurfacing potential biases along three dimensions: (1) object-based, (2)\nperson-based, and (3) geography-based. Object-based biases relate to the size,\ncontext, or diversity of the depicted objects. Person-based metrics focus on\nanalyzing the portrayal of people within the dataset. Geography-based analyses\nconsider the representation of different geographic locations. These three\ndimensions are deeply intertwined in how they interact to bias a dataset, and\nREVISE sheds light on this; the responsibility then lies with the user to\nconsider the cultural and historical context, and to determine which of the\nrevealed biases may be problematic. The tool further assists the user by\nsuggesting actionable steps that may be taken to mitigate the revealed biases.\nOverall, the key aim of our work is to tackle the machine learning bias problem\nearly in the pipeline. REVISE is available at\nhttps://github.com/princetonvisualai/revise-tool",
    "published_date": "2020-04-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07999v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07790v5",
    "title": "Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training",
    "authors": [
      "Joe Stacey",
      "Pasquale Minervini",
      "Haim Dubossarsky",
      "Sebastian Riedel",
      "Tim Rocktäschel"
    ],
    "author_ids": [],
    "abstract": "Natural Language Inference (NLI) datasets contain annotation artefacts\nresulting in spurious correlations between the natural language utterances and\ntheir respective entailment classes. These artefacts are exploited by neural\nnetworks even when only considering the hypothesis and ignoring the premise,\nleading to unwanted biases. Belinkov et al. (2019b) proposed tackling this\nproblem via adversarial training, but this can lead to learned sentence\nrepresentations that still suffer from the same biases. We show that the bias\ncan be reduced in the sentence representations by using an ensemble of\nadversaries, encouraging the model to jointly decrease the accuracy of these\ndifferent adversaries while fitting the data. This approach produces more\nrobust NLI models, outperforming previous de-biasing efforts when generalised\nto 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In\naddition, we find that the optimal number of adversarial classifiers depends on\nthe dimensionality of the sentence representations, with larger sentence\nrepresentations being more difficult to de-bias while benefiting from using a\ngreater number of adversaries.",
    "published_date": "2020-04-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07790v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07682v1",
    "title": "On the use of Benford's law to detect GAN-generated images",
    "authors": [
      "Nicolò Bonettini",
      "Paolo Bestagini",
      "Simone Milani",
      "Stefano Tubaro"
    ],
    "author_ids": [],
    "abstract": "The advent of Generative Adversarial Network (GAN) architectures has given\nanyone the ability of generating incredibly realistic synthetic imagery. The\nmalicious diffusion of GAN-generated images may lead to serious social and\npolitical consequences (e.g., fake news spreading, opinion formation, etc.). It\nis therefore important to regulate the widespread distribution of synthetic\nimagery by developing solutions able to detect them. In this paper, we study\nthe possibility of using Benford's law to discriminate GAN-generated images\nfrom natural photographs. Benford's law describes the distribution of the most\nsignificant digit for quantized Discrete Cosine Transform (DCT) coefficients.\nExtending and generalizing this property, we show that it is possible to\nextract a compact feature vector from an image. This feature vector can be fed\nto an extremely simple classifier for GAN-generated image detection purpose.",
    "published_date": "2020-04-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07682v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07678v1",
    "title": "Online Social Deception and Its Countermeasures for Trustworthy Cyberspace: A Survey",
    "authors": [
      "Zhen Guo",
      "Jin-Hee Cho",
      "Ing-Ray Chen",
      "Srijan Sengupta",
      "Michin Hong",
      "Tanushree Mitra"
    ],
    "author_ids": [],
    "abstract": "We are living in an era when online communication over social network\nservices (SNSs) have become an indispensable part of people's everyday lives.\nAs a consequence, online social deception (OSD) in SNSs has emerged as a\nserious threat in cyberspace, particularly for users vulnerable to such\ncyberattacks. Cyber attackers have exploited the sophisticated features of SNSs\nto carry out harmful OSD activities, such as financial fraud, privacy threat,\nor sexual/labor exploitation. Therefore, it is critical to understand OSD and\ndevelop effective countermeasures against OSD for building a trustworthy SNSs.\nIn this paper, we conducted an extensive survey, covering (i) the\nmultidisciplinary concepts of social deception; (ii) types of OSD attacks and\ntheir unique characteristics compared to other social network attacks and\ncybercrimes; (iii) comprehensive defense mechanisms embracing prevention,\ndetection, and response (or mitigation) against OSD attacks along with their\npros and cons; (iv) datasets/metrics used for validation and verification; and\n(v) legal and ethical concerns related to OSD research. Based on this survey,\nwe provide insights into the effectiveness of countermeasures and the lessons\nfrom existing literature. We conclude this survey paper with an in-depth\ndiscussions on the limitations of the state-of-the-art and recommend future\nresearch directions in this area.",
    "published_date": "2020-04-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07678v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.07667v2",
    "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
    "authors": [
      "Shauli Ravfogel",
      "Yanai Elazar",
      "Hila Gonen",
      "Michael Twiton",
      "Yoav Goldberg"
    ],
    "author_ids": [],
    "abstract": "The ability to control for the kinds of information encoded in neural\nrepresentation has a variety of use cases, especially in light of the challenge\nof interpreting these models. We present Iterative Null-space Projection\n(INLP), a novel method for removing information from neural representations.\nOur method is based on repeated training of linear classifiers that predict a\ncertain property we aim to remove, followed by projection of the\nrepresentations on their null-space. By doing so, the classifiers become\noblivious to that target property, making it hard to linearly separate the data\naccording to it. While applicable for multiple uses, we evaluate our method on\nbias and fairness use-cases, and show that our method is able to mitigate bias\nin word embeddings, as well as to increase fairness in a setting of multi-class\nclassification.",
    "published_date": "2020-04-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07667v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07657v4",
    "title": "Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm",
    "authors": [
      "Muhammad Zaigham Zaheer",
      "Jin-ha Lee",
      "Marcella Astrid",
      "Seung-Ik Lee"
    ],
    "author_ids": [],
    "abstract": "A popular method for anomaly detection is to use the generator of an\nadversarial network to formulate anomaly scores over reconstruction loss of\ninput. Due to the rare occurrence of anomalies, optimizing such networks can be\na cumbersome task. Another possible approach is to use both generator and\ndiscriminator for anomaly detection. However, attributed to the involvement of\nadversarial training, this model is often unstable in a way that the\nperformance fluctuates drastically with each training step. In this study, we\npropose a framework that effectively generates stable results across a wide\nrange of training steps and allows us to use both the generator and the\ndiscriminator of an adversarial model for efficient and robust anomaly\ndetection. Our approach transforms the fundamental role of a discriminator from\nidentifying real and fake data to distinguishing between good and bad quality\nreconstructions. To this end, we prepare training examples for the good quality\nreconstruction by employing the current generator, whereas poor quality\nexamples are obtained by utilizing an old state of the same generator. This\nway, the discriminator learns to detect subtle distortions that often appear in\nreconstructions of the anomaly inputs. Extensive experiments performed on\nCaltech-256 and MNIST image datasets for novelty detection show superior\nresults. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our\nmodel achieves a frame-level AUC of 98.1%, surpassing recent state-of-the-art\nmethods.",
    "published_date": "2020-04-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07657v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07376v4",
    "title": "COVID-19 Antibody Test / Vaccination Certification: There's an app for that",
    "authors": [
      "Marc Eisenstadt",
      "Manoharan Ramachandran",
      "Niaz Chowdhury",
      "Allan Third",
      "John Domingue"
    ],
    "author_ids": [],
    "abstract": "Goal: As the Coronavirus Pandemic of 2019/2020 unfolds, a COVID-19 'Immunity\nPassport' has been mooted as a way to enable individuals to return back to\nwork. While the quality of antibody testing, the availability of vaccines, and\nthe likelihood of even attaining COVID-19 immunity continue to be researched,\nwe address the issues involved in providing tamper-proof and privacy-preserving\ncertification for test results and vaccinations. Methods: We developed a\nprototype mobile phone app and requisite decentralized server architecture that\nfacilitates instant verification of tamper-proof test results. Personally\nidentifiable information is only stored at the user's discretion, and the app\nallows the end-user selectively to present only the specific test result with\nno other personal information revealed. The architecture, designed for\nscalability, relies upon (a) the 2019 World Wide Web Consortium standard called\n'Verifiable Credentials', (b) Tim Berners-Lee's decentralized personal data\nplatform 'Solid', and (c) a Consortium Ethereum-based blockchain. Results: Our\nmobile phone app and decentralized server architecture enable the mixture of\nverifiability and privacy in a manner derived from public/private key pairs and\ndigital signatures, generalized to avoid restrictive ownership of sensitive\ndigital keys and/or data. Benchmark performance tests show it to scale linearly\nin the worst case, as significant processing is done locally on each app. For\nthe test certificate Holder, Issuer (e.g. healthcare staff, pharmacy) and\nVerifier (e.g. employer), it is 'just another app' which takes only minutes to\nuse. Conclusions: The app and decentralized server architecture offer a\nprototype proof of concept that is readily scalable, applicable generically,\nand in effect 'waiting in the wings' for the biological issues, plus key\nethical issues raised in the discussion section, to be resolved.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY",
      "cs.NI",
      "cs.SI",
      "J.3; K.4.1; E.2; D.4.3; C.5.5; C.2.2; E.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07376v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.07354v1",
    "title": "Efficient Algorithms for Battleship",
    "authors": [
      "Loïc Crombez",
      "Guilherme D. da Fonseca",
      "Yan Gerard"
    ],
    "author_ids": [],
    "abstract": "We consider an algorithmic problem inspired by the Battleship game. In the\nvariant of the problem that we investigate, there is a unique ship of shape $S\n\\subset Z^2$ which has been translated in the lattice $Z^2$. We assume that a\nplayer has already hit the ship with a first shot and the goal is to sink the\nship using as few shots as possible, that is, by minimizing the number of\nmissed shots. While the player knows the shape $S$, which position of $S$ has\nbeen hit is not known.\n  Given a shape $S$ of $n$ lattice points, the minimum number of misses that\ncan be achieved in the worst case by any algorithm is called the Battleship\ncomplexity of the shape $S$ and denoted $c(S)$. We prove three bounds on\n$c(S)$, each considering a different class of shapes. First, we have $c(S) \\leq\nn-1$ for arbitrary shapes and the bound is tight for parallelogram-free shapes.\nSecond, we provide an algorithm that shows that $c(S) = O(\\log n)$ if $S$ is an\nHV-convex polyomino. Third, we provide an algorithm that shows that $c(S) =\nO(\\log \\log n)$ if $S$ is a digital convex set. This last result is obtained\nthrough a novel discrete version of the Blaschke-Lebesgue inequality relating\nthe area and the width of any convex body.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07354v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.07301v1",
    "title": "ESResNet: Environmental Sound Classification Based on Visual Domain Models",
    "authors": [
      "Andrey Guzhov",
      "Federico Raue",
      "Jörn Hees",
      "Andreas Dengel"
    ],
    "author_ids": [],
    "abstract": "Environmental Sound Classification (ESC) is an active research area in the\naudio domain and has seen a lot of progress in the past years. However, many of\nthe existing approaches achieve high accuracy by relying on domain-specific\nfeatures and architectures, making it harder to benefit from advances in other\nfields (e.g., the image domain). Additionally, some of the past successes have\nbeen attributed to a discrepancy of how results are evaluated (i.e., on\nunofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall\nprogression of the field.\n  The contribution of this paper is twofold. First, we present a model that is\ninherently compatible with mono and stereo sound inputs. Our model is based on\nsimple log-power Short-Time Fourier Transform (STFT) spectrograms and combines\nthem with several well-known approaches from the image domain (i.e., ResNet,\nSiamese-like networks and attention). We investigate the influence of\ncross-domain pre-training, architectural changes, and evaluate our model on\nstandard datasets. We find that our model out-performs all previously known\napproaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10),\n91.5 % (ESC-50) and 84.2 % / 85.4 % (US8K mono / stereo).\n  Second, we provide a comprehensive overview of the actual state of the field,\nby differentiating several previously reported results on the US8K dataset\nbetween official or unofficial splits. For better reproducibility, our code\n(including any re-implementations) is made available.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07301v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07225v1",
    "title": "Minimizing Interference and Selection Bias in Network Experiment Design",
    "authors": [
      "Zahra Fatemi",
      "Elena Zheleva"
    ],
    "author_ids": [],
    "abstract": "Current approaches to A/B testing in networks focus on limiting interference,\nthe concern that treatment effects can \"spill over\" from treatment nodes to\ncontrol nodes and lead to biased causal effect estimation. Prominent methods\nfor network experiment design rely on two-stage randomization, in which\nsparsely-connected clusters are identified and cluster randomization dictates\nthe node assignment to treatment and control. Here, we show that cluster\nrandomization does not ensure sufficient node randomization and it can lead to\nselection bias in which treatment and control nodes represent different\npopulations of users. To address this problem, we propose a principled\nframework for network experiment design which jointly minimizes interference\nand selection bias. We introduce the concepts of edge spillover probability and\ncluster matching and demonstrate their importance for designing network A/B\ntesting. Our experiments on a number of real-world datasets show that our\nproposed framework leads to significantly lower error in causal effect\nestimation than existing solutions.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07225v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07179v4",
    "title": "Interpretable Probabilistic Password Strength Meters via Deep Learning",
    "authors": [
      "Dario Pasquini",
      "Giuseppe Ateniese",
      "Massimo Bernaschi"
    ],
    "author_ids": [],
    "abstract": "Probabilistic password strength meters have been proved to be the most\naccurate tools to measure password strength. Unfortunately, by construction,\nthey are limited to solely produce an opaque security estimation that fails to\nfully support the user during the password composition. In the present work, we\nmove the first steps towards cracking the intelligibility barrier of this\ncompelling class of meters. We show that probabilistic password meters\ninherently own the capability of describing the latent relation occurring\nbetween password strength and password structure. In our approach, the security\ncontribution of each character composing a password is disentangled and used to\nprovide explicit fine-grained feedback for the user. Furthermore, unlike\nexisting heuristic constructions, our method is free from any human bias, and,\nmore importantly, its feedback has a probabilistic interpretation. In our\ncontribution: (1) we formulate interpretable probabilistic password strength\nmeters; (2) we describe how they can be implemented via an efficient and\nlightweight deep learning framework suitable for client-side operability.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07179v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07173v1",
    "title": "Bias in Multimodal AI: Testbed for Fair Automatic Recruitment",
    "authors": [
      "Alejandro Peña",
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "author_ids": [],
    "abstract": "The presence of decision-making algorithms in society is rapidly increasing\nnowadays, while concerns about their transparency and the possibility of these\nalgorithms becoming new sources of discrimination are arising. In fact, many\nrelevant automated systems have been shown to make decisions based on sensitive\ninformation or discriminate certain social groups (e.g. certain biometric\nsystems for person recognition). With the aim of studying how current\nmultimodal algorithms based on heterogeneous sources of information are\naffected by sensitive elements and inner biases in the data, we propose a\nfictitious automated recruitment testbed: FairCVtest. We train automatic\nrecruitment algorithms using a set of multimodal synthetic profiles consciously\nscored with gender and racial biases. FairCVtest shows the capacity of the\nArtificial Intelligence (AI) behind such recruitment tool to extract sensitive\ninformation from unstructured data, and exploit it in combination to data\nbiases in undesirable (unfair) ways. Finally, we present a list of recent works\ndeveloping techniques capable of removing sensitive information from the\ndecision-making process of deep learning architectures. We have used one of\nthese algorithms (SensitiveNets) to experiment discrimination-aware learning\nfor the elimination of sensitive information in our multimodal AI framework.\nOur methodology and results show how to generate fairer AI-based tools in\ngeneral, and in particular fairer automated recruitment systems.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07173v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.09990v2",
    "title": "A Philosophy of Data",
    "authors": [
      "Alexander M. Mussgnug"
    ],
    "author_ids": [],
    "abstract": "We argue that while this discourse on data ethics is of critical importance,\nit is missing one fundamental point: If more and more efforts in business,\ngovernment, science, and our daily lives are data-driven, we should pay more\nattention to what exactly we are driven by. Therefore, we need more debate on\nwhat fundamental properties constitute data. In the first section of the paper,\nwe work from the fundamental properties necessary for statistical computation\nto a definition of statistical data. We define a statistical datum as the\ncoming together of substantive and numerical properties and differentiate\nbetween qualitative and quantitative data. Subsequently, we qualify our\ndefinition by arguing that for data to be practically useful, it needs to be\ncommensurable in a manner that reveals meaningful differences that allow for\nthe generation of relevant insights through statistical methodologies. In the\nsecond section, we focus on what our conception of data can contribute to the\ndiscourse on data ethics and beyond. First, we hold that the need for useful\ndata to be commensurable rules out an understanding of properties as\nfundamentally unique or equal. Second, we argue that practical concerns lead us\nto increasingly standardize how we operationalize a substantive property; in\nother words, how we formalize the relationship between the substantive and\nnumerical properties of data. Thereby, we also standardize the interpretation\nof a property. With our increasing reliance on data and data technologies,\nthese two characteristics of data affect our collective conception of reality.\nStatistical data's exclusion of the fundamentally unique and equal influences\nour perspective on the world, and the standardization of substantive properties\ncan be viewed as profound ontological practice, entrenching ever more pervasive\ninterpretations of phenomena in our everyday lives.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09990v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.07401v3",
    "title": "Poisoning Attacks on Algorithmic Fairness",
    "authors": [
      "David Solans",
      "Battista Biggio",
      "Carlos Castillo"
    ],
    "author_ids": [],
    "abstract": "Research in adversarial machine learning has shown how the performance of\nmachine learning models can be seriously compromised by injecting even a small\nfraction of poisoning points into the training data. While the effects on model\naccuracy of such poisoning attacks have been widely studied, their potential\neffects on other model performance metrics remain to be evaluated. In this\nwork, we introduce an optimization framework for poisoning attacks against\nalgorithmic fairness, and develop a gradient-based poisoning attack aimed at\nintroducing classification disparities among different groups in the data. We\nempirically show that our attack is effective not only in the white-box\nsetting, in which the attacker has full access to the target model, but also in\na more challenging black-box scenario in which the attacks are optimized\nagainst a substitute model and then transferred to the target model. We believe\nthat our findings pave the way towards the definition of an entirely novel set\nof adversarial attacks targeting algorithmic fairness in different scenarios,\nand that investigating such vulnerabilities will help design more robust\nalgorithms and countermeasures in the future.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07401v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.06861v1",
    "title": "HODET: Hybrid Object DEtection and Tracking using mmWave Radar and Visual Sensors",
    "authors": [
      "Joseph St. Cyr",
      "Joshua Vanderpool",
      "Yu Chen",
      "Xiaohua Li"
    ],
    "author_ids": [],
    "abstract": "Image sensors have been explored heavily in automotive applications for\ncollision avoidance and varying levels of autonomy. It requires a degree of\nbrightness, therefore, the use of an image sensor in nighttime operation or\ndark conditions can be problematic along with challenging weather such as fog.\nRadar sensors have been employed to help cover the various environmental\nchallenges with visible spectrum cameras. Edge computing technology has the\npotential to address a number of issues such as real-time processing\nrequirements, off-loading of processing from congested servers, and size,\nweight, power, and cost (SWaP-C) constraints. This paper proposes a novel\nHybrid Object DEtection and Tracking (HODET) using mmWave Radar and Visual\nSensors at the edge. The HODET is a computing application of low SWaP-C\nelectronics performing object detection, tracking and identification algorithms\nwith the simultaneous use of image and radar sensors. While the machine vision\ncamera alone could estimate the distance of an object, the radar sensor will\nprovide an accurate distance and vector of movement. This additional data\naccuracy can be leveraged to further discriminate a detected object to protect\nagainst spoofing attacks. A real-world smart community public safety monitoring\nscenario is selected to verify the effectiveness of HODET, which detects,\ntracks objects of interests and identify suspicious activities. The\nexperimental results demonstrate the feasibility of the approach.",
    "published_date": "2020-04-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.06861v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.06592v3",
    "title": "InsideBias: Measuring Bias in Deep Networks and Application to Face Gender Biometrics",
    "authors": [
      "Ignacio Serna",
      "Alejandro Peña",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "author_ids": [],
    "abstract": "This work explores the biases in learning processes based on deep neural\nnetwork architectures. We analyze how bias affects deep learning processes\nthrough a toy example using the MNIST database and a case study in gender\ndetection from face images. We employ two gender detection models based on\npopular deep neural networks. We present a comprehensive analysis of bias\neffects when using an unbalanced training dataset on the features learned by\nthe models. We show how bias impacts in the activations of gender detection\nmodels based on face images. We finally propose InsideBias, a novel method to\ndetect biased models. InsideBias is based on how the models represent the\ninformation instead of how they perform, which is the normal practice in other\nexisting methods for bias detection. Our strategy with InsideBias allows to\ndetect biased models with very few samples (only 15 images in our case study).\nOur experiments include 72K face images from 24K identities and 3 ethnic\ngroups.",
    "published_date": "2020-04-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.06592v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.06524v1",
    "title": "Contrastive Examples for Addressing the Tyranny of the Majority",
    "authors": [
      "Viktoriia Sharmanska",
      "Lisa Anne Hendricks",
      "Trevor Darrell",
      "Novi Quadrianto"
    ],
    "author_ids": [],
    "abstract": "Computer vision algorithms, e.g. for face recognition, favour groups of\nindividuals that are better represented in the training data. This happens\nbecause of the generalization that classifiers have to make. It is simpler to\nfit the majority groups as this fit is more important to overall error. We\npropose to create a balanced training dataset, consisting of the original\ndataset plus new data points in which the group memberships are intervened,\nminorities become majorities and vice versa. We show that current generative\nadversarial networks are a powerful tool for learning these data points, called\ncontrastive examples. We experiment with the equalized odds bias measure on\ntabular data as well as image data (CelebA and Diversity in Faces datasets).\nContrastive examples allow us to expose correlations between group membership\nand other seemingly neutral features. Whenever a causal graph is available, we\ncan put those contrastive examples in the perspective of counterfactuals.",
    "published_date": "2020-04-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.06524v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.06003v2",
    "title": "Discrimination of Internal Faults and Other Transients in an Interconnected System with Power Transformers and Phase Angle Regulators",
    "authors": [
      "Pallav Kumar Bera",
      "Can Isik",
      "Vajendra Kumar"
    ],
    "author_ids": [],
    "abstract": "This study solves the problem of accurate detection of internal faults and\nclassification of transients in a 5-bus interconnected system for Phase Angle\nRegulators (PAR) and Power Transformers. The analysis prevents mal-operation of\ndifferential relays in case of transients other than faults which include\nmagnetizing inrush, sympathetic inrush, external faults with CT saturation,\ncapacitor switching, non-linear load switching, and ferroresonance. A gradient\nboosting classifier (GBC) is used to distinguish the internal faults from the\ntransient disturbances based on 1.5 cycles of 3-phase differential currents\nregistered by a change detector. After the detection of an internal fault, GBCs\nare used to locate the faulty unit (Power Transformer, PAR series, or exciting\nunit) and identify the type of fault. In case a transient disturbance is\ndetected, another GBC classifies them into the six transient disturbances. Five\nmost relevant frequency and time domain features obtained using Information\nGain are used to train and test the classifiers. The proposed algorithm\ndistinguishes the internal faults from the other transients with a balanced\naccuracy of 99.95%. The faulty transformer unit is located with a balanced\naccuracy of 99.5% and the different transient disturbances are identified with\na balanced accuracy of 99.3%. Moreover, the reliability of the scheme is\nverified for different rating and connection of the transformers involved, CT\nsaturation, and noise levels in the signals. These GBC classifiers can work\ntogether with a conventional differential relay and offer a supervisory control\nover its operation. PSCAD/EMTDC software is used for simulation of the\ntransients and to develop the two and three-winding transformer models for\ncreating the internal faults including inter-turn and inter-winding faults.",
    "published_date": "2020-04-13T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.06003v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05989v1",
    "title": "Data augmentation using generative networks to identify dementia",
    "authors": [
      "Bahman Mirheidari",
      "Yilin Pan",
      "Daniel Blackburn",
      "Ronan O'Malley",
      "Traci Walker",
      "Annalena Venneri",
      "Markus Reuber",
      "Heidi Christensen"
    ],
    "author_ids": [],
    "abstract": "Data limitation is one of the most common issues in training machine learning\nclassifiers for medical applications. Due to ethical concerns and data privacy,\nthe number of people that can be recruited to such experiments is generally\nsmaller than the number of participants contributing to non-healthcare\ndatasets. Recent research showed that generative models can be used as an\neffective approach for data augmentation, which can ultimately help to train\nmore robust classifiers sparse data domains. A number of studies proved that\nthis data augmentation technique works for image and audio data sets. In this\npaper, we investigate the application of a similar approach to different types\nof speech and audio-based features extracted from interactions recorded with\nour automatic dementia detection system. Using two generative models we show\nhow the generated synthesized samples can improve the performance of a DNN\nbased classifier. The variational autoencoder increased the F-score of a\nfour-way classifier distinguishing the typical patient groups seen in memory\nclinics from 58% to around 74%, a 16% improvement",
    "published_date": "2020-04-13T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05989v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05973v4",
    "title": "Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver Gaze Zone Estimation Dataset",
    "authors": [
      "Shreya Ghosh",
      "Abhinav Dhall",
      "Garima Sharma",
      "Sarthak Gupta",
      "Nicu Sebe"
    ],
    "author_ids": [],
    "abstract": "Labelling of human behavior analysis data is a complex and time consuming\ntask. In this paper, a fully automatic technique for labelling an image based\ngaze behavior dataset for driver gaze zone estimation is proposed. Domain\nknowledge is added to the data recording paradigm and later labels are\ngenerated in an automatic manner using Speech To Text conversion (STT). In\norder to remove the noise in the STT process due to different illumination and\nethnicity of subjects in our data, the speech frequency and energy are\nanalysed. The resultant Driver Gaze in the Wild (DGW) dataset contains 586\nrecordings, captured during different times of the day including evenings. The\nlarge scale dataset contains 338 subjects with an age range of 18-63 years. As\nthe data is recorded in different lighting conditions, an illumination robust\nlayer is proposed in the Convolutional Neural Network (CNN). The extensive\nexperiments show the variance in the dataset resembling real-world conditions\nand the effectiveness of the proposed CNN pipeline. The proposed network is\nalso fine-tuned for the eye gaze prediction task, which shows the\ndiscriminativeness of the representation learnt by our network on the proposed\nDGW dataset. Project Page:\nhttps://sites.google.com/view/drivergazeprediction/home",
    "published_date": "2020-04-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05973v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05887v2",
    "title": "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    "authors": [
      "Maximilian Mozes",
      "Pontus Stenetorp",
      "Bennett Kleinberg",
      "Lewis D. Griffin"
    ],
    "author_ids": [],
    "abstract": "Recent efforts have shown that neural text processing models are vulnerable\nto adversarial examples, but the nature of these examples is poorly understood.\nIn this work, we show that adversarial attacks against CNN, LSTM and\nTransformer-based classification models perform word substitutions that are\nidentifiable through frequency differences between replaced words and their\ncorresponding substitutions. Based on these findings, we propose\nfrequency-guided word substitutions (FGWS), a simple algorithm exploiting the\nfrequency properties of adversarial word substitutions for the detection of\nadversarial examples. FGWS achieves strong performance by accurately detecting\nadversarial examples on the SST-2 and IMDb sentiment datasets, with F1\ndetection scores of up to 91.4% against RoBERTa-based classification models. We\ncompare our approach against a recently proposed perturbation discrimination\nframework and show that we outperform it by up to 13.0% F1.",
    "published_date": "2020-04-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05887v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05793v1",
    "title": "STAS: Adaptive Selecting Spatio-Temporal Deep Features for Improving Bias Correction on Precipitation",
    "authors": [
      "Yiqun Liu",
      "Shouzhen Chen",
      "Lei Chen",
      "Hai Chu",
      "Xiaoyang Xu",
      "Junping Zhang",
      "Leiming Ma"
    ],
    "author_ids": [],
    "abstract": "Numerical Weather Prediction (NWP) can reduce human suffering by predicting\ndisastrous precipitation in time. A commonly-used NWP in the world is the\nEuropean Centre for medium-range weather forecasts (EC). However, it is\nnecessary to correct EC forecast through Bias Correcting on Precipitation\n(BCoP) since we still have not fully understood the mechanism of precipitation,\nmaking EC often have some biases. The existing BCoPs suffers from limited prior\ndata and the fixed Spatio-Temporal (ST) scale. We thus propose an end-to-end\ndeep-learning BCoP model named Spatio-Temporal feature Auto-Selective (STAS)\nmodel to select optimal ST regularity from EC via the ST Feature-selective\nMechanisms (SFM/TFM). Given different input features, these two mechanisms can\nautomatically adjust the spatial and temporal scales for correcting.\nExperiments on an EC public dataset indicate that compared with 8 published\nBCoP methods, STAS shows state-of-the-art performance on several criteria of\nBCoP, named threat scores (TS). Further, ablation studies justify that the\nSFM/TFM indeed work well in boosting the performance of BCoP, especially on the\nheavy precipitation.",
    "published_date": "2020-04-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05793v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05704v4",
    "title": "Visual Grounding Methods for VQA are Working for the Wrong Reasons!",
    "authors": [
      "Robik Shrestha",
      "Kushal Kafle",
      "Christopher Kanan"
    ],
    "author_ids": [],
    "abstract": "Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2.",
    "published_date": "2020-04-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05704v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05563v1",
    "title": "Closing Gaps in Asymptotic Fair Division",
    "authors": [
      "Pasin Manurangsi",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We study a resource allocation setting where $m$ discrete items are to be\ndivided among $n$ agents with additive utilities, and the agents' utilities for\nindividual items are drawn at random from a probability distribution. Since\ncommon fairness notions like envy-freeness and proportionality cannot always be\nsatisfied in this setting, an important question is when allocations satisfying\nthese notions exist. In this paper, we close several gaps in the line of work\non asymptotic fair division. First, we prove that the classical round-robin\nalgorithm is likely to produce an envy-free allocation provided that\n$m=\\Omega(n\\log n/\\log\\log n)$, matching the lower bound from prior work. We\nthen show that a proportional allocation exists with high probability as long\nas $m\\geq n$, while an allocation satisfying envy-freeness up to any item (EFX)\nis likely to be present for any relation between $m$ and $n$. Finally, we\nconsider a related setting where each agent is assigned exactly one item and\nthe remaining items are left unassigned, and show that the transition from\nnon-existence to existence with respect to envy-free assignments occurs at\n$m=en$.",
    "published_date": "2020-04-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.TH",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05563v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.05535v1",
    "title": "Using Photo Modeling Based 3DGRSL to Promote the Sustainability of Geo-Education, a case study from China",
    "authors": [
      "Xuejia Sang",
      "Linfu Xue",
      "Xiaopeng Leng",
      "Xiaoshun Li",
      "Jianping Zhou"
    ],
    "author_ids": [],
    "abstract": "In earth science education, observation of field geological phenomena is very\nimportant. Due to China's huge student population, it is difficult to guarantee\neducation fairness and teaching quality in field teaching. Specimens are\nindispensable geo-education resources. However, the specimen cabinet or picture\nspecimen library has many limitations and it is difficult to meet the\ninternet-spirit or geo-teaching needs. Based on photo modeling, this research\nbuilds a 3D Geo-Resource Sharing Library (3DGRSL) for Geo-Education. It uses\nthe Cesium engine and data-oriented distributed architecture to provide the\neducational resources to many universities. With Browser/Server (B/S)\narchitecture, the system can realize multi-terminal and multi-scenario access\nof mobile phones, tablets, VR, PC, indoor, outdoor, field, providing a flexible\nand convenient way for preserving and sharing scientific information about\ngeo-resources. This makes sense to students who cannot accept field teaching in\nunder-funded colleges, and the ones with mobility problems. Tests and scoring\nresults show that 3DGRSL is a suitable solution for displaying and sharing\ngeological specimens. Which is of great significance for the sustainable use\nand protection of geoscience teaching resources, the maintenance of the right\nto fair education, and the construction of virtual simulation solutions in the\nfuture.",
    "published_date": "2020-04-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.CY",
      "ACM-class: J.2.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05535v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.05167v1",
    "title": "Individual Fairness in Pipelines",
    "authors": [
      "Cynthia Dwork",
      "Christina Ilvento",
      "Meena Jagadeesan"
    ],
    "author_ids": [],
    "abstract": "It is well understood that a system built from individually fair components\nmay not itself be individually fair. In this work, we investigate individual\nfairness under pipeline composition. Pipelines differ from ordinary sequential\nor repeated composition in that individuals may drop out at any stage, and\nclassification in subsequent stages may depend on the remaining \"cohort\" of\nindividuals. As an example, a company might hire a team for a new project and\nat a later point promote the highest performer on the team. Unlike other\nrepeated classification settings, where the degree of unfairness degrades\ngracefully over multiple fair steps, the degree of unfairness in pipelines can\nbe arbitrary, even in a pipeline with just two stages.\n  Guided by a panoply of real-world examples, we provide a rigorous framework\nfor evaluating different types of fairness guarantees for pipelines. We show\nthat na\\\"{i}ve auditing is unable to uncover systematic unfairness and that, in\norder to ensure fairness, some form of dependence must exist between the design\nof algorithms at different stages in the pipeline. Finally, we provide\nconstructions that permit flexibility at later stages, meaning that there is no\nneed to lock in the entire pipeline at the time that the early stage is\nconstructed.",
    "published_date": "2020-04-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05167v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05505v2",
    "title": "Data Age Aware Scheduling for Wireless Powered Mobile-Edge Computing in Industrial Internet of Things",
    "authors": [
      "Hao Wu",
      "Hui Tian",
      "Shaoshuai Fan",
      "Jiazhi Ren"
    ],
    "author_ids": [],
    "abstract": "Wireless powered mobile edge computing has been envisioned as a promising\nparadigm to enhance the computation capability of low-power wireless devices in\nIndustrial Internet of Things. An efficient resource scheduling method is\ncritical yet challenging to design in such a scenario due to stochastic traffic\narrival, time-coupling uplink/downlink decision and incomplete system state\nknowledge. To tackle these challenges, an online optimization algorithm is\nproposed in this paper to maximize long-term system utility balancing\nthroughput and fairness, subject to data age and stability constraints. A set\nof virtual queues is designed to transform the scheduling task, which is hard\nto solve due to time-dependent data age constraints, into a stochastic\noptimization problem. Leveraging Lyapunov and convex optimization techniques,\nthe proposed approach can achieve asymptotically near-optimal online decisions\nwithout any prior statistical knowledge, and maintain the asymptotic optimality\nin the presence of partial and outdated network state information. Numerical\nsimulations corroborate the theoretical analysis and demonstrate the\neffectiveness of the proposed approach.",
    "published_date": "2020-04-11T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05505v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.05499v1",
    "title": "Relaxed Dual Optimal Inequalities for Relaxed Columns: with Application to Vehicle Routing",
    "authors": [
      "Naveed Haghani",
      "Claudio Contardo",
      "Julian Yarkony"
    ],
    "author_ids": [],
    "abstract": "We address the problem of accelerating column generation for set cover\nproblems in which we relax the state space of the columns to do efficient\npricing. We achieve this by adapting the recently introduced smooth and\nflexible dual optimal inequalities (DOI) for use with relaxed columns. Smooth\nDOI exploit the observation that similar items are nearly fungible, and hence\nshould be associated with similarly valued dual variables. Flexible DOI exploit\nthe observation that the change in cost of a column induced by removing an item\ncan be bounded. We adapt these DOI to the problem of capacitated vehicle\nrouting in the context of ng-route relaxations. We demonstrate significant\nspeed ups on a benchmark data set, while provably not weakening the relaxation.",
    "published_date": "2020-04-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05499v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.06568v1",
    "title": "Robust Generalised Quadratic Discriminant Analysis",
    "authors": [
      "Abhik Ghosh",
      "Rita SahaRay",
      "Sayan Chakrabarty",
      "Sayan Bhadra"
    ],
    "author_ids": [],
    "abstract": "Quadratic discriminant analysis (QDA) is a widely used statistical tool to\nclassify observations from different multivariate Normal populations. The\ngeneralized quadratic discriminant analysis (GQDA) classification\nrule/classifier, which generalizes the QDA and the minimum Mahalanobis distance\n(MMD) classifiers to discriminate between populations with underlying\nelliptically symmetric distributions competes quite favorably with the QDA\nclassifier when it is optimal and performs much better when QDA fails under\nnon-Normal underlying distributions, e.g. Cauchy distribution. However, the\nclassification rule in GQDA is based on the sample mean vector and the sample\ndispersion matrix of a training sample, which are extremely non-robust under\ndata contamination. In real world, since it is quite common to face data highly\nvulnerable to outliers, the lack of robustness of the classical estimators of\nthe mean vector and the dispersion matrix reduces the efficiency of the GQDA\nclassifier significantly, increasing the misclassification errors. The present\npaper investigates the performance of the GQDA classifier when the classical\nestimators of the mean vector and the dispersion matrix used therein are\nreplaced by various robust counterparts. Applications to various real data sets\nas well as simulation studies reveal far better performance of the proposed\nrobust versions of the GQDA classifier. A Comparative study has been made to\nadvocate the appropriate choice of the robust estimators to be used in a\nspecific situation of the degree of contamination of the data sets.",
    "published_date": "2020-04-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.CV",
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.06568v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05455v1",
    "title": "Evidence of economic segregation from mobility lockdown during COVID-19 epidemic",
    "authors": [
      "Giovanni Bonaccorsi",
      "Francesco Pierri",
      "Matteo Cinelli",
      "Francesco Porcelli",
      "Alessandro Galeazzi",
      "Andrea Flori",
      "Ana Lucia Schmidt",
      "Carlo Michele Valensise",
      "Antonio Scala",
      "Walter Quattrociocchi",
      "Fabio Pammolli"
    ],
    "author_ids": [],
    "abstract": "In response to the COVID-19 pandemic, National governments have applied\nlockdown restrictions to reduce the infection rate. We perform a massive\nanalysis on near real-time Italian data provided by Facebook to investigate how\nlockdown strategies affect economic conditions of individuals and local\ngovernments. We model the change in mobility as an exogenous shock similar to a\nnatural disaster. We identify two ways through which mobility restrictions\naffect Italian citizens. First, we find that the impact of lockdown is stronger\nin municipalities with higher fiscal capacity. Second, we find a segregation\neffect, since mobility restrictions are stronger in municipalities for which\ninequality is higher and where individuals have lower income per capita.",
    "published_date": "2020-04-11T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05455v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.05405v1",
    "title": "Unveiling COVID-19 from Chest X-ray with deep learning: a hurdles race with small data",
    "authors": [
      "Enzo Tartaglione",
      "Carlo Alberto Barbano",
      "Claudio Berzovini",
      "Marco Calandri",
      "Marco Grangetto"
    ],
    "author_ids": [],
    "abstract": "The possibility to use widespread and simple chest X-ray (CXR) imaging for\nearly screening of COVID-19 patients is attracting much interest from both the\nclinical and the AI community. In this study we provide insights and also raise\nwarnings on what is reasonable to expect by applying deep-learning to COVID\nclassification of CXR images. We provide a methodological guide and critical\nreading of an extensive set of statistical results that can be obtained using\ncurrently available datasets. In particular, we take the challenge posed by\ncurrent small size COVID data and show how significant can be the bias\nintroduced by transfer-learning using larger public non-COVID CXR datasets. We\nalso contribute by providing results on a medium size COVID CXR dataset, just\ncollected by one of the major emergency hospitals in Northern Italy during the\npeak of the COVID pandemic. These novel data allow us to contribute to validate\nthe generalization capacity of preliminary results circulating in the\nscientific community. Our conclusions shed some light into the possibility to\neffectively discriminate COVID using CXR.",
    "published_date": "2020-04-11T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05405v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.05280v2",
    "title": "A Fair and Privacy-Aware EV Discharging Strategy using Decentralized Whale Optimization Algorithm for Minimizing Cost of EVs and the EV Aggregator",
    "authors": [
      "Yingqi Gu",
      "Mingming Liu"
    ],
    "author_ids": [],
    "abstract": "A key motivation to fasten roll-out of electric vehicles (EVs) to the market\nis to implement Vehicle-to-Grid (V2G) functionalities. With V2G in place, EV\nowners can have extra freedom to interact their battery energy with power\ngrids, namely by selling their energy to the grid when their EVs are not in\nuse. On the other hand, EV aggregators and utility companies can leverage the\nflexibility of the collected energy to implement various ancillary services to\nthe grids, which may significantly reduce costs of, for instance, running\nspinning reserve of traditional power plants on the grid side. However, this\nextra freedom also poses practical challenges in terms of how to devise a\ndischarging strategy for a group of EVs that is fair and in some sense optimal.\nIn this paper, we present a new design of EV discharging strategy in a typical\nV2G energy trading framework whilst leveraging the whale optimization algorithm\nin a decentralized manner, a metaheuristic algorithm that has been shown\neffective in solving large-scale centralized optimization problems. We\ndemonstrate that by using simple ideas of data shuffling and aggregation, one\ncan design an EV discharging strategy in a fair, optimal and privacy-aware\nmanner, where the privacy refers to the fact that no critical information of\nEVs should be exchanged with the EV aggregator, and vice versa. The fairness\nimplies that a common discharge rate needs to be sought for all EVs so that no\none gets better benefits than others in the same V2G programme. Simulation\nresults are presented to illustrate the efficacy of our proposed system.",
    "published_date": "2020-04-11T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05280v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.05188v1",
    "title": "On Strong Scaling and Open Source Tools for Analyzing Atom Probe Tomography Data",
    "authors": [
      "Markus Kühbach",
      "Priyanshu Bajaj",
      "Murat Han Celik",
      "Eric Aimo Jägle",
      "Baptiste Gault"
    ],
    "author_ids": [],
    "abstract": "Atom probe tomography (APT) has matured to a versatile nanoanalytical\ncharacterization tool with applications that range from materials science to\ngeology and possibly beyond. Already, well over 100 APT microscopes exist\nworldwide. Information from the APT data requires a post-processing of the\nreconstructed point cloud which is realized via basic implementations of data\nscience methods, mostly executed with proprietary software. Limitations of the\nsoftware have motivated the APT community to develop supplementary\npost-processing tools to cope with increasing method complexity and higher\nquality demands: examples are how to improve method transparency, how to\nsupport batch processing capabilities, and how to document more completely the\nmethods and computational workflows to better align with the FAIR data\nstewardship principles.\n  One gap in the APT software tool landscape has been a collection of open\ntools which support scientific computing hardware. Here, we introduce\nPARAPROBE, an open source, efficient tool for the scientific computing of APT\ndata. We show how to process several computational geometry, spatial\nstatistics, and clustering tasks performantly for datasets as large as two\nbillion ions. Our parallelization efforts yield orders of magnitude performance\ngains and deliver batch processing capabilities. We contribute these tools in\nan effort to open up APT data mining and simplify it to make tools for rigorous\nquantification, sensitivity analyses, and cross-method benchmarking available\nto practitioners.",
    "published_date": "2020-04-10T00:00:00",
    "year": 2020,
    "categories": [
      "physics.comp-ph",
      "cond-mat.mtrl-sci",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05188v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.04907v1",
    "title": "Socioeconomic correlations of urban patterns inferred from aerial images: interpreting activation maps of Convolutional Neural Networks",
    "authors": [
      "Jacob Levy Abitbol",
      "Márton Karsai"
    ],
    "author_ids": [],
    "abstract": "Urbanisation is a great challenge for modern societies, promising better\naccess to economic opportunities while widening socioeconomic inequalities.\nAccurately tracking how this process unfolds has been challenging for\ntraditional data collection methods, while remote sensing information offers an\nalternative to gather a more complete view on these societal changes. By\nfeeding a neural network with satellite images one may recover the\nsocioeconomic information associated to that area, however these models lack to\nexplain how visual features contained in a sample, trigger a given prediction.\nHere we close this gap by predicting socioeconomic status across France from\naerial images and interpreting class activation mappings in terms of urban\ntopology. We show that the model disregards the spatial correlations existing\nbetween urban class and socioeconomic status to derive its predictions. These\nresults pave the way to build interpretable models, which may help to better\ntrack and understand urbanisation and its consequences.",
    "published_date": "2020-04-10T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04907v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.04498v3",
    "title": "Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem",
    "authors": [
      "Danielle Saunders",
      "Bill Byrne"
    ],
    "author_ids": [],
    "abstract": "Training data for NLP tasks often exhibits gender bias in that fewer\nsentences refer to women than to men. In Neural Machine Translation (NMT)\ngender bias has been shown to reduce translation quality, particularly when the\ntarget language has grammatical gender. The recent WinoMT challenge set allows\nus to measure this effect directly (Stanovsky et al, 2019).\n  Ideally we would reduce system bias by simply debiasing all data prior to\ntraining, but achieving this effectively is itself a challenge. Rather than\nattempt to create a `balanced' dataset, we use transfer learning on a small set\nof trusted, gender-balanced examples. This approach gives strong and consistent\nimprovements in gender debiasing with much less computational cost than\ntraining from scratch.\n  A known pitfall of transfer learning on new domains is `catastrophic\nforgetting', which we address both in adaptation and in inference. During\nadaptation we show that Elastic Weight Consolidation allows a performance\ntrade-off between general translation quality and bias reduction. During\ninference we propose a lattice-rescoring scheme which outperforms all systems\nevaluated in Stanovsky et al (2019) on WinoMT with no degradation of general\ntest set BLEU, and we show this scheme can be applied to remove gender bias in\nthe output of `black box` online commercial MT systems. We demonstrate our\napproach translating from English into three languages with varied linguistic\nproperties and data availability.",
    "published_date": "2020-04-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04498v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.04459v1",
    "title": "Fast frequency discrimination and phoneme recognition using a biomimetic membrane coupled to a neural network",
    "authors": [
      "Woo Seok Lee",
      "Hyunjae Kim",
      "Andrew N. Cleland",
      "Kang-Hun Ahn"
    ],
    "author_ids": [],
    "abstract": "In the human ear, the basilar membrane plays a central role in sound\nrecognition. When excited by sound, this membrane responds with a\nfrequency-dependent displacement pattern that is detected and identified by the\nauditory hair cells combined with the human neural system. Inspired by this\nstructure, we designed and fabricated an artificial membrane that produces a\nspatial displacement pattern in response to an audible signal, which we used to\ntrain a convolutional neural network (CNN). When trained with single frequency\ntones, this system can unambiguously distinguish tones closely spaced in\nfrequency. When instead trained to recognize spoken vowels, this system\noutperforms existing methods for phoneme recognition, including the discrete\nFourier transform (DFT), zoom FFT and chirp z-transform, especially when tested\nin short time windows. This sound recognition scheme therefore promises\nsignificant benefits in fast and accurate sound identification compared to\nexisting methods.",
    "published_date": "2020-04-09T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD",
      "physics.bio-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04459v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.04428v1",
    "title": "Care Robots with Sexual Assistance Functions",
    "authors": [
      "Oliver Bendel"
    ],
    "author_ids": [],
    "abstract": "Residents in retirement and nursing homes have sexual needs just like other\npeople. However, the semi-public situation makes it difficult for them to\nsatisfy these existential concerns. In addition, they may not be able to meet a\nsuitable partner or find it difficult to have a relationship for mental or\nphysical reasons. People who live or are cared for at home can also be affected\nby this problem. Perhaps they can host someone more easily and discreetly than\nthe residents of a health facility, but some elderly and disabled people may be\nrestricted in some ways. This article examines the opportunities and risks that\narise with regard to care robots with sexual assistance functions. First of\nall, it deals with sexual well-being. Then it presents robotic systems ranging\nfrom sex robots to care robots. Finally, the focus is on care robots, with the\nauthor exploring technical and design issues. A brief ethical discussion\ncompletes the article. The result is that care robots with sexual assistance\nfunctions could be an enrichment of the everyday life of people in need of\ncare, but that we also have to consider some technical, design and moral\naspects.",
    "published_date": "2020-04-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "cs.CY",
      "cs.HC",
      "I.2.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04428v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.04400v1",
    "title": "Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis",
    "authors": [
      "Jogendra Nath Kundu",
      "Siddharth Seth",
      "Varun Jampani",
      "Mugalodi Rakesh",
      "R. Venkatesh Babu",
      "Anirban Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Camera captured human pose is an outcome of several sources of variation.\nPerformance of supervised 3D pose estimation approaches comes at the cost of\ndispensing with variations, such as shape and appearance, that may be useful\nfor solving other related tasks. As a result, the learned model not only\ninculcates task-bias but also dataset-bias because of its strong reliance on\nthe annotated samples, which also holds true for weakly-supervised models.\nAcknowledging this, we propose a self-supervised learning framework to\ndisentangle such variations from unlabeled video frames. We leverage the prior\nknowledge on human skeleton and poses in the form of a single part-based 2D\npuppet model, human pose articulation constraints, and a set of unpaired 3D\nposes. Our differentiable formalization, bridging the representation gap\nbetween the 3D pose and spatial part maps, not only facilitates discovery of\ninterpretable pose disentanglement but also allows us to operate on videos with\ndiverse camera movements. Qualitative results on unseen in-the-wild datasets\nestablish our superior generalization across multiple tasks beyond the primary\ntasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate\nstate-of-the-art weakly-supervised 3D pose estimation performance on both\nHuman3.6M and MPI-INF-3DHP datasets.",
    "published_date": "2020-04-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04400v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.04221v1",
    "title": "Saliency-based Weighted Multi-label Linear Discriminant Analysis",
    "authors": [
      "Lei Xu",
      "Jenni Raitoharju",
      "Alexandros Iosifidis",
      "Moncef Gabbouj"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a new variant of Linear Discriminant Analysis (LDA)\nto solve multi-label classification tasks. The proposed method is based on a\nprobabilistic model for defining the weights of individual samples in a\nweighted multi-label LDA approach. Linear Discriminant Analysis is a classical\nstatistical machine learning method, which aims to find a linear data\ntransformation increasing class discrimination in an optimal discriminant\nsubspace. Traditional LDA sets assumptions related to Gaussian class\ndistributions and single-label data annotations. To employ the LDA technique in\nmulti-label classification problems, we exploit intuitions coming from a\nprobabilistic interpretation of class saliency to redefine the between-class\nand within-class scatter matrices. The saliency-based weights obtained based on\nvarious kinds of affinity encoding prior information are used to reveal the\nprobability of each instance to be salient for each of its classes in the\nmulti-label problem at hand. The proposed Saliency-based weighted Multi-label\nLDA approach is shown to lead to performance improvements in various\nmulti-label classification problems.",
    "published_date": "2020-04-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04221v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03926v1",
    "title": "MM Algorithms for Joint Independent Subspace Analysis with Application to Blind Single and Multi-Source Extraction",
    "authors": [
      "Robin Scheibler",
      "Nobutaka Ono"
    ],
    "author_ids": [],
    "abstract": "In this work, we propose efficient algorithms for joint independent subspace\nanalysis (JISA), an extension of independent component analysis that deals with\nparallel mixtures, where not all the components are independent. We derive an\nalgorithmic framework for JISA based on the majorization-minimization (MM)\noptimization technique (JISA-MM). We use a well-known inequality for\nsuper-Gaussian sources to derive a surrogate function of the negative\nlog-likelihood of the observed data. The minimization of this surrogate\nfunction leads to a variant of the hybrid exact-approximate diagonalization\nproblem, but where multiple demixing vectors are grouped together. In the\nspirit of auxiliary function based independent vector analysis (AuxIVA), we\npropose several updates that can be applied alternately to one, or jointly to\ntwo, groups of demixing vectors.\n  Recently, blind extraction of one or more sources has gained interest as a\nreasonable way of exploiting larger microphone arrays to achieve better\nseparation. In particular, several MM algorithms have been proposed for\noverdetermined IVA (OverIVA). By applying JISA-MM, we are not only able to\nrederive these in a general manner, but also find several new algorithms. We\nrun extensive numerical experiments to evaluate their performance, and compare\nit to that of full separation with AuxIVA. We find that algorithms using\npairwise updates of two sources, or of one source and the background have the\nfastest convergence, and are able to separate target sources quickly and\nprecisely from the background. In addition, we characterize the performance of\nall algorithms under a large number of noise, reverberation, and background\nmismatch conditions.",
    "published_date": "2020-04-08T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03926v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.03902v2",
    "title": "Deep daxes: Mutual exclusivity arises through both learning biases and pragmatic strategies in neural networks",
    "authors": [
      "Kristina Gulordava",
      "Thomas Brochhagen",
      "Gemma Boleda"
    ],
    "author_ids": [],
    "abstract": "Children's tendency to associate novel words with novel referents has been\ntaken to reflect a bias toward mutual exclusivity. This tendency may be\nadvantageous both as (1) an ad-hoc referent selection heuristic to single out\nreferents lacking a label and as (2) an organizing principle of lexical\nacquisition. This paper investigates under which circumstances\ncross-situational neural models can come to exhibit analogous behavior to\nchildren, focusing on these two possibilities and their interaction. To this\nend, we evaluate neural networks' on both symbolic data and, as a first, on\nlarge-scale image data. We find that constraints in both learning and selection\ncan foster mutual exclusivity, as long as they put words in competition for\nlexical meaning. For computational models, these findings clarify the role of\navailable options for better performance in tasks where mutual exclusivity is\nadvantageous. For cognitive research, they highlight latent interactions\nbetween word learning, referent selection mechanisms, and the structure of\nstimuli of varying complexity: symbolic and visual.",
    "published_date": "2020-04-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03902v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03708v1",
    "title": "Context-Aware Group Captioning via Self-Attention and Contrastive Features",
    "authors": [
      "Zhuowan Li",
      "Quan Tran",
      "Long Mai",
      "Zhe Lin",
      "Alan Yuille"
    ],
    "author_ids": [],
    "abstract": "While image captioning has progressed rapidly, existing works focus mainly on\ndescribing single images. In this paper, we introduce a new task, context-aware\ngroup captioning, which aims to describe a group of target images in the\ncontext of another group of related reference images. Context-aware group\ncaptioning requires not only summarizing information from both the target and\nreference image group but also contrasting between them. To solve this problem,\nwe propose a framework combining self-attention mechanism with contrastive\nfeature construction to effectively summarize common information from each\nimage group while capturing discriminative information between them. To build\nthe dataset for this task, we propose to group the images and generate the\ngroup captions based on single image captions using scene graphs matching. Our\ndatasets are constructed on top of the public Conceptual Captions dataset and\nour new Stock Captions dataset. Experiments on the two datasets show the\neffectiveness of our method on this new task. Related Datasets and code are\nreleased at https://lizw14.github.io/project/groupcap .",
    "published_date": "2020-04-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03708v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03644v1",
    "title": "Causal Relational Learning",
    "authors": [
      "Babak Salimi",
      "Harsh Parikh",
      "Moe Kayali",
      "Sudeepa Roy",
      "Lise Getoor",
      "Dan Suciu"
    ],
    "author_ids": [],
    "abstract": "Causal inference is at the heart of empirical research in natural and social\nsciences and is critical for scientific discovery and informed decision making.\nThe gold standard in causal inference is performing randomized controlled\ntrials; unfortunately these are not always feasible due to ethical, legal, or\ncost constraints. As an alternative, methodologies for causal inference from\nobservational data have been developed in statistical studies and social\nsciences. However, existing methods critically rely on restrictive assumptions\nsuch as the study population consisting of homogeneous elements that can be\nrepresented in a single flat table, where each row is referred to as a unit. In\ncontrast, in many real-world settings, the study domain naturally consists of\nheterogeneous elements with complex relational structure, where the data is\nnaturally represented in multiple related tables. In this paper, we present a\nformal framework for causal inference from such relational data. We propose a\ndeclarative language called CaRL for capturing causal background knowledge and\nassumptions and specifying causal queries using simple Datalog-like rules.CaRL\nprovides a foundation for inferring causality and reasoning about the effect of\ncomplex interventions in relational domains. We present an extensive\nexperimental evaluation on real relational data to illustrate the applicability\nof CaRL in social sciences and healthcare.",
    "published_date": "2020-04-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03644v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03424v3",
    "title": "FACT: A Diagnostic for Group Fairness Trade-offs",
    "authors": [
      "Joon Sik Kim",
      "Jiahao Chen",
      "Ameet Talwalkar"
    ],
    "author_ids": [],
    "abstract": "Group fairness, a class of fairness notions that measure how different groups\nof individuals are treated differently according to their protected attributes,\nhas been shown to conflict with one another, often with a necessary cost in\nloss of model's predictive performance. We propose a general diagnostic that\nenables systematic characterization of these trade-offs in group fairness. We\nobserve that the majority of group fairness notions can be expressed via the\nfairness-confusion tensor, which is the confusion matrix split according to the\nprotected attribute values. We frame several optimization problems that\ndirectly optimize both accuracy and fairness objectives over the elements of\nthis tensor, which yield a general perspective for understanding multiple\ntrade-offs including group fairness incompatibilities. It also suggests an\nalternate post-processing method for designing fair classifiers. On synthetic\nand real datasets, we demonstrate the use cases of our diagnostic, particularly\non understanding the trade-off landscape between accuracy and fairness.",
    "published_date": "2020-04-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03424v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03133v2",
    "title": "Neutralizing Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation",
    "authors": [
      "Seungjae Shin",
      "Kyungwoo Song",
      "JoonHo Jang",
      "Hyemi Kim",
      "Weonyoung Joo",
      "Il-Chul Moon"
    ],
    "author_ids": [],
    "abstract": "Recent research demonstrates that word embeddings, trained on the\nhuman-generated corpus, have strong gender biases in embedding spaces, and\nthese biases can result in the discriminative results from the various\ndownstream tasks. Whereas the previous methods project word embeddings into a\nlinear subspace for debiasing, we introduce a \\textit{Latent Disentanglement}\nmethod with a siamese auto-encoder structure with an adapted gradient reversal\nlayer. Our structure enables the separation of the semantic latent information\nand gender latent information of given word into the disjoint latent\ndimensions. Afterwards, we introduce a \\textit{Counterfactual Generation} to\nconvert the gender information of words, so the original and the modified\nembeddings can produce a gender-neutralized word embedding after geometric\nalignment regularization, without loss of semantic information. From the\nvarious quantitative and qualitative debiasing experiments, our method shows to\nbe better than existing debiasing methods in debiasing word embeddings. In\naddition, Our method shows the ability to preserve semantic information during\ndebiasing by minimizing the semantic information losses for extrinsic NLP\ndownstream tasks.",
    "published_date": "2020-04-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03133v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03064v4",
    "title": "Coarse-to-Fine Gaze Redirection with Numerical and Pictorial Guidance",
    "authors": [
      "Jingjing Chen",
      "Jichao Zhang",
      "Enver Sangineto",
      "Jiayuan Fan",
      "Tao Chen",
      "Nicu Sebe"
    ],
    "author_ids": [],
    "abstract": "Gaze redirection aims at manipulating the gaze of a given face image with\nrespect to a desired direction (i.e., a reference angle) and it can be applied\nto many real life scenarios, such as video-conferencing or taking group photos.\nHowever, previous work on this topic mainly suffers of two limitations: (1)\nLow-quality image generation and (2) Low redirection precision. In this paper,\nwe propose to alleviate these problems by means of a novel gaze redirection\nframework which exploits both a numerical and a pictorial direction guidance,\njointly with a coarse-to-fine learning strategy. Specifically, the coarse\nbranch learns the spatial transformation which warps input image according to\ndesired gaze. On the other hand, the fine-grained branch consists of a\ngenerator network with conditional residual image learning and a multi-task\ndiscriminator. This second branch reduces the gap between the previously warped\nimage and the ground-truth image and recovers finer texture details. Moreover,\nwe propose a numerical and pictorial guidance module~(NPG) which uses a\npictorial gazemap description and numerical angles as an extra guide to further\nimprove the precision of gaze redirection. Extensive experiments on a benchmark\ndataset show that the proposed method outperforms the state-of-the-art\napproaches in terms of both image quality and redirection precision. The code\nis available at https://github.com/jingjingchen777/CFGR",
    "published_date": "2020-04-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03064v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02845v2",
    "title": "Ontologies in CLARIAH: Towards Interoperability in History, Language and Media",
    "authors": [
      "Albert Meroño-Peñuela",
      "Victor de Boer",
      "Marieke van Erp",
      "Richard Zijdeman",
      "Rick Mourits",
      "Willem Melder",
      "Auke Rijpma",
      "Ruben Schalk"
    ],
    "author_ids": [],
    "abstract": "One of the most important goals of digital humanities is to provide\nresearchers with data and tools for new research questions, either by\nincreasing the scale of scholarly studies, linking existing databases, or\nimproving the accessibility of data. Here, the FAIR principles provide a useful\nframework as these state that data needs to be: Findable, as they are often\nscattered among various sources; Accessible, since some might be offline or\nbehind paywalls; Interoperable, thus using standard knowledge representation\nformats and shared vocabularies; and Reusable, through adequate licensing and\npermissions. Integrating data from diverse humanities domains is not trivial,\nresearch questions such as \"was economic wealth equally distributed in the 18th\ncentury?\", or \"what are narratives constructed around disruptive media\nevents?\") and preparation phases (e.g. data collection, knowledge organisation,\ncleaning) of scholars need to be taken into account. In this chapter, we\ndescribe the ontologies and tools developed and integrated in the Dutch\nnational project CLARIAH to address these issues across datasets from three\nfundamental domains or \"pillars\" of the humanities (linguistics, social and\neconomic history, and media studies) that have paradigmatic data\nrepresentations (textual corpora, structured data, and multimedia). We\nsummarise the lessons learnt from using such ontologies and tools in these\ndomains from a generalisation and reusability perspective.",
    "published_date": "2020-04-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02845v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02774v1",
    "title": "SSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds",
    "authors": [
      "Xinge Zhu",
      "Yuexin Ma",
      "Tai Wang",
      "Yan Xu",
      "Jianping Shi",
      "Dahua Lin"
    ],
    "author_ids": [],
    "abstract": "Multi-class 3D object detection aims to localize and classify objects of\nmultiple categories from point clouds. Due to the nature of point clouds, i.e.\nunstructured, sparse and noisy, some features benefit-ting multi-class\ndiscrimination are underexploited, such as shape information. In this paper, we\npropose a novel 3D shape signature to explore the shape information from point\nclouds. By incorporating operations of symmetry, convex hull and chebyshev\nfitting, the proposed shape sig-nature is not only compact and effective but\nalso robust to the noise, which serves as a soft constraint to improve the\nfeature capability of multi-class discrimination. Based on the proposed shape\nsignature, we develop the shape signature networks (SSN) for 3D object\ndetection, which consist of pyramid feature encoding part, shape-aware grouping\nheads and explicit shape encoding objective. Experiments show that the proposed\nmethod performs remarkably better than existing methods on two large-scale\ndatasets. Furthermore, our shape signature can act as a plug-and-play component\nand ablation study shows its effectiveness and good scalability",
    "published_date": "2020-04-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02774v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02717v2",
    "title": "Joint Routing and Scheduling for Large-Scale Deterministic IP Networks",
    "authors": [
      "Jonatan Krolikowski",
      "Sebastien Martin",
      "Paolo Medagliani",
      "Jeremie Leguay",
      "Shuang Chen",
      "Xiaodong Chang",
      "Xuesong Geng"
    ],
    "author_ids": [],
    "abstract": "With the advent of 5G and the evolution of Internet protocols, industrial\napplications are moving from vertical solutions to general purpose IP-based\ninfrastructures that need to meet deterministic Quality of Service (QoS)\nrequirements. The IETF DetNet working group aims at providing an answer to this\nneed with support for (i) deterministic worst-case latency and jitter, and (ii)\nzero packet loss for time-sensitive traffic. In this paper we focus on the\njoint routing and scheduling problem in large scale deterministic networks\nusing Cycle Specified Queuing and Forwarding (CSQF), an extension of Cyclic\nQueuing and Forwarding (CQF) with multiple transmission queues and support of\nsegment routing. In this context, we present two centralized algorithms to\nmaximize traffic acceptance for network planning and online flow admission. We\npropose an effective solution based on column generation and dynamic\nprogramming. Thanks to the reinforcement of the model with valid inequalities,\nwe improve the upper bound and the solution. We demonstrate on realistic\ninstances that we reach an optimality gap smaller than 10% in a few seconds.\nFinally, we also derive an ultra-fast adaptive greedy algorithm to solve the\nproblem at the cost of a small extra gap.",
    "published_date": "2020-04-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02717v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.02554v3",
    "title": "Simultaneously Achieving Ex-ante and Ex-post Fairness",
    "authors": [
      "Haris Aziz"
    ],
    "author_ids": [],
    "abstract": "We present a polynomial-time algorithm that computes an ex-ante envy-free\nlottery over envy-free up to one item (EF1) deterministic allocations. It has\nthe following advantages over a recently proposed algorithm: it does not rely\non the linear programming machinery including separation oracles; it is\nSD-efficient (both ex-ante and ex-post); and the ex-ante outcome is equivalent\nto the outcome returned by the well-known probabilistic serial rule. As a\nresult, we answer a question raised by Freeman, Shah, and Vaish (2020) whether\nthe outcome of the probabilistic serial rule can be implemented by ex-post EF1\nallocations. In the light of a couple of impossibility results that we prove,\nour algorithm can be viewed as satisfying a maximal set of properties. Under\nbinary utilities, our algorithm is also ex-ante group-strategyproof and ex-ante\nPareto optimal. Finally, we also show that checking whether a given random\nallocation can be implemented by a lottery over EF1 and Pareto optimal\nallocations is NP-hard.",
    "published_date": "2020-04-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.DS",
      "91A12, 68Q15",
      "F.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02554v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.02235v4",
    "title": "From Generalized zero-shot learning to long-tail with class descriptors",
    "authors": [
      "Dvir Samuel",
      "Yuval Atzmon",
      "Gal Chechik"
    ],
    "author_ids": [],
    "abstract": "Real-world data is predominantly unbalanced and long-tailed, but deep models\nstruggle to recognize rare classes in the presence of frequent classes. Often,\nclasses can be accompanied by side information like textual descriptions, but\nit is not fully clear how to use them for learning with unbalanced long-tail\ndata. Such descriptions have been mostly used in (Generalized) Zero-shot\nlearning (ZSL), suggesting that ZSL with class descriptions may also be useful\nfor long-tail distributions. We describe DRAGON, a late-fusion architecture for\nlong-tail learning with class descriptors. It learns to (1) correct the bias\ntowards head classes on a sample-by-sample basis; and (2) fuse information from\nclass-descriptions to improve the tail-class accuracy. We also introduce new\nbenchmarks CUB-LT, SUN-LT, AWA-LT for long-tail learning with\nclass-descriptions, building on existing learning-with-attributes datasets and\na version of Imagenet-LT with class descriptors. DRAGON outperforms\nstate-of-the-art models on the new benchmark. It is also a new SoTA on existing\nbenchmarks for GFSL with class descriptors (GFSL-d) and standard (vision-only)\nlong-tailed learning ImageNet-LT, CIFAR-10, 100, and Places365.",
    "published_date": "2020-04-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02235v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02182v3",
    "title": "Imbalanced Data Learning by Minority Class Augmentation using Capsule Adversarial Networks",
    "authors": [
      "Pourya Shamsolmoali",
      "Masoumeh Zareapoor",
      "Linlin Shen",
      "Abdul Hamid Sadka",
      "Jie Yang"
    ],
    "author_ids": [],
    "abstract": "The fact that image datasets are often imbalanced poses an intense challenge\nfor deep learning techniques. In this paper, we propose a method to restore the\nbalance in imbalanced images, by coalescing two concurrent methods, generative\nadversarial networks (GANs) and capsule network. In our model, generative and\ndiscriminative networks play a novel competitive game, in which the generator\ngenerates samples towards specific classes from multivariate probabilities\ndistribution. The discriminator of our model is designed in a way that while\nrecognizing the real and fake samples, it is also requires to assign classes to\nthe inputs. Since GAN approaches require fully observed data during training,\nwhen the training samples are imbalanced, the approaches might generate similar\nsamples which leading to data overfitting. This problem is addressed by\nproviding all the available information from both the class components jointly\nin the adversarial training. It improves learning from imbalanced data by\nincorporating the majority distribution structure in the generation of new\nminority samples. Furthermore, the generator is trained with feature matching\nloss function to improve the training convergence. In addition, prevents\ngeneration of outliers and does not affect majority class space. The\nevaluations show the effectiveness of our proposed methodology; in particular,\nthe coalescing of capsule-GAN is effective at recognizing highly overlapping\nclasses with much fewer parameters compared with the convolutional-GAN.",
    "published_date": "2020-04-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02182v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02133v2",
    "title": "Neuron Linear Transformation: Modeling the Domain Shift for Crowd Counting",
    "authors": [
      "Qi Wang",
      "Tao Han",
      "Junyu Gao",
      "Yuan Yuan"
    ],
    "author_ids": [],
    "abstract": "Cross-domain crowd counting (CDCC) is a hot topic due to its importance in\npublic safety. The purpose of CDCC is to alleviate the domain shift between the\nsource and target domain. Recently, typical methods attempt to extract\ndomain-invariant features via image translation and adversarial learning. When\nit comes to specific tasks, we find that the domain shifts are reflected on\nmodel parameters' differences. To describe the domain gap directly at the\nparameter-level, we propose a Neuron Linear Transformation (NLT) method,\nexploiting domain factor and bias weights to learn the domain shift.\nSpecifically, for a specific neuron of a source model, NLT exploits few labeled\ntarget data to learn domain shift parameters. Finally, the target neuron is\ngenerated via a linear transformation. Extensive experiments and analysis on\nsix real-world datasets validate that NLT achieves top performance compared\nwith other domain adaptation methods. An ablation study also shows that the NLT\nis robust and more effective than supervised and fine-tune training. Code is\navailable at: \\url{https://github.com/taohan10200/NLT}.",
    "published_date": "2020-04-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02133v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02028v1",
    "title": "Measuring Social Biases of Crowd Workers using Counterfactual Queries",
    "authors": [
      "Bhavya Ghai",
      "Q. Vera Liao",
      "Yunfeng Zhang",
      "Klaus Mueller"
    ],
    "author_ids": [],
    "abstract": "Social biases based on gender, race, etc. have been shown to pollute machine\nlearning (ML) pipeline predominantly via biased training datasets.\nCrowdsourcing, a popular cost-effective measure to gather labeled training\ndatasets, is not immune to the inherent social biases of crowd workers. To\nensure such social biases aren't passed onto the curated datasets, it's\nimportant to know how biased each crowd worker is. In this work, we propose a\nnew method based on counterfactual fairness to quantify the degree of inherent\nsocial bias in each crowd worker. This extra information can be leveraged\ntogether with individual worker responses to curate a less biased dataset.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02028v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.02023v3",
    "title": "Towards Query Logs for Privacy Studies: On Deriving Search Queries from Questions",
    "authors": [
      "Asia J. Biega",
      "Jana Schmidt",
      "Rishiraj Saha Roy"
    ],
    "author_ids": [],
    "abstract": "Translating verbose information needs into crisp search queries is a\nphenomenon that is ubiquitous but hardly understood. Insights into this process\ncould be valuable in several applications, including synthesizing large\nprivacy-friendly query logs from public Web sources which are readily available\nto the academic research community. In this work, we take a step towards\nunderstanding query formulation by tapping into the rich potential of community\nquestion answering (CQA) forums. Specifically, we sample natural language (NL)\nquestions spanning diverse themes from the Stack Exchange platform, and conduct\na large-scale conversion experiment where crowdworkers submit search queries\nthey would use when looking for equivalent information. We provide a careful\nanalysis of this data, accounting for possible sources of bias during\nconversion, along with insights into user-specific linguistic patterns and\nsearch behaviors. We release a dataset of 7,000 question-query pairs from this\nstudy to facilitate further research on query understanding.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.02023v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.01967v1",
    "title": "The Paradox of Information Access: Growing Isolation in the Age of Sharing",
    "authors": [
      "Tarek Abdelzaher",
      "Heng Ji",
      "Jinyang Li",
      "Chaoqi Yang",
      "John Dellaverson",
      "Lixia Zhang",
      "Chao Xu",
      "Boleslaw K. Szymanski"
    ],
    "author_ids": [],
    "abstract": "Modern online media, such as Twitter, Instagram, and YouTube, enable anyone\nto become an information producer and to offer online content for potentially\nglobal consumption. By increasing the amount of globally accessible real-time\ninformation, today's ubiquitous producers contribute to a world, where an\nindividual consumes vanishingly smaller fractions of all produced content. In\ngeneral, consumers preferentially select information that closely matches their\nindividual views and values. The bias inherent in such selection is further\nmagnified by today's information curation services that maximize user\nengagement (and thus service revenue) by filtering new content in accordance\nwith observed consumer preferences. Consequently, individuals get exposed to\nincreasingly narrower bands of the ideology spectrum. Societies get fragmented\ninto increasingly ideologically isolated enclaves. These enclaves (or\necho-chambers) then become vulnerable to misinformation spread, which in turn\nfurther magnifies polarization and bias. We call this dynamic the paradox of\ninformation access; a growing ideological fragmentation in the age of sharing.\nThis article describes the technical, economic, and socio-cognitive\ncontributors to this paradox, and explores research directions towards its\nmitigation.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01967v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.01888v6",
    "title": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking",
    "authors": [
      "Yifu Zhang",
      "Chunyu Wang",
      "Xinggang Wang",
      "Wenjun Zeng",
      "Wenyu Liu"
    ],
    "author_ids": [],
    "abstract": "Multi-object tracking (MOT) is an important problem in computer vision which\nhas a wide range of applications. Formulating MOT as multi-task learning of\nobject detection and re-ID in a single network is appealing since it allows\njoint optimization of the two tasks and enjoys high computation efficiency.\nHowever, we find that the two tasks tend to compete with each other which need\nto be carefully addressed. In particular, previous works usually treat re-ID as\na secondary task whose accuracy is heavily affected by the primary detection\ntask. As a result, the network is biased to the primary detection task which is\nnot fair to the re-ID task. To solve the problem, we present a simple yet\neffective approach termed as FairMOT based on the anchor-free object detection\narchitecture CenterNet. Note that it is not a naive combination of CenterNet\nand re-ID. Instead, we present a bunch of detailed designs which are critical\nto achieve good tracking results by thorough empirical studies. The resulting\napproach achieves high accuracy for both detection and tracking. The approach\noutperforms the state-of-the-art methods by a large margin on several public\ndatasets. The source code and pre-trained models are released at\nhttps://github.com/ifzhang/FairMOT.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01888v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01862v1",
    "title": "Identifying Radiological Findings Related to COVID-19 from Medical Literature",
    "authors": [
      "Yuxiao Liang",
      "Pengtao Xie"
    ],
    "author_ids": [],
    "abstract": "Coronavirus disease 2019 (COVID-19) has infected more than one million\nindividuals all over the world and caused more than 55,000 deaths, as of April\n3 in 2020. Radiological findings are important sources of information in\nguiding the diagnosis and treatment of COVID-19. However, the existing studies\non how radiological findings are correlated with COVID-19 are conducted\nseparately by different hospitals, which may be inconsistent or even\nconflicting due to population bias. To address this problem, we develop natural\nlanguage processing methods to analyze a large collection of COVID-19\nliterature containing study reports from hospitals all over the world,\nreconcile these results, and draw unbiased and universally-sensible conclusions\nabout the correlation between radiological findings and COVID-19. We apply our\nmethod to the CORD-19 dataset and successfully extract a set of radiological\nfindings that are closely tied to COVID-19.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01862v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01840v1",
    "title": "Abstracting Fairness: Oracles, Metrics, and Interpretability",
    "authors": [
      "Cynthia Dwork",
      "Christina Ilvento",
      "Guy N. Rothblum",
      "Pragya Sur"
    ],
    "author_ids": [],
    "abstract": "It is well understood that classification algorithms, for example, for\ndeciding on loan applications, cannot be evaluated for fairness without taking\ncontext into account. We examine what can be learned from a fairness oracle\nequipped with an underlying understanding of ``true'' fairness. The oracle\ntakes as input a (context, classifier) pair satisfying an arbitrary fairness\ndefinition, and accepts or rejects the pair according to whether the classifier\nsatisfies the underlying fairness truth. Our principal conceptual result is an\nextraction procedure that learns the underlying truth; moreover, the procedure\ncan learn an approximation to this truth given access to a weak form of the\noracle. Since every ``truly fair'' classifier induces a coarse metric, in which\nthose receiving the same decision are at distance zero from one another and\nthose receiving different decisions are at distance one, this extraction\nprocess provides the basis for ensuring a rough form of metric fairness, also\nknown as individual fairness. Our principal technical result is a higher\nfidelity extractor under a mild technical constraint on the weak oracle's\nconception of fairness. Our framework permits the scenario in which many\nclassifiers, with differing outcomes, may all be considered fair. Our results\nhave implications for interpretablity -- a highly desired but poorly defined\nproperty of classification systems that endeavors to permit a human arbiter to\nreject classifiers deemed to be ``unfair'' or illegitimately derived.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01840v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01836v1",
    "title": "Numerical Analysis of History-dependent Variational-hemivariational Inequalities",
    "authors": [
      "Shufen Wang",
      "Wei Xu",
      "Weimin Han",
      "Wenbin Chen"
    ],
    "author_ids": [],
    "abstract": "In this paper, numerical analysis is carried out for a class of\nhistory-dependent variational-hemivariational inequalities arising in contact\nproblems. Three different numerical treatments for temporal discretization are\nproposed to approximate the continuous model. Fixed-point iteration algorithms\nare employed to implement the implicit scheme and the convergence is proved\nwith a convergence rate independent of the time step-size and mesh grid-size. A\nspecial temporal discretization is introduced for the history-dependent\noperator, leading to numerical schemes for which the unique solvability and\nerror bounds for the temporally discrete systems can be proved without any\nrestriction on the time step-size. As for spatial approximation, the finite\nelement method is applied and an optimal order error estimate for the linear\nelement solutions is provided under appropriate regularity assumptions.\nNumerical examples are presented to illustrate the theoretical results.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "47J20, 65N30, 65N15, 74M15"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01836v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.01817v1",
    "title": "Group Based Deep Shared Feature Learning for Fine-grained Image Classification",
    "authors": [
      "Xuelu Li",
      "Vishal Monga"
    ],
    "author_ids": [],
    "abstract": "Fine-grained image classification has emerged as a significant challenge\nbecause objects in such images have small inter-class visual differences but\nwith large variations in pose, lighting, and viewpoints, etc. Most existing\nwork focuses on highly customized feature extraction via deep network\narchitectures which have been shown to deliver state of the art performance.\nGiven that images from distinct classes in fine-grained classification share\nsignificant features of interest, we present a new deep network architecture\nthat explicitly models shared features and removes their effect to achieve\nenhanced classification results. Our modeling of shared features is based on a\nnew group based learning wherein existing classes are divided into groups and\nmultiple shared feature patterns are discovered (learned). We call this\nframework Group based deep Shared Feature Learning (GSFL) and the resulting\nlearned network as GSFL-Net. Specifically, the proposed GSFL-Net develops a\nspecially designed autoencoder which is constrained by a newly proposed Feature\nExpression Loss to decompose a set of features into their constituent shared\nand discriminative components. During inference, only the discriminative\nfeature component is used to accomplish the classification task. A key benefit\nof our specialized autoencoder is that it is versatile and can be combined with\nstate-of-the-art fine-grained feature extraction models and trained together\nwith them to improve their performance directly. Experiments on benchmark\ndatasets show that GSFL-Net can enhance classification accuracy over the state\nof the art with a more interpretable architecture.",
    "published_date": "2020-04-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01817v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01771v1",
    "title": "Max-Min Fair Beamforming for Cooperative Multigroup Multicasting with Rate-Splitting",
    "authors": [
      "Ahmet Zahid Yalcin",
      "Yavuz Yapici"
    ],
    "author_ids": [],
    "abstract": "The demand of massive access to the same multimedia content at the same time\nis one major challenge for next-generation cellular networks in densely-packed\nurban areas. The content-aware multicast transmission strategies provide\npromising solutions to such use cases involving many mobile users trying to\nfetch the same data. In this work, we consider multigroup multicast\ntransmission with a common message (e.g., multimedia content), in which\ndifferent multicast groups are interested along with their private multicast\nmessages. We further assume that a relay helps the cellular base station (BS)\ndisseminate multicast content to the users experiencing high path loss and/or\nblockage. We propose superposition and concatenated coding schemes, denoted by\nSC and CC, respectively, to transmit the common and private multicast messages.\nIn order to maximize max-min fair (MMF) rates, we design a novel low-complexity\nalternating-optimization algorithm to compute transmit and relay precoders. We\nalso propose rate-splitting (RS) alternatives of SC and CC schemes together\nwith an iterative algorithm to derive dedicated transmit and relay precoders so\nas to maximize MMF rates. We rigorously evaluate the performance of the\nproposed transmission schemes and precoders, and verify the superiority of\nRS-based schemes in overloaded scenarios without any saturation with increasing\nsignal-to-noise ratio.",
    "published_date": "2020-04-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01771v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.03370v2",
    "title": "A white-box analysis on the writer-independent dichotomy transformation applied to offline handwritten signature verification",
    "authors": [
      "Victor L. F. Souza",
      "Adriano L. I. Oliveira",
      "Rafael M. O. Cruz",
      "Robert Sabourin"
    ],
    "author_ids": [],
    "abstract": "High number of writers, small number of training samples per writer with high\nintra-class variability and heavily imbalanced class distributions are among\nthe challenges and difficulties of the offline Handwritten Signature\nVerification (HSV) problem. A good alternative to tackle these issues is to use\na writer-independent (WI) framework. In WI systems, a single model is trained\nto perform signature verification for all writers from a dissimilarity space\ngenerated by the dichotomy transformation. Among the advantages of this\nframework is its scalability to deal with some of these challenges and its ease\nin managing new writers, and hence of being used in a transfer learning\ncontext. In this work, we present a white-box analysis of this approach\nhighlighting how it handles the challenges, the dynamic selection of references\nthrough fusion function, and its application for transfer learning. All the\nanalyses are carried out at the instance level using the instance hardness (IH)\nmeasure. The experimental results show that, using the IH analysis, we were\nable to characterize \"good\" and \"bad\" quality skilled forgeries as well as the\nfrontier region between positive and negative samples. This enables futures\ninvestigations on methods for improving discrimination between genuine\nsignatures and skilled forgeries by considering these characterizations.",
    "published_date": "2020-04-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03370v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01653v6",
    "title": "Orthogonal Inductive Matrix Completion",
    "authors": [
      "Antoine Ledent",
      "Rodrigo Alves",
      "Marius Kloft"
    ],
    "author_ids": [],
    "abstract": "We propose orthogonal inductive matrix completion (OMIC), an interpretable\napproach to matrix completion based on a sum of multiple orthonormal side\ninformation terms, together with nuclear-norm regularization.\n  The approach allows us to inject prior knowledge about the singular vectors\nof the ground truth matrix.\n  We optimize the approach by a provably converging algorithm, which optimizes\nall components of the model simultaneously. We study the generalization\ncapabilities of our method in both the distribution-free setting and in the\ncase where the sampling distribution admits uniform marginals, yielding\nlearning guarantees that improve with the quality of the injected knowledge in\nboth cases. As particular cases of our framework, we present models which can\nincorporate user and item biases or community information in a joint and\nadditive fashion.\n  We analyse the performance of OMIC on several synthetic and real datasets.\n  On synthetic datasets with a sliding scale of user bias relevance, we show\nthat OMIC better adapts to different regimes than other methods. On real-life\ndatasets containing user/items recommendations and relevant side information,\nwe find that OMIC surpasses the state-of-the-art, with the added benefit of\ngreater interpretability.",
    "published_date": "2020-04-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01653v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01459v4",
    "title": "Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples",
    "authors": [
      "Lili Pan",
      "Shijie Ai",
      "Yazhou Ren",
      "Zenglin Xu"
    ],
    "author_ids": [],
    "abstract": "Deep discriminative models (e.g. deep regression forests, deep neural\ndecision forests) have achieved remarkable success recently to solve problems\nsuch as facial age estimation and head pose estimation. Most existing methods\npursue robust and unbiased solutions either through learning discriminative\nfeatures, or reweighting samples. We argue what is more desirable is learning\ngradually to discriminate like our human beings, and hence we resort to\nself-paced learning (SPL). Then, a natural question arises: can self-paced\nregime lead deep discriminative models to achieve more robust and less biased\nsolutions? To this end, this paper proposes a new deep discriminative\nmodel--self-paced deep regression forests with consideration on\nunderrepresented examples (SPUDRFs). It tackles the fundamental ranking and\nselecting problem in SPL from a new perspective: fairness. This paradigm is\nfundamental and could be easily combined with a variety of deep discriminative\nmodels (DDMs). Extensive experiments on two computer vision tasks, i.e., facial\nage estimation and head pose estimation, demonstrate the efficacy of SPUDRFs,\nwhere state-of-the-art performances are achieved.",
    "published_date": "2020-04-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01459v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01418v1",
    "title": "Demographic Bias: A Challenge for Fingervein Recognition Systems?",
    "authors": [
      "P. Drozdowski",
      "B. Prommegger",
      "G. Wimmer",
      "R. Schraml",
      "C. Rathgeb",
      "A. Uhl",
      "C. Busch"
    ],
    "author_ids": [],
    "abstract": "Recently, concerns regarding potential biases in the underlying algorithms of\nmany automated systems (including biometrics) have been raised. In this\ncontext, a biased algorithm produces statistically different outcomes for\ndifferent groups of individuals based on certain (often protected by\nanti-discrimination legislation) attributes such as sex and age. While several\npreliminary studies investigating this matter for facial recognition algorithms\ndo exist, said topic has not yet been addressed for vascular biometric\ncharacteristics. Accordingly, in this paper, several popular types of\nrecognition algorithms are benchmarked to ascertain the matter for fingervein\nrecognition. The experimental evaluation suggests lack of bias for the tested\nalgorithms, although future works with larger datasets are needed to validate\nand confirm those preliminary results.",
    "published_date": "2020-04-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01418v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01355v2",
    "title": "FairALM: Augmented Lagrangian Method for Training Fair Models with Little Regret",
    "authors": [
      "Vishnu Suresh Lokhande",
      "Aditya Kumar Akash",
      "Sathya N. Ravi",
      "Vikas Singh"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decision making based on computer vision and machine learning\ntechnologies continue to permeate our lives. But issues related to biases of\nthese models and the extent to which they treat certain segments of the\npopulation unfairly, have led to concern in the general public. It is now\naccepted that because of biases in the datasets we present to the models, a\nfairness-oblivious training will lead to unfair models. An interesting topic is\nthe study of mechanisms via which the de novo design or training of the model\ncan be informed by fairness measures. Here, we study mechanisms that impose\nfairness concurrently while training the model. While existing fairness based\napproaches in vision have largely relied on training adversarial modules\ntogether with the primary classification/regression task, in an effort to\nremove the influence of the protected attribute or variable, we show how ideas\nbased on well-known optimization concepts can provide a simpler alternative. In\nour proposed scheme, imposing fairness just requires specifying the protected\nattribute and utilizing our optimization routine. We provide a detailed\ntechnical analysis and present experiments demonstrating that various fairness\nmeasures from the literature can be reliably imposed on a number of training\ntasks in vision in a manner that is interpretable.",
    "published_date": "2020-04-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01355v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01254v2",
    "title": "Under the Hood of Neural Networks: Characterizing Learned Representations by Functional Neuron Populations and Network Ablations",
    "authors": [
      "Richard Meyes",
      "Constantin Waubert de Puiseau",
      "Andres Posada-Moreno",
      "Tobias Meisen"
    ],
    "author_ids": [],
    "abstract": "The need for more transparency of the decision-making processes in artificial\nneural networks steadily increases driven by their applications in safety\ncritical and ethically challenging domains such as autonomous driving or\nmedical diagnostics. We address today's lack of transparency of neural networks\nand shed light on the roles of single neurons and groups of neurons within the\nnetwork fulfilling a learned task. Inspired by research in the field of\nneuroscience, we characterize the learned representations by activation\npatterns and network ablations, revealing functional neuron populations that a)\nact jointly in response to specific stimuli or b) have similar impact on the\nnetwork's performance after being ablated. We find that neither a neuron's\nmagnitude or selectivity of activation, nor its impact on network performance\nare sufficient stand-alone indicators for its importance for the overall task.\nWe argue that such indicators are essential for future advances in transfer\nlearning and modern neuroscience.",
    "published_date": "2020-04-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01254v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.01106v2",
    "title": "The Paradox of Information Access: On Modeling Social-Media-Induced Polarization",
    "authors": [
      "Chao Xu",
      "Jinyang Li",
      "Tarek Abdelzaher",
      "Heng Ji",
      "Boleslaw K. Szymanski",
      "John Dellaverson"
    ],
    "author_ids": [],
    "abstract": "The paper develops a stochastic model of drift in human beliefs that shows\nthat today's sheer volume of accessible information, combined with consumers'\nconfirmation bias and natural preference to more outlying content, necessarily\nlead to increased polarization. The model explains the paradox of growing\nideological fragmentation in the age of increased sharing. As social media,\nsearch engines, and other real-time information sharing outlets purport to\nfacilitate access to information, a need for content filtering arises due to\nthe ensuing information overload. In general, consumers select information that\nmatches their individual views and values. The bias inherent in such selection\nis echoed by today's information curation services that maximize user\nengagement by filtering new content in accordance with observed consumer\npreferences. Consequently, individuals get exposed to increasingly narrower\nbands of the ideology spectrum, thus fragmenting society into increasingly\nideologically isolated enclaves. We call this dynamic the paradox of\ninformation access. The model also suggests the disproportionate damage\nattainable with a small infusion of well-positioned misinformation. The paper\ndescribes the modeling methodology, and evaluates modeling results for\ndifferent population sizes and parameter settings.",
    "published_date": "2020-04-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01106v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.01019v3",
    "title": "Face Quality Estimation and Its Correlation to Demographic and Non-Demographic Bias in Face Recognition",
    "authors": [
      "Philipp Terhörst",
      "Jan Niklas Kolf",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "author_ids": [],
    "abstract": "Face quality assessment aims at estimating the utility of a face image for\nthe purpose of recognition. It is a key factor to achieve high face recognition\nperformances. Currently, the high performance of these face recognition systems\ncome with the cost of a strong bias against demographic and non-demographic\nsub-groups. Recent work has shown that face quality assessment algorithms\nshould adapt to the deployed face recognition system, in order to achieve\nhighly accurate and robust quality estimations. However, this could lead to a\nbias transfer towards the face quality assessment leading to discriminatory\neffects e.g. during enrolment. In this work, we present an in-depth analysis of\nthe correlation between bias in face recognition and face quality assessment.\nExperiments were conducted on two publicly available datasets captured under\ncontrolled and uncontrolled circumstances with two popular face embeddings. We\nevaluated four state-of-the-art solutions for face quality assessment towards\nbiases to pose, ethnicity, and age. The experiments showed that the face\nquality assessment solutions assign significantly lower quality values towards\nsubgroups affected by the recognition bias demonstrating that these approaches\nare biased as well. This raises ethical questions towards fairness and\ndiscrimination which future works have to address.",
    "published_date": "2020-04-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01019v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.00935v2",
    "title": "Applying Transparency in Artificial Intelligence based Personalization Systems",
    "authors": [
      "Laura Schelenz",
      "Avi Segal",
      "Kobi Gal"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence based systems increasingly use personalization to\nprovide users with relevant content, products, and solutions. Personalization\nis intended to support users and address their respective needs and\npreferences. However, users are becoming increasingly vulnerable to online\nmanipulation due to algorithmic advancements and lack of transparency. Such\nmanipulation decreases users' levels of trust, autonomy, and satisfaction\nconcerning the systems with which they interact. Increasing transparency is an\nimportant goal for personalization based systems. Unfortunately, system\ndesigners lack guidance in assessing and implementing transparency in their\ndeveloped systems.\n  In this work we combine insights from technology ethics and computer science\nto generate a list of transparency best practices for machine generated\npersonalization. Based on these best practices, we develop a checklist to be\nused by designers wishing to evaluate and increase the transparency of their\nalgorithmic systems. Adopting a designer perspective, we apply the checklist to\nprominent online services and discuss its advantages and shortcomings. We\nencourage researchers to adopt the checklist in various environments and to\nwork towards a consensus-based tool for measuring transparency in the\npersonalization community.",
    "published_date": "2020-04-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.00935v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.00817v1",
    "title": "Combating The Machine Ethics Crisis: An Educational Approach",
    "authors": [
      "Tai Vu"
    ],
    "author_ids": [],
    "abstract": "In recent years, the availability of massive data sets and improved computing\npower have driven the advent of cutting-edge machine learning algorithms.\nHowever, this trend has triggered growing concerns associated with its ethical\nissues. In response to such a phenomenon, this study proposes a feasible\nsolution that combines ethics and computer science materials in artificial\nintelligent classrooms. In addition, the paper presents several arguments and\nevidence in favor of the necessity and effectiveness of this integrated\napproach.",
    "published_date": "2020-04-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.00817v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.00686v2",
    "title": "Bias in Machine Learning -- What is it Good for?",
    "authors": [
      "Thomas Hellström",
      "Virginia Dignum",
      "Suna Bensch"
    ],
    "author_ids": [],
    "abstract": "In public media as well as in scientific publications, the term \\emph{bias}\nis used in conjunction with machine learning in many different contexts, and\nwith many different meanings. This paper proposes a taxonomy of these different\nmeanings, terminology, and definitions by surveying the, primarily scientific,\nliterature on machine learning. In some cases, we suggest extensions and\nmodifications to promote a clear terminology and completeness. The survey is\nfollowed by an analysis and discussion on how different types of biases are\nconnected and depend on each other. We conclude that there is a complex\nrelation between bias occurring in the machine learning pipeline that leads to\na model, and the eventual bias of the model (which is typically related to\nsocial discrimination). The former bias may or may not influence the latter, in\na sometimes bad, and sometime good way.",
    "published_date": "2020-04-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.00686v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.03455v1",
    "title": "Deep Learning Based Multi-Label Text Classification of UNGA Resolutions",
    "authors": [
      "Francesco Sovrano",
      "Monica Palmirani",
      "Fabio Vitali"
    ],
    "author_ids": [],
    "abstract": "The main goal of this research is to produce a useful software for United\nNations (UN), that could help to speed up the process of qualifying the UN\ndocuments following the Sustainable Development Goals (SDGs) in order to\nmonitor the progresses at the world level to fight poverty, discrimination,\nclimate changes. In fact human labeling of UN documents would be a daunting\ntask given the size of the impacted corpus. Thus, automatic labeling must be\nadopted at least as a first step of a multi-phase process to reduce the overall\neffort of cataloguing and classifying. Deep Learning (DL) is nowadays one of\nthe most powerful tools for state-of-the-art (SOTA) AI for this task, but very\noften it comes with the cost of an expensive and error-prone preparation of a\ntraining-set. In the case of multi-label text classification of domain-specific\ntext it seems that we cannot effectively adopt DL without a big-enough\ndomain-specific training-set. In this paper, we show that this is not always\ntrue. In fact we propose a novel method that is able, through statistics like\nTF-IDF, to exploit pre-trained SOTA DL models (such as the Universal Sentence\nEncoder) without any need for traditional transfer learning or any other\nexpensive training procedure. We show the effectiveness of our method in a\nlegal context, by classifying UN Resolutions according to their most related\nSDGs.",
    "published_date": "2020-04-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03455v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.00491v1",
    "title": "Beyond privacy regulations: an ethical approach to data usage in transportation",
    "authors": [
      "Johannes M. van Hulst",
      "Mattia Zeni",
      "Alexander Kröller",
      "Cassandra Moons",
      "Pierluigi Casale"
    ],
    "author_ids": [],
    "abstract": "With the exponential advancement of business technology in recent years,\ndata-driven decision making has become the core of most industries. With the\nrise of new privacy regulations such as the General Data Protection Regulation\nin the European Union and the California Consumer Privacy Act in the United\nStates, companies dealing with personal data had to conform to these changes\nand adapt their processes accordingly. This obviously included the\ntransportation industry with their use of location data. At the other side of\nthe spectrum, users still expect a form of personalization, without having to\ncompromise on their privacy. For this reason, companies across the industries\nstarted applying privacy-enhancing or preserving technologies at scale in their\nproducts as a competitive advantage. In this paper, we describe how Federated\nMachine Learning can be applied to the transportation sector. We present\nuse-cases for which Federated Learning is beneficial in transportation and the\nnew product lifecycle that is required for using such a technology. We see\nFederated Learning as a method that enables us to process privacy-sensitive\ndata, while respecting customer's privacy and one that guides us beyond\nprivacy-regulations and into the world of ethical data-usage.",
    "published_date": "2020-04-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.00491v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.14395v1",
    "title": "COVID-ResNet: A Deep Learning Framework for Screening of COVID19 from Radiographs",
    "authors": [
      "Muhammad Farooq",
      "Abdul Hafeez"
    ],
    "author_ids": [],
    "abstract": "In the last few months, the novel COVID19 pandemic has spread all over the\nworld. Due to its easy transmission, developing techniques to accurately and\neasily identify the presence of COVID19 and distinguish it from other forms of\nflu and pneumonia is crucial. Recent research has shown that the chest Xrays of\npatients suffering from COVID19 depicts certain abnormalities in the\nradiography. However, those approaches are closed source and not made available\nto the research community for re-producibility and gaining deeper insight. The\ngoal of this work is to build open source and open access datasets and present\nan accurate Convolutional Neural Network framework for differentiating COVID19\ncases from other pneumonia cases. Our work utilizes state of the art training\ntechniques including progressive resizing, cyclical learning rate finding and\ndiscriminative learning rates to training fast and accurate residual neural\nnetworks. Using these techniques, we showed the state of the art results on the\nopen-access COVID-19 dataset. This work presents a 3-step technique to\nfine-tune a pre-trained ResNet-50 architecture to improve model performance and\nreduce training time. We call it COVIDResNet. This is achieved through\nprogressively re-sizing of input images to 128x128x3, 224x224x3, and 229x229x3\npixels and fine-tuning the network at each stage. This approach along with the\nautomatic learning rate selection enabled us to achieve the state of the art\naccuracy of 96.23% (on all the classes) on the COVIDx dataset with only 41\nepochs. This work presented a computationally efficient and highly accurate\nmodel for multi-class classification of three different infection types from\nalong with Normal individuals. This model can help in the early screening of\nCOVID19 cases and help reduce the burden on healthcare systems.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "68T10",
      "I.2.1; I.4.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.14395v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.14324v1",
    "title": "On the Integration of LinguisticFeatures into Statistical and Neural Machine Translation",
    "authors": [
      "Eva Vanmassenhove"
    ],
    "author_ids": [],
    "abstract": "New machine translations (MT) technologies are emerging rapidly and with\nthem, bold claims of achieving human parity such as: (i) the results produced\napproach \"accuracy achieved by average bilingual human translators\" (Wu et al.,\n2017b) or (ii) the \"translation quality is at human parity when compared to\nprofessional human translators\" (Hassan et al., 2018) have seen the light of\nday (Laubli et al., 2018). Aside from the fact that many of these papers craft\ntheir own definition of human parity, these sensational claims are often not\nsupported by a complete analysis of all aspects involved in translation.\nEstablishing the discrepancies between the strengths of statistical approaches\nto MT and the way humans translate has been the starting point of our research.\nBy looking at MT output and linguistic theory, we were able to identify some\nremaining issues. The problems range from simple number and gender agreement\nerrors to more complex phenomena such as the correct translation of aspectual\nvalues and tenses. Our experiments confirm, along with other studies\n(Bentivogli et al., 2016), that neural MT has surpassed statistical MT in many\naspects. However, some problems remain and others have emerged. We cover a\nseries of problems related to the integration of specific linguistic features\ninto statistical and neural MT, aiming to analyse and provide a solution to\nsome of them. Our work focuses on addressing three main research questions that\nrevolve around the complex relationship between linguistics and MT in general.\nWe identify linguistic information that is lacking in order for automatic\ntranslation systems to produce more accurate translations and integrate\nadditional features into the existing pipelines. We identify overgeneralization\nor 'algorithmic bias' as a potential drawback of neural MT and link it to many\nof the remaining linguistic issues.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.14324v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.14282v1",
    "title": "Inherent Dependency Displacement Bias of Transition-Based Algorithms",
    "authors": [
      "Mark Anderson",
      "Carlos Gómez-Rodríguez"
    ],
    "author_ids": [],
    "abstract": "A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.14282v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.14263v2",
    "title": "A survey of bias in Machine Learning through the prism of Statistical Parity for the Adult Data Set",
    "authors": [
      "Philippe Besse",
      "Eustasio del Barrio",
      "Paula Gordaliza",
      "Jean-Michel Loubes",
      "Laurent Risser"
    ],
    "author_ids": [],
    "abstract": "Applications based on Machine Learning models have now become an\nindispensable part of the everyday life and the professional world. A critical\nquestion then recently arised among the population: Do algorithmic decisions\nconvey any type of discrimination against specific groups of population or\nminorities? In this paper, we show the importance of understanding how a bias\ncan be introduced into automatic decisions. We first present a mathematical\nframework for the fair learning problem, specifically in the binary\nclassification setting. We then propose to quantify the presence of bias by\nusing the standard Disparate Impact index on the real and well-known Adult\nincome data set. Finally, we check the performance of different approaches\naiming to reduce the bias in binary classification outcomes. Importantly, we\nshow that some intuitive methods are ineffective. This sheds light on the fact\ntrying to make fair machine learning models may be a particularly challenging\ntask, in particular when the training observations contain a bias.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.14263v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13966v3",
    "title": "Individual Fairness in Advertising Auctions through Inverse Proportionality",
    "authors": [
      "Shuchi Chawla",
      "Meena Jagadeesan"
    ],
    "author_ids": [],
    "abstract": "Recent empirical work demonstrates that online advertisement can exhibit bias\nin the delivery of ads across users even when all advertisers bid in a\nnon-discriminatory manner. We study the design of ad auctions that, given fair\nbids, are guaranteed to produce fair outcomes. Following the works of Dwork and\nIlvento (2019) and Chawla et al. (2020), our goal is to design a truthful\nauction that satisfies ``individual fairness'' in its outcomes: informally\nspeaking, users that are similar to each other should obtain similar\nallocations of ads. Within this framework we quantify the tradeoff between\nsocial welfare maximization and fairness.\n  This work makes two conceptual contributions. First, we express the fairness\nconstraint as a kind of stability condition: any two users that are assigned\nmultiplicatively similar values by all the advertisers must receive additively\nsimilar allocations for each advertiser. This value stability constraint is\nexpressed as a function that maps the multiplicative distance between value\nvectors to the maximum allowable $\\ell_{\\infty}$ distance between the\ncorresponding allocations. Standard auctions do not satisfy this kind of value\nstability.\n  Second, we introduce a new class of allocation algorithms called Inverse\nProportional Allocation that achieve a near optimal tradeoff between fairness\nand social welfare for a broad and expressive class of value stability\nconditions. These allocation algorithms are truthful and prior-free, and\nachieve a constant factor approximation to the optimal (unconstrained) social\nwelfare. In particular, the approximation ratio is independent of the number of\nadvertisers in the system. In this respect, these allocation algorithms greatly\nsurpass the guarantees achieved in previous work. We also extend our results to\nbroader notions of fairness that we call subset fairness.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13966v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13947v3",
    "title": "SS-IL: Separated Softmax for Incremental Learning",
    "authors": [
      "Hongjoon Ahn",
      "Jihwan Kwak",
      "Subin Lim",
      "Hyeonsu Bang",
      "Hyojun Kim",
      "Taesup Moon"
    ],
    "author_ids": [],
    "abstract": "We consider class incremental learning (CIL) problem, in which a learning\nagent continuously learns new classes from incrementally arriving training data\nbatches and aims to predict well on all the classes learned so far. The main\nchallenge of the problem is the catastrophic forgetting, and for the\nexemplar-memory based CIL methods, it is generally known that the forgetting is\ncommonly caused by the classification score bias that is injected due to the\ndata imbalance between the new classes and the old classes (in the\nexemplar-memory). While several methods have been proposed to correct such\nscore bias by some additional post-processing, e.g., score re-scaling or\nbalanced fine-tuning, no systematic analysis on the root cause of such bias has\nbeen done. To that end, we analyze that computing the softmax probabilities by\ncombining the output scores for all old and new classes could be the main cause\nof the bias. Then, we propose a new method, dubbed as Separated Softmax for\nIncremental Learning (SS-IL), that consists of separated softmax (SS) output\nlayer combined with task-wise knowledge distillation (TKD) to resolve such\nbias. Throughout our extensive experimental results on several large-scale CIL\nbenchmark datasets, we show our SS-IL achieves strong state-of-the-art accuracy\nthrough attaining much more balanced prediction scores across old and new\nclasses, without any additional post-processing.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13947v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.04812v2",
    "title": "Deep Learning based Frameworks for Handling Imbalance in DGA, Email, and URL Data Analysis",
    "authors": [
      "Simran K",
      "Prathiksha Balakrishna",
      "Vinayakumar Ravi",
      "Soman KP"
    ],
    "author_ids": [],
    "abstract": "Deep learning is a state of the art method for a lot of applications. The\nmain issue is that most of the real-time data is highly imbalanced in nature.\nIn order to avoid bias in training, cost-sensitive approach can be used. In\nthis paper, we propose cost-sensitive deep learning based frameworks and the\nperformance of the frameworks is evaluated on three different Cyber Security\nuse cases which are Domain Generation Algorithm (DGA), Electronic mail (Email),\nand Uniform Resource Locator (URL). Various experiments were performed using\ncost-insensitive as well as cost-sensitive methods and parameters for both of\nthese methods are set based on hyperparameter tuning. In all experiments, the\ncost-sensitive deep learning methods performed better than the cost-insensitive\napproaches. This is mainly due to the reason that cost-sensitive approach gives\nimportance to the classes which have a very less number of samples during\ntraining and this helps to learn all the classes in a more efficient manner.",
    "published_date": "2020-03-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.NE",
      "cs.SI",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04812v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13841v1",
    "title": "A Hierarchical Transformer for Unsupervised Parsing",
    "authors": [
      "Ashok Thillaisundaram"
    ],
    "author_ids": [],
    "abstract": "The underlying structure of natural language is hierarchical; words combine\ninto phrases, which in turn form clauses. An awareness of this hierarchical\nstructure can aid machine learning models in performing many linguistic tasks.\nHowever, most such models just process text sequentially and there is no bias\ntowards learning hierarchical structure encoded into their architecture. In\nthis paper, we extend the recent transformer model (Vaswani et al., 2017) by\nenabling it to learn hierarchical representations. To achieve this, we adapt\nthe ordering mechanism introduced in Shen et al., 2018, to the self-attention\nmodule of the transformer architecture. We train our new model on language\nmodelling and then apply it to the task of unsupervised parsing. We achieve\nreasonable results on the freely available subset of the WSJ10 dataset with an\nF1-score of about 50%.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13841v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13808v1",
    "title": "Fairness Evaluation in Presence of Biased Noisy Labels",
    "authors": [
      "Riccardo Fogliato",
      "Max G'Sell",
      "Alexandra Chouldechova"
    ],
    "author_ids": [],
    "abstract": "Risk assessment tools are widely used around the country to inform decision\nmaking within the criminal justice system. Recently, considerable attention has\nbeen devoted to the question of whether such tools may suffer from racial bias.\nIn this type of assessment, a fundamental issue is that the training and\nevaluation of the model is based on a variable (arrest) that may represent a\nnoisy version of an unobserved outcome of more central interest (offense). We\npropose a sensitivity analysis framework for assessing how assumptions on the\nnoise across groups affect the predictive bias properties of the risk\nassessment model as a predictor of reoffense. Our experimental results on two\nreal world criminal justice data sets demonstrate how even small biases in the\nobserved labels may call into question the conclusions of an analysis based on\nthe noisy outcome.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13808v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.13797v1",
    "title": "An adaptive finite element approach for lifted branched transport problems",
    "authors": [
      "Carolin Dirks",
      "Benedikt Wirth"
    ],
    "author_ids": [],
    "abstract": "We consider so-called branched transport and variants thereof in two space\ndimensions. In these models one seeks an optimal transportation network for a\ngiven mass transportation task. In two space dimensions, they are closely\nconnected to Mumford--Shah-type image processing problems, which in turn can be\nrelated to certain higher-dimensional convex optimization problems via\nso-called functional lifting. We examine the relation between these different\nmodels and exploit it to solve the branched transport model numerically via\nconvex optimization. To this end we develop an efficient numerical treatment\nbased on a specifically designed class of adaptive finite elements. This method\nallows the computation of finely resolved optimal transportation networks\ndespite the high dimensionality of the convex optimization problem and its\ncomplicated set of nonlocal constraints. In particular, by design of the\ndiscretization the infinite set of constraints reduces to a finite number of\ninequalities.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13797v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.13720v3",
    "title": "Deep Molecular Programming: A Natural Implementation of Binary-Weight ReLU Neural Networks",
    "authors": [
      "Marko Vasic",
      "Cameron Chalk",
      "Sarfraz Khurshid",
      "David Soloveichik"
    ],
    "author_ids": [],
    "abstract": "Embedding computation in molecular contexts incompatible with traditional\nelectronics is expected to have wide ranging impact in synthetic biology,\nmedicine, nanofabrication and other fields. A key remaining challenge lies in\ndeveloping programming paradigms for molecular computation that are\nwell-aligned with the underlying chemical hardware and do not attempt to\nshoehorn ill-fitting electronics paradigms. We discover a surprisingly tight\nconnection between a popular class of neural networks (binary-weight ReLU aka\nBinaryConnect) and a class of coupled chemical reactions that are absolutely\nrobust to reaction rates. The robustness of rate-independent chemical\ncomputation makes it a promising target for bioengineering implementation. We\nshow how a BinaryConnect neural network trained in silico using well-founded\ndeep learning optimization techniques, can be compiled to an equivalent\nchemical reaction network, providing a novel molecular programming paradigm. We\nillustrate such translation on the paradigmatic IRIS and MNIST datasets. Toward\nintended applications of chemical computation, we further use our method to\ngenerate a chemical reaction network that can discriminate between different\nvirus types based on gene expression levels. Our work sets the stage for rich\nknowledge transfer between neural network and molecular programming\ncommunities.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.ET",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13720v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13637v2",
    "title": "A game-theoretic approach for Generative Adversarial Networks",
    "authors": [
      "Barbara Franci",
      "Sergio Grammatico"
    ],
    "author_ids": [],
    "abstract": "Generative adversarial networks (GANs) are a class of generative models,\nknown for producing accurate samples. The key feature of GANs is that there are\ntwo antagonistic neural networks: the generator and the discriminator. The main\nbottleneck for their implementation is that the neural networks are very hard\nto train. One way to improve their performance is to design reliable algorithms\nfor the adversarial process. Since the training can be cast as a stochastic\nNash equilibrium problem, we rewrite it as a variational inequality and\nintroduce an algorithm to compute an approximate solution. Specifically, we\npropose a stochastic relaxed forward-backward algorithm for GANs. We prove that\nwhen the pseudogradient mapping of the game is monotone, we have convergence to\nan exact solution or in a neighbourhood of it.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.GT",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13637v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13261v2",
    "title": "Domain-aware Visual Bias Eliminating for Generalized Zero-Shot Learning",
    "authors": [
      "Shaobo Min",
      "Hantao Yao",
      "Hongtao Xie",
      "Chaoqun Wang",
      "Zheng-Jun Zha",
      "Yongdong Zhang"
    ],
    "author_ids": [],
    "abstract": "Recent methods focus on learning a unified semantic-aligned visual\nrepresentation to transfer knowledge between two domains, while ignoring the\neffect of semantic-free visual representation in alleviating the biased\nrecognition problem. In this paper, we propose a novel Domain-aware Visual Bias\nEliminating (DVBE) network that constructs two complementary visual\nrepresentations, i.e., semantic-free and semantic-aligned, to treat seen and\nunseen domains separately. Specifically, we explore cross-attentive\nsecond-order visual statistics to compact the semantic-free representation, and\ndesign an adaptive margin Softmax to maximize inter-class divergences. Thus,\nthe semantic-free representation becomes discriminative enough to not only\npredict seen class accurately but also filter out unseen images, i.e., domain\ndetection, based on the predicted class entropy. For unseen images, we\nautomatically search an optimal semantic-visual alignment architecture, rather\nthan manual designs, to predict unseen classes. With accurate domain detection,\nthe biased recognition problem towards the seen domain is significantly\nreduced. Experiments on five benchmarks for classification and segmentation\nshow that DVBE outperforms existing methods by averaged 5.7% improvement.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13261v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.13255v1",
    "title": "Joint Orthogonal Band and Power Allocation for Energy Fairness in WPT System with Nonlinear Logarithmic Energy Harvesting Model",
    "authors": [
      "Jaeseob Han",
      "Gyeong Ho Lee",
      "Sangdon Park",
      "Jun Kyun Choi"
    ],
    "author_ids": [],
    "abstract": "Wireless power transmission (WPT) is expected to play an important role in\nthe Internet of Things services by providing the perpetual operation of IoT\nsensors. However, to prolong the IoT network's lifetime, the efficient resource\nallocation algorithm is required, in particular, the energy fairness issue\namong IoT sensors has been a critical challenge of the WPT system. In this\npaper, considering energy fairness as the minimum received energy of all energy\npoverty IoT sensors (EPISs), we allocate orthogonal frequency bands to several\nEPISs and transfer the RF power on each orthogonal band, using energy\nbeamforming. Based on the energy poverty, we propose orthogonal frequency bands\nassignment rule, granting the priority to the EPISs with less received energy.\nWe also formulate two transmission power allocation problems, incorporated the\nnonlinear logarithm-energy harvesting (EH) model. First, the total received\npower maximization (TRPM) problem is presented and solved by combining the\nwell-known Karush-Kuhn-Tucker (KKT) conditions with the modified water-filling\nalgorithm. Second, the common received power maximization (CRPM) problem is\nformulated and the optimal solution is derived using the iterative bisection\nsearch method. To apply the bisection search method to the problem, this paper\nproposes a method of specifying the scope of the solution for the objective\nfunction defined by the sum of monotonous functions. In numerical results,\nassuming the mobility of EPISs by the one-dimensional random walk model, the\neffectiveness of the mobility of EPISs on the minimum received energy of all\nEPISs is presented. Finally, the performance of the proposed resource\nallocation schemes is verified by comparing other resources allocation schemes,\nsuch as Round robin and equal power distribution",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.13255v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2004.04644v1",
    "title": "On the Ethics of Building AI in a Responsible Manner",
    "authors": [
      "Shai Shalev-Shwartz",
      "Shaked Shammah",
      "Amnon Shashua"
    ],
    "author_ids": [],
    "abstract": "The AI-alignment problem arises when there is a discrepancy between the goals\nthat a human designer specifies to an AI learner and a potential catastrophic\noutcome that does not reflect what the human designer really wants. We argue\nthat a formalism of AI alignment that does not distinguish between strategic\nand agnostic misalignments is not useful, as it deems all technology as\nun-safe. We propose a definition of a strategic-AI-alignment and prove that\nmost machine learning algorithms that are being used in practice today do not\nsuffer from the strategic-AI-alignment problem. However, without being careful,\ntoday's technology might lead to strategic misalignment.",
    "published_date": "2020-03-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.04644v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.12978v1",
    "title": "SE(3) based Extended Kalman Filter for Spacecraft Attitude Estimation",
    "authors": [
      "Lubin Chang"
    ],
    "author_ids": [],
    "abstract": "In this paper, the spacecraft attitude estimation problem has been\ninvestigated making use of the concept of matrix Lie group. Through formulation\nof the attitude and gyroscope bias as elements of SE(3), the corresponding\nextended Kalman filter, termed as SE(3)-EKF, has been derived. It is shown that\nthe resulting SE(3)-EKF is just the newly-derived geometric extended Kalman\nfilter (GEKF) for spacecraft attitude estimation. This provides a new\nperspective on the GEKF besides the common frame errors definition. Moreover,\nthe SE(3)-EKF with reference frame attitude error is also derived and the\nresulting algorithm bears much resemblance to the right invariant EKF.",
    "published_date": "2020-03-29T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.12978v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.12327v1",
    "title": "An Investigation into the Stochasticity of Batch Whitening",
    "authors": [
      "Lei Huang",
      "Lei Zhao",
      "Yi Zhou",
      "Fan Zhu",
      "Li Liu",
      "Ling Shao"
    ],
    "author_ids": [],
    "abstract": "Batch Normalization (BN) is extensively employed in various network\narchitectures by performing standardization within mini-batches.\n  A full understanding of the process has been a central target in the deep\nlearning communities.\n  Unlike existing works, which usually only analyze the standardization\noperation, this paper investigates the more general Batch Whitening (BW). Our\nwork originates from the observation that while various whitening\ntransformations equivalently improve the conditioning, they show significantly\ndifferent behaviors in discriminative scenarios and training Generative\nAdversarial Networks (GANs).\n  We attribute this phenomenon to the stochasticity that BW introduces.\n  We quantitatively investigate the stochasticity of different whitening\ntransformations and show that it correlates well with the optimization\nbehaviors during training.\n  We also investigate how stochasticity relates to the estimation of population\nstatistics during inference.\n  Based on our analysis, we provide a framework for designing and comparing BW\nalgorithms in different scenarios.\n  Our proposed BW algorithm improves the residual networks by a significant\nmargin on ImageNet classification.\n  Besides, we show that the stochasticity of BW can improve the GAN's\nperformance with, however, the sacrifice of the training stability.",
    "published_date": "2020-03-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.12327v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.12060v1",
    "title": "Negative Margin Matters: Understanding Margin in Few-shot Classification",
    "authors": [
      "Bin Liu",
      "Yue Cao",
      "Yutong Lin",
      "Qi Li",
      "Zheng Zhang",
      "Mingsheng Long",
      "Han Hu"
    ],
    "author_ids": [],
    "abstract": "This paper introduces a negative margin loss to metric learning based\nfew-shot learning methods. The negative margin loss significantly outperforms\nregular softmax loss, and achieves state-of-the-art accuracy on three standard\nfew-shot classification benchmarks with few bells and whistles. These results\nare contrary to the common practice in the metric learning field, that the\nmargin is zero or positive. To understand why the negative margin loss performs\nwell for the few-shot classification, we analyze the discriminability of\nlearned features w.r.t different margins for training and novel classes, both\nempirically and theoretically. We find that although negative margin reduces\nthe feature discriminability for training classes, it may also avoid falsely\nmapping samples of the same novel class to multiple peaks or clusters, and thus\nbenefit the discrimination of novel classes. Code is available at\nhttps://github.com/bl0/negative-margin.few-shot.",
    "published_date": "2020-03-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.12060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.12043v4",
    "title": "From unbiased MDI Feature Importance to Explainable AI for Trees",
    "authors": [
      "Markus Loecher"
    ],
    "author_ids": [],
    "abstract": "We attempt to give a unifying view of the various recent attempts to (i)\nimprove the interpretability of tree-based models and (ii) debias the the\ndefault variable-importance measure in random Forests, Gini importance. In\nparticular, we demonstrate a common thread among the out-of-bag based bias\ncorrection methods and their connection to local explanation for trees. In\naddition, we point out a bias caused by the inclusion of inbag data in the\nnewly developed explainable AI for trees algorithms.",
    "published_date": "2020-03-26T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.12043v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.12375v1",
    "title": "Democratic Value and Money for Decentralized Digital Society",
    "authors": [
      "Bryan Ford"
    ],
    "author_ids": [],
    "abstract": "Classical monetary systems regularly subject the most vulnerable majority of\nthe world's population to debilitating financial shocks, and have manifestly\nallowed uncontrolled global inequality over the long term. Given these basic\nfailures, how can we avoid asking whether mainstream macroeconomic principles\nare actually compatible with democratic principles such as equality or the\nprotection of human rights and dignity? This idea paper takes a constructive\nlook at this question, by exploring how alternate monetary principles might\nresult in a form of money more compatible with democratic principles -- dare we\ncall it \"democratic money\"? In this alternative macroeconomic philosophy, both\nthe supply of and the demand for money must be rooted in people, so as to give\nall people both equal opportunities for economic participation. Money must be\ndesigned around equality, not only across all people alive at a given moment,\nbut also across past and future generations of people, guaranteeing that our\ndescendants cannot be enslaved by their ancestors' economic luck or misfortune.\nDemocratic money must reliably give all people a means to enable everyday\ncommerce, investment, and value creation in good times and bad, and must impose\nhard limits on financial inequality. Democratic money must itself be governed\ndemocratically, and must economically facilitate the needs of citizens in a\ndemocracy for trustworthy and unbiased information with which to make wise\ncollective decisions. An intriguing approach to implementing and deploying\ndemocratic money is via a cryptocurrency built on a proof-of-personhood\nfoundation, giving each opt-in human participant one equal unit of stake. Such\na cryptocurrency would have both interesting similarities to, and important\ndifferences from, a Universal Basic Income (UBI) denominated in an existing\ncurrency.",
    "published_date": "2020-03-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.12375v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.11650v1",
    "title": "Overview of the TREC 2019 Fair Ranking Track",
    "authors": [
      "Asia J. Biega",
      "Fernando Diaz",
      "Michael D. Ekstrand",
      "Sebastian Kohlmeier"
    ],
    "author_ids": [],
    "abstract": "The goal of the TREC Fair Ranking track was to develop a benchmark for\nevaluating retrieval systems in terms of fairness to different content\nproviders in addition to classic notions of relevance. As part of the\nbenchmark, we defined standardized fairness metrics with evaluation protocols\nand released a dataset for the fair ranking problem. The 2019 task focused on\nreranking academic paper abstracts given a query. The objective was to fairly\nrepresent relevant authors from several groups that were unknown at the system\nsubmission time. Thus, the track emphasized the development of systems which\nhave robust performance across a variety of group definitions. Participants\nwere provided with querylog data (queries, documents, and relevance) from\nSemantic Scholar. This paper presents an overview of the track, including the\ntask definition, descriptions of the data and the annotation process, as well\nas a comparison of the performance of submitted systems.",
    "published_date": "2020-03-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.DL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11650v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11634v1",
    "title": "Unfair Exposure of Artists in Music Recommendation",
    "authors": [
      "Himan Abdollahpouri",
      "Robin Burke",
      "Masoud Mansoury"
    ],
    "author_ids": [],
    "abstract": "Fairness in machine learning has been studied by many researchers. In\nparticular, fairness in recommender systems has been investigated to ensure the\nrecommendations meet certain criteria with respect to certain sensitive\nfeatures such as race, gender etc. However, often recommender systems are\nmulti-stakeholder environments in which the fairness towards all stakeholders\nshould be taken care of. It is well-known that the recommendation algorithms\nsuffer from popularity bias; few popular items are over-recommended which leads\nto the majority of other items not getting proportionate attention. This bias\nhas been investigated from the perspective of the users and how it makes the\nfinal recommendations skewed towards popular items in general. In this paper,\nhowever, we investigate the impact of popularity bias in recommendation\nalgorithms on the provider of the items (i.e. the entities who are behind the\nrecommended items). Using a music dataset for our experiments, we show that,\ndue to some biases in the algorithms, different groups of artists with varying\ndegrees of popularity are systematically and consistently treated differently\nthan others.",
    "published_date": "2020-03-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11634v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11249v2",
    "title": "VaB-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning",
    "authors": [
      "Jongwon Choi",
      "Kwang Moo Yi",
      "Jihoon Kim",
      "Jinho Choo",
      "Byoungjip Kim",
      "Jin-Yeop Chang",
      "Youngjune Gwon",
      "Hyung Jin Chang"
    ],
    "author_ids": [],
    "abstract": "Active Learning for discriminative models has largely been studied with the\nfocus on individual samples, with less emphasis on how classes are distributed\nor which classes are hard to deal with. In this work, we show that this is\nharmful. We propose a method based on the Bayes' rule, that can naturally\nincorporate class imbalance into the Active Learning framework. We derive that\nthree terms should be considered together when estimating the probability of a\nclassifier making a mistake for a given sample; i) probability of mislabelling\na class, ii) likelihood of the data given a predicted class, and iii) the prior\nprobability on the abundance of a predicted class. Implementing these terms\nrequires a generative model and an intractable likelihood estimation.\nTherefore, we train a Variational Auto Encoder (VAE) for this purpose. To\nfurther tie the VAE with the classifier and facilitate VAE training, we use the\nclassifiers' deep feature representations as input to the VAE. By considering\nall three probabilities, among them especially the data imbalance, we can\nsubstantially improve the potential of existing methods under limited data\nbudget. We show that our method can be applied to classification tasks on\nmultiple different datasets -- including one that is a real-world dataset with\nheavy data imbalance -- significantly outperforming the state of the art.",
    "published_date": "2020-03-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11249v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11216v2",
    "title": "Event-Triggered Consensus of Homogeneous and Heterogeneous Multi-Agent Systems with Jointly Connected Switching Topologies",
    "authors": [
      "Bin Cheng",
      "Xiangke Wang",
      "Zhongkui Li"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the distributed event-based consensus problem of\nswitching networks satisfying the jointly connected condition. Both the state\nconsensus of homogeneous linear networks and output consensus of heterogeneous\nnetworks are studied. Two kinds of event-based protocols based on local sampled\ninformation are designed, without the need to solve any matrix equation or\ninequality. Theoretical analysis indicates that the proposed event-based\nprotocols guarantee the achievement of consensus and the exclusion of Zeno\nbehaviors for jointly connected undirected switching graphs. These protocols,\nrelying on no global knowledge of the network topology and independent of\nswitching rules, can be devised and utilized in a completely distributed\nmanner. They are able to avoid continuous information exchanges for either\ncontrollers' updating or triggering functions' monitoring, which ensures the\nfeasibility of the presented protocols.",
    "published_date": "2020-03-25T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11216v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.12132v1",
    "title": "On the Emerging Area of Biocybersecurity and Relevant Considerations",
    "authors": [
      "Xavier-Lewis Palmer",
      "Lucas Potter",
      "Saltuk Karahan"
    ],
    "author_ids": [],
    "abstract": "Biocybersecurity is a novel space for the 21st century that meets our\ninnovations in biotechnology and computing head on. Within this space, many\nconsiderations are open for and demand consideration as groups endeavor to\ndevelop products and policies that adequately ensure asset management and\nprotection. Herein, simplified and brief exploration is given followed by some\nsurface discussion of impacts. These impacts concern the end user, ethical and\nlegal considerations, international proceedings, business, and limitations. It\nis hoped that this will be helpful in future considerations towards\nbiocybersecurity policy developments and implementations.\n  Notice: This article has been queued for publication in the Proceedings of\nthe 2020 Future of Information and Communication Conference (FICC)",
    "published_date": "2020-03-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.12132v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.11157v1",
    "title": "AI loyalty: A New Paradigm for Aligning Stakeholder Interests",
    "authors": [
      "Anthony Aguirre",
      "Gaia Dempsey",
      "Harry Surden",
      "Peter B. Reiner"
    ],
    "author_ids": [],
    "abstract": "When we consult with a doctor, lawyer, or financial advisor, we generally\nassume that they are acting in our best interests. But what should we assume\nwhen it is an artificial intelligence (AI) system that is acting on our behalf?\nEarly examples of AI assistants like Alexa, Siri, Google, and Cortana already\nserve as a key interface between consumers and information on the web, and\nusers routinely rely upon AI-driven systems like these to take automated\nactions or provide information. Superficially, such systems may appear to be\nacting according to user interests. However, many AI systems are designed with\nembedded conflicts of interests, acting in ways that subtly benefit their\ncreators (or funders) at the expense of users. To address this problem, in this\npaper we introduce the concept of AI loyalty. AI systems are loyal to the\ndegree that they are designed to minimize, and make transparent, conflicts of\ninterest, and to act in ways that prioritize the interests of users. Properly\ndesigned, such systems could have considerable functional and competitive - not\nto mention ethical - advantages relative to those that do not. Loyal AI\nproducts hold an obvious appeal for the end-user and could serve to promote the\nalignment of the long-term interests of AI developers and customers. To this\nend, we suggest criteria for assessing whether an AI system is sufficiently\ntransparent about conflicts of interest, and acting in a manner that is loyal\nto the user, and argue that AI loyalty should be considered during the\ntechnological design process alongside other important values in AI ethics such\nas fairness, accountability privacy, and equity. We discuss a range of\nmechanisms, from pure market forces to strong regulatory frameworks, that could\nsupport incorporation of AI loyalty into a variety of future AI systems.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11157v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11067v2",
    "title": "Punishing defectors and rewarding cooperators: Do people discriminate between genders?",
    "authors": [
      "Hélène Barcelo",
      "Valerio Capraro"
    ],
    "author_ids": [],
    "abstract": "Do people discriminate between men and women when they have the option to\npunish defectors or reward cooperators? Here we report on four pre-registered\nexperiments, that shed some light on this question. Study 1 (N=544) shows that\npeople do not discriminate between genders when they have the option to punish\n(reward) defectors (cooperators) in a one-shot prisoner's dilemma with\nthird-party punishment/reward. Study 2 (N=253) extends Study 1 to a different\nmethod of punishing/rewarding: participants are asked to rate the behaviour of\na defector/cooperator on a scale of 1 to 5 stars. In this case too, we find\nthat people do not discriminate between genders. Study 3a (N=331) and Study 3b\n(N=310) conceptually replicate Study 2 with a slightly different gender\nmanipulation. These latter studies show that, in situations where they do not\nhave specific beliefs about the gender of the defector/cooperator's partner,\nneither men nor women discriminate between genders.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11067v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.10810v2",
    "title": "Capturing and Explaining Trajectory Singularities using Composite Signal Neural Networks",
    "authors": [
      "Hippolyte Dubois",
      "Patrick Le Callet",
      "Michael Hornberger",
      "Hugo J. Spiers",
      "Antoine Coutrot"
    ],
    "author_ids": [],
    "abstract": "Spatial trajectories are ubiquitous and complex signals. Their analysis is\ncrucial in many research fields, from urban planning to neuroscience. Several\napproaches have been proposed to cluster trajectories. They rely on\nhand-crafted features, which struggle to capture the spatio-temporal complexity\nof the signal, or on Artificial Neural Networks (ANNs) which can be more\nefficient but less interpretable. In this paper we present a novel ANN\narchitecture designed to capture the spatio-temporal patterns characteristic of\na set of trajectories, while taking into account the demographics of the\nnavigators. Hence, our model extracts markers linked to both behaviour and\ndemographics. We propose a composite signal analyser (CompSNN) combining three\nsimple ANN modules. Each of these modules uses different signal representations\nof the trajectory while remaining interpretable. Our CompSNN performs\nsignificantly better than its modules taken in isolation and allows to\nvisualise which parts of the signal were most useful to discriminate the\ntrajectories.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10810v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10807v1",
    "title": "Sum-Rate Maximization and Data Delivery for Wireless Seismic Acquisition",
    "authors": [
      "Abdullah Othman",
      "Wessam Mesbah",
      "Naveed Iqbal",
      "Suhail Al-Dharrab",
      "Ali Muqaibel",
      "Gordon Stüber"
    ],
    "author_ids": [],
    "abstract": "Traditional seismic acquisition systems suffer from a number of difficulties\nrelated to telemetry cables that are used as a means of data transmission.\nTransforming the traditional seismic acquisition system to a wireless system\nhas been considered as a potential solution to most of these difficulties. The\nwireless seismic acquisition system has to serve a huge aggregate data rate\nrequirement as is usually the case in a large wireless sensor network. This\npaper considers the wireless acquisition system, and studies the maximum\nachievable transmission data rates from the geophones to the wireless gateways.\nSuccessive interference cancellation decoding is assumed to be used at the\ngateway nodes. We consider the problem of sum-rate maximization by optimizing\nthe decoding process at each gateway node. The optimization searches for\noptimal decoding set at each gateway, i.e. which group of geophones will be\ndecoded at each gateway. Various integer programming algorithms are proposed\nfor solving the maximization problem. These optimization algorithms are\nsimulated and compared among each other, where it is shown that the ant system\nalgorithm achieves the highest sum-rate with lower computational complexity\ncompared to other algorithms. Furthermore, the data delivery from the gateways\nto the data center is also considered. In this stage, two gateways with\ndifferent buffer sizes are studied. For small-size buffers, two optimization\nproblems are identified and solved. The first problem considers the\nminimization of the total power of the gateways, and the second problem\nconsiders power fairness between the gateways. For large-size buffers, the\nproblem of maximizing the weighted sum rate of the gateways is solved.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10807v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.10778v7",
    "title": "PanNuke Dataset Extension, Insights and Baselines",
    "authors": [
      "Jevgenij Gamper",
      "Navid Alemi Koohbanani",
      "Ksenija Benes",
      "Simon Graham",
      "Mostafa Jahanifar",
      "Syed Ali Khurram",
      "Ayesha Azam",
      "Katherine Hewitt",
      "Nasir Rajpoot"
    ],
    "author_ids": [],
    "abstract": "The emerging area of computational pathology (CPath) is ripe ground for the\napplication of deep learning (DL) methods to healthcare due to the sheer volume\nof raw pixel data in whole-slide images (WSIs) of cancerous tissue slides.\nHowever, it is imperative for the DL algorithms relying on nuclei-level details\nto be able to cope with data from `the clinical wild', which tends to be quite\nchallenging.\n  We study, and extend recently released PanNuke dataset consisting of ~200,000\nnuclei categorized into 5 clinically important classes for the challenging\ntasks of segmenting and classifying nuclei in WSIs. Previous pan-cancer\ndatasets consisted of only up to 9 different tissues and up to 21,000 unlabeled\nnuclei and just over 24,000 labeled nuclei with segmentation masks. PanNuke\nconsists of 19 different tissue types that have been semi-automatically\nannotated and quality controlled by clinical pathologists, leading to a dataset\nwith statistics similar to the clinical wild and with minimal selection bias.\nWe study the performance of segmentation and classification models when applied\nto the proposed dataset and demonstrate the application of models trained on\nPanNuke to whole-slide images. We provide comprehensive statistics about the\ndataset and outline recommendations and research directions to address the\nlimitations of existing DL tools when applied to real-world CPath applications.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10778v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10704v1",
    "title": "Towards Neural Machine Translation for Edoid Languages",
    "authors": [
      "Iroro Orife"
    ],
    "author_ids": [],
    "abstract": "Many Nigerian languages have relinquished their previous prestige and purpose\nin modern society to English and Nigerian Pidgin. For the millions of L1\nspeakers of indigenous languages, there are inequalities that manifest\nthemselves as unequal access to information, communications, health care,\nsecurity as well as attenuated participation in political and civic life. To\nminimize exclusion and promote socio-linguistic and economic empowerment, this\nwork explores the feasibility of Neural Machine Translation (NMT) for the Edoid\nlanguage family of Southern Nigeria. Using the new JW300 public dataset, we\ntrained and evaluated baseline translation models for four widely spoken\nlanguages in this group: \\`Ed\\'o, \\'Es\\'an, Urhobo and Isoko. Trained models,\ncode and datasets have been open-sourced to advance future research efforts on\nEdoid language technology.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10704v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10621v1",
    "title": "A Pitfall of Learning from User-generated Data: In-depth Analysis of Subjective Class Problem",
    "authors": [
      "Kei Nemoto",
      "Shweta Jain"
    ],
    "author_ids": [],
    "abstract": "Research in the supervised learning algorithms field implicitly assumes that\ntraining data is labeled by domain experts or at least semi-professional\nlabelers accessible through crowdsourcing services like Amazon Mechanical Turk.\nWith the advent of the Internet, data has become abundant and a large number of\nmachine learning based systems started being trained with user-generated data,\nusing categorical data as true labels. However, little work has been done in\nthe area of supervised learning with user-defined labels where users are not\nnecessarily experts and might be motivated to provide incorrect labels in order\nto improve their own utility from the system. In this article, we propose two\ntypes of classes in user-defined labels: subjective class and objective class -\nshowing that the objective classes are as reliable as if they were provided by\ndomain experts, whereas the subjective classes are subject to bias and\nmanipulation by the user. We define this as a subjective class issue and\nprovide a framework for detecting subjective labels in a dataset without\nquerying oracle. Using this framework, data mining practitioners can detect a\nsubjective class at an early stage of their projects, and avoid wasting their\nprecious time and resources by dealing with subjective class problem with\ntraditional machine learning techniques.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10621v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10588v2",
    "title": "Approximate Aggregate Queries Under Additive Inequalities",
    "authors": [
      "Mahmoud Abo-Khamis",
      "Sungjin Im",
      "Benjamin Moseley",
      "Kirk Pruhs",
      "Alireza Samadian"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of evaluating certain types of functional aggregation\nqueries on relational data subject to additive inequalities. Such aggregation\nqueries, with a smallish number of additive inequalities, arise\nnaturally/commonly in many applications, particularly in learning applications.\nWe give a relatively complete categorization of the computational complexity of\nsuch problems. We first show that the problem is NP-hard, even in the case of\none additive inequality. Thus we turn to approximating the query. Our main\nresult is an efficient algorithm for approximating, with arbitrarily small\nrelative error, many natural aggregation queries with one additive inequality.\nWe give examples of natural queries that can be efficiently solved using this\nalgorithm. In contrast, we show that the situation with two additive\ninequalities is quite different, by showing that it is NP-hard to evaluate\nsimple aggregation queries, with two additive inequalities, with any bounded\nrelative error.",
    "published_date": "2020-03-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10588v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10354v6",
    "title": "Fairway: A Way to Build Fair ML Software",
    "authors": [
      "Joymallya Chakraborty",
      "Suvodeep Majumder",
      "Zhe Yu",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "Machine learning software is increasingly being used to make decisions that\naffect people's lives. But sometimes, the core part of this software (the\nlearned model), behaves in a biased manner that gives undue advantages to a\nspecific group of people (where those groups are determined by sex, race,\netc.). This \"algorithmic discrimination\" in the AI software systems has become\na matter of serious concern in the machine learning and software engineering\ncommunity. There have been works done to find \"algorithmic bias\" or \"ethical\nbias\" in the software system. Once the bias is detected in the AI software\nsystem, the mitigation of bias is extremely important. In this work, we\na)explain how ground-truth bias in training data affects machine learning model\nfairness and how to find that bias in AI software,b)propose a\nmethodFairwaywhich combines pre-processing and in-processing approach to remove\nethical bias from training data and trained model. Our results show that we can\nfind bias and mitigate bias in a learned model, without much damaging the\npredictive performance of that model. We propose that (1) test-ing for bias and\n(2) bias mitigation should be a routine part of the machine learning software\ndevelopment life cycle. Fairway offers much support for these two purposes.",
    "published_date": "2020-03-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10354v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10340v1",
    "title": "Entropy as a measure of attractiveness and socioeconomic complexity in Rio de Janeiro metropolitan area",
    "authors": [
      "Maxime Lenormand",
      "Horacio Samaniego",
      "Julio C. Chaves",
      "Vinicius F. Vieira",
      "Moacyr A. H. B. da Silva",
      "Alexandre G. Evsukoff"
    ],
    "author_ids": [],
    "abstract": "Defining and measuring spatial inequalities across the urban environment\nremains a complex and elusive task that has been facilitated by the increasing\navailability of large geolocated databases. In this study, we rely on a mobile\nphone dataset and an entropy-based metric to measure the attractiveness of a\nlocation in the Rio de Janeiro Metropolitan Area (Brazil) as the diversity of\nvisitors' location of residence. The results show that the attractiveness of a\ngiven location measured by entropy is an important descriptor of the\nsocioeconomic status of the location, and can thus be used as a proxy for\ncomplex socioeconomic indicators.",
    "published_date": "2020-03-23T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10340v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.10076v1",
    "title": "Absolute Shapley Value",
    "authors": [
      "Jinfei Liu"
    ],
    "author_ids": [],
    "abstract": "Shapley value is a concept in cooperative game theory for measuring the\ncontribution of each participant, which was named in honor of Lloyd Shapley.\nShapley value has been recently applied in data marketplaces for compensation\nallocation based on their contribution to the models. Shapley value is the only\nvalue division scheme used for compensation allocation that meets three\ndesirable criteria: group rationality, fairness, and additivity. In cooperative\ngame theory, the marginal contribution of each contributor to each coalition is\na nonnegative value. However, in machine learning model training, the marginal\ncontribution of each contributor (data tuple) to each coalition (a set of data\ntuples) can be a negative value, i.e., the accuracy of the model trained by a\ndataset with an additional data tuple can be lower than the accuracy of the\nmodel trained by the dataset only.\n  In this paper, we investigate the problem of how to handle the negative\nmarginal contribution when computing Shapley value. We explore three\nphilosophies: 1) taking the original value (Original Shapley Value); 2) taking\nthe larger of the original value and zero (Zero Shapley Value); and 3) taking\nthe absolute value of the original value (Absolute Shapley Value). Experiments\non Iris dataset demonstrate that the definition of Absolute Shapley Value\nsignificantly outperforms the other two definitions in terms of evaluating data\nimportance (the contribution of each data tuple to the trained model).",
    "published_date": "2020-03-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DB",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10076v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10074v3",
    "title": "Dragoon: Private Decentralized HITs Made Practical",
    "authors": [
      "Yuan Lu",
      "Qiang Tang",
      "Guiling Wang"
    ],
    "author_ids": [],
    "abstract": "With the rapid popularity of blockchain, decentralized human intelligence\ntasks (HITs) are proposed to crowdsource human knowledge without relying on\nvulnerable third-party platforms. However, the inherent limits of blockchain\ncause decentralized HITs to face a few \"new\" challenges. For example, the\nconfidentiality of solicited data turns out to be the sine qua non, though it\nwas an arguably dispensable property in the centralized setting. To ensure the\n\"new\" requirement of data privacy, existing decentralized HITs use generic\nzero-knowledge proof frameworks (e.g. SNARK), but scarcely perform well in\npractice, due to the inherently expensive cost of generality.\n  We present a practical decentralized protocol for HITs, which also achieves\nthe fairness between requesters and workers. At the core of our contributions,\nwe avoid the powerful yet highly-costly generic zk-proof tools and propose a\nspecial-purpose scheme to prove the quality of encrypted data. By various\nnon-trivial statement reformations, proving the quality of encrypted data is\nreduced to efficient verifiable decryption, thus making decentralized HITs\npractical. Along the way, we rigorously define the ideal functionality of\ndecentralized HITs and then prove the security due to the ideal-real paradigm.\n  We further instantiate our protocol to implement a system called Dragoon, an\ninstance of which is deployed atop Ethereum to facilitate an image annotation\ntask used by ImageNet. Our evaluations demonstrate its practicality: the\non-chain handling cost of Dragoon is even less than the handling fee of\nAmazon's Mechanical Turk for the same ImageNet HIT.",
    "published_date": "2020-03-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10074v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.10065v1",
    "title": "Linguistically Driven Graph Capsule Network for Visual Question Reasoning",
    "authors": [
      "Qingxing Cao",
      "Xiaodan Liang",
      "Keze Wang",
      "Liang Lin"
    ],
    "author_ids": [],
    "abstract": "Recently, studies of visual question answering have explored various\narchitectures of end-to-end networks and achieved promising results on both\nnatural and synthetic datasets, which require explicitly compositional\nreasoning. However, it has been argued that these black-box approaches lack\ninterpretability of results, and thus cannot perform well on generalization\ntasks due to overfitting the dataset bias. In this work, we aim to combine the\nbenefits of both sides and overcome their limitations to achieve an end-to-end\ninterpretable structural reasoning for general images without the requirement\nof layout annotations. Inspired by the property of a capsule network that can\ncarve a tree structure inside a regular convolutional neural network (CNN), we\npropose a hierarchical compositional reasoning model called the \"Linguistically\ndriven Graph Capsule Network\", where the compositional process is guided by the\nlinguistic parse tree. Specifically, we bind each capsule in the lowest layer\nto bridge the linguistic embedding of a single word in the original question\nwith visual evidence and then route them to the same capsule if they are\nsiblings in the parse tree. This compositional process is achieved by\nperforming inference on a linguistically driven conditional random field (CRF)\nand is performed across multiple graph capsule layers, which results in a\ncompositional reasoning process inside a CNN. Experiments on the CLEVR dataset,\nCLEVR compositional generation test, and FigureQA dataset demonstrate the\neffectiveness and composition generalization ability of our end-to-end model.",
    "published_date": "2020-03-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10065v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.10033v2",
    "title": "Additive Angular Margin for Few Shot Learning to Classify Clinical Endoscopy Images",
    "authors": [
      "Sharib Ali",
      "Binod Bhattarai",
      "Tae-Kyun Kim",
      "Jens Rittscher"
    ],
    "author_ids": [],
    "abstract": "Endoscopy is a widely used imaging modality to diagnose and treat diseases in\nhollow organs as for example the gastrointestinal tract, the kidney and the\nliver. However, due to varied modalities and use of different imaging protocols\nat various clinical centers impose significant challenges when generalising\ndeep learning models. Moreover, the assembly of large datasets from different\nclinical centers can introduce a huge label bias that renders any learnt model\nunusable. Also, when using new modality or presence of images with rare\npatterns, a bulk amount of similar image data and their corresponding labels\nare required for training these models. In this work, we propose to use a\nfew-shot learning approach that requires less training data and can be used to\npredict label classes of test samples from an unseen dataset. We propose a\nnovel additive angular margin metric in the framework of prototypical network\nin few-shot learning setting. We compare our approach to the several\nestablished methods on a large cohort of multi-center, multi-organ, and\nmulti-modal endoscopy data. The proposed algorithm outperforms existing\nstate-of-the-art methods.",
    "published_date": "2020-03-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.10033v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.09956v2",
    "title": "Scalable parallel algorithm for solving non-stationary systems of linear inequalities",
    "authors": [
      "Leonid B. Sokolinsky",
      "Irina M. Sokolinskaya"
    ],
    "author_ids": [],
    "abstract": "In this paper, a scalable iterative projection-type algorithm for solving\nnon-stationary systems of linear inequalities is considered. A non-stationary\nsystem is understood as a large-scale system of inequalities in which\ncoefficients and constant terms can change during the calculation process. The\nproposed parallel algorithm uses the concept of pseudo-projection which\ngeneralizes the notion of orthogonal projection. The parallel pseudo-projection\nalgorithm is implemented using the parallel BSF-skeleton. An analytical\nestimation of the algorithm scalability boundary is obtained on the base of the\nBSF cost metric. The large-scale computational experiments were performed on a\ncluster computing system. The obtained results confirm the efficiency of the\nproposed approach.",
    "published_date": "2020-03-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MS",
      "cs.NA",
      "math.NA",
      "math.OC",
      "90-08",
      "G.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.09956v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.09808v1",
    "title": "Tracking an Auto-Regressive Process with Limited Communication per Unit Time",
    "authors": [
      "Rooji Jinan",
      "Parimal Parag",
      "Himanshu Tyagi"
    ],
    "author_ids": [],
    "abstract": "Samples from a high-dimensional AR[1] process are observed by a sender which\ncan communicate only finitely many bits per unit time to a receiver. The\nreceiver seeks to form an estimate of the process value at every time instant\nin real-time. We consider a time-slotted communication model in a slow-sampling\nregime where multiple communication slots occur between two sampling instants.\nWe propose a successive update scheme which uses communication between sampling\ninstants to refine estimates of the latest sample and study the following\nquestion: Is it better to collect communication of multiple slots to send\nbetter refined estimates, making the receiver wait more for every refinement,\nor to be fast but loose and send new information in every communication\nopportunity? We show that the fast but loose successive update scheme with\nideal spherical codes is universally optimal asymptotically for a large\ndimension. However, most practical quantization codes for fixed dimensions do\nnot meet the ideal performance required for this optimality, and they typically\nwill have a bias in the form of a fixed additive error. Interestingly, our\nanalysis shows that the fast but loose scheme is not an optimal choice in the\npresence of such errors, and a judiciously chosen frequency of updates\noutperforms it.",
    "published_date": "2020-03-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.09808v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.09609v1",
    "title": "Fault-tolerant Coherent H-infinity Control for Linear Quantum Systems",
    "authors": [
      "Yanan Liu",
      "Daoyi Dong",
      "Ian R. Petersen",
      "Qing Gao",
      "Steven X. Ding",
      "Shota Yokoyama",
      "Hidehiro Yonezawa"
    ],
    "author_ids": [],
    "abstract": "Robustness and reliability are two key requirements for developing practical\nquantum control systems. The purpose of this paper is to design a coherent\nfeedback controller for a class of linear quantum systems suffering from\nMarkovian jumping faults so that the closed-loop quantum system has both fault\ntolerance and H-infinity disturbance attenuation performance. This paper first\nextends the physical realization conditions from the time-invariant case to the\ntime-varying case for linear stochastic quantum systems. By relating the fault\ntolerant H-infinity control problem to the dissipation properties and the\nsolutions of Riccati differential equations, an H-infinity controller for the\nquantum system is then designed by solving a set of linear matrix inequalities\n(LMIs). In particular, an algorithm is employed to introduce additional noises\nand to construct the corresponding input matrices to ensure the physical\nrealizability of the quantum controller. For real applications of the developed\nfault-tolerant control strategy, we present a linear quantum system example\nfrom quantum optics, where the amplitude of the pumping field randomly jumps\namong different values. It is demonstrated that a quantum H-infinity controller\ncan be designed and implemented using some basic optical components to achieve\nthe desired control goal.",
    "published_date": "2020-03-21T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.09609v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.09371v1",
    "title": "Learning-based Bias Correction for Ultra-wideband Localization of Resource-constrained Mobile Robots",
    "authors": [
      "Wenda Zhao",
      "Abhishek Goudar",
      "Jacopo Panerati",
      "Angela P. Schoellig"
    ],
    "author_ids": [],
    "abstract": "Accurate indoor localization is a crucial enabling technology for many\nrobotics applications, from warehouse management to monitoring tasks.\nUltra-wideband (UWB) ranging is a promising solution which is low-cost,\nlightweight, and computationally inexpensive compared to alternative\nstate-of-the-art approaches such as simultaneous localization and mapping,\nmaking it especially suited for resource-constrained aerial robots. Many\ncommercially-available ultra-wideband radios, however, provide inaccurate,\nbiased range measurements. In this article, we propose a bias correction\nframework compatible with both two-way ranging and time difference of arrival\nultra-wideband localization. Our method comprises of two steps: (i) statistical\noutlier rejection and (ii) a learning-based bias correction. This approach is\nscalable and frugal enough to be deployed on-board a nano-quadcopter's\nmicrocontroller. Previous research mostly focused on two-way ranging bias\ncorrection and has not been implemented in closed-loop nor using\nresource-constrained robots. Experimental results show that, using our\napproach, the localization error is reduced by ~18.5% and 48% (for TWR and\nTDoA, respectively), and a quadcopter can accurately track trajectories with\nposition information from UWB only.",
    "published_date": "2020-03-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "I.2.6; I.2.9; J.2; J.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.09371v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.09168v4",
    "title": "Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention",
    "authors": [
      "Andres C. Rodriguez",
      "Stefano D'Aronco",
      "Konrad Schindler",
      "Jan Dirk Wegner"
    ],
    "author_ids": [],
    "abstract": "We propose a scheme for supervised image classification that uses privileged\ninformation, in the form of keypoint annotations for the training data, to\nlearn strong models from small and/or biased training sets. Our main motivation\nis the recognition of animal species for ecological applications such as\nbiodiversity modelling, which is challenging because of long-tailed species\ndistributions due to rare species, and strong dataset biases such as repetitive\nscene background in camera traps. To counteract these challenges, we propose a\nvisual attention mechanism that is supervised via keypoint annotations that\nhighlight important object parts. This privileged information, implemented as a\nnovel privileged pooling operation, is only required during training and helps\nthe model to focus on regions that are discriminative. In experiments with\nthree different animal species datasets, we show that deep networks with\nprivileged pooling can use small training sets more efficiently and generalize\nbetter.",
    "published_date": "2020-03-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.09168v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.08800v1",
    "title": "Cardiovascular risk and work stress in biomedical researchers in China: An observational, big data study protocol",
    "authors": [
      "Fang Zhu",
      "Qian Zhang",
      "Hao Chen",
      "Guocheng Shi",
      "Chen Wen",
      "Zhongqun Zhu",
      "Huiwen Chen"
    ],
    "author_ids": [],
    "abstract": "Introduction: Internet technologies could strengthen data collection and\nintegration and have been used extensively in public health research. It is\nnecessary to apply this technology to further investigate the behaviour and\nhealth of biomedical researchers. A browser-based extension was developed by\nresearchers and clinicians to promote the collection and analysis of\nresearchers' behavioural and psychological data. This protocol illustrates an\nobservational study aimed at (1) characterising the health status of biomedical\nresearchers in China and assessing work stress, job satisfaction, role\nconflict, role ambiguity, and family support; (2) identifying the association\nbetween work, behaviour, and health; and (3) investigating the association\nbetween behaviour and mental status. Our findings will contribute to the\nunderstanding of the influences of job, work environment, and family support on\nthe mental and physical health of biomedical researchers. Methods and analysis:\nThis is a prospective observational study; all candidates will be recruited\nfrom China. Participants will install an extension on their Internet browsers,\nwhich will collect data when they are accessing PubMed. A web-based survey will\nbe sent to the user interfaces every 6 months that will involve\nsociodemographic variables, perceived stress scale, job satisfaction scale,\nrole conflict and ambiguity scale, and family support scale. Machine-learning\nalgorithms will analyse the data generated during daily access. Ethics and\ndissemination: This study received ethical approval from the ethics committee\nof the Shanghai Children's Medical Centre (reference number SCMCIRB-K2018082).\nStudy results will be disseminated through peer-reviewed publications and\nconference presentations.",
    "published_date": "2020-03-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08800v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.08619v1",
    "title": "FAURAS: A Proxy-based Framework for Ensuring the Fairness of Adaptive Video Streaming over HTTP/2 Server Push",
    "authors": [
      "Chanh Minh Tran",
      "Tho Nguyen Duc",
      "Phan Xuan Tan",
      "Eiji Kamioka"
    ],
    "author_ids": [],
    "abstract": "HTTP/2 video streaming has caught a lot of attentions in the development of\nmultimedia technologies over the last few years. In HTTP/2, the server push\nmechanism allows the server to deliver more video segments to the client within\na single request in order to deal with the requests explosion problem. As a\nresult, recent research efforts have been focusing on utilizing such a feature\nto enhance the streaming experience while reducing the request-related\noverhead. However, current works only optimize the performance of a single\nclient, without necessary concerns of possible influences on other clients in\nthe same network. When multiple streaming clients compete for a shared\nbandwidth in HTTP/1.1, they are likely to suffer from unfairness, which is\ndefined as the inequality in their bitrate selections. For HTTP/1.1, existing\nworks have proven that the network-assisted solutions are effective in solving\nthe unfairness problem. However, the feasibility of utilizing such an approach\nfor the HTTP/2 server push has not been investigated. Therefore, in this paper,\na novel proxy-based framework is proposed to overcome the unfairness problem in\nadaptive streaming over HTTP/2 with the server push. Experimental results\nconfirm the outperformance of the proposed framework in ensuring the fairness,\nassisting the clients to avoid rebuffering events and lower bitrate degradation\namplitude, while maintaining the mechanism of the server push feature.",
    "published_date": "2020-03-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08619v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.08342v1",
    "title": "Bootstrap Bias Corrected Cross Validation applied to Super Learning",
    "authors": [
      "Krzysztof Mnich",
      "Agnieszka Kitlas Golińska",
      "Aneta Polewko-Klim",
      "Witold R. Rudnicki"
    ],
    "author_ids": [],
    "abstract": "Super learner algorithm can be applied to combine results of multiple base\nlearners to improve quality of predictions. The default method for verification\nof super learner results is by nested cross validation. It has been proposed by\nTsamardinos et al., that nested cross validation can be replaced by resampling\nfor tuning hyper-parameters of the learning algorithms. We apply this idea to\nverification of super learner and compare with other verification methods,\nincluding nested cross validation. Tests were performed on artificial data sets\nof diverse size and on seven real, biomedical data sets. The resampling method,\ncalled Bootstrap Bias Correction, proved to be a reasonably precise and very\ncost-efficient alternative for nested cross validation.",
    "published_date": "2020-03-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.08835v1",
    "title": "Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media",
    "authors": [
      "Massimo Stella"
    ],
    "author_ids": [],
    "abstract": "Mindset reconstruction maps how individuals structure and perceive knowledge,\na map unfolded here by investigating language and its cognitive reflection in\nthe human mind, i.e. the mental lexicon. Textual forma mentis networks (TFMN)\nare glass boxes introduced for extracting, representing and understanding\nmindsets' structure, in Latin \"forma mentis\", from textual data. Combining\nnetwork science, psycholinguistics and Big Data, TFMNs successfully identified\nrelevant concepts, without supervision, in benchmark texts. Once validated,\nTFMNs were applied to the case study of the gender gap in science, which was\nstrongly linked to distorted mindsets by recent studies. Focusing over social\nmedia perception and online discourse, this work analysed 10,000 relevant\ntweets. \"Gender\" and \"gap\" elicited a mostly positive perception, with a\ntrustful/joyous emotional profile and semantic associates that: celebrated\nsuccessful female scientists, related gender gap to wage differences, and hoped\nfor a future resolution. The perception of \"woman\" highlighted discussion about\nsexual harassment and stereotype threat (a form of implicit cognitive bias)\nrelative to women in science \"sacrificing personal skills for success\". The\nreconstructed perception of \"man\" highlighted social users' awareness of the\nmyth of male superiority in science. No anger was detected around \"person\",\nsuggesting that gap-focused discourse got less tense around genderless terms.\nNo stereotypical perception of \"scientist\" was identified online, differently\nfrom real-world surveys. The overall analysis identified the online discourse\nas promoting a mostly stereotype-free, positive/trustful perception of gender\ndisparity, aware of implicit/explicit biases and projected to closing the gap.\nTFMNs opened new ways for investigating perceptions in different groups,\noffering detailed data-informed grounding for policy making.",
    "published_date": "2020-03-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08835v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.08194v1",
    "title": "Optimization of Rate Fairness in Multi-Pair Wireless-Powered Relaying Systems",
    "authors": [
      "Van-Phuc Bui",
      "Van-Dinh Nguyen",
      "Hieu V. Nguyen",
      "Octavia A. Dobre",
      "Oh-Soon Shin"
    ],
    "author_ids": [],
    "abstract": "This letter considers a multi-pair decode-and-forward relay network where a\npower-splitting (PS) protocol is adopted at the energy-constrained relay to\nprovide simultaneous wireless information and energy harvesting (EH). To\nachieve higher efficiency of EH, we propose a new PS-based EH architecture at\nthe relay by incorporating an alternating current (AC) computing logic, which\nis employed to directly use the wirelessly harvested AC energy for\ncomputational blocks. Under a nonlinear EH circuit, our goal is to maximize the\nfairness of end-to-end rate among user pairs subject to power constraints,\nresulting in a nonconvex problem. We propose an iterative algorithm to achieve\na suboptimal and efficient solution to this challenging problem by leveraging\nthe inner approximation framework. Numerical results demonstrate that the\nproposed algorithm outperforms the traditional direct current computing and\nother baseline schemes.",
    "published_date": "2020-03-18T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08194v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.08132v1",
    "title": "Gender Representation in Open Source Speech Resources",
    "authors": [
      "Mahault Garnerin",
      "Solange Rossato",
      "Laurent Besacier"
    ],
    "author_ids": [],
    "abstract": "With the rise of artificial intelligence (AI) and the growing use of\ndeep-learning architectures, the question of ethics, transparency and fairness\nof AI systems has become a central concern within the research community. We\naddress transparency and fairness in spoken language systems by proposing a\nstudy about gender representation in speech resources available through the\nOpen Speech and Language Resource platform. We show that finding gender\ninformation in open source corpora is not straightforward and that gender\nbalance depends on other corpus characteristics (elicited/non elicited speech,\nlow/high resource language, speech task targeted). The paper ends with\nrecommendations about metadata and gender information for researchers in order\nto assure better transparency of the speech systems built using such corpora.",
    "published_date": "2020-03-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08132v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.08086v11",
    "title": "Tracking COVID-19 using online search",
    "authors": [
      "Vasileios Lampos",
      "Maimuna S. Majumder",
      "Elad Yom-Tov",
      "Michael Edelstein",
      "Simon Moura",
      "Yohhei Hamada",
      "Molebogeng X. Rangaka",
      "Rachel A. McKendry",
      "Ingemar J. Cox"
    ],
    "author_ids": [],
    "abstract": "Previous research has demonstrated that various properties of infectious\ndiseases can be inferred from online search behaviour. In this work we use time\nseries of online search query frequencies to gain insights about the prevalence\nof COVID-19 in multiple countries. We first develop unsupervised modelling\ntechniques based on associated symptom categories identified by the United\nKingdom's National Health Service and Public Health England. We then attempt to\nminimise an expected bias in these signals caused by public interest -- as\nopposed to infections -- using the proportion of news media coverage devoted to\nCOVID-19 as a proxy indicator. Our analysis indicates that models based on\nonline searches precede the reported confirmed cases and deaths by 16.7 (10.2 -\n23.2) and 22.1 (17.4 - 26.9) days, respectively. We also investigate transfer\nlearning techniques for mapping supervised models from countries where the\nspread of disease has progressed extensively to countries that are in earlier\nphases of their respective epidemic curves. Furthermore, we compare time series\nof online search activity against confirmed COVID-19 cases or deaths jointly\nacross multiple countries, uncovering interesting querying patterns, including\nthe finding that rarer symptoms are better predictors than common ones.\nFinally, we show that web searches improve the short-term forecasting accuracy\nof autoregressive models for COVID-19 deaths. Our work provides evidence that\nonline search data can be used to develop complementary public health\nsurveillance methods to help inform the COVID-19 response in conjunction with\nmore established approaches.",
    "published_date": "2020-03-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08086v11",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.07886v2",
    "title": "A Relaxed Inertial Forward-Backward-Forward Algorithm for Solving Monotone Inclusions with Application to GANs",
    "authors": [
      "Radu Ioan Bot",
      "Michael Sedlmayer",
      "Phan Tu Vuong"
    ],
    "author_ids": [],
    "abstract": "We introduce a relaxed inertial forward-backward-forward (RIFBF) splitting\nalgorithm for approaching the set of zeros of the sum of a maximally monotone\noperator and a single-valued monotone and Lipschitz continuous operator. This\nwork aims to extend Tseng's forward-backward-forward method by both using\ninertial effects as well as relaxation parameters. We formulate first a second\norder dynamical system which approaches the solution set of the monotone\ninclusion problem to be solved and provide an asymptotic analysis for its\ntrajectories. We provide for RIFBF, which follows by explicit time\ndiscretization, a convergence analysis in the general monotone case as well as\nwhen applied to the solving of pseudo-monotone variational inequalities. We\nillustrate the proposed method by applications to a bilinear saddle point\nproblem, in the context of which we also emphasize the interplay between the\ninertial and the relaxation parameters, and to the training of Generative\nAdversarial Networks (GANs).",
    "published_date": "2020-03-17T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG",
      "47J20, 90C25, 90C30, 90C52"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07886v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.08209v1",
    "title": "Identification and Classification of Phenomena in Multispectral Satellite Imagery Using a New Image Smoother Method and its Applications in Environmental Remote Sensing",
    "authors": [
      "M. Kiani"
    ],
    "author_ids": [],
    "abstract": "In this paper a new method of image smoothing for satellite imagery and its\napplications in environmental remote sensing are presented. This method is\nbased on the global gradient minimization over the whole image. With respect to\nthe image discrete identity, the continuous minimization problem is\ndiscretized. Using the finite difference numerical method of differentiation, a\nsimple yet efficient 5*5-pixel template is derived. Convolution of the derived\ntemplate with the image in different bands results in the discrimination of\nvarious image elements. This method is extremely fast, besides being highly\nprecise. A case study is presented for the northern Iran, covering parts of the\nCaspian Sea. Comparison of the method with the usual Laplacian template reveals\nthat it is more capable of distinguishing phenomena in the image.",
    "published_date": "2020-03-17T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08209v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.07407v3",
    "title": "Nonlinear Stochastic Estimators on the Special Euclidean Group SE(3) using Uncertain IMU and Vision Measurements",
    "authors": [
      "Hashim A Hashim",
      "Frank L Lewis"
    ],
    "author_ids": [],
    "abstract": "Two novel robust nonlinear stochastic full pose (i.e, attitude and position)\nestimators on the Special Euclidean Group SE(3) are proposed using the\navailable uncertain measurements. The resulting estimators utilize the basic\nstructure of the deterministic pose estimators adopting it to the stochastic\nsense. The proposed estimators for six degrees of freedom (DOF) pose\nestimations consider the group velocity vectors to be contaminated with\nconstant bias and Gaussian random noise, unlike nonlinear deterministic pose\nestimators which disregard the noise component in the estimator derivations.\nThe proposed estimators ensure that the closed loop error signals are\nsemi-globally uniformly ultimately bounded in mean square. The efficiency and\nrobustness of the proposed estimators are demonstrated by the numerical results\nwhich test the estimators against high levels of noise and bias associated with\nthe group velocity and body-frame measurements and large initialization error.\nKeywords: Nonlinear stochastic pose filter, pose observer, position, attitude,\nIto, stochastic differential equations, Brownian motion process, adaptive\nestimate, feature, inertial measurement unit, inertial vision system, 6 DOF,\nIMU, SE(3), SO(3), orientation, landmark, Gaussian, noise.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07407v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.07370v1",
    "title": "Harnessing Explanations to Bridge AI and Humans",
    "authors": [
      "Vivian Lai",
      "Samuel Carton",
      "Chenhao Tan"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are increasingly integrated into societally critical\napplications such as recidivism prediction and medical diagnosis, thanks to\ntheir superior predictive power. In these applications, however, full\nautomation is often not desired due to ethical and legal concerns. The research\ncommunity has thus ventured into developing interpretable methods that explain\nmachine predictions. While these explanations are meant to assist humans in\nunderstanding machine predictions and thereby allowing humans to make better\ndecisions, this hypothesis is not supported in many recent studies. To improve\nhuman decision-making with AI assistance, we propose future directions for\nclosing the gap between the efficacy of explanations and improvement in human\nperformance.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07370v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.07341v1",
    "title": "Complexity of Shapes Embedded in ${\\mathbb Z^n}$ with a Bias Towards Squares",
    "authors": [
      "M. Ferhat Arslan",
      "Sibel Tari"
    ],
    "author_ids": [],
    "abstract": "Shape complexity is a hard-to-quantify quality, mainly due to its relative\nnature. Biased by Euclidean thinking, circles are commonly considered as the\nsimplest. However, their constructions as digital images are only\napproximations to the ideal form. Consequently, complexity orders computed in\nreference to circle are unstable. Unlike circles which lose their circleness in\ndigital images, squares retain their qualities. Hence, we consider squares\n(hypercubes in $\\mathbb Z^n$) to be the simplest shapes relative to which\ncomplexity orders are constructed. Using the connection between $L^\\infty$ norm\nand squares we effectively encode squareness-adapted simplification through\nwhich we obtain multi-scale complexity measure, where scale determines the\nlevel of interest to the boundary. The emergent scale above which the effect of\na boundary feature (appendage) disappears is related to the ratio of the\ncontacting width of the appendage to that of the main body. We discuss what\nzero complexity implies in terms of information repetition and constructibility\nand what kind of shapes in addition to squares have zero complexity.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.CG",
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07341v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.07146v2",
    "title": "Inference and Influence of Large-Scale Social Networks Using Snapshot Population Behaviour without Network Data",
    "authors": [
      "Antonia Godoy-Lorite",
      "Nick S. Jones"
    ],
    "author_ids": [],
    "abstract": "Population behaviours, such as voting and vaccination, depend on social\nnetworks. Social networks can differ depending on behaviour type and are\ntypically hidden. However, we do often have large-scale behavioural data,\nalbeit only snapshots taken at one timepoint. We present a method that jointly\ninfers large-scale network structure and a networked model of human behaviour\nusing only snapshot population behavioural data. This exploits the simplicity\nof a few parameter, geometric socio-demographic network model and a spin based\nmodel of behaviour. We illustrate, for the EU Referendum and two London Mayoral\nelections, how the model offers both prediction and the interpretation of our\nhomophilic inclinations. Beyond offering the extraction of behaviour specific\nnetwork structure from large-scale behavioural datasets, our approach yields a\ncrude calculus linking inequalities and social preferences to behavioural\noutcomes. We give examples of potential network sensitive policies: how changes\nto income inequality, a social temperature and homophilic preferences might\nhave reduced polarisation in a recent election.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07146v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.07139v1",
    "title": "Discriminative Feature and Dictionary Learning with Part-aware Model for Vehicle Re-identification",
    "authors": [
      "Huibing Wang",
      "Jinjia Peng",
      "Guangqi Jiang",
      "Fengqiang Xu",
      "Xianping Fu"
    ],
    "author_ids": [],
    "abstract": "With the development of smart cities, urban surveillance video analysis will\nplay a further significant role in intelligent transportation systems.\nIdentifying the same target vehicle in large datasets from non-overlapping\ncameras should be highlighted, which has grown into a hot topic in promoting\nintelligent transportation systems. However, vehicle re-identification (re-ID)\ntechnology is a challenging task since vehicles of the same design or\nmanufacturer show similar appearance. To fill these gaps, we tackle this\nchallenge by proposing Triplet Center Loss based Part-aware Model (TCPM) that\nleverages the discriminative features in part details of vehicles to refine the\naccuracy of vehicle re-identification. TCPM base on part discovery is that\npartitions the vehicle from horizontal and vertical directions to strengthen\nthe details of the vehicle and reinforce the internal consistency of the parts.\nIn addition, to eliminate intra-class differences in local regions of the\nvehicle, we propose external memory modules to emphasize the consistency of\neach part to learn the discriminating features, which forms a global dictionary\nover all categories in dataset. In TCPM, triplet-center loss is introduced to\nensure each part of vehicle features extracted has intra-class consistency and\ninter-class separability. Experimental results show that our proposed TCPM has\nan enormous preference over the existing state-of-the-art methods on benchmark\ndatasets VehicleID and VeRi-776.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07139v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.07122v1",
    "title": "A Novel Framework with Information Fusion and Neighborhood Enhancement for User Identity Linkage",
    "authors": [
      "Siyuan Chen",
      "Jiahai Wang",
      "Xin Du",
      "Yanqing Hu"
    ],
    "author_ids": [],
    "abstract": "User identity linkage across social networks is an essential problem for\ncross-network data mining. Since network structure, profile and content\ninformation describe different aspects of users, it is critical to learn\neffective user representations that integrate heterogeneous information. This\npaper proposes a novel framework with INformation FUsion and Neighborhood\nEnhancement (INFUNE) for user identity linkage. The information fusion\ncomponent adopts a group of encoders and decoders to fuse heterogeneous\ninformation and generate discriminative node embeddings for preliminary\nmatching. Then, these embeddings are fed to the neighborhood enhancement\ncomponent, a novel graph neural network, to produce adaptive neighborhood\nembeddings that reflect the overlapping degree of neighborhoods of varying\ncandidate user pairs. The importance of node embeddings and neighborhood\nembeddings are weighted for final prediction. The proposed method is evaluated\non real-world social network data. The experimental results show that INFUNE\nsignificantly outperforms existing state-of-the-art methods.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07122v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.07060v4",
    "title": "Finding Fair and Efficient Allocations When Valuations Don't Add Up",
    "authors": [
      "Nawal Benabbou",
      "Mithun Chakraborty",
      "Ayumi Igarashi",
      "Yair Zick"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present new results on the fair and efficient allocation of\nindivisible goods to agents whose preferences correspond to {\\em matroid rank\nfunctions}. This is a versatile valuation class with several desirable\nproperties (such as monotonicity and submodularity), which naturally lends\nitself to a number of real-world domains. We use these properties to our\nadvantage; first, we show that when agent valuations are matroid rank\nfunctions, a socially optimal (i.e. utilitarian social welfare-maximizing)\nallocation that achieves envy-freeness up to one item (EF1) exists and is\ncomputationally tractable. We also prove that the Nash welfare-maximizing and\nthe leximin allocations both exhibit this fairness/efficiency combination, by\nshowing that they can be achieved by minimizing any symmetric strictly convex\nfunction over utilitarian optimal outcomes. To the best of our knowledge, this\nis the first valuation function class not subsumed by additive valuations for\nwhich it has been established that an allocation maximizing Nash welfare is\nEF1. Moreover, for a subclass of these valuation functions based on maximum\n(unweighted) bipartite matching, we show that a leximin allocation can be\ncomputed in polynomial time. Additionally, we explore possible extensions of\nour results to fairness criteria other than EF1 as well as to generalizations\nof the above valuation classes.",
    "published_date": "2020-03-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07060v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06920v1",
    "title": "Getting Fairness Right: Towards a Toolbox for Practitioners",
    "authors": [
      "Boris Ruf",
      "Chaouki Boutharouite",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "The potential risk of AI systems unintentionally embedding and reproducing\nbias has attracted the attention of machine learning practitioners and society\nat large. As policy makers are willing to set the standards of algorithms and\nAI techniques, the issue on how to refine existing regulation, in order to\nenforce that decisions made by automated systems are fair and\nnon-discriminatory, is again critical. Meanwhile, researchers have demonstrated\nthat the various existing metrics for fairness are statistically mutually\nexclusive and the right choice mostly depends on the use case and the\ndefinition of fairness.\n  Recognizing that the solutions for implementing fair AI are not purely\nmathematical but require the commitments of the stakeholders to define the\ndesired nature of fairness, this paper proposes to draft a toolbox which helps\npractitioners to ensure fair AI practices. Based on the nature of the\napplication and the available training data, but also on legal requirements and\nethical, philosophical and cultural dimensions, the toolbox aims to identify\nthe most appropriate fairness objective. This approach attempts to structure\nthe complex landscape of fairness metrics and, therefore, makes the different\navailable options more accessible to non-technical people. In the proven\nabsence of a silver bullet solution for fair AI, this toolbox intends to\nproduce the fairest AI systems possible with respect to their local context.",
    "published_date": "2020-03-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06920v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06740v4",
    "title": "Balancing Competing Objectives with Noisy Data: Score-Based Classifiers for Welfare-Aware Machine Learning",
    "authors": [
      "Esther Rolf",
      "Max Simchowitz",
      "Sarah Dean",
      "Lydia T. Liu",
      "Daniel Björkegren",
      "Moritz Hardt",
      "Joshua Blumenstock"
    ],
    "author_ids": [],
    "abstract": "While real-world decisions involve many competing objectives, algorithmic\ndecisions are often evaluated with a single objective function. In this paper,\nwe study algorithmic policies which explicitly trade off between a private\nobjective (such as profit) and a public objective (such as social welfare). We\nanalyze a natural class of policies which trace an empirical Pareto frontier\nbased on learned scores, and focus on how such decisions can be made in noisy\nor data-limited regimes. Our theoretical results characterize the optimal\nstrategies in this class, bound the Pareto errors due to inaccuracies in the\nscores, and show an equivalence between optimal strategies and a rich class of\nfairness-constrained profit-maximizing policies. We then present empirical\nresults in two different contexts -- online content recommendation and\nsustainable abalone fisheries -- to underscore the applicability of our\napproach to a wide range of practical decisions. Taken together, these results\nshed light on inherent trade-offs in using machine learning for decisions that\nimpact social welfare.",
    "published_date": "2020-03-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06740v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06656v1",
    "title": "Audio-Visual Spatial Aligment Requirements of Central and Peripheral Object Events",
    "authors": [
      "Davide Berghi",
      "Hanne Stenzel",
      "Marco Volino",
      "Adrian Hilton",
      "Philip J. B. Jackson"
    ],
    "author_ids": [],
    "abstract": "Immersive audio-visual perception relies on the spatial integration of both\nauditory and visual information which are heterogeneous sensing modalities with\ndifferent fields of reception and spatial resolution. This study investigates\nthe perceived coherence of audiovisual object events presented either centrally\nor peripherally with horizontally aligned/misaligned sound. Various object\nevents were selected to represent three acoustic feature classes. Subjective\ntest results in a simulated virtual environment from 18 participants indicate a\nwider capture region in the periphery, with an outward bias favoring more\nlateral sounds. Centered stimulus results support previous findings for simpler\nscenes.",
    "published_date": "2020-03-14T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.SD",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06656v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.06530v3",
    "title": "Ethics in the digital era",
    "authors": [
      "David Pastor-Escuredo"
    ],
    "author_ids": [],
    "abstract": "Ethics is an ancient matter for human kind, from the origin of civilizations\nethics have been related with the most relevant human concerns and determined\ncultures. Ethics was initially related to religion, politics and philosophy to\nthen be fragmented into specific communities of practice. The undergoing\ndigital revolution enabled by Artificial Intelligence and Data are bringing\nethical wicked problems in the social application of these technologies.\nHowever, a broader perspective is also necessary. We now face global and highly\ndynamics challenges that affect groups and individuals, specially those that\nare most vulnerable. Individual-oriented ethics are no longer sufficient, the\nnew ethic has to consider the several scales in which the current complex\nsociety is organized and the interconnections between different systems. Ethics\nshould also give a response to the systemic changes in behavior produced by\nexternal factors and threats. Furthermore, AI and digital technologies are\nglobal and make us more connected and smart but also more homogeneous,\npredictable and ultimately controllable. Ethic must take a stand to preserve\nand keep promoting individuals rights and uniqueness and cultural\nheterogeneity. Digital technologies have to the foundation for new models of\nsociety and help ensure ethical individual and collective values. For these\nreasons science has to be at the core of the new ethic as it helps understand\nthe complex world. Finally, AI has advanced through the ambition to humanize\nmatter, so we should expect ethics to give a response to the future status of\nmachines and their interactions with humans.",
    "published_date": "2020-03-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06530v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06461v2",
    "title": "Exploring User Opinions of Fairness in Recommender Systems",
    "authors": [
      "Jessie Smith",
      "Nasim Sonboli",
      "Casey Fiesler",
      "Robin Burke"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness for artificial intelligence has become increasingly\nrelevant as these systems become more pervasive in society. One realm of AI,\nrecommender systems, presents unique challenges for fairness due to trade offs\nbetween optimizing accuracy for users and fairness to providers. But what is\nfair in the context of recommendation--particularly when there are multiple\nstakeholders? In an initial exploration of this problem, we ask users what\ntheir ideas of fair treatment in recommendation might be, and why. We analyze\nwhat might cause discrepancies or changes between user's opinions towards\nfairness to eventually help inform the design of fairer and more transparent\nrecommendation algorithms.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06461v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06430v1",
    "title": "Learning Unbiased Representations via Mutual Information Backpropagation",
    "authors": [
      "Ruggero Ragonesi",
      "Riccardo Volpi",
      "Jacopo Cavazza",
      "Vittorio Murino"
    ],
    "author_ids": [],
    "abstract": "We are interested in learning data-driven representations that can generalize\nwell, even when trained on inherently biased data. In particular, we face the\ncase where some attributes (bias) of the data, if learned by the model, can\nseverely compromise its generalization properties. We tackle this problem\nthrough the lens of information theory, leveraging recent findings for a\ndifferentiable estimation of mutual information. We propose a novel end-to-end\noptimization strategy, which simultaneously estimates and minimizes the mutual\ninformation between the learned representation and the data attributes. When\napplied on standard benchmarks, our model shows comparable or superior\nclassification performance with respect to state-of-the-art approaches.\nMoreover, our method is general enough to be applicable to the problem of\n``algorithmic fairness'', with competitive results.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06213v3",
    "title": "Learning and Fairness in Energy Harvesting: A Maximin Multi-Armed Bandits Approach",
    "authors": [
      "Debamita Ghosh",
      "Arun Verma",
      "Manjesh K. Hanawal"
    ],
    "author_ids": [],
    "abstract": "Recent advances in wireless radio frequency (RF) energy harvesting allows\nsensor nodes to increase their lifespan by remotely charging their batteries.\nThe amount of energy harvested by the nodes varies depending on their ambient\nenvironment, and proximity to the source. The lifespan of the sensor network\ndepends on the minimum amount of energy a node can harvest in the network. It\nis thus important to learn the least amount of energy harvested by nodes so\nthat the source can transmit on a frequency band that maximizes this amount. We\nmodel this learning problem as a novel stochastic Maximin Multi-Armed Bandits\n(Maximin MAB) problem and propose an Upper Confidence Bound (UCB) based\nalgorithm named Maximin UCB. Maximin MAB is a generalization of standard MAB\nand enjoys the same performance guarantee as that of the UCB1 algorithm.\nExperimental results validate the performance guarantees of our algorithm.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06213v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06160v1",
    "title": "The impact of incorrect social information on collective wisdom in human groups",
    "authors": [
      "Bertrand Jayles",
      "Ramón Escobedo",
      "Stéphane Cezera",
      "Adrien Blanchet",
      "Tatsuya Kameda",
      "Clément Sire",
      "Guy Theraulaz"
    ],
    "author_ids": [],
    "abstract": "A major problem that resulted from the massive use of social media networks\nis the diffusion of incorrect information. However, very few studies have\ninvestigated the impact of incorrect information on individual and collective\ndecisions. We performed experiments in which participants had to estimate a\nseries of quantities before and after receiving social information. Unbeknownst\nto them, we controlled the degree of inaccuracy of the social information\nthrough \"virtual influencers\", who provided some incorrect information. We find\nthat a large proportion of individuals only partially follow the social\ninformation, thus resisting incorrect information. Moreover, we find that\nincorrect social information can help a group perform better when it\noverestimates the true value, by partly compensating a human underestimation\nbias. Overall, our results suggest that incorrect information does not\nnecessarily impair the collective wisdom of groups, and can even be used to\ndampen the negative effects of known cognitive biases.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06160v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.06152v3",
    "title": "Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study",
    "authors": [
      "Assaf Dauber",
      "Meir Feder",
      "Tomer Koren",
      "Roi Livni"
    ],
    "author_ids": [],
    "abstract": "The notion of implicit bias, or implicit regularization, has been suggested\nas a means to explain the surprising generalization ability of modern-days\noverparameterized learning algorithms. This notion refers to the tendency of\nthe optimization algorithm towards a certain structured solution that often\ngeneralizes well. Recently, several papers have studied implicit regularization\nand were able to identify this phenomenon in various scenarios. We revisit this\nparadigm in arguably the simplest non-trivial setup, and study the implicit\nbias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex\nOptimization. As a first step, we provide a simple construction that rules out\nthe existence of a \\emph{distribution-independent} implicit regularizer that\ngoverns the generalization ability of SGD. We then demonstrate a learning\nproblem that rules out a very general class of \\emph{distribution-dependent}\nimplicit regularizers from explaining generalization, which includes strongly\nconvex regularizers as well as non-degenerate norm-based regularizations.\nCertain aspects of our constructions point out to significant difficulties in\nproviding a comprehensive explanation of an algorithm's generalization\nperformance by solely arguing about its implicit regularization properties.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06152v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.06129v2",
    "title": "LIBRE: The Multiple 3D LiDAR Dataset",
    "authors": [
      "Alexander Carballo",
      "Jacob Lambert",
      "Abraham Monrroy-Cano",
      "David Robert Wong",
      "Patiphon Narksri",
      "Yuki Kitsukawa",
      "Eijiro Takeuchi",
      "Shinpei Kato",
      "Kazuya Takeda"
    ],
    "author_ids": [],
    "abstract": "In this work, we present LIBRE: LiDAR Benchmarking and Reference, a\nfirst-of-its-kind dataset featuring 10 different LiDAR sensors, covering a\nrange of manufacturers, models, and laser configurations. Data captured\nindependently from each sensor includes three different environments and\nconfigurations: static targets, where objects were placed at known distances\nand measured from a fixed position within a controlled environment; adverse\nweather, where static obstacles were measured from a moving vehicle, captured\nin a weather chamber where LiDARs were exposed to different conditions (fog,\nrain, strong light); and finally, dynamic traffic, where dynamic objects were\ncaptured from a vehicle driven on public urban roads, multiple times at\ndifferent times of the day, and including supporting sensors such as cameras,\ninfrared imaging, and odometry devices. LIBRE will contribute to the research\ncommunity to (1) provide a means for a fair comparison of currently available\nLiDARs, and (2) facilitate the improvement of existing self-driving vehicles\nand robotics-related software, in terms of development and tuning of\nLiDAR-based perception algorithms.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.06129v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.07680v2",
    "title": "Designing Tools for Semi-Automated Detection of Machine Learning Biases: An Interview Study",
    "authors": [
      "Po-Ming Law",
      "Sana Malik",
      "Fan Du",
      "Moumita Sinha"
    ],
    "author_ids": [],
    "abstract": "Machine learning models often make predictions that bias against certain\nsubgroups of input data. When undetected, machine learning biases can\nconstitute significant financial and ethical implications. Semi-automated tools\nthat involve humans in the loop could facilitate bias detection. Yet, little is\nknown about the considerations involved in their design. In this paper, we\nreport on an interview study with 11 machine learning practitioners for\ninvestigating the needs surrounding semi-automated bias detection tools. Based\non the findings, we highlight four considerations in designing to guide system\ndesigners who aim to create future tools for bias detection.",
    "published_date": "2020-03-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.07680v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05897v1",
    "title": "Bringing in the outliers: A sparse subspace clustering approach to learn a dictionary of mouse ultrasonic vocalizations",
    "authors": [
      "Jiaxi Wang",
      "Karel Mundnich",
      "Allison T. Knoll",
      "Pat Levitt",
      "Shrikanth Narayanan"
    ],
    "author_ids": [],
    "abstract": "Mice vocalize in the ultrasonic range during social interactions. These\nvocalizations are used in neuroscience and clinical studies to tap into complex\nbehaviors and states. The analysis of these ultrasonic vocalizations (USVs) has\nbeen traditionally a manual process, which is prone to errors and human bias,\nand is not scalable to large scale analysis. We propose a new method to\nautomatically create a dictionary of USVs based on a two-step spectral\nclustering approach, where we split the set of USVs into inlier and outlier\ndata sets. This approach is motivated by the known degrading performance of\nsparse subspace clustering with outliers. We apply spectral clustering to the\ninlier data set and later find the clusters for the outliers. We propose\nquantitative and qualitative performance measures to evaluate our method in\nthis setting, where there is no ground truth. Our approach outperforms two\nbaselines based on k-means and spectral clustering in all of the proposed\nperformance measures, showing greater distances between clusters and more\nvariability between clusters.",
    "published_date": "2020-03-12T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05897v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.05888v1",
    "title": "Co-Design of Delays and Sparse Controllers for Bandwidth-Constrained Cyber-Physical Systems",
    "authors": [
      "Nandini Negi",
      "Aranya Chakrabortty"
    ],
    "author_ids": [],
    "abstract": "We address the problem of sparsity-promoting optimal control of\ncyber-physical systems with feedback delays. The delays are categorized into\ntwo classes - namely, intra-layer delay, and inter-layer delay between the\ncyber and the physical layers. Our objective is to minimize the H2-norm of the\nclosed-loop system by designing an optimal combination of these two delays\nalong with a sparse state-feedback controller, while respecting a given\nbandwidth constraint. We propose a two-loop optimization algorithm for this.\nThe inner loop, based on alternating directions method of multipliers (ADMM),\nhandles the conflicting directions of decreasing H2-norm and increasing\nsparsity of the controller. The outer loop comprises of semidefinite program\n(SDP)-based relaxations of non-convex inequalities necessary for stable\nco-design of the delays with the controller. We illustrate this algorithm using\nsimulations that highlight various aspects of how delays and sparsity impact\nthe stability and $\\mc{H}_2$ performance of a LTI system.",
    "published_date": "2020-03-12T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05888v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.05707v3",
    "title": "Fairness by Learning Orthogonal Disentangled Representations",
    "authors": [
      "Mhd Hasan Sarhan",
      "Nassir Navab",
      "Abouzar Eslami",
      "Shadi Albarqouni"
    ],
    "author_ids": [],
    "abstract": "Learning discriminative powerful representations is a crucial step for\nmachine learning systems. Introducing invariance against arbitrary nuisance or\nsensitive attributes while performing well on specific tasks is an important\nproblem in representation learning. This is mostly approached by purging the\nsensitive information from learned representations. In this paper, we propose a\nnovel disentanglement approach to invariant representation problem. We\ndisentangle the meaningful and sensitive representations by enforcing\northogonality constraints as a proxy for independence. We explicitly enforce\nthe meaningful representation to be agnostic to sensitive information by\nentropy maximization. The proposed approach is evaluated on five publicly\navailable datasets and compared with state of the art methods for learning\nfairness and invariance achieving the state of the art performance on three\ndatasets and comparable performance on the rest. Further, we perform an\nablative study to evaluate the effect of each component.",
    "published_date": "2020-03-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05707v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05684v1",
    "title": "Skeleton Based Action Recognition using a Stacked Denoising Autoencoder with Constraints of Privileged Information",
    "authors": [
      "Zhize Wu",
      "Thomas Weise",
      "Le Zou",
      "Fei Sun",
      "Ming Tan"
    ],
    "author_ids": [],
    "abstract": "Recently, with the availability of cost-effective depth cameras coupled with\nreal-time skeleton estimation, the interest in skeleton-based human action\nrecognition is renewed. Most of the existing skeletal representation approaches\nuse either the joint location or the dynamics model. Differing from the\nprevious studies, we propose a new method called Denoising Autoencoder with\nTemporal and Categorical Constraints (DAE_CTC)} to study the skeletal\nrepresentation in a view of skeleton reconstruction. Based on the concept of\nlearning under privileged information, we integrate action categories and\ntemporal coordinates into a stacked denoising autoencoder in the training\nphase, to preserve category and temporal feature, while learning the hidden\nrepresentation from a skeleton. Thus, we are able to improve the discriminative\nvalidity of the hidden representation. In order to mitigate the variation\nresulting from temporary misalignment, a new method of temporal registration,\ncalled Locally-Warped Sequence Registration (LWSR), is proposed for registering\nthe sequences of inter- and intra-class actions. We finally represent the\nsequences using a Fourier Temporal Pyramid (FTP) representation and perform\nclassification using a combination of LWSR registration, FTP representation,\nand a linear Support Vector Machine (SVM). The experimental results on three\naction data sets, namely MSR-Action3D, UTKinect-Action, and Florence3D-Action,\nshow that our proposal performs better than many existing methods and\ncomparably to the state of the art.",
    "published_date": "2020-03-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05684v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05636v1",
    "title": "Fisher Deep Domain Adaptation",
    "authors": [
      "Yinghua Zhang",
      "Yu Zhang",
      "Ying Wei",
      "Kun Bai",
      "Yangqiu Song",
      "Qiang Yang"
    ],
    "author_ids": [],
    "abstract": "Deep domain adaptation models learn a neural network in an unlabeled target\ndomain by leveraging the knowledge from a labeled source domain. This can be\nachieved by learning a domain-invariant feature space. Though the learned\nrepresentations are separable in the source domain, they usually have a large\nvariance and samples with different class labels tend to overlap in the target\ndomain, which yields suboptimal adaptation performance. To fill the gap, a\nFisher loss is proposed to learn discriminative representations which are\nwithin-class compact and between-class separable. Experimental results on two\nbenchmark datasets show that the Fisher loss is a general and effective loss\nfor deep domain adaptation. Noticeable improvements are brought when it is used\ntogether with widely adopted transfer criteria, including MMD, CORAL and domain\nadversarial loss. For example, an absolute improvement of 6.67% in terms of the\nmean accuracy is attained when the Fisher loss is used together with the domain\nadversarial loss on the Office-Home dataset.",
    "published_date": "2020-03-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05636v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11530v1",
    "title": "Meta-CoTGAN: A Meta Cooperative Training Paradigm for Improving Adversarial Text Generation",
    "authors": [
      "Haiyan Yin",
      "Dingcheng Li",
      "Xu Li",
      "Ping Li"
    ],
    "author_ids": [],
    "abstract": "Training generative models that can generate high-quality text with\nsufficient diversity is an important open problem for Natural Language\nGeneration (NLG) community. Recently, generative adversarial models have been\napplied extensively on text generation tasks, where the adversarially trained\ngenerators alleviate the exposure bias experienced by conventional maximum\nlikelihood approaches and result in promising generation quality. However, due\nto the notorious defect of mode collapse for adversarial training, the\nadversarially trained generators face a quality-diversity trade-off, i.e., the\ngenerator models tend to sacrifice generation diversity severely for increasing\ngeneration quality. In this paper, we propose a novel approach which aims to\nimprove the performance of adversarial text generation via efficiently\ndecelerating mode collapse of the adversarial training. To this end, we\nintroduce a cooperative training paradigm, where a language model is\ncooperatively trained with the generator and we utilize the language model to\nefficiently shape the data distribution of the generator against mode collapse.\nMoreover, instead of engaging the cooperative update for the generator in a\nprincipled way, we formulate a meta learning mechanism, where the cooperative\nupdate to the generator serves as a high level meta task, with an intuition of\nensuring the parameters of the generator after the adversarial update would\nstay resistant against mode collapse. In the experiment, we demonstrate our\nproposed approach can efficiently slow down the pace of mode collapse for the\nadversarial text generators. Overall, our proposed method is able to outperform\nthe baseline approaches with significant margins in terms of both generation\nquality and diversity in the testified domains.",
    "published_date": "2020-03-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11530v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11515v1",
    "title": "Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings",
    "authors": [
      "Haoran Zhang",
      "Amy X. Lu",
      "Mohamed Abdalla",
      "Matthew McDermott",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "In this work, we examine the extent to which embeddings may encode\nmarginalized populations differently, and how this may lead to a perpetuation\nof biases and worsened performance on clinical tasks. We pretrain deep\nembedding models (BERT) on medical notes from the MIMIC-III hospital dataset,\nand quantify potential disparities using two approaches. First, we identify\ndangerous latent relationships that are captured by the contextual word\nembeddings using a fill-in-the-blank method with text from real clinical notes\nand a log probability bias score quantification. Second, we evaluate\nperformance gaps across different definitions of fairness on over 50 downstream\nclinical prediction tasks that include detection of acute and chronic\nconditions. We find that classifiers trained from BERT representations exhibit\nstatistically significant differences in performance, often favoring the\nmajority group with regards to gender, language, ethnicity, and insurance\nstatus. Finally, we explore shortcomings of using adversarial debiasing to\nobfuscate subgroup information in contextual word embeddings, and recommend\nbest practices for such deep embedding models in clinical settings.",
    "published_date": "2020-03-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11515v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05330v3",
    "title": "Fairness by Explicability and Adversarial SHAP Learning",
    "authors": [
      "James M. Hickey",
      "Pietro G. Di Stefano",
      "Vlasios Vasileiou"
    ],
    "author_ids": [],
    "abstract": "The ability to understand and trust the fairness of model predictions,\nparticularly when considering the outcomes of unprivileged groups, is critical\nto the deployment and adoption of machine learning systems. SHAP values provide\na unified framework for interpreting model predictions and feature attribution\nbut do not address the problem of fairness directly. In this work, we propose a\nnew definition of fairness that emphasises the role of an external auditor and\nmodel explicability. To satisfy this definition, we develop a framework for\nmitigating model bias using regularizations constructed from the SHAP values of\nan adversarial surrogate model. We focus on the binary classification task with\na single unprivileged group and link our fairness explicability constraints to\nclassical statistical fairness metrics. We demonstrate our approaches using\ngradient and adaptive boosting on: a synthetic dataset, the UCI Adult (Census)\ndataset and a real-world credit scoring dataset. The models produced were\nfairer and performant.",
    "published_date": "2020-03-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05330v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05249v1",
    "title": "Ethical Guidelines for the Construction of Digital Nudges",
    "authors": [
      "Christian Meske",
      "Ireti Amojo"
    ],
    "author_ids": [],
    "abstract": "Under certain circumstances, humans tend to behave in irrational ways,\nleading to situations in which they make undesirable choices. The concept of\ndigital nudging addresses these limitations of bounded rationality by\nestablishing a libertarian paternalist alternative to nudge users in virtual\nenvironments towards their own preferential choices. Thereby, choice\narchitectures are designed to address biases and heuristics involved in\ncognitive thinking. As research on digital nudging has become increasingly\npopular in the Information Systems community, an increasing necessity for\nethical guidelines has emerged around this concept to safeguard its\nlegitimization in distinction to e.g. persuasion or manipulation. However,\nreflecting on ethical debates regarding digital nudging in academia, we find\nthat current conceptualizations are scare. This is where on the basis of\nexisting literature, we provide a conceptualization of ethical guidelines for\nthe design of digital nudges, and thereby aim to ensure the applicability of\nnudging mechanisms in virtual environments.",
    "published_date": "2020-03-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05249v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.05136v1",
    "title": "CASIA-SURF CeFA: A Benchmark for Multi-modal Cross-ethnicity Face Anti-spoofing",
    "authors": [
      "Ajian Li",
      "Zichang Tan",
      "Xuan Li",
      "Jun Wan",
      "Sergio Escalera",
      "Guodong Guo",
      "Stan Z. Li"
    ],
    "author_ids": [],
    "abstract": "Ethnic bias has proven to negatively affect the performance of face\nrecognition systems, and it remains an open research problem in face\nanti-spoofing. In order to study the ethnic bias for face anti-spoofing, we\nintroduce the largest up to date CASIA-SURF Cross-ethnicity Face Anti-spoofing\n(CeFA) dataset (briefly named CeFA), covering $3$ ethnicities, $3$ modalities,\n$1,607$ subjects, and 2D plus 3D attack types. Four protocols are introduced to\nmeasure the affect under varied evaluation conditions, such as cross-ethnicity,\nunknown spoofs or both of them. To the best of our knowledge, CeFA is the first\ndataset including explicit ethnic labels in current published/released datasets\nfor face anti-spoofing. Then, we propose a novel multi-modal fusion method as a\nstrong baseline to alleviate these bias, namely, the static-dynamic fusion\nmechanism applied in each modality (i.e., RGB, Depth and infrared image).\nLater, a partially shared fusion strategy is proposed to learn complementary\ninformation from multiple modalities. Extensive experiments demonstrate that\nthe proposed method achieves state-of-the-art results on the CASIA-SURF,\nOULU-NPU, SiW and the CeFA dataset.",
    "published_date": "2020-03-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05136v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05441v1",
    "title": "Can Society Function Without Ethical Agents? An Informational Perspective",
    "authors": [
      "Bruno Strulovici"
    ],
    "author_ids": [],
    "abstract": "Many facts are learned through the intermediation of individuals with special\naccess to information, such as law enforcement officers, officials with a\nsecurity clearance, or experts with specific knowledge. This paper considers\nwhether societies can learn about such facts when information is cheap to\nmanipulate, produced sequentially, and these individuals are devoid of ethical\nmotive. The answer depends on an \"information attrition\" condition pertaining\nto the amount of evidence available which distinguishes, for example, between\nreproducible scientific evidence and the evidence generated in a crime.\nApplications to institution enforcement, social cohesion, scientific progress,\nand historical revisionism are discussed.",
    "published_date": "2020-03-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05441v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.05048v1",
    "title": "Auditing ML Models for Individual Bias and Unfairness",
    "authors": [
      "Songkai Xue",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "We consider the task of auditing ML models for individual bias/unfairness. We\nformalize the task in an optimization problem and develop a suite of\ninferential tools for the optimal value. Our tools permit us to obtain\nasymptotic confidence intervals and hypothesis tests that cover the\ntarget/control the Type I error rate exactly. To demonstrate the utility of our\ntools, we use them to reveal the gender and racial biases in Northpointe's\nCOMPAS recidivism prediction instrument.",
    "published_date": "2020-03-11T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05048v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.05026v1",
    "title": "Super-reflective Data: Speculative Imaginings of a World Where Data Works for People",
    "authors": [
      "Max Van Kleek"
    ],
    "author_ids": [],
    "abstract": "It's the year 2020, and every space and place on- and off-line has been\naugmented with digital things that observe, record, transmit, and compute, for\nthe purposes of recording endless data traces of what is happening in the\nworld. Individually, these things (and the invisible services the power them)\nhave reached considerable sophistication in their ability to analyse and\ndissect such observations, turning streams of audio and video into informative\ndata fragments. Yet somehow, individuals as end-users of platforms and services\nhave not seen the full potential of such data. In this speculative paper, we\npropose two hypothetical mini scenarios different from our current digital\nworld. In the former, instead of hoarding it, data controllers turn captured\ndata over to those who need it as quickly as possible, working together to\ncombine, validate, and refine it for maximum usefulness. This simultaneously\naddresses the data fragmentation and privacy problem, by handing over long-term\ndata governance to those that value it the most In the latter, we discuss\nethical dilemmas using the long-term use of such rich data and its tendency to\ncause people to relentlessly optimise.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05026v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.05025v2",
    "title": "Fissile Locks",
    "authors": [
      "Dave Dice",
      "Alex Kogan"
    ],
    "author_ids": [],
    "abstract": "Classic test-and-test (TS) mutual exclusion locks are simple, and enjoy high\nperformance and low latency of ownership transfer under light or no contention.\nHowever, they do not scale gracefully under high contention and do not provide\nany admission order guarantees. Such concerns led to the development of\nscalable queue-based locks, such as a recent Compact NUMA-aware (CNA) lock, a\nvariant of another popular queue-based MCS lock. CNA scales well under load and\nprovides certain admission guarantees, but has more complicated lock handover\noperations than TS and incurs higher latencies at low contention. We propose\nFissile locks, which capture the most desirable properties of both TS and CNA.\nA Fissile lock consists of two underlying locks: a TS lock, which serves as a\nfast path, and a CNA lock, which serves as a slow path. The key feature of\nFissile locks is the ability of threads on the fast path to bypass threads\nenqueued on the slow path, and acquire the lock with less overhead than CNA.\nBypass is bounded (by a tunable parameter) to avoid starvation and ensure\nlong-term fairness. The result is a highly scalable NUMA-aware lock with\nprogress guarantees that performs like TS at low contention and like CNA at\nhigh contention.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.OS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.05025v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.04821v4",
    "title": "Benchmarking TinyML Systems: Challenges and Direction",
    "authors": [
      "Colby R. Banbury",
      "Vijay Janapa Reddi",
      "Max Lam",
      "William Fu",
      "Amin Fazel",
      "Jeremy Holleman",
      "Xinyuan Huang",
      "Robert Hurtado",
      "David Kanter",
      "Anton Lokhmotov",
      "David Patterson",
      "Danilo Pau",
      "Jae-sun Seo",
      "Jeff Sieracki",
      "Urmish Thakker",
      "Marian Verhelst",
      "Poonam Yadav"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in ultra-low-power machine learning (TinyML) hardware\npromises to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted benchmark for\nthese systems. Benchmarking allows us to measure and thereby systematically\ncompare, evaluate, and improve the performance of systems and is therefore\nfundamental to a field reaching maturity. In this position paper, we present\nthe current landscape of TinyML and discuss the challenges and direction\ntowards developing a fair and useful hardware benchmark for TinyML workloads.\nFurthermore, we present our four benchmarks and discuss our selection\nmethodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf\nworking group that is comprised of over 30 organizations.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.PF",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04821v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04794v1",
    "title": "Addressing multiple metrics of group fairness in data-driven decision making",
    "authors": [
      "Marius Miron",
      "Songül Tolan",
      "Emilia Gómez",
      "Carlos Castillo"
    ],
    "author_ids": [],
    "abstract": "The Fairness, Accountability, and Transparency in Machine Learning (FAT-ML)\nliterature proposes a varied set of group fairness metrics to measure\ndiscrimination against socio-demographic groups that are characterized by a\nprotected feature, such as gender or race.Such a system can be deemed as either\nfair or unfair depending on the choice of the metric. Several metrics have been\nproposed, some of them incompatible with each other.We do so empirically, by\nobserving that several of these metrics cluster together in two or three main\nclusters for the same groups and machine learning methods. In addition, we\npropose a robust way to visualize multidimensional fairness in two dimensions\nthrough a Principal Component Analysis (PCA) of the group fairness metrics.\nExperimental results on multiple datasets show that the PCA decomposition\nexplains the variance between the metrics with one to three components.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04794v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04671v1",
    "title": "Realizing Pixel-Level Semantic Learning in Complex Driving Scenes based on Only One Annotated Pixel per Class",
    "authors": [
      "Xi Li",
      "Huimin Ma",
      "Sheng Yi",
      "Yanxian Chen"
    ],
    "author_ids": [],
    "abstract": "Semantic segmentation tasks based on weakly supervised condition have been\nput forward to achieve a lightweight labeling process. For simple images that\nonly include a few categories, researches based on image-level annotations have\nachieved acceptable performance. However, when facing complex scenes, since\nimage contains a large amount of classes, it becomes difficult to learn visual\nappearance based on image tags. In this case, image-level annotations are not\neffective in providing information. Therefore, we set up a new task in which\nonly one annotated pixel is provided for each category. Based on the more\nlightweight and informative condition, a three step process is built for pseudo\nlabels generation, which progressively implement optimal feature representation\nfor each category, image inference and context-location based refinement. In\nparticular, since high-level semantics and low-level imaging feature have\ndifferent discriminative ability for each class under driving scenes, we divide\neach category into \"object\" or \"scene\" and then provide different operations\nfor the two types separately. Further, an alternate iterative structure is\nestablished to gradually improve segmentation performance, which combines\nCNN-based inter-image common semantic learning and imaging prior based\nintra-image modification process. Experiments on Cityscapes dataset demonstrate\nthat the proposed method provides a feasible way to solve weakly supervised\nsemantic segmentation task under complex driving scenes.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04671v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04614v3",
    "title": "Label-Driven Reconstruction for Domain Adaptation in Semantic Segmentation",
    "authors": [
      "Jinyu Yang",
      "Weizhi An",
      "Sheng Wang",
      "Xinliang Zhu",
      "Chaochao Yan",
      "Junzhou Huang"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptation enables to alleviate the need for pixel-wise\nannotation in the semantic segmentation. One of the most common strategies is\nto translate images from the source domain to the target domain and then align\ntheir marginal distributions in the feature space using adversarial learning.\nHowever, source-to-target translation enlarges the bias in translated images\nand introduces extra computations, owing to the dominant data size of the\nsource domain. Furthermore, consistency of the joint distribution in source and\ntarget domains cannot be guaranteed through global feature alignment. Here, we\npresent an innovative framework, designed to mitigate the image translation\nbias and align cross-domain features with the same category. This is achieved\nby 1) performing the target-to-source translation and 2) reconstructing both\nsource and target images from their predicted labels. Extensive experiments on\nadapting from synthetic to real urban scene understanding demonstrate that our\nframework competes favorably against existing state-of-the-art methods.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04614v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04560v1",
    "title": "Frequency Bias in Neural Networks for Input of Non-Uniform Density",
    "authors": [
      "Ronen Basri",
      "Meirav Galun",
      "Amnon Geifman",
      "David Jacobs",
      "Yoni Kasten",
      "Shira Kritchman"
    ],
    "author_ids": [],
    "abstract": "Recent works have partly attributed the generalization ability of\nover-parameterized neural networks to frequency bias -- networks trained with\ngradient descent on data drawn from a uniform distribution find a low frequency\nfit before high frequency ones. As realistic training sets are not drawn from a\nuniform distribution, we here use the Neural Tangent Kernel (NTK) model to\nexplore the effect of variable density on training dynamics. Our results, which\ncombine analytic and empirical observations, show that when learning a pure\nharmonic function of frequency $\\kappa$, convergence at a point $\\x \\in\n\\Sphere^{d-1}$ occurs in time $O(\\kappa^d/p(\\x))$ where $p(\\x)$ denotes the\nlocal density at $\\x$. Specifically, for data in $\\Sphere^1$ we analytically\nderive the eigenfunctions of the kernel associated with the NTK for two-layer\nnetworks. We further prove convergence results for deep, fully connected\nnetworks with respect to the spectral decomposition of the NTK. Our empirical\nstudy highlights similarities and differences between deep and shallow networks\nin this model.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04560v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04549v3",
    "title": "Slice Tuner: A Selective Data Acquisition Framework for Accurate and Fair Machine Learning Models",
    "authors": [
      "Ki Hyun Tae",
      "Steven Euijong Whang"
    ],
    "author_ids": [],
    "abstract": "As machine learning becomes democratized in the era of Software 2.0, a\nserious bottleneck is acquiring enough data to ensure accurate and fair models.\nRecent techniques including crowdsourcing provide cost-effective ways to gather\nsuch data. However, simply acquiring data as much as possible is not\nnecessarily an effective strategy for optimizing accuracy and fairness. For\nexample, if an online app store has enough training data for certain slices of\ndata (say American customers), but not for others, obtaining more American\ncustomer data will only bias the model training. Instead, we contend that one\nneeds to selectively acquire data and propose Slice Tuner, which acquires\npossibly-different amounts of data per slice such that the model accuracy and\nfairness on all slices are optimized. This problem is different than labeling\nexisting data (as in active learning or weak supervision) because the goal is\nobtaining the right amounts of new data. At its core, Slice Tuner maintains\nlearning curves of slices that estimate the model accuracies given more data\nand uses convex optimization to find the best data acquisition strategy. The\nkey challenges of estimating learning curves are that they may be inaccurate if\nthere is not enough data, and there may be dependencies among slices where\nacquiring data for one slice influences the learning curves of others. We solve\nthese issues by iteratively and efficiently updating the learning curves as\nmore data is acquired. We evaluate Slice Tuner on real datasets using\ncrowdsourcing for data acquisition and show that Slice Tuner significantly\noutperforms baselines in terms of model accuracy and fairness, even when the\nlearning curves cannot be reliably estimated.",
    "published_date": "2020-03-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04549v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.11520v1",
    "title": "Joint Multiclass Debiasing of Word Embeddings",
    "authors": [
      "Radomir Popović",
      "Florian Lemmerich",
      "Markus Strohmaier"
    ],
    "author_ids": [],
    "abstract": "Bias in Word Embeddings has been a subject of recent interest, along with\nefforts for its reduction. Current approaches show promising progress towards\ndebiasing single bias dimensions such as gender or race. In this paper, we\npresent a joint multiclass debiasing approach that is capable of debiasing\nmultiple bias dimensions simultaneously. In that direction, we present two\napproaches, HardWEAT and SoftWEAT, that aim to reduce biases by minimizing the\nscores of the Word Embeddings Association Test (WEAT). We demonstrate the\nviability of our methods by debiasing Word Embeddings on three classes of\nbiases (religion, gender and race) in three different publicly available word\nembeddings and show that our concepts can both reduce or even completely\neliminate bias, while maintaining meaningful relationships between vectors in\nword embeddings. Our work strengthens the foundation for more unbiased neural\nrepresentations of textual data.",
    "published_date": "2020-03-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.11520v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04427v1",
    "title": "Transfer Reinforcement Learning under Unobserved Contextual Information",
    "authors": [
      "Yan Zhang",
      "Michael M. Zavlanos"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study a transfer reinforcement learning problem where the\nstate transitions and rewards are affected by the environmental context.\nSpecifically, we consider a demonstrator agent that has access to a\ncontext-aware policy and can generate transition and reward data based on that\npolicy. These data constitute the experience of the demonstrator. Then, the\ngoal is to transfer this experience, excluding the underlying contextual\ninformation, to a learner agent that does not have access to the environmental\ncontext, so that they can learn a control policy using fewer samples. It is\nwell known that, disregarding the causal effect of the contextual information,\ncan introduce bias in the transition and reward models estimated by the\nlearner, resulting in a learned suboptimal policy. To address this challenge,\nin this paper, we develop a method to obtain causal bounds on the transition\nand reward functions using the demonstrator's data, which we then use to obtain\ncausal bounds on the value functions. Using these value function bounds, we\npropose new Q learning and UCB-Q learning algorithms that converge to the true\nvalue function without bias. We provide numerical experiments for robot motion\nplanning problems that validate the proposed value function bounds and\ndemonstrate that the proposed algorithms can effectively make use of the data\nfrom the demonstrator to accelerate the learning process of the learner.",
    "published_date": "2020-03-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04427v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.08747v1",
    "title": "IROF: a low resource evaluation metric for explanation methods",
    "authors": [
      "Laura Rieger",
      "Lars Kai Hansen"
    ],
    "author_ids": [],
    "abstract": "The adoption of machine learning in health care hinges on the transparency of\nthe used algorithms, necessitating the need for explanation methods. However,\ndespite a growing literature on explaining neural networks, no consensus has\nbeen reached on how to evaluate those explanation methods. We propose IROF, a\nnew approach to evaluating explanation methods that circumvents the need for\nmanual evaluation. Compared to other recent work, our approach requires several\norders of magnitude less computational resources and no human input, making it\naccessible to lower resource groups and robust to human bias.",
    "published_date": "2020-03-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08747v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04052v3",
    "title": "On the Texture Bias for Few-Shot CNN Segmentation",
    "authors": [
      "Reza Azad",
      "Abdur R Fayjie",
      "Claude Kauffman",
      "Ismail Ben Ayed",
      "Marco Pedersoli",
      "Jose Dolz"
    ],
    "author_ids": [],
    "abstract": "Despite the initial belief that Convolutional Neural Networks (CNNs) are\ndriven by shapes to perform visual recognition tasks, recent evidence suggests\nthat texture bias in CNNs provides higher performing models when learning on\nlarge labeled training datasets. This contrasts with the perceptual bias in the\nhuman visual cortex, which has a stronger preference towards shape components.\nPerceptual differences may explain why CNNs achieve human-level performance\nwhen large labeled datasets are available, but their performance significantly\ndegrades in lowlabeled data scenarios, such as few-shot semantic segmentation.\nTo remove the texture bias in the context of few-shot learning, we propose a\nnovel architecture that integrates a set of Difference of Gaussians (DoG) to\nattenuate high-frequency local components in the feature space. This produces a\nset of modified feature maps, whose high-frequency components are diminished at\ndifferent standard deviation values of the Gaussian distribution in the spatial\ndomain. As this results in multiple feature maps for a single image, we employ\na bi-directional convolutional long-short-term-memory to efficiently merge the\nmulti scale-space representations. We perform extensive experiments on three\nwell-known few-shot segmentation benchmarks -- Pascal i5, COCO-20i and FSS-1000\n-- and demonstrate that our method outperforms state-of-the-art approaches in\ntwo datasets under the same conditions. The code is available at:\nhttps://github.com/rezazad68/fewshot-segmentation",
    "published_date": "2020-03-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04052v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.03923v2",
    "title": "Deconfounded Image Captioning: A Causal Retrospect",
    "authors": [
      "Xu Yang",
      "Hanwang Zhang",
      "Jianfei Cai"
    ],
    "author_ids": [],
    "abstract": "Dataset bias in vision-language tasks is becoming one of the main problems\nwhich hinders the progress of our community. Existing solutions lack a\nprincipled analysis about why modern image captioners easily collapse into\ndataset bias. In this paper, we present a novel perspective: Deconfounded Image\nCaptioning (DIC), to find out the answer of this question, then retrospect\nmodern neural image captioners, and finally propose a DIC framework: DICv1.0 to\nalleviate the negative effects brought by dataset bias. DIC is based on causal\ninference, whose two principles: the backdoor and front-door adjustments, help\nus review previous studies and design new effective models. In particular, we\nshowcase that DICv1.0 can strengthen two prevailing captioning models and can\nachieve a single-model 131.1 CIDEr-D and 128.4 c40 CIDEr-D on Karpathy split\nand online split of the challenging MS COCO dataset, respectively.\nInterestingly, DICv1.0 is a natural derivation from our causal retrospect,\nwhich opens promising directions for image captioning.",
    "published_date": "2020-03-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.03923v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.08765v1",
    "title": "Salient Facial Features from Humans and Deep Neural Networks",
    "authors": [
      "Shanmeng Sun",
      "Wei Zhen Teoh",
      "Michael Guerzhoy"
    ],
    "author_ids": [],
    "abstract": "In this work, we explore the features that are used by humans and by\nconvolutional neural networks (ConvNets) to classify faces. We use Guided\nBackpropagation (GB) to visualize the facial features that influence the output\nof a ConvNet the most when identifying specific individuals; we explore how to\nbest use GB for that purpose. We use a human intelligence task to find out\nwhich facial features humans find to be the most important for identifying\nspecific individuals. We explore the differences between the saliency\ninformation gathered from humans and from ConvNets.\n  Humans develop biases in employing available information on facial features\nto discriminate across faces. Studies show these biases are influenced both by\nneurological development and by each individual's social experience. In recent\nyears the computer vision community has achieved human-level performance in\nmany face processing tasks with deep neural network-based models. These face\nprocessing systems are also subject to systematic biases due to model\narchitectural choices and training data distribution.",
    "published_date": "2020-03-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.08765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.03699v2",
    "title": "Removing Disparate Impact of Differentially Private Stochastic Gradient Descent on Model Accuracy",
    "authors": [
      "Depeng Xu",
      "Wei Du",
      "Xintao Wu"
    ],
    "author_ids": [],
    "abstract": "When we enforce differential privacy in machine learning, the utility-privacy\ntrade-off is different w.r.t. each group. Gradient clipping and random noise\naddition disproportionately affect underrepresented and complex classes and\nsubgroups, which results in inequality in utility loss. In this work, we\nanalyze the inequality in utility loss by differential privacy and propose a\nmodified differentially private stochastic gradient descent (DPSGD), called\nDPSGD-F, to remove the potential disparate impact of differential privacy on\nthe protected group. DPSGD-F adjusts the contribution of samples in a group\ndepending on the group clipping bias such that differential privacy has no\ndisparate impact on group utility. Our experimental evaluation shows how group\nsample size and group clipping bias affect the impact of differential privacy\nin DPSGD, and how adaptive clipping for each group helps to mitigate the\ndisparate impact caused by differential privacy in DPSGD-F.",
    "published_date": "2020-03-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.03699v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.03481v1",
    "title": "A Comparison of Amputee and Able-Bodied Inter-Subject Variability in Myoelectric Control",
    "authors": [
      "Evan Campbell",
      "Jason Chang",
      "Angkoon Phinyomark",
      "Erik Scheme"
    ],
    "author_ids": [],
    "abstract": "Despite decades of research and development of pattern recognition\napproaches, the clinical usability of myoelectriccontrolled prostheses is still\nlimited. One of the main issues is the high inter-subject variability that\nnecessitates long and frequent user-specific training. Cross-user models\npresent an opportunity to improve clinical viability of myoelectric control\nsystems by leveraging existing data to shorten training. However, due to the\ndifficulty of obtaining large sets of data from amputee populations, data from\nintact-limbed subjects are often supplemented when building cross-user models;\nwhich may not translate well to clinical usability. In this preliminary study,\nthe differences between intact-limbed and amputee cross-user electromyography\n(EMG) patterns were examined. Previously collected EMG data from 20 intact\nlimbed and 10 amputee subjects for different wrist, finger, and grasping\ngestures were analysed. Results using unsupervised clustering showed that\namputees were consistently grouped into a different cluster than intact-limbed\nsubjects and that additional clustering into more subgroups found larger\ndifferences between amputees than able-bodied subjects. Furthermore, a simple\nlinear classifier was able to discriminate between able-bodied and amputee\nsubjects using EMG from multiple gestures with 90% accuracy. These results\nsuggest that using able-bodied subject data alone may be insufficient to\ncapture the necessary inter-subject variance when designing cross-user\nmyoelectric control systems for prosthesis control.",
    "published_date": "2020-03-07T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.03481v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.03151v2",
    "title": "Demographic Bias in Presentation Attack Detection of Iris Recognition Systems",
    "authors": [
      "Meiling Fang",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "author_ids": [],
    "abstract": "With the widespread use of biometric systems, the demographic bias problem\nraises more attention. Although many studies addressed bias issues in biometric\nverification, there are no works that analyze the bias in presentation attack\ndetection (PAD) decisions. Hence, we investigate and analyze the demographic\nbias in iris PAD algorithms in this paper. To enable a clear discussion, we\nadapt the notions of differential performance and differential outcome to the\nPAD problem. We study the bias in iris PAD using three baselines (hand-crafted,\ntransfer-learning, and training from scratch) using the NDCLD-2013 database.\nThe experimental results point out that female users will be significantly less\nprotected by the PAD, in comparison to males.",
    "published_date": "2020-03-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.03151v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.03014v2",
    "title": "A Framework for the Computational Linguistic Analysis of Dehumanization",
    "authors": [
      "Julia Mendelsohn",
      "Yulia Tsvetkov",
      "Dan Jurafsky"
    ],
    "author_ids": [],
    "abstract": "Dehumanization is a pernicious psychological process that often leads to\nextreme intergroup bias, hate speech, and violence aimed at targeted social\ngroups. Despite these serious consequences and the wealth of available data,\ndehumanization has not yet been computationally studied on a large scale.\nDrawing upon social psychology research, we create a computational linguistic\nframework for analyzing dehumanizing language by identifying linguistic\ncorrelates of salient components of dehumanization. We then apply this\nframework to analyze discussions of LGBTQ people in the New York Times from\n1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ\npeople over time. However, we find that the label homosexual has emerged to be\nmuch more strongly associated with dehumanizing attitudes than other labels,\nsuch as gay. Our proposed techniques highlight processes of linguistic\nvariation and change in discourses surrounding marginalized groups.\nFurthermore, the ability to analyze dehumanizing language at a large scale has\nimplications for automatically detecting and understanding media bias as well\nas abusive language online.",
    "published_date": "2020-03-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "J.4; I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.03014v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.02756v2",
    "title": "HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference",
    "authors": [
      "Tianyu Liu",
      "Xin Zheng",
      "Baobao Chang",
      "Zhifang Sui"
    ],
    "author_ids": [],
    "abstract": "Many recent studies have shown that for models trained on datasets for\nnatural language inference (NLI), it is possible to make correct predictions by\nmerely looking at the hypothesis while completely ignoring the premise. In this\nwork, we manage to derive adversarial examples in terms of the hypothesis-only\nbias and explore eligible ways to mitigate such bias. Specifically, we extract\nvarious phrases from the hypotheses (artificial patterns) in the training sets,\nand show that they have been strong indicators to the specific labels. We then\nfigure out `hard' and `easy' instances from the original test sets whose labels\nare opposite to or consistent with those indications. We also set up baselines\nincluding both pretrained models (BERT, RoBERTa, XLNet) and competitive\nnon-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and\nbaselines, we also investigate two debiasing approaches which exploit the\nartificial pattern modeling to mitigate such hypothesis-only bias:\ndown-sampling and adversarial training. We believe those methods can be treated\nas competitive baselines in NLI debiasing tasks.",
    "published_date": "2020-03-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.02756v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.02488v2",
    "title": "Demographic Bias in Biometrics: A Survey on an Emerging Challenge",
    "authors": [
      "P. Drozdowski",
      "C. Rathgeb",
      "A. Dantcheva",
      "N. Damer",
      "C. Busch"
    ],
    "author_ids": [],
    "abstract": "Systems incorporating biometric technologies have become ubiquitous in\npersonal, commercial, and governmental identity management applications. Both\ncooperative (e.g. access control) and non-cooperative (e.g. surveillance and\nforensics) systems have benefited from biometrics. Such systems rely on the\nuniqueness of certain biological or behavioural characteristics of human\nbeings, which enable for individuals to be reliably recognised using automated\nalgorithms.\n  Recently, however, there has been a wave of public and academic concerns\nregarding the existence of systemic bias in automated decision systems\n(including biometrics). Most prominently, face recognition algorithms have\noften been labelled as \"racist\" or \"biased\" by the media, non-governmental\norganisations, and researchers alike.\n  The main contributions of this article are: (1) an overview of the topic of\nalgorithmic bias in the context of biometrics, (2) a comprehensive survey of\nthe existing literature on biometric bias estimation and mitigation, (3) a\ndiscussion of the pertinent technical and social matters, and (4) an outline of\nthe remaining challenges and future work items, both from technological and\nsocial points of view.",
    "published_date": "2020-03-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.02488v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.02366v2",
    "title": "Towards Fair Cross-Domain Adaptation via Generative Learning",
    "authors": [
      "Tongxin Wang",
      "Zhengming Ding",
      "Wei Shao",
      "Haixu Tang",
      "Kun Huang"
    ],
    "author_ids": [],
    "abstract": "Domain Adaptation (DA) targets at adapting a model trained over the\nwell-labeled source domain to the unlabeled target domain lying in different\ndistributions. Existing DA normally assumes the well-labeled source domain is\nclass-wise balanced, which means the size per source class is relatively\nsimilar. However, in real-world applications, labeled samples for some\ncategories in the source domain could be extremely few due to the difficulty of\ndata collection and annotation, which leads to decreasing performance over\ntarget domain on those few-shot categories. To perform fair cross-domain\nadaptation and boost the performance on these minority categories, we develop a\nnovel Generative Few-shot Cross-domain Adaptation (GFCA) algorithm for fair\ncross-domain classification. Specifically, generative feature augmentation is\nexplored to synthesize effective training data for few-shot source classes,\nwhile effective cross-domain alignment aims to adapt knowledge from source to\nfacilitate the target learning. Experimental results on two large cross-domain\nvisual datasets demonstrate the effectiveness of our proposed method on\nimproving both few-shot and overall classification accuracy comparing with the\nstate-of-the-art DA approaches.",
    "published_date": "2020-03-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.02366v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.01747v2",
    "title": "Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to Unobserved Confounding",
    "authors": [
      "Victor Veitch",
      "Anisha Zaveri"
    ],
    "author_ids": [],
    "abstract": "It is a truth universally acknowledged that an observed association without\nknown mechanism must be in want of a causal estimate. However, causal\nestimation from observational data often relies on the (untestable) assumption\nof `no unobserved confounding'. Violations of this assumption can induce bias\nin effect estimates. In principle, such bias could invalidate or reverse the\nconclusions of a study. However, in some cases, we might hope that the\ninfluence of unobserved confounders is weak relative to a `large' estimated\neffect, so the qualitative conclusions are robust to bias from unobserved\nconfounding. The purpose of this paper is to develop \\emph{Austen plots}, a\nsensitivity analysis tool to aid such judgments by making it easier to reason\nabout potential bias induced by unobserved confounding. We formalize\nconfounding strength in terms of how strongly the confounder influences\ntreatment assignment and outcome. For a target level of bias, an Austen plot\nshows the minimum values of treatment and outcome influence required to induce\nthat level of bias. Domain experts can then make subjective judgments about\nwhether such strong confounders are plausible. To aid this judgment, the Austen\nplot additionally displays the estimated influence strength of (groups of) the\nobserved covariates. Austen plots generalize the classic sensitivity analysis\napproach of Imbens [Imb03]. Critically, Austen plots allow any approach for\nmodeling the observed data and producing the initial estimate. We illustrate\nthe tool by assessing biases for several real causal inference problems, using\na variety of machine learning approaches for the initial data analysis. Code is\navailable at https://github.com/anishazaveri/austen_plots",
    "published_date": "2020-03-03T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.01747v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.01665v3",
    "title": "Discriminative Multi-level Reconstruction under Compact Latent Space for One-Class Novelty Detection",
    "authors": [
      "Jaewoo Park",
      "Yoon Gyo Jung",
      "Andrew Beng Jin Teoh"
    ],
    "author_ids": [],
    "abstract": "In one-class novelty detection, a model learns solely on the in-class data to\nsingle out out-class instances. Autoencoder (AE) variants aim to compactly\nmodel the in-class data to reconstruct it exclusively, thus differentiating the\nin-class from out-class by the reconstruction error. However, compact modeling\nin an improper way might collapse the latent representations of the in-class\ndata and thus their reconstruction, which would lead to performance\ndeterioration. Moreover, to properly measure the reconstruction error of\nhigh-dimensional data, a metric is required that captures high-level semantics\nof the data. To this end, we propose Discriminative Compact AE (DCAE) that\nlearns both compact and collapse-free latent representations of the in-class\ndata, thereby reconstructing them both finely and exclusively. In DCAE, (a) we\nforce a compact latent space to bijectively represent the in-class data by\nreconstructing them through internal discriminative layers of generative\nadversarial nets. (b) Based on the deep encoder's vulnerability to open set\nrisk, out-class instances are encoded into the same compact latent space and\nreconstructed poorly without sacrificing the quality of in-class data\nreconstruction. (c) In inference, the reconstruction error is measured by a\nnovel metric that computes the dissimilarity between a query and its\nreconstruction based on the class semantics captured by the internal\ndiscriminator. Extensive experiments on public image datasets validate the\neffectiveness of our proposed model on both novelty and adversarial example\ndetection, delivering state-of-the-art performance.",
    "published_date": "2020-03-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.01665v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2004.00480v1",
    "title": "Novel Meta-Heuristic Model for Discrimination between Iron Deficiency Anemia and B-Thalassemia with CBC Indices Based on Dynamic Harmony Search",
    "authors": [
      "Sultan Noman Qasem",
      "Amir Mosavi"
    ],
    "author_ids": [],
    "abstract": "In recent decades, attention has been directed at anemia classification for\nvarious medical purposes, such as thalassemia screening and predicting iron\ndeficiency anemia (IDA). In this study, a new method has been successfully\ntested for discrimination between IDA and \\b{eta}-thalassemia trait\n(\\b{eta}-TT). The method is based on a Dynamic Harmony Search (DHS). Complete\nblood count (CBC), a fast and inexpensive laboratory test, is used as the input\nof the system. Other models, such as a genetic programming method called\nstructured representation on genetic algorithm in non-linear function fitting\n(STROGANOFF), an artificial neural network (ANN), an adaptive neuro-fuzzy\ninference system (ANFIS), a support vector machine (SVM), k-nearest neighbor\n(KNN), and certain traditional methods, are compared with the proposed method.",
    "published_date": "2020-03-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML",
      "68T05"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.00480v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.01525v1",
    "title": "Evidence-based explanation to promote fairness in AI systems",
    "authors": [
      "Juliana Jansen Ferreira",
      "Mateus de Souza Monteiro"
    ],
    "author_ids": [],
    "abstract": "As Artificial Intelligence (AI) technology gets more intertwined with every\nsystem, people are using AI to make decisions on their everyday activities. In\nsimple contexts, such as Netflix recommendations, or in more complex context\nlike in judicial scenarios, AI is part of people's decisions. People make\ndecisions and usually, they need to explain their decision to others or in some\nmatter. It is particularly critical in contexts where human expertise is\ncentral to decision-making. In order to explain their decisions with AI\nsupport, people need to understand how AI is part of that decision. When\nconsidering the aspect of fairness, the role that AI has on a decision-making\nprocess becomes even more sensitive since it affects the fairness and the\nresponsibility of those people making the ultimate decision. We have been\nexploring an evidence-based explanation design approach to 'tell the story of a\ndecision'. In this position paper, we discuss our approach for AI systems using\nfairness sensitive cases in the literature.",
    "published_date": "2020-03-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.01525v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.01380v2",
    "title": "The Magic Word: A Coding Tutorial-Game to Engage Female Teenagers in App Design",
    "authors": [
      "Bernadette Spieler",
      "Naomi Pfaff",
      "Stefania Makrygiannaki",
      "Wolfgang Slany"
    ],
    "author_ids": [],
    "abstract": "Educational games are commonly used to motivate students and provide enhanced\nlearning opportunities. Apps and mobile games play an increasingly important\nrole in education and smartphones are part of the daily lives of most female\nteenagers: Half of mobile gamers are women and 64% of women prefer smartphones\nto other platforms. However, gender differences in playing behaviour and\npreferences raises concerns about potential gender inequalities when games are\ndeveloped for education. In order to develop a tutorial game that suits the\nfemale target group and provides challenging tasks to solve, girls were\ninvolved at a very early stage of the development cycle and the idea was\ndeveloped on the basis of surveys and focus group discussions. A first\nprototype of the game has been tested in a mixed-gender group to get feedback\nabout the learning content, the worked examples, and the whole structure of the\ngame. Finally, a tutorial game with six worked examples has been released in\nour Luna&Cat app, a programming tool that has been designed for our female\ntarget group in particular.",
    "published_date": "2020-03-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.01380v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.01054v2",
    "title": "Double Trouble in Double Descent : Bias and Variance(s) in the Lazy Regime",
    "authors": [
      "Stéphane d'Ascoli",
      "Maria Refinetti",
      "Giulio Biroli",
      "Florent Krzakala"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks can achieve remarkable generalization performances while\ninterpolating the training data perfectly. Rather than the U-curve emblematic\nof the bias-variance trade-off, their test error often follows a \"double\ndescent\" - a mark of the beneficial role of overparametrization. In this work,\nwe develop a quantitative theory for this phenomenon in the so-called lazy\nlearning regime of neural networks, by considering the problem of learning a\nhigh-dimensional function with random features regression. We obtain a precise\nasymptotic expression for the bias-variance decomposition of the test error,\nand show that the bias displays a phase transition at the interpolation\nthreshold, beyond which it remains constant. We disentangle the variances\nstemming from the sampling of the dataset, from the additive noise corrupting\nthe labels, and from the initialization of the weights. Following up on Geiger\net al. 2019, we first show that the latter two contributions are the crux of\nthe double descent: they lead to the overfitting peak at the interpolation\nthreshold and to the decay of the test error upon overparametrization. We then\nquantify how they are suppressed by ensemble averaging the outputs of K\nindependently initialized estimators. When K is sent to infinity, the test\nerror remains constant beyond the interpolation threshold. We further compare\nthe effects of overparametrizing, ensembling and regularizing. Finally, we\npresent numerical experiments on classic deep learning setups to show that our\nresults hold qualitatively in realistic lazy learning scenarios.",
    "published_date": "2020-03-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.01054v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.00935v1",
    "title": "Toward equipping Artificial Moral Agents with multiple ethical theories",
    "authors": [
      "George Rautenbach",
      "C. Maria Keet"
    ],
    "author_ids": [],
    "abstract": "Artificial Moral Agents (AMA's) is a field in computer science with the\npurpose of creating autonomous machines that can make moral decisions akin to\nhow humans do. Researchers have proposed theoretical means of creating such\nmachines, while philosophers have made arguments as to how these machines ought\nto behave, or whether they should even exist. Of the currently theorised AMA's,\nall research and design has been done with either none or at most one specified\nnormative ethical theory as basis. This is problematic because it narrows down\nthe AMA's functional ability and versatility which in turn causes moral\noutcomes that a limited number of people agree with (thereby undermining an\nAMA's ability to be moral in a human sense). As solution we design a\nthree-layer model for general normative ethical theories that can be used to\nserialise the ethical views of people and businesses for an AMA to use during\nreasoning. Four specific ethical norms (Kantianism, divine command theory,\nutilitarianism, and egoism) were modelled and evaluated as proof of concept for\nnormative modelling. Furthermore, all models were serialised to XML/XSD as\nproof of support for computerisation.",
    "published_date": "2020-03-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "H.1.0; I.2.0; I.6.5; K.m"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00935v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.00801v1",
    "title": "BitcoinF: Achieving Fairness for Bitcoin in Transaction-Fee-Only Model",
    "authors": [
      "Shoeb Siddiqui",
      "Ganesh Vanahalli",
      "Sujit Gujar"
    ],
    "author_ids": [],
    "abstract": "A blockchain, such as Bitcoin, is an append-only, secure, transparent,\ndistributed ledger. A fair blockchain is expected to have healthy metrics; high\nhonest mining power, low processing latency, i.e., low wait times for\ntransactions and stable price of consumption, i.e., the minimum transaction fee\nrequired to have a transaction processed. As Bitcoin matures, the influx of\ntransactions increases and the block rewards become insignificant. We show that\nunder these conditions, it becomes hard to maintain the health of the\nblockchain. In Bitcoin, under these mature operating conditions (MOC), the\nminers would find it challenging to cover their mining costs as there would be\nno more revenue from merely mining a block. It may cause miners not to continue\nmining, threatening the blockchain's security. Further, as we show in this\npaper using simulations, the cost of acting in favor of the health of the\nblockchain, under MOC, is very high in Bitcoin, causing all miners to process\ntransactions greedily. It leads to stranded transactions, i.e., transactions\noffering low transaction fees, experiencing unreasonably high processing\nlatency. To make matters worse, a compounding effect of these stranded\ntransactions is the rising price of consumption. Such phenomena not only induce\nunfairness as experienced by the miners and the users but also deteriorate the\nhealth of the blockchain.\n  We propose BitcoinF transaction processing protocol, a simple, yet highly\neffective modification to the existing Bitcoin protocol to fix these issues of\nunfairness. BitcoinF resolves these issues of unfairness while preserving the\nability of the users to express urgency and have their transactions\nprioritized.",
    "published_date": "2020-03-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00801v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.00707v2",
    "title": "Unbiased Mean Teacher for Cross-domain Object Detection",
    "authors": [
      "Jinhong Deng",
      "Wen Li",
      "Yuhua Chen",
      "Lixin Duan"
    ],
    "author_ids": [],
    "abstract": "Cross-domain object detection is challenging, because object detection model\nis often vulnerable to data variance, especially to the considerable domain\nshift between two distinctive domains. In this paper, we propose a new Unbiased\nMean Teacher (UMT) model for cross-domain object detection. We reveal that\nthere often exists a considerable model bias for the simple mean teacher (MT)\nmodel in cross-domain scenarios, and eliminate the model bias with several\nsimple yet highly effective strategies. In particular, for the teacher model,\nwe propose a cross-domain distillation method for MT to maximally exploit the\nexpertise of the teacher model. Moreover, for the student model, we alleviate\nits bias by augmenting training samples with pixel-level adaptation. Finally,\nfor the teaching process, we employ an out-of-distribution estimation strategy\nto select samples that most fit the current model to further enhance the\ncross-domain distillation process. By tackling the model bias issue with these\nstrategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on\nbenchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes,\nrespectively, which outperforms the existing state-of-the-art results in\nnotable margins. Our implementation is available at\nhttps://github.com/kinredon/umt.",
    "published_date": "2020-03-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00707v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.00683v1",
    "title": "Detection and Mitigation of Bias in Ted Talk Ratings",
    "authors": [
      "Rupam Acharyya",
      "Shouman Das",
      "Ankani Chattoraj",
      "Oishani Sengupta",
      "Md Iftekar Tanveer"
    ],
    "author_ids": [],
    "abstract": "Unbiased data collection is essential to guaranteeing fairness in artificial\nintelligence models. Implicit bias, a form of behavioral conditioning that\nleads us to attribute predetermined characteristics to members of certain\ngroups and informs the data collection process. This paper quantifies implicit\nbias in viewer ratings of TEDTalks, a diverse social platform assessing social\nand professional performance, in order to present the correlations of different\nkinds of bias across sensitive attributes. Although the viewer ratings of these\nvideos should purely reflect the speaker's competence and skill, our analysis\nof the ratings demonstrates the presence of overwhelming and predominant\nimplicit bias with respect to race and gender. In our paper, we present\nstrategies to detect and mitigate bias that are critical to removing unfairness\nin AI.",
    "published_date": "2020-03-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00683v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.00606v1",
    "title": "Participatory Budgeting: Models and Approaches",
    "authors": [
      "Haris Aziz",
      "Nisarg Shah"
    ],
    "author_ids": [],
    "abstract": "Participatory budgeting is a democratic approach to deciding the funding of\npublic projects, which has been adopted in many cities across the world. We\npresent a survey of research on participatory budgeting emerging from the\ncomputational social choice literature, which draws ideas from computer science\nand microeconomic theory. We present a mathematical model for participatory\nbudgeting, which charts existing models across different axes including whether\nthe projects are treated as \"divisible\" or \"indivisible\" and whether there are\nfunding limits on individual projects. We then survey various approaches and\nmethods from the literature, giving special emphasis on issues of preference\nelicitation, welfare objectives, fairness axioms, and voter incentives.\nFinally, we discuss several directions in which research on participatory\nbudgeting can be extended in the future.",
    "published_date": "2020-03-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "91A12, 68Q15",
      "F.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00606v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.00576v2",
    "title": "StructSum: Summarization via Structured Representations",
    "authors": [
      "Vidhisha Balachandran",
      "Artidoro Pagnoni",
      "Jay Yoon Lee",
      "Dheeraj Rajagopal",
      "Jaime Carbonell",
      "Yulia Tsvetkov"
    ],
    "author_ids": [],
    "abstract": "Abstractive text summarization aims at compressing the information of a long\nsource document into a rephrased, condensed summary. Despite advances in\nmodeling techniques, abstractive summarization models still suffer from several\nkey challenges: (i) layout bias: they overfit to the style of training corpora;\n(ii) limited abstractiveness: they are optimized to copying n-grams from the\nsource rather than generating novel abstractive summaries; (iii) lack of\ntransparency: they are not interpretable. In this work, we propose a framework\nbased on document-level structure induction for summarization to address these\nchallenges. To this end, we propose incorporating latent and explicit\ndependencies across sentences in the source document into end-to-end\nsingle-document summarization models. Our framework complements standard\nencoder-decoder summarization models by augmenting them with rich\nstructure-aware document representations based on implicitly learned (latent)\nstructures and externally-derived linguistic (explicit) structures. We show\nthat our summarization framework, trained on the CNN/DM dataset, improves the\ncoverage of content in the source documents, generates more abstractive\nsummaries by generating more novel n-grams, and incorporates interpretable\nsentence-level structures, while performing on par with standard baselines.",
    "published_date": "2020-03-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00576v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.04196v1",
    "title": "Joint Optimization of User Association, Subchannel Allocation, and Power Allocation in Multi-cell Multi-association OFDMA Heterogeneous Networks",
    "authors": [
      "Feng Wang",
      "Wen Chen",
      "Hongying Tang",
      "Qingqing Wu"
    ],
    "author_ids": [],
    "abstract": "Heterogeneous network is a novel network architecture proposed in\nLong-Term-Evolution~(LTE), which highly increases the capacity and coverage\ncompared with the conventional networks. However, in order to provide the best\nservices, appropriate resource management must be applied. In this paper, we\nconsider the joint optimization problem of user association, subchannel\nallocation, and power allocation for downlink transmission in Multi-cell\nMulti-association Orthogonal Frequency Division Multiple Access (OFDMA)\nheterogeneous networks. To solve the optimization problem, we first divide it\ninto two subproblems: 1) user association and subchannel allocation for fixed\npower allocation; 2) power allocation for fixed user association and subchannel\nallocation. Subsequently, we obtain a locally optimal solution for the joint\noptimization problem by solving these two subproblems alternately. For the\nfirst subproblem, we derive the globally optimal solution based on graph\ntheory. For the second subproblem, we obtain a Karush-Kuhn-Tucker (KKT) optimal\nsolution by a low complexity algorithm based on the difference of two convex\nfunctions approximation (DCA) method. In addition, the multi-antenna receiver\ncase and the proportional fairness case are also discussed. Simulation results\ndemonstrate that the proposed algorithms can significantly enhance the overall\nnetwork throughput.",
    "published_date": "2020-03-01T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.04196v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2003.00201v1",
    "title": "What Emotions Make One or Five Stars? Understanding Ratings of Online Product Reviews by Sentiment Analysis and XAI",
    "authors": [
      "Chaehan So"
    ],
    "author_ids": [],
    "abstract": "When people buy products online, they primarily base their decisions on the\nrecommendations of others given in online reviews. The current work analyzed\nthese online reviews by sentiment analysis and used the extracted sentiments as\nfeatures to predict the product ratings by several machine learning algorithms.\nThese predictions were disentangled by various meth-ods of explainable AI (XAI)\nto understand whether the model showed any bias during prediction. Study 1\nbenchmarked these algorithms (knn, support vector machines, random forests,\ngradient boosting machines, XGBoost) and identified random forests and XGBoost\nas best algorithms for predicting the product ratings. In Study 2, the analysis\nof global feature importance identified the sentiment joy and the emotional\nvalence negative as most predictive features. Two XAI visualization methods,\nlocal feature attributions and partial dependency plots, revealed several\nincorrect prediction mechanisms on the instance-level. Performing the\nbenchmarking as classification, Study 3 identified a high no-information rate\nof 64.4% that indicated high class imbalance as underlying reason for the\nidentified problems. In conclusion, good performance by machine learning\nalgorithms must be taken with caution because the dataset, as encountered in\nthis work, could be biased towards certain predictions. This work demonstrates\nhow XAI methods reveal such prediction bias.",
    "published_date": "2020-02-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00201v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.00899v2",
    "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline",
    "authors": [
      "Kate Wilkinson",
      "George Cevora"
    ],
    "author_ids": [],
    "abstract": "Most datasets of interest to the analytics industry are impacted by various\nforms of human bias. The outcomes of Data Analytics [DA] or Machine Learning\n[ML] on such data are therefore prone to replicating the bias. As a result, a\nlarge number of biased decision-making systems based on DA/ML have recently\nattracted attention. In this paper we introduce Rosa, a free, web-based tool to\neasily de-bias datasets with respect to a chosen characteristic. Rosa is based\non the principles of Fair Adversarial Networks, developed by illumr Ltd., and\ncan therefore remove interactive, non-linear, and non-binary bias. Rosa is\nstand-alone pre-processing step / API, meaning it can be used easily with any\nDA/ML pipeline. We test the efficacy of Rosa in removing bias from data-driven\ndecision making systems by performing standard DA tasks on five real-world\ndatasets, selected for their relevance to current DA problems, and also their\nhigh potential for bias. We use simple ML models to model a characteristic of\nanalytical interest, and compare the level of bias in the model output both\nwith and without Rosa as a pre-processing step. We find that in all cases there\nis a substantial decrease in bias of the data-driven decision making systems\nwhen the data is pre-processed with Rosa.",
    "published_date": "2020-02-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00899v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12612v2",
    "title": "A multi-layer approach to disinformation detection on Twitter",
    "authors": [
      "Francesco Pierri",
      "Carlo Piccardi",
      "Stefano Ceri"
    ],
    "author_ids": [],
    "abstract": "We tackle the problem of classifying news articles pertaining to\ndisinformation vs mainstream news by solely inspecting their diffusion\nmechanisms on Twitter. Our technique is inherently simple compared to existing\ntext-based approaches, as it allows to by-pass the multiple levels of\ncomplexity which are found in news content (e.g. grammar, syntax, style). We\nemploy a multi-layer representation of Twitter diffusion networks, and we\ncompute for each layer a set of global network features which quantify\ndifferent aspects of the sharing process. Experimental results with two\nlarge-scale datasets, corresponding to diffusion cascades of news shared\nrespectively in the United States and Italy, show that a simple Logistic\nRegression model is able to classify disinformation vs mainstream networks with\nhigh accuracy (AUROC up to 94%), also when considering the political bias of\ndifferent sources in the classification task. We also highlight differences in\nthe sharing patterns of the two news domains which appear to be\ncountry-independent. We believe that our network-based approach provides useful\ninsights which pave the way to the future development of a system to detect\nmisleading and harmful information spreading on social media.",
    "published_date": "2020-02-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12612v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12528v1",
    "title": "Handling Position Bias for Unbiased Learning to Rank in Hotels Search",
    "authors": [
      "Yinxiao Li"
    ],
    "author_ids": [],
    "abstract": "Nowadays, search ranking and recommendation systems rely on a lot of data to\ntrain machine learning models such as Learning-to-Rank (LTR) models to rank\nresults for a given query, and implicit user feedbacks (e.g. click data) have\nbecome the dominant source of data collection due to its abundance and low\ncost, especially for major Internet companies. However, a drawback of this data\ncollection approach is the data could be highly biased, and one of the most\nsignificant biases is the position bias, where users are biased towards\nclicking on higher ranked results. In this work, we will investigate the\nmarginal importance of properly handling the position bias in an online test\nenvironment in Tripadvisor Hotels search. We propose an empirically effective\nmethod of handling the position bias that fully leverages the user action data.\nWe take advantage of the fact that when user clicks a result, he has almost\ncertainly observed all the results above, and the propensities of the results\nbelow the clicked result will be estimated by a simple but effective position\nbias model. The online A/B test results show that this method leads to an\nimproved search ranking model.",
    "published_date": "2020-02-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12528v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12464v1",
    "title": "\"Do the Right Thing\" for Whom? An Experiment on Ingroup Favouritism, Group Assorting and Moral Suasion",
    "authors": [
      "Ennio Bilancini",
      "Leonardo Boncinelli",
      "Valerio Capraro",
      "Tatiana Celadin",
      "Roberto Di Paolo"
    ],
    "author_ids": [],
    "abstract": "In this paper we investigate the effect of moral suasion on ingroup\nfavouritism. We report a well-powered, pre-registered, two-stage 2x2\nmixed-design experiment. In the first stage, groups are formed on the basis of\nhow participants answer to a set of questions, concerning non-morally relevant\nissues in one treatment (assorting on non-moral preferences), and morally\nrelevant issues in another treatment (assorting on moral preferences). In the\nsecond stage, participants choose how to split a given amount of money between\nparticipants of their own group and participants of the other group, first in\nthe baseline setting and then in a setting where they are told to do what they\nbelieve to be morally right (moral suasion). Our main results are: (i) in the\nbaseline, participants tend to favour their own group to a greater extent when\ngroups are assorted according to moral preferences, compared to when they are\nassorted according to non-moral preferences; (ii) the net effect of moral\nsuasion is to decrease ingroup favouritism, but there is also a non-negligible\nproportion of participants for whom moral suasion increases ingroup\nfavouritism; (iii) the effect of moral suasion is substantially stable across\ngroup assorting and four pre-registered individual characteristics (gender,\npolitical orientation, religiosity, pro-life vs pro-choice ethical\nconvictions).",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "cs.SI",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12464v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.12416v4",
    "title": "Learning in the Frequency Domain",
    "authors": [
      "Kai Xu",
      "Minghai Qin",
      "Fei Sun",
      "Yuhao Wang",
      "Yen-Kuang Chen",
      "Fengbo Ren"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks have achieved remarkable success in computer vision\ntasks. Existing neural networks mainly operate in the spatial domain with fixed\ninput sizes. For practical applications, images are usually large and have to\nbe downsampled to the predetermined input size of neural networks. Even though\nthe downsampling operations reduce computation and the required communication\nbandwidth, it removes both redundant and salient information obliviously, which\nresults in accuracy degradation. Inspired by digital signal processing\ntheories, we analyze the spectral bias from the frequency perspective and\npropose a learning-based frequency selection method to identify the trivial\nfrequency components which can be removed without accuracy loss. The proposed\nmethod of learning in the frequency domain leverages identical structures of\nthe well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN,\nwhile accepting the frequency-domain information as the input. Experiment\nresults show that learning in the frequency domain with static channel\nselection can achieve higher accuracy than the conventional spatial\ndownsampling approach and meanwhile further reduce the input data size.\nSpecifically for ImageNet classification with the same input size, the proposed\nmethod achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and\nMobileNetV2, respectively. Even with half input size, the proposed method still\nimproves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8%\naverage precision improvement on Mask R-CNN for instance segmentation on the\nCOCO dataset.",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12416v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12399v1",
    "title": "ConQUR: Mitigating Delusional Bias in Deep Q-learning",
    "authors": [
      "Andy Su",
      "Jayden Ooi",
      "Tyler Lu",
      "Dale Schuurmans",
      "Craig Boutilier"
    ],
    "author_ids": [],
    "abstract": "Delusional bias is a fundamental source of error in approximate Q-learning.\nTo date, the only techniques that explicitly address delusion require\ncomprehensive search using tabular value estimates. In this paper, we develop\nefficient methods to mitigate delusional bias by training Q-approximators with\nlabels that are \"consistent\" with the underlying greedy policy class. We\nintroduce a simple penalization scheme that encourages Q-labels used across\ntraining batches to remain (jointly) consistent with the expressible policy\nclass. We also propose a search framework that allows multiple Q-approximators\nto be generated and tracked, thus mitigating the effect of premature (implicit)\npolicy commitments. Experimental results demonstrate that these methods can\nimprove the performance of Q-learning in a variety of Atari games, sometimes\ndramatically.",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12093v1",
    "title": "Reducing Geographic Performance Differential for Face Recognition",
    "authors": [
      "Martins Bruveris",
      "Jochem Gietema",
      "Pouria Mortazavian",
      "Mohan Mahadevan"
    ],
    "author_ids": [],
    "abstract": "As face recognition algorithms become more accurate and get deployed more\nwidely, it becomes increasingly important to ensure that the algorithms work\nequally well for everyone. We study the geographic performance\ndifferentials-differences in false acceptance and false rejection rates across\ndifferent countries-when comparing selfies against photos from ID documents. We\nshow how to mitigate geographic performance differentials using sampling\nstrategies despite large imbalances in the dataset. Using vanilla domain\nadaptation strategies to fine-tune a face recognition CNN on domain-specific\ndoc-selfie data improves the performance of the model on such data, but, in the\npresence of imbalanced training data, also significantly increases the\ndemographic bias. We then show how to mitigate this effect by employing\nsampling strategies to balance the training procedure.",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11977v1",
    "title": "Multiple Discrimination and Pairwise CNN for View-based 3D Object Retrieval",
    "authors": [
      "Z. Gao",
      "K. X Xue",
      "S. H Wan"
    ],
    "author_ids": [],
    "abstract": "With the rapid development and wide application of computer, camera device,\nnetwork and hardware technology, 3D object (or model) retrieval has attracted\nwidespread attention and it has become a hot research topic in the computer\nvision domain. Deep learning features already available in 3D object retrieval\nhave been proven to be better than the retrieval performance of hand-crafted\nfeatures. However, most existing networks do not take into account the impact\nof multi-view image selection on network training, and the use of contrastive\nloss alone only forcing the same-class samples to be as close as possible. In\nthis work, a novel solution named Multi-view Discrimination and Pairwise CNN\n(MDPCNN) for 3D object retrieval is proposed to tackle these issues. It can\nsimultaneously input of multiple batches and multiple views by adding the Slice\nlayer and the Concat layer. Furthermore, a highly discriminative network is\nobtained by training samples that are not easy to be classified by clustering.\nLastly, we deploy the contrastive-center loss and contrastive loss as the\noptimization objective that has better intra-class compactness and inter-class\nseparability. Large-scale experiments show that the proposed MDPCNN can achieve\na significant performance over the state-of-the-art algorithms in 3D object\nretrieval.",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11977v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11949v3",
    "title": "Unbiased Scene Graph Generation from Biased Training",
    "authors": [
      "Kaihua Tang",
      "Yulei Niu",
      "Jianqiang Huang",
      "Jiaxin Shi",
      "Hanwang Zhang"
    ],
    "author_ids": [],
    "abstract": "Today's scene graph generation (SGG) task is still far from practical, mainly\ndue to the severe training bias, e.g., collapsing diverse \"human walk on / sit\non / lay on beach\" into \"human on beach\". Given such SGG, the down-stream tasks\nsuch as VQA can hardly infer better scene structures than merely a bag of\nobjects. However, debiasing in SGG is not trivial because traditional debiasing\nmethods cannot distinguish between the good and bad bias, e.g., good context\nprior (e.g., \"person read book\" rather than \"eat\") and bad long-tailed bias\n(e.g., \"near\" dominating \"behind / in front of\"). In this paper, we present a\nnovel SGG framework based on causal inference but not the conventional\nlikelihood. We first build a causal graph for SGG, and perform traditional\nbiased training with the graph. Then, we propose to draw the counterfactual\ncausality from the trained graph to infer the effect from the bad bias, which\nshould be removed. In particular, we use Total Direct Effect (TDE) as the\nproposed final predicate score for unbiased SGG. Note that our framework is\nagnostic to any SGG model and thus can be widely applied in the community who\nseeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit\non the SGG benchmark Visual Genome and several prevailing models, we observed\nsignificant improvements over the previous state-of-the-art methods.",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11949v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11849v1",
    "title": "Comparison of Multi-Class and Binary Classification Machine Learning Models in Identifying Strong Gravitational Lenses",
    "authors": [
      "Hossen Teimoorinia",
      "Robert D. Toyonaga",
      "Sebastien Fabbro",
      "Connor Bottrell"
    ],
    "author_ids": [],
    "abstract": "Typically, binary classification lens-finding schemes are used to\ndiscriminate between lens candidates and non-lenses. However, these models\noften suffer from substantial false-positive classifications. Such false\npositives frequently occur due to images containing objects such as crowded\nsources, galaxies with arms, and also images with a central source and smaller\nsurrounding sources. Therefore, a model might confuse the stated circumstances\nwith an Einstein ring. It has been proposed that by allowing such commonly\nmisclassified image types to constitute their own classes, machine learning\nmodels will more easily be able to learn the difference between images that\ncontain real lenses, and images that contain lens imposters. Using Hubble Space\nTelescope (HST) images, in the F814W filter, we compare the usage of binary and\nmulti-class classification models applied to the lens finding task. From our\nfindings, we conclude there is not a significant benefit to using the\nmulti-class model over a binary model. We will also present the results of a\nsimple lens search using a multi-class machine learning model, and potential\nnew lens candidates.",
    "published_date": "2020-02-27T00:00:00",
    "year": 2020,
    "categories": [
      "astro-ph.GA",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11836v1",
    "title": "No computation without representation: Avoiding data and algorithm biases through diversity",
    "authors": [
      "Caitlin Kuhlman",
      "Latifa Jackson",
      "Rumi Chunara"
    ],
    "author_ids": [],
    "abstract": "The emergence and growth of research on issues of ethics in AI, and in\nparticular algorithmic fairness, has roots in an essential observation that\nstructural inequalities in society are reflected in the data used to train\npredictive models and in the design of objective functions. While research\naiming to mitigate these issues is inherently interdisciplinary, the design of\nunbiased algorithms and fair socio-technical systems are key desired outcomes\nwhich depend on practitioners from the fields of data science and computing.\nHowever, these computing fields broadly also suffer from the same\nunder-representation issues that are found in the datasets we analyze. This\ndisconnect affects the design of both the desired outcomes and metrics by which\nwe measure success. If the ethical AI research community accepts this, we\ntacitly endorse the status quo and contradict the goals of non-discrimination\nand equity which work on algorithmic fairness, accountability, and transparency\nseeks to address. Therefore, we advocate in this work for diversifying\ncomputing as a core priority of the field and our efforts to achieve ethical AI\npractices. We draw connections between the lack of diversity within academic\nand professional computing fields and the type and breadth of the biases\nencountered in datasets, machine learning models, problem formulations, and\ninterpretation of results. Examining the current fairness/ethics in AI\nliterature, we highlight cases where this lack of diverse perspectives has been\nfoundational to the inequity in treatment of underrepresented and protected\ngroup data. We also look to other professional communities, such as in law and\nhealth, where disparities have been reduced both in the educational diversity\nof trainees and among professional practices. We use these lessons to develop\nrecommendations that provide concrete steps for the computing community to\nincrease diversity.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11836v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11651v2",
    "title": "Fair Learning with Private Demographic Data",
    "authors": [
      "Hussein Mozannar",
      "Mesrob I. Ohannessian",
      "Nathan Srebro"
    ],
    "author_ids": [],
    "abstract": "Sensitive attributes such as race are rarely available to learners in real\nworld settings as their collection is often restricted by laws and regulations.\nWe give a scheme that allows individuals to release their sensitive information\nprivately while still allowing any downstream entity to learn\nnon-discriminatory predictors. We show how to adapt non-discriminatory learners\nto work with privatized protected attributes giving theoretical guarantees on\nperformance. Finally, we highlight how the methodology could apply to learning\nfair predictors in settings where protected attributes are only available for a\nsubset of the data.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11651v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11645v1",
    "title": "Facebook Ads as a Demographic Tool to Measure the Urban-Rural Divide",
    "authors": [
      "Daniele Rama",
      "Yelena Mejova",
      "Michele Tizzoni",
      "Kyriaki Kalimeri",
      "Ingmar Weber"
    ],
    "author_ids": [],
    "abstract": "In the global move toward urbanization, making sure the people remaining in\nrural areas are not left behind in terms of development and policy\nconsiderations is a priority for governments worldwide. However, it is\nincreasingly challenging to track important statistics concerning this sparse,\ngeographically dispersed population, resulting in a lack of reliable,\nup-to-date data. In this study, we examine the usefulness of the Facebook\nAdvertising platform, which offers a digital \"census\" of over two billions of\nits users, in measuring potential rural-urban inequalities. We focus on Italy,\na country where about 30% of the population lives in rural areas. First, we\nshow that the population statistics that Facebook produces suffer from\ninstability across time and incomplete coverage of sparsely populated\nmunicipalities. To overcome such limitation, we propose an alternative\nmethodology for estimating Facebook Ads audiences that nearly triples the\ncoverage of the rural municipalities from 19% to 55% and makes feasible\nfine-grained sub-population analysis. Using official national census data, we\nevaluate our approach and confirm known significant urban-rural divides in\nterms of educational attainment and income. Extending the analysis to\nFacebook-specific user \"interests\" and behaviors, we provide further insights\non the divide, for instance, finding that rural areas show a higher interest in\ngambling. Notably, we find that the most predictive features of income in rural\nareas differ from those for urban centres, suggesting researchers need to\nconsider a broader range of attributes when examining rural wellbeing. The\nfindings of this study illustrate the necessity of improving existing tools and\nmethodologies to include under-represented populations in digital demographic\nstudies -- the failure to do so could result in misleading observations,\nconclusions, and most importantly, policies.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11645v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.11442v3",
    "title": "DeBayes: a Bayesian Method for Debiasing Network Embeddings",
    "authors": [
      "Maarten Buyl",
      "Tijl De Bie"
    ],
    "author_ids": [],
    "abstract": "As machine learning algorithms are increasingly deployed for high-impact\nautomated decision making, ethical and increasingly also legal standards demand\nthat they treat all individuals fairly, without discrimination based on their\nage, gender, race or other sensitive traits. In recent years much progress has\nbeen made on ensuring fairness and reducing bias in standard machine learning\nsettings. Yet, for network embedding, with applications in vulnerable domains\nranging from social network analysis to recommender systems, current options\nremain limited both in number and performance. We thus propose DeBayes: a\nconceptually elegant Bayesian method that is capable of learning debiased\nembeddings by using a biased prior. Our experiments show that these\nrepresentations can then be used to perform link prediction that is\nsignificantly more fair in terms of popular metrics such as demographic parity\nand equalized opportunity.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11442v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11437v4",
    "title": "Consensus-Halving: Does It Ever Get Easier?",
    "authors": [
      "Aris Filos-Ratsikas",
      "Alexandros Hollender",
      "Katerina Sotiraki",
      "Manolis Zampetakis"
    ],
    "author_ids": [],
    "abstract": "In the $\\varepsilon$-Consensus-Halving problem, a fundamental problem in fair\ndivision, there are $n$ agents with valuations over the interval $[0,1]$, and\nthe goal is to divide the interval into pieces and assign a label \"$+$\" or\n\"$-$\" to each piece, such that every agent values the total amount of \"$+$\" and\nthe total amount of \"$-$\" almost equally. The problem was recently proven by\nFilos-Ratsikas and Goldberg [2019] to be the first \"natural\" complete problem\nfor the computational class PPA, answering a decade-old open question.\n  In this paper, we examine the extent to which the problem becomes easy to\nsolve, if one restricts the class of valuation functions. To this end, we\nprovide the following contributions. First, we obtain a strengthening of the\nPPA-hardness result of [Filos-Ratsikas and Goldberg, 2019], to the case when\nagents have piecewise uniform valuations with only two blocks. We obtain this\nresult via a new reduction, which is in fact conceptually much simpler than the\ncorresponding one in [Filos-Ratsikas and Goldberg, 2019]. Then, we consider the\ncase of single-block (uniform) valuations and provide a parameterized\npolynomial time algorithm for solving $\\varepsilon$-Consensus-Halving for any\n$\\varepsilon$, as well as a polynomial-time algorithm for $\\varepsilon=1/2$.\nFinally, an important application of our new techniques is the first hardness\nresult for a generalization of Consensus-Halving, the Consensus-$1/k$-Division\nproblem [Simmons and Su, 2003]. In particular, we prove that\n$\\varepsilon$-Consensus-$1/3$-Division is PPAD-hard.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CC",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11437v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.12143v1",
    "title": "Fairness-Aware Learning with Prejudice Free Representations",
    "authors": [
      "Ramanujam Madhavan",
      "Mohit Wadhwa"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are extensively being used to make decisions that\nhave a significant impact on human life. These models are trained over\nhistorical data that may contain information about sensitive attributes such as\nrace, sex, religion, etc. The presence of such sensitive attributes can impact\ncertain population subgroups unfairly. It is straightforward to remove\nsensitive features from the data; however, a model could pick up prejudice from\nlatent sensitive attributes that may exist in the training data. This has led\nto the growing apprehension about the fairness of the employed models. In this\npaper, we propose a novel algorithm that can effectively identify and treat\nlatent discriminating features. The approach is agnostic of the learning\nalgorithm and generalizes well for classification as well as regression tasks.\nIt can also be used as a key aid in proving that the model is free of\ndiscrimination towards regulatory compliance if the need arises. The approach\nhelps to collect discrimination-free features that would improve the model\nperformance while ensuring the fairness of the model. The experimental results\nfrom our evaluations on publicly available real-world datasets show a\nnear-ideal fairness measurement in comparison to other methods.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12143v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11387v1",
    "title": "Gini Index based Initial Coin Offering Mechanism",
    "authors": [
      "Mingyu Guo",
      "Zhenghui Wang",
      "Yuko Sakurai"
    ],
    "author_ids": [],
    "abstract": "As a fundraising method, Initial Coin Offering (ICO) has raised billions of\ndollars for thousands of startups in the past two years. Existing ICO\nmechanisms place more emphasis on the short-term benefits of maximal\nfundraising while ignoring the problem of unbalanced token allocation, which\nnegatively impacts subsequent fundraising and has bad effects on introducing\nnew investors and resources. We propose a new ICO mechanism which uses the\nconcept of Gini index for the very first time as a mechanism design constraint\nto control allocation inequality. Our mechanism maintains an elegant and\nstraightforward structure. It allows the agents to modify their bids as a price\ndiscovery process, while limiting the bids of whales. We analyze the agents'\nequilibrium behaviors under our mechanism. Under natural technical assumptions,\nwe show that most agents have simple dominant strategies and the equilibrium\nrevenue approaches the optimal revenue asymptotically in the number of agents.\nWe verify our mechanism using real ICO dataset we collected, and confirm that\nour mechanism performs well in terms of both allocation fairness and revenue.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11387v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.11328v3",
    "title": "Rethinking Bias-Variance Trade-off for Generalization of Neural Networks",
    "authors": [
      "Zitong Yang",
      "Yaodong Yu",
      "Chong You",
      "Jacob Steinhardt",
      "Yi Ma"
    ],
    "author_ids": [],
    "abstract": "The classical bias-variance trade-off predicts that bias decreases and\nvariance increase with model complexity, leading to a U-shaped risk curve.\nRecent work calls this into question for neural networks and other\nover-parameterized models, for which it is often observed that larger models\ngeneralize better. We provide a simple explanation for this by measuring the\nbias and variance of neural networks: while the bias is monotonically\ndecreasing as in the classical theory, the variance is unimodal or bell-shaped:\nit increases then decreases with the width of the network. We vary the network\narchitecture, loss function, and choice of dataset and confirm that variance\nunimodality occurs robustly for all models we considered. The risk curve is the\nsum of the bias and variance curves and displays different qualitative shapes\ndepending on the relative scale of bias and variance, with the double descent\ncurve observed in recent literature as a special case. We corroborate these\nempirical results with a theoretical analysis of two-layer linear networks with\nrandom first layer. Finally, evaluation on out-of-distribution data shows that\nmost of the drop in accuracy comes from increased bias while variance increases\nby a relatively small amount. Moreover, we find that deeper models decrease\nbias and increase variance for both in-distribution and out-of-distribution\ndata.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11328v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11293v3",
    "title": "Adversarial Ranking Attack and Defense",
    "authors": [
      "Mo Zhou",
      "Zhenxing Niu",
      "Le Wang",
      "Qilin Zhang",
      "Gang Hua"
    ],
    "author_ids": [],
    "abstract": "Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack,\nwhere an imperceptible perturbation could result in misclassification. However,\nthe vulnerability of DNN-based image ranking systems remains under-explored. In\nthis paper, we propose two attacks against deep ranking systems, i.e.,\nCandidate Attack and Query Attack, that can raise or lower the rank of chosen\ncandidates by adversarial perturbations. Specifically, the expected ranking\norder is first represented as a set of inequalities, and then a triplet-like\nobjective function is designed to obtain the optimal perturbation. Conversely,\na defense method is also proposed to improve the ranking system robustness,\nwhich can mitigate all the proposed attacks simultaneously. Our adversarial\nranking attacks and defense are evaluated on datasets including MNIST,\nFashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate\nthat a typical deep ranking system can be effectively compromised by our\nattacks. Meanwhile, the system robustness can be moderately improved with our\ndefense. Furthermore, the transferable and universal properties of our\nadversary illustrate the possibility of realistic black-box attack.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11293v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12162v3",
    "title": "Defending against Backdoor Attack on Deep Neural Networks",
    "authors": [
      "Hao Cheng",
      "Kaidi Xu",
      "Sijia Liu",
      "Pin-Yu Chen",
      "Pu Zhao",
      "Xue Lin"
    ],
    "author_ids": [],
    "abstract": "Although deep neural networks (DNNs) have achieved a great success in various\ncomputer vision tasks, it is recently found that they are vulnerable to\nadversarial attacks. In this paper, we focus on the so-called \\textit{backdoor\nattack}, which injects a backdoor trigger to a small portion of training data\n(also known as data poisoning) such that the trained DNN induces\nmisclassification while facing examples with this trigger. To be specific, we\ncarefully study the effect of both real and synthetic backdoor attacks on the\ninternal response of vanilla and backdoored DNNs through the lens of Gard-CAM.\nMoreover, we show that the backdoor attack induces a significant bias in neuron\nactivation in terms of the $\\ell_\\infty$ norm of an activation map compared to\nits $\\ell_1$ and $\\ell_2$ norm. Spurred by our results, we propose the\n\\textit{$\\ell_\\infty$-based neuron pruning} to remove the backdoor from the\nbackdoored DNN. Experiments show that our method could effectively decrease the\nattack success rate, and also hold a high classification accuracy for clean\nimages.",
    "published_date": "2020-02-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12162v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11169v4",
    "title": "Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes with Applications to Anomaly Detection",
    "authors": [
      "William Paul",
      "I-Jeng Wang",
      "Fady Alajaji",
      "Philippe Burlina"
    ],
    "author_ids": [],
    "abstract": "Our work focuses on unsupervised and generative methods that address the\nfollowing goals: (a) learning unsupervised generative representations that\ndiscover latent factors controlling image semantic attributes, (b) studying how\nthis ability to control attributes formally relates to the issue of latent\nfactor disentanglement, clarifying related but dissimilar concepts that had\nbeen confounded in the past, and (c) developing anomaly detection methods that\nleverage representations learned in (a). For (a), we propose a network\narchitecture that exploits the combination of multiscale generative models with\nmutual information (MI) maximization. For (b), we derive an analytical result\n(Lemma 1) that brings clarity to two related but distinct concepts: the ability\nof generative networks to control semantic attributes of images they generate,\nresulting from MI maximization, and the ability to disentangle latent space\nrepresentations, obtained via total correlation minimization. More\nspecifically, we demonstrate that maximizing semantic attribute control\nencourages disentanglement of latent factors. Using Lemma 1 and adopting MI in\nour loss function, we then show empirically that, for image generation tasks,\nthe proposed approach exhibits superior performance as measured in the quality\nand disentanglement trade space, when compared to other state of the art\nmethods, with quality assessed via the Frechet Inception Distance (FID), and\ndisentanglement via mutual information gap. For (c), we design several systems\nfor anomaly detection exploiting representations learned in (a), and\ndemonstrate their performance benefits when compared to state-of-the-art\ngenerative and discriminative algorithms. The above contributions in\nrepresentation learning have potential applications in addressing other\nimportant problems in computer vision, such as bias and privacy in AI.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11169v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12880v3",
    "title": "Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data",
    "authors": [
      "Marc Finzi",
      "Samuel Stanton",
      "Pavel Izmailov",
      "Andrew Gordon Wilson"
    ],
    "author_ids": [],
    "abstract": "The translation equivariance of convolutional layers enables convolutional\nneural networks to generalize well on image problems. While translation\nequivariance provides a powerful inductive bias for images, we often\nadditionally desire equivariance to other transformations, such as rotations,\nespecially for non-image data. We propose a general method to construct a\nconvolutional layer that is equivariant to transformations from any specified\nLie group with a surjective exponential map. Incorporating equivariance to a\nnew group requires implementing only the group exponential and logarithm maps,\nenabling rapid prototyping. Showcasing the simplicity and generality of our\nmethod, we apply the same model architecture to images, ball-and-stick\nmolecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the\nequivariance of our models is especially impactful, leading to exact\nconservation of linear and angular momentum.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12880v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10992v2",
    "title": "Migration Networks: Applications of Network Analysis to Macroscale Migration Patterns",
    "authors": [
      "Valentin Danchev",
      "Mason A. Porter"
    ],
    "author_ids": [],
    "abstract": "An emerging area of research is the study of macroscale migration patterns as\na network of nodes that represent places (e.g., countries, cities, and rural\nareas) and edges that encode migration ties that connect those places. In this\nchapter, we first review advances in the study of migration networks and recent\nwork that has employed network analysis to examine such networks at different\ngeographical scales. In our discussion, we focus in particular on global scale\nmigration networks. We then propose ways to leverage network analysis in\nconcert with digital technologies and online geolocated data to examine the\nstructure and dynamics of migration networks. The implementation of such\napproaches for studying migration networks faces many challenges, including\nethical ones, methodological ones, socio-technological ones (e.g., data\navailability and reuse), and research reproducibility. We detail these\nchallenges, and we then consider possible ways of linking digital geolocated\ndata to administrative and survey data as a way of harnessing new technologies\nto construct increasingly realistic migration networks (e.g., using multiplex\nnetworks). We also briefly discuss new methods (e.g., multilayer network\nanalysis) in network analysis and adjacent fields (e.g., machine learning) that\ncan help advance understanding of macroscale patterns of migration.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "nlin.AO",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10992v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10936v2",
    "title": "Stochastic encoding of graphs in deep learning allows for complex analysis of gender classification in resting-state and task functional brain networks from the UK Biobank",
    "authors": [
      "Matthew Leming",
      "John Suckling"
    ],
    "author_ids": [],
    "abstract": "Classification of whole-brain functional connectivity MRI data with\nconvolutional neural networks (CNNs) has shown promise, but the complexity of\nthese models impedes understanding of which aspects of brain activity\ncontribute to classification. While visualization techniques have been\ndeveloped to interpret CNNs, bias inherent in the method of encoding abstract\ninput data, as well as the natural variance of deep learning models, detract\nfrom the accuracy of these techniques. We introduce a stochastic encoding\nmethod in an ensemble of CNNs to classify functional connectomes by gender. We\napplied our method to resting-state and task data from the UK BioBank, using\ntwo visualization techniques to measure the salience of three brain networks\ninvolved in task- and resting-states, and their interaction. To regress\nconfounding factors such as head motion, age, and intracranial volume, we\nintroduced a multivariate balancing algorithm to ensure equal distributions of\nsuch covariates between classes in our data. We achieved a final AUROC of\n0.8459. We found that resting-state data classifies more accurately than task\ndata, with the inner salience network playing the most important role of the\nthree networks overall in classification of resting-state data and connections\nto the central executive network in task data.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.NC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10936v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10774v2",
    "title": "Counterfactual fairness: removing direct effects through regularization",
    "authors": [
      "Pietro G. Di Stefano",
      "James M. Hickey",
      "Vlasios Vasileiou"
    ],
    "author_ids": [],
    "abstract": "Building machine learning models that are fair with respect to an\nunprivileged group is a topical problem. Modern fairness-aware algorithms often\nignore causal effects and enforce fairness through modifications applicable to\nonly a subset of machine learning models. In this work, we propose a new\ndefinition of fairness that incorporates causality through the Controlled\nDirect Effect (CDE). We develop regularizations to tackle classical fairness\nmeasures and present a causal regularization that satisfies our new fairness\ndefinition by removing the impact of unprivileged group variables on the model\noutcomes as measured by the CDE. These regularizations are applicable to any\nmodel trained using by iteratively minimizing a loss through differentiation.\nWe demonstrate our approaches using both gradient boosting and logistic\nregression on: a synthetic dataset, the UCI Adult (Census) Dataset, and a\nreal-world credit-risk dataset. Our results were found to mitigate unfairness\nfrom the predictions with small reductions in model performance.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10774v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10764v2",
    "title": "FairRec: Two-Sided Fairness for Personalized Recommendations in Two-Sided Platforms",
    "authors": [
      "Gourab K Patro",
      "Arpita Biswas",
      "Niloy Ganguly",
      "Krishna P. Gummadi",
      "Abhijnan Chakraborty"
    ],
    "author_ids": [],
    "abstract": "We investigate the problem of fair recommendation in the context of two-sided\nonline platforms, comprising customers on one side and producers on the other.\nTraditionally, recommendation services in these platforms have focused on\nmaximizing customer satisfaction by tailoring the results according to the\npersonalized preferences of individual customers. However, our investigation\nreveals that such customer-centric design may lead to unfair distribution of\nexposure among the producers, which may adversely impact their well-being. On\nthe other hand, a producer-centric design might become unfair to the customers.\nThus, we consider fairness issues that span both customers and producers. Our\napproach involves a novel mapping of the fair recommendation problem to a\nconstrained version of the problem of fairly allocating indivisible goods. Our\nproposed FairRec algorithm guarantees at least Maximin Share (MMS) of exposure\nfor most of the producers and Envy-Free up to One item (EF1) fairness for every\ncustomer. Extensive evaluations over multiple real-world datasets show the\neffectiveness of FairRec in ensuring two-sided fairness while incurring a\nmarginal loss in the overall recommendation quality.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10764v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10704v3",
    "title": "Fair and Truthful Mechanisms for Dichotomous Valuations",
    "authors": [
      "Moshe Babaioff",
      "Tomer Ezra",
      "Uriel Feige"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of allocating a set on indivisible items to players\nwith private preferences in an efficient and fair way. We focus on valuations\nthat have dichotomous marginals, in which the added value of any item to a set\nis either 0 or 1, and aim to design truthful allocation mechanisms (without\nmoney) that maximize welfare and are fair. For the case that players have\nsubmodular valuations with dichotomous marginals, we design such a\ndeterministic truthful allocation mechanism. The allocation output by our\nmechanism is Lorenz dominating, and consequently satisfies many desired\nfairness properties, such as being envy-free up to any item (EFX), and\nmaximizing the Nash Social Welfare (NSW). We then show that our mechanism with\nrandom priorities is envy-free ex-ante, while having all the above properties\nex-post. Furthermore, we present several impossibility results precluding\nsimilar results for the larger class of XOS valuations.\n  To gauge the robustness of our positive results, we also study\n$\\epsilon$-dichotomous valuations, in which the added value of any item to a\nset is either non-positive, or in the range $[1, 1 + \\epsilon]$. We show\nseveral impossibility results in this setting, and also a positive result: for\nplayers that have additive $\\epsilon$-dichotomous valuations with sufficiently\nsmall $\\epsilon$, we design a randomized truthful mechanism with strong ex-post\nguarantees. For $\\rho = \\frac{1}{1 + \\epsilon}$, the allocations that it\nproduces generate at least a $\\rho$-fraction of the maximum welfare, and enjoy\n$\\rho$-approximations for various fairness properties, such as being envy-free\nup to one item (EF1), and giving each player at least her maximin share.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10704v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.10678v2",
    "title": "Novel Change of Measure Inequalities with Applications to PAC-Bayesian Bounds and Monte Carlo Estimation",
    "authors": [
      "Yuki Ohnishi",
      "Jean Honorio"
    ],
    "author_ids": [],
    "abstract": "We introduce several novel change of measure inequalities for two families of\ndivergences: $f$-divergences and $\\alpha$-divergences. We show how the\nvariational representation for $f$-divergences leads to novel change of measure\ninequalities. We also present a multiplicative change of measure inequality for\n$\\alpha$-divergences and a generalized version of Hammersley-Chapman-Robbins\ninequality. Finally, we present several applications of our change of measure\ninequalities, including PAC-Bayesian bounds for various classes of losses and\nnon-asymptotic intervals for Monte Carlo estimates.",
    "published_date": "2020-02-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10678v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10361v2",
    "title": "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition",
    "authors": [
      "Xiaolei Huang",
      "Linzi Xing",
      "Franck Dernoncourt",
      "Michael J. Paul"
    ],
    "author_ids": [],
    "abstract": "Existing research on fairness evaluation of document classification models\nmainly uses synthetic monolingual data without ground truth for author\ndemographic attributes. In this work, we assemble and publish a multilingual\nTwitter corpus for the task of hate speech detection with inferred four author\ndemographic factors: age, country, gender and race/ethnicity. The corpus covers\nfive languages: English, Italian, Polish, Portuguese and Spanish. We evaluate\nthe inferred demographic labels with a crowdsourcing platform, Figure Eight. To\nexamine factors that can cause biases, we take an empirical analysis of\ndemographic predictability on the English corpus. We measure the performance of\nfour popular document classifiers and evaluate the fairness and bias of the\nbaseline classifiers on the author-level demographic attributes.",
    "published_date": "2020-02-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10361v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10316v4",
    "title": "Bandit Learning with Delayed Impact of Actions",
    "authors": [
      "Wei Tang",
      "Chien-Ju Ho",
      "Yang Liu"
    ],
    "author_ids": [],
    "abstract": "We consider a stochastic multi-armed bandit (MAB) problem with delayed impact\nof actions. In our setting, actions taken in the past impact the arm rewards in\nthe subsequent future. This delayed impact of actions is prevalent in the real\nworld. For example, the capability to pay back a loan for people in a certain\nsocial group might depend on historically how frequently that group has been\napproved loan applications. If banks keep rejecting loan applications to people\nin a disadvantaged group, it could create a feedback loop and further damage\nthe chance of getting loans for people in that group. In this paper, we\nformulate this delayed and long-term impact of actions within the context of\nmulti-armed bandits. We generalize the bandit setting to encode the dependency\nof this \"bias\" due to the action history during learning. The goal is to\nmaximize the collected utilities over time while taking into account the\ndynamics created by the delayed impacts of historical actions. We propose an\nalgorithm that achieves a regret of $\\tilde{\\mathcal{O}}(KT^{2/3})$ and show a\nmatching regret lower bound of $\\Omega(KT^{2/3})$, where $K$ is the number of\narms and $T$ is the learning horizon. Our results complement the bandit\nliterature by adding techniques to deal with actions with long-term impacts and\nhave implications in designing fair algorithms.",
    "published_date": "2020-02-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10316v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10283v1",
    "title": "The Knowledge Graph Track at OAEI -- Gold Standards, Baselines, and the Golden Hammer Bias",
    "authors": [
      "Sven Hertling",
      "Heiko Paulheim"
    ],
    "author_ids": [],
    "abstract": "The Ontology Alignment Evaluation Initiative (OAEI) is an annual evaluation\nof ontology matching tools. In 2018, we have started the Knowledge Graph track,\nwhose goal is to evaluate the simultaneous matching of entities and schemas of\nlarge-scale knowledge graphs. In this paper, we discuss the design of the track\nand two different strategies of gold standard creation. We analyze results and\nexperiences obtained in first editions of the track, and, by revealing a hidden\ntask, we show that all tools submitted to the track (and probably also to other\ntracks) suffer from a bias which we name the golden hammer bias.",
    "published_date": "2020-02-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10283v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10261v4",
    "title": "Learning from Positive and Unlabeled Data with Arbitrary Positive Shift",
    "authors": [
      "Zayd Hammoudeh",
      "Daniel Lowd"
    ],
    "author_ids": [],
    "abstract": "Positive-unlabeled (PU) learning trains a binary classifier using only\npositive and unlabeled data. A common simplifying assumption is that the\npositive data is representative of the target positive class. This assumption\nrarely holds in practice due to temporal drift, domain shift, and/or\nadversarial manipulation. This paper shows that PU learning is possible even\nwith arbitrarily non-representative positive data given unlabeled data from the\nsource and target distributions. Our key insight is that only the negative\nclass's distribution need be fixed. We integrate this into two statistically\nconsistent methods to address arbitrary positive bias - one approach combines\nnegative-unlabeled learning with unlabeled-unlabeled learning while the other\nuses a novel, recursive risk estimator. Experimental results demonstrate our\nmethods' effectiveness across numerous real-world datasets and forms of\npositive bias, including disjoint positive class-conditional supports.\nAdditionally, we propose a general, simplified approach to address PU risk\nestimation overfitting.",
    "published_date": "2020-02-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10261v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10234v2",
    "title": "FR-Train: A Mutual Information-Based Approach to Fair and Robust Training",
    "authors": [
      "Yuji Roh",
      "Kangwook Lee",
      "Steven Euijong Whang",
      "Changho Suh"
    ],
    "author_ids": [],
    "abstract": "Trustworthy AI is a critical issue in machine learning where, in addition to\ntraining a model that is accurate, one must consider both fair and robust\ntraining in the presence of data bias and poisoning. However, the existing\nmodel fairness techniques mistakenly view poisoned data as an additional bias\nto be fixed, resulting in severe performance degradation. To address this\nproblem, we propose FR-Train, which holistically performs fair and robust model\ntraining. We provide a mutual information-based interpretation of an existing\nadversarial training-based fairness-only method, and apply this idea to\narchitect an additional discriminator that can identify poisoned data using a\nclean validation set and reduce its influence. In our experiments, FR-Train\nshows almost no decrease in fairness and accuracy in the presence of data\npoisoning by both mitigating the bias and defending against poisoning. We also\ndemonstrate how to construct clean validation sets using crowdsourcing, and\nrelease new benchmark datasets.",
    "published_date": "2020-02-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10234v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.10171v1",
    "title": "A Probabilistic Approach to Voting, Allocation, Matching, and Coalition Formation",
    "authors": [
      "Haris Aziz"
    ],
    "author_ids": [],
    "abstract": "Randomisation and time-sharing are some of the oldest methods to achieve\nfairness. I make a case that applying these approaches to social choice\nsettings constitutes a powerful paradigm that deserves an extensive and\nthorough examination. I discuss challenges and opportunities in applying these\napproaches to settings including voting, allocation, matching, and coalition\nformation.",
    "published_date": "2020-02-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "91A12, 68Q15",
      "F.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10171v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.09963v1",
    "title": "Mitigating Class Boundary Label Uncertainty to Reduce Both Model Bias and Variance",
    "authors": [
      "Matthew Almeida",
      "Wei Ding",
      "Scott Crouter",
      "Ping Chen"
    ],
    "author_ids": [],
    "abstract": "The study of model bias and variance with respect to decision boundaries is\ncritically important in supervised classification. There is generally a\ntradeoff between the two, as fine-tuning of the decision boundary of a\nclassification model to accommodate more boundary training samples (i.e.,\nhigher model complexity) may improve training accuracy (i.e., lower bias) but\nhurt generalization against unseen data (i.e., higher variance). By focusing on\njust classification boundary fine-tuning and model complexity, it is difficult\nto reduce both bias and variance. To overcome this dilemma, we take a different\nperspective and investigate a new approach to handle inaccuracy and uncertainty\nin the training data labels, which are inevitable in many applications where\nlabels are conceptual and labeling is performed by human annotators. The\nprocess of classification can be undermined by uncertainty in the labels of the\ntraining data; extending a boundary to accommodate an inaccurately labeled\npoint will increase both bias and variance. Our novel method can reduce both\nbias and variance by estimating the pointwise label uncertainty of the training\nset and accordingly adjusting the training sample weights such that those\nsamples with high uncertainty are weighted down and those with low uncertainty\nare weighted up. In this way, uncertain samples have a smaller contribution to\nthe objective function of the model's learning algorithm and exert less pull on\nthe decision boundary. In a real-world physical activity recognition case\nstudy, the data presents many labeling challenges, and we show that this new\napproach improves model performance and reduces model variance.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09963v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.12144v1",
    "title": "Fair Adversarial Networks",
    "authors": [
      "George Cevora"
    ],
    "author_ids": [],
    "abstract": "The influence of human judgement is ubiquitous in datasets used across the\nanalytics industry, yet humans are known to be sub-optimal decision makers\nprone to various biases. Analysing biased datasets then leads to biased\noutcomes of the analysis. Bias by protected characteristics (e.g. race) is of\nparticular interest as it may not only make the output of analytical process\nsub-optimal, but also illegal. Countering the bias by constraining the\nanalytical outcomes to be fair is problematic because A) fairness lacks a\nuniversally accepted definition, while at the same time some definitions are\nmutually exclusive, and B) the use of optimisation constraints ensuring\nfairness is incompatible with most analytical pipelines. Both problems are\nsolved by methods which remove bias from the data and returning an altered\ndataset. This approach aims to not only remove the actual bias variable (e.g.\nrace), but also alter all proxy variables (e.g. postcode) so the bias variable\nis not detectable from the rest of the data. The advantage of using this\napproach is that the definition of fairness as a lack of detectable bias in the\ndata (as opposed to the output of analysis) is universal and therefore solves\nproblem (A). Furthermore, as the data is altered to remove bias the problem (B)\ndisappears because the analytical pipelines can remain unchanged. This approach\nhas been adopted by several technical solutions. None of them, however, seems\nto be satisfactory in terms of ability to remove multivariate, non-linear and\nnon-binary biases. Therefore, in this paper I propose the concept of Fair\nAdversarial Networks as an easy-to-implement general method for removing bias\nfrom data. This paper demonstrates that Fair Adversarial Networks achieve this\naim.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12144v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09931v1",
    "title": "The Value of Big Data for Credit Scoring: Enhancing Financial Inclusion using Mobile Phone Data and Social Network Analytics",
    "authors": [
      "María Óskarsdóttir",
      "Cristián Bravo",
      "Carlos Sarraute",
      "Jan Vanthienen",
      "Bart Baesens"
    ],
    "author_ids": [],
    "abstract": "Credit scoring is without a doubt one of the oldest applications of\nanalytics. In recent years, a multitude of sophisticated classification\ntechniques have been developed to improve the statistical performance of credit\nscoring models. Instead of focusing on the techniques themselves, this paper\nleverages alternative data sources to enhance both statistical and economic\nmodel performance. The study demonstrates how including call networks, in the\ncontext of positive credit information, as a new Big Data source has added\nvalue in terms of profit by applying a profit measure and profit-based feature\nselection. A unique combination of datasets, including call-detail records,\ncredit and debit account information of customers is used to create scorecards\nfor credit card applicants. Call-detail records are used to build call networks\nand advanced social network analytics techniques are applied to propagate\ninfluence from prior defaulters throughout the network to produce influence\nscores. The results show that combining call-detail records with traditional\ndata in credit scoring models significantly increases their performance when\nmeasured in AUC. In terms of profit, the best model is the one built with only\ncalling behavior features. In addition, the calling behavior features are the\nmost predictive in other models, both in terms of statistical and economic\nperformance. The results have an impact in terms of ethical use of call-detail\nrecords, regulatory implications, financial inclusion, as well as data sharing\nand privacy.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09931v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09916v2",
    "title": "Extended formulation and valid inequalities for the multi-item inventory lot-sizing problem with supplier selection",
    "authors": [
      "Leopoldo E. Cárdenas-Barrón",
      "Rafael A. Melo",
      "Marcio C. Santos"
    ],
    "author_ids": [],
    "abstract": "We consider the multi-item inventory lot-sizing problem with supplier\nselection. The problem consists of determining an optimal purchasing plan in\norder to satisfy dynamic deterministic demands for multiple items over a finite\nplanning horizon, considering that multiple suppliers are available to purchase\nfrom. As the complexity of the problem was an open question, we show that it is\nNP-hard. We propose a facility location extended formulation for the problem\nwhich can be preprocessed based on the cost structure and describe new valid\ninequalities in the original space of variables. Furthermore, we study the\nprojection of the extended formulation into the original space and show the\nconnection between the inequalities generated by this projection and the newly\nproposed inequalities. Additionally, we present a simple and easy to implement\nyet very effective MIP (mixed integer programming) heuristic using the extended\nformulation. Besides, we introduce two new benchmark sets of instances to\nassess the performance of the approaches under different cost structures.\nComputational results show that the preprocessing approach can significantly\nreduce the size of the formulation to be solved, allowing both an increase in\nthe number of instances solved to optimality within the time limit and a\nreduction on the average time to solve them. Moreover, the described\ninequalities can improve the performance of a standard formulation for nearly\nall instance groups. They can also be used to provide strong lower bounds for\ncertain large instances for which the preprocessed facility location\nformulation fails even to provide a linear relaxation bound due to memory\nlimitations. Furthermore, the proposed MIP heuristic outperforms the heuristics\navailable in the literature as it obtains solution values which at least match\nthose reported for all instance groups, strictly improving most of them.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09916v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.10943v2",
    "title": "Data Augmentation for Personal Knowledge Base Population",
    "authors": [
      "Lingraj S Vannur",
      "Balaji Ganesan",
      "Lokesh Nagalapatti",
      "Hima Patel",
      "MN Thippeswamy"
    ],
    "author_ids": [],
    "abstract": "Cold start knowledge base population (KBP) is the problem of populating a\nknowledge base from unstructured documents. While artificial neural networks\nhave led to significant improvements in the different tasks that are part of\nKBP, the overall F1 of the end-to-end system remains quite low. This problem is\nmore acute in personal knowledge bases, which present additional challenges\nwith regard to data protection, fairness and privacy. In this work, we present\na system that uses rule based annotators and a graph neural network for missing\nlink prediction, to populate a more complete, fair and diverse knowledge base\nfrom the TACRED dataset.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.10943v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09808v4",
    "title": "My Fair Bandit: Distributed Learning of Max-Min Fairness with Multi-player Bandits",
    "authors": [
      "Ilai Bistritz",
      "Tavor Z. Baharav",
      "Amir Leshem",
      "Nicholas Bambos"
    ],
    "author_ids": [],
    "abstract": "Consider N cooperative but non-communicating players where each plays one out\nof M arms for T turns. Players have different utilities for each arm,\nrepresentable as an NxM matrix. These utilities are unknown to the players. In\neach turn players select an arm and receive a noisy observation of their\nutility for it. However, if any other players selected the same arm that turn,\nall colliding players will all receive zero utility due to the conflict. No\nother communication or coordination between the players is possible. Our goal\nis to design a distributed algorithm that learns the matching between players\nand arms that achieves max-min fairness while minimizing the regret. We present\nan algorithm and prove that it is regret optimal up to a $\\log\\log T$ factor.\nThis is the first max-min fairness multi-player bandit algorithm with (near)\norder optimal regret.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09808v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.09807v2",
    "title": "Online Stochastic Max-Weight Matching: prophet inequality for vertex and edge arrival models",
    "authors": [
      "Tomer Ezra",
      "Michal Feldman",
      "Nick Gravin",
      "Zhihao Gavin Tang"
    ],
    "author_ids": [],
    "abstract": "We provide prophet inequality algorithms for online weighted matching in\ngeneral (non-bipartite) graphs, under two well-studied arrival models, namely\nedge arrival and vertex arrival. The weight of each edge is drawn independently\nfrom an a-priori known probability distribution. Under edge arrival, the weight\nof each edge is revealed upon arrival, and the algorithm decides whether to\ninclude it in the matching or not. Under vertex arrival, the weights of all\nedges from the newly arriving vertex to all previously arrived vertices are\nrevealed, and the algorithm decides which of these edges, if any, to include in\nthe matching. To study these settings, we introduce a novel unified framework\nof batched prophet inequalities that captures online settings where elements\narrive in batches; in particular it captures matching under the two\naforementioned arrival models. Our algorithms rely on the construction of\nsuitable online contention resolution scheme (OCRS). We first extend the\nframework of OCRS to batched-OCRS, we then establish a reduction from batched\nprophet inequality to batched OCRS, and finally we construct batched OCRSs with\nselectable ratios of 0.337 and 0.5 for edge and vertex arrival models,\nrespectively. Both results improve the state of the art for the corresponding\nsettings. For the vertex arrival, our result is tight. Interestingly, a\npricing-based prophet inequality with comparable competitive ratios is unknown.",
    "published_date": "2020-02-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09807v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.09769v1",
    "title": "Optimistic bounds for multi-output prediction",
    "authors": [
      "Henry WJ Reeve",
      "Ata Kaban"
    ],
    "author_ids": [],
    "abstract": "We investigate the challenge of multi-output learning, where the goal is to\nlearn a vector-valued function based on a supervised data set. This includes a\nrange of important problems in Machine Learning including multi-target\nregression, multi-class classification and multi-label classification. We begin\nour analysis by introducing the self-bounding Lipschitz condition for\nmulti-output loss functions, which interpolates continuously between a\nclassical Lipschitz condition and a multi-dimensional analogue of a smoothness\ncondition. We then show that the self-bounding Lipschitz condition gives rise\nto optimistic bounds for multi-output learning, which are minimax optimal up to\nlogarithmic factors. The proof exploits local Rademacher complexity combined\nwith a powerful minoration inequality due to Srebro, Sridharan and Tewari. As\nan application we derive a state-of-the-art generalization bound for\nmulti-class gradient boosting.",
    "published_date": "2020-02-22T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09769v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09689v1",
    "title": "Fair and Decentralized Exchange of Digital Goods",
    "authors": [
      "Ariel Futoransky",
      "Carlos Sarraute",
      "Daniel Fernandez",
      "Matias Travizano",
      "Ariel Waissbein"
    ],
    "author_ids": [],
    "abstract": "We construct a privacy-preserving, distributed and decentralized marketplace\nwhere parties can exchange data for tokens. In this market, buyers and sellers\nmake transactions in a blockchain and interact with a third party, called\nnotary, who has the ability to vouch for the authenticity and integrity of the\ndata.\n  We introduce a protocol for the data-token exchange where neither party gains\nmore information than what it is paying for, and the exchange is fair: either\nboth parties gets the other's item or neither does. No third party involvement\nis required after setup, and no dispute resolution is needed.",
    "published_date": "2020-02-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09689v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.12759v1",
    "title": "A Novel Decision Tree for Depression Recognition in Speech",
    "authors": [
      "Zhenyu Liu",
      "Dongyu Wang",
      "Lan Zhang",
      "Bin Hu"
    ],
    "author_ids": [],
    "abstract": "Depression is a common mental disorder worldwide which causes a range of\nserious outcomes. The diagnosis of depression relies on patient-reported scales\nand psychiatrist interview which may lead to subjective bias. In recent years,\nmore and more researchers are devoted to depression recognition in speech ,\nwhich may be an effective and objective indicator. This study proposes a new\nspeech segment fusion method based on decision tree to improve the depression\nrecognition accuracy and conducts a validation on a sample of 52 subjects (23\ndepressed patients and 29 healthy controls). The recognition accuracy are 75.8%\nand 68.5% for male and female respectively on gender-dependent models. It can\nbe concluded from the data that the proposed decision tree model can improve\nthe depression classification performance.",
    "published_date": "2020-02-22T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD",
      "q-bio.QM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12759v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09621v1",
    "title": "Global Convergence and Variance-Reduced Optimization for a Class of Nonconvex-Nonconcave Minimax Problems",
    "authors": [
      "Junchi Yang",
      "Negar Kiyavash",
      "Niao He"
    ],
    "author_ids": [],
    "abstract": "Nonconvex minimax problems appear frequently in emerging machine learning\napplications, such as generative adversarial networks and adversarial learning.\nSimple algorithms such as the gradient descent ascent (GDA) are the common\npractice for solving these nonconvex games and receive lots of empirical\nsuccess. Yet, it is known that these vanilla GDA algorithms with constant step\nsize can potentially diverge even in the convex setting. In this work, we show\nthat for a subclass of nonconvex-nonconcave objectives satisfying a so-called\ntwo-sided Polyak-{\\L}ojasiewicz inequality, the alternating gradient descent\nascent (AGDA) algorithm converges globally at a linear rate and the stochastic\nAGDA achieves a sublinear rate. We further develop a variance reduced algorithm\nthat attains a provably faster rate than AGDA when the problem has the\nfinite-sum structure.",
    "published_date": "2020-02-22T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09621v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09471v1",
    "title": "Learning Fairness-aware Relational Structures",
    "authors": [
      "Yue Zhang",
      "Arti Ramesh"
    ],
    "author_ids": [],
    "abstract": "The development of fair machine learning models that effectively avert bias\nand discrimination is an important problem that has garnered attention in\nrecent years. The necessity of encoding complex relational dependencies among\nthe features and variables for competent predictions require the development of\nfair, yet expressive relational models. In this work, we introduce Fair-A3SL, a\nfairness-aware structure learning algorithm for learning relational structures,\nwhich incorporates fairness measures while learning relational graphical model\nstructures. Our approach is versatile in being able to encode a wide range of\nfairness metrics such as statistical parity difference, overestimation,\nequalized odds, and equal opportunity, including recently proposed relational\nfairness measures. While existing approaches employ the fairness measures on\npre-determined model structures post prediction, Fair-A3SL directly learns the\nstructure while optimizing for the fairness measures and hence is able to\nremove any structural bias in the model. We demonstrate the effectiveness of\nour learned model structures when compared with the state-of-the-art fairness\nmodels quantitatively and qualitatively on datasets representing three\ndifferent modeling scenarios: i) a relational dataset, ii) a recidivism\nprediction dataset widely used in studying discrimination, and iii) a\nrecommender systems dataset. Our results show that Fair-A3SL can learn fair,\nyet interpretable and expressive structures capable of making accurate\npredictions.",
    "published_date": "2020-02-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09471v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09343v3",
    "title": "Robust Optimization for Fairness with Noisy Protected Groups",
    "authors": [
      "Serena Wang",
      "Wenshuo Guo",
      "Harikrishna Narasimhan",
      "Andrew Cotter",
      "Maya Gupta",
      "Michael I. Jordan"
    ],
    "author_ids": [],
    "abstract": "Many existing fairness criteria for machine learning involve equalizing some\nmetric across protected groups such as race or gender. However, practitioners\ntrying to audit or enforce such group-based criteria can easily face the\nproblem of noisy or biased protected group information. First, we study the\nconsequences of naively relying on noisy protected group labels: we provide an\nupper bound on the fairness violations on the true groups G when the fairness\ncriteria are satisfied on noisy groups $\\hat{G}$. Second, we introduce two new\napproaches using robust optimization that, unlike the naive approach of only\nrelying on $\\hat{G}$, are guaranteed to satisfy fairness criteria on the true\nprotected groups G while minimizing a training objective. We provide\ntheoretical guarantees that one such approach converges to an optimal feasible\nsolution. Using two case studies, we show empirically that the robust\napproaches achieve better true group fairness guarantees than the naive\napproach.",
    "published_date": "2020-02-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09343v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.09054v1",
    "title": "Designing Fair AI for Managing Employees in Organizations: A Review, Critique, and Design Agenda",
    "authors": [
      "Lionel P. Robert",
      "Casey Pierce",
      "Liz Morris",
      "Sangmi Kim",
      "Rasha Alahmad"
    ],
    "author_ids": [],
    "abstract": "Organizations are rapidly deploying artificial intelligence (AI) systems to\nmanage their workers. However, AI has been found at times to be unfair to\nworkers. Unfairness toward workers has been associated with decreased worker\neffort and increased worker turnover. To avoid such problems, AI systems must\nbe designed to support fairness and redress instances of unfairness. Despite\nthe attention related to AI unfairness, there has not been a theoretical and\nsystematic approach to developing a design agenda. This paper addresses the\nissue in three ways. First, we introduce the organizational justice theory,\nthree different fairness types (distributive, procedural, interactional), and\nthe frameworks for redressing instances of unfairness (retributive justice,\nrestorative justice). Second, we review the design literature that specifically\nfocuses on issues of AI fairness in organizations. Third, we propose a design\nagenda for AI fairness in organizations that applies each of the fairness types\nto organizational scenarios. Then, the paper concludes with implications for\nfuture research.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.09054v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08911v2",
    "title": "Measuring Social Biases in Grounded Vision and Language Embeddings",
    "authors": [
      "Candace Ross",
      "Boris Katz",
      "Andrei Barbu"
    ],
    "author_ids": [],
    "abstract": "We generalize the notion of social biases from language embeddings to\ngrounded vision and language embeddings. Biases are present in grounded\nembeddings, and indeed seem to be equally or more significant than for\nungrounded embeddings. This is despite the fact that vision and language can\nsuffer from different biases, which one might hope could attenuate the biases\nin both. Multiple ways exist to generalize metrics measuring bias in word\nembeddings to this new setting. We introduce the space of generalizations\n(Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations\nanswer different yet important questions about how biases, language, and vision\ninteract. These metrics are used on a new dataset, the first for grounded bias,\ncreated by augmenting extending standard linguistic bias benchmarks with 10,228\nimages from COCO, Conceptual Captions, and Google Images. Dataset construction\nis challenging because vision datasets are themselves very biased. The presence\nof these biases in systems will begin to have real-world consequences as they\nare deployed, making carefully measuring bias and then mitigating it critical\nto building a fair society.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08911v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08681v2",
    "title": "Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice",
    "authors": [
      "Yabin Zhang",
      "Bin Deng",
      "Hui Tang",
      "Lei Zhang",
      "Kui Jia"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the formalism of unsupervised multi-class domain\nadaptation (multi-class UDA), which underlies a few recent algorithms whose\nlearning objectives are only motivated empirically. Multi-Class Scoring\nDisagreement (MCSD) divergence is presented by aggregating the absolute margin\nviolations in multi-class classification, and this proposed MCSD is able to\nfully characterize the relations between any pair of multi-class scoring\nhypotheses. By using MCSD as a measure of domain distance, we develop a new\ndomain adaptation bound for multi-class UDA; its data-dependent, probably\napproximately correct bound is also developed that naturally suggests\nadversarial learning objectives to align conditional feature distributions\nacross source and target domains. Consequently, an algorithmic framework of\nMulti-class Domain-adversarial learning Networks (McDalNets) is developed, and\nits different instantiations via surrogate learning objectives either coincide\nwith or resemble a few recently popular methods, thus (partially) underscoring\ntheir practical effectiveness. Based on our identical theory for multi-class\nUDA, we also introduce a new algorithm of Domain-Symmetric Networks (SymmNets),\nwhich is featured by a novel adversarial strategy of domain confusion and\ndiscrimination. SymmNets affords simple extensions that work equally well under\nthe problem settings of either closed set, partial, or open set UDA. We conduct\ncareful empirical studies to compare different algorithms of McDalNets and our\nnewly introduced SymmNets. Experiments verify our theoretical analysis and show\nthe efficacy of our proposed SymmNets. In addition, we have made our\nimplementation code publicly available.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08681v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08617v1",
    "title": "A minimax approach for inverse variational inequalities",
    "authors": [
      "Pablo Montiel López"
    ],
    "author_ids": [],
    "abstract": "In this work, we characterize the existence of solution for a certain\nvariational inequality by means of a classical minimax theorem. In addition, we\npropose a numerical algorithm for the solution of an inverse problem associated\nwith a variational inequality. To this end we state a collage-type result in\nthis variational framework.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08617v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.08608v4",
    "title": "FrameAxis: Characterizing Microframe Bias and Intensity with Word Embedding",
    "authors": [
      "Haewoon Kwak",
      "Jisun An",
      "Elise Jing",
      "Yong-Yeol Ahn"
    ],
    "author_ids": [],
    "abstract": "Framing is a process of emphasizing a certain aspect of an issue over the\nothers, nudging readers or listeners towards different positions on the issue\neven without making a biased argument. {Here, we propose FrameAxis, a method\nfor characterizing documents by identifying the most relevant semantic axes\n(\"microframes\") that are overrepresented in the text using word embedding. Our\nunsupervised approach can be readily applied to large datasets because it does\nnot require manual annotations. It can also provide nuanced insights by\nconsidering a rich set of semantic axes. FrameAxis is designed to\nquantitatively tease out two important dimensions of how microframes are used\nin the text. \\textit{Microframe bias} captures how biased the text is on a\ncertain microframe, and \\textit{microframe intensity} shows how actively a\ncertain microframe is used. Together, they offer a detailed characterization of\nthe text. We demonstrate that microframes with the highest bias and intensity\nwell align with sentiment, topic, and partisan spectrum by applying FrameAxis\nto multiple datasets from restaurant reviews to political news.} The existing\ndomain knowledge can be incorporated into FrameAxis {by using custom\nmicroframes and by using FrameAxis as an iterative exploratory analysis\ninstrument.} Additionally, we propose methods for explaining the results of\nFrameAxis at the level of individual words and documents. Our method may\naccelerate scalable and sophisticated computational analyses of framing across\ndisciplines.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.CY",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08608v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08563v2",
    "title": "The continuous categorical: a novel simplex-valued exponential family",
    "authors": [
      "Elliott Gordon-Rodriguez",
      "Gabriel Loaiza-Ganem",
      "John P. Cunningham"
    ],
    "author_ids": [],
    "abstract": "Simplex-valued data appear throughout statistics and machine learning, for\nexample in the context of transfer learning and compression of deep networks.\nExisting models for this class of data rely on the Dirichlet distribution or\nother related loss functions; here we show these standard choices suffer\nsystematically from a number of limitations, including bias and numerical\nissues that frustrate the use of flexible network models upstream of these\ndistributions. We resolve these limitations by introducing a novel exponential\nfamily of distributions for modeling simplex-valued data - the continuous\ncategorical, which arises as a nontrivial multivariate generalization of the\nrecently discovered continuous Bernoulli. Unlike the Dirichlet and other\ntypical choices, the continuous categorical results in a well-behaved\nprobabilistic loss function that produces unbiased estimators, while preserving\nthe mathematical simplicity of the Dirichlet. As well as exploring its\ntheoretical properties, we introduce sampling methods for this distribution\nthat are amenable to the reparameterization trick, and evaluate their\nperformance. Lastly, we demonstrate that the continuous categorical outperforms\nstandard choices empirically, across a simulation study, an applied example on\nmulti-party elections, and a neural network compression task.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08563v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08506v2",
    "title": "Causal Inference under Networked Interference and Intervention Policy Enhancement",
    "authors": [
      "Yunpu Ma",
      "Volker Tresp"
    ],
    "author_ids": [],
    "abstract": "Estimating individual treatment effects from data of randomized experiments\nis a critical task in causal inference. The Stable Unit Treatment Value\nAssumption (SUTVA) is usually made in causal inference. However, interference\ncan introduce bias when the assigned treatment on one unit affects the\npotential outcomes of the neighboring units. This interference phenomenon is\nknown as spillover effect in economics or peer effect in social science.\nUsually, in randomized experiments or observational studies with interconnected\nunits, one can only observe treatment responses under interference. Hence, how\nto estimate the superimposed causal effect and recover the individual treatment\neffect in the presence of interference becomes a challenging task in causal\ninference. In this work, we study causal effect estimation under general\nnetwork interference using GNNs, which are powerful tools for capturing the\ndependency in the graph. After deriving causal effect estimators, we further\nstudy intervention policy improvement on the graph under capacity constraint.\nWe give policy regret bounds under network interference and treatment capacity\nconstraint. Furthermore, a heuristic graph structure-dependent error bound for\nGNN-based causal estimators is provided.",
    "published_date": "2020-02-20T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08506v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08412v2",
    "title": "Weakly-supervised Multi-output Regression via Correlated Gaussian Processes",
    "authors": [
      "Seokhyun Chung",
      "Raed Al Kontar",
      "Zhenke Wu"
    ],
    "author_ids": [],
    "abstract": "Multi-output regression seeks to borrow strength and leverage commonalities\nacross different but related outputs in order to enhance learning and\nprediction accuracy. A fundamental assumption is that the output/group\nmembership labels for all observations are known. This assumption is often\nviolated in real applications. For instance, in healthcare datasets, sensitive\nattributes such as ethnicity are often missing or unreported. To this end, we\nintroduce a weakly-supervised multi-output model based on dependent Gaussian\nprocesses. Our approach is able to leverage data without complete group labels\nor possibly only prior belief on group memberships to enhance accuracy across\nall outputs. Through intensive simulations and case studies on an Insulin,\nTestosterone and Bodyfat dataset, we show that our model excels in multi-output\nsettings with missing labels, while being competitive in traditional fully\nlabeled settings. We end by highlighting the possible use of our approach in\nfair inference and sequential decision-making.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08412v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08240v2",
    "title": "Quantum statistical query learning",
    "authors": [
      "Srinivasan Arunachalam",
      "Alex B. Grilo",
      "Henry Yuen"
    ],
    "author_ids": [],
    "abstract": "We propose a learning model called the quantum statistical learning QSQ\nmodel, which extends the SQ learning model introduced by Kearns to the quantum\nsetting. Our model can be also seen as a restriction of the quantum PAC\nlearning model: here, the learner does not have direct access to quantum\nexamples, but can only obtain estimates of measurement statistics on them.\nTheoretically, this model provides a simple yet expressive setting to explore\nthe power of quantum examples in machine learning. From a practical\nperspective, since simpler operations are required, learning algorithms in the\nQSQ model are more feasible for implementation on near-term quantum devices. We\nprove a number of results about the QSQ learning model. We first show that\nparity functions, (log n)-juntas and polynomial-sized DNF formulas are\nefficiently learnable in the QSQ model, in contrast to the classical setting\nwhere these problems are provably hard. This implies that many of the\nadvantages of quantum PAC learning can be realized even in the more restricted\nquantum SQ learning model. It is well-known that weak statistical query\ndimension, denoted by WSQDIM(C), characterizes the complexity of learning a\nconcept class C in the classical SQ model. We show that log(WSQDIM(C)) is a\nlower bound on the complexity of QSQ learning, and furthermore it is tight for\ncertain concept classes C. Additionally, we show that this quantity provides\nstrong lower bounds for the small-bias quantum communication model under\nproduct distributions. Finally, we introduce the notion of private quantum PAC\nlearning, in which a quantum PAC learner is required to be differentially\nprivate. We show that learnability in the QSQ model implies learnability in the\nquantum private PAC model. Additionally, we show that in the private PAC\nlearning setting, the classical and quantum sample complexities are equal, up\nto constant factors.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08240v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08159v4",
    "title": "Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints",
    "authors": [
      "Robin Vogel",
      "Aurélien Bellet",
      "Stephan Clémençon"
    ],
    "author_ids": [],
    "abstract": "Many applications of AI involve scoring individuals using a learned function\nof their attributes. These predictive risk scores are then used to take\ndecisions based on whether the score exceeds a certain threshold, which may\nvary depending on the context. The level of delegation granted to such systems\nin critical applications like credit lending and medical diagnosis will heavily\ndepend on how questions of fairness can be answered. In this paper, we study\nfairness for the problem of learning scoring functions from binary labeled\ndata, a classic learning task known as bipartite ranking. We argue that the\nfunctional nature of the ROC curve, the gold standard measure of ranking\naccuracy in this context, leads to several ways of formulating fairness\nconstraints. We introduce general families of fairness definitions based on the\nAUC and on ROC curves, and show that our ROC-based constraints can be\ninstantiated such that classifiers obtained by thresholding the scoring\nfunction satisfy classification fairness for a desired range of thresholds. We\nestablish generalization bounds for scoring functions learned under such\nconstraints, design practical learning algorithms and show the relevance our\napproach with numerical experiments on real and synthetic data.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08159v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08126v1",
    "title": "Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition",
    "authors": [
      "Shuai Zhang",
      "Jiangyan Yi",
      "Zhengkun Tian",
      "Jianhua Tao",
      "Ye Bai"
    ],
    "author_ids": [],
    "abstract": "Recently, language identity information has been utilized to improve the\nperformance of end-to-end code-switching (CS) speech recognition. However,\nprevious works use an additional language identification (LID) model as an\nauxiliary module, which causes the system complex. In this work, we propose an\nimproved recurrent neural network transducer (RNN-T) model with language bias\nto alleviate the problem. We use the language identities to bias the model to\npredict the CS points. This promotes the model to learn the language identity\ninformation directly from transcription, and no additional LID model is needed.\nWe evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our\nRNN-T baseline, the proposed method can achieve 16.2% and 12.9% relative error\nreduction on two test sets, respectively.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08126v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08041v1",
    "title": "Enlarging Discriminative Power by Adding an Extra Class in Unsupervised Domain Adaptation",
    "authors": [
      "Hai H. Tran",
      "Sumyeong Ahn",
      "Taeyoung Lee",
      "Yung Yi"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the problem of unsupervised domain adaptation that\naims at obtaining a prediction model for the target domain using labeled data\nfrom the source domain and unlabeled data from the target domain. There exists\nan array of recent research based on the idea of extracting features that are\nnot only invariant for both domains but also provide high discriminative power\nfor the target domain. In this paper, we propose an idea of empowering the\ndiscriminativeness: Adding a new, artificial class and training the model on\nthe data together with the GAN-generated samples of the new class. The trained\nmodel based on the new class samples is capable of extracting the features that\nare more discriminative by repositioning data of current classes in the target\ndomain and therefore drawing the decision boundaries more effectively. Our idea\nis highly generic so that it is compatible with many existing methods such as\nDANN, VADA, and DIRT-T. We conduct various experiments for the standard data\ncommonly used for the evaluation of unsupervised domain adaptations and\ndemonstrate that our algorithm achieves the SOTA performance for many\nscenarios.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07989v2",
    "title": "Implicit bias with Ritz-Galerkin method in understanding deep learning for solving PDEs",
    "authors": [
      "Jihong Wang",
      "Zhi-Qin John Xu",
      "Jiwei Zhang",
      "Yaoyu Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper aims at studying the difference between Ritz-Galerkin (R-G) method\nand deep neural network (DNN) method in solving partial differential equations\n(PDEs) to better understand deep learning. To this end, we consider solving a\nparticular Poisson problem, where the information of the right-hand side of the\nequation f is only available at n sample points, that is, f is known at finite\nsample points. Through both theoretical and numerical studies, we show that\nsolution of the R-G method converges to a piecewise linear function for the one\ndimensional (1D) problem or functions of lower regularity for high dimensional\nproblems. With the same setting, DNNs however learn a relative smooth solution\nregardless of the dimension, this is, DNNs implicitly bias towards functions\nwith more low-frequency components among all functions that can fit the\nequation at available data points. This bias is explained by the recent study\nof frequency principle (Xu et al., (2019) [17] and Zhang et al., (2019) [11,\n19]). In addition to the similarity between the traditional numerical methods\nand DNNs in the approximation perspective, our work shows that the implicit\nbias in the learning process, which is different from traditional numerical\nmethods, could help better understand the characteristics of DNNs.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "35Q68, 65N30, 65N35"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07989v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.08774v1",
    "title": "Propose, Test, Release: Differentially private estimation with high probability",
    "authors": [
      "Victor-Emmanuel Brunel",
      "Marco Avella-Medina"
    ],
    "author_ids": [],
    "abstract": "We derive concentration inequalities for differentially private median and\nmean estimators building on the \"Propose, Test, Release\" (PTR) mechanism\nintroduced by Dwork and Lei (2009). We introduce a new general version of the\nPTR mechanism that allows us to derive high probability error bounds for\ndifferentially private estimators. Our algorithms provide the first statistical\nguarantees for differentially private estimation of the median and mean without\nany boundedness assumptions on the data, and without assuming that the target\npopulation parameter lies in some known bounded interval. Our procedures do not\nrely on any truncation of the data and provide the first sub-Gaussian high\nprobability bounds for differentially private median and mean estimation, for\npossibly heavy tailed random variables.",
    "published_date": "2020-02-19T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08774v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07927v2",
    "title": "Studying the Effects of Cognitive Biases in Evaluation of Conversational Agents",
    "authors": [
      "Sashank Santhanam",
      "Alireza Karduni",
      "Samira Shaikh"
    ],
    "author_ids": [],
    "abstract": "Humans quite frequently interact with conversational agents. The rapid\nadvancement in generative language modeling through neural networks has helped\nadvance the creation of intelligent conversational agents. Researchers\ntypically evaluate the output of their models through crowdsourced judgments,\nbut there are no established best practices for conducting such studies.\nMoreover, it is unclear if cognitive biases in decision-making are affecting\ncrowdsourced workers' judgments when they undertake these tasks. To\ninvestigate, we conducted a between-subjects study with 77 crowdsourced workers\nto understand the role of cognitive biases, specifically anchoring bias, when\nhumans are asked to evaluate the output of conversational agents. Our results\nprovide insight into how best to evaluate conversational agents. We find\nincreased consistency in ratings across two experimental conditions may be a\nresult of anchoring bias. We also determine that external factors such as time\nand prior experience in similar tasks have effects on inter-rater consistency.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07927v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07892v2",
    "title": "Fair Clustering with Multiple Colors",
    "authors": [
      "Matteo Böhm",
      "Adriano Fazzone",
      "Stefano Leonardi",
      "Chris Schwiegelshohn"
    ],
    "author_ids": [],
    "abstract": "A fair clustering instance is given a data set $A$ in which every point is\nassigned some color. Colors correspond to various protected attributes such as\nsex, ethnicity, or age. A fair clustering is an instance where membership of\npoints in a cluster is uncorrelated with the coloring of the points.\n  Of particular interest is the case where all colors are equally represented.\nIf we have exactly two colors, Chierrichetti, Kumar, Lattanzi and Vassilvitskii\n(NIPS 2017) showed that various $k$-clustering objectives admit a constant\nfactor approximation. Since then, a number of follow up work has attempted to\nextend this result to a multi-color case, though so far, the only known results\neither result in no-constant factor approximation, apply only to special\nclustering objectives such as $k$-center, yield bicrititeria approximations, or\nrequire $k$ to be constant.\n  In this paper, we present a simple reduction from unconstrained\n$k$-clustering to fair $k$-clustering for a large range of clustering\nobjectives including $k$-median, $k$-means, and $k$-center. The reduction loses\nonly a constant factor in the approximation guarantee, marking the first true\nconstant factor approximation for many of these problems.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07892v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.07789v1",
    "title": "Elitism in Mathematics and Inequality",
    "authors": [
      "Ho-Chun Herbert Chang",
      "Feng Fu"
    ],
    "author_ids": [],
    "abstract": "The Fields Medal, often referred as the Nobel Prize of mathematics, is\nawarded to no more than four mathematician under the age of 40, every four\nyears. In recent years, its conferral has come under scrutiny of math\nhistorians, for rewarding the existing elite rather than its original goal of\nelevating mathematicians from under-represented communities. Prior studies of\nelitism focus on citational practices and sub-fields; the structural forces\nthat prevent equitable access remain unclear. Here we show the flow of elite\nmathematicians between countries and lingo-ethnic identity, using network\nanalysis and natural language processing on 240,000 mathematicians and their\nadvisor-advisee relationships. We found that the Fields Medal helped integrate\nJapan after WWII, through analysis of the elite circle formed around Fields\nMedalists. Arabic, African, and East Asian identities remain under-represented\nat the elite level. Through analysis of inflow and outflow, we rebuts the myth\nthat minority communities create their own barriers to entry. Our results\ndemonstrate concerted efforts by international academic committees, such as\nprize-giving, are a powerful force to give equal access. We anticipate our\nmethodology of academic genealogical analysis can serve as a useful diagnostic\nfor equality within academic fields.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "math.HO",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07789v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07786v1",
    "title": "Investigating Potential Factors Associated with Gender Discrimination in Collaborative Recommender Systems",
    "authors": [
      "Masoud Mansoury",
      "Himan Abdollahpouri",
      "Jessie Smith",
      "Arman Dehpanah",
      "Mykola Pechenizkiy",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "The proliferation of personalized recommendation technologies has raised\nconcerns about discrepancies in their recommendation performance across\ndifferent genders, age groups, and racial or ethnic populations. This varying\ndegree of performance could impact users' trust in the system and may pose\nlegal and ethical issues in domains where fairness and equity are critical\nconcerns, like job recommendation. In this paper, we investigate several\npotential factors that could be associated with discriminatory performance of a\nrecommendation algorithm for women versus men. We specifically study several\ncharacteristics of user profiles and analyze their possible associations with\ndisparate behavior of the system towards different genders. These\ncharacteristics include the anomaly in rating behavior, the entropy of users'\nprofiles, and the users' profile size. Our experimental results on a public\ndataset using four recommendation algorithms show that, based on all the three\nmentioned factors, women get less accurate recommendations than men indicating\nan unfair nature of recommendation algorithms across genders.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07786v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.07738v4",
    "title": "Individual Fairness Revisited: Transferring Techniques from Adversarial Robustness",
    "authors": [
      "Samuel Yeom",
      "Matt Fredrikson"
    ],
    "author_ids": [],
    "abstract": "We turn the definition of individual fairness on its head---rather than\nascertaining the fairness of a model given a predetermined metric, we find a\nmetric for a given model that satisfies individual fairness. This can\nfacilitate the discussion on the fairness of a model, addressing the issue that\nit may be difficult to specify a priori a suitable metric. Our contributions\nare twofold: First, we introduce the definition of a minimal metric and\ncharacterize the behavior of models in terms of minimal metrics. Second, for\nmore complicated models, we apply the mechanism of randomized smoothing from\nadversarial robustness to make them individually fair under a given weighted\n$L^p$ metric. Our experiments show that adapting the minimal metrics of linear\nmodels to more complicated neural networks can lead to meaningful and\ninterpretable fairness guarantees at little cost to utility.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07738v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07734v1",
    "title": "Artificial Intelligent Ethics in the Digital Era: an Engineering Ethical Framework Proposal",
    "authors": [
      "Esteban García-Cuesta"
    ],
    "author_ids": [],
    "abstract": "Nowadays technology is being adopted on every aspect of our lives and it is\none of most important transformation driver in industry. Moreover, many of the\nsystems and digital services that we use daily rely on artificial intelligent\ntechnology capable of modeling social or individual behaviors that in turns\nalso modify personal decisions and actions. In this paper, we briefly discuss,\nfrom a technological perspective, a number of critical issues including the\npurpose of promoting trust and ensure social benefit by the proper use of\nArtificial Intelligent Systems. To achieve this goal we propose a generic\nethical technological framework as a first attempt to define a common context\ntowards developing real engineering ethical by design. We hope that this\ninitial proposal to be useful for early adopters and especially for\nstandardization teams.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07734v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.07147v1",
    "title": "Fair Prediction with Endogenous Behavior",
    "authors": [
      "Christopher Jung",
      "Sampath Kannan",
      "Changhwa Lee",
      "Mallesh M. Pai",
      "Aaron Roth",
      "Rakesh Vohra"
    ],
    "author_ids": [],
    "abstract": "There is increasing regulatory interest in whether machine learning\nalgorithms deployed in consequential domains (e.g. in criminal justice) treat\ndifferent demographic groups \"fairly.\" However, there are several proposed\nnotions of fairness, typically mutually incompatible. Using criminal justice as\nan example, we study a model in which society chooses an incarceration rule.\nAgents of different demographic groups differ in their outside options (e.g.\nopportunity for legal employment) and decide whether to commit crimes. We show\nthat equalizing type I and type II errors across groups is consistent with the\ngoal of minimizing the overall crime rate; other popular notions of fairness\nare not.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "econ.EM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07147v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07676v3",
    "title": "A Possibility in Algorithmic Fairness: Can Calibration and Equal Error Rates Be Reconciled?",
    "authors": [
      "Claire Lazar Reich",
      "Suhas Vijaykumar"
    ],
    "author_ids": [],
    "abstract": "Decision makers increasingly rely on algorithmic risk scores to determine\naccess to binary treatments including bail, loans, and medical interventions.\nIn these settings, we reconcile two fairness criteria that were previously\nshown to be in conflict: calibration and error rate equality. In particular, we\nderive necessary and sufficient conditions for the existence of calibrated\nscores that yield classifications achieving equal error rates at any given\ngroup-blind threshold. We then present an algorithm that searches for the most\naccurate score subject to both calibration and minimal error rate disparity.\nApplied to the COMPAS criminal risk assessment tool, we show that our method\ncan eliminate error disparities while maintaining calibration. In a separate\napplication to credit lending, we compare our procedure to the omission of\nsensitive features and show that it raises both profit and the probability that\ncreditworthy individuals receive loans.",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07676v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07394v1",
    "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning",
    "authors": [
      "Junnan Li",
      "Richard Socher",
      "Steven C. H. Hoi"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have\nbeen devoted to reducing the annotation cost when learning with deep networks.\nTwo prominent directions include learning with noisy labels and semi-supervised\nlearning by exploiting unlabeled data. In this work, we propose DivideMix, a\nnovel framework for learning with noisy labels by leveraging semi-supervised\nlearning techniques. In particular, DivideMix models the per-sample loss\ndistribution with a mixture model to dynamically divide the training data into\na labeled set with clean samples and an unlabeled set with noisy samples, and\ntrains the model on both the labeled and unlabeled data in a semi-supervised\nmanner. To avoid confirmation bias, we simultaneously train two diverged\nnetworks where each network uses the dataset division from the other network.\nDuring the semi-supervised training phase, we improve the MixMatch strategy by\nperforming label co-refinement and label co-guessing on labeled and unlabeled\nsamples, respectively. Experiments on multiple benchmark datasets demonstrate\nsubstantial improvements over state-of-the-art methods. Code is available at\nhttps://github.com/LiJunnan1992/DivideMix .",
    "published_date": "2020-02-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07394v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.07007v1",
    "title": "The Synthesizability of Molecules Proposed by Generative Models",
    "authors": [
      "Wenhao Gao",
      "Connor W. Coley"
    ],
    "author_ids": [],
    "abstract": "The discovery of functional molecules is an expensive and time-consuming\nprocess, exemplified by the rising costs of small molecule therapeutic\ndiscovery. One class of techniques of growing interest for early-stage drug\ndiscovery is de novo molecular generation and optimization, catalyzed by the\ndevelopment of new deep learning approaches. These techniques can suggest novel\nmolecular structures intended to maximize a multi-objective function, e.g.,\nsuitability as a therapeutic against a particular target, without relying on\nbrute-force exploration of a chemical space. However, the utility of these\napproaches is stymied by ignorance of synthesizability. To highlight the\nseverity of this issue, we use a data-driven computer-aided synthesis planning\nprogram to quantify how often molecules proposed by state-of-the-art generative\nmodels cannot be readily synthesized. Our analysis demonstrates that there are\nseveral tasks for which these models generate unrealistic molecular structures\ndespite performing well on popular quantitative benchmarks. Synthetic\ncomplexity heuristics can successfully bias generation toward\nsynthetically-tractable chemical space, although doing so necessarily detracts\nfrom the primary objective. This analysis suggests that to improve the utility\nof these models in real discovery workflows, new algorithm development is\nwarranted.",
    "published_date": "2020-02-17T00:00:00",
    "year": 2020,
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07007v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06856v5",
    "title": "Data and Model Dependencies of Membership Inference Attack",
    "authors": [
      "Shakila Mahjabin Tonni",
      "Dinusha Vatsalan",
      "Farhad Farokhi",
      "Dali Kaafar",
      "Zhigang Lu",
      "Gioacchino Tangari"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) models have been shown to be vulnerable to Membership\nInference Attacks (MIA), which infer the membership of a given data point in\nthe target dataset by observing the prediction output of the ML model. While\nthe key factors for the success of MIA have not yet been fully understood,\nexisting defense mechanisms such as using L2 regularization\n\\cite{10shokri2017membership} and dropout layers \\cite{salem2018ml} take only\nthe model's overfitting property into consideration. In this paper, we provide\nan empirical analysis of the impact of both the data and ML model properties on\nthe vulnerability of ML techniques to MIA. Our results reveal the relationship\nbetween MIA accuracy and properties of the dataset and training model in use.\nIn particular, we show that the size of shadow dataset, the class and feature\nbalance and the entropy of the target dataset, the configurations and fairness\nof the training model are the most influential factors. Based on those\nexperimental findings, we conclude that along with model overfitting, multiple\nproperties jointly contribute to MIA success instead of any single property.\nBuilding on our experimental findings, we propose using those data and model\nproperties as regularizers to protect ML models against MIA. Our results show\nthat the proposed defense mechanisms can reduce the MIA accuracy by up to 25\\%\nwithout sacrificing the ML model prediction utility.",
    "published_date": "2020-02-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06856v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06742v2",
    "title": "Individual Fairness for $k$-Clustering",
    "authors": [
      "Sepideh Mahabadi",
      "Ali Vakilian"
    ],
    "author_ids": [],
    "abstract": "We give a local search based algorithm for $k$-median and $k$-means (and more\ngenerally for any $k$-clustering with $\\ell_p$ norm cost function) from the\nperspective of individual fairness. More precisely, for a point $x$ in a point\nset $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of\nradius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively,\nif a set of $k$ random points are chosen from $P$ as centers, every point $x\\in\nP$ expects to have a center within radius $r(x)$. An individually fair\nclustering provides such a guarantee for every point $x\\in P$. This notion of\nfairness was introduced in [Jung et al., 2019] where they showed how to get an\napproximately feasible $k$-clustering with respect to this fairness condition.\n  In this work, we show how to get a bicriteria approximation for fair\n$k$-clustering: The $k$-median ($k$-means) cost of our solution is within a\nconstant factor of the cost of an optimal fair $k$-clustering, and our solution\napproximately satisfies the fairness condition (also within a constant factor).\nFurther, we complement our theoretical bounds with empirical evaluation.",
    "published_date": "2020-02-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06742v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06644v1",
    "title": "Towards Detection of Subjective Bias using Contextualized Word Embeddings",
    "authors": [
      "Tanvi Dadu",
      "Kartikey Pant",
      "Radhika Mamidi"
    ],
    "author_ids": [],
    "abstract": "Subjective bias detection is critical for applications like propaganda\ndetection, content recommendation, sentiment analysis, and bias neutralization.\nThis bias is introduced in natural language via inflammatory words and phrases,\ncasting doubt over facts, and presupposing the truth. In this work, we perform\ncomprehensive experiments for detecting subjective bias using BERT-based models\non the Wiki Neutrality Corpus(WNC). The dataset consists of $360k$ labeled\ninstances, from Wikipedia edits that remove various instances of the bias. We\nfurther propose BERT-based ensembles that outperform state-of-the-art methods\nlike $BERT_{large}$ by a margin of $5.6$ F1 score.",
    "published_date": "2020-02-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06644v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06592v2",
    "title": "Pipeline Interventions",
    "authors": [
      "Eshwar Ram Arunachaleswaran",
      "Sampath Kannan",
      "Aaron Roth",
      "Juba Ziani"
    ],
    "author_ids": [],
    "abstract": "We introduce the \\emph{pipeline intervention} problem, defined by a layered\ndirected acyclic graph and a set of stochastic matrices governing transitions\nbetween successive layers. The graph is a stylized model for how people from\ndifferent populations are presented opportunities, eventually leading to some\nreward. In our model, individuals are born into an initial position (i.e. some\nnode in the first layer of the graph) according to a fixed probability\ndistribution, and then stochastically progress through the graph according to\nthe transition matrices, until they reach a node in the final layer of the\ngraph; each node in the final layer has a \\emph{reward} associated with it. The\npipeline intervention problem asks how to best make costly changes to the\ntransition matrices governing people's stochastic transitions through the\ngraph, subject to a budget constraint. We consider two objectives: social\nwelfare maximization, and a fairness-motivated maximin objective that seeks to\nmaximize the value to the population (starting node) with the \\emph{least}\nexpected value. We consider two variants of the maximin objective that turn out\nto be distinct, depending on whether we demand a deterministic solution or\nallow randomization. For each objective, we give an efficient approximation\nalgorithm (an additive FPTAS) for constant width networks. We also tightly\ncharacterize the \"price of fairness\" in our setting: the ratio between the\nhighest achievable social welfare and the highest social welfare consistent\nwith a maximin optimal solution. Finally we show that for polynomial width\nnetworks, even approximating the maximin objective to any constant factor is NP\nhard, even for networks with constant depth. This shows that the restriction on\nthe width in our positive results is essential.",
    "published_date": "2020-02-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06592v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06501v1",
    "title": "Convex Fairness Constrained Model Using Causal Effect Estimators",
    "authors": [
      "Hikaru Ogura",
      "Akiko Takeda"
    ],
    "author_ids": [],
    "abstract": "Recent years have seen much research on fairness in machine learning. Here,\nmean difference (MD) or demographic parity is one of the most popular measures\nof fairness. However, MD quantifies not only discrimination but also\nexplanatory bias which is the difference of outcomes justified by explanatory\nfeatures. In this paper, we devise novel models, called FairCEEs, which remove\ndiscrimination while keeping explanatory bias. The models are based on\nestimators of causal effect utilizing propensity score analysis. We prove that\nFairCEEs with the squared loss theoretically outperform a naive MD constraint\nmodel. We provide an efficient algorithm for solving FairCEEs in regression and\nbinary classification tasks. In our experiment on synthetic and real-world data\nin these two tasks, FairCEEs outperformed an existing model that considers\nexplanatory bias in specific cases.",
    "published_date": "2020-02-16T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06501v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06487v2",
    "title": "Maxmin Q-learning: Controlling the Estimation Bias of Q-learning",
    "authors": [
      "Qingfeng Lan",
      "Yangchen Pan",
      "Alona Fyshe",
      "Martha White"
    ],
    "author_ids": [],
    "abstract": "Q-learning suffers from overestimation bias, because it approximates the\nmaximum action value using the maximum estimated action value. Algorithms have\nbeen proposed to reduce overestimation bias, but we lack an understanding of\nhow bias interacts with performance, and the extent to which existing\nalgorithms mitigate bias. In this paper, we 1) highlight that the effect of\noverestimation bias on learning efficiency is environment-dependent; 2) propose\na generalization of Q-learning, called \\emph{Maxmin Q-learning}, which provides\na parameter to flexibly control bias; 3) show theoretically that there exists a\nparameter choice for Maxmin Q-learning that leads to unbiased estimation with a\nlower approximation variance than Q-learning; and 4) prove the convergence of\nour algorithm in the tabular case, as well as convergence of several previous\nQ-learning variants, using a novel Generalized Q-learning framework. We\nempirically verify that our algorithm better controls estimation bias in toy\nenvironments, and that it achieves superior performance on several benchmark\nproblems.",
    "published_date": "2020-02-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06487v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06483v4",
    "title": "Face Recognition: Too Bias, or Not Too Bias?",
    "authors": [
      "Joseph P Robinson",
      "Gennady Livitz",
      "Yann Henon",
      "Can Qin",
      "Yun Fu",
      "Samson Timoner"
    ],
    "author_ids": [],
    "abstract": "We reveal critical insights into problems of bias in state-of-the-art facial\nrecognition (FR) systems using a novel Balanced Faces In the Wild (BFW)\ndataset: data balanced for gender and ethnic groups. We show variations in the\noptimal scoring threshold for face-pairs across different subgroups. Thus, the\nconventional approach of learning a global threshold for all pairs resulting in\nperformance gaps among subgroups. By learning subgroup-specific thresholds, we\nnot only mitigate problems in performance gaps but also show a notable boost in\nthe overall performance. Furthermore, we do a human evaluation to measure the\nbias in humans, which supports the hypothesis that such a bias exists in human\nperception. For the BFW database, source code, and more, visit\ngithub.com/visionjo/facerec-bias-bfw.",
    "published_date": "2020-02-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06483v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11619v1",
    "title": "Election in India: Polling in National Financial Switch",
    "authors": [
      "Subhankar Mishra"
    ],
    "author_ids": [],
    "abstract": "Indian voters from Kashmir to Kanyakumari select their representatives to\nform their parliament by going to polls. India's election is one of the largest\ndemocratic exercise in the world history. About 850 million eligible voters\ndetermine which political party or alliance will form the government and in\nturn, will serve as prime minister. Given the electoral rules of placing a\npolling place within 2 kilometers of every habitation, it comes as no surprise\nthat is indeed a humongous task for the Election Commission of India (ECI). It\nsends around 11 million election workers through tough terrains to reach the\nlast mile. This exercise also comes as ever growing expenditure for the ECI.\nThis paper proposes the use of Automated Teller Machines (ATM) and Point Of\nSale (POS) machines to be used to cover as much as urban, rural and semi-urban\nplaces possible given the wide network of National Financial Switch (NFS) and\nincrease in connectivity through Digital India initiative. This would add to\nthe use of the existing infrastructure to accommodate a free, fair and\ntransparent election.",
    "published_date": "2020-02-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11619v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.06298v1",
    "title": "Extreme Classification via Adversarial Softmax Approximation",
    "authors": [
      "Robert Bamler",
      "Stephan Mandt"
    ],
    "author_ids": [],
    "abstract": "Training a classifier over a large number of classes, known as 'extreme\nclassification', has become a topic of major interest with applications in\ntechnology, science, and e-commerce. Traditional softmax regression induces a\ngradient cost proportional to the number of classes $C$, which often is\nprohibitively expensive. A popular scalable softmax approximation relies on\nuniform negative sampling, which suffers from slow convergence due a poor\nsignal-to-noise ratio. In this paper, we propose a simple training method for\ndrastically enhancing the gradient signal by drawing negative samples from an\nadversarial model that mimics the data distribution. Our contributions are\nthree-fold: (i) an adversarial sampling mechanism that produces negative\nsamples at a cost only logarithmic in $C$, thus still resulting in cheap\ngradient updates; (ii) a mathematical proof that this adversarial sampling\nminimizes the gradient variance while any bias due to non-uniform sampling can\nbe removed; (iii) experimental results on large scale data sets that show a\nreduction of the training time by an order of magnitude relative to several\ncompetitive baselines.",
    "published_date": "2020-02-15T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06298v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2003.00827v2",
    "title": "CheXclusion: Fairness gaps in deep chest X-ray classifiers",
    "authors": [
      "Laleh Seyyed-Kalantari",
      "Guanxiong Liu",
      "Matthew McDermott",
      "Irene Y. Chen",
      "Marzyeh Ghassemi"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems have received much attention recently for their\nability to achieve expert-level performance on clinical tasks, particularly in\nmedical imaging. Here, we examine the extent to which state-of-the-art deep\nlearning classifiers trained to yield diagnostic labels from X-ray images are\nbiased with respect to protected attributes. We train convolution neural\nnetworks to predict 14 diagnostic labels in 3 prominent public chest X-ray\ndatasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation\nof all those datasets. We evaluate the TPR disparity -- the difference in true\npositive rates (TPR) -- among different protected attributes such as patient\nsex, age, race, and insurance type as a proxy for socioeconomic status. We\ndemonstrate that TPR disparities exist in the state-of-the-art classifiers in\nall datasets, for all clinical tasks, and all subgroups. A multi-source dataset\ncorresponds to the smallest disparities, suggesting one way to reduce bias. We\nfind that TPR disparities are not significantly correlated with a subgroup's\nproportional disease burden. As clinical models move from papers to products,\nwe encourage clinical decision makers to carefully audit for algorithmic\ndisparities prior to deployment. Our code can be found at,\nhttps://github.com/LalehSeyyed/CheXclusion",
    "published_date": "2020-02-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00827v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.11621v1",
    "title": "Algorithms for Fair Team Formation in Online Labour Marketplaces",
    "authors": [
      "Giorgio Barnabò",
      "Adriano Fazzone",
      "Stefano Leonardi",
      "Chris Schwiegelshohn"
    ],
    "author_ids": [],
    "abstract": "As freelancing work keeps on growing almost everywhere due to a sharp\ndecrease in communication costs and to the widespread of Internet-based labour\nmarketplaces (e.g., guru.com, feelancer.com, mturk.com, upwork.com), many\nresearchers and practitioners have started exploring the benefits of\noutsourcing and crowdsourcing. Since employers often use these platforms to\nfind a group of workers to complete a specific task, researchers have focused\ntheir efforts on the study of team formation and matching algorithms and on the\ndesign of effective incentive schemes. Nevertheless, just recently, several\nconcerns have been raised on possibly unfair biases introduced through the\nalgorithms used to carry out these selection and matching procedures. For this\nreason, researchers have started studying the fairness of algorithms related to\nthese online marketplaces, looking for intelligent ways to overcome the\nalgorithmic bias that frequently arises. Broadly speaking, the aim is to\nguarantee that, for example, the process of hiring workers through the use of\nmachine learning and algorithmic data analysis tools does not discriminate,\neven unintentionally, on grounds of nationality or gender. In this short paper,\nwe define the Fair Team Formation problem in the following way: given an online\nlabour marketplace where each worker possesses one or more skills, and where\nall workers are divided into two or more not overlapping classes (for examples,\nmen and women), we want to design an algorithm that is able to find a team with\nall the skills needed to complete a given task, and that has the same number of\npeople from all classes. We provide inapproximability results for the Fair Team\nFormation problem together with four algorithms for the problem itself. We also\ntested the effectiveness of our algorithmic solutions by performing experiments\nusing real data from an online labor marketplace.",
    "published_date": "2020-02-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.11621v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06200v1",
    "title": "Fast Fair Regression via Efficient Approximations of Mutual Information",
    "authors": [
      "Daniel Steinberg",
      "Alistair Reid",
      "Simon O'Callaghan",
      "Finnian Lattimore",
      "Lachlan McCalman",
      "Tiberio Caetano"
    ],
    "author_ids": [],
    "abstract": "Most work in algorithmic fairness to date has focused on discrete outcomes,\nsuch as deciding whether to grant someone a loan or not. In these\nclassification settings, group fairness criteria such as independence,\nseparation and sufficiency can be measured directly by comparing rates of\noutcomes between subpopulations. Many important problems however require the\nprediction of a real-valued outcome, such as a risk score or insurance premium.\nIn such regression settings, measuring group fairness criteria is\ncomputationally challenging, as it requires estimating information-theoretic\ndivergences between conditional probability density functions. This paper\nintroduces fast approximations of the independence, separation and sufficiency\ngroup fairness criteria for regression models from their (conditional) mutual\ninformation definitions, and uses such approximations as regularisers to\nenforce fairness within a regularised risk minimisation framework. Experiments\nin real-world datasets indicate that in spite of its superior computational\nefficiency our algorithm still displays state-of-the-art accuracy/fairness\ntradeoffs.",
    "published_date": "2020-02-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06200v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05819v1",
    "title": "Fairness through Experimentation: Inequality in A/B testing as an approach to responsible design",
    "authors": [
      "Guillaume Saint-Jacques",
      "Amir Sepehri",
      "Nicole Li",
      "Igor Perisic"
    ],
    "author_ids": [],
    "abstract": "As technology continues to advance, there is increasing concern about\nindividuals being left behind. Many businesses are striving to adopt\nresponsible design practices and avoid any unintended consequences of their\nproducts and services, ranging from privacy vulnerabilities to algorithmic\nbias. We propose a novel approach to fairness and inclusiveness based on\nexperimentation. We use experimentation because we want to assess not only the\nintrinsic properties of products and algorithms but also their impact on\npeople. We do this by introducing an inequality approach to A/B testing,\nleveraging the Atkinson index from the economics literature. We show how to\nperform causal inference over this inequality measure. We also introduce the\nconcept of site-wide inequality impact, which captures the inclusiveness impact\nof targeting specific subpopulations for experiments, and show how to conduct\nstatistical inference on this impact. We provide real examples from LinkedIn,\nas well as an open-source, highly scalable implementation of the computation of\nthe Atkinson index and its variance in Spark/Scala. We also provide over a\nyear's worth of learnings -- gathered by deploying our method at scale and\nanalyzing thousands of experiments -- on which areas and which kinds of product\ninnovations seem to inherently foster fairness through inclusiveness.",
    "published_date": "2020-02-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "econ.EM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05819v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.05714v1",
    "title": "Automatically Discovering and Learning New Visual Categories with Ranking Statistics",
    "authors": [
      "Kai Han",
      "Sylvestre-Alvise Rebuffi",
      "Sebastien Ehrhardt",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ],
    "author_ids": [],
    "abstract": "We tackle the problem of discovering novel classes in an image collection\ngiven labelled examples of other classes. This setting is similar to\nsemi-supervised learning, but significantly harder because there are no\nlabelled examples for the new classes. The challenge, then, is to leverage the\ninformation contained in the labelled images in order to learn a\ngeneral-purpose clustering model and use the latter to identify the new classes\nin the unlabelled data. In this work we address this problem by combining three\nideas: (1) we suggest that the common approach of bootstrapping an image\nrepresentation using the labeled data only introduces an unwanted bias, and\nthat this can be avoided by using self-supervised learning to train the\nrepresentation from scratch on the union of labelled and unlabelled data; (2)\nwe use rank statistics to transfer the model's knowledge of the labelled\nclasses to the problem of clustering the unlabelled images; and, (3) we train\nthe data representation by optimizing a joint objective function on the\nlabelled and unlabelled subsets of the data, improving both the supervised\nclassification of the labelled data, and the clustering of the unlabelled data.\nWe evaluate our approach on standard classification benchmarks and outperform\ncurrent methods for novel category discovery by a significant margin.",
    "published_date": "2020-02-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05714v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05636v3",
    "title": "A Set of Distinct Facial Traits Learned by Machines Is Not Predictive of Appearance Bias in the Wild",
    "authors": [
      "Ryan Steed",
      "Aylin Caliskan"
    ],
    "author_ids": [],
    "abstract": "Research in social psychology has shown that people's biased, subjective\njudgments about another's personality based solely on their appearance are not\npredictive of their actual personality traits. But researchers and companies\noften utilize computer vision models to predict similarly subjective\npersonality attributes such as \"employability.\" We seek to determine whether\nstate-of-the-art, black box face processing technology can learn human-like\nappearance biases. With features extracted with FaceNet, a widely used face\nrecognition framework, we train a transfer learning model on human subjects'\nfirst impressions of personality traits in other faces as measured by social\npsychologists. We find that features extracted with FaceNet can be used to\npredict human appearance bias scores for deliberately manipulated faces but not\nfor randomly generated faces scored by humans. Additionally, in contrast to\nwork with human biases in social psychology, the model does not find a\nsignificant signal correlating politicians' vote shares with perceived\ncompetence bias. With Local Interpretable Model-Agnostic Explanations (LIME),\nwe provide several explanations for this discrepancy. Our results suggest that\nsome signals of appearance bias documented in social psychology are not\nembedded by the machine learning techniques we investigate. We shed light on\nthe ways in which appearance bias could be embedded in face processing\ntechnology and cast further doubt on the practice of predicting subjective\ntraits based on appearances.",
    "published_date": "2020-02-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05636v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05474v6",
    "title": "Metric-Free Individual Fairness in Online Learning",
    "authors": [
      "Yahav Bechavod",
      "Christopher Jung",
      "Zhiwei Steven Wu"
    ],
    "author_ids": [],
    "abstract": "We study an online learning problem subject to the constraint of individual\nfairness, which requires that similar individuals are treated similarly. Unlike\nprior work on individual fairness, we do not assume the similarity measure\namong individuals is known, nor do we assume that such measure takes a certain\nparametric form. Instead, we leverage the existence of an auditor who detects\nfairness violations without enunciating the quantitative measure. In each\nround, the auditor examines the learner's decisions and attempts to identify a\npair of individuals that are treated unfairly by the learner. We provide a\ngeneral reduction framework that reduces online classification in our model to\nstandard online classification, which allows us to leverage existing online\nlearning algorithms to achieve sub-linear regret and number of fairness\nviolations. Surprisingly, in the stochastic setting where the data are drawn\nindependently from a distribution, we are also able to establish PAC-style\nfairness and accuracy generalization guarantees (Rothblum and Yona [2018]),\ndespite only having access to a very restricted form of fairness feedback. Our\nfairness generalization bound qualitatively matches the uniform convergence\nbound of Rothblum and Yona [2018], while also providing a meaningful accuracy\ngeneralization guarantee. Our results resolve an open question by Gillen et al.\n[2018] by showing that online learning under an unknown individual fairness\nconstraint is possible even without assuming a strong parametric form of the\nunderlying similarity measure.",
    "published_date": "2020-02-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05474v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05322v2",
    "title": "Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral Segmentation of Rock micro-CT Images",
    "authors": [
      "Ying Da Wang",
      "Mehdi Shabaninejad",
      "Ryan T. Armstrong",
      "Peyman Mostaghimi"
    ],
    "author_ids": [],
    "abstract": "Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is\nessential for further Digital Rock Physics (DRP) analysis, however,\nconventional methods such as thresholding, watershed segmentation, and\nconverging active contours are susceptible to user-bias. Deep Convolutional\nNeural Networks (CNNs) have produced accurate pixelwise semantic segmentation\nresults with natural images and $\\mu$CT rock images, however, physical accuracy\nis not well documented. The performance of 4 CNN architectures is tested for 2D\nand 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon\nSandstone are treated as ground truth and used as training and validation data,\nwith a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then\nused to validate physical accuracy. The topology of each segmented phase is\ncalculated, and the absolute permeability and multiphase flow is modelled with\ndirect simulation in single and mixed wetting cases. These physical measures of\nconnectivity, and flow characteristics show high variance and uncertainty, with\nmodels that achieve 95\\%+ in voxelwise accuracy possessing permeabilities and\nconnectivities orders of magnitude off. A new network architecture is also\nintroduced as a hybrid fusion of U-net and ResNet, combining short and long\nskip connections in a Network-in-Network configuration. The 3D implementation\noutperforms all other tested models in voxelwise and physical accuracy\nmeasures. The network architecture and the volume fraction in the dataset (and\nassociated weighting), are factors that not only influence the accuracy\ntrade-off in the voxelwise case, but is especially important in training a\nphysically accurate model for segmentation.",
    "published_date": "2020-02-13T00:00:00",
    "year": 2020,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05322v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05317v1",
    "title": "The Quantum Entropy Cone of Hypergraphs",
    "authors": [
      "Ning Bao",
      "Newton Cheng",
      "Sergio Hernández-Cuenca",
      "Vincent P. Su"
    ],
    "author_ids": [],
    "abstract": "In this work, we generalize the graph-theoretic techniques used for the\nholographic entropy cone to study hypergraphs and their analogously-defined\nentropy cone. This allows us to develop a framework to efficiently compute\nentropies and prove inequalities satisfied by hypergraphs. In doing so, we\ndiscover a class of quantum entropy vectors which reach beyond those of\nholographic states and obey constraints intimately related to the ones obeyed\nby stabilizer states and linear ranks. We show that, at least up to 4 parties,\nthe hypergraph cone is identical to the stabilizer entropy cone, thus\ndemonstrating that the hypergraph framework is broadly applicable to the study\nof entanglement entropy. We conjecture that this equality continues to hold for\nhigher party numbers and report on partial progress on this direction. To\nphysically motivate this conjectured equivalence, we also propose a plausible\nmethod inspired by tensor networks to construct a quantum state from a given\nhypergraph such that their entropy vectors match.",
    "published_date": "2020-02-13T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.IT",
      "hep-th",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05317v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.05245v3",
    "title": "Maximin Fairness with Mixed Divisible and Indivisible Goods",
    "authors": [
      "Xiaohui Bei",
      "Shengxin Liu",
      "Xinhang Lu",
      "Hongao Wang"
    ],
    "author_ids": [],
    "abstract": "We study fair resource allocation when the resources contain a mixture of\ndivisible and indivisible goods, focusing on the well-studied fairness notion\nof maximin share fairness (MMS). With only indivisible goods, a full MMS\nallocation may not exist, but a constant multiplicative approximate allocation\nalways does. We analyze how the MMS approximation guarantee would be affected\nwhen the resources to be allocated also contain divisible goods. In particular,\nwe show that the worst-case MMS approximation guarantee with mixed goods is no\nworse than that with only indivisible goods. However, there exist problem\ninstances to which adding some divisible resources would strictly decrease the\nMMS approximation ratio of the instance. On the algorithmic front, we propose a\nconstructive algorithm that will always produce an $\\alpha$-MMS allocation for\nany number of agents, where $\\alpha$ takes values between $1/2$ and $1$ and is\na monotone increasing function determined by how agents value the divisible\ngoods relative to their MMS values.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05245v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.05183v1",
    "title": "The empirical duality gap of constrained statistical learning",
    "authors": [
      "Luiz F. O. Chamon",
      "Santiago Paternain",
      "Miguel Calvo-Fullana",
      "Alejandro Ribeiro"
    ],
    "author_ids": [],
    "abstract": "This paper is concerned with the study of constrained statistical learning\nproblems, the unconstrained version of which are at the core of virtually all\nof modern information processing. Accounting for constraints, however, is\nparamount to incorporate prior knowledge and impose desired structural and\nstatistical properties on the solutions. Still, solving constrained statistical\nproblems remains challenging and guarantees scarce, leaving them to be tackled\nusing regularized formulations. Though practical and effective, selecting\nregularization parameters so as to satisfy requirements is challenging, if at\nall possible, due to the lack of a straightforward relation between parameters\nand constraints. In this work, we propose to directly tackle the constrained\nstatistical problem overcoming its infinite dimensionality, unknown\ndistributions, and constraints by leveraging finite dimensional\nparameterizations, sample averages, and duality theory. Aside from making the\nproblem tractable, these tools allow us to bound the empirical duality gap,\ni.e., the difference between our approximate tractable solutions and the actual\nsolutions of the original statistical problem. We demonstrate the effectiveness\nand usefulness of this constrained formulation in a fair learning application.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05183v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05156v2",
    "title": "Public Bayesian Persuasion: Being Almost Optimal and Almost Persuasive",
    "authors": [
      "Matteo Castiglioni",
      "Andrea Celli",
      "Nicola Gatti"
    ],
    "author_ids": [],
    "abstract": "Persuasion studies how an informed principal may influence the behavior of\nagents by the strategic provision of payoff-relevant information. We focus on\nthe fundamental multi-receiver model by Arieli and Babichenko (2019), in which\nthere are no inter-agent externalities. Unlike prior works on this problem, we\nstudy the public persuasion problem in the general setting with: (i) arbitrary\nstate spaces; (ii) arbitrary action spaces; (iii) arbitrary sender's utility\nfunctions. We fully characterize the computational complexity of computing a\nbi-criteria approximation of an optimal public signaling scheme. In particular,\nwe show, in a voting setting of independent interest, that solving this problem\nrequires at least a quasi-polynomial number of steps even in settings with a\nbinary action space, assuming the Exponential Time Hypothesis. In doing so, we\nprove that a relaxed version of the Maximum Feasible Subsystem of Linear\nInequalities problem requires at least quasi-polynomial time to be solved.\nFinally, we close the gap by providing a quasi-polynomial time bi-criteria\napproximation algorithm for arbitrary public persuasion problems that, in\nspecific settings, yields a QPTAS.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05156v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05147v1",
    "title": "Multi-Agent Reinforcement Learning and Human Social Factors in Climate Change Mitigation",
    "authors": [
      "Kyle Tilbury",
      "Jesse Hoey"
    ],
    "author_ids": [],
    "abstract": "Many complex real-world problems, such as climate change mitigation, are\nintertwined with human social factors. Climate change mitigation, a social\ndilemma made difficult by the inherent complexities of human behavior, has an\nimpact at a global scale. We propose applying multi-agent reinforcement\nlearning (MARL) in this setting to develop intelligent agents that can\ninfluence the social factors at play in climate change mitigation. There are\nethical, practical, and technical challenges that must be addressed when\ndeploying MARL in this way. In this paper, we present these challenges and\noutline an approach to address them. Understanding how intelligent agents can\nbe used to impact human social factors is important to prevent their abuse and\ncan be beneficial in furthering our knowledge of these complex problems as a\nwhole. The challenges we present are not limited to our specific application\nbut are applicable to broader MARL. Thus, developing MARL for social factors in\nclimate change mitigation helps address general problems hindering MARL's\napplicability to other real-world problems while also motivating discussion on\nthe social implications of MARL deployment.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05147v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05145v2",
    "title": "Weighted Empirical Risk Minimization: Sample Selection Bias Correction based on Importance Sampling",
    "authors": [
      "Robin Vogel",
      "Mastane Achab",
      "Stéphan Clémençon",
      "Charles Tillier"
    ],
    "author_ids": [],
    "abstract": "We consider statistical learning problems, when the distribution $P'$ of the\ntraining observations $Z'_1,\\; \\ldots,\\; Z'_n$ differs from the distribution\n$P$ involved in the risk one seeks to minimize (referred to as the test\ndistribution) but is still defined on the same measurable space as $P$ and\ndominates it. In the unrealistic case where the likelihood ratio\n$\\Phi(z)=dP/dP'(z)$ is known, one may straightforwardly extends the Empirical\nRisk Minimization (ERM) approach to this specific transfer learning setup using\nthe same idea as that behind Importance Sampling, by minimizing a weighted\nversion of the empirical risk functional computed from the 'biased' training\ndata $Z'_i$ with weights $\\Phi(Z'_i)$. Although the importance function\n$\\Phi(z)$ is generally unknown in practice, we show that, in various situations\nfrequently encountered in practice, it takes a simple form and can be directly\nestimated from the $Z'_i$'s and some auxiliary information on the statistical\npopulation $P$. By means of linearization techniques, we then prove that the\ngeneralization capacity of the approach aforementioned is preserved when\nplugging the resulting estimates of the $\\Phi(Z'_i)$'s into the weighted\nempirical risk. Beyond these theoretical guarantees, numerical results provide\nstrong empirical evidence of the relevance of the approach promoted in this\narticle.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05145v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05056v2",
    "title": "Quantum Boosting",
    "authors": [
      "Srinivasan Arunachalam",
      "Reevu Maity"
    ],
    "author_ids": [],
    "abstract": "Suppose we have a weak learning algorithm $\\mathcal{A}$ for a Boolean-valued\nproblem: $\\mathcal{A}$ produces hypotheses whose bias $\\gamma$ is small, only\nslightly better than random guessing (this could, for instance, be due to\nimplementing $\\mathcal{A}$ on a noisy device), can we boost the performance of\n$\\mathcal{A}$ so that $\\mathcal{A}$'s output is correct on $2/3$ of the inputs?\n  Boosting is a technique that converts a weak and inaccurate machine learning\nalgorithm into a strong accurate learning algorithm. The AdaBoost algorithm by\nFreund and Schapire (for which they were awarded the G\\\"odel prize in 2003) is\none of the widely used boosting algorithms, with many applications in theory\nand practice. Suppose we have a $\\gamma$-weak learner for a Boolean concept\nclass $C$ that takes time $R(C)$, then the time complexity of AdaBoost scales\nas $VC(C)\\cdot poly(R(C), 1/\\gamma)$, where $VC(C)$ is the $VC$-dimension of\n$C$. In this paper, we show how quantum techniques can improve the time\ncomplexity of classical AdaBoost. To this end, suppose we have a $\\gamma$-weak\nquantum learner for a Boolean concept class $C$ that takes time $Q(C)$, we\nintroduce a quantum boosting algorithm whose complexity scales as\n$\\sqrt{VC(C)}\\cdot poly(Q(C),1/\\gamma);$ thereby achieving a quadratic quantum\nimprovement over classical AdaBoost in terms of $VC(C)$.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05056v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05049v2",
    "title": "Detect and Correct Bias in Multi-Site Neuroimaging Datasets",
    "authors": [
      "Christian Wachinger",
      "Anna Rieckmann",
      "Sebastian Pölsterl"
    ],
    "author_ids": [],
    "abstract": "The desire to train complex machine learning algorithms and to increase the\nstatistical power in association studies drives neuroimaging research to use\never-larger datasets. The most obvious way to increase sample size is by\npooling scans from independent studies. However, simple pooling is often\nill-advised as selection, measurement, and confounding biases may creep in and\nyield spurious correlations. In this work, we combine 35,320 magnetic resonance\nimages of the brain from 17 studies to examine bias in neuroimaging. In the\nfirst experiment, Name That Dataset, we provide empirical evidence for the\npresence of bias by showing that scans can be correctly assigned to their\nrespective dataset with 71.5% accuracy. Given such evidence, we take a closer\nlook at confounding bias, which is often viewed as the main shortcoming in\nobservational studies. In practice, we neither know all potential confounders\nnor do we have data on them. Hence, we model confounders as unknown, latent\nvariables. Kolmogorov complexity is then used to decide whether the confounded\nor the causal model provides the simplest factorization of the graphical model.\nFinally, we present methods for dataset harmonization and study their ability\nto remove bias in imaging features. In particular, we propose an extension of\nthe recently introduced ComBat algorithm to control for global variation across\nimage features, inspired by adjusting for population stratification in\ngenetics. Our results demonstrate that harmonization can reduce\ndataset-specific information in image features. Further, confounding bias can\nbe reduced and even turned into a causal relationship. However, harmonziation\nalso requires caution as it can easily remove relevant subject-specific\ninformation. Code is available at https://github.com/ai-med/Dataset-Bias.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05049v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.04830v3",
    "title": "Positive Semidefinite Programming: Mixed, Parallel, and Width-Independent",
    "authors": [
      "Arun Jambulapati",
      "Yin Tat Lee",
      "Jerry Li",
      "Swati Padmanabhan",
      "Kevin Tian"
    ],
    "author_ids": [],
    "abstract": "We give the first approximation algorithm for mixed packing and covering\nsemidefinite programs (SDPs) with polylogarithmic dependence on width. Mixed\npacking and covering SDPs constitute a fundamental algorithmic primitive with\nrecent applications in combinatorial optimization, robust learning, and quantum\ncomplexity. The current approximate solvers for positive semidefinite\nprogramming can handle only pure packing instances, and technical hurdles\nprevent their generalization to a wider class of positive instances. For a\ngiven multiplicative accuracy of $\\epsilon$, our algorithm takes\n$O(\\log^3(nd\\rho) \\cdot \\epsilon^{-3})$ parallelizable iterations, where $n$,\n$d$ are dimensions of the problem and $\\rho$ is a width parameter of the\ninstance, generalizing or improving all previous parallel algorithms in the\npositive linear and semidefinite programming literature. When specialized to\npure packing SDPs, our algorithm's iteration complexity is $O(\\log^2 (nd) \\cdot\n\\epsilon^{-2})$, a slight improvement and derandomization of the\nstate-of-the-art (Allen-Zhu et. al. '16, Peng et. al. '16, Wang et. al. '15).\nFor a wide variety of structured instances commonly found in applications, the\niterations of our algorithm run in nearly-linear time.\n  In doing so, we give matrix analytic techniques for overcoming obstacles that\nhave stymied prior approaches to this open problem, as stated in past works\n(Peng et. al. '16, Mahoney et. al. '16). Crucial to our analysis are a\nsimplification of existing algorithms for mixed positive linear programs,\nachieved by removing an asymmetry caused by modifying covering constraints, and\na suite of matrix inequalities whose proofs are based on analyzing the Schur\ncomplements of matrices in a higher dimension. We hope that both our algorithm\nand techniques open the door to improved solvers for positive semidefinite\nprogramming, as well as its applications.",
    "published_date": "2020-02-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.04830v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.04732v1",
    "title": "Generalized Bayesian Cramér-Rao Inequality via Information Geometry of Relative $α$-Entropy",
    "authors": [
      "Kumar Vijay Mishra",
      "M. Ashok Kumar"
    ],
    "author_ids": [],
    "abstract": "The relative $\\alpha$-entropy is the R\\'enyi analog of relative entropy and\narises prominently in information-theoretic problems. Recent information\ngeometric investigations on this quantity have enabled the generalization of\nthe Cram\\'{e}r-Rao inequality, which provides a lower bound for the variance of\nan estimator of an escort of the underlying parametric probability\ndistribution. However, this framework remains unexamined in the Bayesian\nframework. In this paper, we propose a general Riemannian metric based on\nrelative $\\alpha$-entropy to obtain a generalized Bayesian Cram\\'{e}r-Rao\ninequality. This establishes a lower bound for the variance of an unbiased\nestimator for the $\\alpha$-escort distribution starting from an unbiased\nestimator for the underlying distribution. We show that in the limiting case\nwhen the entropy order approaches unity, this framework reduces to the\nconventional Bayesian Cram\\'{e}r-Rao inequality. Further, in the absence of\npriors, the same framework yields the deterministic Cram\\'{e}r-Rao inequality.",
    "published_date": "2020-02-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.04732v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.04486v4",
    "title": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss",
    "authors": [
      "Lenaic Chizat",
      "Francis Bach"
    ],
    "author_ids": [],
    "abstract": "Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss\nwith gradient-based methods are observed to perform well in many supervised\nclassification tasks. Towards understanding this phenomenon, we analyze the\ntraining and generalization behavior of infinitely wide two-layer neural\nnetworks with homogeneous activations. We show that the limits of the gradient\nflow on exponentially tailed losses can be fully characterized as a max-margin\nclassifier in a certain non-Hilbertian space of functions. In presence of\nhidden low-dimensional structures, the resulting margin is independent of the\nambiant dimension, which leads to strong generalization bounds. In contrast,\ntraining only the output layer implicitly solves a kernel support vector\nmachine, which a priori does not enjoy such an adaptivity. Our analysis of\ntraining is non-quantitative in terms of running time but we prove\ncomputational guarantees in simplified settings by showing equivalences with\nonline mirror descent. Finally, numerical experiments suggest that our analysis\ndescribes well the practical behavior of two-layer neural networks with ReLU\nactivation and confirm the statistical benefits of this implicit bias.",
    "published_date": "2020-02-11T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.04486v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.04302v1",
    "title": "Trust dynamics and user attitudes on recommendation errors: preliminary results",
    "authors": [
      "David A. Pelta",
      "Jose L. Verdegay",
      "Maria T. Lamata",
      "Carlos Cruz Corona"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence based systems may be used as digital nudging\ntechniques that can steer or coerce users to make decisions not always aligned\nwith their true interests. When such systems properly address the issues of\nFairness, Accountability, Transparency, and Ethics, then the trust of the user\nin the system would just depend on the system's output. The aim of this paper\nis to propose a model for exploring how good and bad recommendations affect the\noverall trust in an idealized recommender system that issues recommendations\nover a resource with limited capacity. The impact of different users attitudes\non trust dynamics is also considered. Using simulations, we ran a large set of\nexperiments that allowed to observe that: 1) under certain circumstances, all\nthe users ended accepting the recommendations; and 2) the user attitude\n(controlled by a single parameter balancing the gain/loss of trust after a\ngood/bad recommendation) has a great impact in the trust dynamics.",
    "published_date": "2020-02-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.GT",
      "cs.IR",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.04302v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05672v2",
    "title": "Steps Towards Value-Aligned Systems",
    "authors": [
      "Osonde A. Osoba",
      "Benjamin Boudreaux",
      "Douglas Yeung"
    ],
    "author_ids": [],
    "abstract": "Algorithmic (including AI/ML) decision-making artifacts are an established\nand growing part of our decision-making ecosystem. They are indispensable tools\nfor managing the flood of information needed to make effective decisions in a\ncomplex world. The current literature is full of examples of how individual\nartifacts violate societal norms and expectations (e.g. violations of fairness,\nprivacy, or safety norms). Against this backdrop, this discussion highlights an\nunder-emphasized perspective in the literature on assessing value misalignment\nin AI-equipped sociotechnical systems. The research on value misalignment has a\nstrong focus on the behavior of individual tech artifacts. This discussion\nargues for a more structured systems-level approach for assessing\nvalue-alignment in sociotechnical systems. We rely primarily on the research on\nfairness to make our arguments more concrete. And we use the opportunity to\nhighlight how adopting a system perspective improves our ability to explain and\naddress value misalignments better. Our discussion ends with an exploration of\npriority questions that demand attention if we are to assure the value\nalignment of whole systems, not just individual artifacts.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05672v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.04108v3",
    "title": "Adversarial Filters of Dataset Biases",
    "authors": [
      "Ronan Le Bras",
      "Swabha Swayamdipta",
      "Chandra Bhagavatula",
      "Rowan Zellers",
      "Matthew E. Peters",
      "Ashish Sabharwal",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "Large neural models have demonstrated human-level performance on language and\nvision benchmarks, while their performance degrades considerably on adversarial\nor out-of-distribution samples. This raises the question of whether these\nmodels have learned to solve a dataset rather than the underlying task by\noverfitting to spurious dataset biases. We investigate one recently proposed\napproach, AFLite, which adversarially filters such dataset biases, as a means\nto mitigate the prevalent overestimation of machine performance. We provide a\ntheoretical understanding for AFLite, by situating it in the generalized\nframework for optimum bias reduction. We present extensive supporting evidence\nthat AFLite is broadly applicable for reduction of measurable dataset biases,\nand that models trained on the filtered datasets yield better generalization to\nout-of-distribution tasks. Finally, filtering results in a large drop in model\nperformance (e.g., from 92% to 62% for SNLI), while human performance still\nremains high. Our work thus shows that such filtered datasets can pose new\nresearch challenges for robust generalization by serving as upgraded\nbenchmarks.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.04108v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03887v2",
    "title": "Edge Matching with Inequalities, Triangles, Unknown Shape, and Two Players",
    "authors": [
      "Jeffrey Bosboom",
      "Charlotte Chen",
      "Lily Chung",
      "Spencer Compton",
      "Michael Coulombe",
      "Erik D. Demaine",
      "Martin L. Demaine",
      "Ivan Tadeu Ferreira Antunes Filho",
      "Dylan Hendrickson",
      "Adam Hesterberg",
      "Calvin Hsu",
      "William Hu",
      "Oliver Korten",
      "Zhezheng Luo",
      "Lillian Zhang"
    ],
    "author_ids": [],
    "abstract": "We analyze the computational complexity of several new variants of\nedge-matching puzzles. First we analyze inequality (instead of equality)\nconstraints between adjacent tiles, proving the problem NP-complete for strict\ninequalities but polynomial for nonstrict inequalities. Second we analyze three\ntypes of triangular edge matching, of which one is polynomial and the other two\nare NP-complete; all three are #P-complete. Third we analyze the case where no\ntarget shape is specified, and we merely want to place the (square) tiles so\nthat edges match (exactly); this problem is NP-complete. Fourth we consider\nfour 2-player games based on $1 \\times n$ edge matching, all four of which are\nPSPACE-complete. Most of our NP-hardness reductions are parsimonious, newly\nproving #P and ASP-completeness for, e.g., $1 \\times n$ edge matching.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CC",
      "cs.CG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03887v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.03747v2",
    "title": "Unbiased Filtering of a Class of Partially Observed Diffusions",
    "authors": [
      "Ajay Jasra",
      "Kody Law",
      "Fangyuan Yu"
    ],
    "author_ids": [],
    "abstract": "In this article we consider a Monte Carlo-based method to filter partially\nobserved diffusions observed at regular and discrete times. Given access only\nto Euler discretizations of the diffusion process, we present a new procedure\nwhich can return online estimates of the filtering distribution with no\ndiscretization bias and finite variance. Our approach is based upon a novel\ndouble application of the randomization methods of Rhee & Glynn (2015) along\nwith the multilevel particle filter (MLPF) approach of Jasra et al (2017). A\nnumerical comparison of our new approach with the MLPF, on a single processor,\nshows that similar errors are possible for a mild increase in computational\ncost. However, the new method scales strongly to arbitrarily many processors.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03747v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.03717v1",
    "title": "Small world is not enough: Criteria for network choice and conclusiveness of simulations",
    "authors": [
      "Samuel Thiriot"
    ],
    "author_ids": [],
    "abstract": "Most agent-based models include a social network that describes the structure\nof interactions within the artificial population. Because of the dramatic\nimpact of this structure on the simulated dynamics, modelers create this\nnetwork for it to match criteria of plausibility (e.g. the small-world\nproperty). Networks are actually created by one network generator compliant\nwith these criteria, like the Watts-Strogatz algorithm in the case of\nsmall-world networks. However, this practice comes to study the model's\ndynamics over the specific networks generated by one algorithm instead of the\ndynamics over the class of networks of interest, possibly inducing a strong\nbias in results. We identify three problematics related to this bias: (i)\nrepresentativity of a network generator to a class of networks, (ii)\nconclusiveness of simulations over a class of networks and (iii) the gain in\nconclusiveness when refining the criteria for network choice. We propose an\nexperimental protocol and instantiate it on small-world networks for epidemics,\nopinion and culture dynamics. We show that (i) Watts-Strogatz networks are not\nrepresentative of small-world networks (ii) simulation results over\nsmall-worlds are arguably inconclusive, and (iii) even small-world networks\nhaving the same size, density, transitivity and average path length do not lead\nto coherent results. Beyond questioning the relevance of simulation results\nobtained from artificial networks, this research also constitute one more\nargument for the exploration of other approaches that are not solely focused on\nnetworks' statistical properties.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03717v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.03673v2",
    "title": "Rethinking Class-Prior Estimation for Positive-Unlabeled Learning",
    "authors": [
      "Yu Yao",
      "Tongliang Liu",
      "Bo Han",
      "Mingming Gong",
      "Gang Niu",
      "Masashi Sugiyama",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Given only positive (P) and unlabeled (U) data, PU learning can train a\nbinary classifier without any negative data. It has two building blocks: PU\nclass-prior estimation (CPE) and PU classification; the latter has been well\nstudied while the former has received less attention. Hitherto, the\ndistributional-assumption-free CPE methods rely on a critical assumption that\nthe support of the positive data distribution cannot be contained in the\nsupport of the negative data distribution. If this is violated, those CPE\nmethods will systematically overestimate the class prior; it is even worse that\nwe cannot verify the assumption based on the data. In this paper, we rethink\nCPE for PU learning-can we remove the assumption to make CPE always valid? We\nshow an affirmative answer by proposing Regrouping CPE (ReCPE) that builds an\nauxiliary probability distribution such that the support of the positive data\ndistribution is never contained in the support of the negative data\ndistribution. ReCPE can work with any CPE method by treating it as the base\nmethod. Theoretically, ReCPE does not affect its base if the assumption already\nholds for the original probability distribution; otherwise, it reduces the\npositive bias of its base. Empirically, ReCPE improves all state-of-the-art CPE\nmethods on various datasets, implying that the assumption has indeed been\nviolated here.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03673v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03662v3",
    "title": "Improving Face Recognition from Hard Samples via Distribution Distillation Loss",
    "authors": [
      "Yuge Huang",
      "Pengcheng Shen",
      "Ying Tai",
      "Shaoxin Li",
      "Xiaoming Liu",
      "Jilin Li",
      "Feiyue Huang",
      "Rongrong Ji"
    ],
    "author_ids": [],
    "abstract": "Large facial variations are the main challenge in face recognition. To this\nend, previous variation-specific methods make full use of task-related prior to\ndesign special network losses, which are typically not general among different\ntasks and scenarios. In contrast, the existing generic methods focus on\nimproving the feature discriminability to minimize the intra-class distance\nwhile maximizing the interclass distance, which perform well on easy samples\nbut fail on hard samples. To improve the performance on those hard samples for\ngeneral tasks, we propose a novel Distribution Distillation Loss to narrow the\nperformance gap between easy and hard samples, which is a simple, effective and\ngeneric for various types of facial variations. Specifically, we first adopt\nstate-of-the-art classifiers such as ArcFace to construct two similarity\ndistributions: teacher distribution from easy samples and student distribution\nfrom hard samples. Then, we propose a novel distribution-driven loss to\nconstrain the student distribution to approximate the teacher distribution,\nwhich thus leads to smaller overlap between the positive and negative pairs in\nthe student distribution. We have conducted extensive experiments on both\ngeneric large-scale face benchmarks and benchmarks with diverse variations on\nrace, resolution and pose. The quantitative results demonstrate the superiority\nof our method over strong baselines, e.g., Arcface and Cosface.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03662v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03592v3",
    "title": "Post-Comparison Mitigation of Demographic Bias in Face Recognition Using Fair Score Normalization",
    "authors": [
      "Philipp Terhörst",
      "Jan Niklas Kolf",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "author_ids": [],
    "abstract": "Current face recognition systems achieve high progress on several benchmark\ntests. Despite this progress, recent works showed that these systems are\nstrongly biased against demographic sub-groups. Consequently, an easily\nintegrable solution is needed to reduce the discriminatory effect of these\nbiased systems. Previous work mainly focused on learning less biased face\nrepresentations, which comes at the cost of a strongly degraded overall\nrecognition performance. In this work, we propose a novel unsupervised fair\nscore normalization approach that is specifically designed to reduce the effect\nof bias in face recognition and subsequently lead to a significant overall\nperformance boost. Our hypothesis is built on the notation of individual\nfairness by designing a normalization approach that leads to treating similar\nindividuals similarly. Experiments were conducted on three publicly available\ndatasets captured under controlled and in-the-wild circumstances. Results\ndemonstrate that our solution reduces demographic biases, e.g. by up to 82.7%\nin the case when gender is considered. Moreover, it mitigates the bias more\nconsistently than existing works. In contrast to previous works, our fair\nnormalization approach enhances the overall performance by up to 53.2% at false\nmatch rate of 0.001 and up to 82.9% at a false match rate of 0.00001.\nAdditionally, it is easily integrable into existing recognition systems and not\nlimited to face biometrics.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03592v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03536v1",
    "title": "What Changed Your Mind: The Roles of Dynamic Topics and Discourse in Argumentation Process",
    "authors": [
      "Jichuan Zeng",
      "Jing Li",
      "Yulan He",
      "Cuiyun Gao",
      "Michael R. Lyu",
      "Irwin King"
    ],
    "author_ids": [],
    "abstract": "In our world with full of uncertainty, debates and argumentation contribute\nto the progress of science and society. Despite of the increasing attention to\ncharacterize human arguments, most progress made so far focus on the debate\noutcome, largely ignoring the dynamic patterns in argumentation processes. This\npaper presents a study that automatically analyzes the key factors in argument\npersuasiveness, beyond simply predicting who will persuade whom. Specifically,\nwe propose a novel neural model that is able to dynamically track the changes\nof latent topics and discourse in argumentative conversations, allowing the\ninvestigation of their roles in influencing the outcomes of persuasion.\nExtensive experiments have been conducted on argumentative conversations on\nboth social media and supreme court. The results show that our model\noutperforms state-of-the-art models in identifying persuasive arguments via\nexplicitly exploring dynamic factors of topic and discourse. We further analyze\nthe effects of topics and discourse on persuasiveness, and find that they are\nboth useful - topics provide concrete evidence while superior discourse styles\nmay bias participants, especially in social media arguments. In addition, we\ndraw some findings from our empirical results, which will help people better\nengage in future persuasive conversations.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03536v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03508v1",
    "title": "Fair Correlation Clustering",
    "authors": [
      "Saba Ahmadi",
      "Sainyam Galhotra",
      "Barna Saha",
      "Roy Schwartz"
    ],
    "author_ids": [],
    "abstract": "In this paper we study the problem of correlation clustering under fairness\nconstraints. In the classic correlation clustering problem, we are given a\ncomplete graph where each edge is labeled positive or negative. The goal is to\nobtain a clustering of the vertices that minimizes disagreements -- the number\nof negative edges trapped inside a cluster plus positive edges between\ndifferent clusters.\n  We consider two variations of fairness constraint for the problem of\ncorrelation clustering where each node has a color, and the goal is to form\nclusters that do not over-represent vertices of any color.\n  The first variant aims to generate clusters with minimum disagreements, where\nthe distribution of a feature (e.g. gender) in each cluster is same as the\nglobal distribution. For the case of two colors when the desired ratio of the\nnumber of colors in each cluster is $1:p$, we get\n$\\mathcal{O}(p^2)$-approximation algorithm. Our algorithm could be extended to\nthe case of multiple colors. We prove this problem is NP-hard.\n  The second variant considers relative upper and lower bounds on the number of\nnodes of any color in a cluster. The goal is to avoid violating upper and lower\nbounds corresponding to each color in each cluster while minimizing the total\nnumber of disagreements. Along with our theoretical results, we show the\neffectiveness of our algorithm to generate fair clusters by empirical\nevaluation on real world data sets.",
    "published_date": "2020-02-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03508v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03427v1",
    "title": "Graph Neural Distance Metric Learning with Graph-Bert",
    "authors": [
      "Jiawei Zhang"
    ],
    "author_ids": [],
    "abstract": "Graph distance metric learning serves as the foundation for many graph\nlearning problems, e.g., graph clustering, graph classification and graph\nmatching. Existing research works on graph distance metric (or graph kernels)\nlearning fail to maintain the basic properties of such metrics, e.g.,\nnon-negative, identity of indiscernibles, symmetry and triangle inequality,\nrespectively. In this paper, we will introduce a new graph neural network based\ndistance metric learning approaches, namely GB-DISTANCE (GRAPH-BERT based\nNeural Distance). Solely based on the attention mechanism, GB-DISTANCE can\nlearn graph instance representations effectively based on a pre-trained\nGRAPH-BERT model. Different from the existing supervised/unsupervised metrics,\nGB-DISTANCE can be learned effectively in a semi-supervised manner. In\naddition, GB-DISTANCE can also maintain the distance metric basic properties\nmentioned above. Extensive experiments have been done on several benchmark\ngraph datasets, and the results demonstrate that GB-DISTANCE can out-perform\nthe existing baseline methods, especially the recent graph neural network model\nbased graph metrics, with a significant gap in computing the graph distance.",
    "published_date": "2020-02-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03427v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03309v2",
    "title": "A Physiology-Driven Computational Model for Post-Cardiac Arrest Outcome Prediction",
    "authors": [
      "Han B. Kim",
      "Hieu Nguyen",
      "Qingchu Jin",
      "Sharmila Tamby",
      "Tatiana Gelaf Romer",
      "Eric Sung",
      "Ran Liu",
      "Joseph Greenstein",
      "Jose I. Suarez",
      "Christian Storm",
      "Raimond Winslow",
      "Robert D. Stevens"
    ],
    "author_ids": [],
    "abstract": "Patients resuscitated from cardiac arrest (CA) face a high risk of\nneurological disability and death, however pragmatic methods are lacking for\naccurate and reliable prognostication. The aim of this study was to build\ncomputational models to predict post-CA outcome by leveraging high-dimensional\npatient data available early after admission to the intensive care unit (ICU).\nWe hypothesized that model performance could be enhanced by integrating\nphysiological time series (PTS) data and by training machine learning (ML)\nclassifiers. We compared three models integrating features extracted from the\nelectronic health records (EHR) alone, features derived from PTS collected in\nthe first 24hrs after ICU admission (PTS24), and models integrating PTS24 and\nEHR. Outcomes of interest were survival and neurological outcome at ICU\ndischarge. Combined EHR-PTS24 models had higher discrimination (area under the\nreceiver operating characteristic curve [AUC]) than models which used either\nEHR or PTS24 alone, for the prediction of survival (AUC 0.85, 0.80 and 0.68\nrespectively) and neurological outcome (0.87, 0.83 and 0.78). The best ML\nclassifier achieved higher discrimination than the reference logistic\nregression model (APACHE III) for survival (AUC 0.85 vs 0.70) and neurological\noutcome prediction (AUC 0.87 vs 0.75). Feature analysis revealed previously\nunknown factors to be associated with post-CA recovery. Results attest to the\neffectiveness of ML models for post-CA predictive modeling and suggest that PTS\nrecorded in very early phase after resuscitation encode short-term outcome\nprobabilities.",
    "published_date": "2020-02-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML",
      "J.3; I.2.1; I.6.4; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03309v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03276v1",
    "title": "Asymmetric Rejection Loss for Fairer Face Recognition",
    "authors": [
      "Haoyu Qin"
    ],
    "author_ids": [],
    "abstract": "Face recognition performance has seen a tremendous gain in recent years,\nmostly due to the availability of large-scale face images dataset that can be\nexploited by deep neural networks to learn powerful face representations.\nHowever, recent research has shown differences in face recognition performance\nacross different ethnic groups mostly due to the racial imbalance in the\ntraining datasets where Caucasian identities largely dominate other\nethnicities. This is actually symptomatic of the under-representation of\nnon-Caucasian ethnic groups in the celebdom from which face datasets are\nusually gathered, rendering the acquisition of labeled data of the\nunder-represented groups challenging. In this paper, we propose an Asymmetric\nRejection Loss, which aims at making full use of unlabeled images of those\nunder-represented groups, to reduce the racial bias of face recognition models.\nWe view each unlabeled image as a unique class, however as we cannot guarantee\nthat two unlabeled samples are from a distinct class we exploit both labeled\nand unlabeled data in an asymmetric manner in our loss formalism. Extensive\nexperiments show our method's strength in mitigating racial bias, outperforming\nstate-of-the-art semi-supervision methods. Performance on the under-represented\nethnicity groups increases while that on the well-represented group is nearly\nunchanged.",
    "published_date": "2020-02-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03276v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03256v1",
    "title": "Diversity and Inclusion Metrics in Subset Selection",
    "authors": [
      "Margaret Mitchell",
      "Dylan Baker",
      "Nyalleng Moorosi",
      "Emily Denton",
      "Ben Hutchinson",
      "Alex Hanna",
      "Timnit Gebru",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "The ethical concept of fairness has recently been applied in machine learning\n(ML) settings to describe a wide range of constraints and objectives. When\nconsidering the relevance of ethical concepts to subset selection problems, the\nconcepts of diversity and inclusion are additionally applicable in order to\ncreate outputs that account for social power and access differentials. We\nintroduce metrics based on these concepts, which can be applied together,\nseparately, and in tandem with additional fairness constraints. Results from\nhuman subject experiments lend support to the proposed criteria. Social choice\nmethods can additionally be leveraged to aggregate and choose preferable sets,\nand we detail how these may be applied.",
    "published_date": "2020-02-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03256v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03203v2",
    "title": "Eliminating Search Intent Bias in Learning to Rank",
    "authors": [
      "Yingcheng Sun",
      "Richard Kolacinski",
      "Kenneth Loparo"
    ],
    "author_ids": [],
    "abstract": "Click-through data has proven to be a valuable resource for improving\nsearch-ranking quality. Search engines can easily collect click data, but\nbiases introduced in the data can make it difficult to use the data\neffectively. In order to measure the effects of biases, many click models have\nbeen proposed in the literature. However, none of the models can explain the\nobservation that users with different search intent (e.g., informational,\nnavigational, etc.) have different click behaviors. In this paper, we study how\ndifferences in user search intent can influence click activities and determined\nthat there exists a bias between user search intent and the relevance of the\ndocument relevance. Based on this observation, we propose a search intent bias\nhypothesis that can be applied to most existing click models to improve their\nability to learn unbiased relevance. Experimental results demonstrate that\nafter adopting the search intent hypothesis, click models can better interpret\nuser clicks and substantially improve retrieval performance.",
    "published_date": "2020-02-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03203v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03174v3",
    "title": "Fairness and Efficiency in Cake-Cutting with Single-Peaked Preferences",
    "authors": [
      "Bhavook Bhardwaj",
      "Rajnish Kumar",
      "Josue Ortega"
    ],
    "author_ids": [],
    "abstract": "We study the cake-cutting problem when agents have single-peaked preferences\nover the cake. We show that a recently proposed mechanism by Wang-Wu (2019) to\nobtain envy-free allocations can yield large welfare losses. Using a\nsimplifying assumption, we characterize all Pareto optimal allocations, which\nhave a simple structure: are peak-preserving and non-wasteful. Finally, we\nprovide simple alternative mechanisms that Pareto dominate that of Wang-Wu, and\nwhich achieve envy-freeness or Pareto optimality.",
    "published_date": "2020-02-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03174v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.03137v1",
    "title": "Symbiotic Attention with Privileged Information for Egocentric Action Recognition",
    "authors": [
      "Xiaohan Wang",
      "Yu Wu",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "author_ids": [],
    "abstract": "Egocentric video recognition is a natural testbed for diverse interaction\nreasoning. Due to the large action vocabulary in egocentric video datasets,\nrecent studies usually utilize a two-branch structure for action recognition,\nie, one branch for verb classification and the other branch for noun\nclassification. However, correlation studies between the verb and the noun\nbranches have been largely ignored. Besides, the two branches fail to exploit\nlocal features due to the absence of a position-aware attention mechanism. In\nthis paper, we propose a novel Symbiotic Attention framework leveraging\nPrivileged information (SAP) for egocentric video recognition. Finer\nposition-aware object detection features can facilitate the understanding of\nactor's interaction with the object. We introduce these features in action\nrecognition and regard them as privileged information. Our framework enables\nmutual communication among the verb branch, the noun branch, and the privileged\ninformation. This communication process not only injects local details into\nglobal features but also exploits implicit guidance about the spatio-temporal\nposition of an on-going action. We introduce novel symbiotic attention (SA) to\nenable effective communication. It first normalizes the detection guided\nfeatures on one branch to underline the action-relevant information from the\nother branch. SA adaptively enhances the interactions among the three sources.\nTo further catalyze this communication, spatial relations are uncovered for the\nselection of most action-relevant information. It identifies the most valuable\nand discriminative feature for classification. We validate the effectiveness of\nour SAP quantitatively and qualitatively. Notably, it achieves the\nstate-of-the-art on two large-scale egocentric video datasets.",
    "published_date": "2020-02-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03137v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.03112v2",
    "title": "An extension of Koksma's inequality",
    "authors": [
      "Martin Lind"
    ],
    "author_ids": [],
    "abstract": "When applying the quasi-Monte Carlo (QMC) method of numerical integration of\nunivariate functions, Koksma's inequality provides a basic estimate of the\nerror in terms of the discrepancy of the used evaluation points and the total\nvariation of the integrated function. We present an extension of Koksma's\ninequality that is also applicable for functions with infinite total variation.",
    "published_date": "2020-02-08T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "65D30, 26B30"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03112v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.02942v1",
    "title": "On the Robustness of Face Recognition Algorithms Against Attacks and Bias",
    "authors": [
      "Richa Singh",
      "Akshay Agarwal",
      "Maneet Singh",
      "Shruti Nagpal",
      "Mayank Vatsa"
    ],
    "author_ids": [],
    "abstract": "Face recognition algorithms have demonstrated very high recognition\nperformance, suggesting suitability for real world applications. Despite the\nenhanced accuracies, robustness of these algorithms against attacks and bias\nhas been challenged. This paper summarizes different ways in which the\nrobustness of a face recognition algorithm is challenged, which can severely\naffect its intended working. Different types of attacks such as physical\npresentation attacks, disguise/makeup, digital adversarial attacks, and\nmorphing/tampering using GANs have been discussed. We also present a discussion\non the effect of bias on face recognition models and showcase that factors such\nas age and gender variations affect the performance of modern algorithms. The\npaper also presents the potential reasons for these challenges and some of the\nfuture research directions for increasing the robustness of face recognition\nmodels.",
    "published_date": "2020-02-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02942v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.02886v4",
    "title": "Weakly-Supervised Disentanglement Without Compromises",
    "authors": [
      "Francesco Locatello",
      "Ben Poole",
      "Gunnar Rätsch",
      "Bernhard Schölkopf",
      "Olivier Bachem",
      "Michael Tschannen"
    ],
    "author_ids": [],
    "abstract": "Intelligent agents should be able to learn useful representations by\nobserving changes in their environment. We model such observations as pairs of\nnon-i.i.d. images sharing at least one of the underlying factors of variation.\nFirst, we theoretically show that only knowing how many factors have changed,\nbut not which ones, is sufficient to learn disentangled representations.\nSecond, we provide practical algorithms that learn disentangled representations\nfrom pairs of images without requiring annotation of groups, individual\nfactors, or the number of factors that have changed. Third, we perform a\nlarge-scale empirical study and show that such pairs of observations are\nsufficient to reliably learn disentangled representations on several benchmark\ndata sets. Finally, we evaluate our learned representations and find that they\nare simultaneously useful on a diverse suite of tasks, including generalization\nunder covariate shifts, fairness, and abstract reasoning. Overall, our results\ndemonstrate that weak supervision enables learning of useful disentangled\nrepresentations in realistic scenarios.",
    "published_date": "2020-02-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02886v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06161v1",
    "title": "menoci: Lightweight Extensible Web Portal enabling FAIR Data Management for Biomedical Research Projects",
    "authors": [
      "Markus Suhr",
      "Christoph Lehmann",
      "Christian Robert Bauer",
      "Theresa Bender",
      "Cornelius Knopp",
      "Luca Freckmann",
      "Björn Öst Hansen",
      "Christian Henke",
      "Georg Aschenbrandt",
      "Lea Kühlborn",
      "Sophia Rheinländer",
      "Linus Weber",
      "Bartlomiej Marzec",
      "Marcel Hellkamp",
      "Philipp Wieder",
      "Harald Kusch",
      "Ulrich Sax",
      "Sara Yasemin Nussbeck"
    ],
    "author_ids": [],
    "abstract": "Background: Biomedical research projects deal with data management\nrequirements from multiple sources like funding agencies' guidelines, publisher\npolicies, discipline best practices, and their own users' needs. We describe\nfunctional and quality requirements based on many years of experience\nimplementing data management for the CRC 1002 and CRC 1190. A fully equipped\ndata management software should improve documentation of experiments and\nmaterials, enable data storage and sharing according to the FAIR Guiding\nPrinciples while maximizing usability, information security, as well as\nsoftware sustainability and reusability. Results: We introduce the modular web\nportal software menoci for data collection, experiment documentation, data\npublication, sharing, and preservation in biomedical research projects. Menoci\nmodules are based on the Drupal content management system which enables\nlightweight deployment and setup, and creates the possibility to combine\nresearch data management with a customisable project home page or collaboration\nplatform. Conclusions: Management of research data and digital research\nartefacts is transforming from individual researcher or groups best practices\ntowards project- or organisation-wide service infrastructures. To enable and\nsupport this structural transformation process, a vital ecosystem of open\nsource software tools is needed. Menoci is a contribution to this ecosystem of\nresearch data management tools that is specifically designed to support\nbiomedical research projects.",
    "published_date": "2020-02-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DL",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06161v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.02826v3",
    "title": "Conditional Deep Gaussian Processes: multi-fidelity kernel learning",
    "authors": [
      "Chi-Ken Lu",
      "Patrick Shafto"
    ],
    "author_ids": [],
    "abstract": "Deep Gaussian Processes (DGPs) were proposed as an expressive Bayesian model\ncapable of a mathematically grounded estimation of uncertainty. The\nexpressivity of DPGs results from not only the compositional character but the\ndistribution propagation within the hierarchy. Recently, [1] pointed out that\nthe hierarchical structure of DGP well suited modeling the multi-fidelity\nregression, in which one is provided sparse observations with high precision\nand plenty of low fidelity observations. We propose the conditional DGP model\nin which the latent GPs are directly supported by the fixed lower fidelity\ndata. Then the moment matching method in [2] is applied to approximate the\nmarginal prior of conditional DGP with a GP. The obtained effective kernels are\nimplicit functions of the lower-fidelity data, manifesting the expressivity\ncontributed by distribution propagation within the hierarchy. The\nhyperparameters are learned via optimizing the approximate marginal likelihood.\nExperiments with synthetic and high dimensional data show comparable\nperformance against other multi-fidelity regression methods, variational\ninference, and multi-output GP. We conclude that, with the low fidelity data\nand the hierarchical DGP structure, the effective kernel encodes the inductive\nbias for true function allowing the compositional freedom discussed in [3,4].",
    "published_date": "2020-02-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02826v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.02545v2",
    "title": "Contradictory Structure Learning for Semi-supervised Domain Adaptation",
    "authors": [
      "Can Qin",
      "Lichen Wang",
      "Qianqian Ma",
      "Yu Yin",
      "Huan Wang",
      "Yun Fu"
    ],
    "author_ids": [],
    "abstract": "Current adversarial adaptation methods attempt to align the cross-domain\nfeatures, whereas two challenges remain unsolved: 1) the conditional\ndistribution mismatch and 2) the bias of the decision boundary towards the\nsource domain. To solve these challenges, we propose a novel framework for\nsemi-supervised domain adaptation by unifying the learning of opposite\nstructures (UODA). UODA consists of a generator and two classifiers (i.e., the\nsource-scattering classifier and the target-clustering classifier), which are\ntrained for contradictory purposes. The target-clustering classifier attempts\nto cluster the target features to improve intra-class density and enlarge\ninter-class divergence. Meanwhile, the source-scattering classifier is designed\nto scatter the source features to enhance the decision boundary's smoothness.\nThrough the alternation of source-feature expansion and target-feature\nclustering procedures, the target features are well-enclosed within the dilated\nboundary of the corresponding source features. This strategy can make the\ncross-domain features to be precisely aligned against the source bias\nsimultaneously. Moreover, to overcome the model collapse through training, we\nprogressively update the measurement of feature's distance and their\nrepresentation via an adversarial training paradigm. Extensive experiments on\nthe benchmarks of DomainNet and Office-home datasets demonstrate the\nsuperiority of our approach over the state-of-the-art methods.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02545v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.06255v1",
    "title": "Proportional Fairness through Dual Connectivity in Heterogeneous Networks",
    "authors": [
      "Pradnya Kiri Taksande",
      "Prasanna Chaporkar",
      "Pranav Jha",
      "Abhay Karandikar"
    ],
    "author_ids": [],
    "abstract": "Proportional Fair (PF) is a scheduling technique to maintain a balance\nbetween maximizing throughput and ensuring fairness to users. Dual Connectivity\n(DC) technique was introduced by the 3rd Generation Partnership Project (3GPP)\nto improve the mobility robustness and system capacity in heterogeneous\nnetworks. In this paper, we demonstrate the utility of DC in improving\nproportional fairness in the system. We propose a low complexity centralized PF\nscheduling scheme for DC and show that it outperforms the standard PF\nscheduling scheme. Since the problem of dual association of users for\nmaximizing proportional fairness in the system is NP-hard, we propose three\nheuristic user association schemes for DC. We demonstrate that DC, along with\nthe proposed PF scheme, gives remarkable gains on PF utility over single\nconnectivity and performs almost close to the optimal PF scheme in\nheterogeneous networks.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.06255v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.02219v2",
    "title": "A Self-Integration Testbed for Decentralized Socio-technical Systems",
    "authors": [
      "Farzam Fanitabasi",
      "Edward Gaere",
      "Evangelos Pournaras"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things comes along with new challenges for experimenting,\ntesting, and operating decentralized socio-technical systems at large-scale. In\nsuch systems, autonomous agents interact locally with their users, and remotely\nwith other agents to make intelligent collective choices. Via these\ninteractions they self-regulate the consumption and production of distributed\nresources. While such complex systems are often deployed and operated using\ncentralized computing infrastructures, the socio-technical nature of these\ndecentralized systems requires new value-sensitive design paradigms; empowering\ntrust, transparency, and alignment with citizens' social values, such as\nprivacy preservation, autonomy, and fairness among citizens' choices.\nCurrently, instruments and tools to study such systems and guide the\nprototyping process from simulation to live deployment are missing, or not\npractical in this distributed socio-technical context. This paper bridges this\ngap by introducing a novel testbed architecture for decentralized\nsocio-technical systems running on IoT. This new architecture is designed for a\nseamless reusability of (i) application-independent decentralized services by\nan IoT application, and (ii) different IoT applications by the same\ndecentralized service. This dual self-integration promises IoT applications\nthat are simpler to prototype, and can interoperate with decentralized services\nduring runtime to self-integrate more complex functionality. Such integration\nprovides stronger validation of IoT applications, and improves resource\nutilization. Pressure and crash tests during continuous operations of several\nweeks, with more than 80K network joining and leaving of agents, 2.4M parameter\nchanges, and 100M communicated messages, confirm the robustness and\npracticality of the testbed architecture.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.MA",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02219v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.03806v3",
    "title": "Delay Mitigation in Air Traffic Flow Management",
    "authors": [
      "Mehran Makhtoumi"
    ],
    "author_ids": [],
    "abstract": "To mitigate ATFM delay, different approaches have been proposed so far which\ncan be categorized into strategic and tactical domains. The strategical\ntechniques mainly concern airport slot allocation and for the tactical domain,\nthe ATFM function has several solutions available that range from the ground\nand air holding to rerouting actions, which have not gained significant\nefficiency in ATFM delay mitigation due to the fact that delays become apparent\nonly on the tactical level when the strategic flight plan has been filled\nalready. To tackle and address this problem there is a need for an algorithm\nthat can synchronize strategical and tactical schedules. To fill this gap, in\nthis paper the concept of fair buffer scheduling is proposed which can\npotentially contribute to strategical and tactical operations synchronization\nthat would result in ATFM delay mitigation by increasing the system's\nrobustness. The objective is to obtain an optimum fair and efficient buffer\nchoice that mitigates ATFM delay and increases the stakeholders' welfare. Each\nappropriate and efficient approach requires a comprehensive understanding of\nthe strategical buffer scheduling. This study presents a delay cost and flight\nbuffer model that could be used for generating optimal buffer times to be\nconsidered as the initial population for the optimization problem to\ninvestigate the viability of employing fairness measures to obtain schedules\nwith different trade-offs between cost, delay, and fairness.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03806v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.02196v2",
    "title": "AI-GAN: Attack-Inspired Generation of Adversarial Examples",
    "authors": [
      "Tao Bai",
      "Jun Zhao",
      "Jinlin Zhu",
      "Shoudong Han",
      "Jiefeng Chen",
      "Bo Li",
      "Alex Kot"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which are\ncrafted by adding imperceptible perturbations to inputs. Recently different\nattacks and strategies have been proposed, but how to generate adversarial\nexamples perceptually realistic and more efficiently remains unsolved. This\npaper proposes a novel framework called Attack-Inspired GAN (AI-GAN), where a\ngenerator, a discriminator, and an attacker are trained jointly. Once trained,\nit can generate adversarial perturbations efficiently given input images and\ntarget classes. Through extensive experiments on several popular datasets \\eg\nMNIST and CIFAR-10, AI-GAN achieves high attack success rates and reduces\ngeneration time significantly in various settings. Moreover, for the first\ntime, AI-GAN successfully scales to complicated datasets \\eg CIFAR-100 with\naround $90\\%$ success rates among all classes.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02196v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.02194v1",
    "title": "Joint Deep Learning of Facial Expression Synthesis and Recognition",
    "authors": [
      "Yan Yan",
      "Ying Huang",
      "Si Chen",
      "Chunhua Shen",
      "Hanzi Wang"
    ],
    "author_ids": [],
    "abstract": "Recently, deep learning based facial expression recognition (FER) methods\nhave attracted considerable attention and they usually require large-scale\nlabelled training data. Nonetheless, the publicly available facial expression\ndatabases typically contain a small amount of labelled data. In this paper, to\novercome the above issue, we propose a novel joint deep learning of facial\nexpression synthesis and recognition method for effective FER. More\nspecifically, the proposed method involves a two-stage learning procedure.\nFirstly, a facial expression synthesis generative adversarial network (FESGAN)\nis pre-trained to generate facial images with different facial expressions. To\nincrease the diversity of the training images, FESGAN is elaborately designed\nto generate images with new identities from a prior distribution. Secondly, an\nexpression recognition network is jointly learned with the pre-trained FESGAN\nin a unified framework. In particular, the classification loss computed from\nthe recognition network is used to simultaneously optimize the performance of\nboth the recognition network and the generator of FESGAN. Moreover, in order to\nalleviate the problem of data bias between the real images and the synthetic\nimages, we propose an intra-class loss with a novel real data-guided\nback-propagation (RDBP) algorithm to reduce the intra-class variations of\nimages from the same class, which can significantly improve the final\nperformance. Extensive experimental results on public facial expression\ndatabases demonstrate the superiority of the proposed method compared with\nseveral state-of-the-art FER methods.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02194v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.02187v2",
    "title": "On the Fairness of Name-Based Rationing System for Purchases of Masks Policy",
    "authors": [
      "Yu-Ting Liu"
    ],
    "author_ids": [],
    "abstract": "In this paper, mathematical model and condition are built for the analysis of\nfairness of name-based rationing system for purchases of masks policy announced\nand launched in Taiwan.",
    "published_date": "2020-02-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.02187v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.01621v1",
    "title": "Joint Optimization of AI Fairness and Utility: A Human-Centered Approach",
    "authors": [
      "Yunfeng Zhang",
      "Rachel K. E. Bellamy",
      "Kush R. Varshney"
    ],
    "author_ids": [],
    "abstract": "Today, AI is increasingly being used in many high-stakes decision-making\napplications in which fairness is an important concern. Already, there are many\nexamples of AI being biased and making questionable and unfair decisions. The\nAI research community has proposed many methods to measure and mitigate\nunwanted biases, but few of them involve inputs from human policy makers. We\nargue that because different fairness criteria sometimes cannot be\nsimultaneously satisfied, and because achieving fairness often requires\nsacrificing other objectives such as model accuracy, it is key to acquire and\nadhere to human policy makers' preferences on how to make the tradeoff among\nthese objectives. In this paper, we propose a framework and some exemplar\nmethods for eliciting such preferences and for optimizing an AI model according\nto these preferences.",
    "published_date": "2020-02-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.01621v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.01607v2",
    "title": "Anomaly Detection by One Class Latent Regularized Networks",
    "authors": [
      "Chengwei Chen",
      "Pan Chen",
      "Haichuan Song",
      "Yiqing Tao",
      "Yuan Xie",
      "Shouhong Ding",
      "Lizhuang Ma"
    ],
    "author_ids": [],
    "abstract": "Anomaly detection is a fundamental problem in computer vision area with many\nreal-world applications. Given a wide range of images belonging to the normal\nclass, emerging from some distribution, the objective of this task is to\nconstruct the model to detect out-of-distribution images belonging to abnormal\ninstances. Semi-supervised Generative Adversarial Networks (GAN)-based methods\nhave been gaining popularity in anomaly detection task recently. However, the\ntraining process of GAN is still unstable and challenging. To solve these\nissues, a novel adversarial dual autoencoder network is proposed, in which the\nunderlying structure of training data is not only captured in latent feature\nspace, but also can be further restricted in the space of latent representation\nin a discriminant manner, leading to a more accurate detector. In addition, the\nauxiliary autoencoder regarded as a discriminator could obtain an more stable\ntraining process. Experiments show that our model achieves the state-of-the-art\nresults on MNIST and CIFAR10 datasets as well as GTSRB stop signs dataset.",
    "published_date": "2020-02-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.01607v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.01559v1",
    "title": "Whose Side are Ethics Codes On? Power, Responsibility and the Social Good",
    "authors": [
      "Anne L. Washington",
      "Rachel S. Kuo"
    ],
    "author_ids": [],
    "abstract": "The moral authority of ethics codes stems from an assumption that they serve\na unified society, yet this ignores the political aspects of any shared\nresource. The sociologist Howard S. Becker challenged researchers to clarify\ntheir power and responsibility in the classic essay: Whose Side Are We On.\nBuilding on Becker's hierarchy of credibility, we report on a critical\ndiscourse analysis of data ethics codes and emerging conceptualizations of\nbeneficence, or the \"social good\", of data technology. The analysis revealed\nthat ethics codes from corporations and professional associations conflated\nconsumers with society and were largely silent on agency. Interviews with\ncommunity organizers about social change in the digital era supplement the\nanalysis, surfacing the limits of technical solutions to concerns of\nmarginalized communities. Given evidence that highlights the gulf between the\ndocuments and lived experiences, we argue that ethics codes that elevate\nconsumers may simultaneously subordinate the needs of vulnerable populations.\nUnderstanding contested digital resources is central to the emerging field of\npublic interest technology. We introduce the concept of digital differential\nvulnerability to explain disproportionate exposures to harm within data\ntechnology and suggest recommendations for future ethics codes.",
    "published_date": "2020-02-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.01559v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.01077v1",
    "title": "Quantifying the Effects of Recommendation Systems",
    "authors": [
      "Sunshine Chong",
      "Andrés Abeliuk"
    ],
    "author_ids": [],
    "abstract": "Recommendation systems today exert a strong influence on consumer behavior\nand individual perceptions of the world. By using collaborative filtering (CF)\nmethods to create recommendations, it generates a continuous feedback loop in\nwhich user behavior becomes magnified in the algorithmic system. Popular items\nget recommended more frequently, creating the bias that affects and alters user\npreferences. In order to visualize and compare the different biases, we will\nanalyze the effects of recommendation systems and quantify the inequalities\nresulting from them.",
    "published_date": "2020-02-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.01077v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.00698v2",
    "title": "Fairness-Aware Hybrid Precoding for mmWave NOMA Unicast/Multicast Transmissions in Industrial IoT",
    "authors": [
      "Luis F. Abanto-Leon",
      "Gek Hong",
      "Sim"
    ],
    "author_ids": [],
    "abstract": "This paper investigates dual-layer non-orthogonally superimposed\ntransmissions for industrial internet of things (IoT) millimeter-wave\ncommunications. Essentially, the overlayer is a ubiquitous multicast signal\ndevised to serve all the devices in coverage with a common message, i.e.,\ncritical control packet. The underlayer is a composite signal that consists of\nprivate unicast messages. Due to safety implications, it is critical that all\ndevices can decode the multicast information. To ensure this requirement, we\njointly optimize the hybrid precoder, analog combiners, power allocation, and\nfairness. Specifically, we incorporate a power splitting constraint between the\ntwo overlaid signals and enforce supplementary per-device constraints to\nguarantee multicast fairness. Performance is evaluated in terms of the spectral\nefficiency, multicast fairness, and bit error rate, thus corroborating the\nfeasibility of our proposed scheme.",
    "published_date": "2020-02-03T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00698v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.00695v1",
    "title": "FAE: A Fairness-Aware Ensemble Framework",
    "authors": [
      "Vasileios Iosifidis",
      "Besnik Fetahu",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "Automated decision making based on big data and machine learning (ML)\nalgorithms can result in discriminatory decisions against certain protected\ngroups defined upon personal data like gender, race, sexual orientation etc.\nSuch algorithms designed to discover patterns in big data might not only pick\nup any encoded societal biases in the training data, but even worse, they might\nreinforce such biases resulting in more severe discrimination. The majority of\nthus far proposed fairness-aware machine learning approaches focus solely on\nthe pre-, in- or post-processing steps of the machine learning process, that\nis, input data, learning algorithms or derived models, respectively. However,\nthe fairness problem cannot be isolated to a single step of the ML process.\nRather, discrimination is often a result of complex interactions between big\ndata and algorithms, and therefore, a more holistic approach is required. The\nproposed FAE (Fairness-Aware Ensemble) framework combines fairness-related\ninterventions at both pre- and postprocessing steps of the data analysis\nprocess. In the preprocessing step, we tackle the problems of\nunder-representation of the protected group (group imbalance) and of\nclass-imbalance by generating balanced training samples. In the post-processing\nstep, we tackle the problem of class overlapping by shifting the decision\nboundary in the direction of fairness.",
    "published_date": "2020-02-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00695v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.00670v2",
    "title": "Learning-based Max-Min Fair Hybrid Precoding for mmWave Multicasting",
    "authors": [
      "Luis F. Abanto-Leon",
      "Gek Hong",
      "Sim"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the joint design of hybrid transmit precoder and\nanalog receive combiners for single-group multicasting in millimeter-wave\nsystems. We propose LB-GDM, a low-complexity learning-based approach that\nleverages gradient descent with momentum and alternating optimization to design\n(i) the digital and analog constituents of a hybrid transmitter and (ii) the\nanalog combiners of each receiver. In addition, we also extend our proposed\napproach to design fully-digital precoders. We show through numerical\nevaluation that, implementing LB-GDM in either hybrid or digital precoders\nattain superlative performance compared to competing designs based on\nsemidefinite relaxation. Specifically, in terms of minimum signal-to-noise\nratio, we report a remarkable improvement with gains of up to 105% and 101% for\nthe fully-digital and hybrid precoders, respectively.",
    "published_date": "2020-02-03T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00670v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.05679v1",
    "title": "Ethics of Food Recommender Applications",
    "authors": [
      "Daniel Karpati",
      "Amro Najjar",
      "Diego Agustin Ambrossio"
    ],
    "author_ids": [],
    "abstract": "The recent unprecedented popularity of food recommender applications has\nraised several issues related to the ethical, societal and legal implications\nof relying on these applications. In this paper, in order to assess the\nrelevant ethical issues, we rely on the emerging principles across the\nAI\\&Ethics community and define them tailored context specifically. Considering\nthe popular Food Recommender Systems (henceforth F-RS) in the European market\ncannot be regarded as personalised F-RS, we show how merely this lack of\nfeature shifts the relevance of the focal ethical concerns. We identify the\nmajor challenges and propose a scheme for how explicit ethical agendas should\nbe explained. We also argue how a multi-stakeholder approach is indispensable\nto ensure producing long-term benefits for all stakeholders. After proposing\neight ethical desiderata points for F-RS, we present a case-study and assess it\nbased on our proposed desiderata points.",
    "published_date": "2020-02-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.00484v1",
    "title": "DeepLocNet: Deep Observation Classification and Ranging Bias Regression for Radio Positioning Systems",
    "authors": [
      "Sahib Singh Dhanjal",
      "Maani Ghaffari",
      "Ryan M. Eustice"
    ],
    "author_ids": [],
    "abstract": "WiFi technology has been used pervasively in fine-grained indoor\nlocalization, gesture recognition, and adaptive communication. Achieving better\nperformance in these tasks generally boils down to differentiating\nLine-Of-Sight (LOS) from Non-Line-Of-Sight (NLOS) signal propagation reliably\nwhich generally requires expensive/specialized hardware due to the complex\nnature of indoor environments. Hence, the development of low-cost accurate\npositioning systems that exploit available infrastructure is not entirely\nsolved. In this paper, we develop a framework for indoor localization and\ntracking of ubiquitous mobile devices such as smartphones using on-board\nsensors. We present a novel deep LOS/NLOS classifier which uses the Received\nSignal Strength Indicator (RSSI), and can classify the input signal with an\naccuracy of 85\\%. The proposed algorithm can globally localize and track a\nsmartphone (or robot) with a priori unknown location, and with a semi-accurate\nprior map (error within 0.8 m) of the WiFi Access Points (AP). Through\nsimultaneously solving for the trajectory and the map of access points, we\nrecover a trajectory of the device and corrected locations for the access\npoints. Experimental evaluations of the framework show that localization\naccuracy is increased by using the trained deep network; furthermore, the\nsystem becomes robust to any error in the map of APs.",
    "published_date": "2020-02-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00484v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2002.00453v1",
    "title": "DropClass and DropAdapt: Dropping classes for deep speaker representation learning",
    "authors": [
      "Chau Luu",
      "Peter Bell",
      "Steve Renals"
    ],
    "author_ids": [],
    "abstract": "Many recent works on deep speaker embeddings train their feature extraction\nnetworks on large classification tasks, distinguishing between all speakers in\na training set. Empirically, this has been shown to produce\nspeaker-discriminative embeddings, even for unseen speakers. However, it is not\nclear that this is the optimal means of training embeddings that generalize\nwell. This work proposes two approaches to learning embeddings, based on the\nnotion of dropping classes during training. We demonstrate that both approaches\ncan yield performance gains in speaker verification tasks. The first proposed\nmethod, DropClass, works via periodically dropping a random subset of classes\nfrom the training data and the output layer throughout training, resulting in a\nfeature extractor trained on many different classification tasks. Combined with\nan additive angular margin loss, this method can yield a 7.9% relative\nimprovement in equal error rate (EER) over a strong baseline on VoxCeleb. The\nsecond proposed method, DropAdapt, is a means of adapting a trained model to a\nset of enrolment speakers in an unsupervised manner. This is performed by\nfine-tuning a model on only those classes which produce high probability\npredictions when the enrolment speakers are used as input, again also dropping\nthe relevant rows from the output layer. This method yields a large 13.2%\nrelative improvement in EER on VoxCeleb. The code for this paper has been made\npublicly available.",
    "published_date": "2020-02-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00453v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.00213v2",
    "title": "Machine Ethics: The Creation of a Virtuous Machine",
    "authors": [
      "Mohamed Akrout",
      "Robert Steinbauer"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) was initially developed as an implicit moral\nagent to solve simple and clearly defined tasks where all options are\npredictable. However, it is now part of our daily life powering cell phones,\ncameras, watches, thermostats, vacuums, cars, and much more. This has raised\nnumerous concerns and some scholars and practitioners stress the dangers of AI\nand argue against its development as moral agents that can reason about ethics\n(e.g., Bryson 2008; Johnson and Miller 2008; Sharkey 2017; Tonkens 2009; van\nWynsberghe and Robbins 2019). Even though we acknowledge the potential threat,\nin line with most other scholars (e.g., Anderson and Anderson 2010; Moor 2006;\nScheutz 2016; Wallach 2010), we argue that AI advancements cannot be stopped\nand developers need to prepare AI to sustain explicit moral agents and face\nethical dilemmas in complex and morally salient environments.",
    "published_date": "2020-02-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00213v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.00189v2",
    "title": "The Statistical Complexity of Early-Stopped Mirror Descent",
    "authors": [
      "Tomas Vaškevičius",
      "Varun Kanade",
      "Patrick Rebeschini"
    ],
    "author_ids": [],
    "abstract": "Recently there has been a surge of interest in understanding implicit\nregularization properties of iterative gradient-based optimization algorithms.\nIn this paper, we study the statistical guarantees on the excess risk achieved\nby early-stopped unconstrained mirror descent algorithms applied to the\nunregularized empirical risk with the squared loss for linear models and kernel\nmethods. By completing an inequality that characterizes convexity for the\nsquared loss, we identify an intrinsic link between offset Rademacher\ncomplexities and potential-based convergence analysis of mirror descent\nmethods. Our observation immediately yields excess risk guarantees for the path\ntraced by the iterates of mirror descent in terms of offset complexities of\ncertain function classes depending only on the choice of the mirror map,\ninitialization point, step-size, and the number of iterations. We apply our\ntheory to recover, in a clean and elegant manner via rather short proofs, some\nof the recent results in the implicit regularization literature, while also\nshowing how to improve upon them in some settings.",
    "published_date": "2020-02-01T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00189v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.00065v1",
    "title": "Analysis of Gender Inequality In Face Recognition Accuracy",
    "authors": [
      "Vítor Albiero",
      "Krishnapriya K. S.",
      "Kushal Vangara",
      "Kai Zhang",
      "Michael C. King",
      "Kevin W. Bowyer"
    ],
    "author_ids": [],
    "abstract": "We present a comprehensive analysis of how and why face recognition accuracy\ndiffers between men and women. We show that accuracy is lower for women due to\nthe combination of (1) the impostor distribution for women having a skew toward\nhigher similarity scores, and (2) the genuine distribution for women having a\nskew toward lower similarity scores. We show that this phenomenon of the\nimpostor and genuine distributions for women shifting closer towards each other\nis general across datasets of African-American, Caucasian, and Asian faces. We\nshow that the distribution of facial expressions may differ between\nmale/female, but that the accuracy difference persists for image subsets rated\nconfidently as neutral expression. The accuracy difference also persists for\nimage subsets rated as close to zero pitch angle. Even when removing images\nwith forehead partially occluded by hair/hat, the same impostor/genuine\naccuracy difference persists. We show that the female genuine distribution\nimproves when only female images without facial cosmetics are used, but that\nthe female impostor distribution also degrades at the same time. Lastly, we\nshow that the accuracy difference persists even if a state-of-the-art deep\nlearning method is trained from scratch using training data explicitly balanced\nbetween male and female images and subjects.",
    "published_date": "2020-01-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.00065v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11990v2",
    "title": "Deontological Ethics By Monotonicity Shape Constraints",
    "authors": [
      "Serena Wang",
      "Maya Gupta"
    ],
    "author_ids": [],
    "abstract": "We demonstrate how easy it is for modern machine-learned systems to violate\ncommon deontological ethical principles and social norms such as \"favor the\nless fortunate,\" and \"do not penalize good attributes.\" We propose that in some\ncases such ethical principles can be incorporated into a machine-learned model\nby adding shape constraints that constrain the model to respond only positively\nto relevant inputs. We analyze the relationship between these deontological\nconstraints that act on individuals and the consequentialist group-based\nfairness goals of one-sided statistical parity and equal opportunity. This\nstrategy works with sensitive attributes that are Boolean or real-valued such\nas income and age, and can help produce more responsible and trustworthy AI.",
    "published_date": "2020-01-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11990v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11983v1",
    "title": "Cooperative Energy Scheduling for Microgrids under Peak Demand Energy Plans",
    "authors": [
      "Amir Valibeygi",
      "Raymond A de Callafon"
    ],
    "author_ids": [],
    "abstract": "A cooperative energy scheduling method is proposed that allows joint energy\noptimization for a group of microgrids to achieve cost savings that the\nmicrogrids could not achieve individually. The discussed microgrids may be\ncommercial entities in a distribution network under utility electricity rate\nplans comprising both Time of Use (ToU) and peak demand charge. Defining a\nstable operation as a situation where all microgrids would be willing to\nparticipate, it is shown that under such rate plans and in particular due to\nthe peak demand charge, a cost distribution that is seemingly fair does not\nnecessarily result in a stable cooperation. These results are derived in this\npaper using concepts from cooperative games. It is therefore sought to devise a\nstable cost distribution algorithm that, while maximizing some measure of\nfairness among the participating microgrids, ensures they all benefit from\ntheir participation. A simple case study is presented that demonstrates\nfairness and stability aspects of the cooperation.",
    "published_date": "2020-01-31T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11983v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.11905v3",
    "title": "Verifying Tree Ensembles by Reasoning about Potential Instances",
    "authors": [
      "Laurens Devos",
      "Wannes Meert",
      "Jesse Davis"
    ],
    "author_ids": [],
    "abstract": "Imagine being able to ask questions to a black box model such as \"Which\nadversarial examples exist?\", \"Does a specific attribute have a\ndisproportionate effect on the model's prediction?\" or \"What kind of\npredictions could possibly be made for a partially described example?\" This\nlast question is particularly important if your partial description does not\ncorrespond to any observed example in your data, as it provides insight into\nhow the model will extrapolate to unseen data. These capabilities would be\nextremely helpful as they would allow a user to better understand the model's\nbehavior, particularly as it relates to issues such as robustness, fairness,\nand bias. In this paper, we propose such an approach for an ensemble of trees.\nSince, in general, this task is intractable we present a strategy that (1) can\nprune part of the input space given the question asked to simplify the problem;\nand (2) follows a divide and conquer approach that is incremental and can\nalways return some answers and indicates which parts of the input domains are\nstill uncertain. The usefulness of our approach is shown on a diverse set of\nuse cases.",
    "published_date": "2020-01-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11905v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11691v2",
    "title": "Self-Adversarial Learning with Comparative Discrimination for Text Generation",
    "authors": [
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Furu Wei",
      "Ming Zhou"
    ],
    "author_ids": [],
    "abstract": "Conventional Generative Adversarial Networks (GANs) for text generation tend\nto have issues of reward sparsity and mode collapse that affect the quality and\ndiversity of generated samples. To address the issues, we propose a novel\nself-adversarial learning (SAL) paradigm for improving GANs' performance in\ntext generation. In contrast to standard GANs that use a binary classifier as\nits discriminator to predict whether a sample is real or generated, SAL employs\na comparative discriminator which is a pairwise classifier for comparing the\ntext quality between a pair of samples. During training, SAL rewards the\ngenerator when its currently generated sentence is found to be better than its\npreviously generated samples. This self-improvement reward mechanism allows the\nmodel to receive credits more easily and avoid collapsing towards the limited\nnumber of real samples, which not only helps alleviate the reward sparsity\nissue but also reduces the risk of mode collapse. Experiments on text\ngeneration benchmark datasets show that our proposed approach substantially\nimproves both the quality and the diversity, and yields more stable performance\ncompared to the previous GANs for text generation.",
    "published_date": "2020-01-31T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11691v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11585v1",
    "title": "Housing Search in the Age of Big Data: Smarter Cities or the Same Old Blind Spots?",
    "authors": [
      "Geoff Boeing",
      "Max Besbris",
      "Ariela Schachter",
      "John Kuk"
    ],
    "author_ids": [],
    "abstract": "Housing scholars stress the importance of the information environment in\nshaping housing search behavior and outcomes. Rental listings have increasingly\nmoved online over the past two decades and, in turn, online platforms like\nCraigslist are now central to the search process. Do these technology platforms\nserve as information equalizers or do they reflect traditional information\ninequalities that correlate with neighborhood sociodemographics? We synthesize\nand extend analyses of millions of US Craigslist rental listings and find they\nsupply significantly different volumes, quality, and types of information in\ndifferent communities. Technology platforms have the potential to broaden,\ndiversify, and equalize housing search information, but they rely on landlord\nbehavior and, in turn, likely will not reach this potential without a\nsignificant redesign or policy intervention. Smart cities advocates hoping to\nbuild better cities through technology must critically interrogate technology\nplatforms and big data for systematic biases.",
    "published_date": "2020-01-30T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.CY",
      "econ.GN",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11585v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.11401v1",
    "title": "Visuohaptic augmented feedback for enhancing motor skills acquisition",
    "authors": [
      "Ali Asadipour",
      "Kurt Debattista",
      "Alan Chalmers"
    ],
    "author_ids": [],
    "abstract": "Serious games are accepted as an effective approach to deliver augmented\nfeedback in motor (re-) learning processes. The multi-modal nature of the\nconventional computer games (e.g. audiovisual representation) plus the ability\nto interact via haptic-enabled inputs provides a more immersive experience.\nThus, particular disciplines such as medical education in which frequent hands\non rehearsals play a key role in learning core motor skills (e.g. physical\npalpations) may benefit from this technique. Challenges such as the\nimpracticality of verbalising palpation experience by tutors and ethical\nconsiderations may prevent the medical students from correctly learning core\npalpation skills. This work presents a new data glove, built from off-the-shelf\ncomponents which captures pressure sensitivity designed to provide feedback for\npalpation tasks. In this work the data glove is used to control a serious game\nadapted from the infinite runner genre to improve motor skill acquisition. A\ncomparative evaluation on usability and effectiveness of the method using\nmultimodal visualisations, as part of a larger study to enhance pressure\nsensitivity, is presented. Thirty participants divided into a game-playing\ngroup (n = 15) and a control group (n = 15) were invited to perform a simple\npalpation task. The game-playing group significantly outperformed the control\ngroup in which abstract visualisation of force was provided to the users in a\nblind-folded transfer test. The game-based training approach was positively\ndescribed by the game-playing group as enjoyable and engaging.",
    "published_date": "2020-01-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "eess.SP",
      "H.5.2; D.2.2; H.1.2; I.3.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11401v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2002.05657v1",
    "title": "Trustworthy AI in the Age of Pervasive Computing and Big Data",
    "authors": [
      "Abhishek Kumar",
      "Tristan Braud",
      "Sasu Tarkoma",
      "Pan Hui"
    ],
    "author_ids": [],
    "abstract": "The era of pervasive computing has resulted in countless devices that\ncontinuously monitor users and their environment, generating an abundance of\nuser behavioural data. Such data may support improving the quality of service,\nbut may also lead to adverse usages such as surveillance and advertisement. In\nparallel, Artificial Intelligence (AI) systems are being applied to sensitive\nfields such as healthcare, justice, or human resources, raising multiple\nconcerns on the trustworthiness of such systems. Trust in AI systems is thus\nintrinsically linked to ethics, including the ethics of algorithms, the ethics\nof data, or the ethics of practice. In this paper, we formalise the\nrequirements of trustworthy AI systems through an ethics perspective. We\nspecifically focus on the aspects that can be integrated into the design and\ndevelopment of AI systems. After discussing the state of research and the\nremaining challenges, we show how a concrete use-case in smart cities can\nbenefit from these methods.",
    "published_date": "2020-01-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.05657v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11171v1",
    "title": "Going beyond accuracy: estimating homophily in social networks using predictions",
    "authors": [
      "George Berry",
      "Antonio Sirianni",
      "Ingmar Weber",
      "Jisun An",
      "Michael Macy"
    ],
    "author_ids": [],
    "abstract": "In online social networks, it is common to use predictions of node categories\nto estimate measures of homophily and other relational properties. However,\nonline social network data often lacks basic demographic information about the\nnodes. Researchers must rely on predicted node attributes to estimate measures\nof homophily, but little is known about the validity of these measures. We show\nthat estimating homophily in a network can be viewed as a dyadic prediction\nproblem, and that homophily estimates are unbiased when dyad-level residuals\nsum to zero in the network. Node-level prediction models, such as the use of\nnames to classify ethnicity or gender, do not generally have this property and\ncan introduce large biases into homophily estimates. Bias occurs due to error\nautocorrelation along dyads. Importantly, node-level classification performance\nis not a reliable indicator of estimation accuracy for homophily. We compare\nestimation strategies that make predictions at the node and dyad levels,\nevaluating performance in different settings. We propose a novel \"ego-alter\"\nmodeling approach that outperforms standard node and dyad classification\nstrategies. While this paper focuses on homophily, results generalize to other\nrelational measures which aggregate predictions along the dyads in a network.\nWe conclude with suggestions for research designs to study homophily in online\nnetworks. Code for this paper is available at\nhttps://github.com/georgeberry/autocorr.",
    "published_date": "2020-01-30T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11171v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11114v6",
    "title": "A Family of Pairwise Multi-Marginal Optimal Transports that Define a Generalized Metric",
    "authors": [
      "Liang Mi",
      "Azadeh Sheikholeslami",
      "José Bento"
    ],
    "author_ids": [],
    "abstract": "The Optimal transport (OT) problem is rapidly finding its way into machine\nlearning. Favoring its use are its metric properties. Many problems admit\nsolutions with guarantees only for objects embedded in metric spaces, and the\nuse of non-metrics can complicate solving them. Multi-marginal OT (MMOT)\ngeneralizes OT to simultaneously transporting multiple distributions. It\ncaptures important relations that are missed if the transport only involves two\ndistributions. Research on MMOT, however, has been focused on its existence,\nuniqueness, practical algorithms, and the choice of cost functions. There is a\nlack of discussion on the metric properties of MMOT, which limits its\ntheoretical and practical use. Here, we prove new generalized metric properties\nfor a family of pairwise MMOTs. We first explain the difficulty of proving this\nvia two negative results. Afterward, we prove the MMOTs' metric properties.\nFinally, we show that the generalized triangle inequality of this family of\nMMOTs cannot be improved. We illustrate the superiority of our MMOTs over other\ngeneralized metrics, and over non-metrics in both synthetic and real tasks.",
    "published_date": "2020-01-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DM",
      "math.FA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11114v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.10996v1",
    "title": "Functional Sequential Treatment Allocation with Covariates",
    "authors": [
      "Anders Bredahl Kock",
      "David Preinerstorfer",
      "Bezirgen Veliyev"
    ],
    "author_ids": [],
    "abstract": "We consider a multi-armed bandit problem with covariates. Given a realization\nof the covariate vector, instead of targeting the treatment with highest\nconditional expectation, the decision maker targets the treatment which\nmaximizes a general functional of the conditional potential outcome\ndistribution, e.g., a conditional quantile, trimmed mean, or a socio-economic\nfunctional such as an inequality, welfare or poverty measure. We develop\nexpected regret lower bounds for this problem, and construct a near minimax\noptimal assignment policy.",
    "published_date": "2020-01-29T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.10996v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.10994v1",
    "title": "Credit Scoring for Good: Enhancing Financial Inclusion with Smartphone-Based Microlending",
    "authors": [
      "María Óskarsdóttir",
      "Cristián Bravo",
      "Carlos Sarraute",
      "Bart Baesens",
      "Jan Vanthienen"
    ],
    "author_ids": [],
    "abstract": "Globally, two billion people and more than half of the poorest adults do not\nuse formal financial services. Consequently, there is increased emphasis on\ndeveloping financial technology that can facilitate access to financial\nproducts for the unbanked. In this regard, smartphone-based microlending has\nemerged as a potential solution to enhance financial inclusion.\n  We propose a methodology to improve the predictive performance of credit\nscoring models used by these applications. Our approach is composed of several\nsteps, where we mostly focus on engineering appropriate features from the user\ndata. Thereby, we construct pseudo-social networks to identify similar people\nand combine complex network analysis with representation learning. Subsequently\nwe build credit scoring models using advanced machine learning techniques with\nthe goal of obtaining the most accurate credit scores, while also taking into\nconsideration ethical and privacy regulations to avoid unfair discrimination. A\nsuccessful deployment of our proposed methodology could improve the performance\nof microlending smartphone applications and help enhance financial wellbeing\nworldwide.",
    "published_date": "2020-01-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.10994v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.10966v1",
    "title": "Developing a gender classification approach in human face images using modified local binary patterns and tani-moto based nearest neighbor algorithm",
    "authors": [
      "Shervan Fekri-Ershad"
    ],
    "author_ids": [],
    "abstract": "Human identification is a much attention problem in computer vision. Gender\nclassification plays an important role in human identification as preprocess\nstep. So far, various methods have been proposed to solve this problem.\nAbsolutely, classification accuracy is the main challenge for researchers in\ngender classification. But, some challenges such as rotation, gray scale\nvariations, pose, illumination changes may be occurred in smart phone image\ncapturing. In this respect, a multi step approach is proposed in this paper to\nclassify genders in human face images based on improved local binary patters\n(MLBP). LBP is a texture descriptor, which extract local contrast and local\nspatial structure information. Some issues such as noise sensitivity, rotation\nsensitivity and low discriminative features can be considered as disadvantages\nof the basic LBP. MLBP handle disadvantages using a new theory to categorize\nextracted binary patterns of basic LBP. The proposed approach includes two\nstages. First of all, a feature vector is extracted for human face images based\non MLBP. Next, non linear classifiers can be used to classify gender. In this\npaper nearest neighborhood classifier is evaluated based on Tani-Moto metric as\ndistance measure. In the result part, two databases, self-collected and ICPR\nare used as human face database. Results are compared by some state-ofthe-art\nalgorithms in this literature that shows the high quality of the proposed\napproach in terms of accuracy rate. Some of other main advantages of the\nproposed approach are rotation invariant, low noise sensitivity, size invariant\nand low computational complexity. The proposed approach decreases the\ncomputational complexity of smartphone applications because of reducing the\nnumber of database comparisons. It can also improve performance of the\nsynchronous applications in the smarphones because of memory and CPU usage\nreduction.",
    "published_date": "2020-01-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.10966v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11349v1",
    "title": "On Constraint Definability in Tractable Probabilistic Models",
    "authors": [
      "Ioannis Papantonis",
      "Vaishak Belle"
    ],
    "author_ids": [],
    "abstract": "Incorporating constraints is a major concern in probabilistic machine\nlearning. A wide variety of problems require predictions to be integrated with\nreasoning about constraints, from modelling routes on maps to approving loan\npredictions. In the former, we may require the prediction model to respect the\npresence of physical paths between the nodes on the map, and in the latter, we\nmay require that the prediction model respect fairness constraints that ensure\nthat outcomes are not subject to bias. Broadly speaking, constraints may be\nprobabilistic, logical or causal, but the overarching challenge is to determine\nif and how a model can be learnt that handles all the declared constraints. To\nthe best of our knowledge, this is largely an open problem. In this paper, we\nconsider a mathematical inquiry on how the learning of tractable probabilistic\nmodels, such as sum-product networks, is possible while incorporating\nconstraints.",
    "published_date": "2020-01-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11349v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11358v2",
    "title": "Correcting for Selection Bias in Learning-to-rank Systems",
    "authors": [
      "Zohreh Ovaisi",
      "Ragib Ahsan",
      "Yifan Zhang",
      "Kathryn Vasilaky",
      "Elena Zheleva"
    ],
    "author_ids": [],
    "abstract": "Click data collected by modern recommendation systems are an important source\nof observational data that can be utilized to train learning-to-rank (LTR)\nsystems. However, these data suffer from a number of biases that can result in\npoor performance for LTR systems. Recent methods for bias correction in such\nsystems mostly focus on position bias, the fact that higher ranked results\n(e.g., top search engine results) are more likely to be clicked even if they\nare not the most relevant results given a user's query. Less attention has been\npaid to correcting for selection bias, which occurs because clicked documents\nare reflective of what documents have been shown to the user in the first\nplace. Here, we propose new counterfactual approaches which adapt Heckman's\ntwo-stage method and accounts for selection and position bias in LTR systems.\nOur empirical evaluation shows that our proposed methods are much more robust\nto noise and have better accuracy compared to existing unbiased LTR algorithms,\nespecially when there is moderate to no position bias.",
    "published_date": "2020-01-29T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11358v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.10600v2",
    "title": "Prophet Inequalities with Linear Correlations and Augmentations",
    "authors": [
      "Nicole Immorlica",
      "Sahil Singla",
      "Bo Waggoner"
    ],
    "author_ids": [],
    "abstract": "In a classical online decision problem, a decision-maker who is trying to\nmaximize her value inspects a sequence of arriving items to learn their values\n(drawn from known distributions), and decides when to stop the process by\ntaking the current item. The goal is to prove a \"prophet inequality\": that she\ncan do approximately as well as a prophet with foreknowledge of all the values.\nIn this work, we investigate this problem when the values are allowed to be\ncorrelated. Since non-trivial guarantees are impossible for arbitrary\ncorrelations, we consider a natural \"linear\" correlation structure introduced\nby Bateni et al. [ESA 2015] as a generalization of the common-base value model\nof Chawla et al. [GEB 2015].\n  A key challenge is that threshold-based algorithms, which are commonly used\nfor prophet inequalities, no longer guarantee good performance for linear\ncorrelations. We relate this roadblock to another \"augmentations\" challenge\nthat might be of independent interest: many existing prophet inequality\nalgorithms are not robust to slight increase in the values of the arriving\nitems. We leverage this intuition to prove bounds (matching up to constant\nfactors) that decay gracefully with the amount of correlation of the arriving\nitems. We extend these results to the case of selecting multiple items by\ndesigning a new $(1+o(1))$ approximation ratio algorithm that is robust to\naugmentations.",
    "published_date": "2020-01-28T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.10600v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09911v2",
    "title": "The Empirical Core of the Multicommodity Flow Game Without Side Payments",
    "authors": [
      "Coulter Beeson",
      "Bruce Shepherd"
    ],
    "author_ids": [],
    "abstract": "Policy makers focus on stable strategies as the ones adopted by rational\nplayers. If there are many such solutions an important question is how to\nselect amongst them. We study this question for the Multicommodity Flow\nCoalition Game, used to model cooperation between autonomous systems (AS) in\nthe Internet. In short, strategies are flows in a capacitated network. The\npayoff to a node is the total flow which it terminates. Markakis-Saberi show\nthis game is balanced and hence has a non-empty core by Scarf's Theorem. In the\ntransferable utility (TU) version this gives a polynomial-time algorithm to\nfind core elements, but for ASs side payments are not natural. Finding core\nelements in NTU games tends to be computationally more difficult. For this\ngame, the only previous result gives a procedure to find a core element when\nthe supply graph is a path. We extend their work with an algorithm,\nIncorporate, which produces many different core elements.\n  We use our algorithm to evaluate specific instances by generating many core\nvectors. We call these the Empirical Core for a game. We find that sampled core\nvectors are more consistent with respect to social welfare (SW) than for\nfairness (minimum payout). For SW they tend to do as well as the optimal linear\nprogram value $LP_{sw}$. In contrast, there is a larger range in fairness for\nthe empirical core; the fairness values tend to be worse than the optimal\nfairness LP value $LP_{fair}$. We study this discrepancy in the setting of\ngeneral graphs with single-sink demands. In this setting we give an algorithm\nwhich produces core vectors that simultaneously maximize SW and fairness. This\nleads to the following bicriteria result for general games. Given any\ncore-producing algorithm and any $\\lambda \\in (0,1)$, one can produce an\napproximate core vector with fairness (resp. social welfare) at least $\\lambda\nLP_{fair}$ (resp $(1-\\lambda) LP_{sw}$).",
    "published_date": "2020-01-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09911v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09691v2",
    "title": "Multi-Modal Domain Adaptation for Fine-Grained Action Recognition",
    "authors": [
      "Jonathan Munro",
      "Dima Damen"
    ],
    "author_ids": [],
    "abstract": "Fine-grained action recognition datasets exhibit environmental bias, where\nmultiple video sequences are captured from a limited number of environments.\nTraining a model in one environment and deploying in another results in a drop\nin performance due to an unavoidable domain shift. Unsupervised Domain\nAdaptation (UDA) approaches have frequently utilised adversarial training\nbetween the source and target domains. However, these approaches have not\nexplored the multi-modal nature of video within each domain. In this work we\nexploit the correspondence of modalities as a self-supervised alignment\napproach for UDA in addition to adversarial alignment.\n  We test our approach on three kitchens from our large-scale dataset,\nEPIC-Kitchens, using two modalities commonly employed for action recognition:\nRGB and Optical Flow. We show that multi-modal self-supervision alone improves\nthe performance over source-only training by 2.4% on average. We then combine\nadversarial training with multi-modal self-supervision, showing that our\napproach outperforms other UDA methods by 3%.",
    "published_date": "2020-01-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09691v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09604v1",
    "title": "Factors Influencing Perceived Fairness in Algorithmic Decision-Making: Algorithm Outcomes, Development Procedures, and Individual Differences",
    "authors": [
      "Ruotong Wang",
      "F. Maxwell Harper",
      "Haiyi Zhu"
    ],
    "author_ids": [],
    "abstract": "Algorithmic decision-making systems are increasingly used throughout the\npublic and private sectors to make important decisions or assist humans in\nmaking these decisions with real social consequences. While there has been\nsubstantial research in recent years to build fair decision-making algorithms,\nthere has been less research seeking to understand the factors that affect\npeople's perceptions of fairness in these systems, which we argue is also\nimportant for their broader acceptance. In this research, we conduct an online\nexperiment to better understand perceptions of fairness, focusing on three sets\nof factors: algorithm outcomes, algorithm development and deployment\nprocedures, and individual differences. We find that people rate the algorithm\nas more fair when the algorithm predicts in their favor, even surpassing the\nnegative effects of describing algorithms that are very biased against\nparticular demographic groups. We find that this effect is moderated by several\nvariables, including participants' education level, gender, and several aspects\nof the development procedure. Our findings suggest that systems that evaluate\nalgorithmic fairness through users' feedback must consider the possibility of\noutcome favorability bias.",
    "published_date": "2020-01-27T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09604v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09591v4",
    "title": "The complexity of solution sets to equations in hyperbolic groups",
    "authors": [
      "Laura Ciobanu",
      "Murray Elder"
    ],
    "author_ids": [],
    "abstract": "We show that the full set of solutions to systems of equations and\ninequations in a hyperbolic group, as shortlex geodesic words (or any regular\nset of quasigeodesic normal forms), is an EDT0L language whose specification\ncan be computed in NSPACE$(n^2\\log n)$ for the torsion-free case and\nNSPACE$(n^4\\log n)$ in the torsion case. Furthermore, in the presence of\nquasi-isometrically embeddable rational constraints, we show that the full set\nof solutions to systems of equations in a hyperbolic group remains EDT0L. Our\nwork combines the geometric results of Rips, Sela, Dahmani and Guirardel on the\ndecidability of the existential theory of hyperbolic groups with the work of\ncomputer scientists including Plandowski, Je\\.z, Diekert and others on PSPACE\nalgorithms to solve equations in free monoids and groups using compression, and\ninvolves an intricate language-theoretic analysis.",
    "published_date": "2020-01-27T00:00:00",
    "year": 2020,
    "categories": [
      "math.GR",
      "cs.FL",
      "03D05, 20F65, 20F70, 68Q25, 68Q45"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09591v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09455v1",
    "title": "Estimating Error and Bias in Offline Evaluation Results",
    "authors": [
      "Mucun Tian",
      "Michael D. Ekstrand"
    ],
    "author_ids": [],
    "abstract": "Offline evaluations of recommender systems attempt to estimate users'\nsatisfaction with recommendations using static data from prior user\ninteractions. These evaluations provide researchers and developers with first\napproximations of the likely performance of a new system and help weed out bad\nideas before presenting them to users. However, offline evaluation cannot\naccurately assess novel, relevant recommendations, because the most novel items\nwere previously unknown to the user, so they are missing from the historical\ndata and cannot be judged as relevant.\n  We present a simulation study to estimate the error that such missing data\ncauses in commonly-used evaluation metrics in order to assess its prevalence\nand impact. We find that missing data in the rating or observation process\ncauses the evaluation protocol to systematically mis-estimate metric values,\nand in some cases erroneously determine that a popularity-based recommender\noutperforms even a perfect personalized recommender. Substantial breakthroughs\nin recommendation quality, therefore, will be difficult to assess with existing\noffline techniques.",
    "published_date": "2020-01-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09455v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.11591v2",
    "title": "Scalable and Customizable Benchmark Problems for Many-Objective Optimization",
    "authors": [
      "Ivan Reinaldo Meneghini",
      "Marcos Antonio Alves",
      "António Gaspar-Cunha",
      "Frederico Gadelha Guimarães"
    ],
    "author_ids": [],
    "abstract": "Solving many-objective problems (MaOPs) is still a significant challenge in\nthe multi-objective optimization (MOO) field. One way to measure algorithm\nperformance is through the use of benchmark functions (also called test\nfunctions or test suites), which are artificial problems with a well-defined\nmathematical formulation, known solutions and a variety of features and\ndifficulties. In this paper we propose a parameterized generator of scalable\nand customizable benchmark problems for MaOPs. It is able to generate problems\nthat reproduce features present in other benchmarks and also problems with some\nnew features. We propose here the concept of generative benchmarking, in which\none can generate an infinite number of MOO problems, by varying parameters that\ncontrol specific features that the problem should have: scalability in the\nnumber of variables and objectives, bias, deceptiveness, multimodality, robust\nand non-robust solutions, shape of the Pareto front, and constraints. The\nproposed Generalized Position-Distance (GPD) tunable benchmark generator uses\nthe position-distance paradigm, a basic approach to building test functions,\nused in other benchmarks such as Deb, Thiele, Laumanns and Zitzler (DTLZ),\nWalking Fish Group (WFG) and others. It includes scalable problems in any\nnumber of variables and objectives and it presents Pareto fronts with different\ncharacteristics. The resulting functions are easy to understand and visualize,\neasy to implement, fast to compute and their Pareto optimal solutions are\nknown.",
    "published_date": "2020-01-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11591v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09434v1",
    "title": "Block the blocker: Studying the effects of Anti Ad-blocking",
    "authors": [
      "Rohit Gupta",
      "Rohit Panda"
    ],
    "author_ids": [],
    "abstract": "Advertisements generate huge chunks of revenues for websites and online\nbusinesses. Ad-blocker and tracker blocking programs have gained momentum in\nthe last few years with massive debates raging on privacy concerns and\nimproving user experience online. Acceptable Ads programme and Anti Ad-blockers\nare primary elements emerging in recent years that combat ad-blockers.\n  In this paper, we discuss at length data collection of top websites in the\nworld, Germany, DACH region and news category. We generate feature based A/B\ntesting metrics and employ classifier evaluations on them along with then\nanalysing the result. Our paper also discusses how Anti Ad-blockers impact the\neconomic, legal and ethical usage in Germany along with the recent changes in\nGDPR while taking a look at Acceptable ads programme and Whitelisting.",
    "published_date": "2020-01-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09434v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09394v2",
    "title": "Lagrangian Duality for Constrained Deep Learning",
    "authors": [
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck",
      "Terrence WK Mak",
      "Cuong Tran",
      "Federico Baldo",
      "Michele Lombardi"
    ],
    "author_ids": [],
    "abstract": "This paper explores the potential of Lagrangian duality for learning\napplications that feature complex constraints. Such constraints arise in many\nscience and engineering domains, where the task amounts to learning\noptimization problems which must be solved repeatedly and include hard physical\nand operational constraints. The paper also considers applications where the\nlearning task must enforce constraints on the predictor itself, either because\nthey are natural properties of the function to learn or because it is desirable\nfrom a societal standpoint to impose them. This paper demonstrates\nexperimentally that Lagrangian duality brings significant benefits for these\napplications. In energy domains, the combination of Lagrangian duality and deep\nlearning can be used to obtain state-of-the-art results to predict optimal\npower flows, in energy systems, and optimal compressor settings, in gas\nnetworks. In transprecision computing, Lagrangian duality can complement deep\nlearning to impose monotonicity constraints on the predictor without\nsacrificing accuracy. Finally, Lagrangian duality can be used to enforce\nfairness constraints on a predictor and obtain state-of-the-art results when\nminimizing disparate treatments.",
    "published_date": "2020-01-26T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09394v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09328v3",
    "title": "On the Fairness of Randomized Trials for Recommendation with Heterogeneous Demographics and Beyond",
    "authors": [
      "Zifeng Wang",
      "Xi Chen",
      "Rui Wen",
      "Shao-Lun Huang"
    ],
    "author_ids": [],
    "abstract": "Observed events in recommendation are consequence of the decisions made by a\npolicy, thus they are usually selectively labeled, namely the data are Missing\nNot At Random (MNAR), which often causes large bias to the estimate of true\noutcomes risk. A general approach to correct MNAR bias is performing small\nRandomized Controlled Trials (RCTs), where an additional uniform policy is\nemployed to randomly assign items to each user. In this work, we concentrate on\nthe fairness of RCTs under both homogeneous and heterogeneous demographics,\nespecially analyzing the bias for the least favorable group on the latter\nsetting. Considering RCTs' limitations, we propose a novel Counterfactual\nRobust Risk Minimization (CRRM) framework, which is totally free of expensive\nRCTs, and derive its theoretical generalization error bound. At last, empirical\nexperiments are performed on synthetic tasks and real-world data sets,\nsubstantiating our method's superiority both in fairness and generalization.",
    "published_date": "2020-01-25T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09328v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09233v1",
    "title": "Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions",
    "authors": [
      "Kit T. Rodolfa",
      "Erika Salomon",
      "Lauren Haynes",
      "Ivan Higuera Mendieta",
      "Jamie Larson",
      "Rayid Ghani"
    ],
    "author_ids": [],
    "abstract": "The criminal justice system is currently ill-equipped to improve outcomes of\nindividuals who cycle in and out of the system with a series of misdemeanor\noffenses. Often due to constraints of caseload and poor record linkage, prior\ninteractions with an individual may not be considered when an individual comes\nback into the system, let alone in a proactive manner through the application\nof diversion programs. The Los Angeles City Attorney's Office recently created\na new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing\nrecidivism in this population. Here we describe a collaboration with this new\nunit as a case study for the incorporation of predictive equity into machine\nlearning based decision making in a resource-constrained setting. The program\nseeks to improve outcomes by developing individually-tailored social service\ninterventions (i.e., diversions, conditional plea agreements, stayed\nsentencing, or other favorable case disposition based on appropriate social\nservice linkage rather than traditional sentencing methods) for individuals\nlikely to experience subsequent interactions with the criminal justice system,\na time and resource-intensive undertaking that necessitates an ability to focus\nresources on individuals most likely to be involved in a future case. Seeking\nto achieve both efficiency (through predictive accuracy) and equity (improving\noutcomes in traditionally under-served communities and working to mitigate\nexisting disparities in criminal justice outcomes), we discuss the equity\noutcomes we seek to achieve, describe the corresponding choice of a metric for\nmeasuring predictive fairness in this context, and explore a set of options for\nbalancing equity and efficiency when building and selecting machine learning\nmodels in an operational public policy setting.",
    "published_date": "2020-01-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.1; K.4.2; K.5.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09233v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09102v1",
    "title": "Generalized Prager-Synge Inequality and Equilibrated Error Estimators for Discontinuous Elements",
    "authors": [
      "Cuiyu He",
      "Zhiqiang Cai",
      "Shun Zhang"
    ],
    "author_ids": [],
    "abstract": "The well-known Prager-Synge identity is valid in $H^1(\\Omega)$ and serves as\na foundation for developing equilibrated a posteriori error estimators for\ncontinuous elements. In this paper, we introduce a new inequality, that may be\nregarded as a generalization of the Prager-Synge identity, to be valid for\npiecewise $H^1(\\Omega)$ functions for diffusion problems. The inequality is\nproved to be identity in two dimensions.\n  For nonconforming finite element approximation of arbitrary odd order, we\npropose a fully explicit approach that recovers an equilibrated flux in $H(div;\n\\Omega)$ through a local element-wise scheme and that recovers a gradient in\n$H(curl;\\Omega)$ through a simple averaging technique over edges. The resulting\nerror estimator is then proved to be globally reliable and locally efficient.\nMoreover, the reliability and efficiency constants are independent of the jump\nof the diffusion coefficient regardless of its distribution.",
    "published_date": "2020-01-24T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30",
      "G.1.8"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09102v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.08873v4",
    "title": "Simple and Effective Prevention of Mode Collapse in Deep One-Class Classification",
    "authors": [
      "Penny Chong",
      "Lukas Ruff",
      "Marius Kloft",
      "Alexander Binder"
    ],
    "author_ids": [],
    "abstract": "Anomaly detection algorithms find extensive use in various fields. This area\nof research has recently made great advances thanks to deep learning. A recent\nmethod, the deep Support Vector Data Description (deep SVDD), which is inspired\nby the classic kernel-based Support Vector Data Description (SVDD), is capable\nof simultaneously learning a feature representation of the data and a\ndata-enclosing hypersphere. The method has shown promising results in both\nunsupervised and semi-supervised settings. However, deep SVDD suffers from\nhypersphere collapse -- also known as mode collapse, if the architecture of the\nmodel does not comply with certain architectural constraints, e.g. the removal\nof bias terms. These constraints limit the adaptability of the model and in\nsome cases, may affect the model performance due to learning sub-optimal\nfeatures. In this work, we consider two regularizers to prevent hypersphere\ncollapse in deep SVDD. The first regularizer is based on injecting random noise\nvia the standard cross-entropy loss. The second regularizer penalizes the\nminibatch variance when it becomes too small. Moreover, we introduce an\nadaptive weighting scheme to control the amount of penalization between the\nSVDD loss and the respective regularizer. Our proposed regularized variants of\ndeep SVDD show encouraging results and outperform a prominent state-of-the-art\nmethod on a setup where the anomalies have no apparent geometrical structure.",
    "published_date": "2020-01-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.08873v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.08855v1",
    "title": "Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack",
    "authors": [
      "Bo Zhang",
      "Ruotong Yu",
      "Haipei Sun",
      "Yanying Li",
      "Jun Xu",
      "Hui Wang"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms, when applied to sensitive data, pose a potential\nthreat to privacy. A growing body of prior work has demonstrated that\nmembership inference attack (MIA) can disclose specific private information in\nthe training data to an attacker. Meanwhile, the algorithmic fairness of\nmachine learning has increasingly caught attention from both academia and\nindustry. Algorithmic fairness ensures that the machine learning models do not\ndiscriminate a particular demographic group of individuals (e.g., black and\nfemale people). Given that MIA is indeed a learning model, it raises a serious\nconcern if MIA ``fairly'' treats all groups of individuals equally. In other\nwords, whether a particular group is more vulnerable against MIA than the other\ngroups. This paper examines the algorithmic fairness issue in the context of\nMIA and its defenses. First, for fairness evaluation, it formalizes the\nnotation of vulnerability disparity (VD) to quantify the difference of MIA\ntreatment on different demographic groups. Second, it evaluates VD on four\nreal-world datasets, and shows that VD indeed exists in these datasets. Third,\nit examines the impacts of differential privacy, as a defense mechanism of MIA,\non VD. The results show that although DP brings significant change on VD, it\ncannot eliminate VD completely. Therefore, fourth, it designs a new mitigation\nalgorithm named FAIRPICK to reduce VD. An extensive set of experimental results\ndemonstrate that FAIRPICK can effectively reduce VD for both with and without\nthe DP deployment.",
    "published_date": "2020-01-24T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.08855v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.08767v1",
    "title": "Interventions for Ranking in the Presence of Implicit Bias",
    "authors": [
      "L. Elisa Celis",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "Implicit bias is the unconscious attribution of particular qualities (or lack\nthereof) to a member from a particular social group (e.g., defined by gender or\nrace). Studies on implicit bias have shown that these unconscious stereotypes\ncan have adverse outcomes in various social contexts, such as job screening,\nteaching, or policing. Recently, (Kleinberg and Raghavan, 2018) considered a\nmathematical model for implicit bias and showed the effectiveness of the Rooney\nRule as a constraint to improve the utility of the outcome for certain cases of\nthe subset selection problem. Here we study the problem of designing\ninterventions for the generalization of subset selection -- ranking -- that\nrequires to output an ordered set and is a central primitive in various social\nand computational contexts. We present a family of simple and interpretable\nconstraints and show that they can optimally mitigate implicit bias for a\ngeneralization of the model studied in (Kleinberg and Raghavan, 2018).\nSubsequently, we prove that under natural distributional assumptions on the\nutilities of items, simple, Rooney Rule-like, constraints can also surprisingly\nrecover almost all the utility lost due to implicit biases. Finally, we augment\nour theoretical results with empirical findings on real-world distributions\nfrom the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.",
    "published_date": "2020-01-23T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DS",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.08767v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.08378v1",
    "title": "Improving speaker discrimination of target speech extraction with time-domain SpeakerBeam",
    "authors": [
      "Marc Delcroix",
      "Tsubasa Ochiai",
      "Katerina Zmolikova",
      "Keisuke Kinoshita",
      "Naohiro Tawara",
      "Tomohiro Nakatani",
      "Shoko Araki"
    ],
    "author_ids": [],
    "abstract": "Target speech extraction, which extracts a single target source in a mixture\ngiven clues about the target speaker, has attracted increasing attention. We\nhave recently proposed SpeakerBeam, which exploits an adaptation utterance of\nthe target speaker to extract his/her voice characteristics that are then used\nto guide a neural network towards extracting speech of that speaker.\nSpeakerBeam presents a practical alternative to speech separation as it enables\ntracking speech of a target speaker across utterances, and achieves promising\nspeech extraction performance. However, it sometimes fails when speakers have\nsimilar voice characteristics, such as in same-gender mixtures, because it is\ndifficult to discriminate the target speaker from the interfering speakers. In\nthis paper, we investigate strategies for improving the speaker discrimination\ncapability of SpeakerBeam. First, we propose a time-domain implementation of\nSpeakerBeam similar to that proposed for a time-domain audio separation network\n(TasNet), which has achieved state-of-the-art performance for speech\nseparation. Besides, we investigate (1) the use of spatial features to better\ndiscriminate speakers when microphone array recordings are available, (2)\nadding an auxiliary speaker identification loss for helping to learn more\ndiscriminative voice characteristics. We show experimentally that these\nstrategies greatly improve speech extraction performance, especially for\nsame-gender mixtures, and outperform TasNet in terms of target speech\nextraction.",
    "published_date": "2020-01-23T00:00:00",
    "year": 2020,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.08378v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.08311v1",
    "title": "Learning to adapt class-specific features across domains for semantic segmentation",
    "authors": [
      "Mikel Menta",
      "Adriana Romero",
      "Joost van de Weijer"
    ],
    "author_ids": [],
    "abstract": "Recent advances in unsupervised domain adaptation have shown the\neffectiveness of adversarial training to adapt features across domains,\nendowing neural networks with the capability of being tested on a target domain\nwithout requiring any training annotations in this domain. The great majority\nof existing domain adaptation models rely on image translation networks, which\noften contain a huge amount of domain-specific parameters. Additionally, the\nfeature adaptation step often happens globally, at a coarse level, hindering\nits applicability to tasks such as semantic segmentation, where details are of\ncrucial importance to provide sharp results. In this thesis, we present a novel\narchitecture, which learns to adapt features across domains by taking into\naccount per class information. To that aim, we design a conditional pixel-wise\ndiscriminator network, whose output is conditioned on the segmentation masks.\nMoreover, following recent advances in image translation, we adopt the recently\nintroduced StarGAN architecture as image translation backbone, since it is able\nto perform translations across multiple domains by means of a single generator\nnetwork. Preliminary results on a segmentation task designed to assess the\neffectiveness of the proposed approach highlight the potential of the model,\nimproving upon strong baselines and alternative designs.",
    "published_date": "2020-01-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.08311v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.08173v1",
    "title": "Causality based Feature Fusion for Brain Neuro-Developmental Analysis",
    "authors": [
      "Peyman Hosseinzadeh Kassani",
      "Li Xiao",
      "Gemeng Zhang",
      "Julia M. Stephen",
      "Tony W. Wilson",
      "Vince D. Calhoun",
      "Yu Ping Wang"
    ],
    "author_ids": [],
    "abstract": "Human brain development is a complex and dynamic process that is affected by\nseveral factors such as genetics, sex hormones, and environmental changes. A\nnumber of recent studies on brain development have examined functional\nconnectivity (FC) defined by the temporal correlation between time series of\ndifferent brain regions. We propose to add the directional flow of information\nduring brain maturation. To do so, we extract effective connectivity (EC)\nthrough Granger causality (GC) for two different groups of subjects, i.e.,\nchildren and young adults. The motivation is that the inclusion of causal\ninteraction may further discriminate brain connections between two age groups\nand help to discover new connections between brain regions. The contributions\nof this study are threefold. First, there has been a lack of attention to\nEC-based feature extraction in the context of brain development. To this end,\nwe propose a new kernel-based GC (KGC) method to learn nonlinearity of complex\nbrain network, where a reduced Sine hyperbolic polynomial (RSP) neural network\nwas used as our proposed learner. Second, we used causality values as the\nweight for the directional connectivity between brain regions. Our findings\nindicated that the strength of connections was significantly higher in young\nadults relative to children. In addition, our new EC-based feature outperformed\nFC-based analysis from Philadelphia neurocohort (PNC) study with better\ndiscrimination of the different age groups. Moreover, the fusion of these two\nsets of features (FC + EC) improved brain age prediction accuracy by more than\n4%, indicating that they should be used together for brain development studies.",
    "published_date": "2020-01-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.08173v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09778v2",
    "title": "Artificial intelligence in medicine and healthcare: a review and classification of current and near-future applications and their ethical and social Impact",
    "authors": [
      "Emilio Gómez-González",
      "Emilia Gomez",
      "Javier Márquez-Rivas",
      "Manuel Guerrero-Claro",
      "Isabel Fernández-Lizaranzu",
      "María Isabel Relimpio-López",
      "Manuel E. Dorado",
      "María José Mayorga-Buiza",
      "Guillermo Izquierdo-Ayuso",
      "Luis Capitán-Morales"
    ],
    "author_ids": [],
    "abstract": "This paper provides an overview of the current and near-future applications\nof Artificial Intelligence (AI) in Medicine and Health Care and presents a\nclassification according to their ethical and societal aspects, potential\nbenefits and pitfalls, and issues that can be considered controversial and are\nnot deeply discussed in the literature.\n  This work is based on an analysis of the state of the art of research and\ntechnology, including existing software, personal monitoring devices, genetic\ntests and editing tools, personalized digital models, online platforms,\naugmented reality devices, and surgical and companion robotics. Motivated by\nour review, we present and describe the notion of 'extended personalized\nmedicine', we then review existing applications of AI in medicine and\nhealthcare and explore the public perception of medical AI systems, and how\nthey show, simultaneously, extraordinary opportunities and drawbacks that even\nquestion fundamental medical concepts. Many of these topics coincide with\nurgent priorities recently defined by the World Health Organization for the\ncoming decade. In addition, we study the transformations of the roles of\ndoctors and patients in an age of ubiquitous information, identify the risk of\na division of Medicine into 'fake-based', 'patient-generated', and\n'scientifically tailored', and draw the attention of some aspects that need\nfurther thorough analysis and public debate.",
    "published_date": "2020-01-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09778v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.07864v2",
    "title": "Fairness Metrics: A Comparative Analysis",
    "authors": [
      "Pratyush Garg",
      "John Villasenor",
      "Virginia Foggo"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness is receiving significant attention in the academic and\nbroader literature due to the increasing use of predictive algorithms,\nincluding those based on artificial intelligence. One benefit of this trend is\nthat algorithm designers and users have a growing set of fairness measures to\nchoose from. However, this choice comes with the challenge of identifying how\nthe different fairness measures relate to one another, as well as the extent to\nwhich they are compatible or mutually exclusive. We describe some of the most\nwidely used fairness metrics using a common mathematical framework and present\nnew results on the relationships among them. The results presented herein can\nhelp place both specialists and non-specialists in a better position to\nidentify the metric best suited for their application and goals.",
    "published_date": "2020-01-22T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.07864v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.07859v4",
    "title": "A Deep Learning Algorithm for High-Dimensional Exploratory Item Factor Analysis",
    "authors": [
      "Christopher J. Urban",
      "Daniel J. Bauer"
    ],
    "author_ids": [],
    "abstract": "Marginal maximum likelihood (MML) estimation is the preferred approach to\nfitting item response theory models in psychometrics due to the MML estimator's\nconsistency, normality, and efficiency as the sample size tends to infinity.\nHowever, state-of-the-art MML estimation procedures such as the\nMetropolis-Hastings Robbins-Monro (MH-RM) algorithm as well as approximate MML\nestimation procedures such as variational inference (VI) are computationally\ntime-consuming when the sample size and the number of latent factors are very\nlarge. In this work, we investigate a deep learning-based VI algorithm for\nexploratory item factor analysis (IFA) that is computationally fast even in\nlarge data sets with many latent factors. The proposed approach applies a deep\nartificial neural network model called an importance-weighted autoencoder\n(IWAE) for exploratory IFA. The IWAE approximates the MML estimator using an\nimportance sampling technique wherein increasing the number of\nimportance-weighted (IW) samples drawn during fitting improves the\napproximation, typically at the cost of decreased computational efficiency. We\nprovide a real data application that recovers results aligning with\npsychological theory across random starts. Via simulation studies, we show that\nthe IWAE yields more accurate estimates as either the sample size or the number\nof IW samples increases (although factor correlation and intercepts estimates\nexhibit some bias) and obtains similar results to MH-RM in less time. Our\nsimulations also suggest that the proposed approach performs similarly to and\nis potentially faster than constrained joint maximum likelihood estimation, a\nfast procedure that is consistent when the sample size and the number of items\nsimultaneously tend to infinity.",
    "published_date": "2020-01-22T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.07859v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.07806v3",
    "title": "Complete solution of tropical vector inequalities using matrix sparsification",
    "authors": [
      "Nikolai Krivulin"
    ],
    "author_ids": [],
    "abstract": "We examine the problem of finding all solutions of two-sided vector\ninequalities given in the tropical algebra setting, where the unknown vector\nmultiplied by known matrices appears on both sides of the inequality. We offer\na solution that uses sparse matrices to simplify the problem and to construct a\nfamily of solution sets, each defined by a sparse matrix obtained from one of\nthe given matrices by setting some of its entries to zero. All solutions are\nthen combined to present the result in a parametric form in terms of a matrix\nwhose columns form a complete system of generators for the solution. We\ndescribe the computational technique proposed to solve the problem, remark on\nits computational complexity and illustrate this technique with numerical\nexamples.",
    "published_date": "2020-01-21T00:00:00",
    "year": 2020,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY",
      "math.AC",
      "15A80 (Primary), 15A39, 65F50 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.07806v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09784v1",
    "title": "Algorithmic Fairness",
    "authors": [
      "Dana Pessach",
      "Erez Shmueli"
    ],
    "author_ids": [],
    "abstract": "An increasing number of decisions regarding the daily lives of human beings\nare being controlled by artificial intelligence (AI) algorithms in spheres\nranging from healthcare, transportation, and education to college admissions,\nrecruitment, provision of loans and many more realms. Since they now touch on\nmany aspects of our lives, it is crucial to develop AI algorithms that are not\nonly accurate but also objective and fair. Recent studies have shown that\nalgorithmic decision-making may be inherently prone to unfairness, even when\nthere is no intention for it. This paper presents an overview of the main\nconcepts of identifying, measuring and improving algorithmic fairness when\nusing AI algorithms. The paper begins by discussing the causes of algorithmic\nbias and unfairness and the common definitions and measures for fairness.\nFairness-enhancing mechanisms are then reviewed and divided into pre-process,\nin-process and post-process mechanisms. A comprehensive comparison of the\nmechanisms is then conducted, towards a better understanding of which\nmechanisms should be used in different scenarios. The paper then describes the\nmost commonly used fairness-related datasets in this field. Finally, the paper\nends by reviewing several emerging research sub-fields of algorithmic fairness.",
    "published_date": "2020-01-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09784v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.07649v6",
    "title": "Integrating data science ethics into an undergraduate major: A case study",
    "authors": [
      "Benjamin S. Baumer",
      "Randi L. Garcia",
      "Albert Y. Kim",
      "Katherine M. Kinnaird",
      "Miles Q. Ott"
    ],
    "author_ids": [],
    "abstract": "We present a programmatic approach to incorporating ethics into an\nundergraduate major in statistical and data sciences. We discuss\ndepartmental-level initiatives designed to meet the National Academy of\nSciences recommendation for integrating ethics into the curriculum from\ntop-to-bottom as our majors progress from our introductory courses to our\nsenior capstone course, as well as from side-to-side through co-curricular\nprogramming. We also provide six examples of data science ethics modules used\nin five different courses at our liberal arts college, each focusing on a\ndifferent ethical consideration. The modules are designed to be portable such\nthat they can be flexibly incorporated into existing courses at different\nlevels of instruction with minimal disruption to syllabi. We connect our\nefforts to a growing body of literature on the teaching of data science ethics,\npresent assessments of our effectiveness, and conclude with next steps and\nfinal thoughts.",
    "published_date": "2020-01-21T00:00:00",
    "year": 2020,
    "categories": [
      "stat.OT",
      "cs.OH",
      "00A05",
      "K.7.4; K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.07649v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.07573v2",
    "title": "Implementations in Machine Ethics: A Survey",
    "authors": [
      "Suzanne Tolmeijer",
      "Markus Kneer",
      "Cristina Sarasua",
      "Markus Christen",
      "Abraham Bernstein"
    ],
    "author_ids": [],
    "abstract": "Increasingly complex and autonomous systems require machine ethics to\nmaximize the benefits and minimize the risks to society arising from the new\ntechnology. It is challenging to decide which type of ethical theory to employ\nand how to implement it effectively. This survey provides a threefold\ncontribution. First, it introduces a trimorphic taxonomy to analyze machine\nethics implementations with respect to their object (ethical theories), as well\nas their nontechnical and technical aspects. Second, an exhaustive selection\nand description of relevant works is presented. Third, applying the new\ntaxonomy to the selected works, dominant research patterns, and lessons for the\nfield are identified, and future directions for research are suggested.",
    "published_date": "2020-01-21T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.07573v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11475v2",
    "title": "Variational inequalities for ferroelectric constitutive modeling",
    "authors": [
      "Martin Meindlhumer",
      "Astrid Pechstein",
      "Alexander Humer"
    ],
    "author_ids": [],
    "abstract": "This paper is concerned with modeling the polarization process in\nferroelectric media. We develop a thermodynamically consistent model, based on\nphenomenological descriptions of free energy as well as switching and\nsaturation conditions in form of inequalities. Thermodynamically consistent\nmodels naturally lead to variational formulations. We propose to use the\nconcept of variational inequalities. We aim at combining the different\nphenomenological conditions into one variational inequality. In our formulation\nwe use one Lagrange multiplier for each condition (the onset of domain\nswitching and saturation), each satisfying Karush-Kuhn-Tucker conditions. An\nupdate for reversible and remanent quantities is then computed within one, in\ngeneral nonlinear, iteration.",
    "published_date": "2020-01-20T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA",
      "physics.app-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11475v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.06841v1",
    "title": "Dynamic Weighted Fairness with Minimal Disruptions",
    "authors": [
      "Sungjin Im",
      "Benjamin Moseley",
      "Kamesh Munagala",
      "Kirk Pruhs"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider the following dynamic fair allocation problem:\nGiven a sequence of job arrivals and departures, the goal is to maintain an\napproximately fair allocation of the resource against a target fair allocation\npolicy, while minimizing the total number of disruptions, which is the number\nof times the allocation of any job is changed. We consider a rich class of fair\nallocation policies that significantly generalize those considered in previous\nwork.\n  We first consider the models where jobs only arrive, or jobs only depart. We\npresent tight upper and lower bounds for the number of disruptions required to\nmaintain a constant approximate fair allocation every time step. In particular,\nfor the canonical case where jobs have weights and the resource allocation is\nproportional to the job's weight, we show that maintaining a constant\napproximate fair allocation requires $\\Theta(\\log^* n)$ disruptions per job,\nalmost matching the bounds in prior work for the unit weight case. For the more\ngeneral setting where the allocation policy only decreases the allocation to a\njob when new jobs arrive, we show that maintaining a constant approximate fair\nallocation requires $\\Theta(\\log n)$ disruptions per job. We then consider the\nmodel where jobs can both arrive and depart. We first show strong lower bounds\non the number of disruptions required to maintain constant approximate fairness\nfor arbitrary instances. In contrast we then show that there there is an\nalgorithm that can maintain constant approximate fairness with $O(1)$ expected\ndisruptions per job if the weights of the jobs are independent of the jobs\narrival and departure order. We finally show how our results can be extended to\nthe setting with multiple resources.",
    "published_date": "2020-01-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06841v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.06779v2",
    "title": "Predict and Match: Prophet Inequalities with Uncertain Supply",
    "authors": [
      "Reza Alijani",
      "Siddhartha Banerjee",
      "Sreenivas Gollapudi",
      "Kamesh Munagala",
      "Kangning Wang"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of selling perishable items to a stream of buyers in\norder to maximize social welfare. A seller starts with a set of identical\nitems, and each arriving buyer wants any one item, and has a valuation drawn\ni.i.d. from a known distribution. Each item, however, disappears after an a\npriori unknown amount of time that we term the horizon for that item. The\nseller knows the (possibly different) distribution of the horizon for each\nitem, but not its realization till the item actually disappears. As with the\nclassic prophet inequalities, the goal is to design an online pricing scheme\nthat competes with the prophet that knows the horizon and extracts full social\nsurplus (or welfare).\n  Our main results are for the setting where items have independent horizon\ndistributions satisfying the monotone-hazard-rate (MHR) condition. Here, for\nany number of items, we achieve a constant-competitive bound via a conceptually\nsimple policy that balances the rate at which buyers are accepted with the rate\nat which items are removed from the system. We implement this policy via a\nnovel technique of matching via probabilistically simulating departures of the\nitems at future times. Moreover, for a single item and MHR horizon distribution\nwith mean $\\mu$, we show a tight result: There is a fixed pricing scheme that\nhas competitive ratio at most $2 - 1/\\mu$, and this is the best achievable in\nthis class.\n  We further show that our results are best possible. First, we show that the\ncompetitive ratio is unbounded without the MHR assumption even for one item.\nFurther, even when the horizon distributions are i.i.d. MHR and the number of\nitems becomes large, the competitive ratio of any policy is lower bounded by a\nconstant greater than $1$, which is in sharp contrast to the setting with\nidentical deterministic horizons.",
    "published_date": "2020-01-19T00:00:00",
    "year": 2020,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06779v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.06693v1",
    "title": "Fair Transfer of Multiple Style Attributes in Text",
    "authors": [
      "Karan Dabas",
      "Nishtha Madan",
      "Vijay Arya",
      "Sameep Mehta",
      "Gautam Singh",
      "Tanmoy Chakraborty"
    ],
    "author_ids": [],
    "abstract": "To preserve anonymity and obfuscate their identity on online platforms users\nmay morph their text and portray themselves as a different gender or\ndemographic. Similarly, a chatbot may need to customize its communication style\nto improve engagement with its audience. This manner of changing the style of\nwritten text has gained significant attention in recent years. Yet these past\nresearch works largely cater to the transfer of single style attributes. The\ndisadvantage of focusing on a single style alone is that this often results in\ntarget text where other existing style attributes behave unpredictably or are\nunfairly dominated by the new style. To counteract this behavior, it would be\nnice to have a style transfer mechanism that can transfer or control multiple\nstyles simultaneously and fairly. Through such an approach, one could obtain\nobfuscated or written text incorporated with a desired degree of multiple soft\nstyles such as female-quality, politeness, or formalness.\n  In this work, we demonstrate that the transfer of multiple styles cannot be\nachieved by sequentially performing multiple single-style transfers. This is\nbecause each single style-transfer step often reverses or dominates over the\nstyle incorporated by a previous transfer step. We then propose a neural\nnetwork architecture for fairly transferring multiple style attributes in a\ngiven text. We test our architecture on the Yelp data set to demonstrate our\nsuperior performance as compared to existing one-style transfer steps performed\nin a sequence.",
    "published_date": "2020-01-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06693v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.06691v1",
    "title": "Teaching Software Engineering for AI-Enabled Systems",
    "authors": [
      "Christian Kästner",
      "Eunsuk Kang"
    ],
    "author_ids": [],
    "abstract": "Software engineers have significant expertise to offer when building\nintelligent systems, drawing on decades of experience and methods for building\nsystems that are scalable, responsive and robust, even when built on unreliable\ncomponents. Systems with artificial-intelligence or machine-learning (ML)\ncomponents raise new challenges and require careful engineering. We designed a\nnew course to teach software-engineering skills to students with a background\nin ML. We specifically go beyond traditional ML courses that teach modeling\ntechniques under artificial conditions and focus, in lecture and assignments,\non realism with large and changing datasets, robust and evolvable\ninfrastructure, and purposeful requirements engineering that considers ethics\nand fairness as well. We describe the course and our infrastructure and share\nexperience and all material from teaching the course for the first time.",
    "published_date": "2020-01-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06691v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.06615v2",
    "title": "The Risk to Population Health Equity Posed by Automated Decision Systems: A Narrative Review",
    "authors": [
      "Mitchell Burger"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence is already ubiquitous, and is increasingly being used\nto autonomously make ever more consequential decisions. However, there has been\nrelatively little research into the existing and possible consequences for\npopulation health equity. A narrative review was undertaken using a hermeneutic\napproach to explore current and future uses of narrow AI and automated decision\nsystems (ADS) in medicine and public health, issues that have emerged, and\nimplications for equity. Accounts reveal a tremendous expectation on AI to\ntransform medical and public health practices. Prominent demonstrations of AI\ncapability - particularly in diagnostic decision making, risk prediction, and\nsurveillance - are stimulating rapid adoption, spurred by COVID-19. Automated\ndecisions being made have significant consequences for individual and\npopulation health and wellbeing. Meanwhile, it is evident that hazards\nincluding bias, incontestability, and privacy erosion have emerged in sensitive\ndomains such as criminal justice where narrow AI and ADS are in common use.\nReports of issues arising from their use in health are already appearing. As\nthe use of ADS in health expands, it is probable that these hazards will\nmanifest more widely. Bias, incontestability, and privacy erosion give rise to\nmechanisms by which existing social, economic and health disparities are\nperpetuated and amplified. Consequently, there is a significant risk that use\nof ADS in health will exacerbate existing population health inequities. The\nindustrial scale and rapidity with which ADS can be applied heightens the risk\nto population health equity. It is incumbent on health practitioners and policy\nmakers therefore to explore the potential implications of using ADS, to ensure\nthe use of artificial intelligence promotes population health and equity.",
    "published_date": "2020-01-18T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06615v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09055v2",
    "title": "Forecasting Corn Yield with Machine Learning Ensembles",
    "authors": [
      "Mohsen Shahhosseini",
      "Guiping Hu",
      "Sotirios V. Archontoulis"
    ],
    "author_ids": [],
    "abstract": "The emerge of new technologies to synthesize and analyze big data with\nhigh-performance computing, has increased our capacity to more accurately\npredict crop yields. Recent research has shown that Machine learning (ML) can\nprovide reasonable predictions, faster, and with higher flexibility compared to\nsimulation crop modeling. The earlier the prediction during the growing season\nthe better, but this has not been thoroughly investigated as previous studies\nconsidered all data available to predict yields. This paper provides a machine\nlearning based framework to forecast corn yields in three US Corn Belt states\n(Illinois, Indiana, and Iowa) considering complete and partial in-season\nweather knowledge. Several ensemble models are designed using blocked\nsequential procedure to generate out-of-bag predictions. The forecasts are made\nin county-level scale and aggregated for agricultural district, and state level\nscales. Results show that ensemble models based on weighted average of the base\nlearners outperform individual models. Specifically, the proposed ensemble\nmodel could achieve best prediction accuracy (RRMSE of 7.8%) and least mean\nbias error (-6.06 bu/acre) compared to other developed models. Comparing our\nproposed model forecasts with the literature demonstrates the superiority of\nforecasts made by our proposed ensemble model. Results from the scenario of\nhaving partial in-season weather knowledge reveal that decent yield forecasts\ncan be made as early as June 1st. To find the marginal effect of each input\nfeature on the forecasts made by the proposed ensemble model, a methodology is\nsuggested that is the basis for finding feature importance for the ensemble\nmodel. The findings suggest that weather features corresponding to weather in\nweeks 18-24 (May 1st to June 1st) are the most important input features.",
    "published_date": "2020-01-18T00:00:00",
    "year": 2020,
    "categories": [
      "stat.AP",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09055v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.06528v1",
    "title": "Activism by the AI Community: Analysing Recent Achievements and Future Prospects",
    "authors": [
      "Haydn Belfield"
    ],
    "author_ids": [],
    "abstract": "The artificial intelligence community (AI) has recently engaged in activism\nin relation to their employers, other members of the community, and their\ngovernments in order to shape the societal and ethical implications of AI. It\nhas achieved some notable successes, but prospects for further political\norganising and activism are uncertain. We survey activism by the AI community\nover the last six years; apply two analytical frameworks drawing upon the\nliterature on epistemic communities, and worker organising and bargaining; and\nexplore what they imply for the future prospects of the AI community. Success\nthus far has hinged on a coherent shared culture, and high bargaining power due\nto the high demand for a limited supply of AI talent. Both are crucial to the\nfuture of AI activism and worthy of sustained attention.",
    "published_date": "2020-01-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2; K.4; K.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06528v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.06252v1",
    "title": "Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change Detection",
    "authors": [
      "Xinzheng Zhang",
      "Guo Liu",
      "Ce Zhang",
      "Peter M Atkinson",
      "Xiaoheng Tan",
      "Xin Jian",
      "Xichuan Zhou",
      "Yongming Li"
    ],
    "author_ids": [],
    "abstract": "Change detection is one of the fundamental applications of synthetic aperture\nradar (SAR) images. However, speckle noise presented in SAR images has a much\nnegative effect on change detection. In this research, a novel two-phase\nobject-based deep learning approach is proposed for multi-temporal SAR image\nchange detection. Compared with traditional methods, the proposed approach\nbrings two main innovations. One is to classify all pixels into three\ncategories rather than two categories: unchanged pixels, changed pixels caused\nby strong speckle (false changes), and changed pixels formed by real terrain\nvariation (real changes). The other is to group neighboring pixels into\nsegmented into superpixel objects (from pixels) such as to exploit local\nspatial context. Two phases are designed in the methodology: 1) Generate\nobjects based on the simple linear iterative clustering algorithm, and\ndiscriminate these objects into changed and unchanged classes using fuzzy\nc-means (FCM) clustering and a deep PCANet. The prediction of this Phase is the\nset of changed and unchanged superpixels. 2) Deep learning on the pixel sets\nover the changed superpixels only, obtained in the first phase, to discriminate\nreal changes from false changes. SLIC is employed again to achieve new\nsuperpixels in the second phase. Low rank and sparse decomposition are applied\nto these new superpixels to suppress speckle noise significantly. A further\nclustering step is applied to these new superpixels via FCM. A new PCANet is\nthen trained to classify two kinds of changed superpixels to achieve the final\nchange maps. Numerical experiments demonstrate that, compared with benchmark\nmethods, the proposed approach can distinguish real changes from false changes\neffectively with significantly reduced false alarm rates, and achieve up to\n99.71% change detection accuracy using multi-temporal SAR imagery.",
    "published_date": "2020-01-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06252v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.06159v1",
    "title": "A New Fairness Model based on User's Objective for Multi-user Multi-processor Online Scheduling",
    "authors": [
      "Debasis Dwibedy",
      "Rakesh Mohanty"
    ],
    "author_ids": [],
    "abstract": "Resources of a multi-user system in multi-processor online scheduling are\nshared by competing users in which fairness is a major performance criterion\nfor resource allocation. Fairness ensures equality in resource sharing among\nthe users. According to our knowledge, fairness based on the user's objective\nhas neither been comprehensively studied nor a formal fairness model has been\nwell defined in the literature. This motivates us to explore and define a new\nmodel to ensure algorithmic fairness with quantitative performance measures\nbased on optimization of the user's objective. In this paper, we propose a new\nmodel for fairness in Multi-user Multi-processor Online Scheduling\nProblem(MUMPOSP). We introduce and formally define quantitative fairness\nmeasures based on user's objective by optimizing makespan for individual user\nin our proposed fairness model. We also define the unfairness of deprived users\nand absolute fairness of an algorithm. We obtain lower bound results for the\nabsolute fairness for m identical machines with equal length jobs. We show that\nour proposed fairness model can serve as a framework for measuring algorithmic\nfairness by considering various optimality criteria such as flow time and sum\nof completion times.",
    "published_date": "2020-01-17T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "cs.OS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06159v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.06089v2",
    "title": "Fairness Measures for Regression via Probabilistic Classification",
    "authors": [
      "Daniel Steinberg",
      "Alistair Reid",
      "Simon O'Callaghan"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness involves expressing notions such as equity, or\nreasonable treatment, as quantifiable measures that a machine learning\nalgorithm can optimise. Most work in the literature to date has focused on\nclassification problems where the prediction is categorical, such as accepting\nor rejecting a loan application. This is in part because classification\nfairness measures are easily computed by comparing the rates of outcomes,\nleading to behaviours such as ensuring that the same fraction of eligible men\nare selected as eligible women. But such measures are computationally difficult\nto generalise to the continuous regression setting for problems such as\npricing, or allocating payments. The difficulty arises from estimating\nconditional densities (such as the probability density that a system will\nover-charge by a certain amount). For the regression setting we introduce\ntractable approximations of the independence, separation and sufficiency\ncriteria by observing that they factorise as ratios of different conditional\nprobabilities of the protected attributes. We introduce and train machine\nlearning classifiers, distinct from the predictor, as a mechanism to estimate\nthese probabilities from the data. This naturally leads to model agnostic,\ntractable approximations of the criteria, which we explore experimentally.",
    "published_date": "2020-01-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06089v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.06007v1",
    "title": "User-in-the-loop Adaptive Intent Detection for Instructable Digital Assistant",
    "authors": [
      "Nicolas Lair",
      "Clément Delgrange",
      "David Mugisha",
      "Jean-Michel Dussoux",
      "Pierre-Yves Oudeyer",
      "Peter Ford Dominey"
    ],
    "author_ids": [],
    "abstract": "People are becoming increasingly comfortable using Digital Assistants (DAs)\nto interact with services or connected objects. However, for non-programming\nusers, the available possibilities for customizing their DA are limited and do\nnot include the possibility of teaching the assistant new tasks. To make the\nmost of the potential of DAs, users should be able to customize assistants by\ninstructing them through Natural Language (NL). To provide such\nfunctionalities, NL interpretation in traditional assistants should be\nimproved: (1) The intent identification system should be able to recognize new\nforms of known intents, and to acquire new intents as they are expressed by the\nuser. (2) In order to be adaptive to novel intents, the Natural Language\nUnderstanding module should be sample efficient, and should not rely on a\npretrained model. Rather, the system should continuously collect the training\ndata as it learns new intents from the user. In this work, we propose AidMe\n(Adaptive Intent Detection in Multi-Domain Environments), a user-in-the-loop\nadaptive intent detection framework that allows the assistant to adapt to its\nuser by learning his intents as their interaction progresses. AidMe builds its\nrepertoire of intents and collects data to train a model of semantic similarity\nevaluation that can discriminate between the learned intents and autonomously\ndiscover new forms of known intents. AidMe addresses two major issues - intent\nlearning and user adaptation - for instructable digital assistants. We\ndemonstrate the capabilities of AidMe as a standalone system by comparing it\nwith a one-shot learning system and a pretrained NLU module through simulations\nof interactions with a user. We also show how AidMe can smoothly integrate to\nan existing instructable digital assistant.",
    "published_date": "2020-01-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.06007v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09753v1",
    "title": "Race, Gender and Beauty: The Effect of Information Provision on Online Hiring Biases",
    "authors": [
      "Weiwen Leung",
      "Zheng Zhang",
      "Daviti Jibuti",
      "Jinhao Zhao",
      "Maximillian Klein",
      "Casey Pierce",
      "Lionel Robert",
      "Haiyi Zhu"
    ],
    "author_ids": [],
    "abstract": "We conduct a study of hiring bias on a simulation platform where we ask\nAmazon MTurk participants to make hiring decisions for a mathematically\nintensive task. Our findings suggest hiring biases against Black workers and\nless attractive workers and preferences towards Asian workers female workers\nand more attractive workers. We also show that certain UI designs including\nprovision of candidates information at the individual level and reducing the\nnumber of choices can significantly reduce discrimination. However provision of\ncandidates information at the subgroup level can increase discrimination. The\nresults have practical implications for designing better online freelance\nmarketplaces.",
    "published_date": "2020-01-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09753v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.05755v1",
    "title": "ScaIL: Classifier Weights Scaling for Class Incremental Learning",
    "authors": [
      "Eden Belouadah",
      "Adrian Popescu"
    ],
    "author_ids": [],
    "abstract": "Incremental learning is useful if an AI agent needs to integrate data from a\nstream. The problem is non trivial if the agent runs on a limited computational\nbudget and has a bounded memory of past data. In a deep learning approach, the\nconstant computational budget requires the use of a fixed architecture for all\nincremental states. The bounded memory generates data imbalance in favor of new\nclasses and a prediction bias toward them appears. This bias is commonly\ncountered by introducing a data balancing step in addition to the basic network\ntraining. We depart from this approach and propose simple but efficient scaling\nof past class classifier weights to make them more comparable to those of new\nclasses. Scaling exploits incremental state level statistics and is applied to\nthe classifiers learned in the initial state of classes in order to profit from\nall their available data. We also question the utility of the widely used\ndistillation loss component of incremental learning algorithms by comparing it\nto vanilla fine tuning in presence of a bounded memory. Evaluation is done\nagainst competitive baselines using four public datasets. Results show that the\nclassifier weights scaling and the removal of the distillation are both\nbeneficial.",
    "published_date": "2020-01-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.05755v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.05691v3",
    "title": "Learning Spatiotemporal Features via Video and Text Pair Discrimination",
    "authors": [
      "Tianhao Li",
      "Limin Wang"
    ],
    "author_ids": [],
    "abstract": "Current video representations heavily rely on learning from manually\nannotated video datasets which are time-consuming and expensive to acquire. We\nobserve videos are naturally accompanied by abundant text information such as\nYouTube titles and Instagram captions. In this paper, we leverage this\nvisual-textual connection to learn spatiotemporal features in an efficient\nweakly-supervised manner. We present a general cross-modal pair discrimination\n(CPD) framework to capture this correlation between a video and its associated\ntext. Specifically, we adopt noise-contrastive estimation to tackle the\ncomputational issue imposed by the huge amount of pair instance classes and\ndesign a practical curriculum learning strategy. We train our CPD models on\nboth standard video dataset (Kinetics-210k) and uncurated web video dataset\n(Instagram-300k) to demonstrate its effectiveness. Without further fine-tuning,\nthe learnt models obtain competitive results for action classification on\nKinetics under the linear classification protocol. Moreover, our visual model\nprovides an effective initialization to fine-tune on downstream tasks, which\nyields a remarkable performance gain for action recognition on UCF101 and\nHMDB51, compared with the existing state-of-the-art self-supervised training\nmethods. In addition, our CPD model yields a new state of the art for zero-shot\naction recognition on UCF101 by directly utilizing the learnt visual-textual\nembeddings. The code will be made available at\nhttps://github.com/MCG-NJU/CPD-Video.",
    "published_date": "2020-01-16T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.05691v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.05495v1",
    "title": "Stereotypical Bias Removal for Hate Speech Detection Task using Knowledge-based Generalizations",
    "authors": [
      "Pinkesh Badjatiya",
      "Manish Gupta",
      "Vasudeva Varma"
    ],
    "author_ids": [],
    "abstract": "With the ever-increasing cases of hate spread on social media platforms, it\nis critical to design abuse detection mechanisms to proactively avoid and\ncontrol such incidents. While there exist methods for hate speech detection,\nthey stereotype words and hence suffer from inherently biased training. Bias\nremoval has been traditionally studied for structured datasets, but we aim at\nbias mitigation from unstructured text data. In this paper, we make two\nimportant contributions. First, we systematically design methods to quantify\nthe bias for any model and propose algorithms for identifying the set of words\nwhich the model stereotypes. Second, we propose novel methods leveraging\nknowledge-based generalizations for bias-free learning. Knowledge-based\ngeneralization provides an effective way to encode knowledge because the\nabstraction they provide not only generalizes content but also facilitates\nretraction of information from the hate speech detection classifier, thereby\nreducing the imbalance. We experiment with multiple knowledge generalization\npolicies and analyze their effect on general performance and in mitigating\nbias. Our experiments with two real-world datasets, a Wikipedia Talk Pages\ndataset (WikiDetox) of size ~96k and a Twitter dataset of size ~24k, show that\nthe use of knowledge-based generalizations results in better performance by\nforcing the classifier to learn from generalized content. Our methods utilize\nexisting knowledge-bases and can easily be extended to other tasks",
    "published_date": "2020-01-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.05495v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.05414v1",
    "title": "Unbiased evaluation of ranking metrics reveals consistent performance in science and technology citation data",
    "authors": [
      "Shuqi Xu",
      "Manuel Sebastian Mariani",
      "Linyuan Lü",
      "Matúš Medo"
    ],
    "author_ids": [],
    "abstract": "Despite the increasing use of citation-based metrics for research evaluation\npurposes, we do not know yet which metrics best deliver on their promise to\ngauge the significance of a scientific paper or a patent. We assess 17\nnetwork-based metrics by their ability to identify milestone papers and patents\nin three large citation datasets. We find that traditional\ninformation-retrieval evaluation metrics are strongly affected by the interplay\nbetween the age distribution of the milestone items and age biases of the\nevaluated metrics. Outcomes of these metrics are therefore not representative\nof the metrics' ranking ability. We argue in favor of a modified evaluation\nprocedure that explicitly penalizes biased metrics and allows us to reveal\nmetrics' performance patterns that are consistent across the datasets. PageRank\nand LeaderRank turn out to be the best-performing ranking metrics when their\nage bias is suppressed by a simple transformation of the scores that they\nproduce, whereas other popular metrics, including citation count, HITS and\nCollective Influence, produce significantly worse ranking results.",
    "published_date": "2020-01-15T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.DL",
      "cs.IR",
      "physics.data-an"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.05414v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.05046v1",
    "title": "Robot Rights? Let's Talk about Human Welfare Instead",
    "authors": [
      "Abeba Birhane",
      "Jelle van Dijk"
    ],
    "author_ids": [],
    "abstract": "The 'robot rights' debate, and its related question of 'robot\nresponsibility', invokes some of the most polarized positions in AI ethics.\nWhile some advocate for granting robots rights on a par with human beings,\nothers, in a stark opposition argue that robots are not deserving of rights but\nare objects that should be our slaves. Grounded in post-Cartesian philosophical\nfoundations, we argue not just to deny robots 'rights', but to deny that\nrobots, as artifacts emerging out of and mediating human being, are the kinds\nof things that could be granted rights in the first place. Once we see robots\nas mediators of human being, we can understand how the `robots rights' debate\nis focused on first world problems, at the expense of urgent ethical concerns,\nsuch as machine bias, machine elicited human labour exploitation, and erosion\nof privacy all impacting society's least privileged individuals. We conclude\nthat, if human being is our starting point and human welfare is the primary\nconcern, the negative impacts emerging from machinic systems, as well as the\nlack of taking responsibility by people designing, selling and deploying such\nmachines, remains the most pressing ethical discussion in AI.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.05046v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04958v2",
    "title": "Differentially Private and Fair Classification via Calibrated Functional Mechanism",
    "authors": [
      "Jiahao Ding",
      "Xinyue Zhang",
      "Xiaohuan Li",
      "Junyi Wang",
      "Rong Yu",
      "Miao Pan"
    ],
    "author_ids": [],
    "abstract": "Machine learning is increasingly becoming a powerful tool to make decisions\nin a wide variety of applications, such as medical diagnosis and autonomous\ndriving. Privacy concerns related to the training data and unfair behaviors of\nsome decisions with regard to certain attributes (e.g., sex, race) are becoming\nmore critical. Thus, constructing a fair machine learning model while\nsimultaneously providing privacy protection becomes a challenging problem. In\nthis paper, we focus on the design of classification model with fairness and\ndifferential privacy guarantees by jointly combining functional mechanism and\ndecision boundary fairness. In order to enforce $\\epsilon$-differential privacy\nand fairness, we leverage the functional mechanism to add different amounts of\nLaplace noise regarding different attributes to the polynomial coefficients of\nthe objective function in consideration of fairness constraint. We further\npropose an utility-enhancement scheme, called relaxed functional mechanism by\nadding Gaussian noise instead of Laplace noise, hence achieving\n$(\\epsilon,\\delta)$-differential privacy. Based on the relaxed functional\nmechanism, we can design $(\\epsilon,\\delta)$-differentially private and fair\nclassification model. Moreover, our theoretical analysis and empirical results\ndemonstrate that our two approaches achieve both fairness and differential\nprivacy while preserving good utility and outperform the state-of-the-art\nalgorithms.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04958v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04861v1",
    "title": "Fairness in Learning-Based Sequential Decision Algorithms: A Survey",
    "authors": [
      "Xueru Zhang",
      "Mingyan Liu"
    ],
    "author_ids": [],
    "abstract": "Algorithmic fairness in decision-making has been studied extensively in\nstatic settings where one-shot decisions are made on tasks such as\nclassification. However, in practice most decision-making processes are of a\nsequential nature, where decisions made in the past may have an impact on\nfuture data. This is particularly the case when decisions affect the\nindividuals or users generating the data used for future decisions. In this\nsurvey, we review existing literature on the fairness of data-driven sequential\ndecision-making. We will focus on two types of sequential decisions: (1) past\ndecisions have no impact on the underlying user population and thus no impact\non future data; (2) past decisions have an impact on the underlying user\npopulation and therefore the future data, which can then impact future\ndecisions. In each case the impact of various fairness interventions on the\nunderlying population is examined.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04861v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04803v2",
    "title": "Improving Semantic Analysis on Point Clouds via Auxiliary Supervision of Local Geometric Priors",
    "authors": [
      "Lulu Tang",
      "Ke Chen",
      "Chaozheng Wu",
      "Yu Hong",
      "Kui Jia",
      "Zhixin Yang"
    ],
    "author_ids": [],
    "abstract": "Existing deep learning algorithms for point cloud analysis mainly concern\ndiscovering semantic patterns from global configuration of local geometries in\na supervised learning manner. However, very few explore geometric properties\nrevealing local surface manifolds embedded in 3D Euclidean space to\ndiscriminate semantic classes or object parts as additional supervision\nsignals. This paper is the first attempt to propose a unique multi-task\ngeometric learning network to improve semantic analysis by auxiliary geometric\nlearning with local shape properties, which can be either generated via\nphysical computation from point clouds themselves as self-supervision signals\nor provided as privileged information. Owing to explicitly encoding local shape\nmanifolds in favor of semantic analysis, the proposed geometric self-supervised\nand privileged learning algorithms can achieve superior performance to their\nbackbone baselines and other state-of-the-art methods, which are verified in\nthe experiments on the popular benchmarks.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04803v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04754v3",
    "title": "Learning Overlapping Representations for the Estimation of Individualized Treatment Effects",
    "authors": [
      "Yao Zhang",
      "Alexis Bellot",
      "Mihaela van der Schaar"
    ],
    "author_ids": [],
    "abstract": "The choice of making an intervention depends on its potential benefit or harm\nin comparison to alternatives. Estimating the likely outcome of alternatives\nfrom observational data is a challenging problem as all outcomes are never\nobserved, and selection bias precludes the direct comparison of differently\nintervened groups. Despite their empirical success, we show that algorithms\nthat learn domain-invariant representations of inputs (on which to make\npredictions) are often inappropriate, and develop generalization bounds that\ndemonstrate the dependence on domain overlap and highlight the need for\ninvertible latent maps. Based on these results, we develop a deep kernel\nregression algorithm and posterior regularization framework that substantially\noutperforms the state-of-the-art on a variety of benchmarks data sets.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04754v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09762v1",
    "title": "Bias in Data-driven AI Systems -- An Introductory Survey",
    "authors": [
      "Eirini Ntoutsi",
      "Pavlos Fafalios",
      "Ujwal Gadiraju",
      "Vasileios Iosifidis",
      "Wolfgang Nejdl",
      "Maria-Esther Vidal",
      "Salvatore Ruggieri",
      "Franco Turini",
      "Symeon Papadopoulos",
      "Emmanouil Krasanakis",
      "Ioannis Kompatsiaris",
      "Katharina Kinder-Kurlanda",
      "Claudia Wagner",
      "Fariba Karimi",
      "Miriam Fernandez",
      "Harith Alani",
      "Bettina Berendt",
      "Tina Kruegel",
      "Christian Heinze",
      "Klaus Broelemann",
      "Gjergji Kasneci",
      "Thanassis Tiropanis",
      "Steffen Staab"
    ],
    "author_ids": [],
    "abstract": "AI-based systems are widely employed nowadays to make decisions that have\nfar-reaching impacts on individuals and society. Their decisions might affect\neveryone, everywhere and anytime, entailing concerns about potential human\nrights issues. Therefore, it is necessary to move beyond traditional AI\nalgorithms optimized for predictive performance and embed ethical and legal\nprinciples in their design, training and deployment to ensure social good while\nstill benefiting from the huge potential of the AI technology. The goal of this\nsurvey is to provide a broad multi-disciplinary overview of the area of bias in\nAI systems, focusing on technical challenges and solutions as well as to\nsuggest new research directions towards approaches well-grounded in a legal\nframe. In this survey, we focus on data-driven AI, as a large part of AI is\npowered nowadays by (big) data and powerful Machine Learning (ML) algorithms.\nIf otherwise not specified, we use the general term bias to describe problems\nrelated to the gathering or processing of data that might result in prejudiced\ndecisions on the bases of demographic features like race, sex, etc.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09762v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04639v1",
    "title": "Robust Gaussian Process Regression with a Bias Model",
    "authors": [
      "Chiwoo Park",
      "David J. Borth",
      "Nicholas S. Wilson",
      "Chad N. Hunter",
      "Fritz J. Friedersdorf"
    ],
    "author_ids": [],
    "abstract": "This paper presents a new approach to a robust Gaussian process (GP)\nregression. Most existing approaches replace an outlier-prone Gaussian\nlikelihood with a non-Gaussian likelihood induced from a heavy tail\ndistribution, such as the Laplace distribution and Student-t distribution.\nHowever, the use of a non-Gaussian likelihood would incur the need for a\ncomputationally expensive Bayesian approximate computation in the posterior\ninferences. The proposed approach models an outlier as a noisy and biased\nobservation of an unknown regression function, and accordingly, the likelihood\ncontains bias terms to explain the degree of deviations from the regression\nfunction. We entail how the biases can be estimated accurately with other\nhyperparameters by a regularized maximum likelihood estimation. Conditioned on\nthe bias estimates, the robust GP regression can be reduced to a standard GP\nregression problem with analytical forms of the predictive mean and variance\nestimates. Therefore, the proposed approach is simple and very computationally\nattractive. It also gives a very robust and accurate GP estimate for many\ntested scenarios. For the numerical evaluation, we perform a comprehensive\nsimulation study to evaluate the proposed approach with the comparison to the\nexisting robust GP approaches under various simulated scenarios of different\noutlier proportions and different noise levels. The approach is applied to data\nfrom two measurement systems, where the predictors are based on robust\nenvironmental parameter measurements and the response variables utilize more\ncomplex chemical sensing methods that contain a certain percentage of outliers.\nThe utility of the measurement systems and value of the environmental data are\nimproved through the computationally efficient GP regression and bias model.",
    "published_date": "2020-01-14T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML",
      "62G08"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04639v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09765v1",
    "title": "Model-assisted cohort selection with bias analysis for generating large-scale cohorts from the EHR for oncology research",
    "authors": [
      "Benjamin Birnbaum",
      "Nathan Nussbaum",
      "Katharina Seidl-Rathkopf",
      "Monica Agrawal",
      "Melissa Estevez",
      "Evan Estola",
      "Joshua Haimson",
      "Lucy He",
      "Peter Larson",
      "Paul Richardson"
    ],
    "author_ids": [],
    "abstract": "Objective Electronic health records (EHRs) are a promising source of data for\nhealth outcomes research in oncology. A challenge in using EHR data is that\nselecting cohorts of patients often requires information in unstructured parts\nof the record. Machine learning has been used to address this, but even\nhigh-performing algorithms may select patients in a non-random manner and bias\nthe resulting cohort. To improve the efficiency of cohort selection while\nmeasuring potential bias, we introduce a technique called Model-Assisted Cohort\nSelection (MACS) with Bias Analysis and apply it to the selection of metastatic\nbreast cancer (mBC) patients. Materials and Methods We trained a model on\n17,263 patients using term-frequency inverse-document-frequency (TF-IDF) and\nlogistic regression. We used a test set of 17,292 patients to measure algorithm\nperformance and perform Bias Analysis. We compared the cohort generated by MACS\nto the cohort that would have been generated without MACS as reference\nstandard, first by comparing distributions of an extensive set of clinical and\ndemographic variables and then by comparing the results of two analyses\naddressing existing example research questions. Results Our algorithm had an\narea under the curve (AUC) of 0.976, a sensitivity of 96.0%, and an abstraction\nefficiency gain of 77.9%. During Bias Analysis, we found no large differences\nin baseline characteristics and no differences in the example analyses.\nConclusion MACS with bias analysis can significantly improve the efficiency of\ncohort selection on EHR data while instilling confidence that outcomes research\nperformed on the resulting cohort will not be biased.",
    "published_date": "2020-01-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04520v1",
    "title": "Security Vetting Process of Smart-home Assistant Applications: A First Look and Case Studies",
    "authors": [
      "Hang Hu",
      "Limin Yang",
      "Shihan Lin",
      "Gang Wang"
    ],
    "author_ids": [],
    "abstract": "The popularity of smart-home assistant systems such as Amazon Alexa and\nGoogle Home leads to a booming third-party application market (over 70,000\napplications across the two stores). While existing works have revealed\nsecurity issues in these systems, it is not well understood how to help\napplication developers to enforce security requirements. In this paper, we\nperform a preliminary case study to examine the security vetting mechanisms\nadopted by Amazon Alexa and Google Home app stores. With a focus on the\nauthentication mechanisms between Alexa/Google cloud and third-party\napplication servers (i.e. endpoints), we show the current security vetting is\ninsufficient as developer mistakes can not be effectively detected and\nnotified. A weak authentication would allow attackers to spoof the cloud to\ninsert/retrieve data into/from the application endpoints. We validate the\nattack through ethical proof-of-concept experiments. To confirm vulnerable\napplications have indeed passed the security vetting and entered the markets,\nwe develop a heuristic-based searching method. We find 219 real-world Alexa\nendpoints that carry the vulnerability, many of which are related to critical\napplications that control smart home devices and electronic cars. We have\nnotified Amazon and Google about our findings and offered our suggestions to\nmitigate the issue.",
    "published_date": "2020-01-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04520v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.04377v1",
    "title": "When Humans Aren't Optimal: Robots that Collaborate with Risk-Aware Humans",
    "authors": [
      "Minae Kwon",
      "Erdem Biyik",
      "Aditi Talati",
      "Karan Bhasin",
      "Dylan P. Losey",
      "Dorsa Sadigh"
    ],
    "author_ids": [],
    "abstract": "In order to collaborate safely and efficiently, robots need to anticipate how\ntheir human partners will behave. Some of today's robots model humans as if\nthey were also robots, and assume users are always optimal. Other robots\naccount for human limitations, and relax this assumption so that the human is\nnoisily rational. Both of these models make sense when the human receives\ndeterministic rewards: i.e., gaining either $100 or $130 with certainty. But in\nreal world scenarios, rewards are rarely deterministic. Instead, we must make\nchoices subject to risk and uncertainty--and in these settings, humans exhibit\na cognitive bias towards suboptimal behavior. For example, when deciding\nbetween gaining $100 with certainty or $130 only 80% of the time, people tend\nto make the risk-averse choice--even though it leads to a lower expected gain!\nIn this paper, we adopt a well-known Risk-Aware human model from behavioral\neconomics called Cumulative Prospect Theory and enable robots to leverage this\nmodel during human-robot interaction (HRI). In our user studies, we offer\nsupporting evidence that the Risk-Aware model more accurately predicts\nsuboptimal human behavior. We find that this increased modeling accuracy\nresults in safer and more efficient human-robot collaboration. Overall, we\nextend existing rational human models so that collaborative robots can\nanticipate and plan around suboptimal human behavior during HRI.",
    "published_date": "2020-01-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04377v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04335v2",
    "title": "Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society",
    "authors": [
      "Carina Prunkl",
      "Jess Whittlestone"
    ],
    "author_ids": [],
    "abstract": "One way of carving up the broad \"AI ethics and society\" research space that\nhas emerged in recent years is to distinguish between \"near-term\" and\n\"long-term\" research. While such ways of breaking down the research space can\nbe useful, we put forward several concerns about the near/long-term distinction\ngaining too much prominence in how research questions and priorities are\nframed. We highlight some ambiguities and inconsistencies in how the\ndistinction is used, and argue that while there are differing priorities within\nthis broad research community, these differences are not well-captured by the\nnear/long-term distinction. We unpack the near/long-term distinction into four\ndifferent dimensions, and propose some ways that researchers can communicate\nmore clearly about their work and priorities using these dimensions. We suggest\nthat moving towards a more nuanced conversation about research priorities can\nhelp establish new opportunities for collaboration, aid the development of more\nconsistent and coherent research agendas, and enable identification of\npreviously neglected research areas.",
    "published_date": "2020-01-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04335v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04270v1",
    "title": "Perspectives and Ethics of the Autonomous Artificial Thinking Systems",
    "authors": [
      "Joël Colloc"
    ],
    "author_ids": [],
    "abstract": "The feasibility of autonomous artificial thinking systems needs to compare\nthe way the human beings acquire their information and develops the thought\nwith the current capacities of the autonomous information systems. Our model\nuses four hierarchies: the hierarchy of information systems, the cognitive\nhierarchy, the linguistic hierarchy and the digital informative hierarchy that\ncombines artificial intelligence, the power of computers models, methods and\ntools to develop autonomous information systems. The question of the capability\nof autonomous system to provide a form of artificial thought arises with the\nethical consequences on the social life and the perspective of transhumanism.",
    "published_date": "2020-01-13T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04270v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.03959v2",
    "title": "Average AoI in Multi-Source Systems with Source-Aware Packet Management",
    "authors": [
      "Mohammad Moltafet",
      "Markus Leinonen",
      "Marian Codreanu"
    ],
    "author_ids": [],
    "abstract": "We study the information freshness under three different source aware packet\nmanagement policies in a status update system consisting of two independent\nsources and one server. The packets of each source are generated according to\nthe Poisson process and the packets are served according to an exponentially\ndistributed service time. We derive the average age of information (AoI) of\neach source using the stochastic hybrid systems (SHS) technique for each packet\nmanagement policy. In Policy 1, the queue can contain at most two waiting\npackets at the same time (in addition to the packet under service), one packet\nof source 1 and one packet of source 2. When the server is busy at an arrival\nof a packet, the possible packet of the same source waiting in the queue\n(hence, source-aware) is replaced by the arrived fresh packet. In Policy 2, the\nsystem (i.e., the waiting queue and the server) can contain at most two\npackets, one from each source. When the server is busy at an arrival of a\npacket, the possible packet of the same source in the system is replaced by the\nfresh packet. Policy 3 is similar to Policy 2 but it does not permit preemption\nin service, i.e., while a packet is under service all new arrivals from the\nsame source are blocked and cleared. Numerical results are provided to assess\nthe fairness between sources and the sum average AoI of the proposed policies.",
    "published_date": "2020-01-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03959v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.03884v5",
    "title": "Compressibility Measures for Affinely Singular Random Vectors",
    "authors": [
      "Mohammad-Amin Charusaie",
      "Arash Amini",
      "Stefano Rini"
    ],
    "author_ids": [],
    "abstract": "There are several ways to measure the compressibility of a random measure;\nthey include general approaches such as using the rate-distortion curve, as\nwell as more specific notions, such as the Renyi information dimension (RID).\nThe RID parameter indicates the concentration of the measure around\nlower-dimensional subsets of the space. While the evaluation of such\ncompressibility parameters is well-studied for continuous and discrete\nmeasures, the case of discrete-continuous measures is quite subtle. In this\npaper, we focus on a class of multi-dimensional random measures that have\nsingularities on affine lower-dimensional subsets. This class of distributions\nnaturally arises when considering linear transformation of component-wise\nindependent discrete-continuous random variables. To measure the\ncompressibility of such distributions, we introduce the new notion of\ndimensional-rate bias (DRB) which is closely related to the entropy and\ndifferential entropy in discrete and continuous cases, respectively. Similar to\nentropy and differential entropy, DRB is useful in evaluating the mutual\ninformation between distributions of the aforementioned type. Besides the DRB,\nwe also evaluate the the RID of these distributions. We further provide an\nupper-bound for the RID of multi-dimensional random measures that are obtained\nby Lipschitz functions of component-wise independent discrete-continuous random\nvariables ($\\mathbf{X}$). The upper-bound is shown to be achievable when the\nLipschitz function is $A \\mathbf{X}$, where $A$ satisfies\n{\\changed$\\spark({A_{m\\times n}}) = m+1$} (e.g., Vandermonde matrices). When\nconsidering discrete-domain moving-average processes with non-Gaussian\nexcitation noise, the above results allow us to evaluate the block-average RID\nand DRB, as well as to determine a relationship between these parameters and\nother existing compressibility measures.",
    "published_date": "2020-01-12T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IT",
      "cs.IR",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03884v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.03717v2",
    "title": "Verifiable and Auditable Digital Interchange Framework",
    "authors": [
      "Prabal Banerjee",
      "Dushyant Behl",
      "Palanivel Kodeswaran",
      "Chaitanya Kumar",
      "Sushmita Ruj",
      "Sayandeep Sen"
    ],
    "author_ids": [],
    "abstract": "We address the problem of fairness and transparency in online marketplaces\nselling digital content, where all parties are not actively participating in\nthe trade. We present the design, implementation and evaluation of VADER, a\nhighly scalable solution for multi-party fair digital exchange that combines\nthe trusted execution of blockchains with intelligent protocol design and\nincentivization schemes. We prototype VADER on Hyperledger Fabric and\nextensively evaluate our system on a realistic testbed spanning five public\ncloud datacenters, spread across four continents. Our results demonstrate that\nVADER adds only minimal overhead of 16% in median case compared to a baseline\nsolution, while significantly outperforming a naive blockchain based solution\nthat adds an overhead of 764%.",
    "published_date": "2020-01-11T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03717v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.03632v1",
    "title": "Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks",
    "authors": [
      "R. Thomas McCoy",
      "Robert Frank",
      "Tal Linzen"
    ],
    "author_ids": [],
    "abstract": "Learners that are exposed to the same training data might generalize\ndifferently due to differing inductive biases. In neural network models,\ninductive biases could in theory arise from any aspect of the model\narchitecture. We investigate which architectural factors affect the\ngeneralization behavior of neural sequence-to-sequence models trained on two\nsyntactic tasks, English question formation and English tense reinflection. For\nboth tasks, the training set is consistent with a generalization based on\nhierarchical structure and a generalization based on linear order. All\narchitectural factors that we investigated qualitatively affected how models\ngeneralized, including factors with no clear connection to hierarchical\nstructure. For example, LSTMs and GRUs displayed qualitatively different\ninductive biases. However, the only factor that consistently contributed a\nhierarchical bias across tasks was the use of a tree-structured model rather\nthan a model with sequential recurrence, suggesting that human-like syntactic\ngeneralization requires architectural syntactic structure.",
    "published_date": "2020-01-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03632v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.03511v1",
    "title": "Ethics of Technology needs more Political Philosophy",
    "authors": [
      "Johannes Himmelreich"
    ],
    "author_ids": [],
    "abstract": "The ongoing debate on the ethics of self-driving cars typically focuses on\ntwo approaches to answering ethical questions: moral philosophy and social\nscience. I argue that these two approaches are both lacking. We should neither\ndeduce answers from individual moral theories nor should we expect social\nscience to give us complete answers. To supplement these approaches, we should\nturn to political philosophy. The issues we face are collective decisions that\nwe make together rather than individual decisions we make in light of what we\neach have reason to value. Political philosophy adds three basic concerns to\nour conceptual toolkit: reasonable pluralism, human agency, and legitimacy.\nThese three concerns have so far been largely overlooked in the debate on the\nethics of self-driving cars.",
    "published_date": "2020-01-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "K.4.0; K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03511v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.03376v1",
    "title": "microbatchGAN: Stimulating Diversity with Multi-Adversarial Discrimination",
    "authors": [
      "Gonçalo Mordido",
      "Haojin Yang",
      "Christoph Meinel"
    ],
    "author_ids": [],
    "abstract": "We propose to tackle the mode collapse problem in generative adversarial\nnetworks (GANs) by using multiple discriminators and assigning a different\nportion of each minibatch, called microbatch, to each discriminator. We\ngradually change each discriminator's task from distinguishing between real and\nfake samples to discriminating samples coming from inside or outside its\nassigned microbatch by using a diversity parameter $\\alpha$. The generator is\nthen forced to promote variety in each minibatch to make the microbatch\ndiscrimination harder to achieve by each discriminator. Thus, all models in our\nframework benefit from having variety in the generated set to reduce their\nrespective losses. We show evidence that our solution promotes sample diversity\nsince early training stages on multiple datasets.",
    "published_date": "2020-01-10T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03376v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.03203v3",
    "title": "Theory In, Theory Out: The uses of social theory in machine learning for social science",
    "authors": [
      "Jason Radford",
      "Kenneth Joseph"
    ],
    "author_ids": [],
    "abstract": "Research at the intersection of machine learning and the social sciences has\nprovided critical new insights into social behavior. At the same time, a\nvariety of critiques have been raised ranging from technical issues with the\ndata used and features constructed, problematic assumptions built into models,\ntheir limited interpretability, and their contribution to bias and inequality.\nWe argue such issues arise primarily because of the lack of social theory at\nvarious stages of the model building and analysis. In the first half of this\npaper, we walk through how social theory can be used to answer the basic\nmethodological and interpretive questions that arise at each stage of the\nmachine learning pipeline. In the second half, we show how theory can be used\nto assess and compare the quality of different social learning models,\nincluding interpreting, generalizing, and assessing the fairness of models. We\nbelieve this paper can act as a guide for computer and social scientists alike\nto navigate the substantive questions involved in applying the tools of machine\nlearning to social data.",
    "published_date": "2020-01-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03203v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.03152v2",
    "title": "Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias",
    "authors": [
      "Krishna Kumar Singh",
      "Dhruv Mahajan",
      "Kristen Grauman",
      "Yong Jae Lee",
      "Matt Feiszli",
      "Deepti Ghadiyaram"
    ],
    "author_ids": [],
    "abstract": "Existing models often leverage co-occurrences between objects and their\ncontext to improve recognition accuracy. However, strongly relying on context\nrisks a model's generalizability, especially when typical co-occurrence\npatterns are absent. This work focuses on addressing such contextual biases to\nimprove the robustness of the learnt feature representations. Our goal is to\naccurately recognize a category in the absence of its context, without\ncompromising on performance when it co-occurs with context. Our key idea is to\ndecorrelate feature representations of a category from its co-occurring\ncontext. We achieve this by learning a feature subspace that explicitly\nrepresents categories occurring in the absence of context along side a joint\nfeature subspace that represents both categories and context. Our very simple\nyet effective method is extensible to two multi-label tasks -- object and\nattribute classification. On 4 challenging datasets, we demonstrate the\neffectiveness of our method in reducing contextual bias.",
    "published_date": "2020-01-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03152v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.03071v2",
    "title": "Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification",
    "authors": [
      "Chris Dulhanty",
      "Alexander Wong"
    ],
    "author_ids": [],
    "abstract": "Modern face recognition systems leverage datasets containing images of\nhundreds of thousands of specific individuals' faces to train deep\nconvolutional neural networks to learn an embedding space that maps an\narbitrary individual's face to a vector representation of their identity. The\nperformance of a face recognition system in face verification (1:1) and face\nidentification (1:N) tasks is directly related to the ability of an embedding\nspace to discriminate between identities. Recently, there has been significant\npublic scrutiny into the source and privacy implications of large-scale face\nrecognition training datasets such as MS-Celeb-1M and MegaFace, as many people\nare uncomfortable with their face being used to train dual-use technologies\nthat can enable mass surveillance. However, the impact of an individual's\ninclusion in training data on a derived system's ability to recognize them has\nnot previously been studied. In this work, we audit ArcFace, a\nstate-of-the-art, open source face recognition system, in a large-scale face\nidentification experiment with more than one million distractor images. We find\na Rank-1 face identification accuracy of 79.71% for individuals present in the\nmodel's training data and an accuracy of 75.73% for those not present. This\nmodest difference in accuracy demonstrates that face recognition systems using\ndeep learning work better for individuals they are trained on, which has\nserious privacy implications when one considers all major open source face\nrecognition training datasets do not obtain informed consent from individuals\nduring their collection.",
    "published_date": "2020-01-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.03071v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.02878v3",
    "title": "Positive algorithmic bias cannot stop fragmentation in homophilic networks",
    "authors": [
      "Chris Blex",
      "Taha Yasseri"
    ],
    "author_ids": [],
    "abstract": "Fragmentation, echo chambers, and their amelioration in social networks have\nbeen a growing concern in the academic and non-academic world. This paper shows\nhow, under the assumption of homophily, echo chambers and fragmentation are\nsystem-immanent phenomena of highly flexible social networks, even under ideal\nconditions for heterogeneity. We achieve this by finding an analytical,\nnetwork-based solution to the Schelling model and by proving that weak ties do\nnot hinder the process. Furthermore, we derive that no level of positive\nalgorithmic bias in the form of rewiring is capable of preventing fragmentation\nand its effect on reducing the fragmentation speed is negligible.",
    "published_date": "2020-01-09T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.02878v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.02668v1",
    "title": "Coherent Quantum Channel Discrimination",
    "authors": [
      "Mark M. Wilde"
    ],
    "author_ids": [],
    "abstract": "This paper introduces coherent quantum channel discrimination as a coherent\nversion of conventional quantum channel discrimination. Coherent channel\ndiscrimination is phrased here as a quantum interactive proof system between a\nverifier and a prover, wherein the goal of the prover is to distinguish two\nchannels called in superposition in order to distill a Bell state at the end.\nThe key measure considered here is the success probability of distilling a Bell\nstate, and I prove that this success probability does not increase under the\naction of a quantum superchannel, thus establishing this measure as a\nfundamental measure of channel distinguishability. Also, I establish some\nbounds on this success probability in terms of the success probability of\nconventional channel discrimination. Finally, I provide an explicit\nsemi-definite program that can compute the success probability.",
    "published_date": "2020-01-08T00:00:00",
    "year": 2020,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.02668v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.09773v1",
    "title": "Algorithmic Fairness from a Non-ideal Perspective",
    "authors": [
      "Sina Fazelpour",
      "Zachary C. Lipton"
    ],
    "author_ids": [],
    "abstract": "Inspired by recent breakthroughs in predictive modeling, practitioners in\nboth industry and government have turned to machine learning with hopes of\noperationalizing predictions to drive automated decisions. Unfortunately, many\nsocial desiderata concerning consequential decisions, such as justice or\nfairness, have no natural formulation within a purely predictive framework. In\nefforts to mitigate these problems, researchers have proposed a variety of\nmetrics for quantifying deviations from various statistical parities that we\nmight expect to observe in a fair world and offered a variety of algorithms in\nattempts to satisfy subsets of these parities or to trade off the degree to\nwhich they are satisfied against utility. In this paper, we connect this\napproach to \\emph{fair machine learning} to the literature on ideal and\nnon-ideal methodological approaches in political philosophy. The ideal approach\nrequires positing the principles according to which a just world would operate.\nIn the most straightforward application of ideal theory, one supports a\nproposed policy by arguing that it closes a discrepancy between the real and\nthe perfectly just world. However, by failing to account for the mechanisms by\nwhich our non-ideal world arose, the responsibilities of various\ndecision-makers, and the impacts of proposed policies, naive applications of\nideal thinking can lead to misguided interventions. In this paper, we\ndemonstrate a connection between the fair machine learning literature and the\nideal approach in political philosophy, and argue that the increasingly\napparent shortcomings of proposed fair machine learning algorithms reflect\nbroader troubles faced by the ideal approach. We conclude with a critical\ndiscussion of the harms of misguided solutions, a reinterpretation of\nimpossibility results, and directions for future research.",
    "published_date": "2020-01-08T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09773v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.02271v2",
    "title": "Revealing Neural Network Bias to Non-Experts Through Interactive Counterfactual Examples",
    "authors": [
      "Chelsea M. Myers",
      "Evan Freed",
      "Luis Fernando Laris Pardo",
      "Anushay Furqan",
      "Sebastian Risi",
      "Jichen Zhu"
    ],
    "author_ids": [],
    "abstract": "AI algorithms are not immune to biases. Traditionally, non-experts have\nlittle control in uncovering potential social bias (e.g., gender bias) in the\nalgorithms that may impact their lives. We present a preliminary design for an\ninteractive visualization tool CEB to reveal biases in a commonly used AI\nmethod, Neural Networks (NN). CEB combines counterfactual examples and\nabstraction of an NN decision process to empower non-experts to detect bias.\nThis paper presents the design of CEB and initial findings of an expert panel\n(n=6) with AI, HCI, and Social science experts.",
    "published_date": "2020-01-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.02271v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.02112v2",
    "title": "Multitask learning over graphs: An Approach for Distributed, Streaming Machine Learning",
    "authors": [
      "Roula Nassif",
      "Stefan Vlaski",
      "Cedric Richard",
      "Jie Chen",
      "Ali H. Sayed"
    ],
    "author_ids": [],
    "abstract": "The problem of learning simultaneously several related tasks has received\nconsiderable attention in several domains, especially in machine learning with\nthe so-called multitask learning problem or learning to learn problem [1], [2].\nMultitask learning is an approach to inductive transfer learning (using what is\nlearned for one problem to assist in another problem) and helps improve\ngeneralization performance relative to learning each task separately by using\nthe domain information contained in the training signals of related tasks as an\ninductive bias. Several strategies have been derived within this community\nunder the assumption that all data are available beforehand at a fusion center.\nHowever, recent years have witnessed an increasing ability to collect data in a\ndistributed and streaming manner. This requires the design of new strategies\nfor learning jointly multiple tasks from streaming data over distributed (or\nnetworked) systems. This article provides an overview of multitask strategies\nfor learning and adaptation over networks. The working hypothesis for these\nstrategies is that agents are allowed to cooperate with each other in order to\nlearn distinct, though related tasks. The article shows how cooperation steers\nthe network limiting point and how different cooperation rules allow to promote\ndifferent task relatedness models. It also explains how and when cooperation\nover multitask networks outperforms non-cooperative strategies.",
    "published_date": "2020-01-07T00:00:00",
    "year": 2020,
    "categories": [
      "eess.SP",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.02112v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01900v2",
    "title": "Regularization via Structural Label Smoothing",
    "authors": [
      "Weizhi Li",
      "Gautam Dasarathy",
      "Visar Berisha"
    ],
    "author_ids": [],
    "abstract": "Regularization is an effective way to promote the generalization performance\nof machine learning models. In this paper, we focus on label smoothing, a form\nof output distribution regularization that prevents overfitting of a neural\nnetwork by softening the ground-truth labels in the training data in an attempt\nto penalize overconfident outputs. Existing approaches typically use\ncross-validation to impose this smoothing, which is uniform across all training\ndata. In this paper, we show that such label smoothing imposes a quantifiable\nbias in the Bayes error rate of the training data, with regions of the feature\nspace with high overlap and low marginal likelihood having a lower bias and\nregions of low overlap and high marginal likelihood having a higher bias. These\ntheoretical results motivate a simple objective function for data-dependent\nsmoothing to mitigate the potential negative consequences of the operation\nwhile maintaining its desirable properties as a regularizer. We call this\napproach Structural Label Smoothing (SLS). We implement SLS and empirically\nvalidate on synthetic, Higgs, SVHN, CIFAR-10, and CIFAR-100 datasets. The\nresults confirm our theoretical insights and demonstrate the effectiveness of\nthe proposed method in comparison to traditional label smoothing.",
    "published_date": "2020-01-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01900v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01895v1",
    "title": "Machine-learning classifiers for logographic name matching in public health applications: approaches for incorporating phonetic, visual, and keystroke similarity in large-scale probabilistic record linkage",
    "authors": [
      "Philip A. Collender",
      "Zhiyue Tom Hu",
      "Charles Li",
      "Qu Cheng",
      "Xintong Li",
      "Yue You",
      "Song Liang",
      "Changhong Yang",
      "Justin V. Remais"
    ],
    "author_ids": [],
    "abstract": "Approximate string-matching methods to account for complex variation in\nhighly discriminatory text fields, such as personal names, can enhance\nprobabilistic record linkage. However, discriminating between matching and\nnon-matching strings is challenging for logographic scripts, where similarities\nin pronunciation, appearance, or keystroke sequence are not directly encoded in\nthe string data. We leverage a large Chinese administrative dataset with known\nmatch status to develop logistic regression and Xgboost classifiers integrating\nmeasures of visual, phonetic, and keystroke similarity to enhance\nidentification of potentially-matching name pairs. We evaluate three methods of\nleveraging name similarity scores in large-scale probabilistic record linkage,\nwhich can adapt to varying match prevalence and information in supporting\nfields: (1) setting a threshold score based on predicted quality of\nname-matching across all record pairs; (2) setting a threshold score based on\npredicted discriminatory power of the linkage model; and (3) using empirical\nscore distributions among matches and nonmatches to perform Bayesian adjustment\nof matching probabilities estimated from exact-agreement linkage. In\nexperiments on holdout data, as well as data simulated with varying name error\nrates and supporting fields, a logistic regression classifier incorporated via\nthe Bayesian method demonstrated marked improvements over exact-agreement\nlinkage with respect to discriminatory power, match probability estimation, and\naccuracy, reducing the total number of misclassified record pairs by 21% in\ntest data and up to an average of 93% in simulated datasets. Our results\ndemonstrate the value of incorporating visual, phonetic, and keystroke\nsimilarity for logographic name matching, as well as the promise of our\nBayesian approach to leverage name-matching within large-scale record linkage.",
    "published_date": "2020-01-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.CL",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01895v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01861v2",
    "title": "Vamsa: Automated Provenance Tracking in Data Science Scripts",
    "authors": [
      "Mohammad Hossein Namaki",
      "Avrilia Floratou",
      "Fotis Psallidas",
      "Subru Krishnan",
      "Ashvin Agrawal",
      "Yinghui Wu",
      "Yiwen Zhu",
      "Markus Weimer"
    ],
    "author_ids": [],
    "abstract": "There has recently been a lot of ongoing research in the areas of fairness,\nbias and explainability of machine learning (ML) models due to the self-evident\nor regulatory requirements of various ML applications. We make the following\nobservation: All of these approaches require a robust understanding of the\nrelationship between ML models and the data used to train them. In this work,\nwe introduce the ML provenance tracking problem: the fundamental idea is to\nautomatically track which columns in a dataset have been used to derive the\nfeatures/labels of an ML model. We discuss the challenges in capturing such\ninformation in the context of Python, the most common language used by data\nscientists. We then present Vamsa, a modular system that extracts provenance\nfrom Python scripts without requiring any changes to the users' code. Using 26K\nreal data science scripts, we verify the effectiveness of Vamsa in terms of\ncoverage, and performance. We also evaluate Vamsa's accuracy on a smaller\nsubset of manually labeled data. Our analysis shows that Vamsa's precision and\nrecall range from 90.4% to 99.1% and its latency is in the order of\nmilliseconds for average size scripts. Drawing from our experience in deploying\nML models in production, we also present an example in which Vamsa helps\nautomatically identify models that are affected by data corruption issues.",
    "published_date": "2020-01-07T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01861v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01796v5",
    "title": "Fair Active Learning",
    "authors": [
      "Hadis Anahideh",
      "Abolfazl Asudeh",
      "Saravanan Thirumuruganathan"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) is increasingly being used in high-stakes applications\nimpacting society. Therefore, it is of critical importance that ML models do\nnot propagate discrimination. Collecting accurate labeled data in societal\napplications is challenging and costly. Active learning is a promising approach\nto build an accurate classifier by interactively querying an oracle within a\nlabeling budget. We design algorithms for fair active learning that carefully\nselects data points to be labeled so as to balance model accuracy and fairness.\nWe demonstrate the effectiveness and efficiency of our proposed algorithms over\nwidely used benchmark datasets using demographic parity and equalized odds\nnotions of fairness.",
    "published_date": "2020-01-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01796v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01523v3",
    "title": "Think Locally, Act Globally: Federated Learning with Local and Global Representations",
    "authors": [
      "Paul Pu Liang",
      "Terrance Liu",
      "Liu Ziyin",
      "Nicholas B. Allen",
      "Randy P. Auerbach",
      "David Brent",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ],
    "author_ids": [],
    "abstract": "Federated learning is a method of training models on private data distributed\nover multiple devices. To keep device data private, the global model is trained\nby only communicating parameters and updates which poses scalability challenges\nfor large models. To this end, we propose a new federated learning algorithm\nthat jointly learns compact local representations on each device and a global\nmodel across all devices. As a result, the global model can be smaller since it\nonly operates on local representations, reducing the number of communicated\nparameters. Theoretically, we provide a generalization analysis which shows\nthat a combination of local and global models reduces both variance in the data\nas well as variance across device distributions. Empirically, we demonstrate\nthat local models enable communication-efficient training while retaining\nperformance. We also evaluate on the task of personalized mood prediction from\nreal-world mobile data where privacy is key. Finally, local models handle\nheterogeneous data from new devices, and learn fair representations that\nobfuscate protected attributes such as race, age, and gender.",
    "published_date": "2020-01-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.DC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01523v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01511v1",
    "title": "Decentralization in Digital Societies -- A Design Paradox",
    "authors": [
      "Evangelos Pournaras"
    ],
    "author_ids": [],
    "abstract": "Digital societies come with a design paradox: On the one hand, technologies,\nsuch as Internet of Things, pervasive and ubiquitous systems, allow a\ndistributed local intelligence in interconnected devices of our everyday life\nsuch as smart phones, smart thermostats, self-driving cars, etc. On the other\nhand, Big Data collection and storage is managed in a highly centralized\nfashion, resulting in privacy-intrusion, surveillance actions, discriminatory\nand segregation social phenomena. What is the difference between a distributed\nand a decentralized system design? How \"decentralized\" is the processing of our\ndata nowadays? Does centralized design undermine autonomy? Can the level of\ndecentralization in the implemented technologies influence ethical and social\ndimensions, such as social justice? Can decentralization convey sustainability?\nAre there parallelisms between the decentralization of digital technology and\nthe decentralization of urban development?",
    "published_date": "2020-01-06T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01511v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.01227v1",
    "title": "From Learning to Meta-Learning: Reduced Training Overhead and Complexity for Communication Systems",
    "authors": [
      "Osvaldo Simeone",
      "Sangwoo Park",
      "Joonhyuk Kang"
    ],
    "author_ids": [],
    "abstract": "Machine learning methods adapt the parameters of a model, constrained to lie\nin a given model class, by using a fixed learning procedure based on data or\nactive observations. Adaptation is done on a per-task basis, and retraining is\nneeded when the system configuration changes. The resulting inefficiency in\nterms of data and training time requirements can be mitigated, if domain\nknowledge is available, by selecting a suitable model class and learning\nprocedure, collectively known as inductive bias. However, it is generally\ndifficult to encode prior knowledge into an inductive bias, particularly with\nblack-box model classes such as neural networks. Meta-learning provides a way\nto automatize the selection of an inductive bias. Meta-learning leverages data\nor active observations from tasks that are expected to be related to future,\nand a priori unknown, tasks of interest. With a meta-trained inductive bias,\ntraining of a machine learning model can be potentially carried out with\nreduced training data and/or time complexity. This paper provides a high-level\nintroduction to meta-learning with applications to communication systems.",
    "published_date": "2020-01-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01227v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01192v1",
    "title": "Big Data Architecture in Czech Republic Healthcare Service: Requirements, TPC-H Benchmarks and Vertica",
    "authors": [
      "Martin Štufi",
      "Boris Bačić",
      "Leonid Stoimenov"
    ],
    "author_ids": [],
    "abstract": "Big data in healthcare has made a positive difference in advancing analytical\ncapabilities and lowering the costs of medical care. In addition to providing\nanalytical capabilities on platforms supporting current and near-future AI with\nmachine-learning and data-mining algorithms, there is also a need for ethical\nconsiderations mandating new ways to preserve privacy, all of which are\npreconditioned by the growing body of regulations and expectations. The purpose\nof this study is to improve existing clinical care by implementing a big data\nplatform for the Czech Republic National Health Service. Based on the achieved\nperformance and its compliance with mandatory guidelines, the reported big-data\nplatform was selected as the winning solution from the Czech Republic national\ntender (Tender Id. VZ0036628, No. Z2017-035520). The platform, based on\nanalytical Vertica NoSQL database for massive data processing, complies with\nthe TPC-H1 for decision support benchmark, the European Union (EU) and the\nCzech Republic requirements, well-exceeding defined system performance\nthresholds. The reported artefacts and concepts are transferrable to healthcare\nsystems in other countries and are intended to provide personalised autonomous\nassessment from big data in a cost-effective, scalable and high-performance\nmanner. The implemented platform allows: (1) scalability; (2) further\nimplementations of newly-developed machine learning algorithms for\nclassification and predictive analytics; (3) security improvements related to\nElectronic Health Records (EHR) by using automated functions for data\nencryption and decryption; and (4) the use of big data to allow strategic\nplanning in healthcare.",
    "published_date": "2020-01-05T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DC",
      "B.8, C.3, C.4, C.5, E.2, H.0, H.2, H.3, H.4, I.2, I.7, J.3, K.4, K.6",
      "B.8; C.3; C.4; C.5; E.2; H.0; H.2; H.3; H.4; I.2; I.7; J.3; K.4; K.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01192v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01121v1",
    "title": "Biologically-Motivated Deep Learning Method using Hierarchical Competitive Learning",
    "authors": [
      "Takashi Shinozaki"
    ],
    "author_ids": [],
    "abstract": "This study proposes a novel biologically-motivated learning method for deep\nconvolutional neural networks (CNNs). The combination of CNNs and back\npropagation (BP) learning is the most powerful method in recent machine\nlearning regimes. However, it requires large labeled data for training, and\nthis requirement can occasionally become a barrier for real world applications.\nTo address this problem and utilize unlabeled data, I propose to introduce\nunsupervised competitive learning which only requires forward propagating\nsignals as a pre-training method for CNNs. The method was evaluated by image\ndiscrimination tasks using MNIST, CIFAR-10, and ImageNet datasets, and it\nachieved a state-of-the-art performance as a biologically-motivated method in\nthe ImageNet experiment. The results suggested that the method enables\nhigher-level learning representations solely from forward propagating signals\nwithout a backward error signal for the learning of convolutional layers. The\nproposed method could be useful for a variety of poorly labeled data, for\nexample, time series or medical data.",
    "published_date": "2020-01-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.NE",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01121v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01050v2",
    "title": "Discrimination-aware Network Pruning for Deep Model Compression",
    "authors": [
      "Jing Liu",
      "Bohan Zhuang",
      "Zhuangwei Zhuang",
      "Yong Guo",
      "Junzhou Huang",
      "Jinhui Zhu",
      "Mingkui Tan"
    ],
    "author_ids": [],
    "abstract": "We study network pruning which aims to remove redundant channels/kernels and\nhence speed up the inference of deep networks. Existing pruning methods either\ntrain from scratch with sparsity constraints or minimize the reconstruction\nerror between the feature maps of the pre-trained models and the compressed\nones. Both strategies suffer from some limitations: the former kind is\ncomputationally expensive and difficult to converge, while the latter kind\noptimizes the reconstruction error but ignores the discriminative power of\nchannels. In this paper, we propose a simple-yet-effective method called\ndiscrimination-aware channel pruning (DCP) to choose the channels that actually\ncontribute to the discriminative power. Note that a channel often consists of a\nset of kernels. Besides the redundancy in channels, some kernels in a channel\nmay also be redundant and fail to contribute to the discriminative power of the\nnetwork, resulting in kernel level redundancy. To solve this, we propose a\ndiscrimination-aware kernel pruning (DKP) method to further compress deep\nnetworks by removing redundant kernels. To prevent DCP/DKP from selecting\nredundant channels/kernels, we propose a new adaptive stopping condition, which\nhelps to automatically determine the number of selected channels/kernels and\noften results in more compact models with better performance. Extensive\nexperiments on both image classification and face recognition demonstrate the\neffectiveness of our methods. For example, on ILSVRC-12, the resultant\nResNet-50 model with 30% reduction of channels even outperforms the baseline\nmodel by 0.36% in terms of Top-1 accuracy. The pruned MobileNetV1 and\nMobileNetV2 achieve 1.93x and 1.42x inference acceleration on a mobile device,\nrespectively, with negligible performance degradation. The source code and the\npre-trained models are available at https://github.com/SCUT-AILab/DCP.",
    "published_date": "2020-01-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01050v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01046v1",
    "title": "Adversarial-Learned Loss for Domain Adaptation",
    "authors": [
      "Minghao Chen",
      "Shuai Zhao",
      "Haifeng Liu",
      "Deng Cai"
    ],
    "author_ids": [],
    "abstract": "Recently, remarkable progress has been made in learning transferable\nrepresentation across domains. Previous works in domain adaptation are majorly\nbased on two techniques: domain-adversarial learning and self-training.\nHowever, domain-adversarial learning only aligns feature distributions between\ndomains but does not consider whether the target features are discriminative.\nOn the other hand, self-training utilizes the model predictions to enhance the\ndiscrimination of target features, but it is unable to explicitly align domain\ndistributions. In order to combine the strengths of these two methods, we\npropose a novel method called Adversarial-Learned Loss for Domain Adaptation\n(ALDA). We first analyze the pseudo-label method, a typical self-training\nmethod. Nevertheless, there is a gap between pseudo-labels and the ground\ntruth, which can cause incorrect training. Thus we introduce the confusion\nmatrix, which is learned through an adversarial manner in ALDA, to reduce the\ngap and align the feature distributions. Finally, a new loss function is\nauto-constructed from the learned confusion matrix, which serves as the loss\nfor unlabeled target samples. Our ALDA outperforms state-of-the-art approaches\nin four standard domain adaptation datasets. Our code is available at\nhttps://github.com/ZJULearning/ALDA.",
    "published_date": "2020-01-04T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01046v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.01002v2",
    "title": "The extent and drivers of gender imbalance in neuroscience reference lists",
    "authors": [
      "Jordan D. Dworkin",
      "Kristin A. Linn",
      "Erin G. Teich",
      "Perry Zurn",
      "Russell T. Shinohara",
      "Danielle S. Bassett"
    ],
    "author_ids": [],
    "abstract": "Like many scientific disciplines, neuroscience has increasingly attempted to\nconfront pervasive gender imbalances within the field. While much of the\nconversation has centered around publishing and conference participation,\nrecent research in other fields has called attention to the prevalence of\ngender bias in citation practices. Because of the downstream effects that\ncitations can have on visibility and career advancement, understanding and\neliminating gender bias in citation practices is vital for addressing inequity\nin a scientific community. In this study, we sought to determine whether there\nis evidence of gender bias in the citation practices of neuroscientists. Using\ndata from five top neuroscience journals, we find that reference lists tend to\ninclude more papers with men as first and last author than would be expected if\ngender were not a factor in referencing. Importantly, we show that this\novercitation of men and undercitation of women is driven largely by the\ncitation practices of men, and is increasing over time as the field becomes\nmore diverse. We develop a co-authorship network to assess homophily in\nresearchers' social networks, and we find that men tend to overcite men even\nwhen their social networks are representative. We discuss possible mechanisms\nand consider how individual researchers might address these findings in their\nown practices.",
    "published_date": "2020-01-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.01002v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.00964v1",
    "title": "Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing",
    "authors": [
      "Inioluwa Deborah Raji",
      "Timnit Gebru",
      "Margaret Mitchell",
      "Joy Buolamwini",
      "Joonseok Lee",
      "Emily Denton"
    ],
    "author_ids": [],
    "abstract": "Although essential to revealing biased performance, well intentioned attempts\nat algorithmic auditing can have effects that may harm the very populations\nthese measures are meant to protect. This concern is even more salient while\nauditing biometric systems such as facial recognition, where the data is\nsensitive and the technology is often used in ethically questionable manners.\nWe demonstrate a set of five ethical concerns in the particular case of\nauditing commercial facial processing technology, highlighting additional\ndesign considerations and ethical tensions the auditor needs to be aware of so\nas not exacerbate or complement the harms propagated by the audited system. We\ngo further to provide tangible illustrations of these concerns, and conclude by\nreflecting on what these concerns mean for the role of the algorithmic audit\nand the fundamental product limitations they reveal.",
    "published_date": "2020-01-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00964v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.09750v1",
    "title": "Towards a framework for understanding societal and ethical implications of Artificial Intelligence",
    "authors": [
      "Richard Benjamins",
      "Idoia Salazar"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) is one of the most discussed technologies today.\nThere are many innovative applications such as the diagnosis and treatment of\ncancer, customer experience, new business, education, contagious diseases\npropagation and optimization of the management of humanitarian catastrophes.\nHowever, with all those opportunities also comes great responsibility to ensure\ngood and fair practice of AI. The objective of this paper is to identify the\nmain societal and ethical challenges implied by a massive uptake of AI. We have\nsurveyed the literature for the most common challenges and classified them in\nseven groups: 1) Non-desired effects, 2) Liability, 3) Unknown consequences, 4)\nRelation people-robots, 5) Concentration of power and wealth, 6) Intentional\nbad uses, and 7) AI for weapons and warfare. The challenges should be dealt\nwith in different ways depending on their origin; some have technological\nsolutions, while others require ethical, societal, or political answers.\nDepending on the origin, different stakeholders might need to act. Whatever the\nidentified stakeholder, not treating those issues will lead to uncertainty and\nunforeseen consequences with potentially large negative societal impact,\nhurting especially the most vulnerable groups of societies. Technology is\nhelping to take better decisions, and AI is promoting data-driven decisions in\naddition to experience- and intuition-based discussion, with many improvements\nhappening. However, the negative side effects of this technology need to be\nwell understood and acted upon before we launch them massively into the world.",
    "published_date": "2020-01-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.09750v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00858v1",
    "title": "A cutting-plane algorithm for the Steiner team orienteering problem",
    "authors": [
      "Lucas Assunção",
      "Geraldo Robson Mateus"
    ],
    "author_ids": [],
    "abstract": "The Team Orienteering Problem (TOP) is an NP-hard routing problem in which a\nfleet of identical vehicles aims at collecting rewards (prizes) available at\ngiven locations, while satisfying restrictions on the travel times. In TOP,\neach location can be visited by at most one vehicle, and the goal is to\nmaximize the total sum of rewards collected by the vehicles within a given time\nlimit. In this paper, we propose a generalization of TOP, namely the Steiner\nTeam Orienteering Problem (STOP). In STOP, we provide, additionally, a subset\nof mandatory locations. In this sense, STOP also aims at maximizing the total\nsum of rewards collected within the time limit, but, now, every mandatory\nlocation must be visited. In this work, we propose a new commodity-based\nformulation for STOP and use it within a cutting-plane scheme. The algorithm\nbenefits from the compactness and strength of the proposed formulation and\nworks by separating three families of valid inequalities, which consist of some\ngeneral connectivity constraints, classical lifted cover inequalities based on\ndual bounds and a class of conflict cuts. To our knowledge, the last class of\ninequalities is also introduced in this work. A state-of-the-art branch-and-cut\nalgorithm from the literature of TOP is adapted to STOP and used as baseline to\nevaluate the performance of the cutting-plane. Extensive computational\nexperiments show the competitiveness of the new algorithm while solving several\nSTOP and TOP instances. In particular, it is able to solve, in total, 14 more\nTOP instances than any other previous exact algorithm and finds eight new\noptimality certificates. With respect to the new STOP instances introduced in\nthis work, our algorithm solves 30 more instances than the baseline.",
    "published_date": "2020-01-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DS",
      "90C11 (Primary) 90C57, 90B06 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00858v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.00570v1",
    "title": "The Real-World-Weight Cross-Entropy Loss Function: Modeling the Costs of Mislabeling",
    "authors": [
      "Yaoshiang Ho",
      "Samuel Wookey"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a new metric to measure goodness-of-fit for\nclassifiers, the Real World Cost function. This metric factors in information\nabout a real world problem, such as financial impact, that other measures like\naccuracy or F1 do not. This metric is also more directly interpretable for\nusers. To optimize for this metric, we introduce the Real-World- Weight\nCrossentropy loss function, in both binary and single-label classification\nvariants. Both variants allow direct input of real world costs as weights. For\nsingle-label, multicategory classification, our loss function also allows\ndirect penalization of probabilistic false positives, weighted by label, during\nthe training of a machine learning model. We compare the design of our loss\nfunction to the binary crossentropy and categorical crossentropy functions, as\nwell as their weighted variants, to discuss the potential for improvement in\nhandling a variety of known shortcomings of machine learning, ranging from\nimbalanced classes to medical diagnostic error to reinforcement of social bias.\nWe create scenarios that emulate those issues using the MNIST data set and\ndemonstrate empirical results of our new loss function. Finally, we sketch a\nproof of this function based on Maximum Likelihood Estimation and discuss\nfuture directions.",
    "published_date": "2020-01-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00570v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00692v1",
    "title": "FFusionCGAN: An end-to-end fusion method for few-focus images using conditional GAN in cytopathological digital slides",
    "authors": [
      "Xiebo Geng",
      "Sibo Liua",
      "Wei Han",
      "Xu Li",
      "Jiabo Ma",
      "Jingya Yu",
      "Xiuli Liu",
      "Sahoqun Zeng",
      "Li Chen",
      "Shenghua Cheng"
    ],
    "author_ids": [],
    "abstract": "Multi-focus image fusion technologies compress different focus depth images\ninto an image in which most objects are in focus. However, although existing\nimage fusion techniques, including traditional algorithms and deep\nlearning-based algorithms, can generate high-quality fused images, they need\nmultiple images with different focus depths in the same field of view. This\ncriterion may not be met in some cases where time efficiency is required or the\nhardware is insufficient. The problem is especially prominent in large-size\nwhole slide images. This paper focused on the multi-focus image fusion of\ncytopathological digital slide images, and proposed a novel method for\ngenerating fused images from single-focus or few-focus images based on\nconditional generative adversarial network (GAN). Through the adversarial\nlearning of the generator and discriminator, the method is capable of\ngenerating fused images with clear textures and large depth of field. Combined\nwith the characteristics of cytopathological images, this paper designs a new\ngenerator architecture combining U-Net and DenseBlock, which can effectively\nimprove the network's receptive field and comprehensively encode image\nfeatures. Meanwhile, this paper develops a semantic segmentation network that\nidentifies the blurred regions in cytopathological images. By integrating the\nnetwork into the generative model, the quality of the generated fused images is\neffectively improved. Our method can generate fused images from only\nsingle-focus or few-focus images, thereby avoiding the problem of collecting\nmultiple images of different focus depths with increased time and hardware\ncosts. Furthermore, our model is designed to learn the direct mapping of input\nsource images to fused images without the need to manually design complex\nactivity level measurements and fusion rules as in traditional methods.",
    "published_date": "2020-01-03T00:00:00",
    "year": 2020,
    "categories": [
      "cs.CV",
      "eess.IV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00692v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00623v1",
    "title": "Mining Disinformation and Fake News: Concepts, Methods, and Recent Advancements",
    "authors": [
      "Kai Shu",
      "Suhang Wang",
      "Dongwon Lee",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "In recent years, disinformation including fake news, has became a global\nphenomenon due to its explosive growth, particularly on social media. The wide\nspread of disinformation and fake news can cause detrimental societal effects.\nDespite the recent progress in detecting disinformation and fake news, it is\nstill non-trivial due to its complexity, diversity, multi-modality, and costs\nof fact-checking or annotation. The goal of this chapter is to pave the way for\nappreciating the challenges and advancements via: (1) introducing the types of\ninformation disorder on social media and examine their differences and\nconnections; (2) describing important and emerging tasks to combat\ndisinformation for characterization, detection and attribution; and (3)\ndiscussing a weak supervision approach to detect disinformation with limited\nlabeled data. We then provide an overview of the chapters in this book that\nrepresent the recent advancements in three related parts: (1) user engagements\nin the dissemination of information disorder; (2) techniques on detecting and\nmitigating disinformation; and (3) trending issues such as ethics, blockchain,\nclickbaits, etc. We hope this book to be a convenient entry point for\nresearchers, practitioners, and students to understand the problems and\nchallenges, learn state-of-the-art solutions for their specific needs, and\nquickly identify new research problems in their domains.",
    "published_date": "2020-01-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI",
      "cs.CL",
      "H.2.8"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00623v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00596v1",
    "title": "Computing Accessibility Metrics for Argentina",
    "authors": [
      "Carolina Lang",
      "Tobias Carreira",
      "German Cesar Dima",
      "Lucila Berniell",
      "Carlos Sarraute"
    ],
    "author_ids": [],
    "abstract": "We present a tool to calculate distances and travel times between a set of\norigins and a set of destinations, using different modes of transport in\nArgentina. The input data for the tool is a set of destinations (a\ngeo-referenced list of points of city amenities or \"opportunities\", such as\nfirms, schools, hospitals, parks, banks or retail, etc.) and a set of origins\ncharacterized by their geographic coordinates that could be interpreted as\nhouseholds or other. The tool determines, from each origin, which is the\nclosest destination, depending on the distance or travel time and the mode of\ntransport (on foot, by bicycle, by car, and by public transport).\n  The sets of origins and destinations are large sets, which can contain up to\nseveral thousand points. We applied and developed algorithms to improve the\nscalability of the different parts of the procedure. For the public\ntransportation network, we pre-processed the reachable lines from each point\nand used quad-trees to determine the distance between said points and the bus\nline's path.\n  A second objective of this project was to rely only on open data, such as\nOpen Street Map (OSM) data, together with making this tool open source.\nTherefore, the successful development and implementation of this tool is\npotentially beneficial to both public sector agencies as well as NGOs and other\ncivil society organizations that focus their work on the design and\nimplementation of public policies, aimed at improving accessibility in cities\nas a way to reduce spatial inequalities and social exclusion.",
    "published_date": "2020-01-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00596v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.00372v1",
    "title": "Phase-based Information for Voice Pathology Detection",
    "authors": [
      "Thomas Drugman",
      "Thomas Dubuisson",
      "Thierry Dutoit"
    ],
    "author_ids": [],
    "abstract": "In most current approaches of speech processing, information is extracted\nfrom the magnitude spectrum. However recent perceptual studies have underlined\nthe importance of the phase component. The goal of this paper is to investigate\nthe potential of using phase-based features for automatically detecting voice\ndisorders. It is shown that group delay functions are appropriate for\ncharacterizing irregularities in the phonation. Besides the respect of the\nmixed-phase model of speech is discussed. The proposed phase-based features are\nevaluated and compared to other parameters derived from the magnitude spectrum.\nBoth streams are shown to be interestingly complementary. Furthermore\nphase-based features turn out to convey a great amount of relevant information,\nleading to high discrimination performance.",
    "published_date": "2020-01-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.SD",
      "cs.CL",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00372v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00362v1",
    "title": "Numerical Analysis of a Parabolic Variational Inequality System Modeling Biofilm Growth at the Porescale",
    "authors": [
      "Azhar Alhammali",
      "Malgorzata Peszynska"
    ],
    "author_ids": [],
    "abstract": "In this paper we consider a system of two coupled nonlinear\ndiffusion--reaction partial differential equations (PDEs) which model the\ngrowth of biofilm and consumption of the nutrient. At the scale of interest the\nbiofilm density is subject to a pointwise constraint, thus the biofilm PDE is\nframed as a parabolic variational inequality. We derive rigorous error\nestimates for a finite element (FE) approximation to the coupled nonlinear\nsystem and confirm experimentally that the numerical approximation converges at\nthe predicted rate. We also show simulations in which we track the free\nboundary in the domains which resemble the pore scale geometry and in which we\ntest the different modeling assumptions.",
    "published_date": "2020-01-02T00:00:00",
    "year": 2020,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00362v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.00329v2",
    "title": "On Consequentialism and Fairness",
    "authors": [
      "Dallas Card",
      "Noah A. Smith"
    ],
    "author_ids": [],
    "abstract": "Recent work on fairness in machine learning has primarily emphasized how to\ndefine, quantify, and encourage \"fair\" outcomes. Less attention has been paid,\nhowever, to the ethical foundations which underlie such efforts. Among the\nethical perspectives that should be taken into consideration is\nconsequentialism, the position that, roughly speaking, outcomes are all that\nmatter. Although consequentialism is not free from difficulties, and although\nit does not necessarily provide a tractable way of choosing actions (because of\nthe combined problems of uncertainty, subjectivity, and aggregation), it\nnevertheless provides a powerful foundation from which to critique the existing\nliterature on machine learning fairness. Moreover, it brings to the fore some\nof the tradeoffs involved, including the problem of who counts, the pros and\ncons of using a policy, and the relative value of the distant future. In this\npaper we provide a consequentialist critique of common definitions of fairness\nwithin machine learning, as well as a machine learning perspective on\nconsequentialism. We conclude with a broader discussion of the issues of\nlearning and randomization, which have important implications for the ethics of\nautomated decision making systems.",
    "published_date": "2020-01-02T00:00:00",
    "year": 2020,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00329v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00257v2",
    "title": "Multi-transversals for Triangles and the Tuza's Conjecture",
    "authors": [
      "Parinya Chalermsook",
      "Samir Khuller",
      "Pattara Sukprasert",
      "Sumedha Uniyal"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study a primal and dual relationship about triangles: For\nany graph $G$, let $\\nu(G)$ be the maximum number of edge-disjoint triangles in\n$G$, and $\\tau(G)$ be the minimum subset $F$ of edges such that $G \\setminus F$\nis triangle-free. It is easy to see that $\\nu(G) \\leq \\tau(G) \\leq 3 \\nu(G)$,\nand in fact, this rather obvious inequality holds for a much more general\nprimal-dual relation between $k$-hyper matching and covering in hypergraphs.\nTuza conjectured in $1981$ that $\\tau(G) \\leq 2 \\nu(G)$, and this question has\nreceived attention from various groups of researchers in discrete mathematics,\nsettling various special cases such as planar graphs and generalized to bounded\nmaximum average degree graphs, some cases of minor-free graphs, and very dense\ngraphs. Despite these efforts, the conjecture in general graphs has remained\nwide open for almost four decades.\n  In this paper, we provide a proof of a non-trivial consequence of the\nconjecture; that is, for every $k \\geq 2$, there exist a (multi)-set $F\n\\subseteq E(G): |F| \\leq 2k \\nu(G)$ such that each triangle in $G$ overlaps at\nleast $k$ elements in $F$. Our result can be seen as a strengthened statement\nof Krivelevich's result on the fractional version of Tuza's conjecture (and we\ngive some examples illustrating this.) The main technical ingredient of our\nresult is a charging argument, that locally identifies edges in $F$ based on a\nlocal view of the packing solution. This idea might be useful in further\nstudying the primal-dual relations in general and the Tuza's conjecture in\nparticular.",
    "published_date": "2020-01-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.DM",
      "cs.DS",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00257v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/2001.00818v1",
    "title": "A Framework for Democratizing AI",
    "authors": [
      "Shakkeel Ahmed",
      "Ravi S. Mula",
      "Soma S. Dhavala"
    ],
    "author_ids": [],
    "abstract": "Machine Learning and Artificial Intelligence are considered an integral part\nof the Fourth Industrial Revolution. Their impact, and far-reaching\nconsequences, while acknowledged, are yet to be comprehended. These\ntechnologies are very specialized, and few organizations and select highly\ntrained professionals have the wherewithal, in terms of money, manpower, and\nmight, to chart the future. However, concentration of power can lead to\nmarginalization, causing severe inequalities. Regulatory agencies and\ngovernments across the globe are creating national policies, and laws around\nthese technologies to protect the rights of the digital citizens, as well as to\nempower them. Even private, not-for-profit organizations are also contributing\nto democratizing the technologies by making them \\emph{accessible} and\n\\emph{affordable}. However, accessibility and affordability are all but a few\nof the facets of democratizing the field. Others include, but not limited to,\n\\emph{portability}, \\emph{explainability}, \\emph{credibility}, \\emph{fairness},\namong others. As one can imagine, democratizing AI is a multi-faceted problem,\nand it requires advancements in science, technology and policy. At\n\\texttt{mlsquare}, we are developing scientific tools in this space.\nSpecifically, we introduce an opinionated, extensible, \\texttt{Python}\nframework that provides a single point of interface to a variety of solutions\nin each of the categories mentioned above. We present the design details, APIs\nof the framework, reference implementations, road map for development, and\nguidelines for contributions.",
    "published_date": "2020-01-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00818v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00153v1",
    "title": "Dual Adversarial Domain Adaptation",
    "authors": [
      "Yuntao Du",
      "Zhiwen Tan",
      "Qian Chen",
      "Xiaowen Zhang",
      "Yirong Yao",
      "Chongjun Wang"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain adaptation aims at transferring knowledge from the\nlabeled source domain to the unlabeled target domain. Previous adversarial\ndomain adaptation methods mostly adopt the discriminator with binary or\n$K$-dimensional output to perform marginal or conditional alignment\nindependently. Recent experiments have shown that when the discriminator is\nprovided with domain information in both domains and label information in the\nsource domain, it is able to preserve the complex multimodal information and\nhigh semantic information in both domains. Following this idea, we adopt a\ndiscriminator with $2K$-dimensional output to perform both domain-level and\nclass-level alignments simultaneously in a single discriminator. However, a\nsingle discriminator can not capture all the useful information across domains\nand the relationships between the examples and the decision boundary are rarely\nexplored before. Inspired by multi-view learning and latest advances in domain\nadaptation, besides the adversarial process between the discriminator and the\nfeature extractor, we also design a novel mechanism to make two discriminators\npit against each other, so that they can provide diverse information for each\nother and avoid generating target features outside the support of the source\ndomain. To the best of our knowledge, it is the first time to explore a dual\nadversarial strategy in domain adaptation. Moreover, we also use the\nsemi-supervised learning regularization to make the representations more\ndiscriminative. Comprehensive experiments on two real-world datasets verify\nthat our method outperforms several state-of-the-art domain adaptation methods.",
    "published_date": "2020-01-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00153v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.04832v1",
    "title": "Modeling and Counteracting Exposure Bias in Recommender Systems",
    "authors": [
      "Sami Khenissi",
      "Olfa Nasraoui"
    ],
    "author_ids": [],
    "abstract": "What we discover and see online, and consequently our opinions and decisions,\nare becoming increasingly affected by automated machine learned predictions.\nSimilarly, the predictive accuracy of learning machines heavily depends on the\nfeedback data that we provide them. This mutual influence can lead to\nclosed-loop interactions that may cause unknown biases which can be exacerbated\nafter several iterations of machine learning predictions and user feedback.\nMachine-caused biases risk leading to undesirable social effects ranging from\npolarization to unfairness and filter bubbles.\n  In this paper, we study the bias inherent in widely used recommendation\nstrategies such as matrix factorization. Then we model the exposure that is\nborne from the interaction between the user and the recommender system and\npropose new debiasing strategies for these systems.\n  Finally, we try to mitigate the recommendation system bias by engineering\nsolutions for several state of the art recommender system models.\n  Our results show that recommender systems are biased and depend on the prior\nexposure of the user. We also show that the studied bias iteratively decreases\ndiversity in the output recommendations. Our debiasing method demonstrates the\nneed for alternative recommendation strategies that take into account the\nexposure process in order to reduce bias.\n  Our research findings show the importance of understanding the nature of and\ndealing with bias in machine learning models such as recommender systems that\ninteract directly with humans, and are thus causing an increasing influence on\nhuman discovery and decision making",
    "published_date": "2020-01-01T00:00:00",
    "year": 2020,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.04832v1",
    "is_ai_related": true
  }
]