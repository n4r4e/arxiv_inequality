[
  {
    "id": "http://arxiv.org/abs/1912.12827v2",
    "title": "Max-Min Fairness in IRS-Aided Multi-Cell MISO Systems via Joint Transmit and Reflective Beamforming",
    "authors": [
      "Hailiang Xie",
      "Jie Xu",
      "Ya-Feng Liu"
    ],
    "author_ids": [],
    "abstract": "This paper investigates an intelligent reflecting surface (IRS)-aided\nmulti-cell multiple-input single-output (MISO) system consisting of several\nmulti-antenna base stations (BSs) each communicating with a single-antenna\nuser, in which an IRS is dedicatedly deployed for assisting the wireless\ntransmission and suppressing the inter-cell interference. Under this setup, we\njointly optimize the coordinated transmit beamforming at the BSs and the\nreflective beamforming at the IRS, for the purpose of maximizing the minimum\nweighted received signal-to-interference-plus-noise ratio (SINR) at users,\nsubject to the individual maximum transmit power constraints at the BSs and the\nreflection constraints at the IRS. To solve the difficult non-convex minimum\nSINR maximization problem, we propose efficient algorithms based on alternating\noptimization, in which the transmit and reflective beamforming vectors are\noptimized in an alternating manner. In particular, we use the second-order-cone\nprogramming (SOCP) for optimizing the coordinated transmit beamforming, and\ndevelop two efficient designs for updating the reflective beamforming based on\nthe techniques of semi-definite relaxation (SDR) and successive convex\napproximation (SCA), respectively. Numerical results show that the use of IRS\nleads to significantly higher SINR values than benchmark schemes without IRS or\nwithout proper reflective beamforming optimization; while the developed\nSCA-based solution outperforms the SDR-based one with lower implementation\ncomplexity.",
    "published_date": "2019-12-30T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12827v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.12800v1",
    "title": "Likelihood Ratios and Generative Classifiers for Unsupervised Out-of-Domain Detection In Task Oriented Dialog",
    "authors": [
      "Varun Gangal",
      "Abhinav Arora",
      "Arash Einolghozati",
      "Sonal Gupta"
    ],
    "author_ids": [],
    "abstract": "The task of identifying out-of-domain (OOD) input examples directly at\ntest-time has seen renewed interest recently due to increased real world\ndeployment of models. In this work, we focus on OOD detection for natural\nlanguage sentence inputs to task-based dialog systems. Our findings are\nthree-fold: First, we curate and release ROSTD (Real Out-of-Domain Sentences\nFrom Task-oriented Dialog) - a dataset of 4K OOD examples for the publicly\navailable dataset from (Schuster et al. 2019). In contrast to existing settings\nwhich synthesize OOD examples by holding out a subset of classes, our examples\nwere authored by annotators with apriori instructions to be out-of-domain with\nrespect to the sentences in an existing dataset. Second, we explore likelihood\nratio based approaches as an alternative to currently prevalent paradigms.\nSpecifically, we reformulate and apply these approaches to natural language\ninputs. We find that they match or outperform the latter on all datasets, with\nlarger improvements on non-artificial OOD benchmarks such as our dataset. Our\nablations validate that specifically using likelihood ratios rather than plain\nlikelihood is necessary to discriminate well between OOD and in-domain data.\nThird, we propose learning a generative classifier and computing a marginal\nlikelihood (ratio) for OOD detection. This allows us to use a principled\nlikelihood while at the same time exploiting training-time labels. We find that\nthis approach outperforms both simple likelihood (ratio) based and other prior\napproaches. We are hitherto the first to investigate the use of generative\nclassifiers for OOD detection at test-time.",
    "published_date": "2019-12-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12800v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12747v1",
    "title": "Worst-Case Optimal Radix Triejoin",
    "authors": [
      "Alan Fekete",
      "Brody Franks",
      "Herbert Jordan",
      "Bernhard Scholz"
    ],
    "author_ids": [],
    "abstract": "Relatively recently, the field of join processing has been swayed by the\ndiscovery of a new class of multi-way join algorithms. The new algorithms join\nmultiple relations simultaneously rather than perform a series of pairwise\njoins. The new join algorithms satisfy stronger worst-case runtime complexity\nguarantees than any of the existing approaches based on pairwise joins -- they\nare worst-case optimal in data complexity. These research efforts have resulted\nin a flurry of papers documenting theoretical and some practical contributions.\nHowever, there is still the quest of making the new worst-case optimal join\nalgorithms truly practical in terms of (1) ease of implementation and (2)\nsecondary index efficiency in terms of number of indexes created to answer a\nquery.\n  In this paper, we present a simple worst-case optimal multi-way join\nalgorithm called the radix triejoin. Radix triejoin uses a binary encoding for\nreducing the domain of a database. Our main technical contribution is that\ndomain reduction allows a bit-interleaving of attribute values that gives rise\nto a query-independent relation representation, permitting the computation of\nmultiple queries over the same relations worst-case optimally without having to\nconstruct additional secondary indexes. We also generalise the core algorithm\nto conjunctive queries with inequality constraints and provide a new proof\ntechnique for the worst-case optimal join result.",
    "published_date": "2019-12-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12747v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.12712v2",
    "title": "Haptic communication optimises joint decisions and affords implicit confidence sharing",
    "authors": [
      "Giovanni Pezzulo",
      "Lucas Roche",
      "Ludovic Saint-Bauzel"
    ],
    "author_ids": [],
    "abstract": "Group decisions can outperform the choices of the best individual group\nmembers. Previous research suggested that optimal group decisions require\nindividuals to communicate explicitly (e.g., verbally) their confidence levels.\nOur study addresses the untested hypothesis that implicit communication using a\nsensorimotor channel -- haptic coupling -- may afford optimal group decisions,\ntoo. We report that haptically coupled dyads solve a perceptual discrimination\ntask more accurately than their best individual members; and five times faster\nthan dyads using explicit communication. Furthermore, our computational\nanalyses indicate that the haptic channel affords implicit confidence sharing.\nWe found that dyads take leadership over the choice and communicate their\nconfidence in it by modulating both the timing and the force of their\nmovements. Our findings may pave the way to negotiation technologies using fast\nsensorimotor communication to solve problems in groups.",
    "published_date": "2019-12-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO",
      "cs.HC",
      "cs.SY",
      "eess.SY",
      "H.5.2, J.4",
      "H.5.2; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12712v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.12541v1",
    "title": "Approximating Nash Social Welfare under Submodular Valuations through (Un)Matchings",
    "authors": [
      "Jugal Garg",
      "Pooja Kulkarni",
      "Rucha Kulkarni"
    ],
    "author_ids": [],
    "abstract": "We study the problem of approximating maximum Nash social welfare (NSW) when\nallocating m indivisible items among n asymmetric agents with submodular\nvaluations. The NSW is a well-established notion of fairness and efficiency,\ndefined as the weighted geometric mean of agents' valuations. For special cases\nof the problem with symmetric agents and additive(-like) valuation functions,\napproximation algorithms have been designed using approaches customized for\nthese specific settings, and they fail to extend to more general settings.\nHence, no approximation algorithm with factor independent of m is known either\nfor asymmetric agents with additive valuations or for symmetric agents beyond\nadditive(-like) valuations.\n  In this paper, we extend our understanding of the NSW problem to far more\ngeneral settings. Our main contribution is two approximation algorithms for\nasymmetric agents with additive and submodular valuations respectively. Both\nalgorithms are simple to understand and involve non-trivial modifications of a\ngreedy repeated matchings approach. Allocations of high valued items are done\nseparately by un-matching certain items and re-matching them, by processes that\nare different in both algorithms. We show that these approaches achieve\napproximation factors of O(n) and O(n log n) for additive and submodular case\nrespectively, which is independent of the number of items. For additive\nvaluations, our algorithm outputs an allocation that also achieves the fairness\nproperty of envy-free up to one item (EF1).\n  Furthermore, we show that the NSW problem under submodular valuations is\nstrictly harder than all currently known settings with an e/(e-1) factor of the\nhardness of approximation, even for constantly many agents. For this case, we\nprovide a different approximation algorithm that achieves a factor of e/(e-1),\nhence resolving it completely.",
    "published_date": "2019-12-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12541v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.12463v2",
    "title": "Arachne: Search Based Repair of Deep Neural Networks",
    "authors": [
      "Jeongju Sohn",
      "Sungmin Kang",
      "Shin Yoo"
    ],
    "author_ids": [],
    "abstract": "The rapid and widespread adoption of Deep Neural Networks (DNNs) has called\nfor ways to test their behaviour, and many testing approaches have successfully\nrevealed misbehaviour of DNNs. However, it is relatively unclear what one can\ndo to correct such behaviour after revelation, as retraining involves costly\ndata collection and does not guarantee to fix the underlying issue. This paper\nintroduces Arachne, a novel program repair technique for DNNs, which directly\nrepairs DNNs using their input-output pairs as a specification. Arachne\nlocalises neural weights on which it can generate effective patches and uses\nDifferential Evolution to optimise the localised weights and correct the\nmisbehaviour. An empirical study using different benchmarks shows that Arachne\ncan fix specific misclassifications of a DNN without reducing general accuracy\nsignificantly. On average, patches generated by Arachne generalise to 61.3% of\nunseen misbehaviour, whereas those by a state-of-the-art DNN repair technique\ngeneralise only to 10.2% and sometimes to none while taking tens of times more\nthan Arachne. We also show that Arachne can address fairness issues by\ndebiasing a gender classification model. Finally, we successfully apply Arachne\nto a text sentiment model to show that it generalises beyond Convolutional\nNeural Networks.",
    "published_date": "2019-12-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12463v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12385v2",
    "title": "Statistical Loss and Analysis for Deep Learning in Hyperspectral Image Classification",
    "authors": [
      "Zhiqiang Gong",
      "Ping Zhong",
      "Weidong Hu"
    ],
    "author_ids": [],
    "abstract": "Nowadays, deep learning methods, especially the convolutional neural networks\n(CNNs), have shown impressive performance on extracting abstract and high-level\nfeatures from the hyperspectral image. However, general training process of\nCNNs mainly considers the pixel-wise information or the samples' correlation to\nformulate the penalization while ignores the statistical properties especially\nthe spectral variability of each class in the hyperspectral image. These\nsamples-based penalizations would lead to the uncertainty of the training\nprocess due to the imbalanced and limited number of training samples. To\novercome this problem, this work characterizes each class from the\nhyperspectral image as a statistical distribution and further develops a novel\nstatistical loss with the distributions, not directly with samples for deep\nlearning. Based on the Fisher discrimination criterion, the loss penalizes the\nsample variance of each class distribution to decrease the intra-class variance\nof the training samples. Moreover, an additional diversity-promoting condition\nis added to enlarge the inter-class variance between different class\ndistributions and this could better discriminate samples from different classes\nin hyperspectral image. Finally, the statistical estimation form of the\nstatistical loss is developed with the training samples through multi-variant\nstatistical analysis. Experiments over the real-world hyperspectral images show\nthe effectiveness of the developed statistical loss for deep learning.",
    "published_date": "2019-12-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12385v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12215v3",
    "title": "Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation",
    "authors": [
      "Hao Tang",
      "Dan Xu",
      "Yan Yan",
      "Philip H. S. Torr",
      "Nicu Sebe"
    ],
    "author_ids": [],
    "abstract": "In this paper, we address the task of semantic-guided scene generation. One\nopen challenge in scene generation is the difficulty of the generation of small\nobjects and detailed local texture, which has been widely observed in global\nimage-level generation methods. To tackle this issue, in this work we consider\nlearning the scene generation in a local context, and correspondingly design a\nlocal class-specific generative network with semantic maps as a guidance, which\nseparately constructs and learns sub-generators concentrating on the generation\nof different classes, and is able to provide more scene details. To learn more\ndiscriminative class-specific feature representations for the local generation,\na novel classification module is also proposed. To combine the advantage of\nboth the global image-level and the local class-specific generation, a joint\ngeneration network is designed with an attention fusion module and a\ndual-discriminator structure embedded. Extensive experiments on two scene image\ngeneration tasks show superior generation performance of the proposed model.\nThe state-of-the-art results are established by large margins on both tasks and\non challenging public benchmarks. The source code and trained models are\navailable at https://github.com/Ha0Tang/LGGAN.",
    "published_date": "2019-12-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12215v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12151v1",
    "title": "A Water-Filling Primal-Dual Algorithm for Approximating Non-Linear Covering Problems",
    "authors": [
      "Andrés Fielbaum",
      "Ignacio Morales",
      "José Verschae"
    ],
    "author_ids": [],
    "abstract": "Obtaining strong linear relaxations of capacitated covering problems\nconstitute a major technical challenge even for simple settings. For one of the\nmost basic cases, the Knapsack-Cover (Min-Knapsack) problem, the relaxation\nbased on knapsack-cover inequalities achieves an integrality gap of 2. These\ninequalities have been exploited in more general environments, many of which\nadmit primal-dual approximation algorithms.\n  Inspired by problems from power and transport systems, we introduce a new\ngeneral setting in which items can be taken fractionally to cover a given\ndemand. The cost incurred by an item is given by an arbitrary non-decreasing\nfunction of the chosen fraction. We generalize the knapsack-cover inequalities\nto this setting an use them to obtain a $(2+\\varepsilon)$-approximate\nprimal-dual algorithm. Our procedure has a natural interpretation as a\nbucket-filling algorithm, which effectively balances the difficulties given by\nhaving different slopes in the cost functions: when some superior portion of an\nitem presents a low slope, it helps to increase the priority with which the\ninferior portions may be taken. We also present a rounding algorithm with an\napproximation guarantee of 2.\n  We generalize our algorithm to the Unsplittable Flow-Cover problem on a line,\nalso for the setting where items can be taken fractionally. For this problem we\nobtain a $(4+\\varepsilon)$-approximation algorithm in polynomial time, almost\nmatching the $4$-approximation known for the classical setting.",
    "published_date": "2019-12-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12151v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.12150v5",
    "title": "The Chi-Square Test of Distance Correlation",
    "authors": [
      "Cencheng Shen",
      "Sambit Panda",
      "Joshua T. Vogelstein"
    ],
    "author_ids": [],
    "abstract": "Distance correlation has gained much recent attention in the data science\ncommunity: the sample statistic is straightforward to compute and\nasymptotically equals zero if and only if independence, making it an ideal\nchoice to discover any type of dependency structure given sufficient sample\nsize. One major bottleneck is the testing process: because the null\ndistribution of distance correlation depends on the underlying random variables\nand metric choice, it typically requires a permutation test to estimate the\nnull and compute the p-value, which is very costly for large amount of data. To\novercome the difficulty, in this paper we propose a chi-square test for\ndistance correlation. Method-wise, the chi-square test is non-parametric,\nextremely fast, and applicable to bias-corrected distance correlation using any\nstrong negative type metric or characteristic kernel. The test exhibits a\nsimilar testing power as the standard permutation test, and can be utilized for\nK-sample and partial testing. Theory-wise, we show that the underlying\nchi-square distribution well approximates and dominates the limiting null\ndistribution in upper tail, prove the chi-square test can be valid and\nuniversally consistent for testing independence, and establish a testing power\ninequality with respect to the permutation test.",
    "published_date": "2019-12-27T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12150v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00081v2",
    "title": "Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries",
    "authors": [
      "Patrick Gage Kelley",
      "Yongwei Yang",
      "Courtney Heldreth",
      "Christopher Moessner",
      "Aaron Sedley",
      "Andreas Kramm",
      "David T. Newman",
      "Allison Woodruff"
    ],
    "author_ids": [],
    "abstract": "As the influence and use of artificial intelligence (AI) have grown and its\ntransformative potential has become more apparent, many questions have been\nraised regarding the economic, political, social, and ethical implications of\nits use. Public opinion plays an important role in these discussions,\ninfluencing product adoption, commercial development, research funding, and\nregulation. In this paper we present results of an in-depth survey of public\nopinion of artificial intelligence conducted with 10,005 respondents spanning\neight countries and six continents. We report widespread perception that AI\nwill have significant impact on society, accompanied by strong support for the\nresponsible development and use of AI, and also characterize the public's\nsentiment towards AI with four key themes (exciting, useful, worrying, and\nfuturistic) whose prevalence distinguishes response to AI in different\ncountries.",
    "published_date": "2019-12-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.1; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00081v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12012v1",
    "title": "Graduate Employment Prediction with Bias",
    "authors": [
      "Teng Guo",
      "Feng Xia",
      "Shihao Zhen",
      "Xiaomei Bai",
      "Dongyu Zhang",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "The failure of landing a job for college students could cause serious social\nconsequences such as drunkenness and suicide. In addition to academic\nperformance, unconscious biases can become one key obstacle for hunting jobs\nfor graduating students. Thus, it is necessary to understand these unconscious\nbiases so that we can help these students at an early stage with more\npersonalized intervention. In this paper, we develop a framework, i.e., MAYA\n(Multi-mAjor emploYment stAtus) to predict students' employment status while\nconsidering biases. The framework consists of four major components. Firstly,\nwe solve the heterogeneity of student courses by embedding academic performance\ninto a unified space. Then, we apply a generative adversarial network (GAN) to\novercome the class imbalance problem. Thirdly, we adopt Long Short-Term Memory\n(LSTM) with a novel dropout mechanism to comprehensively capture sequential\ninformation among semesters. Finally, we design a bias-based regularization to\ncapture the job market biases. We conduct extensive experiments on a\nlarge-scale educational dataset and the results demonstrate the effectiveness\nof our prediction framework.",
    "published_date": "2019-12-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12012v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11945v1",
    "title": "On the Morality of Artificial Intelligence",
    "authors": [
      "Alexandra Luccioni",
      "Yoshua Bengio"
    ],
    "author_ids": [],
    "abstract": "Much of the existing research on the social and ethical impact of Artificial\nIntelligence has been focused on defining ethical principles and guidelines\nsurrounding Machine Learning (ML) and other Artificial Intelligence (AI)\nalgorithms [IEEE, 2017, Jobin et al., 2019]. While this is extremely useful for\nhelping define the appropriate social norms of AI, we believe that it is\nequally important to discuss both the potential and risks of ML and to inspire\nthe community to use ML for beneficial objectives. In the present article,\nwhich is specifically aimed at ML practitioners, we thus focus more on the\nlatter, carrying out an overview of existing high-level ethical frameworks and\nguidelines, but above all proposing both conceptual and practical principles\nand guidelines for ML research and deployment, insisting on concrete actions\nthat can be taken by practitioners to pursue a more ethical and moral practice\nof ML aimed at using AI for social good.",
    "published_date": "2019-12-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11945v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11646v2",
    "title": "A priori error analysis of a numerical stochastic homogenization method",
    "authors": [
      "Julian Fischer",
      "Dietmar Gallistl",
      "Daniel Peterseim"
    ],
    "author_ids": [],
    "abstract": "This paper provides an a~priori error analysis of a localized orthogonal\ndecomposition method (LOD) for the numerical stochastic homogenization of a\nmodel random diffusion problem. If the uniformly elliptic and bounded random\ncoefficient field of the model problem is stationary and satisfies a\nquantitative decorrelation assumption in form of the spectral gap inequality,\nthen the expected $L^2$ error of the method can be estimated, up to logarithmic\nfactors, by $H+(\\varepsilon/H)^{d/2}$; $\\varepsilon$ being the small\ncorrelation length of the random coefficient and $H$ the width of the coarse\nfinite element mesh that determines the spatial resolution. The proof bridges\nrecent results of numerical homogenization and quantitative stochastic\nhomogenization.",
    "published_date": "2019-12-25T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA",
      "35R60, 65N12, 65N15, 65N30, 73B27, 74Q05"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11646v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.11642v2",
    "title": "Competing Ratio Loss for Discriminative Multi-class Image Classification",
    "authors": [
      "Ke Zhang",
      "Yurong Guo",
      "Xinsheng Wang",
      "Dongliang Chang",
      "Zhenbing Zhao",
      "Zhanyu Ma",
      "Tony X. Han"
    ],
    "author_ids": [],
    "abstract": "The development of deep convolutional neural network architecture is critical\nto the improvement of image classification task performance. Many image\nclassification studies use deep convolutional neural network and focus on\nmodifying the network structure to improve image classification performance.\nConversely, our study focuses on loss function design. Cross-entropy Loss (CEL)\nhas been widely used for training deep convolutional neural network for the\ntask of multi-class classification. Although CEL has been successfully\nimplemented in several image classification tasks, it only focuses on the\nposterior probability of the correct class. For this reason, a negative log\nlikelihood ratio loss (NLLR) was proposed to better differentiate between the\ncorrect class and the competing incorrect ones. However, during the training of\nthe deep convolutional neural network, the value of NLLR is not always positive\nor negative, which severely affects the convergence of NLLR. Our proposed\ncompeting ratio loss (CRL) calculates the posterior probability ratio between\nthe correct class and the competing incorrect classes to further enlarge the\nprobability difference between the correct and incorrect classes. We added\nhyperparameters to CRL, thereby ensuring its value to be positive and that the\nupdate size of backpropagation is suitable for the CRL's fast convergence. To\ndemonstrate the performance of CRL, we conducted experiments on general image\nclassification tasks (CIFAR10/100, SVHN, ImageNet), the fine-grained image\nclassification tasks (CUB200-2011 and Stanford Car), and the challenging face\nage estimation task (using Adience). Experimental results show the\neffectiveness and robustness of the proposed loss function on different deep\nconvolutional neural network architectures and different image classification\ntasks.",
    "published_date": "2019-12-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11642v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11602v4",
    "title": "Leveraging Lead Bias for Zero-shot Abstractive News Summarization",
    "authors": [
      "Chenguang Zhu",
      "Ziyi Yang",
      "Robert Gmyr",
      "Michael Zeng",
      "Xuedong Huang"
    ],
    "author_ids": [],
    "abstract": "A typical journalistic convention in news articles is to deliver the most\nsalient information in the beginning, also known as the lead bias. While this\nphenomenon can be exploited in generating a summary, it has a detrimental\neffect on teaching a model to discriminate and extract important information in\ngeneral. We propose that this lead bias can be leveraged in our favor in a\nsimple and effective way to pre-train abstractive news summarization models on\nlarge-scale unlabeled news corpora: predicting the leading sentences using the\nrest of an article. We collect a massive news corpus and conduct data cleaning\nand filtering via statistical analysis. We then apply self-supervised\npre-training on this dataset to existing generation models BART and T5 for\ndomain adaptation. Via extensive experiments on six benchmark datasets, we show\nthat this approach can dramatically improve the summarization quality and\nachieve state-of-the-art results for zero-shot news summarization without any\nfine-tuning. For example, in the DUC2003 dataset, the ROUGE-1 score of BART\nincreases 13.7% after the lead-bias pre-training. We deploy the model in\nMicrosoft News and provide public APIs as well as a demo website for\nmulti-lingual news summarization.",
    "published_date": "2019-12-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11602v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11595v2",
    "title": "The Windfall Clause: Distributing the Benefits of AI for the Common Good",
    "authors": [
      "Cullen O'Keefe",
      "Peter Cihon",
      "Ben Garfinkel",
      "Carrick Flynn",
      "Jade Leung",
      "Allan Dafoe"
    ],
    "author_ids": [],
    "abstract": "As the transformative potential of AI has become increasingly salient as a\nmatter of public and political interest, there has been growing discussion\nabout the need to ensure that AI broadly benefits humanity. This in turn has\nspurred debate on the social responsibilities of large technology companies to\nserve the interests of society at large. In response, ethical principles and\ncodes of conduct have been proposed to meet the escalating demand for this\nresponsibility to be taken seriously. As yet, however, few institutional\ninnovations have been suggested to translate this responsibility into legal\ncommitments which apply to companies positioned to reap large financial gains\nfrom the development and use of AI. This paper offers one potentially\nattractive tool for addressing such issues: the Windfall Clause, which is an ex\nante commitment by AI firms to donate a significant amount of any eventual\nextremely large profits. By this we mean an early commitment that profits that\na firm could not earn without achieving fundamental, economically\ntransformative breakthroughs in AI capabilities will be donated to benefit\nhumanity broadly, with particular attention towards mitigating any downsides\nfrom deployment of windfall-generating AI.",
    "published_date": "2019-12-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11595v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11203v1",
    "title": "Stochastic Fairness and Language-Theoretic Fairness in Planning on Nondeterministic Domains",
    "authors": [
      "Benjamin Aminof",
      "Giuseppe De Giacomo",
      "Sasha Rubin"
    ],
    "author_ids": [],
    "abstract": "We address two central notions of fairness in the literature of planning on\nnondeterministic fully observable domains. The first, which we call stochastic\nfairness, is classical, and assumes an environment which operates\nprobabilistically using possibly unknown probabilities. The second, which is\nlanguage-theoretic, assumes that if an action is taken from a given state\ninfinitely often then all its possible outcomes should appear infinitely often\n(we call this state-action fairness). While the two notions coincide for\nstandard reachability goals, they diverge for temporally extended goals. This\nimportant difference has been overlooked in the planning literature, and we\nargue has led to confusion in a number of published algorithms which use\nreductions that were stated for state-action fairness, for which they are\nincorrect, while being correct for stochastic fairness. We remedy this and\nprovide an optimal sound and complete algorithm for solving state-action fair\nplanning for LTL/LTLf goals, as well as a correct proof of the lower bound of\nthe goal-complexity (our proof is general enough that it provides new proofs\nalso for the no-fairness and stochastic-fairness cases). Overall, we show that\nstochastic fairness is better behaved than state-action fairness.",
    "published_date": "2019-12-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.FL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11203v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11171v3",
    "title": "Geometry-Aware Generation of Adversarial Point Clouds",
    "authors": [
      "Yuxin Wen",
      "Jiehong Lin",
      "Ke Chen",
      "C. L. Philip Chen",
      "Kui Jia"
    ],
    "author_ids": [],
    "abstract": "Machine learning models have been shown to be vulnerable to adversarial\nexamples. While most of the existing methods for adversarial attack and defense\nwork on the 2D image domain, a few recent attempts have been made to extend\nthem to 3D point cloud data. However, adversarial results obtained by these\nmethods typically contain point outliers, which are both noticeable and easy to\ndefend against using the simple techniques of outlier removal. Motivated by the\ndifferent mechanisms by which humans perceive 2D images and 3D shapes, in this\npaper we propose the new design of \\emph{geometry-aware objectives}, whose\nsolutions favor (the discrete versions of) the desired surface properties of\nsmoothness and fairness. To generate adversarial point clouds, we use a\ntargeted attack misclassification loss that supports continuous pursuit of\nincreasingly malicious signals. Regularizing the targeted attack loss with our\nproposed geometry-aware objectives results in our proposed method,\nGeometry-Aware Adversarial Attack ($GeoA^3$). The results of $GeoA^3$ tend to\nbe more harmful, arguably harder to defend against, and of the key adversarial\ncharacterization of being imperceptible to humans. While the main focus of this\npaper is to learn to generate adversarial point clouds, we also present a\nsimple but effective algorithm termed $Geo_{+}A^3$-IterNormPro, with Iterative\nNormal Projection (IterNorPro) that solves a new objective function\n$Geo_{+}A^3$, towards surface-level adversarial attacks via generation of\nadversarial point clouds. We quantitatively evaluate our methods on both\nsynthetic and physical objects in terms of attack success rate and geometric\nregularity. For a qualitative evaluation, we conduct subjective studies by\ncollecting human preferences from Amazon Mechanical Turk. Comparative results\nin comprehensive experiments confirm the advantages of our proposed methods.",
    "published_date": "2019-12-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11171v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11095v1",
    "title": "Defining AI in Policy versus Practice",
    "authors": [
      "P. M. Krafft",
      "Meg Young",
      "Michael Katell",
      "Karen Huang",
      "Ghislain Bugingo"
    ],
    "author_ids": [],
    "abstract": "Recent concern about harms of information technologies motivate consideration\nof regulatory action to forestall or constrain certain developments in the\nfield of artificial intelligence (AI). However, definitional ambiguity hampers\nthe possibility of conversation about this urgent topic of public concern.\nLegal and regulatory interventions require agreed-upon definitions, but\nconsensus around a definition of AI has been elusive, especially in policy\nconversations. With an eye towards practical working definitions and a broader\nunderstanding of positions on these issues, we survey experts and review\npublished policy documents to examine researcher and policy-maker conceptions\nof AI. We find that while AI researchers favor definitions of AI that emphasize\ntechnical functionality, policy-makers instead use definitions that compare\nsystems to human thinking and behavior. We point out that definitions adhering\nclosely to the functionality of AI systems are more inclusive of technologies\nin use today, whereas definitions that emphasize human-like capabilities are\nmost applicable to hypothetical future technologies. As a result of this gap,\nethical and regulatory efforts may overemphasize concern about future\ntechnologies at the expense of pressing issues with existing deployed\ntechnologies.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11084v2",
    "title": "Where Are We? Using Scopus to Map the Literature at the Intersection Between Artificial Intelligence and Research on Crime",
    "authors": [
      "Gian Maria Campedelli"
    ],
    "author_ids": [],
    "abstract": "Research on Artificial Intelligence (AI) applications has spread over many\nscientific disciplines. Scientists have tested the power of intelligent\nalgorithms developed to predict (or learn from) natural, physical and social\nphenomena. This also applies to crime-related research problems. Nonetheless,\nstudies that map the current state of the art at the intersection between AI\nand crime are lacking. What are the current research trends in terms of topics\nin this area? What is the structure of scientific collaboration when\nconsidering works investigating criminal issues using machine learning, deep\nlearning, and AI in general? What are the most active countries in this\nspecific scientific sphere? Using data retrieved from the Scopus database, this\nwork quantitatively analyzes 692 published works at the intersection between AI\nand crime employing network science to respond to these questions. Results show\nthat researchers are mainly focusing on cyber-related criminal topics and that\nrelevant themes such as algorithmic discrimination, fairness, and ethics are\nconsiderably overlooked. Furthermore, data highlight the extremely disconnected\nstructure of co-authorship networks. Such disconnectedness may represent a\nsubstantial obstacle to a more solid community of scientists interested in\nthese topics. Additionally, the graph of scientific collaboration indicates\nthat countries that are more prone to engage in international partnerships are\ngenerally less central in the network. This means that scholars working in\nhighly productive countries (e.g. the United States, China) tend to mostly\ncollaborate domestically. Finally, current issues and future developments\nwithin this scientific area are also discussed.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11084v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12106v1",
    "title": "White Noise Analysis of Neural Networks",
    "authors": [
      "Ali Borji",
      "Sikun Lin"
    ],
    "author_ids": [],
    "abstract": "A white noise analysis of modern deep neural networks is presented to unveil\ntheir biases at the whole network level or the single neuron level. Our\nanalysis is based on two popular and related methods in psychophysics and\nneurophysiology namely classification images and spike triggered analysis.\nThese methods have been widely used to understand the underlying mechanisms of\nsensory systems in humans and monkeys. We leverage them to investigate the\ninherent biases of deep neural networks and to obtain a first-order\napproximation of their functionality. We emphasize on CNNs since they are\ncurrently the state of the art methods in computer vision and are a decent\nmodel of human visual processing. In addition, we study multi-layer\nperceptrons, logistic regression, and recurrent neural networks. Experiments\nover four classic datasets, MNIST, Fashion-MNIST, CIFAR-10, and ImageNet, show\nthat the computed bias maps resemble the target classes and when used for\nclassification lead to an over twofold performance than the chance level.\nFurther, we show that classification images can be used to attack a black-box\nclassifier and to detect adversarial patch attacks. Finally, we utilize spike\ntriggered averaging to derive the filters of CNNs and explore how the behavior\nof a network changes when neurons in different layers are modulated. Our effort\nillustrates a successful example of borrowing from neurosciences to study ANNs\nand highlights the importance of cross-fertilization and synergy across machine\nlearning, deep learning, and computational neuroscience.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12106v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10979v1",
    "title": "Privacy Attacks on Network Embeddings",
    "authors": [
      "Michael Ellers",
      "Michael Cochez",
      "Tobias Schumacher",
      "Markus Strohmaier",
      "Florian Lemmerich"
    ],
    "author_ids": [],
    "abstract": "Data ownership and data protection are increasingly important topics with\nethical and legal implications, e.g., with the right to erasure established in\nthe European General Data Protection Regulation (GDPR). In this light, we\ninvestigate network embeddings, i.e., the representation of network nodes as\nlow-dimensional vectors. We consider a typical social network scenario with\nnodes representing users and edges relationships between them. We assume that a\nnetwork embedding of the nodes has been trained. After that, a user demands the\nremoval of his data, requiring the full deletion of the corresponding network\ninformation, in particular the corresponding node and incident edges. In that\nsetting, we analyze whether after the removal of the node from the network and\nthe deletion of the vector representation of the respective node in the\nembedding significant information about the link structure of the removed node\nis still encoded in the embedding vectors of the remaining nodes. This would\nrequire a (potentially computationally expensive) retraining of the embedding.\nFor that purpose, we deploy an attack that leverages information from the\nremaining network and embedding to recover information about the neighbors of\nthe removed node. The attack is based on (i) measuring distance changes in\nnetwork embeddings and (ii) a machine learning classifier that is trained on\nnetworks that are constructed by removing additional nodes. Our experiments\ndemonstrate that substantial information about the edges of a removed node/user\ncan be retrieved across many different datasets. This implies that to fully\nprotect the privacy of users, node deletion requires complete retraining - or\nat least a significant modification - of original network embeddings. Our\nresults suggest that deleting the corresponding vector representation from\nnetwork embeddings alone is not sufficient from a privacy perspective.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10979v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10795v1",
    "title": "(Mis)Information Operations: An Integrated Perspective",
    "authors": [
      "Matteo Cinelli",
      "Mauro Conti",
      "Livio Finos",
      "Francesco Grisolia",
      "Petra Kralj Novak",
      "Antonio Peruzzi",
      "Maurizio Tesconi",
      "Fabiana Zollo",
      "Walter Quattrociocchi"
    ],
    "author_ids": [],
    "abstract": "The massive diffusion of social media fosters disintermediation and changes\nthe way users are informed, the way they process reality, and the way they\nengage in public debate. The cognitive layer of users and the related social\ndynamics define the nature and the dimension of informational threats. Users\nshow the tendency to interact with information adhering to their preferred\nnarrative and to ignore dissenting information. Confirmation bias seems to\naccount for users decisions about consuming and spreading content; and, at the\nsame time, aggregation of favored information within those communities\nreinforces group polarization. In this work, the authors address the problem of\n(mis)information operations with a holistic and integrated approach. Cognitive\nweakness induced by this new information environment are considered. Moreover,\n(mis)information operations, with particular reference to the Italian context,\nare considered; and the fact that the phenomenon is more complex than expected\nis highlighted. The paper concludes by providing an integrated research roadmap\naccounting for the possible future technological developments.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10795v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.10564v1",
    "title": "Teaching Responsible Data Science: Charting New Pedagogical Territory",
    "authors": [
      "Julia Stoyanovich",
      "Armanda Lewis"
    ],
    "author_ids": [],
    "abstract": "Although numerous ethics courses are available, with many focusing\nspecifically on technology and computer ethics, pedagogical approaches employed\nin these courses rely exclusively on texts rather than on software development\nor data analysis. Technical students often consider these courses unimportant\nand a distraction from the \"real\" material. To develop instructional materials\nand methodologies that are thoughtful and engaging, we must strive for balance:\nbetween texts and coding, between critique and solution, and between\ncutting-edge research and practical applicability. Finding such balance is\nparticularly difficult in the nascent field of responsible data science (RDS),\nwhere we are only starting to understand how to interface between the\nintrinsically different methodologies of engineering and social sciences. In\nthis paper we recount a recent experience in developing and teaching an RDS\ncourse to graduate and advanced undergraduate students in data science. We then\ndive into an area that is critically important to RDS -- transparency and\ninterpretability of machine-assisted decision-making, and tie this area to the\nneeds of emerging RDS curricula. Recounting our own experience, and leveraging\nliterature on pedagogical methods in data science and beyond, we propose the\nnotion of an \"object-to-interpret-with\". We link this notion to \"nutritional\nlabels\" -- a family of interpretability tools that are gaining popularity in\nRDS research and practice. With this work we aim to contribute to the nascent\narea of RDS education, and to inspire others in the community to come together\nto develop a deeper theoretical understanding of the pedagogical needs of RDS,\nand contribute concrete educational materials and methodologies that others can\nuse. All course materials are publicly available at\nhttps://dataresponsibly.github.io/courses.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10564v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10563v1",
    "title": "Fair Matching in Dynamic Kidney Exchange",
    "authors": [
      "Irena Gao"
    ],
    "author_ids": [],
    "abstract": "Kidney transplants are sharply overdemanded in the United States. A recent\ninnovation to address organ shortages is a kidney exchange, in which willing\nbut medically incompatible patient-donor pairs swap donors so that two\nsuccessful transplants occur. Proposed rules for matching such pairs include\nstatic fair matching rules, which improve matching for a particular group, such\nas highly-sensitized patients. However, in dynamic environments, it seems\nintuitively fair to prioritize time-critical pairs. We consider the tradeoff\nbetween established sensitization fairness and time fairness in dynamic\nenvironments. We design two algorithms, SENS and TIME, and study their patient\nloss. We show that the there is a theoretical advantage to prioritizing\ntime-critical patients (around 9.18% tradeoff on U.S. data) rather than\nsensitized patients. Our results suggest that time fairness needs to be\nconsidered in kidney exchange. We then propose a batching algorithm for current\nbranch-and-price solvers that balances both fairness needs.",
    "published_date": "2019-12-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10563v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.10531v1",
    "title": "The CoCo-Beholder: Enabling Comprehensive Evaluation of Congestion Control Algorithms",
    "authors": [
      "Evgeniya Khasina"
    ],
    "author_ids": [],
    "abstract": "The recent endeavors of the research community to unite efforts on the design\nand evaluation of congestion control algorithms have created a growing\ncollection of congestion control schemes called Pantheon. However, the virtual\nnetwork emulator that comes with the collection has very limited capabilities:\nit can run flows of only one scheme at once, and the flows cannot have\nindividual network settings, as the topology is point-to-point. This thesis\naddresses those limitations and presents CoCo-Beholder, a human-friendly\nemulator providing the popular dumbbell topology of any size, each link of\nwhich may have individual rate, delay, and queue size. The central link of the\ntopology may also have a variable delay with optional jitter. Flows of\ndifferent schemes may run between the halves of the topology at once, and for\neach flow, the direction and starting time can be chosen. CoCo-Beholder's\nreliability is ensured by testing schemes in the dumbbell topology of size one\nand comparing the results against those of Pantheon emulator. With\nCoCo-Beholder, the thesis successfully reproduces experiments from a recent\npaper that evaluated the fairness and RTT-fairness of schemes using a real\nhardware dumbbell testbed. Finally, the thesis explores the behavior of schemes\nunder the square-wave delay using CoCo-Beholder's variable delay feature.",
    "published_date": "2019-12-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI",
      "C.2.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10531v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.10489v1",
    "title": "Recurrent Feedback Improves Feedforward Representations in Deep Neural Networks",
    "authors": [
      "Siming Yan",
      "Xuyang Fang",
      "Bowen Xiao",
      "Harold Rockwell",
      "Yimeng Zhang",
      "Tai Sing Lee"
    ],
    "author_ids": [],
    "abstract": "The abundant recurrent horizontal and feedback connections in the primate\nvisual cortex are thought to play an important role in bringing global and\nsemantic contextual information to early visual areas during perceptual\ninference, helping to resolve local ambiguity and fill in missing details. In\nthis study, we find that introducing feedback loops and horizontal recurrent\nconnections to a deep convolution neural network (VGG16) allows the network to\nbecome more robust against noise and occlusion during inference, even in the\ninitial feedforward pass. This suggests that recurrent feedback and contextual\nmodulation transform the feedforward representations of the network in a\nmeaningful and interesting way. We study the population codes of neurons in the\nnetwork, before and after learning with feedback, and find that learning with\nfeedback yielded an increase in discriminability (measured by d-prime) between\nthe different object classes in the population codes of the neurons in the\nfeedforward path, even at the earliest layer that receives feedback. We find\nthat recurrent feedback, by injecting top-down semantic meaning to the\npopulation activities, helps the network learn better feedforward paths to\nrobustly map noisy image patches to the latent representations corresponding to\nimportant visual concepts of each object class, resulting in greater robustness\nof the network against noises and occlusion as well as better fine-grained\nrecognition.",
    "published_date": "2019-12-22T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.NC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10489v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10389v1",
    "title": "Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning",
    "authors": [
      "Eun Seo Jo",
      "Timnit Gebru"
    ],
    "author_ids": [],
    "abstract": "A growing body of work shows that many problems in fairness, accountability,\ntransparency, and ethics in machine learning systems are rooted in decisions\nsurrounding the data collection and annotation process. In spite of its\nfundamental nature however, data collection remains an overlooked part of the\nmachine learning (ML) pipeline. In this paper, we argue that a new\nspecialization should be formed within ML that is focused on methodologies for\ndata collection and annotation: efforts that require institutional frameworks\nand procedures. Specifically for sociocultural data, parallels can be drawn\nfrom archives and libraries. Archives are the longest standing communal effort\nto gather human information and archive scholars have already developed the\nlanguage and procedures to address and discuss many challenges pertaining to\ndata collection such as consent, power, inclusivity, transparency, and ethics &\nprivacy. We discuss these five key approaches in document collection practices\nin archives that can inform data collection in sociocultural ML. By showing\ndata collection practices from another field, we encourage ML research to be\nmore cognizant and systematic in data collection and draw from\ninterdisciplinary expertise.",
    "published_date": "2019-12-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12184v1",
    "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation",
    "authors": [
      "Chia-Mu Yu",
      "Ching-Tang Chang",
      "Yen-Wu Ti"
    ],
    "author_ids": [],
    "abstract": "Recent advances in AI technology have made the forgery of digital images and\nvideos easier, and it has become significantly more difficult to identify such\nforgeries. These forgeries, if disseminated with malicious intent, can\nnegatively impact social and political stability, and pose significant ethical\nand legal challenges as well. Deepfake is a variant of auto-encoders that use\ndeep learning techniques to identify and exchange images of a person's face in\na picture or film. Deepfake can result in an erosion of public trust in digital\nimages and videos, which has far-reaching effects on political and social\nstability. This study therefore proposes a solution for facial forgery\ndetection to determine if a picture or film has ever been processed by\nDeepfake. The proposed solution reaches detection efficiency by using the\nrecently proposed separable convolutional neural network (CNN) and image\nsegmentation. In addition, this study also examined how different image\nsegmentation methods affect detection results. Finally, the ensemble model is\nused to improve detection capabilities. Experiment results demonstrated the\nexcellent performance of the proposed solution.",
    "published_date": "2019-12-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12184v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10193v1",
    "title": "Eliminating cross-camera bias for vehicle re-identification",
    "authors": [
      "Jinjia Peng",
      "Guangqi Jiang",
      "Dongyan Chen",
      "Tongtong Zhao",
      "Huibing Wang",
      "Xianping Fu"
    ],
    "author_ids": [],
    "abstract": "Vehicle re-identification (reID) often requires recognize a target vehicle in\nlarge datasets captured from multi-cameras. It plays an important role in the\nautomatic analysis of the increasing urban surveillance videos, which has\nbecome a hot topic in recent years. However, the appearance of vehicle images\nis easily affected by the environment that various illuminations, different\nbackgrounds and viewpoints, which leads to the large bias between different\ncameras. To address this problem, this paper proposes a cross-camera adaptation\nframework (CCA), which smooths the bias by exploiting the common space between\ncameras for all samples. CCA first transfers images from multi-cameras into one\ncamera to reduce the impact of the illumination and resolution, which generates\nthe samples with the similar distribution. Then, to eliminate the influence of\nbackground and focus on the valuable parts, we propose an attention alignment\nnetwork (AANet) to learn powerful features for vehicle reID. Specially, in\nAANet, the spatial transfer network with attention module is introduced to\nlocate a series of the most discriminative regions with high-attention weights\nand suppress the background. Moreover, comprehensive experimental results have\ndemonstrated that our proposed CCA can achieve excellent performances on\nbenchmark datasets VehicleID and VeRi-776.",
    "published_date": "2019-12-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10193v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10178v1",
    "title": "DBP: Discrimination Based Block-Level Pruning for Deep Model Acceleration",
    "authors": [
      "Wenxiao Wang",
      "Shuai Zhao",
      "Minghao Chen",
      "Jinming Hu",
      "Deng Cai",
      "Haifeng Liu"
    ],
    "author_ids": [],
    "abstract": "Neural network pruning is one of the most popular methods of accelerating the\ninference of deep convolutional neural networks (CNNs). The dominant pruning\nmethods, filter-level pruning methods, evaluate their performance through the\nreduction ratio of computations and deem that a higher reduction ratio of\ncomputations is equivalent to a higher acceleration ratio in terms of inference\ntime. However, we argue that they are not equivalent if parallel computing is\nconsidered. Given that filter-level pruning only prunes filters in layers and\ncomputations in a layer usually run in parallel, most computations reduced by\nfilter-level pruning usually run in parallel with the un-reduced ones. Thus,\nthe acceleration ratio of filter-level pruning is limited. To get a higher\nacceleration ratio, it is better to prune redundant layers because computations\nof different layers cannot run in parallel. In this paper, we propose our\nDiscrimination based Block-level Pruning method (DBP). Specifically, DBP takes\na sequence of consecutive layers (e.g., Conv-BN-ReLu) as a block and removes\nredundant blocks according to the discrimination of their output features. As a\nresult, DBP achieves a considerable acceleration ratio by reducing the depth of\nCNNs. Extensive experiments show that DBP has surpassed state-of-the-art\nfilter-level pruning methods in both accuracy and acceleration ratio. Our code\nwill be made available soon.",
    "published_date": "2019-12-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10178v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10158v1",
    "title": "Regularized Operating Envelope with Interpretability and Implementability Constraints",
    "authors": [
      "Qiyao Wang",
      "Haiyan Wang",
      "Chetan Gupta",
      "Susumu Serita"
    ],
    "author_ids": [],
    "abstract": "Operating envelope is an important concept in industrial operations. Accurate\nidentification for operating envelope can be extremely beneficial to\nstakeholders as it provides a set of operational parameters that optimizes some\nkey performance indicators (KPI) such as product quality, operational safety,\nequipment efficiency, environmental impact, etc. Given the importance,\ndata-driven approaches for computing the operating envelope are gaining\npopularity. These approaches typically use classifiers such as support vector\nmachines, to set the operating envelope by learning the boundary in the\noperational parameter spaces between the manually assigned `large KPI' and\n`small KPI' groups. One challenge to these approaches is that the assignment to\nthese groups is often ad-hoc and hence arbitrary. However, a bigger challenge\nwith these approaches is that they don't take into account two key features\nthat are needed to operationalize operating envelopes: (i) interpretability of\nthe envelope by the operator and (ii) implementability of the envelope from a\npractical standpoint. In this work, we propose a new definition for operating\nenvelope which directly targets the expected magnitude of KPI (i.e., no need to\narbitrarily bin the data instances into groups) and accounts for the\ninterpretability and the implementability. We then propose a regularized `GA +\npenalty' algorithm that outputs an envelope where the user can tradeoff between\nbias and variance. The validity of our proposed algorithm is demonstrated by\ntwo sets of simulation studies and an application to a real-world challenge in\nthe mining processes of a flotation plant.",
    "published_date": "2019-12-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10158v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10080v1",
    "title": "Dynamic Prediction of ICU Mortality Risk Using Domain Adaptation",
    "authors": [
      "Tiago Alves",
      "Alberto Laender",
      "Adriano Veloso",
      "Nivio Ziviani"
    ],
    "author_ids": [],
    "abstract": "Early recognition of risky trajectories during an Intensive Care Unit (ICU)\nstay is one of the key steps towards improving patient survival. Learning\ntrajectories from physiological signals continuously measured during an ICU\nstay requires learning time-series features that are robust and discriminative\nacross diverse patient populations. Patients within different ICU populations\n(referred here as domains) vary by age, conditions and interventions. Thus,\nmortality prediction models using patient data from a particular ICU population\nmay perform suboptimally in other populations because the features used to\ntrain such models have different distributions across the groups. In this\npaper, we explore domain adaptation strategies in order to learn mortality\nprediction models that extract and transfer complex temporal features from\nmultivariate time-series ICU data. Features are extracted in a way that the\nstate of the patient in a certain time depends on the previous state. This\nenables dynamic predictions and creates a mortality risk space that describes\nthe risk of a patient at a particular time. Experiments based on cross-ICU\npopulations reveals that our model outperforms all considered baselines. Gains\nin terms of AUC range from 4% to 8% for early predictions when compared with a\nrecent state-of-the-art representative for ICU mortality prediction. In\nparticular, models for the Cardiac ICU population achieve AUC numbers as high\nas 0.88, showing excellent clinical utility for early mortality prediction.\nFinally, we present an explanation of factors contributing to the possible ICU\noutcomes, so that our models can be used to complement clinical reasoning.",
    "published_date": "2019-12-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10080v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.09815v2",
    "title": "Solving Equation Systems in $ω$-categorical Algebras",
    "authors": [
      "Manuel Bodirsky",
      "Thomas Quinn-Gregson"
    ],
    "author_ids": [],
    "abstract": "We study the computational complexity of deciding whether a given set of term\nequalities and inequalities has a solution in an $\\omega$-categorical algebra\n$\\mathfrak{A}$. There are $\\omega$-categorical groups where this problem is\nundecidable. We show that if $\\mathfrak{A}$ is an $\\omega$-categorical\nsemilattice or an abelian group, then the problem is in P or NP-hard. The hard\ncases are precisely those where Pol$(\\mathfrak{A},\\neq)$ has a uniformly\ncontinuous minor-preserving map to the clone of projections on a two-element\nset. The results provide information about algebras $\\mathfrak{A}$ such that\nPol$(\\mathfrak{A},\\neq)$ does not satisfy this condition, and they are of\nindependent interest in universal algebra. In our proofs we rely on the\nBarto-Pinsker theorem about the existence of pseudo-Siggers polymorphisms. To\nthe best of our knowledge, this is the first time that the pseudo-Siggers\nidentity has been used to prove a complexity dichotomy.",
    "published_date": "2019-12-20T00:00:00",
    "year": 2019,
    "categories": [
      "math.LO",
      "cs.CC",
      "math.RA",
      "03C10, 08A70, 08A40",
      "F.4.1; I.1.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.09815v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.09727v3",
    "title": "Computation of the maximal invariant set of discrete-time linear systems subject to a class of non-convex constraints",
    "authors": [
      "Zheming Wang",
      "Raphaël M. Jungers",
      "Chong-Jin Ong"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of computing the maximal invariant set of\ndiscrete-time linear systems subject to a class of non-convex constraints that\nadmit quadratic relaxations. These non-convex constraints include semialgebraic\nsets and other smooth constraints with Lipschitz gradient. With these quadratic\nrelaxations, a sufficient condition for set invariance is derived and it can be\nformulated as a set of linear matrix inequalities. Based on the sufficient\ncondition, a new algorithm is presented with finite-time convergence to the\nactual maximal invariant set under mild assumptions. This algorithm can be also\nextended to switched linear systems and some special nonlinear systems. The\nperformance of this algorithm is demonstrated on several numerical examples.",
    "published_date": "2019-12-20T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.09727v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.09534v1",
    "title": "Convergence to periodic regimes in nonlinear feedback systems with a strongly convex backlash",
    "authors": [
      "Igor G. Vladimirov",
      "Ian R. Petersen"
    ],
    "author_ids": [],
    "abstract": "This paper considers a class of nonlinear systems consisting of a linear part\nwith an external input and a nonlinear feedback with a backlash. Assuming that\nthe latter is specified by a strongly convex set, we establish estimates for\nthe Lyapunov exponents which quantify the rate of convergence of the system\ntrajectories to a forced periodic regime when the input is a periodic function\nof time. These results employ enhanced dissipation inequalities for\ndifferential inclusions with strongly convex sets, which were used previously\nfor the Moreau sweeping process.",
    "published_date": "2019-12-19T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.DS",
      "math.OC",
      "34C05, 34C25, 34C55, 34A40, 26B25, 34D08, 34D45"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.09534v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.09040v1",
    "title": "Reducing Selection Bias in Counterfactual Reasoning for Individual Treatment Effects Estimation",
    "authors": [
      "Zichen Zhang",
      "Qingfeng Lan",
      "Lei Ding",
      "Yue Wang",
      "Negar Hassanpour",
      "Russell Greiner"
    ],
    "author_ids": [],
    "abstract": "Counterfactual reasoning is an important paradigm applicable in many fields,\nsuch as healthcare, economics, and education. In this work, we propose a novel\nmethod to address the issue of \\textit{selection bias}. We learn two groups of\nlatent random variables, where one group corresponds to variables that only\ncause selection bias, and the other group is relevant for outcome prediction.\nThey are learned by an auto-encoder where an additional regularized loss based\non Pearson Correlation Coefficient (PCC) encourages the de-correlation between\nthe two groups of random variables. This allows for explicitly alleviating\nselection bias by only keeping the latent variables that are relevant for\nestimating individual treatment effects. Experimental results on a synthetic\ntoy dataset and a benchmark dataset show that our algorithm is able to achieve\nstate-of-the-art performance and improve the result of its counterpart that\ndoes not explicitly model the selection bias.",
    "published_date": "2019-12-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.09040v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08964v1",
    "title": "Exploring AI Futures Through Role Play",
    "authors": [
      "Shahar Avin",
      "Ross Gruetzemacher",
      "James Fox"
    ],
    "author_ids": [],
    "abstract": "We present an innovative methodology for studying and teaching the impacts of\nAI through a role play game. The game serves two primary purposes: 1) training\nAI developers and AI policy professionals to reflect on and prepare for future\nsocial and ethical challenges related to AI and 2) exploring possible futures\ninvolving AI technology development, deployment, social impacts, and\ngovernance. While the game currently focuses on the inter relations between\nshort --, mid and long term impacts of AI, it has potential to be adapted for a\nbroad range of scenarios, exploring in greater depths issues of AI policy\nresearch and affording training within organizations. The game presented here\nhas undergone two years of development and has been tested through over 30\nevents involving between 3 and 70 participants. The game is under active\ndevelopment, but preliminary findings suggest that role play is a promising\nmethodology for both exploring AI futures and training individuals and\norganizations in thinking about, and reflecting on, the impacts of AI and\nstrategic mistakes that can be avoided today.",
    "published_date": "2019-12-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08964v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08954v1",
    "title": "An Adversarial Perturbation Oriented Domain Adaptation Approach for Semantic Segmentation",
    "authors": [
      "Jihan Yang",
      "Ruijia Xu",
      "Ruiyu Li",
      "Xiaojuan Qi",
      "Xiaoyong Shen",
      "Guanbin Li",
      "Liang Lin"
    ],
    "author_ids": [],
    "abstract": "We focus on Unsupervised Domain Adaptation (UDA) for the task of semantic\nsegmentation. Recently, adversarial alignment has been widely adopted to match\nthe marginal distribution of feature representations across two domains\nglobally. However, this strategy fails in adapting the representations of the\ntail classes or small objects for semantic segmentation since the alignment\nobjective is dominated by head categories or large objects. In contrast to\nadversarial alignment, we propose to explicitly train a domain-invariant\nclassifier by generating and defensing against pointwise feature space\nadversarial perturbations. Specifically, we firstly perturb the intermediate\nfeature maps with several attack objectives (i.e., discriminator and\nclassifier) on each individual position for both domains, and then the\nclassifier is trained to be invariant to the perturbations. By perturbing each\nposition individually, our model treats each location evenly regardless of the\ncategory or object size and thus circumvents the aforementioned issue.\nMoreover, the domain gap in feature space is reduced by extrapolating source\nand target perturbed features towards each other with attack on the domain\ndiscriminator. Our approach achieves the state-of-the-art performance on two\nchallenging domain adaptation tasks for semantic segmentation: GTA5 ->\nCityscapes and SYNTHIA -> Cityscapes.",
    "published_date": "2019-12-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08954v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08905v1",
    "title": "The Spectral Bias of the Deep Image Prior",
    "authors": [
      "Prithvijit Chakrabarty",
      "Subhransu Maji"
    ],
    "author_ids": [],
    "abstract": "The \"deep image prior\" proposed by Ulyanov et al. is an intriguing property\nof neural nets: a convolutional encoder-decoder network can be used as a prior\nfor natural images. The network architecture implicitly introduces a bias; If\nwe train the model to map white noise to a corrupted image, this bias guides\nthe model to fit the true image before fitting the corrupted regions.\n  This paper explores why the deep image prior helps in denoising natural\nimages. We present a novel method to analyze trajectories generated by the deep\nimage prior optimization and demonstrate:\n  (i) convolution layers of the an encoder-decoder decouple the frequency\ncomponents of the image, learning each at different rates\n  (ii) the model fits lower frequencies first, making early stopping behave as\na low pass filter.\n  The experiments study an extension of Cheng et al which showed that at\ninitialization, the deep image prior is equivalent to a stationary Gaussian\nprocess.",
    "published_date": "2019-12-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08905v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10007v2",
    "title": "CAT(0) geometry, robots, and society",
    "authors": [
      "Federico Ardila"
    ],
    "author_ids": [],
    "abstract": "How do we move a robot efficiently from one position to another? To answer\nthis question, we need to understand its configuration space, a 'map' where we\ncan find every possible position of the robot. Unfortunately, these maps are\nvery large, they live in high dimensions, and they are very difficult to\nvisualize. Fortunately, for some discrete robots they are CAT(0) cubical\ncomplexes, a family of spaces with favorable properties. In this case, using\nideas from combinatorics and geometric group theory, we can construct a 'remote\ncontrol' to navigate these complicated maps, and move the robots optimally.\nAlong the way, we face larger ethical questions that we cannot ignore.",
    "published_date": "2019-12-18T00:00:00",
    "year": 2019,
    "categories": [
      "math.HO",
      "cs.RO",
      "math.CO",
      "05-02, 67-02, 51F99, 68T40, 97A40, 97P70"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10007v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.08388v2",
    "title": "Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms During High-Demand Hours",
    "authors": [
      "Vedant Nanda",
      "Pan Xu",
      "Karthik Abinav Sankararaman",
      "John P. Dickerson",
      "Aravind Srinivasan"
    ],
    "author_ids": [],
    "abstract": "Rideshare platforms, when assigning requests to drivers, tend to maximize\nprofit for the system and/or minimize waiting time for riders. Such platforms\ncan exacerbate biases that drivers may have over certain types of requests. We\nconsider the case of peak hours when the demand for rides is more than the\nsupply of drivers. Drivers are well aware of their advantage during the peak\nhours and can choose to be selective about which rides to accept. Moreover, if\nin such a scenario, the assignment of requests to drivers (by the platform) is\nmade only to maximize profit and/or minimize wait time for riders, requests of\na certain type (e.g. from a non-popular pickup location, or to a non-popular\ndrop-off location) might never be assigned to a driver. Such a system can be\nhighly unfair to riders. However, increasing fairness might come at a cost of\nthe overall profit made by the rideshare platform. To balance these conflicting\ngoals, we present a flexible, non-adaptive algorithm, \\lpalg, that allows the\nplatform designer to control the profit and fairness of the system via\nparameters $\\alpha$ and $\\beta$ respectively. We model the matching problem as\nan online bipartite matching where the set of drivers is offline and requests\narrive online. Upon the arrival of a request, we use \\lpalg to assign it to a\ndriver (the driver might then choose to accept or reject it) or reject the\nrequest. We formalize the measures of profit and fairness in our setting and\nshow that by using \\lpalg, the competitive ratios for profit and fairness\nmeasures would be no worse than $\\alpha/e$ and $\\beta/e$ respectively.\nExtensive experimental results on both real-world and synthetic datasets\nconfirm the validity of our theoretical lower bounds. Additionally, they show\nthat $\\lpalg$ under some choice of $(\\alpha, \\beta)$ can beat two natural\nheuristics, Greedy and Uniform, on \\emph{both} fairness and profit.",
    "published_date": "2019-12-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08388v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08342v1",
    "title": "Finite-Time Convergence of Continuous-Time Optimization Algorithms via Differential Inclusions",
    "authors": [
      "Orlando Romero",
      "Mouhacine Benosman"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose two discontinuous dynamical systems in continuous\ntime with guaranteed prescribed finite-time local convergence to strict local\nminima of a given cost function. Our approach consists of exploiting a\nLyapunov-based differential inequality for differential inclusions, which leads\nto finite-time stability and thus finite-time convergence with a provable bound\non the settling time. In particular, for exact solutions to the aforementioned\ndifferential inequality, the settling-time bound is also exact, thus achieving\nprescribed finite-time convergence. We thus construct a class of discontinuous\ndynamical systems, of second order with respect to the cost function, that\nserve as continuous-time optimization algorithms with finite-time convergence\nand prescribed convergence time. Finally, we illustrate our results on the\nRosenbrock function.",
    "published_date": "2019-12-18T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08286v1",
    "title": "On the Bias-Variance Tradeoff: Textbooks Need an Update",
    "authors": [
      "Brady Neal"
    ],
    "author_ids": [],
    "abstract": "The main goal of this thesis is to point out that the bias-variance tradeoff\nis not always true (e.g. in neural networks). We advocate for this lack of\nuniversality to be acknowledged in textbooks and taught in introductory courses\nthat cover the tradeoff. We first review the history of the bias-variance\ntradeoff, its prevalence in textbooks, and some of the main claims made about\nthe bias-variance tradeoff. Through extensive experiments and analysis, we show\na lack of a bias-variance tradeoff in neural networks when increasing network\nwidth. Our findings seem to contradict the claims of the landmark work by Geman\net al. (1992). Motivated by this contradiction, we revisit the experimental\nmeasurements in Geman et al. (1992). We discuss that there was never strong\nevidence for a tradeoff in neural networks when varying the number of\nparameters. We observe a similar phenomenon beyond supervised learning, with a\nset of deep reinforcement learning experiments. We argue that textbook and\nlecture revisions are in order to convey this nuanced modern understanding of\nthe bias-variance tradeoff.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08286v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.09318v1",
    "title": "AI and Holistic Review: Informing Human Reading in College Admissions",
    "authors": [
      "AJ Alvero",
      "Noah Arthurs",
      "anthony lising antonio",
      "Benjamin W. Domingue",
      "Ben Gebre-Medhin",
      "Sonia Giebel",
      "Mitchell L. Stevens"
    ],
    "author_ids": [],
    "abstract": "College admissions in the United States is carried out by a human-centered\nmethod of evaluation known as holistic review, which typically involves reading\noriginal narrative essays submitted by each applicant. The legitimacy and\nfairness of holistic review, which gives human readers significant discretion\nover determining each applicant's fitness for admission, has been repeatedly\nchallenged in courtrooms and the public sphere. Using a unique corpus of\n283,676 application essays submitted to a large, selective, state university\nsystem between 2015 and 2016, we assess the extent to which applicant\ndemographic characteristics can be inferred from application essays. We find a\nrelatively interpretable classifier (logistic regression) was able to predict\ngender and household income with high levels of accuracy. Findings suggest that\ndata auditing might be useful in informing holistic review, and perhaps other\nevaluative systems, by checking potential bias in human or computational\nreadings.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.09318v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08189v4",
    "title": "Learning from Discriminatory Training Data",
    "authors": [
      "Przemyslaw A. Grabowicz",
      "Nicholas Perello",
      "Kenta Takatsu"
    ],
    "author_ids": [],
    "abstract": "Supervised learning systems are trained using historical data and, if the\ndata was tainted by discrimination, they may unintentionally learn to\ndiscriminate against protected groups. We propose that fair learning methods,\ndespite training on potentially discriminatory datasets, shall perform well on\nfair test datasets. Such dataset shifts crystallize application scenarios for\nspecific fair learning methods. For instance, the removal of direct\ndiscrimination can be represented as a particular dataset shift problem. For\nthis scenario, we propose a learning method that provably minimizes model error\non fair datasets, while blindly training on datasets poisoned with direct\nadditive discrimination. The method is compatible with existing legal systems\nand provides a solution to the widely discussed issue of protected groups'\nintersectionality by striking a balance between the protected groups.\nTechnically, the method applies probabilistic interventions, has causal and\ncounterfactual formulations, and is computationally lightweight - it can be\nused with any supervised learning model to prevent discrimination via proxies\nwhile maximizing model accuracy for business necessity.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "physics.soc-ph",
      "I.2.6; K.4.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08189v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08173v3",
    "title": "Function Approximation via The Subsampled Poincar\\' e Inequality",
    "authors": [
      "Yifan Chen",
      "Thomas Y. Hou"
    ],
    "author_ids": [],
    "abstract": "Function approximation and recovery via some sampled data have long been\nstudied in a wide array of applied mathematics and statistics fields. Analytic\ntools, such as the Poincar\\'e inequality, have been handy for estimating the\napproximation errors in different scales. The purpose of this paper is to study\na generalized Poincar\\' e inequality, where the measurement function is of\nsubsampled type, with a small but non-zero lengthscale that will be made\nprecise. Our analysis identifies this inequality as a basic tool for function\nrecovery problems. We discuss and demonstrate the optimality of the inequality\nconcerning the subsampled lengthscale, connecting it to existing results in the\nliterature. In application to function approximation problems, the\napproximation accuracy using different basis functions and under different\nregularity assumptions is established by using the subsampled Poincar\\'e\ninequality. We observe that the error bound blows up as the subsampled\nlengthscale approaches zero, due to the fact that the underlying function is\nnot regular enough to have well-defined pointwise values. A weighted version of\nthe Poincar\\' e inequality is proposed to address this problem; its optimality\nis also discussed.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08173v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.08026v2",
    "title": "ORCA: a Benchmark for Data Web Crawlers",
    "authors": [
      "Michael Röder",
      "Geraldo de Souza",
      "Denis Kuchelev",
      "Abdelmoneim Amer Desouki",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "author_ids": [],
    "abstract": "The number of RDF knowledge graphs available on the Web grows constantly.\nGathering these graphs at large scale for downstream applications hence\nrequires the use of crawlers. Although Data Web crawlers exist, and general Web\ncrawlers could be adapted to focus on the Data Web, there is currently no\nbenchmark to fairly evaluate their performance. Our work closes this gap by\npresenting the Orca benchmark. Orca generates a synthetic Data Web, which is\ndecoupled from the original Web and enables a fair and repeatable comparison of\nData Web crawlers. Our evaluations show that Orca can be used to reveal the\ndifferent advantages and disadvantages of existing crawlers. The benchmark is\nopen-source and available at https://github.com/dice-group/orca.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.PF"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08026v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.08022v1",
    "title": "Numerical Studies of a Hemivariational Inequality for a Viscoelastic Contact Problem with Damage",
    "authors": [
      "Weimin Han",
      "Michal Jureczka",
      "Anna Ochal"
    ],
    "author_ids": [],
    "abstract": "This paper is devoted to the study of a hemivariational inequality modeling\nthe quasistatic bilateral frictional contact between a viscoelastic body and a\nrigid foundation. The damage effect is built into the model through a parabolic\ndifferential inclusion for the damage function. A solution existence and\nuniqueness result is presented. A fully discrete scheme is introduced with the\ntime derivative of the damage function approximated by the backward finite\ndifferent and the spatial derivatives approximated by finite elements. An\noptimal order error estimate is derived for the fully discrete scheme when\nlinear elements are used for the velocity and displacement variables, and\npiecewise constants are used for the damage function. Simulation results on\nnumerical examples are reported illustrating the performance of the fully\ndiscrete scheme and the theoretically predicted convergence orders.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 65M06, 47J20, 74M10, 74M15"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08022v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.07938v2",
    "title": "How Personal is Machine Learning Personalization?",
    "authors": [
      "Travis Greene",
      "Galit Shmueli"
    ],
    "author_ids": [],
    "abstract": "Though used extensively, the concept and process of machine learning (ML)\npersonalization have generally received little attention from academics,\npractitioners, and the general public. We describe the ML approach as relying\non the metaphor of the person as a feature vector and contrast this with\nhumanistic views of the person. In light of the recent calls by the IEEE to\nconsider the effects of ML on human well-being, we ask whether ML\npersonalization can be reconciled with these humanistic views of the person,\nwhich highlight the importance of moral and social identity. As human behavior\nincreasingly becomes digitized, analyzed, and predicted, to what extent do our\nsubsequent decisions about what to choose, buy, or do, made both by us and\nothers, reflect who we are as persons? This paper first explicates the term\npersonalization by considering ML personalization and highlights its relation\nto humanistic conceptions of the person, then proposes several dimensions for\nevaluating the degree of personalization of ML personalized scores. By doing\nso, we hope to contribute to current debate on the issues of algorithmic bias,\ntransparency, and fairness in machine learning.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.HC",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07938v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.07805v4",
    "title": "Wheel-INS: A Wheel-mounted MEMS IMU-based Dead Reckoning System",
    "authors": [
      "Xiaoji Niu",
      "Yibin Wu",
      "Jian Kuang"
    ],
    "author_ids": [],
    "abstract": "To improve the accuracy and robustness of the inertial navigation systems\n(INS) for wheeled robots without adding additional component cost, we propose\nWheel-INS, a complete dead reckoning solution based on a wheel-mounted\nmicroelectromechanical system (MEMS) inertial measurement unit (IMU). There are\ntwo major advantages by mounting an IMU to the center of a non-steering wheel\nof the ground vehicle. Firstly, the gyroscope outputs can be used to calculate\nthe wheel speed, so as to replace the traditional odometer to mitigate the\nerror drift of INS. Secondly, with the rotation of the wheel, the constant bias\nerror of the inertial sensor can be canceled to some extent. The installation\nscheme of the wheel-mounted IMU (Wheel-IMU), the system characteristics, and\nthe dead reckoning error analysis are described. Experimental results show that\nthe maximum position drift of Wheel-INS in the horizontal plane is less than\n1.8% of the total traveled distance, reduced by 23% compared to the\nconventional odometer-aided INS (ODO/INS). In addition, Wheel-INS outperforms\nODO/INS because of its inherent immunity to constant bias error of gyroscopes.\nThe source code and experimental datasets used in this paper is made available\nto the community (https://github.com/i2Nav-WHU/Wheel-INS).",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07805v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.07804v1",
    "title": "LTLf Synthesis with Fairness and Stability Assumptions",
    "authors": [
      "Shufang Zhu",
      "Giuseppe De Giacomo",
      "Geguang Pu",
      "Moshe Vardi"
    ],
    "author_ids": [],
    "abstract": "In synthesis, assumptions are constraints on the environment that rule out\ncertain environment behaviors. A key observation here is that even if we\nconsider systems with LTLf goals on finite traces, environment assumptions need\nto be expressed over infinite traces, since accomplishing the agent goals may\nrequire an unbounded number of environment action. To solve synthesis with\nrespect to finite-trace LTLf goals under infinite-trace assumptions, we could\nreduce the problem to LTL synthesis. Unfortunately, while synthesis in LTLf and\nin LTL have the same worst-case complexity (both 2EXPTIME-complete), the\nalgorithms available for LTL synthesis are much more difficult in practice than\nthose for LTLf synthesis. In this work we show that in interesting cases we can\navoid such a detour to LTL synthesis and keep the simplicity of LTLf synthesis.\nSpecifically, we develop a BDD-based fixpoint-based technique for handling\nbasic forms of fairness and of stability assumptions. We show, empirically,\nthat this technique performs much better than standard LTL synthesis.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.GT",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07804v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00089v3",
    "title": "Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics",
    "authors": [
      "Debjani Saha",
      "Candice Schumann",
      "Duncan C. McElfresh",
      "John P. Dickerson",
      "Michelle L. Mazurek",
      "Michael Carl Tschantz"
    ],
    "author_ids": [],
    "abstract": "Bias in machine learning has manifested injustice in several areas, such as\nmedicine, hiring, and criminal justice. In response, computer scientists have\ndeveloped myriad definitions of fairness to correct this bias in fielded\nalgorithms. While some definitions are based on established legal and ethical\nnorms, others are largely mathematical. It is unclear whether the general\npublic agrees with these fairness definitions, and perhaps more importantly,\nwhether they understand these definitions. We take initial steps toward\nbridging this gap between ML researchers and the public, by addressing the\nquestion: does a lay audience understand a basic definition of ML fairness? We\ndevelop a metric to measure comprehension of three such\ndefinitions--demographic parity, equal opportunity, and equalized odds. We\nevaluate this metric using an online survey, and investigate the relationship\nbetween comprehension and sentiment, demographics, and the definition itself.",
    "published_date": "2019-12-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00089v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.07726v1",
    "title": "Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy",
    "authors": [
      "Kaiyu Yang",
      "Klint Qinami",
      "Li Fei-Fei",
      "Jia Deng",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Computer vision technology is being used by many but remains representative\nof only a few. People have reported misbehavior of computer vision models,\nincluding offensive prediction results and lower performance for\nunderrepresented groups. Current computer vision models are typically developed\nusing datasets consisting of manually annotated images or videos; the data and\nlabel distributions in these datasets are critical to the models' behavior. In\nthis paper, we examine ImageNet, a large-scale ontology of images that has\nspurred the development of many modern computer vision methods. We consider\nthree key factors within the \"person\" subtree of ImageNet that may lead to\nproblematic behavior in downstream computer vision technology: (1) the stagnant\nconcept vocabulary of WordNet, (2) the attempt at exhaustive illustration of\nall categories with images, and (3) the inequality of representation in the\nimages within concepts. We seek to illuminate the root causes of these concerns\nand take the first steps to mitigate them constructively.",
    "published_date": "2019-12-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07726v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.07676v1",
    "title": "Convergence Analysis of Penalty Based Numerical Methods for Constrained Inequality Problems",
    "authors": [
      "Weimin Han",
      "Mircea Sofonea"
    ],
    "author_ids": [],
    "abstract": "This paper presents a general convergence theory of penalty based numerical\nmethods for elliptic constrained inequality problems, including variational\ninequalities, hemivariational inequalities, and variational-hemivariational\ninequalities. The constraint is relaxed by a penalty formulation and is\nre-stored as the penalty parameter tends to zero. The main theoretical result\nof the paper is the convergence of the penalty based numerical solutions to the\nsolution of the constrained inequality problem as the mesh-size and the penalty\nparameter approach zero simultaneously but independently. The convergence of\nthe penalty based numerical methods is first established for a general elliptic\nvariational-hemivariational inequality with constraints, and then for\nhemivariational inequalities and variational inequalities as special cases.\nApplications to problems in contact mechanics are described.",
    "published_date": "2019-12-16T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07676v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.07398v2",
    "title": "Accuracy comparison across face recognition algorithms: Where are we on measuring race bias?",
    "authors": [
      "Jacqueline G. Cavazos",
      "P. Jonathon Phillips",
      "Carlos D. Castillo",
      "Alice J. O'Toole"
    ],
    "author_ids": [],
    "abstract": "Previous generations of face recognition algorithms differ in accuracy for\nimages of different races (race bias). Here, we present the possible underlying\nfactors (data-driven and scenario modeling) and methodological considerations\nfor assessing race bias in algorithms. We discuss data driven factors (e.g.,\nimage quality, image population statistics, and algorithm architecture), and\nscenario modeling factors that consider the role of the \"user\" of the algorithm\n(e.g., threshold decisions and demographic constraints). To illustrate how\nthese issues apply, we present data from four face recognition algorithms (a\nprevious-generation algorithm and three deep convolutional neural networks,\nDCNNs) for East Asian and Caucasian faces. First, dataset difficulty affected\nboth overall recognition accuracy and race bias, such that race bias increased\nwith item difficulty. Second, for all four algorithms, the degree of bias\nvaried depending on the identification decision threshold. To achieve equal\nfalse accept rates (FARs), East Asian faces required higher identification\nthresholds than Caucasian faces, for all algorithms. Third, demographic\nconstraints on the formulation of the distributions used in the test, impacted\nestimates of algorithm accuracy. We conclude that race bias needs to be\nmeasured for individual applications and we provide a checklist for measuring\nthis bias in face recognition algorithms.",
    "published_date": "2019-12-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07398v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.07376v1",
    "title": "Algorithmic Injustices: Towards a Relational Ethics",
    "authors": [
      "Abeba Birhane",
      "Fred Cummins"
    ],
    "author_ids": [],
    "abstract": "It has become trivial to point out how decision-making processes in various\nsocial, political and economical sphere are assisted by automated systems.\nImproved efficiency, the hallmark of these systems, drives the mass scale\nintegration of automated systems into daily life. However, as a robust body of\nresearch in the area of algorithmic injustice shows, algorithmic tools embed\nand perpetuate societal and historical biases and injustice. In particular, a\npersistent recurring trend within the literature indicates that society's most\nvulnerable are disproportionally impacted. When algorithmic injustice and bias\nis brought to the fore, most of the solutions on offer 1) revolve around\ntechnical solutions and 2) do not focus centre disproportionally impacted\ngroups. This paper zooms out and draws the bigger picture. It 1) argues that\nconcerns surrounding algorithmic decision making and algorithmic injustice\nrequire fundamental rethinking above and beyond technical solutions, and 2)\noutlines a way forward in a manner that centres vulnerable groups through the\nlens of relational ethics.",
    "published_date": "2019-12-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07376v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.07211v1",
    "title": "Fairness Assessment for Artificial Intelligence in Financial Industry",
    "authors": [
      "Yukun Zhang",
      "Longsheng Zhou"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) is an important driving force for the\ndevelopment and transformation of the financial industry. However, with the\nfast-evolving AI technology and application, unintentional bias, insufficient\nmodel validation, immature contingency plan and other underestimated threats\nmay expose the company to operational and reputational risks. In this paper, we\nfocus on fairness evaluation, one of the key components of AI Governance,\nthrough a quantitative lens. Statistical methods are reviewed for imbalanced\ndata treatment and bias mitigation. These methods and fairness evaluation\nmetrics are then applied to a credit card default payment example.",
    "published_date": "2019-12-16T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07211v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.10818v1",
    "title": "Artificial mental phenomena: Psychophysics as a framework to detect perception biases in AI models",
    "authors": [
      "Lizhen Liang",
      "Daniel E. Acuna"
    ],
    "author_ids": [],
    "abstract": "Detecting biases in artificial intelligence has become difficult because of\nthe impenetrable nature of deep learning. The central difficulty is in relating\nunobservable phenomena deep inside models with observable, outside quantities\nthat we can measure from inputs and outputs. For example, can we detect\ngendered perceptions of occupations (e.g., female librarian, male electrician)\nusing questions to and answers from a word embedding-based system? Current\ntechniques for detecting biases are often customized for a task, dataset, or\nmethod, affecting their generalization. In this work, we draw from\nPsychophysics in Experimental Psychology---meant to relate quantities from the\nreal world (i.e., \"Physics\") into subjective measures in the mind (i.e.,\n\"Psyche\")---to propose an intellectually coherent and generalizable framework\nto detect biases in AI. Specifically, we adapt the two-alternative forced\nchoice task (2AFC) to estimate potential biases and the strength of those\nbiases in black-box models. We successfully reproduce previously-known biased\nperceptions in word embeddings and sentiment analysis predictions. We discuss\nhow concepts in experimental psychology can be naturally applied to\nunderstanding artificial mental phenomena, and how psychophysics can form a\nuseful methodological foundation to study fairness in AI.",
    "published_date": "2019-12-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.10818v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.06883v1",
    "title": "On the Apparent Conflict Between Individual and Group Fairness",
    "authors": [
      "Reuben Binns"
    ],
    "author_ids": [],
    "abstract": "A distinction has been drawn in fair machine learning research between\n`group' and `individual' fairness measures. Many technical research papers\nassume that both are important, but conflicting, and propose ways to minimise\nthe trade-offs between these measures. This paper argues that this apparent\nconflict is based on a misconception. It draws on theoretical discussions from\nwithin the fair machine learning research, and from political and legal\nphilosophy, to argue that individual and group fairness are not fundamentally\nin conflict. First, it outlines accounts of egalitarian fairness which\nencompass plausible motivations for both group and individual fairness, thereby\nsuggesting that there need be no conflict in principle. Second, it considers\nthe concept of individual justice, from legal philosophy and jurisprudence\nwhich seems similar but actually contradicts the notion of individual fairness\nas proposed in the fair machine learning literature. The conclusion is that the\napparent conflict between individual and group fairness is more of an artifact\nof the blunt application of fairness measures, rather than a matter of\nconflicting principles. In practice, this conflict may be resolved by a nuanced\nconsideration of the sources of `unfairness' in a particular deployment\ncontext, and the carefully justified application of measures to mitigate it.",
    "published_date": "2019-12-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.06883v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.00918v1",
    "title": "Fairness in Multi-agent Reinforcement Learning for Stock Trading",
    "authors": [
      "Wenhang Bao"
    ],
    "author_ids": [],
    "abstract": "Unfair stock trading strategies have been shown to be one of the most\nnegative perceptions that customers can have concerning trading and may result\nin long-term losses for a company. Investment banks usually place trading\norders for multiple clients with the same target assets but different order\nsizes and diverse requirements such as time frame and risk aversion level,\nthereby total earning and individual earning cannot be optimized at the same\ntime. Orders executed earlier would affect the market price level, so late\nexecution usually means additional implementation cost. In this paper, we\npropose a novel scheme that utilizes multi-agent reinforcement learning systems\nto derive stock trading strategies for all clients which keep a balance between\nrevenue and fairness. First, we demonstrate that Reinforcement learning (RL) is\nable to learn from experience and adapt the trading strategies to the complex\nmarket environment. Secondly, we show that the Multi-agent RL system allows\ndeveloping trading strategies for all clients individually, thus optimizing\nindividual revenue. Thirdly, we use the Generalized Gini Index (GGI)\naggregation function to control the fairness level of the revenue across all\nclients. Lastly, we empirically demonstrate the superiority of the novel scheme\nin improving fairness meanwhile maintaining optimization of revenue.",
    "published_date": "2019-12-14T00:00:00",
    "year": 2019,
    "categories": [
      "q-fin.TR",
      "cs.LG",
      "cs.MA",
      "q-fin.ST"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.00918v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.06842v1",
    "title": "Fine-grained Recognition: Accounting for Subtle Differences between Similar Classes",
    "authors": [
      "Guolei Sun",
      "Hisham Cholakkal",
      "Salman Khan",
      "Fahad Shahbaz Khan",
      "Ling Shao"
    ],
    "author_ids": [],
    "abstract": "The main requisite for fine-grained recognition task is to focus on subtle\ndiscriminative details that make the subordinate classes different from each\nother. We note that existing methods implicitly address this requirement and\nleave it to a data-driven pipeline to figure out what makes a subordinate class\ndifferent from the others. This results in two major limitations: First, the\nnetwork focuses on the most obvious distinctions between classes and overlooks\nmore subtle inter-class variations. Second, the chance of misclassifying a\ngiven sample in any of the negative classes is considered equal, while in fact,\nconfusions generally occur among only the most similar classes. Here, we\npropose to explicitly force the network to find the subtle differences among\nclosely related classes. In this pursuit, we introduce two key novelties that\ncan be easily plugged into existing end-to-end deep learning pipelines. On one\nhand, we introduce diversification block which masks the most salient features\nfor an input to force the network to use more subtle cues for its correct\nclassification. Concurrently, we introduce a gradient-boosting loss function\nthat focuses only on the confusing classes for each sample and therefore moves\nswiftly along the direction on the loss surface that seeks to resolve these\nambiguities. The synergy between these two blocks helps the network to learn\nmore effective feature representations. Comprehensive experiments are performed\non five challenging datasets. Our approach outperforms existing methods using\nsimilar experimental setting on all five datasets.",
    "published_date": "2019-12-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.06842v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.06808v3",
    "title": "Environmental Sound Classification with Parallel Temporal-spectral Attention",
    "authors": [
      "Helin Wang",
      "Yuexian Zou",
      "Dading Chong",
      "Wenwu Wang"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks (CNN) are one of the best-performing neural\nnetwork architectures for environmental sound classification (ESC). Recently,\ntemporal attention mechanisms have been used in CNN to capture the useful\ninformation from the relevant time frames for audio classification, especially\nfor weakly labelled data where the onset and offset times of the sound events\nare not applied. In these methods, however, the inherent spectral\ncharacteristics and variations are not explicitly exploited when obtaining the\ndeep features. In this paper, we propose a novel parallel temporal-spectral\nattention mechanism for CNN to learn discriminative sound representations,\nwhich enhances the temporal and spectral features by capturing the importance\nof different time frames and frequency bands. Parallel branches are constructed\nto allow temporal attention and spectral attention to be applied respectively\nin order to mitigate interference from the segments without the presence of\nsound events. The experiments on three environmental sound classification (ESC)\ndatasets and two acoustic scene classification (ASC) datasets show that our\nmethod improves the classification performance and also exhibits robustness to\nnoise.",
    "published_date": "2019-12-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.06808v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.08055v1",
    "title": "Fair Contextual Multi-Armed Bandits: Theory and Experiments",
    "authors": [
      "Yifang Chen",
      "Alex Cuellar",
      "Haipeng Luo",
      "Jignesh Modi",
      "Heramb Nemlekar",
      "Stefanos Nikolaidis"
    ],
    "author_ids": [],
    "abstract": "When an AI system interacts with multiple users, it frequently needs to make\nallocation decisions. For instance, a virtual agent decides whom to pay\nattention to in a group setting, or a factory robot selects a worker to deliver\na part. Demonstrating fairness in decision making is essential for such systems\nto be broadly accepted. We introduce a Multi-Armed Bandit algorithm with\nfairness constraints, where fairness is defined as a minimum rate that a task\nor a resource is assigned to a user. The proposed algorithm uses contextual\ninformation about the users and the task and makes no assumptions on how the\nlosses capturing the performance of different users are generated. We provide\ntheoretical guarantees of performance and empirical results from simulation and\nan online user study. The results highlight the benefit of accounting for\ncontexts in fair decision making, especially when users perform better at some\ncontexts and worse at others.",
    "published_date": "2019-12-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08055v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.06352v1",
    "title": "Random Access with Opportunity Detection in Wireless Networks",
    "authors": [
      "Jinho Choi",
      "Seung-Woo Ko",
      "Koji Yamamoto",
      "Seong-Lyun Kim"
    ],
    "author_ids": [],
    "abstract": "This letter proposes a novel random medium access control (MAC) based on a\ntransmission opportunity prediction, which can be measured in a form of a\nconditional success probability given transmitter-side interference. A\ntransmission probability depends on the opportunity prediction, preventing\nindiscriminate transmissions and reducing excessive interference causing\ncollisions. Using stochastic geometry, we derive a fixed-point equation to\nprovide the optimal transmission probability maximizing a proportionally fair\nthroughput. Its approximated solution is given in closed form. The proposed MAC\nis applicable to full-duplex networks, leading to significant throughput\nimprovement by allowing more nodes to transmit.",
    "published_date": "2019-12-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.06352v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.06171v1",
    "title": "Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination",
    "authors": [
      "Miranda Bogen",
      "Aaron Rieke",
      "Shazeda Ahmed"
    ],
    "author_ids": [],
    "abstract": "Organizations cannot address demographic disparities that they cannot see.\nRecent research on machine learning and fairness has emphasized that awareness\nof sensitive attributes, such as race and sex, is critical to the development\nof interventions. However, on the ground, the existence of these data cannot be\ntaken for granted.\n  This paper uses the domains of employment, credit, and healthcare in the\nUnited States to surface conditions that have shaped the availability of\nsensitive attribute data. For each domain, we describe how and when private\ncompanies collect or infer sensitive attribute data for antidiscrimination\npurposes. An inconsistent story emerges: Some companies are required by law to\ncollect sensitive attribute data, while others are prohibited from doing so.\nStill others, in the absence of legal mandates, have determined that collection\nand imputation of these data are appropriate to address disparities.\n  This story has important implications for fairness research and its future\napplications. If companies that mediate access to life opportunities are unable\nor hesitant to collect or infer sensitive attribute data, then proposed\ntechniques to detect and mitigate bias in machine learning models might never\nbe implemented outside the lab. We conclude that today's legal requirements and\ncorporate practices, while highly inconsistent across domains, offer lessons\nfor how to approach the collection and inference of sensitive data in\nappropriate circumstances. We urge stakeholders, including machine learning\npractitioners, to actively help chart a path forward that takes both policy\ngoals and technical needs into account.",
    "published_date": "2019-12-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.06171v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.07661v2",
    "title": "It's easy to fool yourself: Case studies on identifying bias and confounding in bio-medical datasets",
    "authors": [
      "Subhashini Venugopalan",
      "Arunachalam Narayanaswamy",
      "Samuel Yang",
      "Anton Geraschenko",
      "Scott Lipnick",
      "Nina Makhortova",
      "James Hawrot",
      "Christine Marques",
      "Joao Pereira",
      "Michael Brenner",
      "Lee Rubin",
      "Brian Wainger",
      "Marc Berndl"
    ],
    "author_ids": [],
    "abstract": "Confounding variables are a well known source of nuisance in biomedical\nstudies. They present an even greater challenge when we combine them with\nblack-box machine learning techniques that operate on raw data. This work\npresents two case studies. In one, we discovered biases arising from systematic\nerrors in the data generation process. In the other, we found a spurious source\nof signal unrelated to the prediction task at hand. In both cases, our\nprediction models performed well but under careful examination hidden\nconfounders and biases were revealed. These are cautionary tales on the limits\nof using machine learning techniques on raw data from scientific experiments.",
    "published_date": "2019-12-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "eess.IV",
      "q-bio.QM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07661v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.06105v4",
    "title": "Bell Diagonal and Werner state generation: entanglement, non-locality, steering and discord on the IBM quantum computer",
    "authors": [
      "Elias Riedel Gårding",
      "Nicolas Schwaller",
      "Su Yeon Chang",
      "Samuel Bosch",
      "Willy Robert Laborde",
      "Javier Naya Hernandez",
      "Chun Lam Chan",
      "Frédéric Gessler",
      "Xinyu Si",
      "Marc-André Dupertuis",
      "Nicolas Macris"
    ],
    "author_ids": [],
    "abstract": "We propose the first correct special-purpose quantum circuits for preparation\nof Bell-diagonal states (BDS), and implement them on the IBM Quantum computer,\ncharacterizing and testing complex aspects of their quantum correlations in the\nfull parameter space. Among the circuits proposed, one involves only two\nquantum bits but requires adapted quantum tomography routines handling\nclassical bits in parallel. The entire class of Bell-diagonal states is\ngenerated, and a number of characteristic indicators, namely entanglement of\nformation, CHSH non-locality, steering and discord, are experimentally\nevaluated over the full parameter space and compared with theory. As a\nby-product of this work we also find a remarkable general inequality between\n\"quantum discord\" and \"asymmetric relative entropy of discord\": the former\nnever exceeds the latter. We also prove that for all BDS the two coincide.",
    "published_date": "2019-12-12T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.06105v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.05770v1",
    "title": "Algorithmic Price Discrimination",
    "authors": [
      "Rachel Cummings",
      "Nikhil R. Devanur",
      "Zhiyi Huang",
      "Xiangning Wang"
    ],
    "author_ids": [],
    "abstract": "We consider a generalization of the third degree price discrimination problem\nstudied in Bergemann et al. (2015), where an intermediary between the buyer and\nthe seller can design market segments to maximize any linear combination of\nconsumer surplus and seller revenue. Unlike in Bergemann et al. (2015), we\nassume that the intermediary only has partial information about the buyer's\nvalue. We consider three different models of information, with increasing order\nof difficulty. In the first model, we assume that the intermediary's\ninformation allows him to construct a probability distribution of the buyer's\nvalue. Next we consider the sample complexity model, where we assume that the\nintermediary only sees samples from this distribution. Finally, we consider a\nbandit online learning model, where the intermediary can only observe past\npurchasing decisions of the buyer, rather than her exact value. For each of\nthese models, we present algorithms to compute optimal or near optimal market\nsegmentation.",
    "published_date": "2019-12-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05770v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.05620v3",
    "title": "Judge, Jury & Encryptioner: Exceptional Device Access with a Social Cost",
    "authors": [
      "Sacha Servan-Schreiber",
      "Archer Wheeler"
    ],
    "author_ids": [],
    "abstract": "We present Judge, Jury and Encryptioner (JJE) an exceptional access scheme\nfor unlocking devices that does not give unilateral power to any single\nauthority. JJE achieves this by placing final approval to unlock a device in\nthe hands of peer devices. JJE distributes maintenance of the protocol across a\nnetwork of \"custodians\" such as courts, government agencies, civil rights\nwatchdogs, and academic institutions. Unlock requests, however, can only be\napproved by a randomly selected set of recently active peer devices that must\nbe physically located by law enforcement in order to gain access to the locked\ndevice. This requires that law enforcement expend both human and monetary\nresources and pay a \"social cost\" in order to find and request the\nparticipation of random device owners in the unlock process. Compared to other\nproposed exceptional access schemes, we believe that JJE mitigates the risk of\nmass surveillance, law enforcement abuse, and vulnerability to unlawful\nattackers. While we propose a concrete construction, our primary goal with JJE\nis to spur discussion on ethical exceptional access schemes that balance\nprivacy of individuals and the desires for law enforcement. JJE transparently\nreveals the use of exceptional access to the public and enforces a fixed social\ncost that, we believe, can be an effective deterrent to mass surveillance and\nabuse.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05620v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.05534v1",
    "title": "Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition",
    "authors": [
      "Jinwoo Choi",
      "Chen Gao",
      "Joseph C. E. Messou",
      "Jia-Bin Huang"
    ],
    "author_ids": [],
    "abstract": "Human activities often occur in specific scene contexts, e.g., playing\nbasketball on a basketball court. Training a model using existing video\ndatasets thus inevitably captures and leverages such bias (instead of using the\nactual discriminative cues). The learned representation may not generalize well\nto new action classes or different tasks. In this paper, we propose to mitigate\nscene bias for video representation learning. Specifically, we augment the\nstandard cross-entropy loss for action classification with 1) an adversarial\nloss for scene types and 2) a human mask confusion loss for videos where the\nhuman actors are masked out. These two losses encourage learning\nrepresentations that are unable to predict the scene types and the correct\nactions when there is no evidence. We validate the effectiveness of our method\nby transferring our pre-trained model to three different tasks, including\naction classification, temporal localization, and spatio-temporal action\ndetection. Our results show consistent improvement over the baseline model\nwithout debiasing.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05511v3",
    "title": "Measurement and Fairness",
    "authors": [
      "Abigail Z. Jacobs",
      "Hanna Wallach"
    ],
    "author_ids": [],
    "abstract": "We propose measurement modeling from the quantitative social sciences as a\nframework for understanding fairness in computational systems. Computational\nsystems often involve unobservable theoretical constructs, such as\nsocioeconomic status, teacher effectiveness, and risk of recidivism. Such\nconstructs cannot be measured directly and must instead be inferred from\nmeasurements of observable properties (and other unobservable theoretical\nconstructs) thought to be related to them -- i.e., operationalized via a\nmeasurement model. This process, which necessarily involves making assumptions,\nintroduces the potential for mismatches between the theoretical understanding\nof the construct purported to be measured and its operationalization. We argue\nthat many of the harms discussed in the literature on fairness in computational\nsystems are direct results of such mismatches. We show how some of these harms\ncould have been anticipated and, in some cases, mitigated if viewed through the\nlens of measurement modeling. To do this, we contribute fairness-oriented\nconceptualizations of construct reliability and construct validity that unite\ntraditions from political science, education, and psychology and provide a set\nof tools for making explicit and testing assumptions about constructs and their\noperationalizations. We then turn to fairness itself, an essentially contested\nconstruct that has different theoretical understandings in different contexts.\nWe argue that this contestedness underlies recent debates about fairness\ndefinitions: although these debates appear to be about different\noperationalizations, they are, in fact, debates about different theoretical\nunderstandings of fairness. We show how measurement modeling can provide a\nframework for getting to the core of these debates.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05511v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05474v1",
    "title": "Female Librarians and Male Computer Programmers? Gender Bias in Occupational Images on Digital Media Platforms",
    "authors": [
      "Vivek Singh",
      "Mary Chayko",
      "Raj Inamdar",
      "Diana Floegel"
    ],
    "author_ids": [],
    "abstract": "Media platforms, technological systems, and search engines act as conduits\nand gatekeepers for all kinds of information. They often influence, reflect,\nand reinforce gender stereotypes, including those that represent occupations.\nThis study examines the prevalence of gender stereotypes on digital media\nplatforms and considers how human efforts to create and curate messages\ndirectly may impact these stereotypes. While gender stereotyping in social\nmedia and algorithms has received some examination in recent literature, its\nprevalence in different types of platforms (e.g., wiki vs. news vs. social\nnetwork) and under differing conditions (e.g., degrees of human and machine led\ncontent creation and curation) has yet to be studied. This research explores\nthe extent to which stereotypes of certain strongly gendered professions\n(librarian, nurse, computer programmer, civil engineer) persist and may vary\nacross digital platforms (Twitter, the New York Times online, Wikipedia, and\nShutterstock). The results suggest that gender stereotypes are most likely to\nbe challenged when human beings act directly to create and curate content in\ndigital platforms, and that highly algorithmic approaches for curation showed\nlittle inclination towards breaking stereotypes. Implications for the more\ninclusive design and use of digital media platforms, particularly with regard\nto mediated occupational messaging, are discussed.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05474v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.05421v1",
    "title": "Just Add Functions: A Neural-Symbolic Language Model",
    "authors": [
      "David Demeter",
      "Doug Downey"
    ],
    "author_ids": [],
    "abstract": "Neural network language models (NNLMs) have achieved ever-improving accuracy\ndue to more sophisticated architectures and increasing amounts of training\ndata. However, the inductive bias of these models (formed by the distributional\nhypothesis of language), while ideally suited to modeling most running text,\nresults in key limitations for today's models. In particular, the models often\nstruggle to learn certain spatial, temporal, or quantitative relationships,\nwhich are commonplace in text and are second-nature for human readers. Yet, in\nmany cases, these relationships can be encoded with simple mathematical or\nlogical expressions. How can we augment today's neural models with such\nencodings?\n  In this paper, we propose a general methodology to enhance the inductive bias\nof NNLMs by incorporating simple functions into a neural architecture to form a\nhierarchical neural-symbolic language model (NSLM). These functions explicitly\nencode symbolic deterministic relationships to form probability distributions\nover words. We explore the effectiveness of this approach on numbers and\ngeographic locations, and show that NSLMs significantly reduce perplexity in\nsmall-corpus language modeling, and that the performance improvement persists\nfor rare tokens even on much larger corpora. The approach is simple and\ngeneral, and we discuss how it can be applied to other word classes beyond\nnumbers and geography.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05421v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05407v1",
    "title": "UCT-ADP Progressive Bias Algorithm for Solving Gomoku",
    "authors": [
      "Xu Cao",
      "Yanghao Lin"
    ],
    "author_ids": [],
    "abstract": "We combine Adaptive Dynamic Programming (ADP), a reinforcement learning\nmethod and UCB applied to trees (UCT) algorithm with a more powerful heuristic\nfunction based on Progressive Bias method and two pruning strategies for a\ntraditional board game Gomoku. For the Adaptive Dynamic Programming part, we\ntrain a shallow forward neural network to give a quick evaluation of Gomoku\nboard situations. UCT is a general approach in MCTS as a tree policy. Our\nframework use UCT to balance the exploration and exploitation of Gomoku game\ntrees while we also apply powerful pruning strategies and heuristic function to\nre-select the available 2-adjacent grids of the state and use ADP instead of\nsimulation to give estimated values of expanded nodes. Experiment result shows\nthat this method can eliminate the search depth defect of the simulation\nprocess and converge to the correct value faster than single UCT. This approach\ncan be applied to design new Gomoku AI and solve other Gomoku-like board game.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05407v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05291v2",
    "title": "A Stable Nuclear Future? The Impact of Autonomous Systems and Artificial Intelligence",
    "authors": [
      "Michael C. Horowitz",
      "Paul Scharre",
      "Alexander Velez-Green"
    ],
    "author_ids": [],
    "abstract": "The potential for advances in information-age technologies to undermine\nnuclear deterrence and influence the potential for nuclear escalation\nrepresents a critical question for international politics. One challenge is\nthat uncertainty about the trajectory of technologies such as autonomous\nsystems and artificial intelligence (AI) makes assessments difficult. This\npaper evaluates the relative impact of autonomous systems and artificial\nintelligence in three areas: nuclear command and control, nuclear delivery\nplatforms and vehicles, and conventional applications of autonomous systems\nwith consequences for nuclear stability. We argue that countries may be more\nlikely to use risky forms of autonomy when they fear that their second-strike\ncapabilities will be undermined. Additionally, the potential deployment of\nuninhabited, autonomous nuclear delivery platforms and vehicles could raise the\nprospect for accidents and miscalculation. Conventional military applications\nof autonomous systems could simultaneously influence nuclear force postures and\nfirst-strike stability in previously unanticipated ways. In particular, the\nneed to fight at machine speed and the cognitive risk introduced by automation\nbias could increase the risk of unintended escalation. Finally, used properly,\nthere should be many applications of more autonomous systems in nuclear\noperations that can increase reliability, reduce the risk of accidents, and buy\nmore time for decision-makers in a crisis.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05291v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05238v1",
    "title": "BERT has a Moral Compass: Improvements of ethical and moral values of machines",
    "authors": [
      "Patrick Schramowski",
      "Cigdem Turan",
      "Sophie Jentzsch",
      "Constantin Rothkopf",
      "Kristian Kersting"
    ],
    "author_ids": [],
    "abstract": "Allowing machines to choose whether to kill humans would be devastating for\nworld peace and security. But how do we equip machines with the ability to\nlearn ethical or even moral choices? Jentzsch et al.(2019) showed that applying\nmachine learning to human texts can extract deontological ethical reasoning\nabout \"right\" and \"wrong\" conduct by calculating a moral bias score on a\nsentence level using sentence embeddings. The machine learned that it is\nobjectionable to kill living beings, but it is fine to kill time; It is\nessential to eat, yet one might not eat dirt; it is important to spread\ninformation, yet one should not spread misinformation. However, the evaluated\nmoral bias was restricted to simple actions -- one verb -- and a ranking of\nactions with surrounding context. Recently BERT ---and variants such as RoBERTa\nand SBERT--- has set a new state-of-the-art performance for a wide range of NLP\ntasks. But has BERT also a better moral compass? In this paper, we discuss and\nshow that this is indeed the case. Thus, recent improvements of language\nrepresentations also improve the representation of the underlying ethical and\nmoral values of the machine. We argue that through an advanced semantic\nrepresentation of text, BERT allows one to get better insights of moral and\nethical values implicitly represented in text. This enables the Moral Choice\nMachine (MCM) to extract more accurate imprints of moral choices and ethical\nvalues.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05238v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05164v3",
    "title": "Third-Degree Price Discrimination Versus Uniform Pricing",
    "authors": [
      "Dirk Bergemann",
      "Francisco Castro",
      "Gabriel Weintraub"
    ],
    "author_ids": [],
    "abstract": "We compare the profit of the optimal third-degree price discrimination policy\nagainst a uniform pricing policy. A uniform pricing policy offers the same\nprice to all segments of the market. Our main result establishes that for a\nbroad class of third-degree price discrimination problems with concave profit\nfunctions (in the price space) and common support, a uniform price is\nguaranteed to achieve one half of the optimal monopoly profits. This profit\nbound holds for any number of segments and prices that the seller might use\nunder third-degree price discrimination. We establish that these conditions are\ntight and that weakening either common support or concavity can lead to\narbitrarily poor profit comparisons even for regular or monotone hazard rate\ndistributions.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "econ.GN",
      "cs.GT",
      "econ.TH",
      "q-fin.EC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05164v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.05153v1",
    "title": "Sampling for Bayesian Mixture Models: MCMC with Polynomial-Time Mixing",
    "authors": [
      "Wenlong Mou",
      "Nhat Ho",
      "Martin J. Wainwright",
      "Peter L. Bartlett",
      "Michael I. Jordan"
    ],
    "author_ids": [],
    "abstract": "We study the problem of sampling from the power posterior distribution in\nBayesian Gaussian mixture models, a robust version of the classical posterior.\nThis power posterior is known to be non-log-concave and multi-modal, which\nleads to exponential mixing times for some standard MCMC algorithms. We\nintroduce and study the Reflected Metropolis-Hastings Random Walk (RMRW)\nalgorithm for sampling. For symmetric two-component Gaussian mixtures, we prove\nthat its mixing time is bounded as $d^{1.5}(d + \\Vert \\theta_{0}\n\\Vert^2)^{4.5}$ as long as the sample size $n$ is of the order $d (d + \\Vert\n\\theta_{0} \\Vert^2)$. Notably, this result requires no conditions on the\nseparation of the two means. En route to proving this bound, we establish some\nnew results of possible independent interest that allow for combining\nPoincar\\'{e} inequalities for conditional and marginal densities.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.DS",
      "cs.LG",
      "math.PR",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05153v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/2001.11552v1",
    "title": "Unwanted Advances in Higher Education: Uncovering Sexual Harassment Experiences in Academia with Text Mining",
    "authors": [
      "Amir Karami",
      "Cynthia Nicole White",
      "Kayla Ford",
      "Suzanne Swan",
      "Melek Yildiz Spinel"
    ],
    "author_ids": [],
    "abstract": "Sexual harassment in academia is often a hidden problem because victims are\nusually reluctant to report their experiences. Recently, a web survey was\ndeveloped to provide an opportunity to share thousands of sexual harassment\nexperiences in academia. Using an efficient approach, this study collected and\ninvestigated more than 2,000 sexual harassment experiences to better understand\nthese unwanted advances in higher education. This paper utilized text mining to\ndisclose hidden topics and explore their weight across three variables:\nharasser gender, institution type, and victim's field of study. We mapped the\ntopics on five themes drawn from the sexual harassment literature and found\nthat more than 50% of the topics were assigned to the unwanted sexual attention\ntheme. Fourteen percent of the topics were in the gender harassment theme, in\nwhich insulting, sexist, or degrading comments or behavior was directed towards\nwomen. Five percent of the topics involved sexual coercion (a benefit is\noffered in exchange for sexual favors), 5% involved sex discrimination, and 7%\nof the topics discussed retaliation against the victim for reporting the\nharassment, or for simply not complying with the harasser. Findings highlight\nthe power differential between faculty and students, and the toll on students\nwhen professors abuse their power. While some topics did differ based on type\nof institution, there were no differences between the topics based on gender of\nharasser or field of study. This research can be beneficial to researchers in\nfurther investigation of this paper's dataset, and to policymakers in improving\nexisting policies to create a safe and supportive environment in academia.",
    "published_date": "2019-12-11T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.CL",
      "cs.CY",
      "cs.SI",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2001.11552v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.05538v2",
    "title": "Taking Ethics, Fairness, and Bias Seriously in Machine Learning for Disaster Risk Management",
    "authors": [
      "Robert Soden",
      "Dennis Wagenaar",
      "Dave Luo",
      "Annegien Tijssen"
    ],
    "author_ids": [],
    "abstract": "This paper highlights an important, if under-examined, set of questions about\nthe deployment of machine learning technologies in the field of disaster risk\nmanagement (DRM). While emerging tools show promising capacity to support\nscientific efforts to better understand and mitigate the threats posed by\ndisasters and climate change, our field must undertake a much more careful\nassessment of the potential negative impacts that machine learning technologies\nmay create. We also argue that attention to these issues in the context of\nmachine learning affords the opportunity to have discussions about potential\nethics, bias, and fairness concerns within disaster data more broadly. In what\nfollows, we first describe some of the uses and potential benefits of\nmachine-learning technology in disaster risk management. We then draw on\nresearch from other fields to speculate about potential negative impacts.\nFinally, we outline a research agenda for how our disaster risk management can\nbegin to take these issues seriously and ensure that deployments of\nmachine-learning tools are conducted in a responsible and beneficial manner.",
    "published_date": "2019-12-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05538v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.04883v4",
    "title": "Roles for Computing in Social Change",
    "authors": [
      "Rediet Abebe",
      "Solon Barocas",
      "Jon Kleinberg",
      "Karen Levy",
      "Manish Raghavan",
      "David G. Robinson"
    ],
    "author_ids": [],
    "abstract": "A recent normative turn in computer science has brought concerns about\nfairness, bias, and accountability to the core of the field. Yet recent\nscholarship has warned that much of this technical work treats problematic\nfeatures of the status quo as fixed, and fails to address deeper patterns of\ninjustice and inequality. While acknowledging these critiques, we posit that\ncomputational research has valuable roles to play in addressing social problems\n-- roles whose value can be recognized even from a perspective that aspires\ntoward fundamental social change. In this paper, we articulate four such roles,\nthrough an analysis that considers the opportunities as well as the significant\nrisks inherent in such work. Computing research can serve as a diagnostic,\nhelping us to understand and measure social problems with precision and\nclarity. As a formalizer, computing shapes how social problems are explicitly\ndefined --- changing how those problems, and possible responses to them, are\nunderstood. Computing serves as rebuttal when it illuminates the boundaries of\nwhat is possible through technical means. And computing acts as synecdoche when\nit makes long-standing social problems newly salient in the public eye. We\noffer these paths forward as modalities that leverage the particular strengths\nof computational work in the service of social change, without overclaiming\ncomputing's capacity to solve social problems on its own.",
    "published_date": "2019-12-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.04883v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.04696v2",
    "title": "The Unfairness of Popularity Bias in Music Recommendation: A Reproducibility Study",
    "authors": [
      "Dominik Kowald",
      "Markus Schedl",
      "Elisabeth Lex"
    ],
    "author_ids": [],
    "abstract": "Research has shown that recommender systems are typically biased towards\npopular items, which leads to less popular items being underrepresented in\nrecommendations. The recent work of Abdollahpouri et al. in the context of\nmovie recommendations has shown that this popularity bias leads to unfair\ntreatment of both long-tail items as well as users with little interest in\npopular items. In this paper, we reproduce the analyses of Abdollahpouri et al.\nin the context of music recommendation. Specifically, we investigate three user\ngroups from the LastFM music platform that are categorized based on how much\ntheir listening preferences deviate from the most popular music among all\nLastFM users in the dataset: (i) low-mainstream users, (ii) medium-mainstream\nusers, and (iii) high-mainstream users. In line with Abdollahpouri et al., we\nfind that state-of-the-art recommendation algorithms favor popular items also\nin the music domain. However, their proposed Group Average Popularity metric\nyields different results for LastFM than for the movie domain, presumably due\nto the larger number of available items (i.e., music artists) in the LastFM\ndataset we use. Finally, we compare the accuracy results of the recommendation\nalgorithms for the three user groups and find that the low-mainstreaminess\ngroup significantly receives the worst recommendations.",
    "published_date": "2019-12-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.04696v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.12123v1",
    "title": "Bias Remediation in Driver Drowsiness Detection systems using Generative Adversarial Networks",
    "authors": [
      "Mkhuseli Ngxande",
      "Jules-Raymond Tapamo",
      "Michael Burke"
    ],
    "author_ids": [],
    "abstract": "Datasets are crucial when training a deep neural network. When datasets are\nunrepresentative, trained models are prone to bias because they are unable to\ngeneralise to real world settings. This is particularly problematic for models\ntrained in specific cultural contexts, which may not represent a wide range of\nraces, and thus fail to generalise. This is a particular challenge for Driver\ndrowsiness detection, where many publicly available datasets are\nunrepresentative as they cover only certain ethnicity groups. Traditional\naugmentation methods are unable to improve a model's performance when tested on\nother groups with different facial attributes, and it is often challenging to\nbuild new, more representative datasets. In this paper, we introduce a novel\nframework that boosts the performance of detection of drowsiness for different\nethnicity groups. Our framework improves Convolutional Neural Network (CNN)\ntrained for prediction by using Generative Adversarial networks (GAN) for\ntargeted data augmentation based on a population bias visualisation strategy\nthat groups faces with similar facial attributes and highlights where the model\nis failing. A sampling method selects faces where the model is not performing\nwell, which are used to fine-tune the CNN. Experiments show the efficacy of our\napproach in improving driver drowsiness detection for under represented\nethnicity groups. Here, models trained on publicly available datasets are\ncompared with a model trained using the proposed data augmentation strategy.\nAlthough developed in the context of driver drowsiness detection, the proposed\nframework is not limited to the driver drowsiness detection task, but can be\napplied to other applications.",
    "published_date": "2019-12-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.12123v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.04533v3",
    "title": "Exact expressions for double descent and implicit regularization via surrogate random design",
    "authors": [
      "Michał Dereziński",
      "Feynman Liang",
      "Michael W. Mahoney"
    ],
    "author_ids": [],
    "abstract": "Double descent refers to the phase transition that is exhibited by the\ngeneralization error of unregularized learning models when varying the ratio\nbetween the number of parameters and the number of training samples. The recent\nsuccess of highly over-parameterized machine learning models such as deep\nneural networks has motivated a theoretical analysis of the double descent\nphenomenon in classical models such as linear regression which can also\ngeneralize well in the over-parameterized regime. We provide the first exact\nnon-asymptotic expressions for double descent of the minimum norm linear\nestimator. Our approach involves constructing a special determinantal point\nprocess which we call surrogate random design, to replace the standard i.i.d.\ndesign of the training sample. This surrogate design admits exact expressions\nfor the mean squared error of the estimator while preserving the key properties\nof the standard design. We also establish an exact implicit regularization\nresult for over-parameterized training samples. In particular, we show that,\nfor the surrogate design, the implicit bias of the unregularized minimum norm\nestimator precisely corresponds to solving a ridge-regularized least squares\nproblem on the population distribution. In our analysis we introduce a new\nmathematical tool of independent interest: the class of random matrices for\nwhich determinant commutes with expectation.",
    "published_date": "2019-12-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.04533v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.04177v5",
    "title": "Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation",
    "authors": [
      "Ainesh Bakshi",
      "Nadiia Chepurko",
      "David P. Woodruff"
    ],
    "author_ids": [],
    "abstract": "Recently, Musco and Woodruff (FOCS, 2017) showed that given an $n \\times n$\npositive semidefinite (PSD) matrix $A$, it is possible to compute a\n$(1+\\epsilon)$-approximate relative-error low-rank approximation to $A$ by\nquerying $O(nk/\\epsilon^{2.5})$ entries of $A$ in time $O(nk/\\epsilon^{2.5} +n\nk^{\\omega-1}/\\epsilon^{2(\\omega-1)})$. They also showed that any relative-error\nlow-rank approximation algorithm must query $\\Omega(nk/\\epsilon)$ entries of\n$A$, this gap has since remained open. Our main result is to resolve this\nquestion by obtaining an optimal algorithm that queries $O(nk/\\epsilon)$\nentries of $A$ and outputs a relative-error low-rank approximation in\n$O(n(k/\\epsilon)^{\\omega-1})$ time. Note, our running time improves that of\nMusco and Woodruff, and matches the information-theoretic lower bound if the\nmatrix-multiplication exponent $\\omega$ is $2$.\n  We then extend our techniques to negative-type distance matrices. Bakshi and\nWoodruff (NeurIPS, 2018) showed a bi-criteria, relative-error low-rank\napproximation which queries $O(nk/\\epsilon^{2.5})$ entries and outputs a\nrank-$(k+4)$ matrix. We show that the bi-criteria guarantee is not necessary\nand obtain an $O(nk/\\epsilon)$ query algorithm, which is optimal. Our algorithm\napplies to all distance matrices that arise from metrics satisfying\nnegative-type inequalities, including $\\ell_1, \\ell_2,$ spherical metrics and\nhypermetrics.\n  Next, we introduce a new robust low-rank approximation model which captures\nPSD matrices that have been corrupted with noise. While a sample complexity\nlower bound precludes sublinear algorithms for arbitrary PSD matrices, we\nprovide the first sublinear time and query algorithms when the corruption on\nthe diagonal entries is bounded. As a special case, we show sample-optimal\nsublinear time algorithms for low-rank approximation of correlation matrices\ncorrupted by noise.",
    "published_date": "2019-12-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.04177v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03802v3",
    "title": "Group Fairness in Bandit Arm Selection",
    "authors": [
      "Candice Schumann",
      "Zhi Lang",
      "Nicholas Mattei",
      "John P. Dickerson"
    ],
    "author_ids": [],
    "abstract": "We propose a novel formulation of group fairness with biased feedback in the\ncontextual multi-armed bandit (CMAB) setting. In the CMAB setting, a sequential\ndecision maker must, at each time step, choose an arm to pull from a finite set\nof arms after observing some context for each of the potential arm pulls. In\nour model, arms are partitioned into two or more sensitive groups based on some\nprotected feature(s) (e.g., age, race, or socio-economic status). Initial\nrewards received from pulling an arm may be distorted due to some unknown\nsocietal or measurement bias. We assume that in reality these groups are equal\ndespite the biased feedback received by the agent. To alleviate this, we learn\na societal bias term which can be used to both find the source of bias and to\npotentially fix the problem outside of the algorithm. We provide a novel\nalgorithm that can accommodate this notion of fairness for an arbitrary number\nof groups, and provide a theoretical bound on the regret for our algorithm. We\nvalidate our algorithm using synthetic data and two real-world datasets for\nintervention settings wherein we want to allocate resources fairly across\ngroups.",
    "published_date": "2019-12-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03802v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03729v2",
    "title": "The Classes PPA-$k$: Existence from Arguments Modulo $k$",
    "authors": [
      "Alexandros Hollender"
    ],
    "author_ids": [],
    "abstract": "The complexity classes PPA-$k$, $k \\geq 2$, have recently emerged as the main\ncandidates for capturing the complexity of important problems in fair division,\nin particular Alon's Necklace-Splitting problem with $k$ thieves. Indeed, the\nproblem with two thieves has been shown complete for PPA = PPA-2. In this work,\nwe present structural results which provide a solid foundation for the further\nstudy of these classes. Namely, we investigate the classes PPA-$k$ in terms of\n(i) equivalent definitions, (ii) inner structure, (iii) relationship to each\nother and to other TFNP classes, and (iv) closure under Turing reductions.",
    "published_date": "2019-12-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CC",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03729v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.03727v1",
    "title": "Monotone Submodular Diversity functions for Categorical Vectors with Application to Diversification of Seeds for Targeted Influence Maximization",
    "authors": [
      "Antonio Caliò",
      "Andrea Tagarelli"
    ],
    "author_ids": [],
    "abstract": "Embedding diversity into knowledge discovery tasks is of crucial importance\nto enhance the meaningfulness of the mined patterns with high-impact aspects\nrelated to novelty, serendipity, and ethics. Surprisingly, in the classic\nproblem of influence maximization in social networks, relatively little study\nhas been devoted to diversity and its integration into the objective function\nof an influence maximization method.\n  In this work, we propose the integration of a side-information-based notion\nof seed diversity into the objective function of a targeted influence\nmaximization problem. Starting from the assumption that side-information is\navailable at node level in the general form of categorical attribute values, we\ndesign a class of monotone submodular functions specifically conceived for\ndetermining the diversity within a set of categorical profiles associated with\nthe seeds to be discovered. This allows us to develop an efficient scalable\napproximate method, with a constant-factor guarantee of optimality. More\nprecisely, we formulate the attribute-based diversity-sensitive targeted\ninfluence maximization problem under the state-of-the-art reverse influence\nsampling framework, and we develop a method, dubbed ADITUM, that ensures a\n(1-1/e-\\epsilon)-approximate solution under the general triggering diffusion\nmodel. We experimentally evaluated ADITUM on five real-world networks,\nincluding comparison with methods that exploit numerical-attribute-based\ndiversity and topology-driven diversity in influence maximization.",
    "published_date": "2019-12-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.DS",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03727v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03718v1",
    "title": "Improved Covariance Matrix Estimator using Shrinkage Transformation and Random Matrix Theory",
    "authors": [
      "Samruddhi Deshmukh",
      "Amartansh Dubey"
    ],
    "author_ids": [],
    "abstract": "One of the major challenges in multivariate analysis is the estimation of\npopulation covariance matrix from sample covariance matrix (SCM). Most recent\ncovariance matrix estimators use either shrinkage transformations or asymptotic\nresults from Random Matrix Theory (RMT). Shrinkage techniques help in pulling\nextreme correlation values towards certain target values whereas tools from RMT\nhelp in removing noisy eigenvalues of SCM. Both of these techniques use\ndifferent approaches to achieve a similar goal which is to remove noisy\ncorrelations and add structure to SCM to overcome the bias-variance trade-off.\nIn this paper, we first critically evaluate the pros and cons of these two\ntechniques and then propose an improved estimator which exploits the advantages\nof both by taking an optimally weighted convex combination of covariance\nmatrices estimated by an improved shrinkage transformation and a RMT based\nfilter. It is a generalized estimator which can adapt to changing sampling\nnoise conditions in various datasets by performing hyperparameter optimization.\nWe show the effectiveness of this estimator on the problem of designing a\nfinancial portfolio with minimum risk. We have chosen this problem because the\ncomplex properties of stock market data provide extreme conditions to test the\nrobustness of a covariance estimator. Using data from four of the world's\nlargest stock exchanges, we show that our proposed estimator outperforms\nexisting estimators in minimizing the out-of-sample risk of the portfolio and\nhence predicts population statistics more precisely. Since covariance analysis\nis a crucial statistical tool, this estimator can be used in a wide range of\nmachine learning, signal processing and high dimensional pattern recognition\napplications.",
    "published_date": "2019-12-08T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ME",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03718v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03699v3",
    "title": "Minimum Class Confusion for Versatile Domain Adaptation",
    "authors": [
      "Ying Jin",
      "Ximei Wang",
      "Mingsheng Long",
      "Jianmin Wang"
    ],
    "author_ids": [],
    "abstract": "There are a variety of Domain Adaptation (DA) scenarios subject to label sets\nand domain configurations, including closed-set and partial-set DA, as well as\nmulti-source and multi-target DA. It is notable that existing DA methods are\ngenerally designed only for a specific scenario, and may underperform for\nscenarios they are not tailored to. To this end, this paper studies Versatile\nDomain Adaptation (VDA), where one method can handle several different DA\nscenarios without any modification. Towards this goal, a more general inductive\nbias other than the domain alignment should be explored. We delve into a\nmissing piece of existing methods: class confusion, the tendency that a\nclassifier confuses the predictions between the correct and ambiguous classes\nfor target examples, which is common in different DA scenarios. We uncover that\nreducing such pairwise class confusion leads to significant transfer gains.\nWith this insight, we propose a general loss function: Minimum Class Confusion\n(MCC). It can be characterized as (1) a non-adversarial DA method without\nexplicitly deploying domain alignment, enjoying faster convergence speed; (2) a\nversatile approach that can handle four existing scenarios: Closed-Set,\nPartial-Set, Multi-Source, and Multi-Target DA, outperforming the\nstate-of-the-art methods in these scenarios, especially on one of the largest\nand hardest datasets to date (7.3% on DomainNet). Its versatility is further\njustified by two scenarios proposed in this paper: Multi-Source Partial DA and\nMulti-Target Partial DA. In addition, it can also be used as a general\nregularizer that is orthogonal and complementary to a variety of existing DA\nmethods, accelerating convergence and pushing these readily competitive methods\nto stronger ones. Code is available at\nhttps://github.com/thuml/Versatile-Domain-Adaptation.",
    "published_date": "2019-12-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03699v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03593v1",
    "title": "Towards a Critical Race Methodology in Algorithmic Fairness",
    "authors": [
      "Alex Hanna",
      "Emily Denton",
      "Andrew Smart",
      "Jamila Smith-Loud"
    ],
    "author_ids": [],
    "abstract": "We examine the way race and racial categories are adopted in algorithmic\nfairness frameworks. Current methodologies fail to adequately account for the\nsocially constructed nature of race, instead adopting a conceptualization of\nrace as a fixed attribute. Treating race as an attribute, rather than a\nstructural, institutional, and relational phenomenon, can serve to minimize the\nstructural aspects of algorithmic unfairness. In this work, we focus on the\nhistory of racial categories and turn to critical race theory and sociological\nwork on race and ethnicity to ground conceptualizations of race for fairness\nresearch, drawing on lessons from public health, biomedical research, and\nsocial survey research. We argue that algorithmic fairness researchers need to\ntake into account the multidimensionality of race, take seriously the processes\nof conceptualizing and operationalizing race, focus on social processes which\nproduce racial inequality, and consider perspectives of those most affected by\nsociotechnical systems.",
    "published_date": "2019-12-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03593v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.03523v1",
    "title": "BoPF: Mitigating the Burstiness-Fairness Tradeoff in Multi-Resource Clusters",
    "authors": [
      "Tan N. Le",
      "Xiao Sun",
      "Mosharaf Chowdhury",
      "Zhenhua Liu"
    ],
    "author_ids": [],
    "abstract": "Simultaneously supporting latency- and throughout-sensitive workloads in a\nshared environment is an increasingly more common challenge in big data\nclusters. Despite many advances, existing cluster schedulers force the same\nperformance goal - fairness in most cases - on all jobs. Latency-sensitive jobs\nsuffer, while throughput-sensitive ones thrive. Using prioritization does the\nopposite: it opens up a path for latency-sensitive jobs to dominate. In this\npaper, we tackle the challenges in supporting both short-term performance and\nlong-term fairness simultaneously with high resource utilization by proposing\nBounded Priority Fairness (BoPF). BoPF provides short-term resource guarantees\nto latency-sensitive jobs and maintains long-term fairness for\nthroughput-sensitive jobs. BoPF is the first scheduler that can provide\nlong-term fairness, burst guarantee, and Pareto efficiency in a strategyproof\nmanner for multi-resource scheduling. Deployments and large-scale simulations\nshow that BoPF closely approximates the performance of Strict Priority as well\nas the fairness characteristics of DRF. In deployments, BoPF speeds up\nlatency-sensitive jobs by 5.38 times compared to DRF, while still maintaining\nlong-term fairness. In the meantime, BoPF improves the average completion times\nof throughput-sensitive jobs by up to 3.05 times compared to Strict Priority.",
    "published_date": "2019-12-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03523v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.03455v1",
    "title": "Digital Twin: Acquiring High-Fidelity 3D Avatar from a Single Image",
    "authors": [
      "Ruizhe Wang",
      "Chih-Fan Chen",
      "Hao Peng",
      "Xudong Liu",
      "Oliver Liu",
      "Xin Li"
    ],
    "author_ids": [],
    "abstract": "We present an approach to generate high fidelity 3D face avatar with a\nhigh-resolution UV texture map from a single image. To estimate the face\ngeometry, we use a deep neural network to directly predict vertex coordinates\nof the 3D face model from the given image. The 3D face geometry is further\nrefined by a non-rigid deformation process to more accurately capture facial\nlandmarks before texture projection. A key novelty of our approach is to train\nthe shape regression network on facial images synthetically generated using a\nhigh-quality rendering engine. Moreover, our shape estimator fully leverages\nthe discriminative power of deep facial identity features learned from millions\nof facial images. We have conducted extensive experiments to demonstrate the\nsuperiority of our optimized 2D-to-3D rendering approach, especially its\nexcellent generalization property on real-world selfie images. Our proposed\nsystem of rendering 3D avatars from 2D images has a wide range of applications\nfrom virtual/augmented reality (VR/AR) and telepsychiatry to human-computer\ninteraction and social networks.",
    "published_date": "2019-12-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03455v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03227v1",
    "title": "Self-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning",
    "authors": [
      "Jannik Zürn",
      "Wolfram Burgard",
      "Abhinav Valada"
    ],
    "author_ids": [],
    "abstract": "Mobile robots operating in unknown urban environments encounter a wide range\nof complex terrains to which they must adapt their planned trajectory for safe\nand efficient navigation. Most existing approaches utilize supervised learning\nto classify terrains from either an exteroceptive or a proprioceptive sensor\nmodality. However, this requires a tremendous amount of manual labeling effort\nfor each newly encountered terrain as well as for variations of terrains caused\nby changing environmental conditions. In this work, we propose a novel terrain\nclassification framework leveraging an unsupervised proprioceptive classifier\nthat learns from vehicle-terrain interaction sounds to self-supervise an\nexteroceptive classifier for pixel-wise semantic segmentation of images. To\nthis end, we first learn a discriminative embedding space for vehicle-terrain\ninteraction sounds from triplets of audio clips formed using visual features of\nthe corresponding terrain patches and cluster the resulting embeddings. We\nsubsequently use these clusters to label the visual terrain patches by\nprojecting the traversed tracks of the robot into the camera images. Finally,\nwe use the sparsely labeled images to train our semantic segmentation network\nin a weakly supervised manner. We present extensive quantitative and\nqualitative results that demonstrate that our proprioceptive terrain classifier\nexceeds the state-of-the-art among unsupervised methods and our self-supervised\nexteroceptive semantic segmentation model achieves a comparable performance to\nsupervised learning with manually labeled data.",
    "published_date": "2019-12-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03227v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.07367v1",
    "title": "A Model-driven and Data-driven Fusion Framework for Accurate Air Quality Prediction",
    "authors": [
      "Haolin Fei",
      "Xiaofeng Wu",
      "Chunbo Luo"
    ],
    "author_ids": [],
    "abstract": "Air quality is closely related to public health. Health issues such as\ncardiovascular diseases and respiratory diseases, may have connection with long\nexposure to highly polluted environment. Therefore, accurate air quality\nforecasts are extremely important to those who are vulnerable. To estimate the\nvariation of several air pollution concentrations, previous researchers used\nvarious approaches, such as the Community Multiscale Air Quality model (CMAQ)\nor neural networks. Although CMAQ model considers a coverage of the historic\nair pollution data and meteorological variables, extra bias is introduced due\nto additional adjustment. In this paper, a combination of model-based strategy\nand data-driven method namely the physical-temporal collection(PTC) model is\nproposed, aiming to fix the systematic error that traditional models deliver.\nIn the data-driven part, the first components are the temporal pattern and the\nweather pattern to measure important features that contribute to the prediction\nperformance. The less relevant input variables will be removed to eliminate\nnegative weights in network training. Then, we deploy a long-short-term-memory\n(LSTM) to fetch the preliminary results, which will be further corrected by a\nneural network (NN) involving the meteorological index as well as other\npollutants concentrations. The data-set we applied for forecasting is from\nJanuary 1st, 2016 to December 31st, 2016. According to the results, our PTC\nachieves an excellent performance compared with the baseline model (CMAQ\nprediction, GRU, DNN and etc.). This joint model-based data-driven method for\nair quality prediction can be easily deployed on stations without extra\nadjustment, providing results with high-time-resolution information for\nvulnerable members to prevent heavy air pollution ahead.",
    "published_date": "2019-12-06T00:00:00",
    "year": 2019,
    "categories": [
      "stat.AP",
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.07367v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.02761v2",
    "title": "Measuring Social Bias in Knowledge Graph Embeddings",
    "authors": [
      "Joseph Fisher",
      "Dave Palfrey",
      "Christos Christodoulopoulos",
      "Arpit Mittal"
    ],
    "author_ids": [],
    "abstract": "It has recently been shown that word embeddings encode social biases, with a\nharmful impact on downstream tasks. However, to this point there has been no\nsimilar work done in the field of graph embeddings. We present the first study\non social bias in knowledge graph embeddings, and propose a new metric suitable\nfor measuring such bias. We conduct experiments on Wikidata and Freebase, and\nshow that, as with word embeddings, harmful social biases related to\nprofessions are encoded in the embeddings with respect to gender, religion,\nethnicity and nationality. For example, graph embeddings encode the information\nthat men are more likely to be bankers, and women more likely to be\nhomekeepers. As graph embeddings become increasingly utilized, we suggest that\nit is important the existence of such biases are understood and steps taken to\nmitigate their impact.",
    "published_date": "2019-12-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.02761v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.02585v1",
    "title": "Local Voting: A New Distributed Bandwidth Reservation Algorithm for 6TiSCH Networks",
    "authors": [
      "Dimitrios J. Vergados",
      "Katina Kralevska",
      "Yuming Jiang",
      "Angelos Michalas"
    ],
    "author_ids": [],
    "abstract": "The IETF 6TiSCH working group fosters the adaptation of IPv6-based protocols\ninto Internet of Things by introducing the 6TiSCH Operation Sublayer (6top).\nThe 6TiSCH architecture integrates the high reliability and low-energy\nconsumption of IEEE 802.15.4e Time Slotted Channel Hopping (TSCH) with IPv6.\nIEEE 802.15.4e TSCH defines only the communication between nodes through a\nschedule but it does not specify how the resources are allocated for\ncommunication between the nodes in 6TiSCH networks. We propose a distributed\nalgorithm for bandwidth allocation, called Local Voting, that adapts the\nschedule to the network conditions. The algorithm tries to equalize the link\nload (defined as the ratio of the queue length plus the new packet arrivals,\nover the number of allocated cells) through cell reallocation by calculating\nthe number of cells to be added or released by 6top. Simulation results show\nthat equalizing the load throughout 6TiSCH network provides better fairness in\nterms of load, reduces the queue sizes and packets reach the root faster\ncompared to representative algorithms from the literature. Local Voting\ncombines good delay performance and energy efficiency that are crucial features\nfor Industrial Internet-of-Things applications.",
    "published_date": "2019-12-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.02585v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.02499v2",
    "title": "Perfectly Parallel Fairness Certification of Neural Networks",
    "authors": [
      "Caterina Urban",
      "Maria Christakis",
      "Valentin Wüstholz",
      "Fuyuan Zhang"
    ],
    "author_ids": [],
    "abstract": "Recently, there is growing concern that machine-learning models, which\ncurrently assist or even automate decision making, reproduce, and in the worst\ncase reinforce, bias of the training data. The development of tools and\ntechniques for certifying fairness of these models or describing their biased\nbehavior is, therefore, critical. In this paper, we propose a perfectly\nparallel static analysis for certifying causal fairness of feed-forward neural\nnetworks used for classification of tabular data. When certification succeeds,\nour approach provides definite guarantees, otherwise, it describes and\nquantifies the biased behavior. We design the analysis to be sound, in practice\nalso exact, and configurable in terms of scalability and precision, thereby\nenabling pay-as-you-go certification. We implement our approach in an\nopen-source tool and demonstrate its effectiveness on models trained with\npopular datasets.",
    "published_date": "2019-12-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.PL",
      "cs.CY",
      "cs.LG",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.02499v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.03130v1",
    "title": "Learnt dynamics generalizes across tasks, datasets, and populations",
    "authors": [
      "U. Mahmood",
      "M. M. Rahman",
      "A. Fedorov",
      "Z. Fu",
      "V. D. Calhoun",
      "S. M. Plis"
    ],
    "author_ids": [],
    "abstract": "Differentiating multivariate dynamic signals is a difficult learning problem\nas the feature space may be large yet often only a few training examples are\navailable. Traditional approaches to this problem either proceed from\nhandcrafted features or require large datasets to combat the m >> n problem. In\nthis paper, we show that the source of the problem---signal dynamics---can be\nused to our advantage and noticeably improve classification performance on a\nrange of discrimination tasks when training data is scarce. We demonstrate that\nself-supervised pre-training guided by signal dynamics produces embedding that\ngeneralizes across tasks, datasets, data collection sites, and data\ndistributions. We perform an extensive evaluation of this approach on a range\nof tasks including simulated data, keyword detection problem, and a range of\nfunctional neuroimaging data, where we show that a single embedding learnt on\nhealthy subjects generalizes across a number of disorders, age groups, and\ndatasets.",
    "published_date": "2019-12-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.03130v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.01842v1",
    "title": "Algorithmic Discrimination: Formulation and Exploration in Deep Learning-based Face Biometrics",
    "authors": [
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez",
      "Manuel Cebrian",
      "Nick Obradovich",
      "Iyad Rahwan"
    ],
    "author_ids": [],
    "abstract": "The most popular face recognition benchmarks assume a distribution of\nsubjects without much attention to their demographic attributes. In this work,\nwe perform a comprehensive discrimination-aware experimentation of deep\nlearning-based face recognition. The main aim of this study is focused on a\nbetter understanding of the feature space generated by deep models, and the\nperformance achieved over different demographic groups. We also propose a\ngeneral formulation of algorithmic discrimination with application to face\nbiometrics. The experiments are conducted over the new DiveFace database\ncomposed of 24K identities from six different demographic groups. Two popular\nface recognition models are considered in the experimental framework: ResNet-50\nand VGG-Face. We experimentally show that demographic groups highly represented\nin popular face databases have led to popular pre-trained deep face models\npresenting strong algorithmic discrimination. That discrimination can be\nobserved both qualitatively at the feature space of the deep models and\nquantitatively in large performance differences when applying those models in\ndifferent demographic groups, e.g. for face biometrics.",
    "published_date": "2019-12-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.01842v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.01799v1",
    "title": "Addressing Marketing Bias in Product Recommendations",
    "authors": [
      "Mengting Wan",
      "Jianmo Ni",
      "Rishabh Misra",
      "Julian McAuley"
    ],
    "author_ids": [],
    "abstract": "Modern collaborative filtering algorithms seek to provide personalized\nproduct recommendations by uncovering patterns in consumer-product\ninteractions. However, these interactions can be biased by how the product is\nmarketed, for example due to the selection of a particular human model in a\nproduct image. These correlations may result in the underrepresentation of\nparticular niche markets in the interaction data; for example, a female user\nwho would potentially like motorcycle products may be less likely to interact\nwith them if they are promoted using stereotypically 'male' images.\n  In this paper, we first investigate this correlation between users'\ninteraction feedback and products' marketing images on two real-world\ne-commerce datasets. We further examine the response of several standard\ncollaborative filtering algorithms to the distribution of consumer-product\nmarket segments in the input interaction data, revealing that marketing\nstrategy can be a source of bias for modern recommender systems. In order to\nprotect recommendation performance on underrepresented market segments, we\ndevelop a framework to address this potential marketing bias. Quantitative\nresults demonstrate that the proposed approach significantly improves the\nrecommendation fairness across different market segments, with a negligible\nloss (or better) recommendation accuracy.",
    "published_date": "2019-12-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.01799v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.02094v1",
    "title": "A Step Towards Exposing Bias in Trained Deep Convolutional Neural Network Models",
    "authors": [
      "Daniel Omeiza"
    ],
    "author_ids": [],
    "abstract": "We present Smooth Grad-CAM++, a technique which combines two recent\ntechniques: SMOOTHGRAD and Grad-CAM++. Smooth Grad-CAM++ has the capability of\neither visualizing a layer, subset of feature maps, or subset of neurons within\na feature map at each instance. We experimented with few images, and we\ndiscovered that Smooth Grad-CAM++ produced more visually sharp maps with larger\nnumber of salient pixels highlighted in the given input images when compared\nwith other methods. Smooth Grad-CAM++ will give insight into what our deep CNN\nmodels (including models trained on medical scan or imagery) learn. Hence\ninforming decisions on creating a representative training set.",
    "published_date": "2019-12-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.02094v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.01978v2",
    "title": "FANNet: Formal Analysis of Noise Tolerance, Training Bias and Input Sensitivity in Neural Networks",
    "authors": [
      "Mahum Naseer",
      "Mishal Fatima Minhas",
      "Faiq Khalid",
      "Muhammad Abdullah Hanif",
      "Osman Hasan",
      "Muhammad Shafique"
    ],
    "author_ids": [],
    "abstract": "With a constant improvement in the network architectures and training\nmethodologies, Neural Networks (NNs) are increasingly being deployed in\nreal-world Machine Learning systems. However, despite their impressive\nperformance on \"known inputs\", these NNs can fail absurdly on the \"unseen\ninputs\", especially if these real-time inputs deviate from the training dataset\ndistributions, or contain certain types of input noise. This indicates the low\nnoise tolerance of NNs, which is a major reason for the recent increase of\nadversarial attacks. This is a serious concern, particularly for\nsafety-critical applications, where inaccurate results lead to dire\nconsequences. We propose a novel methodology that leverages model checking for\nthe Formal Analysis of Neural Network (FANNet) under different input noise\nranges. Our methodology allows us to rigorously analyze the noise tolerance of\nNNs, their input node sensitivity, and the effects of training bias on their\nperformance, e.g., in terms of classification accuracy. For evaluation, we use\na feed-forward fully-connected NN architecture trained for the Leukemia\nclassification. Our experimental results show $\\pm 11\\%$ noise tolerance for\nthe given trained network, identify the most sensitive input nodes, and confirm\nthe biasness of the available training dataset.",
    "published_date": "2019-12-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.01978v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.01094v2",
    "title": "Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?",
    "authors": [
      "Avrim Blum",
      "Kevin Stangl"
    ],
    "author_ids": [],
    "abstract": "Multiple fairness constraints have been proposed in the literature, motivated\nby a range of concerns about how demographic groups might be treated unfairly\nby machine learning classifiers. In this work we consider a different\nmotivation; learning from biased training data. We posit several ways in which\ntraining data may be biased, including having a more noisy or negatively biased\nlabeling process on members of a disadvantaged group, or a decreased prevalence\nof positive or negative examples from the disadvantaged group, or both.\n  Given such biased training data, Empirical Risk Minimization (ERM) may\nproduce a classifier that not only is biased but also has suboptimal accuracy\non the true data distribution. We examine the ability of fairness-constrained\nERM to correct this problem. In particular, we find that the Equal Opportunity\nfairness constraint (Hardt, Price, and Srebro 2016) combined with ERM will\nprovably recover the Bayes Optimal Classifier under a range of bias models. We\nalso consider other recovery methods including reweighting the training data,\nEqualized Odds, and Demographic Parity. These theoretical results provide\nadditional motivation for considering fairness interventions even if an actor\ncares primarily about accuracy.",
    "published_date": "2019-12-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.01094v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00898v1",
    "title": "Online information of vaccines: information quality is an ethical responsibility of search engines",
    "authors": [
      "Pietro Ghezzi",
      "Peter G Bannister",
      "Gonzalo Casino",
      "Alessia Catalani",
      "Michel Goldman",
      "Jessica Morley",
      "Marie Neunez",
      "Andreu Prados",
      "Mariarosaria Taddeo",
      "Tania Vanzolini",
      "Luciano Floridi"
    ],
    "author_ids": [],
    "abstract": "The fact that internet companies may record our personal data and track our\nonline behavior for commercial or political purpose has emphasized aspects\nrelated to online privacy. This has also led to the development of search\nengines that promise no tracking and privacy. Search engines also have a major\nrole in spreading low-quality health information such as that of anti-vaccine\nwebsites. This study investigates the relationship between search engines'\napproach to privacy and the scientific quality of the information they return.\nWe analyzed the first 30 webpages returned searching 'vaccines autism' in\nEnglish, Spanish, Italian and French. The results show that alternative search\nengines (Duckduckgo, Ecosia, Qwant, Swisscows and Mojeek) may return more\nanti-vaccine pages (10 to 53 percent) than Google.com (zero). Some localized\nversions of Google, however, returned more anti-vaccine webpages (up to 10\npercent) than Google.com. Our study suggests that designing a search engine\nthat is privacy savvy and avoids issues with filter bubbles that can result\nfrom user tracking is necessary but insufficient; instead, mechanisms should be\ndeveloped to test search engines from the perspective of information quality\n(particularly for health-related webpages), before they can be deemed\ntrustworthy providers of public health information.",
    "published_date": "2019-12-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00898v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00840v3",
    "title": "Distributional Locational Marginal Pricing Based Optimization for Electric Vehicle Charging Management",
    "authors": [
      "Abhishek Tyagi",
      "Ram Bhagat"
    ],
    "author_ids": [],
    "abstract": "Electric power generation, transmission, and distribution systems are\nattracting a large amount of interest from researchers with the development of\nthe smart grid technologies. A smart grid aims at effective control and\nconditioning of the distribution of electricity. Pricing signal based\ndistribution system are seen as one of the novel ways to achieve control in a\nsmart grid. In our work, we propose to use a pricing signal modeled after the\nlocational marginal price in the transmission system to locally provide price\ndata to the users. The formulation and implementation of the distributional\nlocational marginal price (DLMP) are achieved to develop a fair pricing model.\nThe work is further practically implemented to a grid with Electric Vehicles in\naddition to the conventional load. The increasing popularity of EVs because of\ntheir ability to reduce greenhouse gas (GHG) emissions will pose a greater\nchallenge in terms of demand on the power grid. We propose to use the DLMP\nmodeling to alleviate congestion in the power grid and develop an optimal\ncharging schedule for EVs.",
    "published_date": "2019-12-02T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00840v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00651v1",
    "title": "An Investigation of Biases in Web Search Engine Query Suggestions",
    "authors": [
      "Malte Bonart",
      "Anastasiia Samokhina",
      "Gernot Heisenberg",
      "Philipp Schaer"
    ],
    "author_ids": [],
    "abstract": "Survey-based studies suggest that search engines are trusted more than social\nmedia or even traditional news, although cases of false information or\ndefamation are known. In this study, we analyze query suggestion features of\nthree search engines to see if these features introduce some bias into the\nquery and search process that might compromise this trust. We test our approach\non person-related search suggestions by querying the names of politicians from\nthe German Bundestag before the German federal election of 2017.\n  This study introduces a framework to systematically examine and automatically\nanalyze the varieties in different query suggestions for person names offered\nby major search engines. To test our framework, we collected data from the\nGoogle, Bing, and DuckDuckGo query suggestion APIs over a period of four months\nfor 629 different names of German politicians. The suggestions were clustered\nand statistically analyzed with regards to different biases, like gender,\nparty, or age and with regards to the stability of the suggestions over time.",
    "published_date": "2019-12-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00651v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00578v1",
    "title": "Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models",
    "authors": [
      "Shruti Bhargava",
      "David Forsyth"
    ],
    "author_ids": [],
    "abstract": "The task of image captioning implicitly involves gender identification.\nHowever, due to the gender bias in data, gender identification by an image\ncaptioning model suffers. Also, the gender-activity bias, owing to the\nword-by-word prediction, influences other words in the caption prediction,\nresulting in the well-known problem of label bias. In this work, we investigate\ngender bias in the COCO captioning dataset and show that it engenders not only\nfrom the statistical distribution of genders with contexts but also from the\nflawed annotation by the human annotators. We look at the issues created by\nthis bias in the trained models. We propose a technique to get rid of the bias\nby splitting the task into 2 subtasks: gender-neutral image captioning and\ngender classification. By this decoupling, the gender-context influence can be\neradicated. We train the gender-neutral image captioning model, which gives\ncomparable results to a gendered model even when evaluating against a dataset\nthat possesses a similar bias as the training data. Interestingly, the\npredictions by this model on images with no humans, are also visibly different\nfrom the one trained on gendered captions. We train gender classifiers using\nthe available bounding box and mask-based annotations for the person in the\nimage. This allows us to get rid of the context and focus on the person to\npredict the gender. By substituting the genders into the gender-neutral\ncaptions, we get the final gendered predictions. Our predictions achieve\nsimilar performance to a model trained with gender, and at the same time are\ndevoid of gender bias. Finally, our main result is that on an\nanti-stereotypical dataset, our model outperforms a popular image captioning\nmodel which is trained with gender.",
    "published_date": "2019-12-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00578v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00385v4",
    "title": "The Group Loss for Deep Metric Learning",
    "authors": [
      "Ismail Elezi",
      "Sebastiano Vascon",
      "Alessandro Torcinovich",
      "Marcello Pelillo",
      "Laura Leal-Taixe"
    ],
    "author_ids": [],
    "abstract": "Deep metric learning has yielded impressive results in tasks such as\nclustering and image retrieval by leveraging neural networks to obtain highly\ndiscriminative feature embeddings, which can be used to group samples into\ndifferent classes. Much research has been devoted to the design of smart loss\nfunctions or data mining strategies for training such networks. Most methods\nconsider only pairs or triplets of samples within a mini-batch to compute the\nloss function, which is commonly based on the distance between embeddings. We\npropose Group Loss, a loss function based on a differentiable label-propagation\nmethod that enforces embedding similarity across all samples of a group while\npromoting, at the same time, low-density regions amongst data points belonging\nto different groups. Guided by the smoothness assumption that \"similar objects\nshould belong to the same group\", the proposed loss trains the neural network\nfor a classification task, enforcing a consistent labelling amongst samples\nwithin a class. We show state-of-the-art results on clustering and image\nretrieval on several datasets, and show the potential of our method when\ncombined with other techniques such as ensembles",
    "published_date": "2019-12-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00385v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.01439v3",
    "title": "Generalization Error Bounds Via Rényi-, $f$-Divergences and Maximal Leakage",
    "authors": [
      "Amedeo Roberto Esposito",
      "Michael Gastpar",
      "Ibrahim Issa"
    ],
    "author_ids": [],
    "abstract": "In this work, the probability of an event under some joint distribution is\nbounded by measuring it with the product of the marginals instead (which is\ntypically easier to analyze) together with a measure of the dependence between\nthe two random variables. These results find applications in adaptive data\nanalysis, where multiple dependencies are introduced and in learning theory,\nwhere they can be employed to bound the generalization error of a learning\nalgorithm. Bounds are given in terms of Sibson's Mutual Information,\n$\\alpha-$Divergences, Hellinger Divergences, and $f-$Divergences. A case of\nparticular interest is the Maximal Leakage (or Sibson's Mutual Information of\norder infinity), since this measure is robust to post-processing and composes\nadaptively. The corresponding bound can be seen as a generalization of\nclassical bounds, such as Hoeffding's and McDiarmid's inequalities, to the case\nof dependent random variables.",
    "published_date": "2019-12-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.01439v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00320v4",
    "title": "Discriminative Joint Probability Maximum Mean Discrepancy (DJP-MMD) for Domain Adaptation",
    "authors": [
      "Wen Zhang",
      "Dongrui Wu"
    ],
    "author_ids": [],
    "abstract": "Maximum mean discrepancy (MMD) has been widely adopted in domain adaptation\nto measure the discrepancy between the source and target domain distributions.\nMany existing domain adaptation approaches are based on the joint MMD, which is\ncomputed as the (weighted) sum of the marginal distribution discrepancy and the\nconditional distribution discrepancy; however, a more natural metric may be\ntheir joint probability distribution discrepancy. Additionally, most metrics\nonly aim to increase the transferability between domains, but ignores the\ndiscriminability between different classes, which may result in insufficient\nclassification performance. To address these issues, discriminative joint\nprobability MMD (DJP-MMD) is proposed in this paper to replace the\nfrequently-used joint MMD in domain adaptation. It has two desirable\nproperties: 1) it provides a new theoretical basis for computing the\ndistribution discrepancy, which is simpler and more accurate; 2) it increases\nthe transferability and discriminability simultaneously. We validate its\nperformance by embedding it into a joint probability domain adaptation\nframework. Experiments on six image classification datasets demonstrated that\nthe proposed DJP-MMD can outperform traditional MMDs.",
    "published_date": "2019-12-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00320v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00264v1",
    "title": "Towards Efficient Integration of Blockchain for IoT Security: The Case Study of IoT Remote Access",
    "authors": [
      "Chenglong Fu",
      "Qiang Zeng",
      "Xiaojiang Du"
    ],
    "author_ids": [],
    "abstract": "The booming Internet of Things (IoT) market has drawn tremendous interest\nfrom cyber attackers. The centralized cloud-based IoT service architecture has\nserious limitations in terms of security, availability, and scalability, and is\nsubject to single points of failure (SPOF). Recently, accommodating IoT\nservices on blockchains has become a trend for better security, privacy, and\nreliability. However, blockchain's shortcomings of high cost, low throughput,\nand long latency make it unsuitable for IoT applications. In this paper, we\ntake a retrospection of existing blockchain-based IoT solutions and propose a\nframework for efficient blockchain and IoT integration. Following the\nframework, we design a novel blockchain-assisted decentralized IoT remote\naccessing system, RS-IoT, which has the advantage of defending IoT devices\nagainst zero-day attacks without relying on any trusted third-party. By\nintroducing incentives and penalties enforced by smart contracts, our work\nenables \"an economic approach\" to thwarting the majority of attackers who aim\nto achieve monetary gains. Our work presents an example of how blockchain can\nbe used to ensure the fairness of service trading in a decentralized\nenvironment and punish misbehaviors objectively. We show the security of RS-IoT\nvia detailed security analyses. Finally, we demonstrate its scalability,\nefficiency, and usability through a proof-of-concept implementation on the\nEthereum testnet blockchain.",
    "published_date": "2019-11-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00264v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00220v1",
    "title": "Energy Efficiency Optimization: A New Trade-off Between Fairness and Total System Performance",
    "authors": [
      "Christos N. Efrem",
      "Athanasios D. Panagopoulos"
    ],
    "author_ids": [],
    "abstract": "The total energy efficiency (TEE), defined as the ratio between the total\ndata rate and the total power consumption, is considered the most meaningful\nperformance metric in terms of energy efficiency (EE). Nevertheless, it does\nnot depend directly on the EE of each link and its maximization leads to\nunfairness between the links. On the other hand, the maximization of the\nminimum EE (MEE), i.e., the minimum of the EEs of all links, guarantees the\nfairest power allocation, but it does not contain any explicit information\nabout the total system performance. The main trend in current research is to\nmaximize TEE and MEE separately. Unlike previous contributions, this letter\npresents a general multi-objective approach for EE optimization that takes into\naccount both TEE and MEE at the same time, and thus achieves various trade-off\npoints in the MEE-TEE plane. Due to the nonconvex form of the resulting\nproblem, we propose a low-complexity algorithm leveraging the theory of\nsequential convex optimization (SCO). Last but not least, we provide a novel\ntheoretical result for the complexity of SCO algorithms.",
    "published_date": "2019-11-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00220v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00177v2",
    "title": "Urban Driving with Conditional Imitation Learning",
    "authors": [
      "Jeffrey Hawke",
      "Richard Shen",
      "Corina Gurau",
      "Siddharth Sharma",
      "Daniele Reda",
      "Nikolay Nikolov",
      "Przemyslaw Mazur",
      "Sean Micklethwaite",
      "Nicolas Griffiths",
      "Amar Shah",
      "Alex Kendall"
    ],
    "author_ids": [],
    "abstract": "Hand-crafting generalised decision-making rules for real-world urban\nautonomous driving is hard. Alternatively, learning behaviour from\neasy-to-collect human driving demonstrations is appealing. Prior work has\nstudied imitation learning (IL) for autonomous driving with a number of\nlimitations. Examples include only performing lane-following rather than\nfollowing a user-defined route, only using a single camera view or heavily\ncropped frames lacking state observability, only lateral (steering) control,\nbut not longitudinal (speed) control and a lack of interaction with traffic.\nImportantly, the majority of such systems have been primarily evaluated in\nsimulation - a simple domain, which lacks real-world complexities. Motivated by\nthese challenges, we focus on learning representations of semantics, geometry\nand motion with computer vision for IL from human driving demonstrations. As\nour main contribution, we present an end-to-end conditional imitation learning\napproach, combining both lateral and longitudinal control on a real vehicle for\nfollowing urban routes with simple traffic. We address inherent dataset bias by\ndata balancing, training our final policy on approximately 30 hours of\ndemonstrations gathered over six months. We evaluate our method on an\nautonomous vehicle by driving 35km of novel routes in European urban streets.",
    "published_date": "2019-11-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00177v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00142v1",
    "title": "Fooling the Crowd with Deep Learning-based Methods",
    "authors": [
      "Christian Marzahl",
      "Marc Aubreville",
      "Christof A. Bertram",
      "Stefan Gerlach",
      "Jennifer Maier",
      "Jörn Voigt",
      "Jenny Hill",
      "Robert Klopfleisch",
      "Andreas Maier"
    ],
    "author_ids": [],
    "abstract": "Modern, state-of-the-art deep learning approaches yield human like\nperformance in numerous object detection and classification tasks. The\nfoundation for their success is the availability of training datasets of\nsubstantially high quantity, which are expensive to create, especially in the\nfield of medical imaging. Recently, crowdsourcing has been applied to create\nlarge datasets for a broad range of disciplines. This study aims to explore the\nchallenges and opportunities of crowd-algorithm collaboration for the object\ndetection task of grading cytology whole slide images. We compared the\nclassical crowdsourcing performance of twenty participants with their results\nfrom crowd-algorithm collaboration. All participants performed both modes in\nrandom order on the same twenty images. Additionally, we introduced artificial\nsystematic flaws into the precomputed annotations to estimate a bias towards\naccepting precomputed annotations. We gathered 9524 annotations on 800 images\nfrom twenty participants organised into four groups in concordance to their\nlevel of expertise with cytology. The crowd-algorithm mode improved on average\nthe participants' classification accuracy by 7%, the mean average precision by\n8% and the inter-observer Fleiss' kappa score by 20%, and reduced the time\nspent by 31%. However, two thirds of the artificially modified false labels\nwere not recognised as such by the contributors. This study shows that\ncrowd-algorithm collaboration is a promising new approach to generate large\ndatasets when it is ensured that a carefully designed setup eliminates\npotential biases.",
    "published_date": "2019-11-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00142v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12990v3",
    "title": "Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization",
    "authors": [
      "Jung Hyun Lee",
      "Jihun Yun",
      "Sung Ju Hwang",
      "Eunho Yang"
    ],
    "author_ids": [],
    "abstract": "Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged as one of the key ingredients to reduce\nthe size of neural networks for their deployments to resource-limited devices.\nIn order to overcome the nature of transforming continuous activations and\nweights to discrete ones, recent study called Relaxed Quantization (RQ)\n[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that\nallows this transformation with efficient gradient-based optimization. However,\nRQ with this Gumbel-Softmax relaxation still suffers from bias-variance\ntrade-off depending on the temperature parameter of Gumbel-Softmax. To resolve\nthe issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses\nmulti-class straight-through estimator to effectively reduce the bias and\nvariance, along with a new regularization technique, DropBits that replaces\ndropout regularization to randomly drop the bits instead of neurons to further\nreduce the bias of the multi-class straight-through estimator in SRQ. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer\nusing DropBits. We experimentally validate our method on various benchmark\ndatasets and network architectures, and also support the quantized lottery\nticket hypothesis: learning heterogeneous quantization levels outperforms the\ncase using the same but fixed quantization levels from scratch.",
    "published_date": "2019-11-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12990v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12780v2",
    "title": "Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers",
    "authors": [
      "Colin Paterson",
      "Radu Calinescu",
      "Chiara Picardi"
    ],
    "author_ids": [],
    "abstract": "Regions of high-dimensional input spaces that are underrepresented in\ntraining datasets reduce machine-learnt classifier performance, and may lead to\ncorner cases and unwanted bias for classifiers used in decision making systems.\nWhen these regions belong to otherwise well-represented classes, their presence\nand negative impact are very hard to identify. We propose an approach for the\ndetection and mitigation of such rare subclasses in deep neural network\nclassifiers. The new approach is underpinned by an easy-to-compute commonality\nmetric that supports the detection of rare subclasses, and comprises methods\nfor reducing the impact of these subclasses during both model training and\nmodel exploitation. We demonstrate our approach using two well-known datasets,\nMNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses\nand producing models which compensate for subclass rarity. In addition we\ndemonstrate how our run-time approach increases the ability of users to\nidentify samples likely to be misclassified at run-time.",
    "published_date": "2019-11-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12780v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12587v1",
    "title": "FairPrep: Promoting Data to a First-Class Citizen in Studies on Fairness-Enhancing Interventions",
    "authors": [
      "Sebastian Schelter",
      "Yuxuan He",
      "Jatin Khilnani",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "The importance of incorporating ethics and legal compliance into\nmachine-assisted decision-making is broadly recognized. Further, several lines\nof recent work have argued that critical opportunities for improving data\nquality and representativeness, controlling for bias, and allowing humans to\noversee and impact computational processes are missed if we do not consider the\nlifecycle stages upstream from model training and deployment. Yet, very little\nhas been done to date to provide system-level support to data scientists who\nwish to develop and deploy responsible machine learning methods. We aim to fill\nthis gap and present FairPrep, a design and evaluation framework for\nfairness-enhancing interventions.\n  FairPrep is based on a developer-centered design, and helps data scientists\nfollow best practices in software engineering and machine learning. As part of\nour contribution, we identify shortcomings in existing empirical studies for\nanalyzing fairness-enhancing interventions. We then show how FairPrep can be\nused to measure the impact of sound best practices, such as hyperparameter\ntuning and feature scaling. In particular, our results suggest that the high\nvariability of the outcomes of fairness-enhancing interventions observed in\nprevious studies is often an artifact of a lack of hyperparameter tuning.\nFurther, we show that the choice of a data cleaning method can impact the\neffectiveness of fairness-enhancing interventions.",
    "published_date": "2019-11-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DB",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12587v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12558v1",
    "title": "Addressing Time Bias in Bipartite Graph Ranking for Important Node Identification",
    "authors": [
      "Hao Liao",
      "Jiao Wu",
      "Mingyang Zhou",
      "Alexandre Vidmer"
    ],
    "author_ids": [],
    "abstract": "The goal of the ranking problem in networks is to rank nodes from best to\nworst, according to a chosen criterion. In this work, we focus on ranking the\nnodes according to their quality. The problem of ranking the nodes in bipartite\nnetworks is valuable for many real-world applications. For instance,\nhigh-quality products can be promoted on an online shop or highly reputed\nrestaurants attract more people on venues review platforms. However, many\nclassical ranking algorithms share a common drawback: they tend to rank older\nmovies higher than newer movies, though some newer movies may have a high\nquality. This time bias originates from the fact that older nodes in a network\ntend to have more connections than newer ones. In the study, we develop a\nranking method using a rebalance approach to diminish the time bias of the\nrankings in bipartite graphs.",
    "published_date": "2019-11-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12275v1",
    "title": "Fooling with facts: Quantifying anchoring bias through a large-scale online experiment",
    "authors": [
      "Taha Yasseri",
      "Jannie Reher"
    ],
    "author_ids": [],
    "abstract": "Living in the 'Information Age' means that not only access to information has\nbecome easier but also that the distribution of information is more dynamic\nthan ever. Through a large-scale online field experiment, we provide new\nempirical evidence for the presence of the anchoring bias in people's judgment\ndue to irrational reliance on a piece of information that they are initially\ngiven. The comparison of the anchoring stimuli and respective responses across\ndifferent tasks reveals a positive, yet complex relationship between the\nanchors and the bias in participants' predictions of the outcomes of events in\nthe future. Participants in the treatment group were equally susceptible to the\nanchors regardless of their level of engagement, previous performance, or\ngender. Given the strong and ubiquitous influence of anchors quantified here,\nwe should take great care to closely monitor and regulate the distribution of\ninformation online to facilitate less biased decision making.",
    "published_date": "2019-11-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12275v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00782v2",
    "title": "The relationship between trust in AI and trustworthy machine learning technologies",
    "authors": [
      "Ehsan Toreini",
      "Mhairi Aitken",
      "Kovila Coopamootoo",
      "Karen Elliott",
      "Carlos Gonzalez Zelaya",
      "Aad van Moorsel"
    ],
    "author_ids": [],
    "abstract": "To build AI-based systems that users and the public can justifiably trust one\nneeds to understand how machine learning technologies impact trust put in these\nservices. To guide technology developments, this paper provides a systematic\napproach to relate social science concepts of trust with the technologies used\nin AI-based services and products. We conceive trust as discussed in the ABI\n(Ability, Benevolence, Integrity) framework and use a recently proposed mapping\nof ABI on qualities of technologies. We consider four categories of machine\nlearning technologies, namely these for Fairness, Explainability, Auditability\nand Safety (FEAS) and discuss if and how these possess the required qualities.\nTrust can be impacted throughout the life cycle of AI-based systems, and we\nintroduce the concept of Chain of Trust to discuss technological needs for\ntrust in different stages of the life cycle. FEAS has obvious relations with\nknown frameworks and therefore we relate FEAS to a variety of international\nPrincipled AI policy and technology frameworks that have emerged in recent\nyears.",
    "published_date": "2019-11-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00782v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12116v1",
    "title": "Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey",
    "authors": [
      "Vanessa Buhrmester",
      "David Münch",
      "Michael Arens"
    ],
    "author_ids": [],
    "abstract": "Deep Learning is a state-of-the-art technique to make inference on extensive\nor complex data. As a black box model due to their multilayer nonlinear\nstructure, Deep Neural Networks are often criticized to be non-transparent and\ntheir predictions not traceable by humans. Furthermore, the models learn from\nartificial datasets, often with bias or contaminated discriminating content.\nThrough their increased distribution, decision-making algorithms can contribute\npromoting prejudge and unfairness which is not easy to notice due to lack of\ntransparency. Hence, scientists developed several so-called explanators or\nexplainers which try to point out the connection between input and output to\nrepresent in a simplified way the inner structure of machine learning black\nboxes. In this survey we differ the mechanisms and properties of explaining\nsystems for Deep Neural Networks for Computer Vision tasks. We give a\ncomprehensive overview about taxonomy of related studies and compare several\nsurvey papers that deal with explainability in general. We work out the\ndrawbacks and gaps and summarize further research ideas.",
    "published_date": "2019-11-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12116v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11981v2",
    "title": "Class-Conditional Domain Adaptation on Semantic Segmentation",
    "authors": [
      "Yue Wang",
      "Yuke Li",
      "James H. Elder",
      "Runmin Wu",
      "Huchuan Lu"
    ],
    "author_ids": [],
    "abstract": "Semantic segmentation is an important sub-task for many applications, but\npixel-level ground truth labeling is costly and there is a tendency to overfit\nthe training data, limiting generalization. Unsupervised domain adaptation can\npotentially address these problems, allowing systems trained on labelled\ndatasets from one or more source domains (including less expensive synthetic\ndomains) to be adapted to novel target domains. The conventional approach is to\nautomatically align the representational distributions of source and target\ndomains. One limitation of this approach is that it tends to disadvantage lower\nprobability classes. We address this problem by introducing a Class-Conditional\nDomain Adaptation method (CCDA). It includes a class-conditional multi-scale\ndiscriminator and the class-conditional loss. This novel CCDA method encourages\nthe network to shift the domain in a class-conditional manner, and it equalizes\nloss over classes. We evaluate our CCDA method on two transfer tasks and\ndemonstrate performance comparable to state-of-the-art methods.",
    "published_date": "2019-11-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11981v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.13248v1",
    "title": "To Trust, or Not to Trust? A Study of Human Bias in Automated Video Interview Assessments",
    "authors": [
      "Chee Wee Leong",
      "Katrina Roohr",
      "Vikram Ramanarayanan",
      "Michelle P. Martin-Raugh",
      "Harrison Kell",
      "Rutuja Ubale",
      "Yao Qian",
      "Zydrune Mladineo",
      "Laura McCulla"
    ],
    "author_ids": [],
    "abstract": "Supervised systems require human labels for training. But, are humans\nthemselves always impartial during the annotation process? We examine this\nquestion in the context of automated assessment of human behavioral tasks.\nSpecifically, we investigate whether human ratings themselves can be trusted at\ntheir face value when scoring video-based structured interviews, and whether\nsuch ratings can impact machine learning models that use them as training data.\nWe present preliminary empirical evidence that indicates there might be biases\nin such annotations, most of which are visual in nature.",
    "published_date": "2019-11-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.LG",
      "I.2.0; H.1.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.13248v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11936v2",
    "title": "Generalizing Complex Hypotheses on Product Distributions: Auctions, Prophet Inequalities, and Pandora's Problem",
    "authors": [
      "Chenghao Guo",
      "Zhiyi Huang",
      "Zhihao Gavin Tang",
      "Xinzhi Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper explores a theory of generalization for learning problems on\nproduct distributions, complementing the existing learning theories in the\nsense that it does not rely on any complexity measures of the hypothesis\nclasses. The main contributions are two general sample complexity bounds: (1)\n$\\tilde{O} \\big( \\frac{nk}{\\epsilon^2} \\big)$ samples are sufficient and\nnecessary for learning an $\\epsilon$-optimal hypothesis in any problem on an\n$n$-dimensional product distribution, whose marginals have finite supports of\nsizes at most $k$; (2) $\\tilde{O} \\big( \\frac{n}{\\epsilon^2} \\big)$ samples are\nsufficient and necessary for any problem on $n$-dimensional product\ndistributions if it satisfies a notion of strong monotonicity from the\nalgorithmic game theory literature. As applications of these theories, we match\nthe optimal sample complexity for single-parameter revenue maximization (Guo et\nal., STOC 2019), improve the state-of-the-art for multi-parameter revenue\nmaximization (Gonczarowski and Weinberg, FOCS 2018) and prophet inequality\n(Correa et al., EC 2019), and provide the first and tight sample complexity\nbound for Pandora's problem.",
    "published_date": "2019-11-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11936v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11834v2",
    "title": "Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation",
    "authors": [
      "Zeyu Wang",
      "Klint Qinami",
      "Ioannis Christos Karakozis",
      "Kyle Genova",
      "Prem Nair",
      "Kenji Hata",
      "Olga Russakovsky"
    ],
    "author_ids": [],
    "abstract": "Computer vision models learn to perform a task by capturing relevant\nstatistics from training data. It has been shown that models learn spurious\nage, gender, and race correlations when trained for seemingly unrelated tasks\nlike activity recognition or image captioning. Various mitigation techniques\nhave been presented to prevent models from utilizing or learning such biases.\nHowever, there has been little systematic comparison between these techniques.\nWe design a simple but surprisingly effective visual recognition benchmark for\nstudying bias mitigation. Using this benchmark, we provide a thorough analysis\nof a wide range of techniques. We highlight the shortcomings of popular\nadversarial training approaches for bias mitigation, propose a simple but\nsimilarly effective alternative to the inference-time Reducing Bias\nAmplification method of Zhao et al., and design a domain-independent training\ntechnique that outperforms all other methods. Finally, we validate our findings\non the attribute classification task in the CelebA dataset, where attribute\npresence is known to be correlated with the gender of people in the image, and\ndemonstrate that the proposed technique is effective at mitigating real-world\ngender bias.",
    "published_date": "2019-11-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11834v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11641v1",
    "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
    "authors": [
      "Yonatan Bisk",
      "Rowan Zellers",
      "Ronan Le Bras",
      "Jianfeng Gao",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a\ntoothpick? Questions requiring this kind of physical commonsense pose a\nchallenge to today's natural language understanding systems. While recent\npretrained models (such as BERT) have made progress on question answering over\nmore abstract domains - such as news articles and encyclopedia entries, where\ntext is plentiful - in more physical domains, text is inherently limited due to\nreporting bias. Can AI systems learn to reliably answer physical common-sense\nquestions without experiencing the physical world? In this paper, we introduce\nthe task of physical commonsense reasoning and a corresponding benchmark\ndataset Physical Interaction: Question Answering or PIQA. Though humans find\nthe dataset easy (95% accuracy), large pretrained models struggle (77%). We\nprovide analysis about the dimensions of knowledge that existing models lack,\nwhich offers significant opportunities for future research.",
    "published_date": "2019-11-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11641v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00783v1",
    "title": "Consider ethical and social challenges in smart grid research",
    "authors": [
      "Valentin Robu",
      "David Flynn",
      "Merlinda Andoni",
      "Maizura Mokhtar"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence and Machine Learning are increasingly seen as key\ntechnologies for building more decentralised and resilient energy grids, but\nresearchers must consider the ethical and social implications of their use",
    "published_date": "2019-11-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00783v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11426v1",
    "title": "A finite-volume scheme for a cross-diffusion model arising from interacting many-particle population systems",
    "authors": [
      "Ansgar Jüngel",
      "Antoine Zurek"
    ],
    "author_ids": [],
    "abstract": "A finite-volume scheme for a cross-diffusion model arising from the\nmean-field limit of an interacting particle system for multiple population\nspecies is studied. The existence of discrete solutions and a discrete entropy\nproduction inequality is proved. The proof is based on a weighted quadratic\nentropy that is not the sum of the entropies of the population species.",
    "published_date": "2019-11-26T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA",
      "35K51, 35K55, 35Q92, 65M08"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11426v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.00833v1",
    "title": "Mis-classified Vector Guided Softmax Loss for Face Recognition",
    "authors": [
      "Xiaobo Wang",
      "Shifeng Zhang",
      "Shuo Wang",
      "Tianyu Fu",
      "Hailin Shi",
      "Tao Mei"
    ],
    "author_ids": [],
    "abstract": "Face recognition has witnessed significant progress due to the advances of\ndeep convolutional neural networks (CNNs), the central task of which is how to\nimprove the feature discrimination. To this end, several margin-based\n(\\textit{e.g.}, angular, additive and additive angular margins) softmax loss\nfunctions have been proposed to increase the feature margin between different\nclasses. However, despite great achievements have been made, they mainly suffer\nfrom three issues: 1) Obviously, they ignore the importance of informative\nfeatures mining for discriminative learning; 2) They encourage the feature\nmargin only from the ground truth class, without realizing the discriminability\nfrom other non-ground truth classes; 3) The feature margin between different\nclasses is set to be same and fixed, which may not adapt the situations very\nwell. To cope with these issues, this paper develops a novel loss function,\nwhich adaptively emphasizes the mis-classified feature vectors to guide the\ndiscriminative feature learning. Thus we can address all the above issues and\nachieve more discriminative face features. To the best of our knowledge, this\nis the first attempt to inherit the advantages of feature margin and feature\nmining into a unified loss function. Experimental results on several benchmarks\nhave demonstrated the effectiveness of our method over state-of-the-art\nalternatives.",
    "published_date": "2019-11-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00833v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.12303v1",
    "title": "Automated Peer-to-peer Negotiation for Energy Contract Settlements in Residential Cooperatives",
    "authors": [
      "Shantanu Chakraborty",
      "Tim Baarslag",
      "Michael Kaisers"
    ],
    "author_ids": [],
    "abstract": "This paper presents an automated peer-to-peer negotiation strategy for\nsettling energy contracts among prosumers in a Residential Energy Cooperative\nconsidering heterogeneity prosumer preferences. The heterogeneity arises from\nprosumers' evaluation of energy contracts through multiple societal and\nenvironmental criteria and the prosumers' private preferences over those\ncriteria. The prosumers engage in bilateral negotiations with peers to mutually\nagree on periodical energy contracts/loans consisting of the energy volume to\nbe exchanged at that period and the return time of the exchanged energy. The\nnegotiating prosumers navigate through a common negotiation domain consisting\nof potential energy contracts and evaluate those contracts from their\nvaluations on the entailed criteria against a utility function that is robust\nagainst generation and demand uncertainty. From the repeated interactions, a\nprosumer gradually learns about the compatibility of its peers in reaching\nenergy contracts that are closer to Nash solutions. Empirical evaluation on\nreal demand, generation and storage profiles -- in multiple system scales --\nillustrates that the proposed negotiation based strategy can increase the\nsystem efficiency (measured by utilitarian social welfare) and fairness\n(measured by Nash social welfare) over a baseline strategy and an individual\nflexibility control strategy representing the status quo strategy. We thus\nelicit system benefits from peer-to-peer flexibility exchange already without\nany central coordination and market operator, providing a simple yet flexible\nand effective paradigm that complements existing markets.",
    "published_date": "2019-11-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.12303v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11240v1",
    "title": "My Approach = Your Apparatus? Entropy-Based Topic Modeling on Multiple Domain-Specific Text Collections",
    "authors": [
      "Julian Risch",
      "Ralf Krestel"
    ],
    "author_ids": [],
    "abstract": "Comparative text mining extends from genre analysis and political bias\ndetection to the revelation of cultural and geographic differences, through to\nthe search for prior art across patents and scientific papers. These\napplications use cross-collection topic modeling for the exploration,\nclustering, and comparison of large sets of documents, such as digital\nlibraries. However, topic modeling on documents from different collections is\nchallenging because of domain-specific vocabulary. We present a\ncross-collection topic model combined with automatic domain term extraction and\nphrase segmentation. This model distinguishes collection-specific and\ncollection-independent words based on information entropy and reveals\ncommonalities and differences of multiple text collections. We evaluate our\nmodel on patents, scientific papers, newspaper articles, forum posts, and\nWikipedia articles. In comparison to state-of-the-art cross-collection topic\nmodeling, our model achieves up to 13% higher topic coherence, up to 4% lower\nperplexity, and up to 31% higher document classification accuracy. More\nimportantly, our approach is the first topic model that ensures disjunct\ngeneral and specific word distributions, resulting in clear-cut topic\nrepresentations.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11240v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.00761v1",
    "title": "On the Legal Compatibility of Fairness Definitions",
    "authors": [
      "Alice Xiang",
      "Inioluwa Deborah Raji"
    ],
    "author_ids": [],
    "abstract": "Past literature has been effective in demonstrating ideological gaps in\nmachine learning (ML) fairness definitions when considering their use in\ncomplex socio-technical systems. However, we go further to demonstrate that\nthese definitions often misunderstand the legal concepts from which they\npurport to be inspired, and consequently inappropriately co-opt legal language.\nIn this paper, we demonstrate examples of this misalignment and discuss the\ndifferences in ML terminology and their legal counterparts, as well as what\nboth the legal and ML fairness communities can learn from these tensions. We\nfocus this paper on U.S. anti-discrimination law since the ML fairness research\ncommunity regularly references terms from this body of law.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.00761v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.01959v1",
    "title": "When Autonomous Intelligent Goodware will Fight Autonomous Intelligent Malware: A Possible Future of Cyber Defense",
    "authors": [
      "Paul Théron",
      "Alexander Kott"
    ],
    "author_ids": [],
    "abstract": "In the coming years, the future of military combat will include, on one hand,\nartificial intelligence-optimized complex command, control, communications,\ncomputers, intelligence, surveillance and reconnaissance (C4ISR) and networks\nand, on the other hand, autonomous intelligent Things fighting autonomous\nintelligent Things at a fast pace. Under this perspective, enemy forces will\nseek to disable or disturb our autonomous Things and our complex\ninfrastructures and systems. Autonomy, scale and complexity in our defense\nsystems will trigger new cyber-attack strategies, and autonomous intelligent\nmalware (AIM) will be part of the picture. Should these cyber-attacks succeed\nwhile human operators remain unaware or unable to react fast enough due to the\nspeed, scale or complexity of the mission, systems or attacks, missions would\nfail, our networks and C4ISR would be heavily disrupted, and command and\ncontrol would be disabled. New cyber-defense doctrines and technologies are\ntherefore required. Autonomous cyber defense (ACyD) is a new field of research\nand technology driven by the defense sector in anticipation of such threats to\nfuture military infrastructures, systems and operations. It will be implemented\nvia swarms of autonomous intelligent cyber-defense agents (AICAs) that will\nfight AIM within our networks and systems. This paper presents this\ncyber-defense technology of the future, the current state of the art in this\nfield and its main challenges. First, we review the rationale of the ACyD\nconcept and its associated AICA technology. Then, we present the current\nresearch results from NATO's IST-152 Research Task Group on the AICA Reference\nArchitecture. We then develop the 12 main technological challenges that must be\nresolved in the coming years, besides ethical and political issues.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.01959v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11099v1",
    "title": "Polyhedral study of the Convex Recoloring problem",
    "authors": [
      "Manoel Campêlo",
      "Phablo F. S. Moura",
      "Joel C. Soares"
    ],
    "author_ids": [],
    "abstract": "A coloring of the vertices of a connected graph is convex if each color class\ninduces a connected subgraph. We address the convex recoloring (CR) problem\ndefined as follows. Given a graph $G$ and a coloring of its vertices, recolor a\nminimum number of vertices of $G$ so that the resulting coloring is convex.\nThis problem, known to be NP-hard even on paths, was firstly motivated by\napplications on perfect phylogenies. In this work, we study CR on general\ngraphs from a polyhedral point of view. First, we introduce a full-dimensional\npolytope based on the idea of connected subgraphs, and present a class of valid\ninequalities with righthand side one that comprises all facet-defining\ninequalities with binary coefficients when the input graph is a tree. Moreover,\nwe define a general class of inequalities with righthand side in $\\{1, \\ldots,\nk\\}$, where $k$ is the amount of colors used in the initial coloring, and show\nsufficient conditions for validity and facetness of such inequalities. Finally,\nwe report on computational experiments for an application on mobile networks\nthat can be modeled by the polytope of CR on paths. We evaluate the potential\nof the proposed inequalities to reduce the integrality gaps.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11099v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.11088v1",
    "title": "First Year Computer Science Projects at Coventry University: Activity-led integrative team projects with continuous assessment",
    "authors": [
      "Simon Billings",
      "Matthew England"
    ],
    "author_ids": [],
    "abstract": "We describe the group projects undertaken by first year undergraduate\nComputer Science students at Coventry University. These are integrative course\nprojects: designed to bring together the topics from the various modules\nstudents take, to apply them as a coherent whole. They follow an activity-led\napproach, with students given a loose brief and a lot of freedom in how to\ndevelop their project.\n  We outline the new regulations at Coventry University which eases the use of\nsuch integrative projects. We then describe our continuous assessment approach:\nwhere students earn a weekly mark by demonstrating progress to a teacher as an\nopen presentation to the class. It involves a degree of self and peer\nassessment and allows for an assessment of group work that is both fair, and\nseen to be fair. It builds attendance, self-study / continuous engagement\nhabits, public speaking / presentation skills, and rewards group members for\nmaking meaningful individual contributions.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "K.3.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11088v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.11025v1",
    "title": "Women, politics and Twitter: Using machine learning to change the discourse",
    "authors": [
      "Lana Cuthbertson",
      "Alex Kearney",
      "Riley Dawson",
      "Ashia Zawaduk",
      "Eve Cuthbertson",
      "Ann Gordon-Tighe",
      "Kory W Mathewson"
    ],
    "author_ids": [],
    "abstract": "Including diverse voices in political decision-making strengthens our\ndemocratic institutions. Within the Canadian political system, there is gender\ninequality across all levels of elected government. Online abuse, such as\nhateful tweets, leveled at women engaged in politics contributes to this\ninequity, particularly tweets focusing on their gender. In this paper, we\npresent ParityBOT: a Twitter bot which counters abusive tweets aimed at women\nin politics by sending supportive tweets about influential female leaders and\nfacts about women in public life. ParityBOT is the first artificial\nintelligence-based intervention aimed at affecting online discourse for women\nin politics for the better. The goal of this project is to: $1$) raise\nawareness of issues relating to gender inequity in politics, and $2$)\npositively influence public discourse in politics. The main contribution of\nthis paper is a scalable model to classify and respond to hateful tweets with\nquantitative and qualitative assessments. The ParityBOT abusive classification\nsystem was validated on public online harassment datasets. We conclude with\nanalysis of the impact of ParityBOT, drawing from data gathered during\ninterventions in both the $2019$ Alberta provincial and $2019$ Canadian federal\nelections.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11025v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11018v1",
    "title": "A Self-Adaptive Synthetic Over-Sampling Technique for Imbalanced Classification",
    "authors": [
      "Xiaowei Gu",
      "Plamen P Angelov",
      "Eduardo Almeida Soares"
    ],
    "author_ids": [],
    "abstract": "Traditionally, in supervised machine learning, (a significant) part of the\navailable data (usually 50% to 80%) is used for training and the rest for\nvalidation. In many problems, however, the data is highly imbalanced in regard\nto different classes or does not have good coverage of the feasible data space\nwhich, in turn, creates problems in validation and usage phase. In this paper,\nwe propose a technique for synthesising feasible and likely data to help\nbalance the classes as well as to boost the performance in terms of confusion\nmatrix as well as overall. The idea, in a nutshell, is to synthesise data\nsamples in close vicinity to the actual data samples specifically for the less\nrepresented (minority) classes. This has also implications to the so-called\nfairness of machine learning. In this paper, we propose a specific method for\nsynthesising data in a way to balance the classes and boost the performance,\nespecially of the minority classes. It is generic and can be applied to\ndifferent base algorithms, e.g. support vector machine, k-nearest neighbour,\ndeep networks, rule-based classifiers, decision trees, etc. The results\ndemonstrated that: i) a significantly more balanced (and fair) classification\nresults can be achieved; ii) that the overall performance as well as the\nperformance per class measured by confusion matrix can be boosted. In addition,\nthis approach can be very valuable for the cases when the number of actual\navailable labelled data is small which itself is one of the problems of the\ncontemporary machine learning.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11018v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10922v3",
    "title": "Towards Better Understanding of Disentangled Representations via Mutual Information",
    "authors": [
      "Xiaojiang Yang",
      "Wendong Bi",
      "Yitong Sun",
      "Yu Cheng",
      "Junchi Yan"
    ],
    "author_ids": [],
    "abstract": "Most existing works on disentangled representation learning are solely built\nupon an marginal independence assumption: all factors in disentangled\nrepresentations should be statistically independent. This assumption is\nnecessary but definitely not sufficient for the disentangled representations\nwithout additional inductive biases in the modeling process, which is shown\ntheoretically in recent studies. We argue in this work that disentangled\nrepresentations should be characterized by their relation with observable data.\nIn particular, we formulate such a relation through the concept of mutual\ninformation: the mutual information between each factor of the disentangled\nrepresentations and data should be invariant conditioned on values of the other\nfactors. Together with the widely accepted independence assumption, we further\nbridge it with the conditional independence of factors in representations\nconditioned on data. Moreover, we note that conditional independence of latent\nvariables has been imposed on most VAE-type models and InfoGAN due to the\nartificial choice of factorized approximate posterior $q(\\rvz|\\rvx)$ in the\nencoders. Such an arrangement of encoders introduces a crucial inductive bias\nfor disentangled representations. To demonstrate the importance of our proposed\nassumption and the related inductive bias, we show in experiments that\nviolating the assumption leads to decline of disentanglement among factors in\nthe learned representations.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10922v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.11558v1",
    "title": "FairyTED: A Fair Rating Predictor for TED Talk Data",
    "authors": [
      "Rupam Acharyya",
      "Shouman Das",
      "Ankani Chattoraj",
      "Md. Iftekhar Tanveer"
    ],
    "author_ids": [],
    "abstract": "With the recent trend of applying machine learning in every aspect of human\nlife, it is important to incorporate fairness into the core of the predictive\nalgorithms. We address the problem of predicting the quality of public speeches\nwhile being fair with respect to sensitive attributes of the speakers, e.g.\ngender and race. We use the TED talks as an input repository of public speeches\nbecause it consists of speakers from a diverse community and has a wide\noutreach. Utilizing the theories of Causal Models, Counterfactual Fairness and\nstate-of-the-art neural language models, we propose a mathematical framework\nfor fair prediction of the public speaking quality. We employ grounded\nassumptions to construct a causal model capturing how different attributes\naffect public speaking quality. This causal model contributes in generating\ncounterfactual data to train a fair predictive model. Our framework is general\nenough to utilize any assumption within the causal model. Experimental results\nshow that while prediction accuracy is comparable to recent work on this\ndataset, our predictions are counterfactually fair with respect to a novel\nmetric when compared to true data labels. The FairyTED setup not only allows\norganizers to make informed and diverse selection of speakers from the\nunobserved counterfactual possibilities but it also ensures that viewers and\nnew users are not influenced by unfair and unbalanced ratings from arbitrary\nvisitors to the www.ted.com website when deciding to view a talk.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10787v1",
    "title": "A Causal Inference Method for Reducing Gender Bias in Word Embedding Relations",
    "authors": [
      "Zekun Yang",
      "Juan Feng"
    ],
    "author_ids": [],
    "abstract": "Word embedding has become essential for natural language processing as it\nboosts empirical performances of various tasks. However, recent research\ndiscovers that gender bias is incorporated in neural word embeddings, and\ndownstream tasks that rely on these biased word vectors also produce\ngender-biased results. While some word-embedding gender-debiasing methods have\nbeen developed, these methods mainly focus on reducing gender bias associated\nwith gender direction and fail to reduce the gender bias presented in word\nembedding relations. In this paper, we design a causal and simple approach for\nmitigating gender bias in word vector relation by utilizing the statistical\ndependency between gender-definition word embeddings and gender-biased word\nembeddings. Our method attains state-of-the-art results on gender-debiasing\ntasks, lexical- and sentence-level evaluation tasks, and downstream coreference\nresolution tasks.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10787v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10720v3",
    "title": "Non-parametric Uni-modality Constraints for Deep Ordinal Classification",
    "authors": [
      "Soufiane Belharbi",
      "Ismail Ben Ayed",
      "Luke McCaffrey",
      "Eric Granger"
    ],
    "author_ids": [],
    "abstract": "We propose a new constrained-optimization formulation for deep ordinal\nclassification, in which uni-modality of the label distribution is enforced\nimplicitly via a set of inequality constraints over all the pairs of adjacent\nlabels. Based on (c-1) constraints for c labels, our model is non-parametric\nand, therefore, more flexible than the existing deep ordinal classification\ntechniques. Unlike these, it does not restrict the learned representation to a\nsingle and specific parametric model (or penalty) imposed on all the labels.\nTherefore, it enables the training to explore larger spaces of solutions, while\nremoving the need for ad hoc choices and scaling up to large numbers of labels.\nIt can be used in conjunction with any standard classification loss and any\ndeep architecture. To tackle the ensuing challenging optimization problem, we\nsolve a sequence of unconstrained losses based on a powerful extension of the\nlog-barrier method.\n  This handles effectively competing constraints and accommodates standard SGD\nfor deep networks, while avoiding computationally expensive Lagrangian dual\nsteps and outperforming substantially penalty methods. Furthermore, we propose\na new performance metric for ordinal classification, as a proxy to measure\ndistribution uni-modality, referred to as the Sides Order Index (SOI). We\nreport comprehensive evaluations and comparisons to state-of-the-art methods on\nbenchmark public datasets for several ordinal classification tasks, showing the\nmerits of our approach in terms of label consistency, classification accuracy\nand scalability. Importantly, enforcing label consistency with our model does\nnot incur higher classification errors, unlike many existing ordinal\nclassification methods. A public reproducible PyTorch implementation is\nprovided.\n(https://github.com/sbelharbi/unimodal-prob-deep-oc-free-distribution)",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10720v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10713v4",
    "title": "Prototype Rectification for Few-Shot Learning",
    "authors": [
      "Jinlu Liu",
      "Liang Song",
      "Yongqiang Qin"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning requires to recognize novel classes with scarce labeled\ndata. Prototypical network is useful in existing researches, however, training\non narrow-size distribution of scarce data usually tends to get biased\nprototypes. In this paper, we figure out two key influencing factors of the\nprocess: the intra-class bias and the cross-class bias. We then propose a\nsimple yet effective approach for prototype rectification in transductive\nsetting. The approach utilizes label propagation to diminish the intra-class\nbias and feature shifting to diminish the cross-class bias. We also conduct\ntheoretical analysis to derive its rationality as well as the lower bound of\nthe performance. Effectiveness is shown on three few-shot benchmarks. Notably,\nour approach achieves state-of-the-art performance on both miniImageNet (70.31%\non 1-shot and 81.89% on 5-shot) and tieredImageNet (78.74% on 1-shot and 86.92%\non 5-shot).",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10713v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10692v1",
    "title": "Mitigate Bias in Face Recognition using Skewness-Aware Reinforcement Learning",
    "authors": [
      "Mei Wang",
      "Weihong Deng"
    ],
    "author_ids": [],
    "abstract": "Racial equality is an important theme of international human rights law, but\nit has been largely obscured when the overall face recognition accuracy is\npursued blindly. More facts indicate racial bias indeed degrades the fairness\nof recognition system and the error rates on non-Caucasians are usually much\nhigher than Caucasians. To encourage fairness, we introduce the idea of\nadaptive margin to learn balanced performance for different races based on\nlarge margin losses. A reinforcement learning based race balance network\n(RL-RBN) is proposed. We formulate the process of finding the optimal margins\nfor non-Caucasians as a Markov decision process and employ deep Q-learning to\nlearn policies for an agent to select appropriate margin by approximating the\nQ-value function. Guided by the agent, the skewness of feature scatter between\nraces can be reduced. Besides, we provide two ethnicity aware training\ndatasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be\nutilized to study racial bias from both data and algorithm aspects. Extensive\nexperiments on RFW database show that RL-RBN successfully mitigates racial bias\nand learns more balanced performance for different races.",
    "published_date": "2019-11-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10692v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10640v1",
    "title": "Algorithmic Bias in Recidivism Prediction: A Causal Perspective",
    "authors": [
      "Aria Khademi",
      "Vasant Honavar"
    ],
    "author_ids": [],
    "abstract": "ProPublica's analysis of recidivism predictions produced by Correctional\nOffender Management Profiling for Alternative Sanctions (COMPAS) software tool\nfor the task, has shown that the predictions were racially biased against\nAfrican American defendants. We analyze the COMPAS data using a causal\nreformulation of the underlying algorithmic fairness problem. Specifically, we\nassess whether COMPAS exhibits racial bias against African American defendants\nusing FACT, a recently introduced causality grounded measure of algorithmic\nfairness. We use the Neyman-Rubin potential outcomes framework for causal\ninference from observational data to estimate FACT from COMPAS data. Our\nanalysis offers strong evidence that COMPAS exhibits racial bias against\nAfrican American defendants. We further show that the FACT estimates from\nCOMPAS data are robust in the presence of unmeasured confounding.",
    "published_date": "2019-11-24T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10640v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10527v3",
    "title": "Merging Deterministic Policy Gradient Estimations with Varied Bias-Variance Tradeoff for Effective Deep Reinforcement Learning",
    "authors": [
      "Gang Chen"
    ],
    "author_ids": [],
    "abstract": "Deep reinforcement learning (DRL) on Markov decision processes (MDPs) with\ncontinuous action spaces is often approached by directly training parametric\npolicies along the direction of estimated policy gradients (PGs). Previous\nresearch revealed that the performance of these PG algorithms depends heavily\non the bias-variance tradeoffs involved in estimating and using PGs. A notable\napproach towards balancing this tradeoff is to merge both on-policy and\noff-policy gradient estimations. However existing PG merging methods can be\nsample inefficient and are not suitable to train deterministic policies\ndirectly. To address these issues, this paper introduces elite PGs and\nstrengthens their variance reduction effect by adopting elitism and policy\nconsolidation techniques to regularize policy training based on policy\nbehavioral knowledge extracted from elite trajectories. Meanwhile, we propose a\ntwo-step method to merge elite PGs and conventional PGs as a new extension of\nthe conventional interpolation merging method. At both the theoretical and\nexperimental levels, we show that both two-step merging and interpolation\nmerging can induce varied bias-variance tradeoffs during policy training. They\nenable us to effectively use elite PGs and mitigate their performance impact on\ntrained policies. Our experiments also show that two-step merging can\noutperform interpolation merging and several state-of-the-art algorithms on six\nbenchmark control tasks.",
    "published_date": "2019-11-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10527v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10496v2",
    "title": "Two Causal Principles for Improving Visual Dialog",
    "authors": [
      "Jiaxin Qi",
      "Yulei Niu",
      "Jianqiang Huang",
      "Hanwang Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper unravels the design tricks adopted by us, the champion team\nMReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for\nimproving Visual Dialog (VisDial). By \"improving\", we mean that they can\npromote almost every existing VisDial model to the state-of-the-art performance\non the leader-board. Such a major improvement is only due to our careful\ninspection on the causality behind the model and data, finding that the\ncommunity has overlooked two causalities in VisDial. Intuitively, Principle 1\nsuggests: we should remove the direct input of the dialog history to the answer\nmodel, otherwise a harmful shortcut bias will be introduced; Principle 2 says:\nthere is an unobserved confounder for history, question, and answer, leading to\nspurious correlations from training data. In particular, to remove the\nconfounder suggested in Principle 2, we propose several causal intervention\nalgorithms, which make the training fundamentally different from the\ntraditional likelihood estimation. Note that the two principles are\nmodel-agnostic, so they are applicable in any VisDial model. The code is\navailable at https://github.com/simpleshinobu/visdial-principles.",
    "published_date": "2019-11-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10496v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.10433v1",
    "title": "Empowering Artists, Songwriters & Musicians in a Data Cooperative through Blockchains and Smart Contracts",
    "authors": [
      "Thomas Hardjono",
      "Alex Pentland"
    ],
    "author_ids": [],
    "abstract": "Over the last decade there has been a continuing decline in social trust on\nthe part of individuals with regards to the handling and fair use of personal\ndata, digital assets and other related rights in general. At the same time,\nthere has been a change in the employment patterns for many people through the\nemergence of the gig economy. These gig workers include artists, songwriters\nand musicians in the music industry. We discuss the notion of the data\ncooperative with fiduciary responsibilities to its members, which is similar in\npurpose to credit unions in the financial sector. A data cooperative for\nartists and musicians allows the community to share IT resources, such as data\nstorage, analytics processing, blockchains and distributed ledgers. A\ncooperative can also employ smart contracts to remedy the various challenges\ncurrently faced by the music industry with regards to the license tracking\nmanagement.",
    "published_date": "2019-11-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10433v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.10059v1",
    "title": "Deriving star cluster parameters with convolutional neural networks. II. Extinction and cluster/background classification",
    "authors": [
      "J. Bialopetravičius",
      "D. Narbutis"
    ],
    "author_ids": [],
    "abstract": "Context. Convolutional neural networks (CNNs) have been established as the\ngo-to method for fast object detection and classification on natural images.\nThis opens the door for astrophysical parameter inference on the exponentially\nincreasing amount of sky survey data. Until now, star cluster analysis was\nbased on integral or resolved stellar photometry, which limits the amount of\ninformation that can be extracted from individual pixels of cluster images.\n  Aims. We aim to create a CNN capable of inferring star cluster evolutionary,\nstructural, and environmental parameters from multi-band images, as well to\ndemonstrate its capabilities in discriminating genuine clusters from galactic\nstellar backgrounds.\n  Methods. A CNN based on the deep residual network (ResNet) architecture was\ncreated and trained to infer cluster ages, masses, sizes, and extinctions, with\nrespect to the degeneracies between them. Mock clusters placed on M83 Hubble\nSpace Telescope (HST) images utilizing three photometric passbands (F336W,\nF438W, and F814W) were used. The CNN is also capable of predicting the\nlikelihood of a cluster's presence in an image, as well as quantifying its\nvisibility (signal-to-noise).\n  Results. The CNN was tested on mock images of artificial clusters and has\ndemonstrated reliable inference results for clusters of ages $\\lesssim$100 Myr,\nextinctions $A_V$ between 0 and 3 mag, masses between $3\\times10^3$ and\n$3\\times10^5$ ${\\rm M_\\odot}$, and sizes between 0.04 and 0.4 arcsec at the\ndistance of the M83 galaxy. Real M83 galaxy cluster parameter inference tests\nwere performed with objects taken from previous studies and have demonstrated\nconsistent results.",
    "published_date": "2019-11-22T00:00:00",
    "year": 2019,
    "categories": [
      "astro-ph.GA",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10059v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.09992v1",
    "title": "Competitive Equilibrium with Generic Budgets: Beyond Additive",
    "authors": [
      "Moshe Babaioff",
      "Noam Nisan",
      "Inbal Talgam-Cohen"
    ],
    "author_ids": [],
    "abstract": "We study competitive equilibrium in the canonical Fisher market model, but\nwith indivisible goods. In this model, every agent has a budget of artificial\ncurrency with which to purchase bundles of goods. Equilibrium prices match\nbetween demand and supply---at such prices, all agents simultaneously get their\nfavorite within-budget bundle, and the market clears. Unfortunately, a\ncompetitive equilibrium may not exist when the goods are indivisible, even in\nextremely simple markets such as two agents with exactly the same budget and a\nsingle item. Yet in this example, once the budgets are slightly\nperturbed---i.e., made generic---a competitive equilibrium is guaranteed to\nexist. In this paper we explore the extent to which generic budgets can\nguarantee equilibrium existence (and thus related fairness guarantees) in\nmarkets with multiple items. We complement our results in [Babaioff et al.,\n2019] for additive preferences by exploring the case of general monotone\npreferences, establishing positive results for small numbers of items and\nmapping the limits of our approach. We then consider cardinal preferences,\ndefine a hierarchy of such preference classes and establish relations among\nthem, and for some classes prove equilibrium existence under generic budgets.",
    "published_date": "2019-11-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.09992v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.11555v1",
    "title": "Fair Multi-party Machine Learning -- a Game Theoretic approach",
    "authors": [
      "Zhiliang Chen"
    ],
    "author_ids": [],
    "abstract": "High performance machine learning models have become highly dependent on the\navailability of large quantity and quality of training data. To achieve this,\nvarious central agencies such as the government have suggested for different\ndata providers to pool their data together to learn a unified predictive model,\nwhich performs better. However, these providers are usually profit-driven and\nwould only agree to participate inthe data sharing process if the process is\ndeemed both profitable and fair for themselves. Due to the lack of existing\nliterature, it is unclear whether a fair and stable outcome is possible in such\ndata sharing processes. Hence, we wish to investigate the outcomes surrounding\nthese scenarios and study if data providers would even agree to collaborate in\nthe first place. Tapping on cooperative game concepts in Game Theory, we\nintroduce the data sharing process between a group of agents as a new class of\ncooperative games with modified definition of stability and fairness. Using\nthese new definitions, we then theoretically study the optimal and suboptimal\noutcomes of such data sharing processes and their sensitivity to\nperturbation.Through experiments, we present intuitive insights regarding\ntheoretical results analysed in this paper and discuss various ways in which\ndata can be valued reasonably.",
    "published_date": "2019-11-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.11555v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.09709v3",
    "title": "Automatically Neutralizing Subjective Bias in Text",
    "authors": [
      "Reid Pryzant",
      "Richard Diehl Martinez",
      "Nathan Dass",
      "Sadao Kurohashi",
      "Dan Jurafsky",
      "Diyi Yang"
    ],
    "author_ids": [],
    "abstract": "Texts like news, encyclopedias, and some social media strive for objectivity.\nYet bias in the form of inappropriate subjectivity - introducing attitudes via\nframing, presupposing truth, and casting doubt - remains ubiquitous. This kind\nof bias erodes our collective trust and fuels social conflict. To address this\nissue, we introduce a novel testbed for natural language generation:\nautomatically bringing inappropriately subjective text into a neutral point of\nview (\"neutralizing\" biased text). We also offer the first parallel corpus of\nbiased language. The corpus contains 180,000 sentence pairs and originates from\nWikipedia edits that removed various framings, presuppositions, and attitudes\nfrom biased sentences. Last, we propose two strong encoder-decoder baselines\nfor the task. A straightforward yet opaque CONCURRENT system uses a BERT\nencoder to identify subjective words as part of the generation process. An\ninterpretable and controllable MODULAR algorithm separates these steps, using\n(1) a BERT-based classifier to identify problematic words and (2) a novel join\nembedding through which the classifier can edit the hidden states of the\nencoder. Large-scale human evaluation across four domains (encyclopedias, news\nheadlines, books, and political speeches) suggests that these algorithms are a\nfirst step towards the automatic identification and reduction of bias.",
    "published_date": "2019-11-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.09709v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.09602v2",
    "title": "Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech",
    "authors": [
      "David Harwath",
      "Wei-Ning Hsu",
      "James Glass"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a method for learning discrete linguistic units by\nincorporating vector quantization layers into neural models of visually\ngrounded speech. We show that our method is capable of capturing both\nword-level and sub-word units, depending on how it is configured. What\ndifferentiates this paper from prior work on speech unit learning is the choice\nof training objective. Rather than using a reconstruction-based loss, we use a\ndiscriminative, multimodal grounding objective which forces the learned units\nto be useful for semantic image retrieval. We evaluate the sub-word units on\nthe ZeroSpeech 2019 challenge, achieving a 27.3\\% reduction in ABX error rate\nover the top-performing submission, while keeping the bitrate approximately the\nsame. We also present experiments demonstrating the noise robustness of these\nunits. Finally, we show that a model with multiple quantizers can\nsimultaneously learn phone-like detectors at a lower layer and word-like\ndetectors at a higher layer. We show that these detectors are highly accurate,\ndiscovering 279 words with an F1 score of greater than 0.5.",
    "published_date": "2019-11-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.09602v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.09402v2",
    "title": "Max-Min Fair Precoder Design and Power Allocation for MU-MIMO NOMA",
    "authors": [
      "Ahmet Zahid Yalcin",
      "Mustafa Kagan Cetin",
      "Melda Yuksel"
    ],
    "author_ids": [],
    "abstract": "In this paper, a downlink multiple input multiple output (MIMO)\nnon-orthogonal multiple access (NOMA) wireless communication system is\nconsidered. In NOMA systems, the base station has unicast data for all users,\nand multiple users in a group share the same resources. The objective is to\ndesign transmit precoders and power allocation coefficients jointly that\nprovide max-min fairness (MMF) among the strongest users in each group, while\nmaintaining minimum target rates for all the other users. The problem is solved\nvia two main iterative approaches. The first method is based on semi-definite\nrelaxation (SDR) and successive convex approximation (SCA), and the second\nmethod is based on the equivalency between achievable rate and minimum mean\nsquare error (MMSE) expressions. For the latter approach, Karush-Kuhn-Tucker\n(KKT) optimality conditions are derived and the expressions satisfied by the\noptimal receivers, MMSE weights and the optimal precoders are obtained.\nProposed algorithms are compared with rate-splitting (RS), orthogonal multiple\naccess (OMA) and multi-user linear precoding (MULP) schemes in terms of MMF\nrates, energy efficiency and complexity. It is shown that while RS has the best\nMMF rates and energy efficiency, the MMSE approach based on KKT optimality\nconditions has the least complexity. Moreover, the SDR/SCA approach offers an\nexcellent tradeoff. It offers high MMF rates, low complexity and superior\nenergy efficiency.",
    "published_date": "2019-11-21T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.09402v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.10571v2",
    "title": "Efficient Estimation of Equilibria in Large Aggregative Games with Coupling Constraints",
    "authors": [
      "Paulin Jacquot",
      "Cheng Wan",
      "Olivier Beaude",
      "Nadia Oudjane"
    ],
    "author_ids": [],
    "abstract": "Aggregative games have many industrial applications, and computing an\nequilibrium in those games is challenging when the number of players is large.\nIn the framework of atomic aggregative games with coupling constraints, we show\nthat variational Nash equilibria of a large aggregative game can be\napproximated by a Wardrop equilibrium of an auxiliary population game of\nsmaller dimension. Each population of this auxiliary game corresponds to a\ngroup of atomic players of the initial large game. This approach enables an\nefficient computation of an approximated equilibrium, as the variational\ninequality characterizing the Wardrop equilibrium is of smaller dimension than\nthe initial one. This is illustrated on an example in the smart grid context.",
    "published_date": "2019-11-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.10571v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.09071v3",
    "title": "The Origins and Prevalence of Texture Bias in Convolutional Neural Networks",
    "authors": [
      "Katherine L. Hermann",
      "Ting Chen",
      "Simon Kornblith"
    ],
    "author_ids": [],
    "abstract": "Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to\nclassify images by texture rather than by shape. How pervasive is this bias,\nand where does it come from? We find that, when trained on datasets of images\nwith conflicting shape and texture, CNNs learn to classify by shape at least as\neasily as by texture. What factors, then, produce the texture bias in CNNs\ntrained on ImageNet? Different unsupervised training objectives and different\narchitectures have small but significant and largely independent effects on the\nlevel of texture bias. However, all objectives and architectures still lead to\nmodels that make texture-based classification decisions a majority of the time,\neven if shape information is decodable from their hidden representations. The\neffect of data augmentation is much larger. By taking less aggressive random\ncrops at training time and applying simple, naturalistic augmentation (color\ndistortion, noise, and blur), we train models that classify ambiguous images by\nshape a majority of the time, and outperform baselines on out-of-distribution\ntest sets. Our results indicate that apparent differences in the way humans and\nImageNet-trained CNNs process images may arise not primarily from differences\nin their internal workings, but from differences in the data that they see.",
    "published_date": "2019-11-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.09071v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08966v1",
    "title": "Evaluating the Transferability and Adversarial Discrimination of Convolutional Neural Networks for Threat Object Detection and Classification within X-Ray Security Imagery",
    "authors": [
      "Yona Falinie A. Gaus",
      "Neelanjan Bhowmik",
      "Samet Akcay",
      "Toby P. Breckon"
    ],
    "author_ids": [],
    "abstract": "X-ray imagery security screening is essential to maintaining transport\nsecurity against a varying profile of threat or prohibited items. Particular\ninterest lies in the automatic detection and classification of weapons such as\nfirearms and knives within complex and cluttered X-ray security imagery. Here,\nwe address this problem by exploring various end-to-end object detection\nConvolutional Neural Network (CNN) architectures. We evaluate several leading\nvariants spanning the Faster R-CNN, Mask R-CNN, and RetinaNet architectures to\nexplore the transferability of such models between varying X-ray scanners with\ndiffering imaging geometries, image resolutions and material colour profiles.\nWhilst the limited availability of X-ray threat imagery can pose a challenge,\nwe employ a transfer learning approach to evaluate whether such inter-scanner\ngeneralisation may exist over a multiple class detection problem. Overall, we\nachieve maximal detection performance using a Faster R-CNN architecture with a\nResNet$_{101}$ classification network, obtaining 0.88 and 0.86 of mean Average\nPrecision (mAP) for a three-class and two class item from varying X-ray imaging\nsources. Our results exhibit a remarkable degree of generalisability in terms\nof cross-scanner performance (mAP: 0.87, firearm detection: 0.94 AP). In\naddition, we examine the inherent adversarial discriminative capability of such\nnetworks using a specifically generated adversarial dataset for firearms\ndetection - with a variable low false positive, as low as 5%, this shows both\nthe challenge and promise of such threat detection within X-ray security\nimagery.",
    "published_date": "2019-11-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08966v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08925v3",
    "title": "Multi-Group Multicast Beamforming: Optimal Structure and Efficient Algorithms",
    "authors": [
      "Min Dong",
      "Qiqi Wang"
    ],
    "author_ids": [],
    "abstract": "This paper considers the multi-group multicast beamforming optimization\nproblem, for which the optimal solution has been unknown due to the non-convex\nand NP-hard nature of the problem. By utilizing the successive convex\napproximation numerical method and Lagrangian duality, we obtain the optimal\nmulticast beamforming solution structure for both the quality-of-service (QoS)\nproblem and the max-min fair (MMF) problem. The optimal structure brings\nvaluable insights into multicast beamforming: We show that the notion of\nuplink-downlink duality can be generalized to the multicast beamforming\nproblem. The optimal multicast beamformer is a weighted MMSE filter based on a\ngroup-channel direction: a generalized version of the optimal downlink\nmulti-user unicast beamformer. We also show that there is an inherent\nlow-dimensional structure in the optimal multicast beamforming solution\nindependent of the number of transmit antennas, leading to efficient numerical\nalgorithm design, especially for systems with large antenna arrays. We propose\nefficient algorithms to compute the multicast beamformer based on the optimal\nbeamforming structure. Through asymptotic analysis, we characterize the\nasymptotic behavior of the multicast beamformers as the number of antennas\ngrows, and in turn, provide simple closed-form approximate multicast\nbeamformers for both the QoS and MMF problems. This approximation offers\npractical multicast beamforming solutions with a near-optimal performance at\nvery low computational complexity for large-scale antenna systems.",
    "published_date": "2019-11-20T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08925v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.08864v1",
    "title": "Impact of the Net Neutrality Repeal on Communication Networks",
    "authors": [
      "Hatem A. Alharbi",
      "Taisir E. H. Elgorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "author_ids": [],
    "abstract": "Network neutrality (net neutrality) is the principle of treating equally all\nInternet traffic regardless of its source, destination, content, application or\nother related distinguishing metrics. Under net neutrality, ISPs are compelled\nto charge all content providers (CPs) the same per Gbps rate despite the\ngrowing profit achieved by CPs. In this paper, we study the impact of the\nrepeal of net neutrality on communication networks by developing a\ntechno-economic Mixed Integer Linear Programming (MILP) model to maximize the\npotential profit ISPs can achieve by offering their services to CPs. We focus\non video delivery as video traffic accounts for 78% of the cloud traffic. We\nconsider an ISP that offers CPs different classes of service representing\ntypical video content qualities including standard definition (SD), high\ndefinition (HD) and ultra-high definition (UHD) video. The MILP model maximizes\nthe ISP profit by optimizing the prices of the different classes according to\nthe users demand sensitivity to the change in price, referred to as Price\nElasticity of Demand (PED). We analyze how PED impacts the profit in different\nCP delivery scenarios in cloud-fog architectures. The results show that the\nrepeal of net neutrality can potentially increase ISPs profit by a factor of 8\nwith a pricing scheme that discriminates against data intensive content. Also,\nthe repeal of net neutrality positively impacts the network energy efficiency\nby reducing the core network power consumption by 55% as a result of\nsuppressing data intensive content compared to the net neutrality scenario.",
    "published_date": "2019-11-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08864v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.08799v1",
    "title": "Solving Online Threat Screening Games using Constrained Action Space Reinforcement Learning",
    "authors": [
      "Sanket Shah",
      "Arunesh Sinha",
      "Pradeep Varakantham",
      "Andrew Perrault",
      "Milind Tambe"
    ],
    "author_ids": [],
    "abstract": "Large-scale screening for potential threats with limited resources and\ncapacity for screening is a problem of interest at airports, seaports, and\nother ports of entry. Adversaries can observe screening procedures and arrive\nat a time when there will be gaps in screening due to limited resource\ncapacities. To capture this game between ports and adversaries, this problem\nhas been previously represented as a Stackelberg game, referred to as a Threat\nScreening Game (TSG). Given the significant complexity associated with solving\nTSGs and uncertainty in arrivals of customers, existing work has assumed that\nscreenees arrive and are allocated security resources at the beginning of the\ntime window. In practice, screenees such as airport passengers arrive in bursts\ncorrelated with flight time and are not bound by fixed time windows. To address\nthis, we propose an online threat screening model in which screening strategy\nis determined adaptively as a passenger arrives while satisfying a hard bound\non acceptable risk of not screening a threat. To solve the online problem with\na hard bound on risk, we formulate it as a Reinforcement Learning (RL) problem\nwith constraints on the action space (hard bound on risk). We provide a novel\nway to efficiently enforce linear inequality constraints on the action output\nin Deep Reinforcement Learning. We show that our solution allows us to\nsignificantly reduce screenee wait time while guaranteeing a bound on risk.",
    "published_date": "2019-11-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08799v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.09531v1",
    "title": "Towards FAIR protocols and workflows: The OpenPREDICT case study",
    "authors": [
      "Remzi Celebi",
      "Joao Rebelo Moreira",
      "Ahmed A. Hassan",
      "Sandeep Ayyar",
      "Lars Ridder",
      "Tobias Kuhn",
      "Michel Dumontier"
    ],
    "author_ids": [],
    "abstract": "It is essential for the advancement of science that scientists and\nresearchers share, reuse and reproduce workflows and protocols used by others.\nThe FAIR principles are a set of guidelines that aim to maximize the value and\nusefulness of research data, and emphasize a number of important points\nregarding the means by which digital objects are found and reused by others.\nThe question of how to apply these principles not just to the static input and\noutput data but also to the dynamic workflows and protocols that consume and\nproduce them is still under debate and poses a number of challenges. In this\npaper we describe our inclusive and overarching approach to apply the FAIR\nprinciples to workflows and protocols and demonstrate its benefits. We apply\nand evaluate our approach on a case study that consists of making the PREDICT\nworkflow, a highly cited drug repurposing workflow, open and FAIR. This\nincludes FAIRification of the involved datasets, as well as applying semantic\ntechnologies to represent and store data about the detailed versions of the\ngeneral protocol, of the concrete workflow instructions, and of their execution\ntraces. A semantic model was proposed to better address these specific\nrequirements and were evaluated by answering competency questions. This\nsemantic model consists of classes and relations from a number of existing\nontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then\nto formulate and answer new kinds of competency questions. Our evaluation shows\nthe high degree to which our FAIRified OpenPREDICT workflow now adheres to the\nFAIR principles and the practicality and usefulness of being able to answer our\nnew competency questions.",
    "published_date": "2019-11-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.09531v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08603v1",
    "title": "Forbidden knowledge in machine learning -- Reflections on the limits of research and publication",
    "authors": [
      "Thilo Hagendorff"
    ],
    "author_ids": [],
    "abstract": "Certain research strands can yield \"forbidden knowledge\". This term refers to\nknowledge that is considered too sensitive, dangerous or taboo to be produced\nor shared. Discourses about such publication restrictions are already\nentrenched in scientific fields like IT security, synthetic biology or nuclear\nphysics research. This paper makes the case for transferring this discourse to\nmachine learning research. Some machine learning applications can very easily\nbe misused and unfold harmful consequences, for instance with regard to\ngenerative video or text synthesis, personality analysis, behavior\nmanipulation, software vulnerability detection and the like. Up to now, the\nmachine learning research community embraces the idea of open access. However,\nthis is opposed to precautionary efforts to prevent the malicious use of\nmachine learning applications. Information about or from such applications may,\nif improperly disclosed, cause harm to people, organizations or whole\nsocieties. Hence, the goal of this work is to outline norms that can help to\ndecide whether and when the dissemination of such information should be\nprevented. It proposes review parameters for the machine learning community to\nestablish an ethical framework on how to deal with forbidden knowledge and\ndual-use applications.",
    "published_date": "2019-11-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08603v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08089v2",
    "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making with Deep Learning",
    "authors": [
      "Mark Sendak",
      "Madeleine Elish",
      "Michael Gao",
      "Joseph Futoma",
      "William Ratliff",
      "Marshall Nichols",
      "Armando Bedoya",
      "Suresh Balu",
      "Cara O'Brien"
    ],
    "author_ids": [],
    "abstract": "Machine learning technologies are increasingly developed for use in\nhealthcare. While research communities have focused on creating\nstate-of-the-art models, there has been less focus on real world implementation\nand the associated challenges to accuracy, fairness, accountability, and\ntransparency that come from actual, situated use. Serious questions remain\nunder examined regarding how to ethically build models, interpret and explain\nmodel output, recognize and account for biases, and minimize disruptions to\nprofessional expertise and work cultures. We address this gap in the literature\nand provide a detailed case study covering the development, implementation, and\nevaluation of Sepsis Watch, a machine learning-driven tool that assists\nhospital clinicians in the early diagnosis and treatment of sepsis. We, the\nteam that developed and evaluated the tool, discuss our conceptualization of\nthe tool not as a model deployed in the world but instead as a socio-technical\nsystem requiring integration into existing social and professional contexts.\nRather than focusing on model interpretability to ensure a fair and accountable\nmachine learning, we point toward four key values and practices that should be\nconsidered when developing machine learning to support clinical\ndecision-making: rigorously define the problem in context, build relationships\nwith stakeholders, respect professional discretion, and create ongoing feedback\nloops with stakeholders. Our work has significant implications for future\nresearch regarding mechanisms of institutional accountability and\nconsiderations for designing machine learning systems. Our work underscores the\nlimits of model interpretability as a solution to ensure transparency,\naccuracy, and accountability in practice. Instead, our work demonstrates other\nmeans and goals to achieve FATML values in design and in practice.",
    "published_date": "2019-11-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08089v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08080v4",
    "title": "Jointly De-biasing Face Recognition and Demographic Attribute Estimation",
    "authors": [
      "Sixue Gong",
      "Xiaoming Liu",
      "Anil K. Jain"
    ],
    "author_ids": [],
    "abstract": "We address the problem of bias in automated face recognition and demographic\nattribute estimation algorithms, where errors are lower on certain cohorts\nbelonging to specific demographic groups. We present a novel de-biasing\nadversarial network (DebFace) that learns to extract disentangled feature\nrepresentations for both unbiased face recognition and demographics estimation.\nThe proposed network consists of one identity classifier and three demographic\nclassifiers (for gender, age, and race) that are trained to distinguish\nidentity and demographic attributes, respectively. Adversarial learning is\nadopted to minimize correlation among feature factors so as to abate bias\ninfluence from other factors. We also design a new scheme to combine\ndemographics with identity features to strengthen robustness of face\nrepresentation in different demographic groups. The experimental results show\nthat our approach is able to reduce bias in face recognition as well as\ndemographics estimation while achieving state-of-the-art performance.",
    "published_date": "2019-11-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08080v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08054v2",
    "title": "Policy-Gradient Training of Fair and Unbiased Ranking Functions",
    "authors": [
      "Himank Yadav",
      "Zhengxiao Du",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "While implicit feedback (e.g., clicks, dwell times, etc.) is an abundant and\nattractive source of data for learning to rank, it can produce unfair ranking\npolicies for both exogenous and endogenous reasons. Exogenous reasons typically\nmanifest themselves as biases in the training data, which then get reflected in\nthe learned ranking policy and often lead to rich-get-richer dynamics.\nMoreover, even after the correction of such biases, reasons endogenous to the\ndesign of the learning algorithm can still lead to ranking policies that do not\nallocate exposure among items in a fair way. To address both exogenous and\nendogenous sources of unfairness, we present the first learning-to-rank\napproach that addresses both presentation bias and merit-based fairness of\nexposure simultaneously. Specifically, we define a class of amortized\nfairness-of-exposure constraints that can be chosen based on the needs of an\napplication, and we show how these fairness criteria can be enforced despite\nthe selection biases in implicit feedback data. The key result is an efficient\nand flexible policy-gradient algorithm, called FULTR, which is the first to\nenable the use of counterfactual estimators for both utility estimation and\nfairness constraints. Beyond the theoretical justification of the framework, we\nshow empirically that the proposed algorithm can learn accurate and fair\nranking policies from biased and noisy feedback.",
    "published_date": "2019-11-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08054v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08024v1",
    "title": "A Bias Trick for Centered Robust Principal Component Analysis",
    "authors": [
      "Baokun He",
      "Guihong Wan",
      "Haim Schweitzer"
    ],
    "author_ids": [],
    "abstract": "Outlier based Robust Principal Component Analysis (RPCA) requires centering\nof the non-outliers. We show a \"bias trick\" that automatically centers these\nnon-outliers. Using this bias trick we obtain the first RPCA algorithm that is\noptimal with respect to centering.",
    "published_date": "2019-11-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08024v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.07729v4",
    "title": "ImmuNeCS: Neural Committee Search by an Artificial Immune System",
    "authors": [
      "Luc Frachon",
      "Wei Pang",
      "George M. Coghill"
    ],
    "author_ids": [],
    "abstract": "Current Neural Architecture Search techniques can suffer from a few\nshortcomings, including high computational cost, excessive bias from the search\nspace, conceptual complexity or uncertain empirical benefits over random\nsearch. In this paper, we present ImmuNeCS, an attempt at addressing these\nissues with a method that offers a simple, flexible, and efficient way of\nbuilding deep learning models automatically, and we demonstrate its\neffectiveness in the context of convolutional neural networks. Instead of\nsearching for the 1-best architecture for a given task, we focus on building a\npopulation of neural networks that are then ensembled into a neural network\ncommittee, an approach we dub 'Neural Committee Search'. To ensure sufficient\nperformance from the committee, our search algorithm is based on an artificial\nimmune system that balances individual performance with population diversity.\nThis allows us to stop the search when accuracy starts to plateau, and to\nbridge the performance gap through ensembling. In order to justify our method,\nwe first verify that the chosen search space exhibits the locality property. To\nfurther improve efficiency, we also combine partial evaluation, weight\ninheritance, and progressive search. First, experiments are run to verify the\nvalidity of these techniques. Then, preliminary experimental results on two\npopular computer vision benchmarks show that our method consistently\noutperforms random search and yields promising results within reasonable GPU\nbudgets. An additional experiment also shows that ImmuNeCS's solutions transfer\neffectively to a more difficult task, where they achieve results comparable to\na direct search on the new task. We believe these findings can open the way for\nnew, accessible alternatives to traditional NAS.",
    "published_date": "2019-11-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NE",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.07729v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.07679v1",
    "title": "Towards Quantification of Bias in Machine Learning for Healthcare: A Case Study of Renal Failure Prediction",
    "authors": [
      "Josie Williams",
      "Narges Razavian"
    ],
    "author_ids": [],
    "abstract": "As machine learning (ML) models, trained on real-world datasets, become\ncommon practice, it is critical to measure and quantify their potential biases.\nIn this paper, we focus on renal failure and compare a commonly used\ntraditional risk score, Tangri, with a more powerful machine learning model,\nwhich has access to a larger variable set and trained on 1.6 million patients'\nEHR data. We will compare and discuss the generalization and applicability of\nthese two models, in an attempt to quantify biases of status quo clinical\npractice, compared to ML-driven models.",
    "published_date": "2019-11-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.07679v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.07574v1",
    "title": "Bias-Aware Heapified Policy for Active Learning",
    "authors": [
      "Wen-Yen Chang",
      "Wen-Huan Chiang",
      "Shao-Hao Lu",
      "Tingfan Wu",
      "Min Sun"
    ],
    "author_ids": [],
    "abstract": "The data efficiency of learning-based algorithms is more and more important\nsince high-quality and clean data is expensive as well as hard to collect. In\norder to achieve high model performance with the least number of samples,\nactive learning is a technique that queries the most important subset of data\nfrom the original dataset. In active learning domain, one of the mainstream\nresearch is the heuristic uncertainty-based method which is useful for the\nlearning-based system. Recently, a few works propose to apply policy\nreinforcement learning (PRL) for querying important data. It seems more general\nthan heuristic uncertainty-based method owing that PRL method depends on data\nfeature which is reliable than human prior. However, there have two problems -\nsample inefficiency of policy learning and overconfidence, when applying PRL on\nactive learning. To be more precise, sample inefficiency of policy learning\noccurs when sampling within a large action space, in the meanwhile, class\nimbalance can lead to the overconfidence. In this paper, we propose a\nbias-aware policy network called Heapified Active Learning (HAL), which\nprevents overconfidence, and improves sample efficiency of policy learning by\nheapified structure without ignoring global inforamtion(overview of the whole\nunlabeled set). In our experiment, HAL outperforms other baseline methods on\nMNIST dataset and duplicated MNIST. Last but not least, we investigate the\ngeneralization of the HAL policy learned on MNIST dataset by directly applying\nit on MNIST-M. We show that the agent can generalize and outperform\ndirectly-learned policy under constrained labeled sets.",
    "published_date": "2019-11-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.07574v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.07272v2",
    "title": "Unsupervised Visual Representation Learning with Increasing Object Shape Bias",
    "authors": [
      "Zhibo Wang",
      "Shen Yan",
      "Xiaoyu Zhang",
      "Niels Lobo"
    ],
    "author_ids": [],
    "abstract": "(Very early draft)Traditional supervised learning keeps pushing convolution\nneural network(CNN) achieving state-of-art performance. However, lack of\nlarge-scale annotation data is always a big problem due to the high cost of it,\neven ImageNet dataset is over-fitted by complex models now. The success of\nunsupervised learning method represented by the Bert model in natural language\nprocessing(NLP) field shows its great potential. And it makes that unlimited\ntraining samples becomes possible and the great universal generalization\nability changes NLP research direction directly. In this article, we purpose a\nnovel unsupervised learning method based on contrastive predictive coding.\nUnder that, we are able to train model with any non-annotation images and\nimprove model's performance to reach state-of-art performance at the same level\nof model complexity. Beside that, since the number of training images could be\nunlimited amplification, an universal large-scale pre-trained computer vision\nmodel is possible in the future.",
    "published_date": "2019-11-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.07272v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.07053v1",
    "title": "Maintaining Discrimination and Fairness in Class Incremental Learning",
    "authors": [
      "Bowen Zhao",
      "Xi Xiao",
      "Guojun Gan",
      "Bin Zhang",
      "Shutao Xia"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) have been applied in class incremental learning,\nwhich aims to solve common real-world problems of learning new classes\ncontinually. One drawback of standard DNNs is that they are prone to\ncatastrophic forgetting. Knowledge distillation (KD) is a commonly used\ntechnique to alleviate this problem. In this paper, we demonstrate it can\nindeed help the model to output more discriminative results within old classes.\nHowever, it cannot alleviate the problem that the model tends to classify\nobjects into new classes, causing the positive effect of KD to be hidden and\nlimited. We observed that an important factor causing catastrophic forgetting\nis that the weights in the last fully connected (FC) layer are highly biased in\nclass incremental learning. In this paper, we propose a simple and effective\nsolution motivated by the aforementioned observations to address catastrophic\nforgetting. Firstly, we utilize KD to maintain the discrimination within old\nclasses. Then, to further maintain the fairness between old classes and new\nclasses, we propose Weight Aligning (WA) that corrects the biased weights in\nthe FC layer after normal training process. Unlike previous work, WA does not\nrequire any extra parameters or a validation set in advance, as it utilizes the\ninformation provided by the biased weights themselves. The proposed method is\nevaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings.\nExperimental results show that the proposed method can effectively alleviate\ncatastrophic forgetting and significantly outperform state-of-the-art methods.",
    "published_date": "2019-11-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.07053v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.06997v2",
    "title": "Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game",
    "authors": [
      "Ngoc-Trung Tran",
      "Viet-Hung Tran",
      "Ngoc-Bao Nguyen",
      "Linxiao Yang",
      "Ngai-Man Cheung"
    ],
    "author_ids": [],
    "abstract": "Self-supervised (SS) learning is a powerful approach for representation\nlearning using unlabeled data. Recently, it has been applied to Generative\nAdversarial Networks (GAN) training. Specifically, SS tasks were proposed to\naddress the catastrophic forgetting issue in the GAN discriminator. In this\nwork, we perform an in-depth analysis to understand how SS tasks interact with\nlearning of generator. From the analysis, we identify issues of SS tasks which\nallow a severely mode-collapsed generator to excel the SS tasks. To address the\nissues, we propose new SS tasks based on a multi-class minimax game. The\ncompetition between our proposed SS tasks in the game encourages the generator\nto learn the data distribution and generate diverse samples. We provide both\ntheoretical and empirical analysis to support that our proposed SS tasks have\nbetter convergence property. We conduct experiments to incorporate our proposed\nSS tasks into two different GAN baseline models. Our approach establishes\nstate-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet\n$32\\times32$ and Stacked-MNIST datasets, outperforming existing works by\nconsiderable margins in some cases. Our unconditional GAN model approaches\nperformance of conditional GAN without using labeled data. Our code:\nhttps://github.com/tntrung/msgan",
    "published_date": "2019-11-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06997v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08556v1",
    "title": "Towards Reducing Bias in Gender Classification",
    "authors": [
      "Komal K. Teru",
      "Aishik Chakraborty"
    ],
    "author_ids": [],
    "abstract": "Societal bias towards certain communities is a big problem that affects a lot\nof machine learning systems. This work aims at addressing the racial bias\npresent in many modern gender recognition systems. We learn race invariant\nrepresentations of human faces with an adversarially trained autoencoder model.\nWe show that such representations help us achieve less biased performance in\ngender classification. We use variance in classification accuracy across\ndifferent races as a surrogate for the racial bias of the model and achieve a\ndrop of over 40% in variance with race invariant representations.",
    "published_date": "2019-11-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08556v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.06957v1",
    "title": "An Induced Multi-Relational Framework for Answer Selection in Community Question Answer Platforms",
    "authors": [
      "Kanika Narang",
      "Chaoqi Yang",
      "Adit Krishnan",
      "Junting Wang",
      "Hari Sundaram",
      "Carolyn Sutter"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the question of identifying the best candidate answer to\na question on Community Question Answer (CQA) forums. The problem is important\nbecause Individuals often visit CQA forums to seek answers to nuanced\nquestions. We develop a novel induced relational graph convolutional network\n(IR-GCN) framework to address the question. We make three contributions. First,\nwe introduce a modular framework that separates the construction of the graph\nwith the label selection mechanism. We use equivalence relations to induce a\ngraph comprising cliques and identify two label assignment mechanisms---label\ncontrast, label sharing. Then, we show how to encode these assignment\nmechanisms in GCNs. Second, we show that encoding contrast creates\ndiscriminative magnification---enhancing the separation between nodes in the\nembedding space. Third, we show a surprising result---boosting techniques\nimprove learning over familiar stacking, fusion, or aggregation approaches for\nneural architectures. We show strong results over the state-of-the-art neural\nbaselines in extensive experiments on 50 StackExchange communities.",
    "published_date": "2019-11-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06957v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08554v1",
    "title": "Classification as Decoder: Trading Flexibility for Control in Medical Dialogue",
    "authors": [
      "Sam Shleifer",
      "Manish Chablani",
      "Anitha Kannan",
      "Namit Katariya",
      "Xavier Amatriain"
    ],
    "author_ids": [],
    "abstract": "Generative seq2seq dialogue systems are trained to predict the next word in\ndialogues that have already occurred. They can learn from large unlabeled\nconversation datasets, build a deeper understanding of conversational context,\nand generate a wide variety of responses. This flexibility comes at the cost of\ncontrol, a concerning tradeoff in doctor/patient interactions. Inaccuracies,\ntypos, or undesirable content in the training data will be reproduced by the\nmodel at inference time. We trade a small amount of labeling effort and some\nloss of response variety in exchange for quality control. More specifically, a\npretrained language model encodes the conversational context, and we finetune a\nclassification head to map an encoded conversational context to a response\nclass, where each class is a noisily labeled group of interchangeable\nresponses. Experts can update these exemplar responses over time as best\npractices change without retraining the classifier or invalidating old training\ndata. Expert evaluation of 775 unseen doctor/patient conversations shows that\nonly 12% of the discriminative model's responses are worse than the what the\ndoctor ended up writing, compared to 18% for the generative model.",
    "published_date": "2019-11-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08554v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.06935v1",
    "title": "Fairness With Minimal Harm: A Pareto-Optimal Approach For Healthcare",
    "authors": [
      "Natalia Martinez",
      "Martin Bertran",
      "Guillermo Sapiro"
    ],
    "author_ids": [],
    "abstract": "Common fairness definitions in machine learning focus on balancing notions of\ndisparity and utility. In this work, we study fairness in the context of risk\ndisparity among sub-populations. We are interested in learning models that\nminimize performance discrepancies across sensitive groups without causing\nunnecessary harm. This is relevant to high-stakes domains such as healthcare,\nwhere non-maleficence is a core principle. We formalize this objective using\nPareto frontiers, and provide analysis, based on recent works in fairness, to\nexemplify scenarios were perfect fairness might not be feasible without doing\nunnecessary harm. We present a methodology for training neural networks that\nachieve our goal by dynamically re-balancing subgroups risks. We argue that\neven in domains where fairness at cost is required, finding a\nnon-unnecessary-harm fairness model is the optimal initial step. We demonstrate\nthis methodology on real case-studies of predicting ICU patient mortality, and\nclassifying skin lesions from dermatoscopic images.",
    "published_date": "2019-11-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06935v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.06837v1",
    "title": "Dynamic Modeling and Equilibria in Fair Decision Making",
    "authors": [
      "Joshua Williams",
      "J. Zico Kolter"
    ],
    "author_ids": [],
    "abstract": "Recent studies on fairness in automated decision making systems have both\ninvestigated the potential future impact of these decisions on the population\nat large, and emphasized that imposing ''typical'' fairness constraints such as\ndemographic parity or equality of opportunity does not guarantee a benefit to\ndisadvantaged groups. However, these previous studies have focused on either\nsimple one-step cost/benefit criteria, or on discrete underlying state spaces.\nIn this work, we first propose a natural continuous representation of\npopulation state, governed by the Beta distribution, using a loan granting\nsetting as a running example. Next, we apply a model of population dynamics\nunder lending decisions, and show that when conditional payback probabilities\nare estimated correctly 1) ``optimal'' behavior by lenders can lead to\n''Matthew Effect'' bifurcations (i.e., ''the rich get richer and the poor get\npoorer''), but that 2) many common fairness constraints on the allowable\npolicies cause groups to converge to the same equilibrium point. Last, we\ncontrast our results in the case of misspecified conditional probability\nestimates with prior work, and show that for this model, different levels of\ngroup misestimation guarantees that even fair policies lead to bifurcations. We\nillustrate some of the modeling conclusions on real data from credit scoring.",
    "published_date": "2019-11-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06837v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.06723v1",
    "title": "A nonparametric framework for inferring orders of categorical data from category-real ordered pairs",
    "authors": [
      "Chainarong Amornbunchornvej",
      "Navaporn Surasvadi",
      "Anon Plangprasopchok",
      "Suttipong Thajchayapong"
    ],
    "author_ids": [],
    "abstract": "Given a dataset of careers and incomes, how large a difference of income\nbetween any pair of careers would be? Given a dataset of travel time records,\nhow long do we need to spend more when choosing a public transportation mode\n$A$ instead of $B$ to travel? In this paper, we propose a framework that is\nable to infer orders of categories as well as magnitudes of difference of real\nnumbers between each pair of categories using Estimation statistics framework.\nNot only reporting whether an order of categories exists, but our framework\nalso reports the magnitude of difference of each consecutive pairs of\ncategories in the order. In large dataset, our framework is scalable well\ncompared with the existing framework. The proposed framework has been applied\nto two real-world case studies: 1) ordering careers by incomes based on\ninformation of 350,000 households living in Khon Kaen province, Thailand, and\n2) ordering sectors by closing prices based on 1060 companies' closing prices\nof NASDAQ stock markets between years 2000 and 2016. The results of careers\nordering show income inequality among different careers. The stock market\nresults illustrate dynamics of sector domination that can change over time. Our\napproach is able to be applied in any research area that has category-real\nordered pairs. Our proposed \"Dominant-Distribution Network\" provides a novel\napproach to gain new insight of analyzing category orders. The software of this\nframework is available for researchers or practitioners within R package:\nEDOIF.",
    "published_date": "2019-11-15T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ME",
      "cs.CY",
      "math.ST",
      "physics.data-an",
      "stat.ML",
      "stat.TH",
      "62G07, 06A06",
      "G.3; I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06723v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.06685v1",
    "title": "Fair Data Adaptation with Quantile Preservation",
    "authors": [
      "Drago Plečko",
      "Nicolai Meinshausen"
    ],
    "author_ids": [],
    "abstract": "Fairness of classification and regression has received much attention\nrecently and various, partially non-compatible, criteria have been proposed.\nThe fairness criteria can be enforced for a given classifier or, alternatively,\nthe data can be adapated to ensure that every classifier trained on the data\nwill adhere to desired fairness criteria. We present a practical data adaption\nmethod based on quantile preservation in causal structural equation models. The\ndata adaptation is based on a presumed counterfactual model for the data. While\nthe counterfactual model itself cannot be verified experimentally, we show that\ncertain population notions of fairness are still guaranteed even if the\ncounterfactual model is misspecified. The precise nature of the fulfilled\nnon-causal fairness notion (such as demographic parity, separation or\nsufficiency) depends on the structure of the underlying causal model and the\nchoice of resolving variables. We describe an implementation of the proposed\ndata adaptation procedure based on Random Forests and demonstrate its practical\nuse on simulated and real-world data.",
    "published_date": "2019-11-15T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06685v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.06129v1",
    "title": "A Bayesian/Information Theoretic Model of Bias Learning",
    "authors": [
      "Jonathan Baxter"
    ],
    "author_ids": [],
    "abstract": "In this paper the problem of learning appropriate bias for an environment of\nrelated tasks is examined from a Bayesian perspective. The environment of\nrelated tasks is shown to be naturally modelled by the concept of an {\\em\nobjective} prior distribution. Sampling from the objective prior corresponds to\nsampling different learning tasks from the environment. It is argued that for\nmany common machine learning problems, although we don't know the true\n(objective) prior for the problem, we do have some idea of a set of possible\npriors to which the true prior belongs. It is shown that under these\ncircumstances a learner can use Bayesian inference to learn the true prior by\nsampling from the objective prior. Bounds are given on the amount of\ninformation required to learn a task when it is simultaneously learnt with\nseveral other tasks. The bounds show that if the learner has little knowledge\nof the true prior, and the dimensionality of the true prior is small, then\nsampling multiple tasks is highly advantageous.",
    "published_date": "2019-11-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.06129v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05932v1",
    "title": "GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs",
    "authors": [
      "Yuan Liu",
      "Zehong Shen",
      "Zhixuan Lin",
      "Sida Peng",
      "Hujun Bao",
      "Xiaowei Zhou"
    ],
    "author_ids": [],
    "abstract": "Finding local correspondences between images with different viewpoints\nrequires local descriptors that are robust against geometric transformations.\nAn approach for transformation invariance is to integrate out the\ntransformations by pooling the features extracted from transformed versions of\nan image. However, the feature pooling may sacrifice the distinctiveness of the\nresulting descriptors. In this paper, we introduce a novel visual descriptor\nnamed Group Invariant Feature Transform (GIFT), which is both discriminative\nand robust to geometric transformations. The key idea is that the features\nextracted from the transformed versions of an image can be viewed as a function\ndefined on the group of the transformations. Instead of feature pooling, we use\ngroup convolutions to exploit underlying structures of the extracted features\non the group, resulting in descriptors that are both discriminative and\nprovably invariant to the group of transformations. Extensive experiments show\nthat GIFT outperforms state-of-the-art methods on several benchmark datasets\nand practically improves the performance of relative pose estimation.",
    "published_date": "2019-11-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05932v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05784v8",
    "title": "Pricing Multi-Interval Dispatch under Uncertainty Part I: Dispatch-Following Incentives",
    "authors": [
      "Ye Guo",
      "Cong Chen",
      "Lang Tong"
    ],
    "author_ids": [],
    "abstract": "Pricing multi-interval economic dispatch of electric power under operational\nuncertainty is considered in this two-part paper. Part I investigates\ndispatch-following incentives of profit-maximizing generators and shows that,\nunder mild conditions, no uniform-pricing scheme for the rolling-window\neconomic dispatch provides dispatch-following incentives that avoid\ndiscriminative out-of-the-market uplifts. A nonuniform pricing mechanism,\nreferred to as the temporal locational marginal pricing (TLMP), is proposed. As\nan extension of the standard locational marginal pricing (LMP), TLMP takes into\naccount both generation and ramping-induced opportunity costs. It eliminates\nthe need for the out-of-the-market uplifts and guarantees full\ndispatch-following incentives regardless of the accuracy of the demand\nforecasts used in the dispatch. It is also shown that, under TLMP, a\nprice-taking market participant has incentives to bid truthfully with its\nmarginal cost of generation. Part II of the paper extends the theoretical\nresults developed in Part I to more general network settings. It investigates a\nbroader set of performance measures, including the incentives of the truthful\nrevelation of ramping limits, revenue adequacy of the operator, consumer\npayments, generator profits, and price volatility under the rolling-window\ndispatch model with demand forecast errors.",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05784v8",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.05683v1",
    "title": "Modeling patterns of smartphone usage and their relationship to cognitive health",
    "authors": [
      "Jonas Rauber",
      "Emily B. Fox",
      "Leon A. Gatys"
    ],
    "author_ids": [],
    "abstract": "The ubiquity of smartphone usage in many people's lives make it a rich source\nof information about a person's mental and cognitive state. In this work we\nanalyze 12 weeks of phone usage data from 113 older adults, 31 with diagnosed\ncognitive impairment and 82 without. We develop structured models of users'\nsmartphone interactions to reveal differences in phone usage patterns between\npeople with and without cognitive impairment. In particular, we focus on\ninferring specific types of phone usage sessions that are predictive of\ncognitive impairment. Our model achieves an AUROC of 0.79 when discriminating\nbetween healthy and symptomatic subjects, and its interpretability enables\nnovel insights into which aspects of phone usage strongly relate with cognitive\nhealth in our dataset.",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.HC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05683v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05461v3",
    "title": "On the Complexity of Labeled Datasets",
    "authors": [
      "Rodrigo Fernandes de Mello"
    ],
    "author_ids": [],
    "abstract": "The Statistical Learning Theory (SLT) provides the foundation to ensure that\na supervised algorithm generalizes the mapping $f: \\mathcal{X} \\to \\mathcal{Y}$\ngiven $f$ is selected from its search space bias $\\mathcal{F}$. SLT depends on\nthe Shattering coefficient function $\\mathcal{N}(\\mathcal{F},n)$ to upper bound\nthe empirical risk minimization principle, from which one can estimate the\nnecessary training sample size to ensure the probabilistic learning convergence\nand, most importantly, the characterization of the capacity of $\\mathcal{F}$,\nincluding its underfitting and overfitting abilities while addressing specific\ntarget problems. However, the analytical solution of the Shattering coefficient\nis still an open problem since the first studies by Vapnik and Chervonenkis in\n$1962$, which we address on specific datasets, in this paper, by employing\nequivalence relations from Topology, data separability results by Har-Peled and\nJones, and combinatorics. Our approach computes the Shattering coefficient for\nboth binary and multi-class datasets, leading to the following additional\ncontributions: (i) the estimation of the required number of hyperplanes in the\nworst and best-case classification scenarios and the respective $\\Omega$ and\n$O$ complexities; (ii) the estimation of the training sample sizes required to\nensure supervised learning; and (iii) the comparison of dataset embeddings,\nonce they (re)organize samples into some new space configuration. All results\nintroduced and discussed along this paper are supported by the R package\nshattering (https://cran.r-project.org/web/packages/shattering).",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05461v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05447v1",
    "title": "Proofs of conservation inequalities for Levin's notion of mutual information of 1974",
    "authors": [
      "Nikolay Vereshchagin"
    ],
    "author_ids": [],
    "abstract": "In this paper we consider Levin's notion of mutual information in infinite\n0-1-sequences, as defined in [Leonid Levin. Laws of Information Conservation\n(Nongrowth) and Aspects of the Foundation of Probability Theory. Problems of\ninformation transmission, vol. 10 (1974), pp. 206--211]. The respective\ninformation conservation inequalities were stated in that paper without proofs.\nLater some proofs appeared in the literature, however no proof of the\nprobabilistic conservation inequality has been published yet. In this paper we\nprove that inequality and for the sake of completeness we present also short\nproofs of other properties of the said notion.",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05447v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.05426v3",
    "title": "Mean-Field Transmission Power Control in Dense Networks, Part II -- Social Welfare Evaluation",
    "authors": [
      "Yuchi Wu",
      "Junfeng Wu",
      "Minyi Huang",
      "Ling Shi"
    ],
    "author_ids": [],
    "abstract": "We consider uplink power control in wireless communication when massive users\ncompete over the channel resources. In Part I, we have formulated massive\ntransmission power control contest in a mean-field game framework. In this\npart, our goal is to investigate whether the power-domain non-orthogonal\nmultiple access (NOMA) protocol can regulate the non-cooperative channel access\nbehaviors, i.e., steering the competition among the non-cooperative users in a\ndirection with improved efficiency and fairness. It is compared with the CDMA\nprotocol, which drives each user to fiercely compete against the population,\nhence the efficiency of channel usage is sacrificed. The existence and\nuniqueness of an equilibrium strategy under CDMA and NOMA have already been\ncharacterized in Part I. In this paper, we adopt the social welfare of the\npopulation as the performance metric, which is defined as the expectation of\nutility over the distribution of different types of channel users. It is shown\nthat under the corresponding equilibrium strategies, NOMA outperforms CDMA in\nthe social welfare achieved, which is illustrated through simulation with\ndifferent unit price for power consumption. Moreover, it can be observed from\nnumerical results that NOMA can improve the fairness of the achieved data rates\namong different users.",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05426v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.05395v1",
    "title": "Allowing for equal opportunities for artists in music recommendation",
    "authors": [
      "Christine Bauer"
    ],
    "author_ids": [],
    "abstract": "Promoting diversity in the music sector is widely discussed on the media.\nWhile the major problem may lie deep in our society, music information\nretrieval contributes to promoting diversity or may create unequal\nopportunities for artists. For example, considering the known problem of\npopularity bias in music recommendation, it is important to investigate whether\nthe short head of popular music artists and the long tail of less popular ones\nshow similar patterns of diversity---in terms of, for example, age, gender, or\nethnic origin---or the popularity bias amplifies a positive or negative effect.\nI advocate for reasonable opportunities for artists---for (currently) popular\nartists and artists in the long-tail alike---in music recommender systems. In\nthis work, I represent the position that we need to develop a deep\nunderstanding of the biases and inequalities because it is the essential basis\nto design approaches for music recommendation that provide reasonable\nopportunities. Thus, research needs to investigate the various reasons that\nhinder equal opportunity and diversity in music recommendation.",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05395v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.05369v2",
    "title": "Fair Adversarial Gradient Tree Boosting",
    "authors": [
      "Vincent Grari",
      "Boris Ruf",
      "Sylvain Lamprier",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "Fair classification has become an important topic in machine learning\nresearch. While most bias mitigation strategies focus on neural networks, we\nnoticed a lack of work on fair classifiers based on decision trees even though\nthey have proven very efficient. In an up-to-date comparison of\nstate-of-the-art classification algorithms in tabular data, tree boosting\noutperforms deep learning. For this reason, we have developed a novel approach\nof adversarial gradient tree boosting. The objective of the algorithm is to\npredict the output $Y$ with gradient tree boosting while minimizing the ability\nof an adversarial neural network to predict the sensitive attribute $S$. The\napproach incorporates at each iteration the gradient of the neural network\ndirectly in the gradient tree boosting. We empirically assess our approach on 4\npopular data sets and compare against state-of-the-art algorithms. The results\nshow that our algorithm achieves a higher accuracy while obtaining the same\nlevel of fairness, as measured using a set of different common fairness\ndefinitions.",
    "published_date": "2019-11-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05369v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05124v1",
    "title": "Imperfect bifurcations in opinion dynamics under external fields",
    "authors": [
      "Francisco Freitas",
      "Allan R. Vieira",
      "Celia Anteneodo"
    ],
    "author_ids": [],
    "abstract": "We investigate, through a kinetic-exchange model, the impact that an external\nfield, like advertising and propaganda, has on opinion dynamics. We address the\nsituations where two opposite alternatives can be selected but the possibility\nof indecision also exists. In this model, individuals influence each other\nthrough pairwise interactions, which can be of agreement or disagreement, and\nthere are also external fields that can skew decision making. Two parameters\nare used to model the interactions with the field: one measures the sensitivity\nof the individuals to be influenced, another quantifies in which direction. We\nstudy this model in a fully connected social network scenario, by means of\nnumerical simulations of the kinetic exchange dynamics and analytical results\nderived from the mean-field rate equations. We show how the external bias gives\nrise to imperfect bifurcations, and cusp catastrophes, allowing abrupt changes\nand hysteresis depending on the level of disagreement in interpersonal\ninteractions and on the strength of the external influence.",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cond-mat.stat-mech",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05124v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.04931v2",
    "title": "Efficient Fair Principal Component Analysis",
    "authors": [
      "Mohammad Mahdi Kamani",
      "Farzin Haddadpour",
      "Rana Forsati",
      "Mehrdad Mahdavi"
    ],
    "author_ids": [],
    "abstract": "It has been shown that dimension reduction methods such as PCA may be\ninherently prone to unfairness and treat data from different sensitive groups\nsuch as race, color, sex, etc., unfairly. In pursuit of fairness-enhancing\ndimensionality reduction, using the notion of Pareto optimality, we propose an\nadaptive first-order algorithm to learn a subspace that preserves fairness,\nwhile slightly compromising the reconstruction loss. Theoretically, we provide\nsufficient conditions that the solution of the proposed algorithm belongs to\nthe Pareto frontier for all sensitive groups; thereby, the optimal trade-off\nbetween overall reconstruction loss and fairness constraints is guaranteed. We\nalso provide the convergence analysis of our algorithm and show its efficacy\nthrough empirical studies on different datasets, which demonstrates superior\nperformance in comparison with state-of-the-art algorithms. The proposed\nfairness-aware PCA algorithm can be efficiently generalized to multiple group\nsensitive features and effectively reduce the unfairness decisions in\ndownstream tasks such as classification.",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04931v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.04929v1",
    "title": "Fairness-Aware Neural Réyni Minimization for Continuous Features",
    "authors": [
      "Vincent Grari",
      "Boris Ruf",
      "Sylvain Lamprier",
      "Marcin Detyniecki"
    ],
    "author_ids": [],
    "abstract": "The past few years have seen a dramatic rise of academic and societal\ninterest in fair machine learning. While plenty of fair algorithms have been\nproposed recently to tackle this challenge for discrete variables, only a few\nideas exist for continuous ones. The objective in this paper is to ensure some\nindependence level between the outputs of regression models and any given\ncontinuous sensitive variables. For this purpose, we use the\nHirschfeld-Gebelein-R\\'enyi (HGR) maximal correlation coefficient as a fairness\nmetric. We propose two approaches to minimize the HGR coefficient. First, by\nreducing an upper bound of the HGR with a neural network estimation of the\n$\\chi^{2}$ divergence. Second, by minimizing the HGR directly with an\nadversarial neural network architecture. The idea is to predict the output Y\nwhile minimizing the ability of an adversarial neural network to find the\nestimated transformations which are required to predict the HGR coefficient. We\nempirically assess and compare our approaches and demonstrate significant\nimprovements on previously presented work in the field.",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04929v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.04827v1",
    "title": "Artist and style exposure bias in collaborative filtering based music recommendations",
    "authors": [
      "Andres Ferraro",
      "Dmitry Bogdanov",
      "Xavier Serra",
      "Jason Yoon"
    ],
    "author_ids": [],
    "abstract": "Algorithms have an increasing influence on the music that we consume and\nunderstanding their behavior is fundamental to make sure they give a fair\nexposure to all artists across different styles. In this on-going work we\ncontribute to this research direction analyzing the impact of collaborative\nfiltering recommendations from the perspective of artist and music style\nexposure given by the system. We first analyze the distribution of the\nrecommendations considering the exposure of different styles or genres and\ncompare it to the users' listening behavior. This comparison suggests that the\nsystem is reinforcing the popularity of the items. Then, we simulate the effect\nof the system in the long term with a feedback loop. From this simulation we\ncan see how the system gives less opportunity to the majority of artists,\nconcentrating the users on fewer items. The results of our analysis demonstrate\nthe need for a better evaluation methodology for current music recommendation\nalgorithms, not only limited to user-focused relevance metrics.",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04827v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.04768v1",
    "title": "Debugging Crashes using Continuous Contrast Set Mining",
    "authors": [
      "Rebecca Qian",
      "Yang Yu",
      "Wonhee Park",
      "Vijayaraghavan Murali",
      "Stephen Fink",
      "Satish Chandra"
    ],
    "author_ids": [],
    "abstract": "Facebook operates a family of services used by over two billion people daily\non a huge variety of mobile devices. Many devices are configured to upload\ncrash reports should the app crash for any reason. Engineers monitor and triage\nmillions of crash reports logged each day to check for bugs, regressions, and\nany other quality problems. Debugging groups of crashes is a manually intensive\nprocess that requires deep domain expertise and close inspection of traces and\ncode, often under time constraints. We use contrast set mining, a form of\ndiscriminative pattern mining, to learn what distinguishes one group of crashes\nfrom another. Prior works focus on discretization to apply contrast mining to\ncontinuous data. We propose the first direct application of contrast learning\nto continuous data, without the need for discretization. We also define a\nweighted anomaly score that unifies continuous and categorical contrast sets\nwhile mitigating bias, as well as uncertainty measures that communicate\nconfidence to developers. We demonstrate the value of our novel statistical\nimprovements by applying it on a challenging dataset from Facebook production\nlogs, where we achieve 40x speedup over baseline approaches using\ndiscretization.",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04768v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.04678v1",
    "title": "Uniform Partition in Population Protocol Model under Weak Fairness",
    "authors": [
      "Hiroto Yasumi",
      "Fukuhito Ooshita",
      "Michiko Inoue"
    ],
    "author_ids": [],
    "abstract": "We focus on a uniform partition problem in a population protocol model. The\nuniform partition problem aims to divide a population into k groups of the same\nsize, where k is a given positive integer. In the case of k=2 (called uniform\nbipartition), a previous work clarified space complexity under various\nassumptions: 1) an initialized base station (BS) or no BS, 2) weak or global\nfairness, 3) designated or arbitrary initial states of agents, and 4) symmetric\nor asymmetric protocols, except for the setting that agents execute a protocol\nfrom arbitrary initial states under weak fairness in the model with an\ninitialized base station. In this paper, we clarify the space complexity for\nthis remaining setting. In this setting, we prove that P states are necessary\nand sufficient to realize asymmetric protocols, and that P+1 states are\nnecessary and sufficient to realize symmetric protocols, where P is the known\nupper bound of the number of agents. From these results and the previous work,\nwe have clarified the solvability of the uniform bipartition for each\ncombination of assumptions. Additionally, we newly consider an assumption on a\nmodel of a non-initialized BS and clarify solvability and space complexity in\nthe assumption. Moreover, the results in this paper can be applied to the case\nthat k is an arbitrary integer (called uniform k-partition).",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04678v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.04625v1",
    "title": "Building the National Radio Recordings Database: A Big Data Approach to Documenting Audio Heritage",
    "authors": [
      "Emily Goodmann",
      "Mark A. Matienzo",
      "Shawn VanCour",
      "William Vanden Dries"
    ],
    "author_ids": [],
    "abstract": "This paper traces strategies used by the Radio Preservation Task Force of the\nLibrary of Congress's National Recording Preservation Board to develop a\npublicly searchable database documenting extant radio materials held by\ncollecting institutions throughout the country. Having aggregated metadata on\n2,500 unique collections to date, the project has encountered a series of\nlogistical challenges that are not only technical in nature but also\ninstitutional and social, raising critical issues involving organizational\nstructure, political representation, and the ethics of data access. As the\nproject continues to expand and evolve, lessons from its early development\noffer valuable reminders of the human judgment, hidden labor, and interpersonal\nrelations required for successful big data work.",
    "published_date": "2019-11-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL",
      "cs.CY",
      "H.3.7; K.4.1; J.5"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04625v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.04594v6",
    "title": "Rate-Regularization and Generalization in VAEs",
    "authors": [
      "Alican Bozkurt",
      "Babak Esmaeili",
      "Jean-Baptiste Tristan",
      "Dana H. Brooks",
      "Jennifer G. Dy",
      "Jan-Willem van de Meent"
    ],
    "author_ids": [],
    "abstract": "Variational autoencoders optimize an objective that combines a reconstruction\nloss (the distortion) and a KL term (the rate). The rate is an upper bound on\nthe mutual information, which is often interpreted as a regularizer that\ncontrols the degree of compression. We here examine whether inclusion of the\nrate also acts as an inductive bias that improves generalization. We perform\nrate-distortion analyses that control the strength of the rate term, the\nnetwork capacity, and the difficulty of the generalization problem. Decreasing\nthe strength of the rate paradoxically improves generalization in most\nsettings, and reducing the mutual information typically leads to underfitting.\nMoreover, we show that generalization continues to improve even after the\nmutual information saturates, indicating that the gap on the bound (i.e. the KL\ndivergence relative to the inference marginal) affects generalization. This\nsuggests that the standard Gaussian prior is not an inductive bias that\ntypically aids generalization, prompting work to understand what choices of\npriors improve generalization in VAEs.",
    "published_date": "2019-11-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04594v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.04542v2",
    "title": "Explainable Artificial Intelligence (XAI) for 6G: Improving Trust between Human and Machine",
    "authors": [
      "Weisi Guo"
    ],
    "author_ids": [],
    "abstract": "As the 5th Generation (5G) mobile networks are bringing about global societal\nbenefits, the design phase for the 6th Generation (6G) has started. 6G will\nneed to enable greater levels of autonomy, improve human machine interfacing,\nand achieve deep connectivity in more diverse environments. The need for\nincreased explainability to enable trust is critical for 6G as it manages a\nwide range of mission critical services (e.g. autonomous driving) to safety\ncritical tasks (e.g. remote surgery). As we migrate from traditional\nmodel-based optimisation to deep learning, the trust we have in our\noptimisation modules decrease. This loss of trust means we cannot understand\nthe impact of: 1) poor/bias/malicious data, and 2) neural network design on\ndecisions; nor can we explain to the engineer or the public the network's\nactions. In this review, we outline the core concepts of Explainable Artificial\nIntelligence (XAI) for 6G, including: public and legal motivations, definitions\nof explainability, performance vs. explainability trade-offs, methods to\nimprove explainability, and frameworks to incorporate XAI into future wireless\nsystems. Our review is grounded in cases studies for both PHY and MAC layer\noptimisation, and provide the community with an important research area to\nembark upon.",
    "published_date": "2019-11-11T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.LG",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04542v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.08292v1",
    "title": "Fairness through Equality of Effort",
    "authors": [
      "Wen Huang",
      "Yongkai Wu",
      "Lu Zhang",
      "Xintao Wu"
    ],
    "author_ids": [],
    "abstract": "Fair machine learning is receiving an increasing attention in machine\nlearning fields. Researchers in fair learning have developed correlation or\nassociation-based measures such as demographic disparity, mistreatment\ndisparity, calibration, causal-based measures such as total effect, direct and\nindirect discrimination, and counterfactual fairness, and fairness notions such\nas equality of opportunity and equal odds that consider both decisions in the\ntraining data and decisions made by predictive models. In this paper, we\ndevelop a new causal-based fairness notation, called equality of effort.\nDifferent from existing fairness notions which mainly focus on discovering the\ndisparity of decisions between two groups of individuals, the proposed equality\nof effort notation helps answer questions like to what extend a legitimate\nvariable should change to make a particular individual achieve a certain\noutcome level and addresses the concerns whether the efforts made to achieve\nthe same outcome level for individuals from the protected group and that from\nthe unprotected group are different. We develop algorithms for determining\nwhether an individual or a group of individuals is discriminated in terms of\nequality of effort. We also develop an optimization-based method for removing\ndiscriminatory effects from the data if discrimination is detected. We conduct\nempirical evaluations to compare the equality of effort and existing fairness\nnotion and show the effectiveness of our proposed algorithms.",
    "published_date": "2019-11-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08292v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.04322v1",
    "title": "Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness",
    "authors": [
      "Zhu Li",
      "Adrian Perez-Suay",
      "Gustau Camps-Valls",
      "Dino Sejdinovic"
    ],
    "author_ids": [],
    "abstract": "Current adoption of machine learning in industrial, societal and economical\nactivities has raised concerns about the fairness, equity and ethics of\nautomated decisions. Predictive models are often developed using biased\ndatasets and thus retain or even exacerbate biases in their decisions and\nrecommendations. Removing the sensitive covariates, such as gender or race, is\ninsufficient to remedy this issue since the biases may be retained due to other\nrelated covariates. We present a regularization approach to this problem that\ntrades off predictive accuracy of the learned models (with respect to biased\nlabels) for the fairness in terms of statistical parity, i.e. independence of\nthe decisions from the sensitive covariates. In particular, we consider a\ngeneral framework of regularized empirical risk minimization over reproducing\nkernel Hilbert spaces and impose an additional regularizer of dependence\nbetween predictors and sensitive covariates using kernel-based measures of\ndependence, namely the Hilbert-Schmidt Independence Criterion (HSIC) and its\nnormalized version. This approach leads to a closed-form solution in the case\nof squared loss, i.e. ridge regression. Moreover, we show that the dependence\nregularizer has an interpretation as modifying the corresponding Gaussian\nprocess (GP) prior. As a consequence, a GP model with a prior that encourages\nfairness to sensitive variables can be derived, allowing principled\nhyperparameter selection and studying of the relative relevance of covariates\nunder fairness constraints. Experimental results in synthetic examples and in\nreal problems of income and crime prediction illustrate the potential of the\napproach to improve fairness of automated decisions.",
    "published_date": "2019-11-11T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04322v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03949v1",
    "title": "Interpretable Multiple-Kernel Prototype Learning for Discriminative Representation and Feature Selection",
    "authors": [
      "Babak Hosseini",
      "Barbara Hammer"
    ],
    "author_ids": [],
    "abstract": "Prototype-based methods are of the particular interest for domain specialists\nand practitioners as they summarize a dataset by a small set of\nrepresentatives. Therefore, in a classification setting, interpretability of\nthe prototypes is as significant as the prediction accuracy of the algorithm.\nNevertheless, the state-of-the-art methods make inefficient trade-offs between\nthese concerns by sacrificing one in favor of the other, especially if the\ngiven data has a kernel-based representation. In this paper, we propose a novel\ninterpretable multiple-kernel prototype learning (IMKPL) to construct highly\ninterpretable prototypes in the feature space, which are also efficient for the\ndiscriminative representation of the data. Our method focuses on the local\ndiscrimination of the classes in the feature space and shaping the prototypes\nbased on condensed class-homogeneous neighborhoods of data. Besides, IMKPL\nlearns a combined embedding in the feature space in which the above objectives\nare better fulfilled. When the base kernels coincide with the data dimensions,\nthis embedding results in a discriminative features selection. We evaluate\nIMKPL on several benchmarks from different domains which demonstrate its\nsuperiority to the related state-of-the-art methods regarding both\ninterpretability and discriminative representation.",
    "published_date": "2019-11-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03949v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03891v3",
    "title": "Social Bias Frames: Reasoning about Social and Power Implications of Language",
    "authors": [
      "Maarten Sap",
      "Saadia Gabriel",
      "Lianhui Qin",
      "Dan Jurafsky",
      "Noah A. Smith",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "Warning: this paper contains content that may be offensive or upsetting.\n  Language has the power to reinforce stereotypes and project social biases\nonto others. At the core of the challenge is that it is rarely what is stated\nexplicitly, but rather the implied meanings, that frame people's judgments\nabout others. For example, given a statement that \"we shouldn't lower our\nstandards to hire more women,\" most listeners will infer the implicature\nintended by the speaker -- that \"women (candidates) are less qualified.\" Most\nsemantic formalisms, to date, do not capture such pragmatic implications in\nwhich people express social biases and power differentials in language.\n  We introduce Social Bias Frames, a new conceptual formalism that aims to\nmodel the pragmatic frames in which people project social biases and\nstereotypes onto others. In addition, we introduce the Social Bias Inference\nCorpus to support large-scale modelling and evaluation with 150k structured\nannotations of social media posts, covering over 34k implications about a\nthousand demographic groups.\n  We then establish baseline approaches that learn to recover Social Bias\nFrames from unstructured text. We find that while state-of-the-art neural\nmodels are effective at high-level categorization of whether a given statement\nprojects unwanted social bias (80% F1), they are not effective at spelling out\nmore detailed explanations in terms of Social Bias Frames. Our study motivates\nfuture work that combines structured pragmatic inference with commonsense\nreasoning on social implications.",
    "published_date": "2019-11-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03891v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03855v4",
    "title": "Correcting Sociodemographic Selection Biases for Population Prediction from Social Media",
    "authors": [
      "Salvatore Giorgi",
      "Veronica Lynn",
      "Keshav Gupta",
      "Farhan Ahmed",
      "Sandra Matz",
      "Lyle Ungar",
      "H. Andrew Schwartz"
    ],
    "author_ids": [],
    "abstract": "Social media is increasingly used for large-scale population predictions,\nsuch as estimating community health statistics. However, social media users are\nnot typically a representative sample of the intended population -- a\n\"selection bias\". Within the social sciences, such a bias is typically\naddressed with restratification techniques, where observations are reweighted\naccording to how under- or over-sampled their socio-demographic groups are.\nYet, restratifaction is rarely evaluated for improving prediction. In this\ntwo-part study, we first evaluate standard, \"out-of-the-box\" restratification\ntechniques, finding they provide no improvement and often even degraded\nprediction accuracies across four tasks of esimating U.S. county population\nhealth statistics from Twitter. The core reasons for degraded performance seem\nto be tied to their reliance on either sparse or shrunken estimates of each\npopulation's socio-demographics. In the second part of our study, we develop\nand evaluate Robust Poststratification, which consists of three methods to\naddress these problems: (1) estimator redistribution to account for shrinking,\nas well as (2) adaptive binning and (3) informed smoothing to handle sparse\nsocio-demographic estimates. We show that each of these methods leads to\nsignificant improvement in prediction accuracies over the standard\nrestratification approaches. Taken together, Robust Poststratification enables\nstate-of-the-art prediction accuracies, yielding a 53.0% increase in variance\nexplained (R^2) in the case of surveyed life satisfaction, and a 17.8% average\nincrease across all tasks.",
    "published_date": "2019-11-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03855v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03842v2",
    "title": "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation",
    "authors": [
      "Emily Dinan",
      "Angela Fan",
      "Adina Williams",
      "Jack Urbanek",
      "Douwe Kiela",
      "Jason Weston"
    ],
    "author_ids": [],
    "abstract": "Models often easily learn biases present in the training data, and their\npredictions directly reflect this bias. We analyze gender bias in dialogue\ndata, and examine how this bias is actually amplified in subsequent generative\nchit-chat dialogue models. We measure gender bias in six existing dialogue\ndatasets, and focus on the most biased one, the multi-player text-based fantasy\nadventure dataset LIGHT, as a testbed for our bias mitigation techniques. The\nLIGHT dataset is highly imbalanced with respect to gender, containing\npredominantly male characters, likely because it is entirely collected by\ncrowdworkers and reflects common biases that exist in fantasy or medieval\nsettings. We consider three techniques to mitigate gender bias: counterfactual\ndata augmentation, targeted data collection, and bias controlled training. We\nshow that our proposed techniques mitigate gender bias in LIGHT by balancing\nthe genderedness of generated dialogue utterances and are particularly\neffective in combination. We quantify performance using various evaluation\nmethods---such as quantity of gendered words, a dialogue safety classifier, and\nhuman studies---all of which show that our models generate less gendered, but\nequally engaging chit-chat responses.",
    "published_date": "2019-11-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03842v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1912.11078v2",
    "title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview",
    "authors": [
      "Deven Shah",
      "H. Andrew Schwartz",
      "Dirk Hovy"
    ],
    "author_ids": [],
    "abstract": "An increasing number of works in natural language processing have addressed\nthe effect of bias on the predicted outcomes, introducing mitigation techniques\nthat act on different parts of the standard NLP pipeline (data and models).\nHowever, these works have been conducted in isolation, without a unifying\nframework to organize efforts within the field. This leads to repetitive\napproaches, and puts an undue focus on the effects of bias, rather than on\ntheir origins. Research focused on bias symptoms rather than the underlying\norigins could limit the development of effective countermeasures. In this\npaper, we propose a unifying conceptualization: the predictive bias framework\nfor NLP. We summarize the NLP literature and propose a general mathematical\ndefinition of predictive bias in NLP along with a conceptual framework,\ndifferentiating four main origins of biases: label bias, selection bias, model\noveramplification, and semantic bias. We discuss how past work has countered\neach bias origin. Our framework serves to guide an introductory overview of\npredictive bias in NLP, integrating existing work into a single structure and\nopening avenues for future research.",
    "published_date": "2019-11-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.11078v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.04964v1",
    "title": "The Bias-Expressivity Trade-off",
    "authors": [
      "Julius Lauw",
      "Dominique Macias",
      "Akshay Trikha",
      "Julia Vendemiatti",
      "George D. Montanez"
    ],
    "author_ids": [],
    "abstract": "Learning algorithms need bias to generalize and perform better than random\nguessing. We examine the flexibility (expressivity) of biased algorithms. An\nexpressive algorithm can adapt to changing training data, altering its outcome\nbased on changes in its input. We measure expressivity by using an\ninformation-theoretic notion of entropy on algorithm outcome distributions,\ndemonstrating a trade-off between bias and expressivity. To the degree an\nalgorithm is biased is the degree to which it can outperform uniform random\nsampling, but is also the degree to which is becomes inflexible. We derive\nbounds relating bias to expressivity, proving the necessary trade-offs inherent\nin trying to create strongly performing yet flexible algorithms.",
    "published_date": "2019-11-09T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04964v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03674v1",
    "title": "Preservation of Anomalous Subgroups On Machine Learning Transformed Data",
    "authors": [
      "Samuel C. Maina",
      "Reginald E. Bryant",
      "William O. Goal",
      "Robert-Florian Samoilescu",
      "Kush R. Varshney",
      "Komminist Weldemariam"
    ],
    "author_ids": [],
    "abstract": "In this paper, we investigate the effect of machine learning based\nanonymization on anomalous subgroup preservation. In particular, we train a\nbinary classifier to discover the most anomalous subgroup in a dataset by\nmaximizing the bias between the group's predicted odds ratio from the model and\nobserved odds ratio from the data. We then perform anonymization using a\nvariational autoencoder (VAE) to synthesize an entirely new dataset that would\nideally be drawn from the distribution of the original data. We repeat the\nanomalous subgroup discovery task on the new data and compare it to what was\nidentified pre-anonymization. We evaluated our approach using publicly\navailable datasets from the financial industry. Our evaluation confirmed that\nthe approach was able to produce synthetic datasets that preserved a high level\nof subgroup differentiation as identified initially in the original dataset.\nSuch a distinction was maintained while having distinctly different records\nbetween the synthetic and original dataset. Finally, we packed the above end to\nend process into what we call Utility Guaranteed Deep Privacy (UGDP) system.\nUGDP can be easily extended to onboard alternative generative approaches such\nas GANs to synthesize tabular data.",
    "published_date": "2019-11-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03674v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03642v3",
    "title": "Towards Understanding Gender Bias in Relation Extraction",
    "authors": [
      "Andrew Gaut",
      "Tony Sun",
      "Shirlyn Tang",
      "Yuxin Huang",
      "Jing Qian",
      "Mai ElSherief",
      "Jieyu Zhao",
      "Diba Mirza",
      "Elizabeth Belding",
      "Kai-Wei Chang",
      "William Yang Wang"
    ],
    "author_ids": [],
    "abstract": "Recent developments in Neural Relation Extraction (NRE) have made significant\nstrides towards Automated Knowledge Base Construction (AKBC). While much\nattention has been dedicated towards improvements in accuracy, there have been\nno attempts in the literature to our knowledge to evaluate social biases in NRE\nsystems. We create WikiGenderBias, a distantly supervised dataset with a human\nannotated test set. WikiGenderBias has sentences specifically curated to\nanalyze gender bias in relation extraction systems. We use WikiGenderBias to\nevaluate systems for bias and find that NRE systems exhibit gender biased\npredictions and lay groundwork for future evaluation of bias in NRE. We also\nanalyze how name anonymization, hard debiasing for word embeddings, and\ncounterfactual data augmentation affect gender bias in predictions and\nperformance.",
    "published_date": "2019-11-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03642v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03624v1",
    "title": "Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination",
    "authors": [
      "Jae Woong Soh",
      "Gu Yong Park",
      "Junho Jo",
      "Nam Ik Cho"
    ],
    "author_ids": [],
    "abstract": "Recently, many convolutional neural networks for single image\nsuper-resolution (SISR) have been proposed, which focus on reconstructing the\nhigh-resolution images in terms of objective distortion measures. However, the\nnetworks trained with objective loss functions generally fail to reconstruct\nthe realistic fine textures and details that are essential for better\nperceptual quality. Recovering the realistic details remains a challenging\nproblem, and only a few works have been proposed which aim at increasing the\nperceptual quality by generating enhanced textures. However, the generated fake\ndetails often make undesirable artifacts and the overall image looks somewhat\nunnatural. Therefore, in this paper, we present a new approach to\nreconstructing realistic super-resolved images with high perceptual quality,\nwhile maintaining the naturalness of the result. In particular, we focus on the\ndomain prior properties of SISR problem. Specifically, we define the\nnaturalness prior in the low-level domain and constrain the output image in the\nnatural manifold, which eventually generates more natural and realistic images.\nOur results show better naturalness compared to the recent super-resolution\nalgorithms including perception-oriented ones.",
    "published_date": "2019-11-09T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03623v1",
    "title": "Analyzing Bias in Sensitive Personal Information Used to Train Financial Models",
    "authors": [
      "Reginald Bryant",
      "Celia Cintas",
      "Isaac Wambugu",
      "Andrew Kinai",
      "Komminist Weldemariam"
    ],
    "author_ids": [],
    "abstract": "Bias in data can have unintended consequences that propagate to the design,\ndevelopment, and deployment of machine learning models. In the financial\nservices sector, this can result in discrimination from certain financial\ninstruments and services. At the same time, data privacy is of paramount\nimportance, and recent data breaches have seen reputational damage for large\ninstitutions. Presented in this paper is a trusted model-lifecycle management\nplatform that attempts to ensure consumer data protection, anonymization, and\nfairness. Specifically, we examine how datasets can be reproduced using deep\nlearning techniques to effectively retain important statistical features in\ndatasets whilst simultaneously protecting data privacy and enabling safe and\nsecure sharing of sensitive personal information beyond the current\nstate-of-practice.",
    "published_date": "2019-11-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03623v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05755v1",
    "title": "An Introduction to Artificial Intelligence and Solutions to the Problems of Algorithmic Discrimination",
    "authors": [
      "Nicholas Schmidt",
      "Bryce Stephens"
    ],
    "author_ids": [],
    "abstract": "There is substantial evidence that Artificial Intelligence (AI) and Machine\nLearning (ML) algorithms can generate bias against minorities, women, and other\nprotected classes. Federal and state laws have been enacted to protect\nconsumers from discrimination in credit, housing, and employment, where\nregulators and agencies are tasked with enforcing these laws. Additionally,\nthere are laws in place to ensure that consumers understand why they are denied\naccess to services and products, such as consumer loans. In this article, we\nprovide an overview of the potential benefits and risks associated with the use\nof algorithms and data, and focus specifically on fairness. While our\nobservations generalize to many contexts, we focus on the fairness concerns\nraised in consumer credit and the legal requirements of the Equal Credit and\nOpportunity Act. We propose a methodology for evaluating algorithmic fairness\nand minimizing algorithmic bias that aligns with the provisions of federal and\nstate anti-discrimination statutes that outlaw overt, disparate treatment, and,\nspecifically, disparate impact discrimination. We argue that while the use of\nAI and ML algorithms heighten potential discrimination risks, these risks can\nbe evaluated and mitigated, but doing so requires a deep understanding of these\nalgorithms and the contexts and domains in which they are being used.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG",
      "68T01",
      "K.5.2; K.4.1; K.4.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05755v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03562v1",
    "title": "The State of NLP Literature: A Diachronic Analysis of the ACL Anthology",
    "authors": [
      "Saif M. Mohammad"
    ],
    "author_ids": [],
    "abstract": "The ACL Anthology (AA) is a digital repository of tens of thousands of\narticles on Natural Language Processing (NLP). This paper examines the\nliterature as a whole to identify broad trends in productivity, focus, and\nimpact. It presents the analyses in a sequence of questions and answers. The\ngoal is to record the state of the AA literature: who and how many of us are\npublishing? what are we publishing on? where and in what form are we\npublishing? and what is the impact of our publications? The answers are usually\nin the form of numbers, graphs, and inter-connected visualizations. Special\nemphasis is laid on the demographics and inclusiveness of NLP publishing.\nNotably, we find that only about 30% of first authors are female, and that this\npercentage has not improved since the year 2000. We also show that, on average,\nfemale first authors are cited less than male first authors, even when\ncontrolling for experience. We hope that recording citation and participation\ngaps across demographic groups will encourage more inclusiveness and fairness\nin research.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03385v1",
    "title": "Low-Level Linguistic Controls for Style Transfer and Content Preservation",
    "authors": [
      "Katy Gero",
      "Chris Kedzie",
      "Jonathan Reeve",
      "Lydia Chilton"
    ],
    "author_ids": [],
    "abstract": "Despite the success of style transfer in image processing, it has seen\nlimited progress in natural language generation. Part of the problem is that\ncontent is not as easily decoupled from style in the text domain. Curiously, in\nthe field of stylometry, content does not figure prominently in practical\nmethods of discriminating stylistic elements, such as authorship and genre.\nRather, syntax and function words are the most salient features. Drawing on\nthis work, we model style as a suite of low-level linguistic controls, such as\nfrequency of pronouns, prepositions, and subordinate clause constructions. We\ntrain a neural encoder-decoder model to reconstruct reference sentences given\nonly content words and the setting of the controls. We perform style transfer\nby keeping the content words fixed while adjusting the controls to be\nindicative of another style. In experiments, we show that the model reliably\nresponds to the linguistic controls and perform both automatic and manual\nevaluations on style transfer. We find we can fool a style classifier 84% of\nthe time, and that our model produces highly diverse and stylistically\ndistinctive outputs. This work introduces a formal, extendable model of style\nthat can add control to any neural text generation system.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03385v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03379v1",
    "title": "Does the use of open, non-anonymous peer review in scholarly publishing introduce bias? Evidence from the F1000 post-publication open peer review publishing model",
    "authors": [
      "Mike Thelwall",
      "Verena Weigert",
      "Liz Allen",
      "Zena Nyakoojo",
      "Eleanor-Rose Papas"
    ],
    "author_ids": [],
    "abstract": "This study examines whether there is any evidence of bias in two areas of\ncommon critique of open, non-anonymous peer review - and used in the\npost-publication, peer review system operated by the open-access scholarly\npublishing platform F1000Research. First, is there evidence of bias where a\nreviewer based in a specific country assesses the work of an author also based\nin the same country? Second, are reviewers influenced by being able to see the\ncomments and know the origins of previous reviewer? Methods: Scrutinising the\nopen peer review comments published on F1000Research, we assess the extent of\ntwo frequently cited potential influences on reviewers that may be the result\nof the transparency offered by a fully attributable, open peer review\npublishing model: the national affiliations of authors and reviewers, and the\nability of reviewers to view previously-published reviewer reports before\nsubmitting their own. The effects of these potential influences were\ninvestigated for all first versions of articles published by 8 July 2019 to\nF1000Research. In 16 out of the 20 countries with the most articles, there was\na tendency for reviewers based in the same country to give a more positive\nreview. The difference was statistically significant in one. Only 3 countries\nhad the reverse tendency. Second, there is no evidence of a conformity bias.\nWhen reviewers mentioned a previous review in their peer review report, they\nwere not more likely to give the same overall judgement. Although reviewers who\nhad longer to potentially read a previously published reviewer reports were\nslightly less likely to agree with previous reviewer judgements, this could be\ndue to these articles being difficult to judge rather than deliberate\nnon-conformity.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03379v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.03216v1",
    "title": "AI Ethics for Systemic Issues: A Structural Approach",
    "authors": [
      "Agnes Schim van der Loeff",
      "Iggy Bassi",
      "Sachin Kapila",
      "Jevgenij Gamper"
    ],
    "author_ids": [],
    "abstract": "The debate on AI ethics largely focuses on technical improvements and\nstronger regulation to prevent accidents or misuse of AI, with solutions\nrelying on holding individual actors accountable for responsible AI\ndevelopment. While useful and necessary, we argue that this \"agency\" approach\ndisregards more indirect and complex risks resulting from AI's interaction with\nthe socio-economic and political context. This paper calls for a \"structural\"\napproach to assessing AI's effects in order to understand and prevent such\nsystemic risks where no individual can be held accountable for the broader\nnegative impacts. This is particularly relevant for AI applied to systemic\nissues such as climate change and food security which require political\nsolutions and global cooperation. To properly address the wide range of AI\nrisks and ensure 'AI for social good', agency-focused policies must be\ncomplemented by policies informed by a structural approach.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03216v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03122v2",
    "title": "Promptness and Bounded Fairness in Concurrent and Parameterized Systems",
    "authors": [
      "Swen Jacobs",
      "Mouhammad Sakr",
      "Martin Zimmermann"
    ],
    "author_ids": [],
    "abstract": "We investigate the satisfaction of specifications in Prompt Linear Temporal\nLogic (Prompt-LTL) by concurrent systems. Prompt-LTL is an extension of LTL\nthat allows to specify parametric bounds on the satisfaction of eventualities,\nthus adding a quantitative aspect to the specification language. We establish a\nconnection between bounded fairness, bounded stutter equivalence, and the\nsatisfaction of Prompt-LTL\\X formulas. Based on this connection, we prove the\nfirst cutoff results for different classes of systems with a parametric number\nof components and quantitative specifications, thereby identifying previously\nunknown decidable fragments of the parameterized model checking problem.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LO",
      "cs.FL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03122v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.03105v2",
    "title": "Unified Sample-Optimal Property Estimation in Near-Linear Time",
    "authors": [
      "Yi Hao",
      "Alon Orlitsky"
    ],
    "author_ids": [],
    "abstract": "We consider the fundamental learning problem of estimating properties of\ndistributions over large domains. Using a novel piecewise-polynomial\napproximation technique, we derive the first unified methodology for\nconstructing sample- and time-efficient estimators for all sufficiently smooth,\nsymmetric and non-symmetric, additive properties. This technique yields\nnear-linear-time computable estimators whose approximation values are\nasymptotically optimal and highly-concentrated, resulting in the first: 1)\nestimators achieving the $\\mathcal{O}(k/(\\varepsilon^2\\log k))$ min-max\n$\\varepsilon$-error sample complexity for all $k$-symbol Lipschitz properties;\n2) unified near-optimal differentially private estimators for a variety of\nproperties; 3) unified estimator achieving optimal bias and near-optimal\nvariance for five important properties; 4) near-optimal sample-complexity\nestimators for several important symmetric properties over both domain sizes\nand confidence levels. In addition, we establish a McDiarmid's inequality under\nPoisson sampling, which is of independent interest.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03105v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03064v3",
    "title": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation",
    "authors": [
      "Po-Sen Huang",
      "Huan Zhang",
      "Ray Jiang",
      "Robert Stanforth",
      "Johannes Welbl",
      "Jack Rae",
      "Vishal Maini",
      "Dani Yogatama",
      "Pushmeet Kohli"
    ],
    "author_ids": [],
    "abstract": "Advances in language modeling architectures and the availability of large\ntext corpora have driven progress in automatic text generation. While this\nresults in models capable of generating coherent texts, it also prompts models\nto internalize social biases present in the training corpus. This paper aims to\nquantify and reduce a particular type of bias exhibited by language models:\nbias in the sentiment of generated text. Given a conditioning context (e.g., a\nwriting prompt) and a language model, we analyze if (and how) the sentiment of\nthe generated text is affected by changes in values of sensitive attributes\n(e.g., country names, occupations, genders) in the conditioning context using a\nform of counterfactual evaluation. We quantify sentiment bias by adopting\nindividual and group fairness metrics from the fair machine learning\nliterature, and demonstrate that large-scale models trained on two different\ncorpora (news articles, and Wikipedia) exhibit considerable levels of bias. We\nthen propose embedding and sentiment prediction-derived regularization on the\nlanguage model's latent representations. The regularizations improve fairness\nmetrics while retaining comparable levels of perplexity and semantic\nsimilarity.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03064v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03020v2",
    "title": "A Human-in-the-loop Framework to Construct Context-aware Mathematical Notions of Outcome Fairness",
    "authors": [
      "Mohammad Yaghini",
      "Andreas Krause",
      "Hoda Heidari"
    ],
    "author_ids": [],
    "abstract": "Existing mathematical notions of fairness fail to account for the context of\ndecision-making. We argue that moral consideration of contextual factors is an\ninherently human task. So we present a framework to learn context-aware\nmathematical formulations of fairness by eliciting people's situated fairness\nassessments. Our family of fairness notions corresponds to a new interpretation\nof economic models of Equality of Opportunity (EOP), and it includes most\nexisting notions of fairness as special cases. Our human-in-the-loop approach\nis designed to learn the appropriate parameters of the EOP family by utilizing\nhuman responses to pair-wise questions about decision subjects' circumstance\nand deservingness, and the harm/benefit imposed on them. We illustrate our\nframework in a hypothetical criminal risk assessment scenario by conducting a\nseries of human-subject experiments on Amazon Mechanical Turk. Our work takes\nan important initial step toward empowering stakeholders to have a voice in the\nformulation of fairness for Machine Learning.",
    "published_date": "2019-11-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03020v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.02969v2",
    "title": "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance",
    "authors": [
      "R. Thomas McCoy",
      "Junghyun Min",
      "Tal Linzen"
    ],
    "author_ids": [],
    "abstract": "If the same neural network architecture is trained multiple times on the same\ndataset, will it make similar linguistic generalizations across runs? To study\nthis question, we fine-tuned 100 instances of BERT on the Multi-genre Natural\nLanguage Inference (MNLI) dataset and evaluated them on the HANS dataset, which\nevaluates syntactic generalization in natural language inference. On the MNLI\ndevelopment set, the behavior of all instances was remarkably consistent, with\naccuracy ranging between 83.6% and 84.8%. In stark contrast, the same models\nvaried widely in their generalization performance. For example, on the simple\ncase of subject-object swap (e.g., determining that \"the doctor visited the\nlawyer\" does not entail \"the lawyer visited the doctor\"), accuracy ranged from\n0.00% to 66.2%. Such variation is likely due to the presence of many local\nminima that are equally attractive to a low-bias learner such as a neural\nnetwork; decreasing the variability may therefore require models with stronger\ninductive biases.",
    "published_date": "2019-11-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02969v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.02768v4",
    "title": "Confidence Intervals for Policy Evaluation in Adaptive Experiments",
    "authors": [
      "Vitor Hadad",
      "David A. Hirshberg",
      "Ruohan Zhan",
      "Stefan Wager",
      "Susan Athey"
    ],
    "author_ids": [],
    "abstract": "Adaptive experiment designs can dramatically improve statistical efficiency\nin randomized trials, but they also complicate statistical inference. For\nexample, it is now well known that the sample mean is biased in adaptive\ntrials. Inferential challenges are exacerbated when our parameter of interest\ndiffers from the parameter the trial was designed to target, such as when we\nare interested in estimating the value of a sub-optimal treatment after running\na trial to determine the optimal treatment using a stochastic bandit design. In\nthis context, typical estimators that use inverse propensity weighting to\neliminate sampling bias can be problematic: their distributions become skewed\nand heavy-tailed as the propensity scores decay to zero. In this paper, we\npresent a class of estimators that overcome these issues. Our approach is to\nadaptively reweight the terms of an augmented inverse propensity weighting\nestimator to control the contribution of each term to the estimator's variance.\nThis adaptive weighting scheme prevents estimates from becoming heavy-tailed,\nensuring asymptotically correct coverage. It also reduces variance, allowing us\nto test hypotheses with greater power - especially hypotheses that were not\ntargeted by the experimental design. We validate the accuracy of the resulting\nestimates and their confidence intervals in numerical experiments and show our\nmethods compare favorably to existing alternatives in terms of RMSE and\ncoverage.",
    "published_date": "2019-11-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02768v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.02715v3",
    "title": "Fair Allocation through Selective Information Acquisition",
    "authors": [
      "William Cai",
      "Johann Gaebler",
      "Nikhil Garg",
      "Sharad Goel"
    ],
    "author_ids": [],
    "abstract": "Public and private institutions must often allocate scare resources under\nuncertainty. Banks, for example, extend credit to loan applicants based in part\non their estimated likelihood of repaying a loan. But when the quality of\ninformation differs across candidates (e.g., if some applicants lack\ntraditional credit histories), common lending strategies can lead to\ndisparities across groups. Here we consider a setting in which decision makers\n-- before allocating resources -- can choose to spend some of their limited\nbudget further screening select individuals. We present a computationally\nefficient algorithm for deciding whom to screen that maximizes a standard\nmeasure of social welfare. Intuitively, decision makers should screen\ncandidates on the margin, for whom the additional information could plausibly\nalter the allocation. We formalize this idea by showing the problem can be\nreduced to solving a series of linear programs. Both on synthetic and\nreal-world datasets, this strategy improves utility, illustrating the value of\ntargeted information acquisition in such decisions. Further, when there is\nsocial value for distributing resources to groups for whom we have a priori\npoor information -- like those without credit scores -- our approach can\nsubstantially improve the allocation of limited assets.",
    "published_date": "2019-11-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02715v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.04336v1",
    "title": "Fair Meta-Learning: Learning How to Learn Fairly",
    "authors": [
      "Dylan Slack",
      "Sorelle Friedler",
      "Emile Givental"
    ],
    "author_ids": [],
    "abstract": "Data sets for fairness relevant tasks can lack examples or be biased\naccording to a specific label in a sensitive attribute. We demonstrate the\nusefulness of weight based meta-learning approaches in such situations. For\nmodels that can be trained through gradient descent, we demonstrate that there\nare some parameter configurations that allow models to be optimized from a few\nnumber of gradient steps and with minimal data which are both fair and\naccurate. To learn such weight sets, we adapt the popular MAML algorithm to\nFair-MAML by the inclusion of a fairness regularization term. In practice,\nFair-MAML allows practitioners to train fair machine learning models from only\na few examples when data from related tasks is available. We empirically\nexhibit the value of this technique by comparing to relevant baselines.",
    "published_date": "2019-11-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.04336v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.02506v1",
    "title": "Algorithms and Adaptivity Gaps for Stochastic $k$-TSP",
    "authors": [
      "Haotian Jiang",
      "Jian Li",
      "Daogao Liu",
      "Sahil Singla"
    ],
    "author_ids": [],
    "abstract": "Given a metric $(V,d)$ and a $\\textsf{root} \\in V$, the classic\n$\\textsf{$k$-TSP}$ problem is to find a tour originating at the $\\textsf{root}$\nof minimum length that visits at least $k$ nodes in $V$. In this work,\nmotivated by applications where the input to an optimization problem is\nuncertain, we study two stochastic versions of $\\textsf{$k$-TSP}$.\n  In Stoch-Reward $k$-TSP, originally defined by Ene-Nagarajan-Saket [ENS17],\neach vertex $v$ in the given metric $(V,d)$ contains a stochastic reward $R_v$.\nThe goal is to adaptively find a tour of minimum expected length that collects\nat least reward $k$; here \"adaptively\" means our next decision may depend on\nprevious outcomes. Ene et al. give an $O(\\log k)$-approximation adaptive\nalgorithm for this problem, and left open if there is an $O(1)$-approximation\nalgorithm. We totally resolve their open question and even give an\n$O(1)$-approximation \\emph{non-adaptive} algorithm for this problem.\n  We also introduce and obtain similar results for the Stoch-Cost $k$-TSP\nproblem. In this problem each vertex $v$ has a stochastic cost $C_v$, and the\ngoal is to visit and select at least $k$ vertices to minimize the expected\n\\emph{sum} of tour length and cost of selected vertices. This problem\ngeneralizes the Price of Information framework [Singla18] from deterministic\nprobing costs to metric probing costs.\n  Our techniques are based on two crucial ideas: \"repetitions\" and \"critical\nscaling\". We show using Freedman's and Jogdeo-Samuels' inequalities that for\nour problems, if we truncate the random variables at an ideal threshold and\nrepeat, then their expected values form a good surrogate. Unfortunately, this\nideal threshold is adaptive as it depends on how far we are from achieving our\ntarget $k$, so we truncate at various different scales and identify a\n\"critical\" scale.",
    "published_date": "2019-11-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02506v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.02471v1",
    "title": "Designing Evaluations of Machine Learning Models for Subjective Inference: The Case of Sentence Toxicity",
    "authors": [
      "Agathe Balayn",
      "Alessandro Bozzon"
    ],
    "author_ids": [],
    "abstract": "Machine Learning (ML) is increasingly applied in real-life scenarios, raising\nconcerns about bias in automatic decision making. We focus on bias as a notion\nof opinion exclusion, that stems from the direct application of traditional ML\npipelines to infer subjective properties. We argue that such ML systems should\nbe evaluated with subjectivity and bias in mind. Considering the lack of\nevaluation standards yet to create evaluation benchmarks, we propose an initial\nlist of specifications to define prior to creating evaluation datasets, in\norder to later accurately evaluate the biases. With the example of a sentence\ntoxicity inference system, we illustrate how the specifications support the\nanalysis of biases related to subjectivity. We highlight difficulties in\ninstantiating these specifications and list future work for the crowdsourcing\ncommunity to help the creation of appropriate evaluation datasets.",
    "published_date": "2019-11-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02471v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.02278v1",
    "title": "Optimization with soft Dice can lead to a volumetric bias",
    "authors": [
      "Jeroen Bertels",
      "David Robben",
      "Dirk Vandermeulen",
      "Paul Suetens"
    ],
    "author_ids": [],
    "abstract": "Segmentation is a fundamental task in medical image analysis. The clinical\ninterest is often to measure the volume of a structure. To evaluate and compare\nsegmentation methods, the similarity between a segmentation and a predefined\nground truth is measured using metrics such as the Dice score. Recent\nsegmentation methods based on convolutional neural networks use a\ndifferentiable surrogate of the Dice score, such as soft Dice, explicitly as\nthe loss function during the learning phase. Even though this approach leads to\nimproved Dice scores, we find that, both theoretically and empirically on four\nmedical tasks, it can introduce a volumetric bias for tasks with high inherent\nuncertainty. As such, this may limit the method's clinical applicability.",
    "published_date": "2019-11-06T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02278v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.01917v1",
    "title": "Scenarios and Recommendations for Ethical Interpretive AI",
    "authors": [
      "John Licato",
      "Zaid Marji",
      "Sophia Abraham"
    ],
    "author_ids": [],
    "abstract": "Artificially intelligent systems, given a set of non-trivial ethical rules to\nfollow, will inevitably be faced with scenarios which call into question the\nscope of those rules. In such cases, human reasoners typically will engage in\ninterpretive reasoning, where interpretive arguments are used to support or\nattack claims that some rule should be understood a certain way. Artificially\nintelligent reasoners, however, currently lack the ability to carry out\nhuman-like interpretive reasoning, and we argue that bridging this gulf is of\ntremendous importance to human-centered AI. In order to better understand how\nfuture artificial reasoners capable of human-like interpretive reasoning must\nbe developed, we have collected a dataset of ethical rules, scenarios designed\nto invoke interpretive reasoning, and interpretations of those scenarios. We\nperform a qualitative analysis of our dataset, and summarize our findings in\nthe form of practical recommendations.",
    "published_date": "2019-11-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.01917v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.01916v4",
    "title": "Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems",
    "authors": [
      "Xuezhi Wang",
      "Nithum Thain",
      "Anu Sinha",
      "Flavien Prost",
      "Ed H. Chi",
      "Jilin Chen",
      "Alex Beutel"
    ],
    "author_ids": [],
    "abstract": "How can we build recommender systems to take into account fairness?\nReal-world recommender systems are often composed of multiple models, built by\nmultiple teams. However, most research on fairness focuses on improving\nfairness in a single model. Further, recent research on classification fairness\nhas shown that combining multiple \"fair\" classifiers can still result in an\n\"unfair\" classification system. This presents a significant challenge: how do\nwe understand and improve fairness in recommender systems composed of multiple\ncomponents?\n  In this paper, we study the compositionality of recommender fairness. We\nconsider two recently proposed fairness ranking metrics: equality of exposure\nand pairwise ranking accuracy. While we show that fairness in recommendation is\nnot guaranteed to compose, we provide theory for a set of conditions under\nwhich fairness of individual models does compose. We then present an analytical\nframework for both understanding whether a real system's signals can achieve\ncompositional fairness, and improving which component would have the greatest\nimpact on the fairness of the overall system. In addition to the theoretical\nresults, we find on multiple datasets -- including a large-scale real-world\nrecommender system -- that the overall system's end-to-end fairness is largely\nachievable by improving fairness in individual components.",
    "published_date": "2019-11-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.01916v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.02479v1",
    "title": "Algorithms and Statistical Models for Scientific Discovery in the Petabyte Era",
    "authors": [
      "Brian Nord",
      "Andrew J. Connolly",
      "Jamie Kinney",
      "Jeremy Kubica",
      "Gautaum Narayan",
      "Joshua E. G. Peek",
      "Chad Schafer",
      "Erik J. Tollerud",
      "Camille Avestruz",
      "G. Jogesh Babu",
      "Simon Birrer",
      "Douglas Burke",
      "João Caldeira",
      "Douglas A. Caldwell",
      "Joleen K. Carlberg",
      "Yen-Chi Chen",
      "Chuanfei Dong",
      "Eric D. Feigelson",
      "V. Zach Golkhou",
      "Vinay Kashyap",
      "T. S. Li",
      "Thomas Loredo",
      "Luisa Lucie-Smith",
      "Kaisey S. Mandel",
      "J. R. Martínez-Galarza",
      "Adam A. Miller",
      "Priyamvada Natarajan",
      "Michelle Ntampaka",
      "Andy Ptak",
      "David Rapetti",
      "Lior Shamir",
      "Aneta Siemiginowska",
      "Brigitta M. Sipőcz",
      "Arfon M. Smith",
      "Nhan Tran",
      "Ricardo Vilalta",
      "Lucianne M. Walkowicz",
      "John ZuHone"
    ],
    "author_ids": [],
    "abstract": "The field of astronomy has arrived at a turning point in terms of size and\ncomplexity of both datasets and scientific collaboration. Commensurately,\nalgorithms and statistical models have begun to adapt --- e.g., via the onset\nof artificial intelligence --- which itself presents new challenges and\nopportunities for growth. This white paper aims to offer guidance and ideas for\nhow we can evolve our technical and collaborative frameworks to promote\nefficient algorithmic development and take advantage of opportunities for\nscientific discovery in the petabyte era. We discuss challenges for discovery\nin large and complex data sets; challenges and requirements for the next stage\nof development of statistical methodologies and algorithmic tool sets; how we\nmight change our paradigms of collaboration and education; and the ethical\nimplications of scientists' contributions to widely applicable algorithms and\ncomputational modeling. We start with six distinct recommendations that are\nsupported by the commentary following them. This white paper is related to a\nlarger corpus of effort that has taken place within and around the Petabytes to\nScience Workshops (https://petabytestoscience.github.io/).",
    "published_date": "2019-11-05T00:00:00",
    "year": 2019,
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02479v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.01509v1",
    "title": "Understanding racial bias in health using the Medical Expenditure Panel Survey data",
    "authors": [
      "Moninder Singh",
      "Karthikeyan Natesan Ramamurthy"
    ],
    "author_ids": [],
    "abstract": "Over the years, several studies have demonstrated that there exist\nsignificant disparities in health indicators in the United States population\nacross various groups. Healthcare expense is used as a proxy for health in\nalgorithms that drive healthcare systems and this exacerbates the existing\nbias. In this work, we focus on the presence of racial bias in health\nindicators in the publicly available, and nationally representative Medical\nExpenditure Panel Survey (MEPS) data. We show that predictive models for care\nmanagement trained using this data inherit this bias. Finally, we demonstrate\nthat this inherited bias can be reduced significantly using simple mitigation\ntechniques.",
    "published_date": "2019-11-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.01509v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.01485v1",
    "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
    "authors": [
      "Yi Chern Tan",
      "L. Elisa Celis"
    ],
    "author_ids": [],
    "abstract": "Social bias in machine learning has drawn significant attention, with work\nranging from demonstrations of bias in a multitude of applications, curating\ndefinitions of fairness for different contexts, to developing algorithms to\nmitigate bias. In natural language processing, gender bias has been shown to\nexist in context-free word embeddings. Recently, contextual word\nrepresentations have outperformed word embeddings in several downstream NLP\ntasks. These word representations are conditioned on their context within a\nsentence, and can also be used to encode the entire sentence. In this paper, we\nanalyze the extent to which state-of-the-art models for contextual word\nrepresentations, such as BERT and GPT-2, encode biases with respect to gender,\nrace, and intersectional identities. Towards this, we propose assessing bias at\nthe contextual word level. This novel approach captures the contextual effects\nof bias missing in context-free word embeddings, yet avoids confounding effects\nthat underestimate bias at the sentence encoding level. We demonstrate evidence\nof bias at the corpus level, find varying evidence of bias in embedding\nassociation tests, show in particular that racial bias is strongly encoded in\ncontextual word models, and observe that bias effects for intersectional\nminorities are exacerbated beyond their constituent minority identities.\nFurther, evaluating bias effects at the contextual word level captures biases\nthat are not captured at the sentence level, confirming the need for our novel\napproach.",
    "published_date": "2019-11-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.01485v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.01468v2",
    "title": "Auditing and Achieving Intersectional Fairness in Classification Problems",
    "authors": [
      "Giulio Morina",
      "Viktoriia Oliinyk",
      "Julian Waton",
      "Ines Marusic",
      "Konstantinos Georgatzis"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms are extensively used to make increasingly more\nconsequential decisions about people, so achieving optimal predictive\nperformance can no longer be the only focus. A particularly important\nconsideration is fairness with respect to race, gender, or any other sensitive\nattribute. This paper studies intersectional fairness, where intersections of\nmultiple sensitive attributes are considered. Prior research has mainly focused\non fairness with respect to a single sensitive attribute, with intersectional\nfairness being comparatively less studied despite its critical importance for\nthe safety of modern machine learning systems. We present a comprehensive\nframework for auditing and achieving intersectional fairness in classification\nproblems: we define a suite of metrics to assess intersectional fairness in the\ndata or model outputs by extending known single-attribute fairness metrics, and\npropose methods for robustly estimating them even when some intersectional\nsubgroups are underrepresented. Furthermore, we develop post-processing\ntechniques to mitigate any detected intersectional bias in a classification\nmodel. Our techniques do not rely on any assumptions regarding the underlying\nmodel and preserve predictive performance at a guaranteed level of fairness.\nFinally, we give guidance on a practical implementation, showing how the\nproposed methods perform on a real-world dataset.",
    "published_date": "2019-11-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.01468v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05647v1",
    "title": "Long-range Event-level Prediction and Response Simulation for Urban Crime and Global Terrorism with Granger Networks",
    "authors": [
      "Timmy Li",
      "Yi Huang",
      "James Evans",
      "Ishanu Chattopadhyay"
    ],
    "author_ids": [],
    "abstract": "Large-scale trends in urban crime and global terrorism are well-predicted by\nsocio-economic drivers, but focused, event-level predictions have had limited\nsuccess. Standard machine learning approaches are promising, but lack\ninterpretability, are generally interpolative, and ineffective for precise\nfuture interventions with costly and wasteful false positives. Here, we are\nintroducing Granger Network inference as a new forecasting approach for\nindividual infractions with demonstrated performance far surpassing past\nresults, yet transparent enough to validate and extend social theory.\nConsidering the problem of predicting crime in the City of Chicago, we achieve\nan average AUC of ~90\\% for events predicted a week in advance within spatial\ntiles approximately $1000$ ft across. Instead of pre-supposing that crimes\nunfold across contiguous spaces akin to diffusive systems, we learn the local\ntransport rules from data. As our key insights, we uncover indications of\nsuburban bias -- how law-enforcement response is modulated by socio-economic\ncontexts with disproportionately negative impacts in the inner city -- and how\nthe dynamics of violent and property crimes co-evolve and constrain each other\n-- lending quantitative support to controversial pro-active policing policies.\nTo demonstrate broad applicability to spatio-temporal phenomena, we analyze\nterror attacks in the middle-east in the recent past, and achieve an AUC of\n~80% for predictions made a week in advance, and within spatial tiles measuring\napproximately 120 miles across. We conclude that while crime operates near an\nequilibrium quickly dissipating perturbations, terrorism does not. Indeed\nterrorism aims to destabilize social order, as shown by its dynamics being\nsusceptible to run-away increases in event rates under small perturbations.",
    "published_date": "2019-11-04T00:00:00",
    "year": 2019,
    "categories": [
      "stat.AP",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05647v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05731v1",
    "title": "Reporting on Decision-Making Algorithms and some Related Ethical Questions",
    "authors": [
      "Benoît Otjacques"
    ],
    "author_ids": [],
    "abstract": "Companies report on their financial performance for decades. More recently\nthey have also started to report on their environmental impact and their social\nresponsibility. The latest trend is now to deliver one single integrated report\nwhere all stakeholders of the company can easily connect all facets of the\nbusiness with their impact considered in a broad sense. The main purpose of\nthis integrated approach is to avoid delivering data related to disconnected\nsilos, which consequently makes it very difficult to globally assess the\noverall performance of an entity or a business line. In this paper, we focus on\nhow companies report on risks and ethical issues related to the increasing use\nof Artificial Intelligence (AI). We explain some of these risks and potential\nissues. Next, we identify some recent initiatives by various stakeholders to\ndefine a global ethical framework for AI. Finally, we illustrate with four\ncases that companies are very shy to report on these facets of AI.",
    "published_date": "2019-11-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00934v2",
    "title": "Finite-Sample Analysis of Decentralized Temporal-Difference Learning with Linear Function Approximation",
    "authors": [
      "Jun Sun",
      "Gang Wang",
      "Georgios B. Giannakis",
      "Qinmin Yang",
      "Zaiyue Yang"
    ],
    "author_ids": [],
    "abstract": "Motivated by the emerging use of multi-agent reinforcement learning (MARL) in\nengineering applications such as networked robotics, swarming drones, and\nsensor networks, we investigate the policy evaluation problem in a fully\ndecentralized setting, using temporal-difference (TD) learning with linear\nfunction approximation to handle large state spaces in practice. The goal of a\ngroup of agents is to collaboratively learn the value function of a given\npolicy from locally private rewards observed in a shared environment, through\nexchanging local estimates with neighbors. Despite their simplicity and\nwidespread use, our theoretical understanding of such decentralized TD learning\nalgorithms remains limited. Existing results were obtained based on i.i.d. data\nsamples, or by imposing an `additional' projection step to control the\n`gradient' bias incurred by the Markovian observations. In this paper, we\nprovide a finite-sample analysis of the fully decentralized TD(0) learning\nunder both i.i.d. as well as Markovian samples, and prove that all local\nestimates converge linearly to a small neighborhood of the optimum. The\nresultant error bounds are the first of its type---in the sense that they hold\nunder the most practical assumptions ---which is made possible by means of a\nnovel multi-step Lyapunov analysis.",
    "published_date": "2019-11-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00934v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00852v1",
    "title": "The Relationship between the Consistency of Users' Ratings and Recommendation Calibration",
    "authors": [
      "Masoud Mansoury",
      "Himan Abdollahpouri",
      "Joris Rombouts",
      "Mykola Pechenizkiy"
    ],
    "author_ids": [],
    "abstract": "Fairness in recommender systems has recently received attention from\nresearchers. Unfair recommendations have negative impact on the effectiveness\nof recommender systems as it may degrade users' satisfaction, loyalty, and at\nworst, it can lead to or perpetuate undesirable social dynamics. One of the\nfactors that may impact fairness is calibration, the degree to which users'\npreferences on various item categories are reflected in the recommendations\nthey receive.\n  The ability of a recommendation algorithm for generating effective\nrecommendations may depend on the meaningfulness of the input data and the\namount of information available in users' profile. In this paper, we aim to\nexplore the relationship between the consistency of users' ratings behavior and\nthe degree of calibrated recommendations they receive. We conduct our analysis\non different groups of users based on the consistency of their ratings. Our\nexperimental results on a movie dataset and several recommendation algorithms\nshow that there is a positive correlation between the consistency of users'\nratings behavior and the degree of calibration in their recommendations,\nmeaning that user groups with higher inconsistency in their ratings receive\nless calibrated recommendations.",
    "published_date": "2019-11-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00852v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.00786v3",
    "title": "Ethical Dilemmas in Strategic Games",
    "authors": [
      "Pavel Naumov",
      "Rui-Jie Yew"
    ],
    "author_ids": [],
    "abstract": "An agent, or a coalition of agents, faces an ethical dilemma between several\nstatements if she is forced to make a conscious choice between which of these\nstatements will be true. This paper proposes to capture ethical dilemmas as a\nmodality in strategic game settings with and without limit on sacrifice and for\nperfect and imperfect information games. The authors show that the dilemma\nmodality cannot be defined through the earlier proposed blameworthiness\nmodality. The main technical result is a sound and complete axiomatization of\nthe properties of this modality with sacrifice in games with perfect\ninformation.",
    "published_date": "2019-11-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00786v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00677v2",
    "title": "Fairness Violations and Mitigation under Covariate Shift",
    "authors": [
      "Harvineet Singh",
      "Rina Singh",
      "Vishwali Mhasawade",
      "Rumi Chunara"
    ],
    "author_ids": [],
    "abstract": "We study the problem of learning fair prediction models for unseen test sets\ndistributed differently from the train set. Stability against changes in data\ndistribution is an important mandate for responsible deployment of models. The\ndomain adaptation literature addresses this concern, albeit with the notion of\nstability limited to that of prediction accuracy. We identify sufficient\nconditions under which stable models, both in terms of prediction accuracy and\nfairness, can be learned. Using the causal graph describing the data and the\nanticipated shifts, we specify an approach based on feature selection that\nexploits conditional independencies in the data to estimate accuracy and\nfairness metrics for the test set. We show that for specific fairness\ndefinitions, the resulting model satisfies a form of worst-case optimality. In\ncontext of a healthcare task, we illustrate the advantages of the approach in\nmaking more equitable decisions.",
    "published_date": "2019-11-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00677v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00569v4",
    "title": "Mitigating the Effects of Non-Identifiability on Inference for Bayesian Neural Networks with Latent Variables",
    "authors": [
      "Yaniv Yacoby",
      "Weiwei Pan",
      "Finale Doshi-Velez"
    ],
    "author_ids": [],
    "abstract": "Bayesian Neural Networks with Latent Variables (BNN+LVs) capture predictive\nuncertainty by explicitly modeling model uncertainty (via priors on network\nweights) and environmental stochasticity (via a latent input noise variable).\nIn this work, we first show that BNN+LV suffers from a serious form of\nnon-identifiability: explanatory power can be transferred between the model\nparameters and latent variables while fitting the data equally well. We\ndemonstrate that as a result, in the limit of infinite data, the posterior mode\nover the network weights and latent variables is asymptotically biased away\nfrom the ground-truth. Due to this asymptotic bias, traditional inference\nmethods may in practice yield parameters that generalize poorly and misestimate\nuncertainty. Next, we develop a novel inference procedure that explicitly\nmitigates the effects of likelihood non-identifiability during training and\nyields high-quality predictions as well as uncertainty estimates. We\ndemonstrate that our inference method improves upon benchmark methods across a\nrange of synthetic and real data-sets.",
    "published_date": "2019-11-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00569v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00558v1",
    "title": "Prediction Modeling and Analysis for Telecom Customer Churn in Two Months",
    "authors": [
      "Lingling Yang",
      "Dongyang Li",
      "Yao Lu"
    ],
    "author_ids": [],
    "abstract": "A practical churn customer prediction model is critical to retain customers\nfor telecom companies in the saturated and competitive market. Previous studies\nfocus on predicting churn customers in current or next month, in which telecom\ncompanies don't have enough time to develop and carry out churn management\nstrategies. In this paper, we propose a new T+2 churn customer prediction\nmodel, in which the churn customers in two months are recognized and the\none-month window T+1 is reserved to carry out churn management strategies.\nHowever, the predictions for churn customers in two months are much more\ndifficult than in current or next month because of the weaker correlation\nbetween the customer information and churn states. Two characteristics of\ntelecom dataset, the discrimination between churn and non-churn customers is\ncomplicated and the class imbalance problem is serious, are observed. To\ndiscriminate the churn customers accurately, random forest (RF) classifier is\nchosen because RF solves the nonlinear separable problem with low bias and low\nvariance and handles high feature spaces and large number of training examples.\nTo overcome the imbalance problem, synthetic minority over-sampling with\nborderline or tomek link, in which the distribution of the samples remains and\nthe number of the training examples becomes larger, is applied. Overall, a\nprecision ratio of about 50% with a recall ratio of about 50% is achieved in\nthe T+2 churn prediction. The proposed prediction model provides an accurate\nand operable churn customer prediction model for telecom companies.",
    "published_date": "2019-11-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00558v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00461v1",
    "title": "On the Unintended Social Bias of Training Language Generation Models with Data from Local Media",
    "authors": [
      "Omar U. Florez"
    ],
    "author_ids": [],
    "abstract": "There are concerns that neural language models may preserve some of the\nstereotypes of the underlying societies that generate the large corpora needed\nto train these models. For example, gender bias is a significant problem when\ngenerating text, and its unintended memorization could impact the user\nexperience of many applications (e.g., the smart-compose feature in Gmail).\n  In this paper, we introduce a novel architecture that decouples the\nrepresentation learning of a neural model from its memory management role. This\narchitecture allows us to update a memory module with an equal ratio across\ngender types addressing biased correlations directly in the latent space. We\nexperimentally show that our approach can mitigate the gender bias\namplification in the automatic generation of articles news while providing\nsimilar perplexity values when extending the Sequence2Sequence architecture.",
    "published_date": "2019-11-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00461v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05489v1",
    "title": "Fair treatment allocations in social networks",
    "authors": [
      "James Atwood",
      "Hansa Srinivasan",
      "Yoni Halpern",
      "D Sculley"
    ],
    "author_ids": [],
    "abstract": "Simulations of infectious disease spread have long been used to understand\nhow epidemics evolve and how to effectively treat them. However, comparatively\nlittle attention has been paid to understanding the fairness implications of\ndifferent treatment strategies -- that is, how might such strategies distribute\nthe expected disease burden differentially across various subgroups or\ncommunities in the population? In this work, we define the precision disease\ncontrol problem -- the problem of optimally allocating vaccines in a social\nnetwork in a step-by-step fashion -- and we use the ML Fairness Gym to simulate\nepidemic control and study it from both an efficiency and fairness perspective.\nWe then present an exploratory analysis of several different environments and\ndiscuss the fairness implications of different treatment strategies.",
    "published_date": "2019-11-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05489v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00153v1",
    "title": "On hybrid precoder/combiner for downlink mmWave massive MU-MIMO systems",
    "authors": [
      "Alvaro Javier Ortega",
      "Raimundo Sampaio-Neto",
      "Rodrigo Pereira David"
    ],
    "author_ids": [],
    "abstract": "We propose four hybrid combiner/precoder for downlink mmWave massive MU-MIMO\nsystems. The design of a hybrid combiner/precoder is divided in two parts,\nanalog and digital. The system baseband model shows that the signal processed\nby the mobile station can be interpreted as a received signal in the presence\nof colored Gaussian noise, therefore, since the digital part of the combiner\nand precoder do not have constraints for their generation, their designs can be\nbased on any traditional signal processing that takes into account this kind of\nnoise. To the best of our knowledge, this was not considered by previous works.\nA more realistic and appropriate design is described in this paper. Also, the\napproaches adopted in the literature for the designing of the\ncombiner'/precoder' analog parts do not try to avoid or even reduce the inter\nuser/symbol interference, they concentrate on increasing the signal-to-noise\nratio (SNR). We propose a simple solution that decreases the interference while\nmaintaining large SNR. In addition, one of the proposed hybrid combiners\nreaches the maximum value of our objective function according with the\nHadamard's inequality. Numerical results illustrate the BER performance\nimprovements resulting from our proposals. In addition, a simple detection\napproach can be used for data estimation without significant performance loss.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00153v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.00147v1",
    "title": "Predicting the Politics of an Image Using Webly Supervised Data",
    "authors": [
      "Christopher Thomas",
      "Adriana Kovashka"
    ],
    "author_ids": [],
    "abstract": "The news media shape public opinion, and often, the visual bias they contain\nis evident for human observers. This bias can be inferred from how different\nmedia sources portray different subjects or topics. In this paper, we model\nvisual political bias in contemporary media sources at scale, using webly\nsupervised data. We collect a dataset of over one million unique images and\nassociated news articles from left- and right-leaning news sources, and develop\na method to predict the image's political leaning. This problem is particularly\nchallenging because of the enormous intra-class visual and semantic diversity\nof our data. We propose a two-stage method to tackle this problem. In the first\nstage, the model is forced to learn relevant visual concepts that, when joined\nwith document embeddings computed from articles paired with the images, enable\nthe model to predict bias. In the second stage, we remove the requirement of\nthe text domain and train a visual classifier from the features of the former\nmodel. We show this two-stage approach facilitates learning and outperforms\nseveral strong baselines. We also present extensive qualitative results\ndemonstrating the nuances of the data.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00147v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.00067v1",
    "title": "DNA: Dynamic Social Network Alignment",
    "authors": [
      "Li Sun",
      "Zhongbao Zhang",
      "Pengxin Ji",
      "Jian Wen",
      "Sen Su",
      "Philip S. Yu"
    ],
    "author_ids": [],
    "abstract": "Social network alignment, aligning different social networks on their common\nusers, is receiving dramatic attention from both academic and industry. All\nexisting studies consider the social network to be static and neglect its\ninherent dynamics. In fact, the dynamics of social networks contain the\ndiscriminative pattern of an individual, which can be leveraged to facilitate\nsocial network alignment. Hence, we for the first time propose to study the\nproblem of aligning dynamic social networks. Towards this end, we propose a\nnovel Dynamic social Network Alignment (DNA) framework, a unified optimization\napproach over deep neural architectures, to unfold the fruitful dynamics to\nperform alignment. However, it faces tremendous challenges in both modeling and\noptimization: (1) To model the intra-network dynamics, we explore the local\ndynamics of the latent pattern in friending evolvement and the global\nconsistency of the representation similarity with neighbors. We design a novel\ndeep neural architecture to obtain the dual embedding capturing local dynamics\nand global consistency for each user. (2) To model the inter-network alignment,\nwe exploit the underlying identity of an individual from the dual embedding in\neach dynamic social network. We design a unified optimization approach\ninterplaying proposed deep neural architectures to construct a common subspace\nof identity embeddings. (3) To address this optimization problem, we design an\neffective alternating algorithm with solid theoretical guarantees.We conduct\nextensive experiments on real-world datasets and show that the proposed DNA\nframework substantially outperforms the state-of-the-art methods.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00067v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14659v3",
    "title": "Masked Language Model Scoring",
    "authors": [
      "Julian Salazar",
      "Davis Liang",
      "Toan Q. Nguyen",
      "Katrin Kirchhoff"
    ],
    "author_ids": [],
    "abstract": "Pretrained masked language models (MLMs) require finetuning for most NLP\ntasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood\nscores (PLLs), which are computed by masking tokens one by one. We show that\nPLLs outperform scores from autoregressive language models like GPT-2 in a\nvariety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an\nend-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on\nstate-of-the-art baselines for low-resource translation pairs, with further\ngains from domain adaptation. We attribute this success to PLL's unsupervised\nexpression of linguistic acceptability without a left-to-right bias, greatly\nimproving on scores from GPT-2 (+10 points on island effects, NPI licensing in\nBLiMP). One can finetune MLMs to give scores without masking, enabling\ncomputation in a single inference pass. In all, PLLs and their associated\npseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of\npretrained MLMs; e.g., we use a single cross-lingual model to rescore\ntranslations in multiple languages. We release our library for language model\nscoring at https://github.com/awslabs/mlm-scoring.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14659v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14634v2",
    "title": "Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators",
    "authors": [
      "Reinhard Heckel",
      "Mahdi Soltanolkotabi"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNNs) have emerged as highly successful tools\nfor image generation, recovery, and restoration. A major contributing factor to\nthis success is that convolutional networks impose strong prior assumptions\nabout natural images. A surprising experiment that highlights this\narchitectural bias towards natural images is that one can remove noise and\ncorruptions from a natural image without using any training data, by simply\nfitting (via gradient descent) a randomly initialized, over-parameterized\nconvolutional generator to the corrupted image. While this over-parameterized\nnetwork can fit the corrupted image perfectly, surprisingly after a few\niterations of gradient descent it generates an almost uncorrupted image. This\nintriguing phenomenon enables state-of-the-art CNN-based denoising and\nregularization of other inverse problems. In this paper, we attribute this\neffect to a particular architectural choice of convolutional networks, namely\nconvolutions with fixed interpolating filters. We then formally characterize\nthe dynamics of fitting a two-layer convolutional generator to a noisy signal\nand prove that early-stopped gradient descent denoises/regularizes. Our proof\nrelies on showing that convolutional generators fit the structured part of an\nimage significantly faster than the corrupted portion.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14634v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14497v2",
    "title": "Probabilistic Bias Mitigation in Word Embeddings",
    "authors": [
      "Hailey Joren",
      "David Alvarez-Melis"
    ],
    "author_ids": [],
    "abstract": "It has been shown that word embeddings derived from large corpora tend to\nincorporate biases present in their training data. Various methods for\nmitigating these biases have been proposed, but recent work has demonstrated\nthat these methods hide but fail to truly remove the biases, which can still be\nobserved in word nearest-neighbor statistics. In this work we propose a\nprobabilistic view of word embedding bias. We leverage this framework to\npresent a novel method for mitigating bias which relies on probabilistic\nobservations to yield a more robust bias mitigation algorithm. We demonstrate\nthat this method effectively reduces bias according to three separate measures\nof bias while maintaining embedding quality across various popular benchmark\nsemantic tasks",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14497v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14472v1",
    "title": "Learning Fairness in Multi-Agent Systems",
    "authors": [
      "Jiechuan Jiang",
      "Zongqing Lu"
    ],
    "author_ids": [],
    "abstract": "Fairness is essential for human society, contributing to stability and\nproductivity. Similarly, fairness is also the key for many multi-agent systems.\nTaking fairness into multi-agent learning could help multi-agent systems become\nboth efficient and stable. However, learning efficiency and fairness\nsimultaneously is a complex, multi-objective, joint-policy optimization. To\ntackle these difficulties, we propose FEN, a novel hierarchical reinforcement\nlearning model. We first decompose fairness for each agent and propose\nfair-efficient reward that each agent learns its own policy to optimize. To\navoid multi-objective conflict, we design a hierarchy consisting of a\ncontroller and several sub-policies, where the controller maximizes the\nfair-efficient reward by switching among the sub-policies that provides diverse\nbehaviors to interact with the environment. FEN can be trained in a fully\ndecentralized way, making it easy to be deployed in real-world applications.\nEmpirically, we show that FEN easily learns both fairness and efficiency and\nsignificantly outperforms baselines in a variety of multi-agent scenarios.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14472v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14465v2",
    "title": "Recurrent Averaging Inequalities in Multi-Agent Control and Social Dynamics Modeling",
    "authors": [
      "Anton V. Proskurnikov",
      "Giuseppe Calafiore",
      "Ming Cao"
    ],
    "author_ids": [],
    "abstract": "Many multi-agent control algorithms and dynamic agent-based models arising in\nnatural and social sciences are based on the principle of iterative averaging.\nEach agent is associated to a value of interest, which may represent, for\ninstance, the opinion of an individual in a social group, the velocity vector\nof a mobile robot in a flock, or the measurement of a sensor within a sensor\nnetwork. This value is updated, at each iteration, to a weighted average of\nitself and of the values of the adjacent agents. It is well known that, under\nnatural assumptions on the network's graph connectivity, this local averaging\nprocedure eventually leads to global consensus, or synchronization of the\nvalues at all nodes. Applications of iterative averaging include, but are not\nlimited to, algorithms for distributed optimization, for solution of linear and\nnonlinear equations, for multi-robot coordination and for opinion formation in\nsocial groups. Although these algorithms have similar structures, the\nmathematical techniques used for their analysis are diverse, and conditions for\ntheir convergence and differ from case to case. In this paper, we review many\nof these algorithms and we show that their properties can be analyzed in a\nunified way by using a novel tool based on recurrent averaging inequalities\n(RAIs). We develop a theory of RAIs and apply it to the analysis of several\nimportant multi-agent algorithms recently proposed in the literature.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SI",
      "cs.SY",
      "math.OC",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14465v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.14210v1",
    "title": "Methodological Blind Spots in Machine Learning Fairness: Lessons from the Philosophy of Science and Computer Science",
    "authors": [
      "Samuel Deng",
      "Achille Varzi"
    ],
    "author_ids": [],
    "abstract": "In the ML fairness literature, there have been few investigations through the\nviewpoint of philosophy, a lens that encourages the critical evaluation of\nbasic assumptions. The purpose of this paper is to use three ideas from the\nphilosophy of science and computer science to tease out blind spots in the\nassumptions that underlie ML fairness: abstraction, induction, and measurement.\nThrough this investigation, we hope to warn of these methodological blind spots\nand encourage further interdisciplinary investigation in fair-ML through the\nframework of philosophy.",
    "published_date": "2019-10-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14210v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14161v1",
    "title": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "authors": [
      "Hila Gonen",
      "Yova Kementchedjhieva",
      "Yoav Goldberg"
    ],
    "author_ids": [],
    "abstract": "Many natural languages assign grammatical gender also to inanimate nouns in\nthe language. In such languages, words that relate to the gender-marked nouns\nare inflected to agree with the noun's gender. We show that this affects the\nword representations of inanimate nouns, resulting in nouns with the same\ngender being closer to each other than nouns with different gender. While\n\"embedding debiasing\" methods fail to remove the effect, we demonstrate that a\ncareful application of methods that neutralize grammatical gender signals from\nthe words' context when training word embeddings is effective in removing it.\nFixing the grammatical gender bias yields a positive effect on the quality of\nthe resulting word embeddings, both in monolingual and cross-lingual settings.\nWe note that successfully removing gender signals, while achievable, is not\ntrivial to do and that a language-specific morphological analyzer, together\nwith careful usage of it, are essential for achieving good results.",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14161v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14129v1",
    "title": "Dividing a Graphical Cake",
    "authors": [
      "Xiaohui Bei",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We consider the classical cake-cutting problem where we wish to fairly divide\na heterogeneous resource, often modeled as a cake, among interested agents.\nWork on the subject typically assumes that the cake is represented by an\ninterval. In this paper, we introduce a generalized setting where the cake can\nbe in the form of the set of edges of an undirected graph. This allows us to\nmodel the division of road or cable networks. Unlike in the canonical setting,\ncommon fairness criteria such as proportionality cannot always be satisfied in\nour setting if each agent must receive a connected subgraph. We determine the\noptimal approximation of proportionality that can be obtained for any number of\nagents with arbitrary valuations, and exhibit tight guarantees for each graph\nin the case of two agents. In addition, when more than one connected piece per\nagent is allowed, we establish the best egalitarian welfare guarantee for each\ntotal number of connected pieces. We also study a number of variants and\nextensions, including when approximate equitability is considered, or when the\nitem to be divided is undesirable (also known as chore division).",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14129v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.14120v1",
    "title": "What is Fair? Exploring Pareto-Efficiency for Fairness Constrained Classifiers",
    "authors": [
      "Ananth Balashankar",
      "Alyssa Lees",
      "Chris Welty",
      "Lakshminarayanan Subramanian"
    ],
    "author_ids": [],
    "abstract": "The potential for learned models to amplify existing societal biases has been\nbroadly recognized. Fairness-aware classifier constraints, which apply equality\nmetrics of performance across subgroups defined on sensitive attributes such as\nrace and gender, seek to rectify inequity but can yield non-uniform degradation\nin performance for skewed datasets. In certain domains, imbalanced degradation\nof performance can yield another form of unintentional bias. In the spirit of\nconstructing fairness-aware algorithms as societal imperative, we explore an\nalternative: Pareto-Efficient Fairness (PEF). Theoretically, we prove that PEF\nidentifies the operating point on the Pareto curve of subgroup performances\nclosest to the fairness hyperplane, maximizing multiple subgroup accuracy.\nEmpirically we demonstrate that PEF outperforms by achieving Pareto levels in\naccuracy for all subgroups compared to strict fairness constraints in several\nUCI datasets.",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14120v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.14008v3",
    "title": "Approximately Stable Committee Selection",
    "authors": [
      "Zhihao Jiang",
      "Kamesh Munagala",
      "Kangning Wang"
    ],
    "author_ids": [],
    "abstract": "In the committee selection problem, we are given $m$ candidates, and $n$\nvoters. Candidates can have different weights. A committee is a subset of\ncandidates, and its weight is the sum of weights of its candidates. Each voter\nexpresses an ordinal ranking over all possible committees. The only assumption\nwe make on preferences is monotonicity: If $S \\subseteq S'$ are two committees,\nthen any voter weakly prefers $S'$ to $S$. We study a general notion of group\nfairness via stability: A committee of given total weight $K$ is stable if no\ncoalition of voters can deviate and choose a committee of proportional weight,\nso that all these voters strictly prefer the new committee to the existing one.\nExtending this notion to approximation, for parameter $c \\ge 1$, a committee\n$S$ of weight $K$ is said to be $c$-approximately stable if for any other\ncommittee $S'$ of weight $K'$, the fraction of voters that strictly prefer $S'$\nto $S$ is strictly less than $\\frac{c K'}{K}$. When $c = 1$, this condition is\nequivalent to classical core stability. The question we ask is: Does a\n$c$-approximately stable committee of weight at most any given value $K$ always\nexist for constant $c$? It is relatively easy to show that there exist monotone\npreferences for which $c \\ge 2$. However, even for simple and widely studied\npreference structures, a non-trivial upper bound on $c$ has been elusive. In\nthis paper, we show that $c = O(1)$ for all monotone preference structures. Our\nproof proceeds via showing an existence result for a randomized notion of\nstability, and iteratively rounding the resulting fractional solution.",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.14008v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.13983v1",
    "title": "DADI: Dynamic Discovery of Fair Information with Adversarial Reinforcement Learning",
    "authors": [
      "Michiel A. Bakker",
      "Duy Patrick Tu",
      "Humberto Riverón Valdés",
      "Krishna P. Gummadi",
      "Kush R. Varshney",
      "Adrian Weller",
      "Alex Pentland"
    ],
    "author_ids": [],
    "abstract": "We introduce a framework for dynamic adversarial discovery of information\n(DADI), motivated by a scenario where information (a feature set) is used by\nthird parties with unknown objectives. We train a reinforcement learning agent\nto sequentially acquire a subset of the information while balancing accuracy\nand fairness of predictors downstream. Based on the set of already acquired\nfeatures, the agent decides dynamically to either collect more information from\nthe set of available features or to stop and predict using the information that\nis currently available. Building on previous work exploring adversarial\nrepresentation learning, we attain group fairness (demographic parity) by\nrewarding the agent with the adversary's loss, computed over the final feature\nset. Importantly, however, the framework provides a more general starting point\nfor fair or private dynamic information discovery. Finally, we demonstrate\nempirically, using two real-world datasets, that we can trade-off fairness and\npredictive performance",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.13983v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.13913v4",
    "title": "Toward Gender-Inclusive Coreference Resolution",
    "authors": [
      "Yang Trista Cao",
      "Hal Daumé III"
    ],
    "author_ids": [],
    "abstract": "Correctly resolving textual mentions of people fundamentally entails making\ninferences about those people. Such inferences raise the risk of systemic\nbiases in coreference resolution systems, including biases that can harm binary\nand non-binary trans and cis stakeholders. To better understand such biases, we\nforeground nuanced conceptualizations of gender from sociology and\nsociolinguistics, and develop two new datasets for interrogating bias in crowd\nannotations and in existing coreference resolution systems. Through these\nstudies, conducted on English text, we confirm that without acknowledging and\nbuilding systems that recognize the complexity of gender, we build systems that\nlead to many potential harms.",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.13913v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.13607v2",
    "title": "Mathematical decisions and non-causal elements of explainable AI",
    "authors": [
      "Atoosa Kasirzadeh"
    ],
    "author_ids": [],
    "abstract": "The social implications of algorithmic decision-making in sensitive contexts\nhave generated lively debates among multiple stakeholders, such as moral and\npolitical philosophers, computer scientists, and the public. Yet, the lack of a\ncommon language and a conceptual framework for an appropriate bridging of the\nmoral, technical, and political aspects of the debate prevents the discussion\nto be as effective as it can be. Social scientists and psychologists are\ncontributing to this debate by gathering a wealth of empirical data, yet a\nphilosophical analysis of the social implications of algorithmic\ndecision-making remains comparatively impoverished. In attempting to address\nthis lacuna, this paper argues that a hierarchy of different types of\nexplanations for why and how an algorithmic decision outcome is achieved can\nestablish the relevant connection between the moral and technical aspects of\nalgorithmic decision-making. In particular, I offer a multi-faceted conceptual\nframework for the explanations and the interpretations of algorithmic\ndecisions, and I claim that this framework can lay the groundwork for a focused\ndiscussion among multiple stakeholders about the social implications of\nalgorithmic decision-making, as well as AI governance and ethics more\ngenerally.",
    "published_date": "2019-10-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.13607v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.03268v1",
    "title": "Inducing brain-relevant bias in natural language processing models",
    "authors": [
      "Dan Schwartz",
      "Mariya Toneva",
      "Leila Wehbe"
    ],
    "author_ids": [],
    "abstract": "Progress in natural language processing (NLP) models that estimate\nrepresentations of word sequences has recently been leveraged to improve the\nunderstanding of language processing in the brain. However, these models have\nnot been specifically designed to capture the way the brain represents language\nmeaning. We hypothesize that fine-tuning these models to predict recordings of\nbrain activity of people reading text will lead to representations that encode\nmore brain-activity-relevant language information. We demonstrate that a\nversion of BERT, a recently introduced and powerful language model, can improve\nthe prediction of brain activity after fine-tuning. We show that the\nrelationship between language and brain activity learned by BERT during this\nfine-tuning transfers across multiple participants. We also show that, for some\nparticipants, the fine-tuned representations learned from both\nmagnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI)\nare better for predicting fMRI than the representations learned from fMRI\nalone, indicating that the learned representations capture\nbrain-activity-relevant information that is not simply an artifact of the\nmodality. While changes to language representations help the model predict\nbrain activity, they also do not harm the model's ability to perform downstream\nNLP tasks. Our findings are notable for research on language understanding in\nthe brain.",
    "published_date": "2019-10-29T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.NC",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.03268v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.13122v1",
    "title": "Algorithmic decision-making in AVs: Understanding ethical and technical concerns for smart cities",
    "authors": [
      "Hazel Si Min Lim",
      "Araz Taeihagh"
    ],
    "author_ids": [],
    "abstract": "Autonomous Vehicles (AVs) are increasingly embraced around the world to\nadvance smart mobility and more broadly, smart, and sustainable cities.\nAlgorithms form the basis of decision-making in AVs, allowing them to perform\ndriving tasks autonomously, efficiently, and more safely than human drivers and\noffering various economic, social, and environmental benefits. However,\nalgorithmic decision-making in AVs can also introduce new issues that create\nnew safety risks and perpetuate discrimination. We identify bias, ethics, and\nperverse incentives as key ethical issues in the AV algorithms' decision-making\nthat can create new safety risks and discriminatory outcomes. Technical issues\nin the AVs' perception, decision-making and control algorithms, limitations of\nexisting AV testing and verification methods, and cybersecurity vulnerabilities\ncan also undermine the performance of the AV system. This article investigates\nthe ethical and technical concerns surrounding algorithmic decision-making in\nAVs by exploring how driving decisions can perpetuate discrimination and create\nnew safety risks for the public. We discuss steps taken to address these\nissues, highlight the existing research gaps and the need to mitigate these\nissues through the design of AV's algorithms and of policies and regulations to\nfully realise AVs' benefits for smart and sustainable cities.",
    "published_date": "2019-10-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.13122v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12999v6",
    "title": "A Decentralized Parallel Algorithm for Training Generative Adversarial Nets",
    "authors": [
      "Mingrui Liu",
      "Wei Zhang",
      "Youssef Mroueh",
      "Xiaodong Cui",
      "Jerret Ross",
      "Tianbao Yang",
      "Payel Das"
    ],
    "author_ids": [],
    "abstract": "Generative Adversarial Networks (GANs) are a powerful class of generative\nmodels in the deep learning community. Current practice on large-scale GAN\ntraining utilizes large models and distributed large-batch training strategies,\nand is implemented on deep learning frameworks (e.g., TensorFlow, PyTorch,\netc.) designed in a centralized manner. In the centralized network topology,\nevery worker needs to either directly communicate with the central node or\nindirectly communicate with all other workers in every iteration. However, when\nthe network bandwidth is low or network latency is high, the performance would\nbe significantly degraded. Despite recent progress on decentralized algorithms\nfor training deep neural networks, it remains unclear whether it is possible to\ntrain GANs in a decentralized manner. The main difficulty lies at handling the\nnonconvex-nonconcave min-max optimization and the decentralized communication\nsimultaneously. In this paper, we address this difficulty by designing the\n\\textbf{first gradient-based decentralized parallel algorithm} which allows\nworkers to have multiple rounds of communications in one iteration and to\nupdate the discriminator and generator simultaneously, and this design makes it\namenable for the convergence analysis of the proposed decentralized algorithm.\nTheoretically, our proposed decentralized algorithm is able to solve a class of\nnon-convex non-concave min-max problems with provable non-asymptotic\nconvergence to first-order stationary point. Experimental results on GANs\ndemonstrate the effectiveness of the proposed algorithm.",
    "published_date": "2019-10-28T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12999v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12854v2",
    "title": "Learning Fair and Interpretable Representations via Linear Orthogonalization",
    "authors": [
      "Yuzi He",
      "Keith Burghardt",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "To reduce human error and prejudice, many high-stakes decisions have been\nturned over to machine algorithms. However, recent research suggests that this\ndoes not remove discrimination, and can perpetuate harmful stereotypes. While\nalgorithms have been developed to improve fairness, they typically face at\nleast one of three shortcomings: they are not interpretable, their prediction\nquality deteriorates quickly compared to unbiased equivalents, and they are not\neasily transferable across models. To address these shortcomings, we propose a\ngeometric method that removes correlations between data and any number of\nprotected variables. Further, we can control the strength of debiasing through\nan adjustable parameter to address the trade-off between prediction quality and\nfairness. The resulting features are interpretable and can be used with many\npopular models, such as linear regression, random forest, and multilayer\nperceptrons. The resulting predictions are found to be more accurate and fair\ncompared to several state-of-the-art fair AI algorithms across a variety of\nbenchmark datasets. Our work shows that debiasing data is a simple and\neffective solution toward improving fairness.",
    "published_date": "2019-10-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12854v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12809v4",
    "title": "Minimax Weight and Q-Function Learning for Off-Policy Evaluation",
    "authors": [
      "Masatoshi Uehara",
      "Jiawei Huang",
      "Nan Jiang"
    ],
    "author_ids": [],
    "abstract": "We provide theoretical investigations into off-policy evaluation in\nreinforcement learning using function approximators for (marginalized)\nimportance weights and value functions. Our contributions include: (1) A new\nestimator, MWL, that directly estimates importance ratios over the state-action\ndistributions, removing the reliance on knowledge of the behavior policy as in\nprior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by\nswapping the roles of importance weights and value-functions in MWL. MQL has an\nintuitive interpretation of minimizing average Bellman errors and can be\ncombined with MWL in a doubly robust manner. (3) Several additional results\nthat offer further insights into these methods, including the sample complexity\nanalyses of MWL and MQL, their asymptotic optimality in the tabular setting,\nhow the learned importance weights depend the choice of the discriminator\nclass, and how our methods provide a unified view of some old and new\nalgorithms in RL.",
    "published_date": "2019-10-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12809v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12695v2",
    "title": "AI Ethics in Industry: A Research Framework",
    "authors": [
      "Ville Vakkuri",
      "Kai-Kristian Kemell",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "Artificial Intelligence (AI) systems exert a growing influence on our\nsociety. As they become more ubiquitous, their potential negative impacts also\nbecome evident through various real-world incidents. Following such early\nincidents, academic and public discussion on AI ethics has highlighted the need\nfor implementing ethics in AI system development. However, little currently\nexists in the way of frameworks for understanding the practical implementation\nof AI ethics. In this paper, we discuss a research framework for implementing\nAI ethics in industrial settings. The framework presents a starting point for\nempirical studies into AI ethics but is still being developed further based on\nits practical utilization.",
    "published_date": "2019-10-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12695v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12514v1",
    "title": "Multi-sequence Cardiac MR Segmentation with Adversarial Domain Adaptation Network",
    "authors": [
      "Jiexiang Wang",
      "Hongyu Huang",
      "Chaoqi Chen",
      "Wenao Ma",
      "Yue Huang",
      "Xinghao Ding"
    ],
    "author_ids": [],
    "abstract": "Automatic and accurate segmentation of the ventricles and myocardium from\nmulti-sequence cardiac MRI (CMR) is crucial for the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). However, due\nto the existence of domain shift among different modalities of datasets, the\nperformance of deep neural networks drops significantly when the training and\ntesting datasets are distinct. In this paper, we propose an unsupervised domain\nalignment method to explicitly alleviate the domain shifts among different\nmodalities of CMR sequences, \\emph{e.g.,} bSSFP, LGE, and T2-weighted. Our\nsegmentation network is attention U-Net with pyramid pooling module, where\nmulti-level feature space and output space adversarial learning are proposed to\ntransfer discriminative domain knowledge across different datasets. Moreover,\nwe further introduce a group-wise feature recalibration module to enforce the\nfine-grained semantic-level feature alignment that matching features from\ndifferent networks but with the same class label. We evaluate our method on the\nmulti-sequence cardiac MR Segmentation Challenge 2019 datasets, which contain\nthree different modalities of MRI sequences. Extensive experimental results\nshow that the proposed methods can obtain significant segmentation improvements\ncompared with the baseline models.",
    "published_date": "2019-10-28T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12514v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12430v1",
    "title": "Differentiable Convex Optimization Layers",
    "authors": [
      "Akshay Agrawal",
      "Brandon Amos",
      "Shane Barratt",
      "Stephen Boyd",
      "Steven Diamond",
      "Zico Kolter"
    ],
    "author_ids": [],
    "abstract": "Recent work has shown how to embed differentiable optimization problems (that\nis, problems whose solutions can be backpropagated through) as layers within\ndeep learning architectures. This method provides a useful inductive bias for\ncertain problems, but existing software for differentiable optimization layers\nis rigid and difficult to apply to new settings. In this paper, we propose an\napproach to differentiating through disciplined convex programs, a subclass of\nconvex optimization problems used by domain-specific languages (DSLs) for\nconvex optimization. We introduce disciplined parametrized programming, a\nsubset of disciplined convex programming, and we show that every disciplined\nparametrized program can be represented as the composition of an affine map\nfrom parameters to problem data, a solver, and an affine map from the solver's\nsolution to a solution of the original problem (a new form we refer to as\naffine-solver-affine form). We then demonstrate how to efficiently\ndifferentiate through each of these components, allowing for end-to-end\nanalytical differentiation through the entire convex program. We implement our\nmethodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex\noptimization, and additionally implement differentiable layers for disciplined\nconvex programs in PyTorch and TensorFlow 2.0. Our implementation significantly\nlowers the barrier to using convex optimization problems in differentiable\nprograms. We present applications in linear machine learning models and in\nstochastic control, and we show that our layer is competitive (in execution\ntime) compared to specialized differentiable solvers from past work.",
    "published_date": "2019-10-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12244v5",
    "title": "Investigating MMM Ponzi scheme on Bitcoin",
    "authors": [
      "Yazan Boshmaf",
      "Charitha Elvitigala",
      "Husam Al Jawaheri",
      "Primal Wijesekera",
      "Mashael Al Sabah"
    ],
    "author_ids": [],
    "abstract": "Cybercriminals exploit cryptocurrencies to carry out illicit activities. In\nthis paper, we focus on Ponzi schemes that operate on Bitcoin and perform an\nin-depth analysis of MMM, one of the oldest and most popular Ponzi schemes.\nBased on 423K transactions involving 16K addresses, we show that: (1) Starting\nSep 2014, the scheme goes through three phases over three years. At its peak,\nMMM circulated more than 150M dollars a day, after which it collapsed by the\nend of Jun 2016. (2) There is a high income inequality between MMM members,\nwith the daily Gini index reaching more than 0.9. The scheme also exhibits a\nzero-sum investment model, in which one member's loss is another member's gain.\nThe percentage of victims who never made any profit has grown from 0% to 41% in\nfive months, during which the top-earning scammer has made 765K dollars in\nprofit. (3) The scheme has a global reach with 80 different member countries\nbut a highly-asymmetrical flow of money between them. While India and Indonesia\nhave the largest pairwise flow in MMM, members in Indonesia have received 12x\nmore money than they have sent to their counterparts in India.",
    "published_date": "2019-10-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12244v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.12137v2",
    "title": "Symbolic Controller Synthesis for Büchi Specifications on Stochastic Systems",
    "authors": [
      "Rupak Majumdar",
      "Kaushik Mallik",
      "Sadegh Soudjani"
    ],
    "author_ids": [],
    "abstract": "We consider the policy synthesis problem for continuous-state controlled\nMarkov processes evolving in discrete time, when the specification is given as\na B\\\"uchi condition (visit a set of states infinitely often). We decompose\ncomputation of the maximal probability of satisfying the B\\\"uchi condition into\ntwo steps. The first step is to compute the maximal qualitative winning set,\nfrom where the B\\\"uchi condition can be enforced with probability one. The\nsecond step is to find the maximal probability of reaching the already computed\nqualitative winning set. In contrast with finite-state models, we show that\nsuch a computation only gives a lower bound on the maximal probability where\nthe gap can be non-zero.\n  In this paper we focus on approximating the qualitative winning set, while\npointing out that the existing approaches for unbounded reachability\ncomputation can solve the second step. We provide an abstraction-based\ntechnique to approximate the qualitative winning set by simultaneously using an\nover- and under-approximation of the probabilistic transition relation. Since\nwe are interested in qualitative properties, the abstraction is\nnon-probabilistic; instead, the probabilistic transitions are assumed to be\nunder the control of a (fair) adversary. Thus, we reduce the original policy\nsynthesis problem to a B\\\"uchi game under a fairness assumption and\ncharacterize upper and lower bounds on winning sets as nested fixed point\nexpressions in the $\\mu$-calculus. This characterization immediately provides a\nsymbolic algorithm scheme. Further, a winning strategy computed on the abstract\ngame can be refined to a policy on the controlled Markov process.\n  We describe a concrete abstraction procedure and demonstrate our algorithm on\ntwo case studies.",
    "published_date": "2019-10-26T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12137v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.12091v2",
    "title": "Understanding Isomorphism Bias in Graph Data Sets",
    "authors": [
      "Sergei Ivanov",
      "Sergei Sviridov",
      "Evgeny Burnaev"
    ],
    "author_ids": [],
    "abstract": "In recent years there has been a rapid increase in classification methods on\ngraph structured data. Both in graph kernels and graph neural networks, one of\nthe implicit assumptions of successful state-of-the-art models was that\nincorporating graph isomorphism features into the architecture leads to better\nempirical performance. However, as we discover in this work, commonly used data\nsets for graph classification have repeating instances which cause the problem\nof isomorphism bias, i.e. artificially increasing the accuracy of the models by\nmemorizing target information from the training set. This prevents fair\ncompetition of the algorithms and raises a question of the validity of the\nobtained results. We analyze 54 data sets, previously extensively used for\ngraph-related tasks, on the existence of isomorphism bias, give a set of\nrecommendations to machine learning practitioners to properly set up their\nmodels, and open source new data sets for the future experiments.",
    "published_date": "2019-10-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12091v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12008v2",
    "title": "Fair Generative Modeling via Weak Supervision",
    "authors": [
      "Kristy Choi",
      "Aditya Grover",
      "Trisha Singh",
      "Rui Shu",
      "Stefano Ermon"
    ],
    "author_ids": [],
    "abstract": "Real-world datasets are often biased with respect to key demographic factors\nsuch as race and gender. Due to the latent nature of the underlying factors,\ndetecting and mitigating bias is especially challenging for unsupervised\nmachine learning. We present a weakly supervised algorithm for overcoming\ndataset bias for deep generative models. Our approach requires access to an\nadditional small, unlabeled reference dataset as the supervision signal, thus\nsidestepping the need for explicit labels on the underlying bias factors. Using\nthis supplementary dataset, we detect the bias in existing datasets via a\ndensity ratio technique and learn generative models which efficiently achieve\nthe twin goals of: 1) data efficiency by using training examples from both\nbiased and reference datasets for learning; and 2) data generation close in\ndistribution to the reference dataset at test time. Empirically, we demonstrate\nthe efficacy of our approach which reduces bias w.r.t. latent factors by an\naverage of up to 34.6% over baselines for comparable image generation using\ngenerative adversarial networks.",
    "published_date": "2019-10-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12008v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11868v1",
    "title": "Bias-Variance Tradeoff in a Sliding Window Implementation of the Stochastic Gradient Algorithm",
    "authors": [
      "Yakup Ceki Papo"
    ],
    "author_ids": [],
    "abstract": "This paper provides a framework to analyze stochastic gradient algorithms in\na mean squared error (MSE) sense using the asymptotic normality result of the\nstochastic gradient descent (SGD) iterates. We perform this analysis by taking\nthe asymptotic normality result and applying it to the finite iteration case.\nSpecifically, we look at problems where the gradient estimators are biased and\nhave reduced variance and compare the iterates generated by these gradient\nestimators to the iterates generated by the SGD algorithm. We use the work of\nFabian to characterize the mean and the variance of the distribution of the\niterates in terms of the bias and the covariance matrix of the gradient\nestimators. We introduce the sliding window SGD (SW-SGD) algorithm, with its\nproof of convergence, which incurs a lower MSE than the SGD algorithm on\nquadratic and convex problems. Lastly, we present some numerical results to\nshow the effectiveness of this framework and the superiority of SW-SGD\nalgorithm over the SGD algorithm.",
    "published_date": "2019-10-25T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11868v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11779v1",
    "title": "Toward a better trade-off between performance and fairness with kernel-based distribution matching",
    "authors": [
      "Flavien Prost",
      "Hai Qian",
      "Qiuwen Chen",
      "Ed H. Chi",
      "Jilin Chen",
      "Alex Beutel"
    ],
    "author_ids": [],
    "abstract": "As recent literature has demonstrated how classifiers often carry unintended\nbiases toward some subgroups, deploying machine learned models to users demands\ncareful consideration of the social consequences. How should we address this\nproblem in a real-world system? How should we balance core performance and\nfairness metrics? In this paper, we introduce a MinDiff framework for\nregularizing classifiers toward different fairness metrics and analyze a\ntechnique with kernel-based statistical dependency tests. We run a thorough\nstudy on an academic dataset to compare the Pareto frontier achieved by\ndifferent regularization approaches, and apply our kernel-based method to two\nlarge-scale industrial systems demonstrating real-world improvements.",
    "published_date": "2019-10-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11779v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11645v4",
    "title": "Reducing Domain Gap by Reducing Style Bias",
    "authors": [
      "Hyeonseob Nam",
      "HyunJae Lee",
      "Jongchan Park",
      "Wonjun Yoon",
      "Donggeun Yoo"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNNs) often fail to maintain their performance\nwhen they confront new test domains, which is known as the problem of domain\nshift. Recent studies suggest that one of the main causes of this problem is\nCNNs' strong inductive bias towards image styles (i.e. textures) which are\nsensitive to domain changes, rather than contents (i.e. shapes). Inspired by\nthis, we propose to reduce the intrinsic style bias of CNNs to close the gap\nbetween domains. Our Style-Agnostic Networks (SagNets) disentangle style\nencodings from class categories to prevent style biased predictions and focus\nmore on the contents. Extensive experiments show that our method effectively\nreduces the style bias and makes the model more robust under domain shift. It\nachieves remarkable performance improvements in a wide range of cross-domain\ntasks including domain generalization, unsupervised domain adaptation, and\nsemi-supervised domain adaptation on multiple datasets.",
    "published_date": "2019-10-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11645v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11482v1",
    "title": "Human Action Recognition Using Deep Multilevel Multimodal (M2) Fusion of Depth and Inertial Sensors",
    "authors": [
      "Zeeshan Ahmad",
      "Naimul Khan"
    ],
    "author_ids": [],
    "abstract": "Multimodal fusion frameworks for Human Action Recognition (HAR) using depth\nand inertial sensor data have been proposed over the years. In most of the\nexisting works, fusion is performed at a single level (feature level or\ndecision level), missing the opportunity to fuse rich mid-level features\nnecessary for better classification. To address this shortcoming, in this\npaper, we propose three novel deep multilevel multimodal fusion frameworks to\ncapitalize on different fusion strategies at various stages and to leverage the\nsuperiority of multilevel fusion. At input, we transform the depth data into\ndepth images called sequential front view images (SFIs) and inertial sensor\ndata into signal images. Each input modality, depth and inertial, is further\nmade multimodal by taking convolution with the Prewitt filter. Creating\n\"modality within modality\" enables further complementary and discriminative\nfeature extraction through Convolutional Neural Networks (CNNs). CNNs are\ntrained on input images of each modality to learn low-level, high-level and\ncomplex features. Learned features are extracted and fused at different stages\nof the proposed frameworks to combine discriminative and complementary\ninformation. These highly informative features are served as input to a\nmulti-class Support Vector Machine (SVM). We evaluate the proposed frameworks\non three publicly available multimodal HAR datasets, namely, UTD Multimodal\nHuman Action Dataset (MHAD), Berkeley MHAD, and UTD-MHAD Kinect V2.\nExperimental results show the supremacy of the proposed fusion frameworks over\nexisting methods.",
    "published_date": "2019-10-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11482v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11452v1",
    "title": "Fairness Sample Complexity and the Case for Human Intervention",
    "authors": [
      "Ananth Balashankar",
      "Alyssa Lees"
    ],
    "author_ids": [],
    "abstract": "With the aim of building machine learning systems that incorporate standards\nof fairness and accountability, we explore explicit subgroup sample complexity\nbounds. The work is motivated by the observation that classifier predictions\nfor real world datasets often demonstrate drastically different metrics, such\nas accuracy, when subdivided by specific sensitive variable subgroups. The\nreasons for these discrepancies are varied and not limited to the influence of\nmitigating variables, institutional bias, underlying population distributions\nas well as sampling bias. Among the numerous definitions of fairness that\nexist, we argue that at a minimum, principled ML practices should ensure that\nclassification predictions are able to mirror the underlying sub-population\ndistributions. However, as the number of sensitive variables increase,\npopulations meeting at the intersectionality of these variables may simply not\nexist or may not be large enough to provide accurate samples for\nclassification. In these increasingly likely scenarios, we make the case for\nhuman intervention and applying situational and individual definitions of\nfairness. In this paper we present lower bounds of subgroup sample complexity\nfor metric-fair learning based on the theory of Probably Approximately Metric\nFair Learning. We demonstrate that for a classifier to approach a definition of\nfairness in terms of specific sensitive variables, adequate subgroup population\nsamples need to exist and the model dimensionality has to be aligned with\nsubgroup population distributions. In cases where this is not feasible, we\npropose an approach using individual fairness definitions for achieving\nalignment. We look at two commonly explored UCI datasets under this lens and\nsuggest human interventions for data collection for specific subgroups to\nachieve approximate individual fairness for linear hypotheses.",
    "published_date": "2019-10-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11452v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11671v2",
    "title": "Hierarchical Prototype Learning for Zero-Shot Recognition",
    "authors": [
      "Xingxing Zhang",
      "Shupeng Gui",
      "Zhenfeng Zhu",
      "Yao Zhao",
      "Ji Liu"
    ],
    "author_ids": [],
    "abstract": "Zero-Shot Learning (ZSL) has received extensive attention and successes in\nrecent years especially in areas of fine-grained object recognition, retrieval,\nand image captioning. Key to ZSL is to transfer knowledge from the seen to the\nunseen classes via auxiliary semantic prototypes (e.g., word or attribute\nvectors). However, the popularly learned projection functions in previous works\ncannot generalize well due to non-visual components included in semantic\nprototypes. Besides, the incompleteness of provided prototypes and captured\nimages has less been considered by the state-of-the-art approaches in ZSL. In\nthis paper, we propose a hierarchical prototype learning formulation to provide\na systematical solution (named HPL) for zero-shot recognition. Specifically,\nHPL is able to obtain discriminability on both seen and unseen class domains by\nlearning visual prototypes respectively under the transductive setting. To\nnarrow the gap of two domains, we further learn the interpretable\nsuper-prototypes in both visual and semantic spaces. Meanwhile, the two spaces\nare further bridged by maximizing their structural consistency. This not only\nfacilitates the representativeness of visual prototypes, but also alleviates\nthe loss of information of semantic prototypes. An extensive group of\nexperiments are then carefully designed and presented, demonstrating that HPL\nobtains remarkably more favorable efficiency and effectiveness, over currently\navailable alternatives under various settings.",
    "published_date": "2019-10-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11671v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.10872v1",
    "title": "Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition",
    "authors": [
      "Ninareh Mehrabi",
      "Thamme Gowda",
      "Fred Morstatter",
      "Nanyun Peng",
      "Aram Galstyan"
    ],
    "author_ids": [],
    "abstract": "We study the bias in several state-of-the-art named entity recognition (NER)\nmodels---specifically, a difference in the ability to recognize male and female\nnames as PERSON entity types. We evaluate NER models on a dataset containing\n139 years of U.S. census baby names and find that relatively more female names,\nas opposed to male names, are not recognized as PERSON entities. We study the\nextent of this bias in several NER systems that are used prominently in\nindustry and academia. In addition, we also report a bias in the datasets on\nwhich these models were trained. The result of this analysis yields a new\nbenchmark for gender bias evaluation in named entity recognition systems. The\ndata and code for the application of this benchmark will be publicly available\nfor researchers to use.",
    "published_date": "2019-10-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.10872v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.10255v2",
    "title": "An Empirical Study on Learning Fairness Metrics for COMPAS Data with Human Supervision",
    "authors": [
      "Hanchen Wang",
      "Nina Grgic-Hlaca",
      "Preethi Lahoti",
      "Krishna P. Gummadi",
      "Adrian Weller"
    ],
    "author_ids": [],
    "abstract": "The notion of individual fairness requires that similar people receive\nsimilar treatment. However, this is hard to achieve in practice since it is\ndifficult to specify the appropriate similarity metric. In this work, we\nattempt to learn such similarity metric from human annotated data. We gather a\nnew dataset of human judgments on a criminal recidivism prediction (COMPAS)\ntask. By assuming the human supervision obeys the principle of individual\nfairness, we leverage prior work on metric learning, evaluate the performance\nof several metric learning methods on our dataset, and show that the learned\nmetrics outperform the Euclidean and Precision metric under various criteria.\nWe do not provide a way to directly learn a similarity metric satisfying the\nindividual fairness, but to provide an empirical study on how to derive the\nsimilarity metric from human supervisors, then future work can use this as a\ntool to understand human supervision.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.10255v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.10241v1",
    "title": "Achieving Ethical Algorithmic Behaviour in the Internet-of-Things: a Review",
    "authors": [
      "Seng W. Loke"
    ],
    "author_ids": [],
    "abstract": "The Internet-of-Things is emerging as a vast inter-connected space of devices\nand things surrounding people, many of which are increasingly capable of\nautonomous action, from automatically sending data to cloud servers for\nanalysis, changing the behaviour of smart objects, to changing the physical\nenvironment. A wide range of ethical concerns has arisen in their usage and\ndevelopment in recent years. Such concerns are exacerbated by the increasing\nautonomy given to connected things. This paper reviews, via examples, the\nlandscape of ethical issues, and some recent approaches to address these\nissues, concerning connected things behaving autonomously, as part of the\nInternet-of-Things. We consider ethical issues in relation to device operations\nand accompanying algorithms. Examples of concerns include unsecured consumer\ndevices, data collection with health related Internet-of-Things, hackable\nvehicles and behaviour of autonomous vehicles in dilemma situations,\naccountability with Internet-of-Things systems, algorithmic bias, uncontrolled\ncooperation among things, and automation affecting user choice and control.\nCurrent ideas towards addressing a range of ethical concerns are reviewed and\ncompared, including programming ethical behaviour, whitebox algorithms,\nblackbox validation, algorithmic social contracts, enveloping IoT systems, and\nguidelines and code of ethics for IoT developers - a suggestion from the\nanalysis is that a multi-pronged approach could be useful, based on the context\nof operation and deployment.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.10241v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.10045v2",
    "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
    "authors": [
      "Alejandro Barredo Arrieta",
      "Natalia Díaz-Rodríguez",
      "Javier Del Ser",
      "Adrien Bennetot",
      "Siham Tabik",
      "Alberto Barbado",
      "Salvador García",
      "Sergio Gil-López",
      "Daniel Molina",
      "Richard Benjamins",
      "Raja Chatila",
      "Francisco Herrera"
    ],
    "author_ids": [],
    "abstract": "In the last years, Artificial Intelligence (AI) has achieved a notable\nmomentum that may deliver the best of expectations over many application\nsectors across the field. For this to occur, the entire community stands in\nfront of the barrier of explainability, an inherent problem of AI techniques\nbrought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not\npresent in the last hype of AI. Paradigms underlying this problem fall within\nthe so-called eXplainable AI (XAI) field, which is acknowledged as a crucial\nfeature for the practical deployment of AI models. This overview examines the\nexisting literature in the field of XAI, including a prospect toward what is\nyet to be reached. We summarize previous efforts to define explainability in\nMachine Learning, establishing a novel definition that covers prior conceptual\npropositions with a major focus on the audience for which explainability is\nsought. We then propose and discuss about a taxonomy of recent contributions\nrelated to the explainability of different Machine Learning models, including\nthose aimed at Deep Learning methods for which a second taxonomy is built. This\nliterature analysis serves as the background for a series of challenges faced\nby XAI, such as the crossroads between data fusion and explainability. Our\nprospects lead toward the concept of Responsible Artificial Intelligence,\nnamely, a methodology for the large-scale implementation of AI methods in real\norganizations with fairness, model explainability and accountability at its\ncore. Our ultimate goal is to provide newcomers to XAI with a reference\nmaterial in order to stimulate future research advances, but also to encourage\nexperts and professionals from other disciplines to embrace the benefits of AI\nin their activity sectors, without any prior bias for its lack of\ninterpretability.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.10045v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12583v1",
    "title": "Solidarity should be a core ethical principle of Artificial Intelligence",
    "authors": [
      "Miguel Luengo-Oroz"
    ],
    "author_ids": [],
    "abstract": "Solidarity is one of the fundamental values at the heart of the construction\nof peaceful societies and present in more than one third of world's\nconstitutions. Still, solidarity is almost never included as a principle in\nethical guidelines for the development of AI. Solidarity as an AI principle (1)\nshares the prosperity created by AI, implementing mechanisms to redistribute\nthe augmentation of productivity for all; and shares the burdens, making sure\nthat AI does not increase inequality and no human is left behind. Solidarity as\nan AI principle (2) assesses the long term implications before developing and\ndeploying AI systems so no groups of humans become irrelevant because of AI\nsystems. Considering solidarity as a core principle for AI development will\nprovide not just an human-centric but a more humanity-centric approach to AI.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12583v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.09956v1",
    "title": "Artificial Intelligence and the Future of Psychiatry: Qualitative Findings from a Global Physician Survey",
    "authors": [
      "Charlotte Blease",
      "Cosima Locher",
      "Marisa Leon-Carlyle",
      "P. Murali Doraiswamy"
    ],
    "author_ids": [],
    "abstract": "The potential for machine learning to disrupt the medical profession is the\nsubject of ongoing debate within biomedical informatics. This study aimed to\nexplore psychiatrists' opinions about the potential impact of innovations in\nartificial intelligence and machine learning on psychiatric practice. In Spring\n2019, we conducted a web-based survey of 791 psychiatrists from 22 countries\nworldwide. The survey measured opinions about the likelihood future technology\nwould fully replace physicians in performing ten key psychiatric tasks. This\nstudy involved qualitative descriptive analysis of written response to three\nopen-ended questions in the survey. Comments were classified into four major\ncategories in relation to the impact of future technology on\npatient-psychiatric interactions, the quality of patient medical care, the\nprofession of psychiatry, and health systems. Overwhelmingly, psychiatrists\nwere skeptical that technology could fully replace human empathy. Many\npredicted that 'man and machine' would increasingly collaborate in undertaking\nclinical decisions, with mixed opinions about the benefits and harms of such an\narrangement. Participants were optimistic that technology might improve\nefficiencies and access to care, and reduce costs. Ethical and regulatory\nconsiderations received limited attention. This study presents timely\ninformation of psychiatrists' view about the scope of artificial intelligence\nand machine learning on psychiatric practice. Psychiatrists expressed divergent\nviews about the value and impact of future technology with worrying omissions\nabout practice guidelines, and ethical and regulatory issues.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09956v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.09929v2",
    "title": "Fairness evaluation during the conceptual design of heat grids with quantum annealers",
    "authors": [
      "Kelvin Loh"
    ],
    "author_ids": [],
    "abstract": "This paper presents a workflow to evaluate the macro scale thermal fairness\nfor producers in a district heating network during the conceptual design phase\nof such a network. It uses the workflow to evaluate two types of proposed\ntopologies for a future network to be constructed in Europe. The workflow is\nalso novel in the sense that it has been demonstrated in the paper that within\nthe implementation, the load balancing step can be solved readily by a quantum\ncomputing system (in particular, a quantum annealer from DWave). The concept of\nfairness is also addressed in the workflow and the results show that there\nexist an optimum number of producers for a given district heating topology.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.DM",
      "cs.SY",
      "math.OC",
      "quant-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09929v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.09875v2",
    "title": "Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry",
    "authors": [
      "David Wisth",
      "Marco Camurri",
      "Maurice Fallon"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present a novel factor graph formulation to estimate the\npose and velocity of a quadruped robot on slippery and deformable terrain. The\nfactor graph introduces a preintegrated velocity factor that incorporates\nvelocity inputs from leg odometry and also estimates related biases. From our\nexperimentation we have seen that it is difficult to model uncertainties at the\ncontact point such as slip or deforming terrain, as well as leg flexibility. To\naccommodate for these effects and to minimize leg odometry drift, we extend the\nrobot's state vector with a bias term for this preintegrated velocity factor.\nThe bias term can be accurately estimated thanks to the tight fusion of the\npreintegrated velocity factor with stereo vision and IMU factors, without which\nit would be unobservable. The system has been validated on several scenarios\nthat involve dynamic motions of the ANYmal robot on loose rocks, slopes and\nmuddy ground. We demonstrate a 26% improvement of relative pose error compared\nto our previous work and 52% compared to a state-of-the-art proprioceptive\nstate estimator.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09875v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.09786v1",
    "title": "On Fairness in Committee-based Blockchains",
    "authors": [
      "Yackolley Amoussou-Guenou",
      "Antonella del Pozzo",
      "Maria Potop-Butucaru",
      "Sara Tucci-Piergiovanni"
    ],
    "author_ids": [],
    "abstract": "Committee-based blockchains are among the most popular alternatives of\nproof-of-work based blockchains, such as Bitcoin. They provide strong\nconsistency (no fork) under classical assumptions, and avoid using\nenergy-consuming mechanisms to add new blocks in the blockchain. For each\nblock, these blockchains use a committee that executes Byzantine-fault tolerant\ndistributed consensus to decide the next block they will add in the blockchain.\nUnlike Bitcoin, where there is only one creator per block with high\nprobability, in committee-based blockchain any block is cooperatively created.\nIn order to incentivize committee members to participate to the creation of new\nblocks rewarding schemes have to be designed. In this paper, we study the\nfairness of rewarding in committee-based blockchains and we provide necessary\nand sufficient conditions on the system communication under which it is\npossible to have a fair reward mechanism.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09786v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.09779v2",
    "title": "Bridging the Gap Between $f$-GANs and Wasserstein GANs",
    "authors": [
      "Jiaming Song",
      "Stefano Ermon"
    ],
    "author_ids": [],
    "abstract": "Generative adversarial networks (GANs) have enjoyed much success in learning\nhigh-dimensional distributions. Learning objectives approximately minimize an\n$f$-divergence ($f$-GANs) or an integral probability metric (Wasserstein GANs)\nbetween the model and the data distribution using a discriminator. Wasserstein\nGANs enjoy superior empirical performance, but in $f$-GANs the discriminator\ncan be interpreted as a density ratio estimator which is necessary in some GAN\napplications. In this paper, we bridge the gap between $f$-GANs and Wasserstein\nGANs (WGANs). First, we list two constraints over variational $f$-divergence\nestimation objectives that preserves the optimal solution. Next, we minimize\nover a Lagrangian relaxation of the constrained objective, and show that it\ngeneralizes critic objectives of both $f$-GAN and WGAN. Based on this\ngeneralization, we propose a novel practical objective, named KL-Wasserstein\nGAN (KL-WGAN). We demonstrate empirical success of KL-WGAN on synthetic\ndatasets and real-world image generation benchmarks, and achieve\nstate-of-the-art FID scores on CIFAR10 image generation.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09779v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.09728v3",
    "title": "Convolutional Prototype Learning for Zero-Shot Recognition",
    "authors": [
      "Zhizhe Liu",
      "Xingxing Zhang",
      "Zhenfeng Zhu",
      "Shuai Zheng",
      "Yao Zhao",
      "Jian Cheng"
    ],
    "author_ids": [],
    "abstract": "Zero-shot learning (ZSL) has received increasing attention in recent years\nespecially in areas of fine-grained object recognition, retrieval, and image\ncaptioning. The key to ZSL is to transfer knowledge from the seen to the unseen\nclasses via auxiliary class attribute vectors. However, the popularly learned\nprojection functions in previous works cannot generalize well since they assume\nthe distribution consistency between seen and unseen domains at\nsample-level.Besides, the provided non-visual and unique class attributes can\nsignificantly degrade the recognition performance in semantic space. In this\npaper, we propose a simple yet effective convolutional prototype learning (CPL)\nframework for zero-shot recognition. By assuming distribution consistency at\ntask-level, our CPL is capable of transferring knowledge smoothly to recognize\nunseen samples.Furthermore, inside each task, discriminative visual prototypes\nare learned via a distance based training mechanism. Consequently, we can\nperform recognition in visual space, instead of semantic space. An extensive\ngroup of experiments are then carefully designed and presented, demonstrating\nthat CPL obtains more favorable effectiveness, over currently available\nalternatives under various settings.",
    "published_date": "2019-10-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09728v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.09242v1",
    "title": "On large-scale genre classification in symbolically encoded music by automatic identification of repeating patterns",
    "authors": [
      "Andres Ferraro",
      "Kjell Lemström"
    ],
    "author_ids": [],
    "abstract": "The importance of repetitions in music is well-known. In this paper, we study\nmusic repetitions in the context of effective and efficient automatic genre\nclassification in large-scale music-databases. We aim at enhancing the access\nand organization of pieces of music in Digital Libraries by allowing automatic\ncategorization of entire collections by considering only their musical content.\nWe handover to the public a set of genre-specific patterns to support research\nin musicology. The patterns can be used, for instance, to explore and analyze\nthe relations between musical genres. There are many existing algorithms that\ncould be used to identify and extract repeating patterns in symbolically\nencoded music. In our case, the extracted patterns are used as representations\nof the pieces of music on the underlying corpus and, consecutively, to train\nand evaluate a classifier to automatically identify genres. In this paper, we\napply two very fast algorithms enabling us to experiment on large and diverse\ncorpora. Thus, we are able to find patterns with strong discrimination power\nthat can be used in various applications. We carried out experiments on a\ncorpus containing over 40,000 MIDI files annotated with at least one genre. The\nexperiments suggest that our approach is scalable and capable of dealing with\nreal-world-size music collections.",
    "published_date": "2019-10-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09242v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.09170v1",
    "title": "Mining GOLD Samples for Conditional GANs",
    "authors": [
      "Sangwoo Mo",
      "Chiheon Kim",
      "Sungwoong Kim",
      "Minsu Cho",
      "Jinwoo Shin"
    ],
    "author_ids": [],
    "abstract": "Conditional generative adversarial networks (cGANs) have gained a\nconsiderable attention in recent years due to its class-wise controllability\nand superior quality for complex generation tasks. We introduce a simple yet\neffective approach to improving cGANs by measuring the discrepancy between the\ndata distribution and the model distribution on given samples. The proposed\nmeasure, coined the gap of log-densities (GOLD), provides an effective\nself-diagnosis for cGANs while being efficienty computed from the\ndiscriminator. We propose three applications of the GOLD: example re-weighting,\nrejection sampling, and active learning, which improve the training, inference,\nand data selection of cGANs, respectively. Our experimental results demonstrate\nthat the proposed methods outperform corresponding baselines for all three\napplications on different image datasets.",
    "published_date": "2019-10-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.09170v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12586v1",
    "title": "PC-Fairness: A Unified Framework for Measuring Causality-based Fairness",
    "authors": [
      "Yongkai Wu",
      "Lu Zhang",
      "Xintao Wu",
      "Hanghang Tong"
    ],
    "author_ids": [],
    "abstract": "A recent trend of fair machine learning is to define fairness as\ncausality-based notions which concern the causal connection between protected\nattributes and decisions. However, one common challenge of all causality-based\nfairness notions is identifiability, i.e., whether they can be uniquely\nmeasured from observational data, which is a critical barrier to applying these\nnotions to real-world situations. In this paper, we develop a framework for\nmeasuring different causality-based fairness. We propose a unified definition\nthat covers most of previous causality-based fairness notions, namely the\npath-specific counterfactual fairness (PC fairness). Based on that, we propose\na general method in the form of a constrained optimization problem for bounding\nthe path-specific counterfactual fairness under all unidentifiable situations.\nExperiments on synthetic and real-world datasets show the correctness and\neffectiveness of our method.",
    "published_date": "2019-10-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12586v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.08948v1",
    "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
    "authors": [
      "Yoan Dinkov",
      "Ahmed Ali",
      "Ivan Koychev",
      "Preslav Nakov"
    ],
    "author_ids": [],
    "abstract": "We address the problem of predicting the leading political ideology, i.e.,\nleft-center-right bias, for YouTube channels of news media. Previous work on\nthe problem has focused exclusively on text and on analysis of the language\nused, topics discussed, sentiment, and the like. In contrast, here we study\nvideos, which yields an interesting multimodal setup. Starting with gold\nannotations about the leading political ideology of major world news media from\nMedia Bias/Fact Check, we searched on YouTube to find their corresponding\nchannels, and we downloaded a recent sample of videos from each channel. We\ncrawled more than 1,000 YouTube hours along with the corresponding subtitles\nand metadata, thus producing a new multimodal dataset. We further developed a\nmultimodal deep-learning architecture for the task. Our analysis shows that the\nuse of acoustic signal helped to improve bias detection by more than 6%\nabsolute over using text and metadata only. We release the dataset to the\nresearch community, hoping to help advance the field of multi-modal political\nbias detection.",
    "published_date": "2019-10-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.SD",
      "eess.AS",
      "68T50",
      "I.2.7"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08948v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.08670v1",
    "title": "Context-Driven Data Mining through Bias Removal and Data Incompleteness Mitigation",
    "authors": [
      "Feras A. Batarseh",
      "Ajay Kulkarni"
    ],
    "author_ids": [],
    "abstract": "The results of data mining endeavors are majorly driven by data quality.\nThroughout these deployments, serious show-stopper problems are still\nunresolved, such as: data collection ambiguities, data imbalance, hidden biases\nin data, the lack of domain information, and data incompleteness. This paper is\nbased on the premise that context can aid in mitigating these issues. In a\ntraditional data science lifecycle, context is not considered. Context-driven\nData Science Lifecycle (C-DSL); the main contribution of this paper, is\ndeveloped to address these challenges. Two case studies (using data-sets from\nsports events) are developed to test C-DSL. Results from both case studies are\nevaluated using common data mining metrics such as: coefficient of\ndetermination (R2 value) and confusion matrices. The work presented in this\npaper aims to re-define the lifecycle and introduce tangible improvements to\nits outcomes.",
    "published_date": "2019-10-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08670v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.08665v1",
    "title": "NASIB: Neural Architecture Search withIn Budget",
    "authors": [
      "Abhishek Singh",
      "Anubhav Garg",
      "Jinan Zhou",
      "Shiv Ram Dubey",
      "Debo Dutta"
    ],
    "author_ids": [],
    "abstract": "Neural Architecture Search (NAS) represents a class of methods to generate\nthe optimal neural network architecture and typically iterate over candidate\narchitectures till convergence over some particular metric like validation\nloss. They are constrained by the available computation resources, especially\nin enterprise environments. In this paper, we propose a new approach for NAS,\ncalled NASIB, which adapts and attunes to the computation resources (budget)\navailable by varying the exploration vs. exploitation trade-off. We reduce the\nexpert bias by searching over an augmented search space induced by\nSuperkernels. The proposed method can provide the architecture search useful\nfor different computation resources and different domains beyond image\nclassification of natural images where we lack bespoke architecture motifs and\ndomain expertise. We show, on CIFAR10, that itis possible to search over a\nspace that comprises of 12x more candidate operations than the traditional\nprior art in just 1.5 GPU days, while reaching close to state of the art\naccuracy. While our method searches over an exponentially larger search space,\nit could lead to novel architectures that require lesser domain expertise,\ncompared to the majority of the existing methods.",
    "published_date": "2019-10-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08665v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.08519v1",
    "title": "Texture Bias Of CNNs Limits Few-Shot Classification Performance",
    "authors": [
      "Sam Ringer",
      "Will Williams",
      "Tom Ash",
      "Remi Francis",
      "David MacLeod"
    ],
    "author_ids": [],
    "abstract": "Accurate image classification given small amounts of labelled data (few-shot\nclassification) remains an open problem in computer vision. In this work we\nexamine how the known texture bias of Convolutional Neural Networks (CNNs)\naffects few-shot classification performance. Although texture bias can help in\nstandard image classification, in this work we show it significantly harms\nfew-shot classification performance. After correcting this bias we demonstrate\nstate-of-the-art performance on the competitive miniImageNet task using a\nmethod far simpler than the current best performing few-shot learning\napproaches.",
    "published_date": "2019-10-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08519v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.08229v1",
    "title": "Dynamic Resource Allocation and Activity Management for Energy Efficiency and Fairness in Heterogeneous Networks",
    "authors": [
      "Amir Behrouzi-Far",
      "Ezhan Karasan"
    ],
    "author_ids": [],
    "abstract": "Higher energy consumption of Heterogeneous Networks (HetNet), compared to\nMacro Only Networks (MONET), raises a great concern about the energy efficiency\nof HetNets. In this work we study a dynamic activation strategy, which changes\nthe state of small cells between Active and Idle according to the dynamically\nchanging user traffic, in order to increase the energy efficiency of HetNets.\nMoreover, we incorporate dynamic inter-tier bandwidth allocation to our model.\nThe proposed Dynamic Bandwidth Allocation and Dynamic Activation (DBADA)\nstrategy is applied in cell-edge deployment of small cells, where HotSpot\nregions are located far from the master base station. Our objective is to\nmaximize the sum utility of the network with minimum energy consumption. To\nensure proportional fairness among users, we used logarithmic utility function.\nTo evaluate the performance of the proposed strategy, the median, 10-percentile\nand the sum of users' data rates and the network energy consumption are\nevaluated by simulation. Our simulation results shows that the DBADA strategy\nimproves the energy consumed per unit of users' data rate by up to $25\\%$. It\nalso achieves lower energy consumption by at least $25\\%$, compared to always\nactive scenario for small cells.",
    "published_date": "2019-10-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08229v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.08036v1",
    "title": "Predicting retrosynthetic pathways using a combined linguistic model and hyper-graph exploration strategy",
    "authors": [
      "Philippe Schwaller",
      "Riccardo Petraglia",
      "Valerio Zullo",
      "Vishnu H Nair",
      "Rico Andreas Haeuselmann",
      "Riccardo Pisoni",
      "Costas Bekas",
      "Anna Iuliano",
      "Teodoro Laino"
    ],
    "author_ids": [],
    "abstract": "We present an extension of our Molecular Transformer architecture combined\nwith a hyper-graph exploration strategy for automatic retrosynthesis route\nplanning without human intervention. The single-step retrosynthetic model sets\na new state of the art for predicting reactants as well as reagents, solvents\nand catalysts for each retrosynthetic step. We introduce new metrics (coverage,\nclass diversity, round-trip accuracy and Jensen-Shannon divergence) to evaluate\nthe single-step retrosynthetic models, using the forward prediction and a\nreaction classification model always based on the transformer architecture. The\nhypergraph is constructed on the fly, and the nodes are filtered and further\nexpanded based on a Bayesian-like probability. We critically assessed the\nend-to-end framework with several retrosynthesis examples from literature and\nacademic exams. Overall, the frameworks has a very good performance with few\nweaknesses due to the bias induced during the training process. The use of the\nnewly introduced metrics opens up the possibility to optimize entire\nretrosynthetic frameworks through focusing on the performance of the\nsingle-step model only.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08036v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07999v1",
    "title": "DeepFork: Supervised Prediction of Information Diffusion in GitHub",
    "authors": [
      "Ramya Akula",
      "Niloofar Yousefi",
      "Ivan Garibay"
    ],
    "author_ids": [],
    "abstract": "Information spreads on complex social networks extremely fast, in other\nwords, a piece of information can go viral within no time. Often it is hard to\nbarricade this diffusion prior to the significant occurrence of chaos, be it a\nsocial media or an online coding platform. GitHub is one such trending online\nfocal point for any business to reach their potential contributors and\ncustomers, simultaneously. By exploiting such software development paradigm,\nmillions of free software emerged lately in diverse communities. To understand\nhuman influence, information spread and evolution of transmitted information\namong assorted users in GitHub, we developed a deep neural network model:\nDeepFork, a supervised machine learning based approach that aims to predict\ninformation diffusion in complex social networks; considering node as well as\ntopological features. In our empirical studies, we observed that information\ndiffusion can be detected by link prediction using supervised learning.\nDeepFork outperforms other machine learning models as it better learns the\ndiscriminative patterns from the input features. DeepFork aids in understanding\ninformation spread and evolution through a bipartite network of users and\nrepositories i.e., information flow from a user to repository to user.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07999v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07892v3",
    "title": "WOTBoost: Weighted Oversampling Technique in Boosting for imbalanced learning",
    "authors": [
      "Wenhao Zhang",
      "Ramin Ramezani",
      "Arash Naeim"
    ],
    "author_ids": [],
    "abstract": "Machine learning classifiers often stumble over imbalanced datasets where\nclasses are not equally represented. This inherent bias towards the majority\nclass may result in low accuracy in labeling minority class. Imbalanced\nlearning is prevalent in many real-world applications, such as medical\nresearch, network intrusion detection, and fraud detection in credit card\ntransactions, etc. A good number of research works have been reported to tackle\nthis challenging problem. For example, Synthetic Minority Over-sampling\nTEchnique (SMOTE) and ADAptive SYNthetic sampling approach (ADASYN) use\noversampling techniques to balance the skewed datasets. In this paper, we\npropose a novel method that combines a Weighted Oversampling Technique and\nensemble Boosting method (WOTBoost) to improve the classification accuracy of\nminority data without sacrificing the accuracy of the majority class. WOTBoost\nadjusts its oversampling strategy at each round of boosting to synthesize more\ntargeted minority data samples. The adjustment is enforced using a weighted\ndistribution. We compare WOTBoost with other four classification models (i.e.,\ndecision tree, SMOTE + decision tree, ADASYN + decision tree, SMOTEBoost)\nextensively on 18 public accessible imbalanced datasets. WOTBoost achieves the\nbest G mean on 6 datasets and highest AUC score on 7 datasets.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07892v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07870v2",
    "title": "Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing",
    "authors": [
      "Sanghamitra Dutta",
      "Dennis Wei",
      "Hazar Yueksel",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Kush R. Varshney"
    ],
    "author_ids": [],
    "abstract": "A trade-off between accuracy and fairness is almost taken as a given in the\nexisting literature on fairness in machine learning. Yet, it is not preordained\nthat accuracy should decrease with increased fairness. Novel to this work, we\nexamine fair classification through the lens of mismatched hypothesis testing:\ntrying to find a classifier that distinguishes between two ideal distributions\nwhen given two mismatched distributions that are biased. Using Chernoff\ninformation, a tool in information theory, we theoretically demonstrate that,\ncontrary to popular belief, there always exist ideal distributions such that\noptimal fairness and accuracy (with respect to the ideal distributions) are\nachieved simultaneously: there is no trade-off. Moreover, the same classifier\nyields the lack of a trade-off with respect to ideal distributions while\nyielding a trade-off when accuracy is measured with respect to the given\n(possibly biased) dataset. To complement our main result, we formulate an\noptimization to find ideal distributions and derive fundamental limits to\nexplain why a trade-off exists on the given biased dataset. We also derive\nconditions under which active data collection can alleviate the\nfairness-accuracy trade-off in the real world. Our results lead us to contend\nthat it is problematic to measure accuracy with respect to data that reflects\nbias, and instead, we should be considering accuracy with respect to ideal,\nunbiased data.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07870v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07843v2",
    "title": "Max-min Fairness of K-user Cooperative Rate-Splitting in MISO Broadcast Channel with User Relaying",
    "authors": [
      "Yijie Mao",
      "Bruno Clerckx",
      "Jian Zhang",
      "Victor O. K. Li",
      "Mohammed Arafah"
    ],
    "author_ids": [],
    "abstract": "Cooperative Rate-Splitting (CRS) strategy, relying on linearly precoded\nrate-splitting at the transmitter and opportunistic transmission of the common\nmessage by the relaying user, has recently been shown to outperform typical\nNon-cooperative Rate-Splitting (NRS), Cooperative Non-Orthogonal Multiple\nAccess (C-NOMA) and Space Division Multiple Access (SDMA) in a two-user\nMultiple Input Single Output (MISO) Broadcast Channel (BC) with user relaying.\nIn this work, the existing two-user CRS transmission strategy is generalized to\nthe K-user case. We study the problem of jointly optimizing the precoders,\nmessage split, time slot allocation, and relaying user scheduling with the\nobjective of maximizing the minimum rate among users. An efficient\nself-organizing relaying protocol is first proposed followed by a Successive\nConvex Approximation (SCA)-based algorithm to jointly optimize time slot,\nprecoders and message split. Numerical results show that the worst-case\nachievable rate achieved by CRS is significantly increased over that of NRS and\nSDMA in a wide range of network loads and user deployments. Importantly, the\nproposed SCA-based algorithm dramatically reduces the computational complexity\nwithout any rate loss compared with the conventional algorithm in the\nliterature of CRS. Therefore, we conclude that the proposed K-user CRS is more\npowerful than the existing transmission schemes.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07843v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.07809v1",
    "title": "Do you see what I see? Taking perspective of others using facial images",
    "authors": [
      "Yustinus Eko Soelistio"
    ],
    "author_ids": [],
    "abstract": "Albeit many HCI / emotion recognition studies use facial expressive images,\nfew scrutinize the accuracies of the people (experimenters and participants) in\nperceiving the expressions representing the intended emotions. The\nmisinterpretation of the expression will put bias in the data and introduce\nquestions on the validity of the studies. The possibility of misinterpretation\nof the expressions will be the focus of the experiment conducted in this study.\nThe experiment will evaluate the ability of people in taking the perspective of\nothers in spite of their current emotions and gender, and whether the\nexpressions can be universally perceived. This study find that it is relatively\nsafe to use facial expressive images for research as long as the emotions are\nexclusively within the six basic emotions.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07809v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.07775v3",
    "title": "The other side of the Coin: Risks of the Libra Blockchain",
    "authors": [
      "Louis Abraham",
      "Dominique Guégan"
    ],
    "author_ids": [],
    "abstract": "Libra was presented as a cryptocurrency on June 18, 2019 by Facebook. On the\nsame day, Facebook announced plans for Calibra, a subsidiary in charge of the\ndevelopment of an electronic wallet and financial services. In view of the\nprimary risk of sovereignty posed by the creation of Libra, regulators and\nCentral Banks quickly took very clear positions against the project and\nexpressed a lot of questions focusing on regulation aspects and national\nsovereignty.\n  The purpose of this paper is to provide a holistic analysis of the project\nencompassing several aspects of its implementation and the issues it raises. We\naddress a set of questions that are part of the cryptocurrency environment and\nblockchain technology that support the Libra project. We describe the\ngovernance of the project based on two levels, one for the Association and the\nother for the Libra Blockchain. We identify the main risks considering at the\nsame time political, financial, economic, technological and ethical risks. We\nemphasize the difficulty to regulate such a project as it will depend on\nseveral countries whose legislations are very different. Finally, the future of\nthis kind of projects is discussed through the emergence of Central Bank\nDigital Currencies.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07775v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.07747v4",
    "title": "Mutual Information-driven Subject-invariant and Class-relevant Deep Representation Learning in BCI",
    "authors": [
      "Eunjin Jeon",
      "Wonjun Ko",
      "Jee Seok Yoon",
      "Heung-Il Suk"
    ],
    "author_ids": [],
    "abstract": "In recent years, deep learning-based feature representation methods have\nshown a promising impact in electroencephalography (EEG)-based brain-computer\ninterface (BCI). Nonetheless, owing to high intra- and inter-subject\nvariabilities, many studies on decoding EEG were designed in a subject-specific\nmanner by using calibration samples, with no concern of its practical use,\nhampered by time-consuming steps and a large data requirement. To this end,\nrecent studies adopted a transfer learning strategy, especially domain\nadaptation techniques. Among those, to our knowledge, an adversarial learning\nhas shown its potential in BCIs. In the meantime, it is known that adversarial\nlearning-based domain adaptation methods are prone to negative transfer that\ndisrupts learning generalized feature representations, applicable to diverse\ndomains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel\nframework that learns class-relevant and subject-invariant feature\nrepresentations in an information-theoretic manner, without using adversarial\nlearning. To be specific, we devise two operational components in a deep\nnetwork that explicitly estimate mutual information between feature\nrepresentations; (1) to decompose features in an intermediate layer into\nclass-relevant and class-irrelevant ones, (2) to enrich class-discriminative\nfeature representation. On two large EEG datasets, we validated the\neffectiveness of our proposed framework by comparing with several comparative\nmethods in performance. Further, we conducted rigorous analyses by performing\nan ablation study in regard to the components in our network, explaining our\nmodel's decision on input EEG signals via layer-wise relevance propagation, and\nvisualizing the distribution of learned features via t-SNE.",
    "published_date": "2019-10-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.HC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07747v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.10486v3",
    "title": "Does Gender Matter? Towards Fairness in Dialogue Systems",
    "authors": [
      "Haochen Liu",
      "Jamell Dacon",
      "Wenqi Fan",
      "Hui Liu",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "author_ids": [],
    "abstract": "Recently there are increasing concerns about the fairness of Artificial\nIntelligence (AI) in real-world applications such as computer vision and\nrecommendations. For example, recognition algorithms in computer vision are\nunfair to black people such as poorly detecting their faces and inappropriately\nidentifying them as \"gorillas\". As one crucial application of AI, dialogue\nsystems have been extensively applied in our society. They are usually built\nwith real human conversational data; thus they could inherit some fairness\nissues which are held in the real world. However, the fairness of dialogue\nsystems has not been well investigated. In this paper, we perform a pioneering\nstudy about the fairness issues in dialogue systems. In particular, we\nconstruct a benchmark dataset and propose quantitative measures to understand\nfairness in dialogue models. Our studies demonstrate that popular dialogue\nmodels show significant prejudice towards different genders and races. Besides,\nto mitigate the bias in dialogue systems, we propose two simple but effective\ndebiasing methods. Experiments show that our methods can reduce the bias in\ndialogue systems significantly. The dataset and the implementation are released\nto foster fairness research in dialogue systems.",
    "published_date": "2019-10-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.10486v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07604v2",
    "title": "Global Saliency: Aggregating Saliency Maps to Assess Dataset Artefact Bias",
    "authors": [
      "Jacob Pfau",
      "Albert T. Young",
      "Maria L. Wei",
      "Michael J. Keiser"
    ],
    "author_ids": [],
    "abstract": "In high-stakes applications of machine learning models, interpretability\nmethods provide guarantees that models are right for the right reasons. In\nmedical imaging, saliency maps have become the standard tool for determining\nwhether a neural model has learned relevant robust features, rather than\nartefactual noise. However, saliency maps are limited to local model\nexplanation because they interpret predictions on an image-by-image basis. We\npropose aggregating saliency globally, using semantic segmentation masks, to\nprovide quantitative measures of model bias across a dataset. To evaluate\nglobal saliency methods, we propose two metrics for quantifying the validity of\nsaliency explanations. We apply the global saliency method to skin lesion\ndiagnosis to determine the effect of artefacts, such as ink, on model bias.",
    "published_date": "2019-10-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07604v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07186v1",
    "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation",
    "authors": [
      "Ziyang Tang",
      "Yihao Feng",
      "Lihong Li",
      "Dengyong Zhou",
      "Qiang Liu"
    ],
    "author_ids": [],
    "abstract": "Infinite horizon off-policy policy evaluation is a highly challenging task\ndue to the excessively large variance of typical importance sampling (IS)\nestimators. Recently, Liu et al. (2018a) proposed an approach that\nsignificantly reduces the variance of infinite-horizon off-policy evaluation by\nestimating the stationary density ratio, but at the cost of introducing\npotentially high biases due to the error in density ratio estimation. In this\npaper, we develop a bias-reduced augmentation of their method, which can take\nadvantage of a learned value function to obtain higher accuracy. Our method is\ndoubly robust in that the bias vanishes when either the density ratio or the\nvalue function estimation is perfect. In general, when either of them is\naccurate, the bias can also be reduced. Both theoretical and empirical results\nshow that our method yields significant advantages over previous methods.",
    "published_date": "2019-10-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07186v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07162v3",
    "title": "Conditional Learning of Fair Representations",
    "authors": [
      "Han Zhao",
      "Amanda Coston",
      "Tameem Adel",
      "Geoffrey J. Gordon"
    ],
    "author_ids": [],
    "abstract": "We propose a novel algorithm for learning fair representations that can\nsimultaneously mitigate two notions of disparity among different demographic\nsubgroups in the classification setting. Two key components underpinning the\ndesign of our algorithm are balanced error rate and conditional alignment of\nrepresentations. We show how these two components contribute to ensuring\naccuracy parity and equalized false-positive and false-negative rates across\ngroups without impacting demographic parity. Furthermore, we also demonstrate\nboth in theory and on two real-world experiments that the proposed algorithm\nleads to a better utility-fairness trade-off on balanced datasets compared with\nexisting algorithms on learning fair representations for classification.",
    "published_date": "2019-10-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07162v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07089v1",
    "title": "Challenges of Human-Aware AI Systems",
    "authors": [
      "Subbarao Kambhampati"
    ],
    "author_ids": [],
    "abstract": "From its inception, AI has had a rather ambivalent relationship to\nhumans---swinging between their augmentation and replacement. Now, as AI\ntechnologies enter our everyday lives at an ever increasing pace, there is a\ngreater need for AI systems to work synergistically with humans. To do this\neffectively, AI systems must pay more attention to aspects of intelligence that\nhelped humans work with each other---including social intelligence. I will\ndiscuss the research challenges in designing such human-aware AI systems,\nincluding modeling the mental states of humans in the loop, recognizing their\ndesires and intentions, providing proactive support, exhibiting explicable\nbehavior, giving cogent explanations on demand, and engendering trust. I will\nsurvey the progress made so far on these challenges, and highlight some\npromising directions. I will also touch on the additional ethical quandaries\nthat such systems pose. I will end by arguing that the quest for human-aware AI\nsystems broadens the scope of AI enterprise, necessitates and facilitates true\ninter-disciplinary collaborations, and can go a long way towards increasing\npublic acceptance of AI technologies.",
    "published_date": "2019-10-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07089v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06962v2",
    "title": "SegSort: Segmentation by Discriminative Sorting of Segments",
    "authors": [
      "Jyh-Jing Hwang",
      "Stella X. Yu",
      "Jianbo Shi",
      "Maxwell D. Collins",
      "Tien-Ju Yang",
      "Xiao Zhang",
      "Liang-Chieh Chen"
    ],
    "author_ids": [],
    "abstract": "Almost all existing deep learning approaches for semantic segmentation tackle\nthis task as a pixel-wise classification problem. Yet humans understand a scene\nnot in terms of pixels, but by decomposing it into perceptual groups and\nstructures that are the basic building blocks of recognition. This motivates us\nto propose an end-to-end pixel-wise metric learning approach that mimics this\nprocess. In our approach, the optimal visual representation determines the\nright segmentation within individual images and associates segments with the\nsame semantic classes across images. The core visual learning problem is\ntherefore to maximize the similarity within segments and minimize the\nsimilarity between segments. Given a model trained this way, inference is\nperformed consistently by extracting pixel-wise embeddings and clustering, with\nthe semantic label determined by the majority vote of its nearest neighbors\nfrom an annotated set.\n  As a result, we present the SegSort, as a first attempt using deep learning\nfor unsupervised semantic segmentation, achieving $76\\%$ performance of its\nsupervised counterpart. When supervision is available, SegSort shows consistent\nimprovements over conventional approaches based on pixel-wise softmax training.\nAdditionally, our approach produces more precise boundaries and consistent\nregion predictions. The proposed SegSort further produces an interpretable\nresult, as each choice of label can be easily understood from the retrieved\nnearest segments.",
    "published_date": "2019-10-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06962v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06950v1",
    "title": "Jointly Discriminative and Generative Recurrent Neural Networks for Learning from fMRI",
    "authors": [
      "Nicha C. Dvornek",
      "Xiaoxiao Li",
      "Juntang Zhuang",
      "James S. Duncan"
    ],
    "author_ids": [],
    "abstract": "Recurrent neural networks (RNNs) were designed for dealing with time-series\ndata and have recently been used for creating predictive models from functional\nmagnetic resonance imaging (fMRI) data. However, gathering large fMRI datasets\nfor learning is a difficult task. Furthermore, network interpretability is\nunclear. To address these issues, we utilize multitask learning and design a\nnovel RNN-based model that learns to discriminate between classes while\nsimultaneously learning to generate the fMRI time-series data. Employing the\nlong short-term memory (LSTM) structure, we develop a discriminative model\nbased on the hidden state and a generative model based on the cell state. The\naddition of the generative model constrains the network to learn functional\ncommunities represented by the LSTM nodes that are both consistent with the\ndata generation as well as useful for the classification task. We apply our\napproach to the classification of subjects with autism vs. healthy controls\nusing several datasets from the Autism Brain Imaging Data Exchange. Experiments\nshow that our jointly discriminative and generative model improves\nclassification learning while also producing robust and meaningful functional\ncommunities for better model understanding.",
    "published_date": "2019-10-15T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.LG",
      "q-bio.NC",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06950v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.07633v1",
    "title": "Towards a Precipitation Bias Corrector against Noise and Maldistribution",
    "authors": [
      "Xiaoyang Xu",
      "Yiqun Liu",
      "Hanqing Chao",
      "Youcheng Luo",
      "Hai Chu",
      "Lei Chen",
      "Junping Zhang",
      "Leiming Ma"
    ],
    "author_ids": [],
    "abstract": "With broad applications in various public services like aviation management\nand urban disaster warning, numerical precipitation prediction plays a crucial\nrole in weather forecast. However, constrained by the limitation of observation\nand conventional meteorological models, the numerical precipitation predictions\nare often highly biased. To correct this bias, classical correction methods\nheavily depend on profound experts who have knowledge in aerodynamics,\nthermodynamics and meteorology. As precipitation can be influenced by countless\nfactors, however, the performances of these expert-driven methods can drop\ndrastically when some un-modeled factors change. To address this issue, this\npaper presents a data-driven deep learning model which mainly includes two\nblocks, i.e. a Denoising Autoencoder Block and an Ordinal Regression Block. To\nthe best of our knowledge, it is the first expert-free models for bias\ncorrection. The proposed model can effectively correct the numerical\nprecipitation prediction based on 37 basic meteorological data from European\nCentre for Medium-Range Weather Forecasts (ECMWF). Experiments indicate that\ncompared with several classical machine learning algorithms and deep learning\nmodels, our method achieves the best correcting performance and meteorological\nindex, namely the threat scores (TS), obtaining satisfactory visualization\neffect.",
    "published_date": "2019-10-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07633v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06673v1",
    "title": "SafeCritic: Collision-Aware Trajectory Prediction",
    "authors": [
      "Tessa van der Heiden",
      "Naveen Shankar Nagaraja",
      "Christian Weiss",
      "Efstratios Gavves"
    ],
    "author_ids": [],
    "abstract": "Navigating complex urban environments safely is a key to realize fully\nautonomous systems. Predicting future locations of vulnerable road users, such\nas pedestrians and cyclists, thus, has received a lot of attention in the\nrecent years. While previous works have addressed modeling interactions with\nthe static (obstacles) and dynamic (humans) environment agents, we address an\nimportant gap in trajectory prediction. We propose SafeCritic, a model that\nsynergizes generative adversarial networks for generating multiple \"real\"\ntrajectories with reinforcement learning to generate \"safe\" trajectories. The\nDiscriminator evaluates the generated candidates on whether they are consistent\nwith the observed inputs. The Critic network is environmentally aware to prune\ntrajectories that are in collision or are in violation with the environment.\nThe auto-encoding loss stabilizes training and prevents mode-collapse. We\ndemonstrate results on two large scale data sets with a considerable\nimprovement over state-of-the-art. We also show that the Critic is able to\nclassify the safety of trajectories.",
    "published_date": "2019-10-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06673v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06569v1",
    "title": "Probabilistic Time of Arrival Localization",
    "authors": [
      "Fernando Perez-Cruz",
      "Pablo M. Olmos",
      "Michael Minyi Zhang",
      "Howard Huang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we take a new approach for time of arrival geo-localization.\nWe show that the main sources of error in metropolitan areas are due to\nenvironmental imperfections that bias our solutions, and that we can rely on a\nprobabilistic model to learn and compensate for them. The resulting\nlocalization error is validated using measurements from a live LTE cellular\nnetwork to be less than 10 meters, representing an order-of-magnitude\nimprovement.",
    "published_date": "2019-10-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06569v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1911.05497v1",
    "title": "Going Negative Online? -- A Study of Negative Advertising on Social Media",
    "authors": [
      "Hongtao Liu"
    ],
    "author_ids": [],
    "abstract": "A growing number of empirical studies suggest that negative advertising is\neffective in campaigning, while the mechanisms are rarely mentioned. With the\nscandal of Cambridge Analytica and Russian intervention behind the Brexit and\nthe 2016 presidential election, people have become aware of the political ads\non social media and have pressured congress to restrict political advertising\non social media. Following the related legislation, social media companies\nbegan disclosing their political ads archive for transparency during the summer\nof 2018 when the midterm election campaign was just beginning. This research\ncollects the data of the related political ads in the context of the U.S.\nmidterm elections since August to study the overall pattern of political ads on\nsocial media and uses sets of machine learning methods to conduct sentiment\nanalysis on these ads to classify the negative ads. A novel approach is applied\nthat uses AI image recognition to study the image data. Through data\nvisualization, this research shows that negative advertising is still the\nminority, Republican advertisers and third party organizations are more likely\nto engage in negative advertising than their counterparts. Based on ordinal\nregressions, this study finds that anger evoked information-seeking is one of\nthe main mechanisms causing negative ads to be more engaging and effective\nrather than the negative bias theory. Overall, this study provides a unique\nunderstanding of political advertising on social media by applying innovative\ndata science methods. Further studies can extend the findings, methods, and\ndatasets in this study, and several suggestions are given for future research.",
    "published_date": "2019-10-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "stat.AP",
      "stat.CO",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05497v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05996v1",
    "title": "A unified framework of predicting binary interestingness of images based on discriminant correlation analysis and multiple kernel learning",
    "authors": [
      "Qiang Sun",
      "Liting Wang",
      "Maohui Li",
      "Longtao Zhang",
      "Yuxiang Yang"
    ],
    "author_ids": [],
    "abstract": "In the modern content-based image retrieval systems, there is an increasingly\ninterest in constructing a computationally effective model to predict the\ninterestingness of images since the measure of image interestingness could\nimprove the human-centered search satisfaction and the user experience in\ndifferent applications. In this paper, we propose a unified framework to\npredict the binary interestingness of images based on discriminant correlation\nanalysis (DCA) and multiple kernel learning (MKL) techniques. More specially,\non the one hand, to reduce feature redundancy in describing the interestingness\ncues of images, the DCA or multi-set discriminant correlation analysis (MDCA)\ntechnique is adopted to fuse multiple feature sets of the same type for\nindividual cues by taking into account the class structure among the samples\ninvolved to describe the three classical interestingness cues,\nunusualness,aesthetics as well as general preferences, with three sets of\ncompact and representative features; on the other hand, to make good use of the\nheterogeneity from the three sets of high-level features for describing the\ninterestingness cues, the SimpleMKL method is employed to enhance the\ngeneralization ability of the built model for the task of the binary\ninterestingness classification. Experimental results on the publicly-released\ninterestingness prediction data set have demonstrated the rationality and\neffectiveness of the proposed framework in the binary prediction of image\ninterestingness where we have conducted several groups of comparative studies\nacross different interestingness feature combinations, different\ninterestingness cues, as well as different feature types for the three\ninterestingness cues.",
    "published_date": "2019-10-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05996v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.11235v2",
    "title": "Rethinking Exposure Bias In Language Modeling",
    "authors": [
      "Yifan Xu",
      "Kening Zhang",
      "Haoyu Dong",
      "Yuezhou Sun",
      "Wenlong Zhao",
      "Zhuowen Tu"
    ],
    "author_ids": [],
    "abstract": "Exposure bias describes the phenomenon that a language model trained under\nthe teacher forcing schema may perform poorly at the inference stage when its\npredictions are conditioned on its previous predictions unseen from the\ntraining corpus. Recently, several generative adversarial networks (GANs) and\nreinforcement learning (RL) methods have been introduced to alleviate this\nproblem. Nonetheless, a common issue in RL and GANs training is the sparsity of\nreward signals. In this paper, we adopt two simple strategies, multi-range\nreinforcing, and multi-entropy sampling, to amplify and denoise the reward\nsignal. Our model produces an improvement over competing models with regards to\nBLEU scores and road exam, a new metric we designed to measure the robustness\nagainst exposure bias in language models.",
    "published_date": "2019-10-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.11235v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05755v3",
    "title": "The Impact of Popularity Bias on Fairness and Calibration in Recommendation",
    "authors": [
      "Himan Abdollahpouri",
      "Masoud Mansoury",
      "Robin Burke",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Recently there has been a growing interest in fairness-aware recommender\nsystems, including fairness in providing consistent performance across\ndifferent users or groups of users. A recommender system could be considered\nunfair if the recommendations do not fairly represent the tastes of a certain\ngroup of users while other groups receive recommendations that are consistent\nwith their preferences. In this paper, we use a metric called miscalibration\nfor measuring how a recommendation algorithm is responsive to users' true\npreferences and we consider how various algorithms may result in different\ndegrees of miscalibration. A well-known type of bias in recommendation is\npopularity bias where few popular items are over-represented in\nrecommendations, while the majority of other items do not get significant\nexposure. We conjecture that popularity bias is one important factor leading to\nmiscalibration in recommendation. Our experimental results using two real-world\ndatasets show that there is a strong correlation between how different user\ngroups are affected by algorithmic popularity bias and their level of interest\nin popular items. Moreover, we show algorithms with greater popularity bias\namplification tend to have greater miscalibration.",
    "published_date": "2019-10-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05755v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.05723v1",
    "title": "Seasonal Entropy, Diversity and Inequality Measures of Submitted and Accepted Papers Distributions In Peer-Reviewed Journals",
    "authors": [
      "Marcel Ausloos",
      "Olgica Nedic",
      "Aleksandar Dekanski"
    ],
    "author_ids": [],
    "abstract": "This paper presents a novel method for finding features in the analysis of\nvariable distributions stemming from time series. We apply the methodology to\nthe case of submitted and accepted papers in peer-reviewed journals. We provide\na comparative study of editorial decisions for papers submitted to two\npeer-reviewed journals: the Journal of the Serbian Chemical Society (JSCS) and\nthis MDPI Entropy journal. We cover three recent years for which the fate of\nsubmitted papers, about 600 papers to JSCS and 2500 to Entropy, is completely\ndetermined. Instead of comparing the number distributions of these papers as a\nfunction of time with respect to a uniform distribution, we analyze the\nrelevant probabilities, from which we derive the information entropy. It is\nargued that such probabilities are indeed more relevant for authors than the\nactual number of submissions. We tie this entropy analysis to the so called\ndiversity of the variable distributions. Furthermore, we emphasize the\ncorrespondence between the entropy and the diversity with inequality measures,\nlike the Herfindahl-Hirschman index and the Theil index, itself being in the\nclass of entropy measures; the Gini coefficient which also measures the\ndiversity in ranking is calculated for further discussion. In this sample, the\nseasonal aspects of the peer review process are outlined. It is found that the\nuse of such indices, non linear transformations of the data distributions,\nallow to distinguish features and evolutions of peer review process as a\nfunction of time as well as comparing non-uniformity of distributions.\nFurthermore, t- and z- statistical tests are applied in order to measure the\nsignificance (p-level) of the findings, i.e. whether papers are more likely to\nbe accepted if they are submitted during a few specific months or \"season\"; the\npredictability strength depends on the journal.",
    "published_date": "2019-10-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05723v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.05657v1",
    "title": "How are attributes expressed in face DCNNs?",
    "authors": [
      "Prithviraj Dhar",
      "Ankan Bansal",
      "Carlos D. Castillo",
      "Joshua Gleason",
      "P. Jonathon Phillips",
      "Rama Chellappa"
    ],
    "author_ids": [],
    "abstract": "As deep networks become increasingly accurate at recognizing faces, it is\nvital to understand how these networks process faces. While these networks are\nsolely trained to recognize identities, they also contain face related\ninformation such as sex, age, and pose of the face. The networks are not\ntrained to learn these attributes. We introduce expressivity as a measure of\nhow much a feature vector informs us about an attribute, where a feature vector\ncan be from internal or final layers of a network. Expressivity is computed by\na second neural network whose inputs are features and attributes. The output of\nthe second neural network approximates the mutual information between feature\nvectors and an attribute. We investigate the expressivity for two different\ndeep convolutional neural network (DCNN) architectures: a Resnet-101 and an\nInception Resnet v2. In the final fully connected layer of the networks, we\nfound the order of expressivity for facial attributes to be Age > Sex > Yaw.\nAdditionally, we studied the changes in the encoding of facial attributes over\ntraining iterations. We found that as training progresses, expressivities of\nyaw, sex, and age decrease. Our technique can be a tool for investigating the\nsources of bias in a network and a step towards explaining the network's\nidentity decisions.",
    "published_date": "2019-10-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05657v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05646v1",
    "title": "\"Bring Your Own Greedy\"+Max: Near-Optimal $1/2$-Approximations for Submodular Knapsack",
    "authors": [
      "Dmitrii Avdiukhin",
      "Grigory Yaroslavtsev",
      "Samson Zhou"
    ],
    "author_ids": [],
    "abstract": "The problem of selecting a small-size representative summary of a large\ndataset is a cornerstone of machine learning, optimization and data science.\nMotivated by applications to recommendation systems and other scenarios with\nquery-limited access to vast amounts of data, we propose a new rigorous\nalgorithmic framework for a standard formulation of this problem as a\nsubmodular maximization subject to a linear (knapsack) constraint. Our\nframework is based on augmenting all partial Greedy solutions with the best\nadditional item. It can be instantiated with negligible overhead in any model\nof computation, which allows the classic \\greedy algorithm and its variants to\nbe implemented. We give such instantiations in the offline (Greedy+Max),\nmulti-pass streaming (Sieve+Max) and distributed (Distributed+Max) settings.\nOur algorithms give ($1/2-\\epsilon$)-approximation with most other key\nparameters of interest being near-optimal. Our analysis is based on a new set\nof first-order linear differential inequalities and their robust approximate\nversions. Experiments on typical datasets (movie recommendations, influence\nmaximization) confirm scalability and high quality of solutions obtained via\nour framework. Instance-specific approximations are typically in the 0.6-0.7\nrange and frequently beat even the $(1-1/e) \\approx 0.63$ worst-case barrier\nfor polynomial-time algorithms.",
    "published_date": "2019-10-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05646v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06745v3",
    "title": "Improve Model Generalization and Robustness to Dataset Bias with Bias-regularized Learning and Domain-guided Augmentation",
    "authors": [
      "Yundong Zhang",
      "Hang Wu",
      "Huiye Liu",
      "Li Tong",
      "May D Wang"
    ],
    "author_ids": [],
    "abstract": "Deep Learning has thrived on the emergence of biomedical big data. However,\nmedical datasets acquired at different institutions have inherent bias caused\nby various confounding factors such as operation policies, machine protocols,\ntreatment preference and etc. As the result, models trained on one dataset,\nregardless of volume, cannot be confidently utilized for the others. In this\nstudy, we investigated model robustness to dataset bias using three large-scale\nChest X-ray datasets: first, we assessed the dataset bias using vanilla\ntraining baseline; second, we proposed a novel multi-source domain\ngeneralization model by (a) designing a new bias-regularized loss function; and\n(b) synthesizing new data for domain augmentation. We showed that our model\nsignificantly outperformed the baseline and other approaches on data from\nunseen domain in terms of accuracy and various bias measures, without\nretraining or finetuning. Our method is generally applicable to other\nbiomedical data, providing new algorithms for training models robust to bias\nfor big data analysis and applications. Demo training code is publicly\navailable.",
    "published_date": "2019-10-12T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06745v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05387v2",
    "title": "The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data",
    "authors": [
      "Amanda Gentzel",
      "Dan Garant",
      "David Jensen"
    ],
    "author_ids": [],
    "abstract": "Causal inference is central to many areas of artificial intelligence,\nincluding complex reasoning, planning, knowledge-base construction, robotics,\nexplanation, and fairness. An active community of researchers develops and\nenhances algorithms that learn causal models from data, and this work has\nproduced a series of impressive technical advances. However, evaluation\ntechniques for causal modeling algorithms have remained somewhat primitive,\nlimiting what we can learn from experimental studies of algorithm performance,\nconstraining the types of algorithms and model representations that researchers\nconsider, and creating a gap between theory and practice. We argue for more\nfrequent use of evaluation techniques that examine interventional measures\nrather than structural or observational measures, and that evaluate those\nmeasures on empirical data rather than synthetic data. We survey the current\npractice in evaluation and show that the techniques we recommend are rarely\nused in practice. We show that such techniques are feasible and that data sets\nare available to conduct such evaluations. We also show that these techniques\nproduce substantially different results than using structural measures and\nsynthetic data.",
    "published_date": "2019-10-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05387v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05265v1",
    "title": "Automating dynamic consent decisions for the processing of social media data in health research",
    "authors": [
      "Chris Norval",
      "Tristan Henderson"
    ],
    "author_ids": [],
    "abstract": "Social media have become a rich source of data, particularly in health\nresearch. Yet, the use of such data raises significant ethical questions about\nthe need for the informed consent of those being studied. Consent mechanisms,\nif even obtained, are typically broad and inflexible, or place a significant\nburden on the participant. Machine learning algorithms show much promise for\nfacilitating a 'middle ground' approach: using trained models to predict and\nautomate granular consent decisions. Such techniques, however, raise a myriad\nof follow-on ethical and technical considerations. In this paper, we present an\nexploratory user study (n = 67) in which we find that we can predict the\nappropriate flow of health-related social media data with reasonable accuracy,\nwhile minimising undesired data leaks. We then attempt to deconstruct the\nfindings of this study, identifying and discussing a number of real-world\nimplications if such a technique were put into practice.",
    "published_date": "2019-10-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05265v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05221v1",
    "title": "Non-Uniform Time-Step Deep Q-Network for Carrier-Sense Multiple Access in Heterogeneous Wireless Networks",
    "authors": [
      "Yiding Yu",
      "Soung Chang Liew",
      "Taotao Wang"
    ],
    "author_ids": [],
    "abstract": "This paper investigates a new class of carrier-sense multiple access (CSMA)\nprotocols that employ deep reinforcement learning (DRL) techniques, referred to\nas carrier-sense deep-reinforcement learning multiple access (CS-DLMA). The\ngoal of CS-DLMA is to enable efficient and equitable spectrum sharing among a\ngroup of co-located heterogeneous wireless networks. Existing CSMA protocols,\nsuch as the medium access control (MAC) of WiFi, are designed for a homogeneous\nnetwork in which all nodes adopt the same protocol. Such protocols suffer from\nsevere performance degradation in a heterogeneous environment where there are\nnodes adopting other MAC protocols. CS-DLMA aims to circumvent this problem by\nmaking use of DRL. In particular, this paper adopts alpha-fairness as the\ngeneral objective of CS-DLMA. With alpha-fairness, CS-DLMA can achieve a range\nof different objectives when coexisting with other MACs by changing the value\nof alpha. A salient feature of CS-DLMA is that it can achieve these objectives\nwithout knowing the coexisting MACs through a learning process based on DRL.\nThe underpinning DRL technique in CS-DLMA is deep Q-network (DQN). However, the\nconventional DQN algorithms are not suitable for CS-DLMA due to their uniform\ntime-step assumption. In CSMA protocols, time steps are non-uniform in that the\ntime duration required for carrier sensing is smaller than the duration of data\ntransmission. This paper introduces a non-uniform time-step formulation of DQN\nto address this issue. Our simulation results show that CS-DLMA can achieve the\ngeneral alpha-fairness objective when coexisting with TDMA, ALOHA, and WiFi\nprotocols by adjusting its own transmission strategy. Interestingly, we also\nfind that CS-DLMA is more Pareto efficient than other CSMA protocols when\ncoexisting with WiFi.",
    "published_date": "2019-10-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05221v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.05113v2",
    "title": "Fairness in Clustering with Multiple Sensitive Attributes",
    "authors": [
      "Savitha Sam Abraham",
      "Deepak P",
      "Sowmya S Sundaram"
    ],
    "author_ids": [],
    "abstract": "A clustering may be considered as fair on pre-specified sensitive attributes\nif the proportions of sensitive attribute groups in each cluster reflect that\nin the dataset. In this paper, we consider the task of fair clustering for\nscenarios involving multiple multi-valued or numeric sensitive attributes. We\npropose a fair clustering method, \\textit{FairKM} (Fair K-Means), that is\ninspired by the popular K-Means clustering formulation. We outline a\ncomputational notion of fairness which is used along with a cluster coherence\nobjective, to yield the FairKM clustering method. We empirically evaluate our\napproach, wherein we quantify both the quality and fairness of clusters, over\nreal-world datasets. Our experimental evaluation illustrates that the clusters\ngenerated by FairKM fare significantly better on both clustering quality and\nfair representation of sensitive attribute groups compared to the clusters from\na state-of-the-art baseline fair clustering method.",
    "published_date": "2019-10-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.05113v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.04985v4",
    "title": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition",
    "authors": [
      "Mengjia Yan",
      "Mengao Zhao",
      "Zining Xu",
      "Qian Zhang",
      "Guoli Wang",
      "Zhizhong Su"
    ],
    "author_ids": [],
    "abstract": "To improve the discriminative and generalization ability of lightweight\nnetwork for face recognition, we propose an efficient variable group\nconvolutional network called VarGFaceNet. Variable group convolution is\nintroduced by VarGNet to solve the conflict between small computational cost\nand the unbalance of computational intensity inside a block. We employ variable\ngroup convolution to design our network which can support large scale face\nidentification while reduce computational cost and parameters. Specifically, we\nuse a head setting to reserve essential information at the start of the network\nand propose a particular embedding setting to reduce parameters of\nfully-connected layer for embedding. To enhance interpretation ability, we\nemploy an equivalence of angular distillation loss to guide our lightweight\nnetwork and we apply recursive knowledge distillation to relieve the\ndiscrepancy between the teacher model and the student model. The champion of\ndeepglint-light track of LFR (2019) challenge demonstrates the effectiveness of\nour model and approach. Implementation of VarGFaceNet will be released at\nhttps://github.com/zma-c-137/VarGFaceNet soon.",
    "published_date": "2019-10-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.04985v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.04849v1",
    "title": "Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies",
    "authors": [
      "Xinyun Chen",
      "Lu Wang",
      "Yizhe Hang",
      "Heng Ge",
      "Hongyuan Zha"
    ],
    "author_ids": [],
    "abstract": "We consider off-policy policy evaluation when the trajectory data are\ngenerated by multiple behavior policies. Recent work has shown the key role\nplayed by the state or state-action stationary distribution corrections in the\ninfinite horizon context for off-policy policy evaluation. We propose estimated\nmixture policy (EMP), a novel class of partially policy-agnostic methods to\naccurately estimate those quantities. With careful analysis, we show that EMP\ngives rise to estimates with reduced variance for estimating the state\nstationary distribution correction while it also offers a useful induction bias\nfor estimating the state-action stationary distribution correction. In\nextensive experiments with both continuous and discrete environments, we\ndemonstrate that our algorithm offers significantly improved accuracy compared\nto the state-of-the-art methods.",
    "published_date": "2019-10-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.04849v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.04597v1",
    "title": "Machine Learning with Multi-Site Imaging Data: An Empirical Study on the Impact of Scanner Effects",
    "authors": [
      "Ben Glocker",
      "Robert Robinson",
      "Daniel C. Castro",
      "Qi Dou",
      "Ender Konukoglu"
    ],
    "author_ids": [],
    "abstract": "This is an empirical study to investigate the impact of scanner effects when\nusing machine learning on multi-site neuroimaging data. We utilize structural\nT1-weighted brain MRI obtained from two different studies, Cam-CAN and UK\nBiobank. For the purpose of our investigation, we construct a dataset\nconsisting of brain scans from 592 age- and sex-matched individuals, 296\nsubjects from each original study. Our results demonstrate that even after\ncareful pre-processing with state-of-the-art neuroimaging pipelines a\nclassifier can easily distinguish between the origin of the data with very high\naccuracy. Our analysis on the example application of sex classification\nsuggests that current approaches to harmonize data are unable to remove\nscanner-specific bias leading to overly optimistic performance estimates and\npoor generalization. We conclude that multi-site data harmonization remains an\nopen challenge and particular care needs to be taken when using such data with\nadvanced machine learning methods for predictive modelling.",
    "published_date": "2019-10-10T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.04597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06155v1",
    "title": "GeoSES -- um Índice Socioeconômico para Estudos de Saúde no Brasil",
    "authors": [
      "Ligia Vizeu Barrozo",
      "Michel Fornaciali",
      "Carmen Diva Saldiva de André",
      "Guilherme Augusto Zimeo Morais",
      "Giselle Mansur",
      "William Cabral-Miranda",
      "João Ricardo Sato",
      "Edson Amaro Júnior"
    ],
    "author_ids": [],
    "abstract": "Objective: to define an index that summarizes the main dimensions of the\nsocioeconomic context for research purposes, evaluation and monitoring health\ninequalities. Methods: the index was created from the 2010 Brazilian\nDemographic Census, whose variables selection was guided by theoretical\nreferences for health studies, including seven socioeconomic dimensions:\neducation, mobility, poverty, wealth, income, segregation and deprivation of\nresources and services. The index was developed using principal component\nanalysis, and was evaluated for its construct, content and applicability\ncomponents. Results: GeoSES-BR dimensions showed good association with HDI-M\n(above 0.85). The model with the poverty dimension best explained the relative\nrisk of avoidable cause mortality in Brazil. In the intraurban scale, the model\nwith GeoSES-IM was the one that best explained the relative risk of mortality\nfrom circulatory system diseases. Conclusion: GeoSES showed significant\nexplanatory potential in the studied scales.",
    "published_date": "2019-10-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.OH",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06155v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.04071v5",
    "title": "BIAS: Transparent reporting of biomedical image analysis challenges",
    "authors": [
      "Lena Maier-Hein",
      "Annika Reinke",
      "Michal Kozubek",
      "Anne L. Martel",
      "Tal Arbel",
      "Matthias Eisenmann",
      "Allan Hanbuary",
      "Pierre Jannin",
      "Henning Müller",
      "Sinan Onogur",
      "Julio Saez-Rodriguez",
      "Bram van Ginneken",
      "Annette Kopp-Schneider",
      "Bennett Landman"
    ],
    "author_ids": [],
    "abstract": "The number of biomedical image analysis challenges organized per year is\nsteadily increasing. These international competitions have the purpose of\nbenchmarking algorithms on common data sets, typically to identify the best\nmethod for a given problem. Recent research, however, revealed that common\npractice related to challenge reporting does not allow for adequate\ninterpretation and reproducibility of results. To address the discrepancy\nbetween the impact of challenges and the quality (control), the Biomedical I\nmage Analysis ChallengeS (BIAS) initiative developed a set of recommendations\nfor the reporting of challenges. The BIAS statement aims to improve the\ntransparency of the reporting of a biomedical image analysis challenge\nregardless of field of application, image modality or task category assessed.\nThis article describes how the BIAS statement was developed and presents a\nchecklist which authors of biomedical image analysis challenges are encouraged\nto include in their submission when giving a paper on a challenge into review.\nThe purpose of the checklist is to standardize and facilitate the review\nprocess and raise interpretability and reproducibility of challenge results by\nmaking relevant information explicit.",
    "published_date": "2019-10-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.CY",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.04071v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03905v1",
    "title": "A Semi-Supervised Maximum Margin Metric Learning Approach for Small Scale Person Re-identification",
    "authors": [
      "T M Feroz Ali",
      "Subhasis Chaudhuri"
    ],
    "author_ids": [],
    "abstract": "In video surveillance, person re-identification is the task of searching\nperson images in non-overlapping cameras. Though supervised methods for person\nre-identification have attained impressive performance, obtaining large scale\ncross-view labeled training data is very expensive. However, unlabelled data is\navailable in abundance. In this paper, we propose a semi-supervised metric\nlearning approach that can utilize information in unlabelled data with the help\nof a few labelled training samples. We also address the small sample size\nproblem that inherently occurs due to the few labeled training data. Our method\nlearns a discriminative space where within class samples collapse to singular\npoints, achieving the least within class variance, and then use a maximum\nmargin criterion over a high dimensional kernel space to maximally separate the\ndistinct class samples. A maximum margin criterion with two levels of high\ndimensional mappings to kernel space is used to obtain better cross-view\ndiscrimination of the identities. Cross-view affinity learning with reciprocal\nnearest neighbor constraints is used to mine new pseudo-classes from the\nunlabelled data and update the distance metric iteratively. We attain\nstate-of-the-art performance on four challenging datasets with a large margin.",
    "published_date": "2019-10-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03905v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03880v2",
    "title": "Compatible features for Monotonic Policy Improvement",
    "authors": [
      "Marcin B. Tomczak",
      "Sergio Valcarcel Macua",
      "Enrique Munoz de Cote",
      "Peter Vrancx"
    ],
    "author_ids": [],
    "abstract": "Recent policy optimization approaches have achieved substantial empirical\nsuccess by constructing surrogate optimization objectives. The Approximate\nPolicy Iteration objective (Schulman et al., 2015a; Kakade and Langford, 2002)\nhas become a standard optimization target for reinforcement learning problems.\nUsing this objective in practice requires an estimator of the advantage\nfunction. Policy optimization methods such as those proposed in Schulman et al.\n(2015b) estimate the advantages using a parametric critic. In this work we\nestablish conditions under which the parametric approximation of the critic\ndoes not introduce bias to the updates of surrogate objective. These results\nhold for a general class of parametric policies, including deep neural\nnetworks. We obtain a result analogous to the compatible features derived for\nthe original Policy Gradient Theorem (Sutton et al., 1999). As a result, we\nalso identify a previously unknown bias that current state-of-the-art policy\noptimization algorithms (Schulman et al., 2015a, 2017) have introduced by not\nemploying these compatible features.",
    "published_date": "2019-10-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03880v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03676v4",
    "title": "Representation Learning with Statistical Independence to Mitigate Bias",
    "authors": [
      "Ehsan Adeli",
      "Qingyu Zhao",
      "Adolf Pfefferbaum",
      "Edith V. Sullivan",
      "Li Fei-Fei",
      "Juan Carlos Niebles",
      "Kilian M. Pohl"
    ],
    "author_ids": [],
    "abstract": "Presence of bias (in datasets or tasks) is inarguably one of the most\ncritical challenges in machine learning applications that has alluded to\npivotal debates in recent years. Such challenges range from spurious\nassociations between variables in medical studies to the bias of race in gender\nor face recognition systems. Controlling for all types of biases in the dataset\ncuration stage is cumbersome and sometimes impossible. The alternative is to\nuse the available data and build models incorporating fair representation\nlearning. In this paper, we propose such a model based on adversarial training\nwith two competing objectives to learn features that have (1) maximum\ndiscriminative power with respect to the task and (2) minimal statistical mean\ndependence with the protected (bias) variable(s). Our approach does so by\nincorporating a new adversarial loss function that encourages a vanished\ncorrelation between the bias and the learned features. We apply our method to\nsynthetic data, medical images (containing task bias), and a dataset for gender\nclassification (containing dataset bias). Our results show that the learned\nfeatures by our method not only result in superior prediction performance but\nalso are unbiased. The code is available at\nhttps://github.com/QingyuZhao/BR-Net/.",
    "published_date": "2019-10-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03676v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03667v1",
    "title": "REFUGE Challenge: A Unified Framework for Evaluating Automated Methods for Glaucoma Assessment from Fundus Photographs",
    "authors": [
      "José Ignacio Orlando",
      "Huazhu Fu",
      "João Barbossa Breda",
      "Karel van Keer",
      "Deepti R. Bathula",
      "Andrés Diaz-Pinto",
      "Ruogu Fang",
      "Pheng-Ann Heng",
      "Jeyoung Kim",
      "JoonHo Lee",
      "Joonseok Lee",
      "Xiaoxiao Li",
      "Peng Liu",
      "Shuai Lu",
      "Balamurali Murugesan",
      "Valery Naranjo",
      "Sai Samarth R. Phaye",
      "Sharath M. Shankaranarayana",
      "Apoorva Sikka",
      "Jaemin Son",
      "Anton van den Hengel",
      "Shujun Wang",
      "Junyan Wu",
      "Zifeng Wu",
      "Guanghui Xu",
      "Yongli Xu",
      "Pengshuai Yin",
      "Fei Li",
      "Xiulan Zhang",
      "Yanwu Xu",
      "Xiulan Zhang",
      "Hrvoje Bogunović"
    ],
    "author_ids": [],
    "abstract": "Glaucoma is one of the leading causes of irreversible but preventable\nblindness in working age populations. Color fundus photography (CFP) is the\nmost cost-effective imaging modality to screen for retinal disorders. However,\nits application to glaucoma has been limited to the computation of a few\nrelated biomarkers such as the vertical cup-to-disc ratio. Deep learning\napproaches, although widely applied for medical image analysis, have not been\nextensively used for glaucoma assessment due to the limited size of the\navailable data sets. Furthermore, the lack of a standardize benchmark strategy\nmakes difficult to compare existing methods in a uniform way. In order to\novercome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE\n(\\url{https://refuge.grand-challenge.org}), held in conjunction with MICCAI\n2018. The challenge consisted of two primary tasks, namely optic disc/cup\nsegmentation and glaucoma classification. As part of REFUGE, we have publicly\nreleased a data set of 1200 fundus images with ground truth segmentations and\nclinical glaucoma labels, currently the largest existing one. We have also\nbuilt an evaluation framework to ease and ensure fairness in the comparison of\ndifferent models, encouraging the development of novel techniques in the field.\n12 teams qualified and participated in the online challenge. This paper\nsummarizes their methods and analyzes their corresponding results. In\nparticular, we observed that two of the top-ranked teams outperformed two human\nexperts in the glaucoma classification task. Furthermore, the segmentation\nresults were in general consistent with the ground truth annotations, with\ncomplementary outcomes that can be further exploited by ensembling the results.",
    "published_date": "2019-10-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03667v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03515v1",
    "title": "Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide Development",
    "authors": [
      "Carol J. Smith"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) holds great promise to empower us with knowledge\nand augment our effectiveness. We can -- and must -- ensure that we keep humans\nsafe and in control, particularly with regard to government and public sector\napplications that affect broad populations. How can AI development teams\nharness the power of AI systems and design them to be valuable to humans?\nDiverse teams are needed to build trustworthy artificial intelligent systems,\nand those teams need to coalesce around a shared set of ethics. There are many\ndiscussions in the AI field about ethics and trust, but there are few\nframeworks available for people to use as guidance when creating these systems.\nThe Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences\ndescribed in this paper, when used with a set of technical ethics, will guide\nAI development teams to create AI systems that are accountable, de-risked,\nrespectful, secure, honest, and usable. To support the team's efforts,\nactivities to understand people's needs and concerns will be introduced along\nwith the themes to support the team's efforts. For example, usability testing\ncan help determine if the audience understands how the AI system works and\ncomplies with the HMT Framework. The HMT Framework is based on reviews of\nexisting ethical codes and best practices in human-computer interaction and\nsoftware development. Human-machine teams are strongest when human users can\ntrust AI systems to behave as expected, safely, securely, and understandably.\nUsing the HMT Framework to design trustworthy AI systems will provide support\nto teams in identifying potential issues ahead of time and making great\nexperiences for humans.",
    "published_date": "2019-10-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03515v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03367v1",
    "title": "A note on computational approaches for the antibandwidth problem",
    "authors": [
      "Markus Sinnl"
    ],
    "author_ids": [],
    "abstract": "In this note, we consider the antibandwidth problem, also known as dual\nbandwidth problem, separation problem and maximum differential coloring\nproblem. Given a labeled graph (i.e., a numbering of the vertices of a graph),\nthe antibandwidth of a node is defined as the minimum absolute difference of\nits labeling to the labeling of all its adjacent vertices. The goal in the\nantibandwidth problem is to find a labeling maximizing the antibandwidth. The\nproblem is NP-hard in general graphs and has applications in diverse areas like\nscheduling, radio frequency assignment, obnoxious facility location and\nmap-coloring. There has been much work on deriving theoretical bounds for the\nproblem and also in the design of metaheuristics in recent years. However, the\noptimality gaps between the best known solution values and reported upper\nbounds for the HarwellBoeing Matrix-instances, which are the commonly used\nbenchmark instances for this problem, are often very large (e.g., up to 577%).\nThe upper bounds reported in literature are based on the theoretical bounds\ninvolving simple graph characteristics, i.e., size, order and degree, and a\nmixed-integer programming (MIP) model. We present new MIP models for the\nproblem, together with valid inequalities, and design a branch-and-cut\nalgorithm and an iterative solution algorithm based on them. These algorithms\nalso include two starting heuristics and a primal heuristic. We also present a\nconstraint programming approach, and calculate upper bounds based on the\nstability number and chromatic number. Our computational study shows that the\ndeveloped approaches allow to find the proven optimal solution for eight\ninstances from literature, where the optimal solution was unknown and also\nprovide reduced gaps for eleven additional instances, including improved\nsolution values for seven instances, the largest optimality gap is now 46%.",
    "published_date": "2019-10-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03367v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.03365v3",
    "title": "A Penalized Inequality-Constrained Approach for Robust Beamforming with DoF Limitation",
    "authors": [
      "Wenqiang Pu",
      "Jinjun Xiao",
      "Tao Zhang",
      "Zhi-Quan Luo"
    ],
    "author_ids": [],
    "abstract": "A well-known challenge in beamforming is how to optimally utilize the degrees\nof freedom (DoF) of the array to design a robust beamformer, especially when\nthe array DoF is limited. In this paper, we leverage the tool of constrained\nconvex optimization and propose a penalized inequality-constrained minimum\nvariance (P-ICMV) beamformer to address this challenge. Specifically, a\nwell-targeted objective function and inequality constraints are proposed to\nachieve the design goals. By penalizing the maximum gain of the beamformer at\nany interfering directions, the total interference power can be efficiently\nmitigated with limited DoF. Multiple robust constraints on the target\nprotection and interference suppression can be introduced to increase the\nrobustness of the beamformer against steering vector mismatch. By integrating\nthe noise reduction, interference suppression, and target protection, the\nproposed formulation can efficiently obtain a robust beamformer design while\noptimally trading off various design goals. To numerically solve this problem,\nwe formulate the P-ICMV beamformer design as a convex second-order cone program\n(SOCP) and propose a low complexity iterative algorithm based on the\nalternating direction method of multipliers (ADMM). Three applications are\nsimulated to demonstrate the effectiveness of the proposed beamformer.",
    "published_date": "2019-10-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03365v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.03061v3",
    "title": "Keeping Designers in the Loop: Communicating Inherent Algorithmic Trade-offs Across Multiple Objectives",
    "authors": [
      "Bowen Yu",
      "Ye Yuan",
      "Loren Terveen",
      "Zhiwei Steven Wu",
      "Jodi Forlizzi",
      "Haiyi Zhu"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence algorithms have been used to enhance a wide variety\nof products and services, including assisting human decision making in\nhigh-stakes contexts. However, these algorithms are complex and have\ntrade-offs, notably between prediction accuracy and fairness to population\nsubgroups. This makes it hard for designers to understand algorithms and design\nproducts or services in a way that respects users' goals, values, and needs. We\nproposed a method to help designers and users explore algorithms, visualize\ntheir trade-offs, and select algorithms with trade-offs consistent with their\ngoals and needs. We evaluated our method on the problem of predicting criminal\ndefendants' likelihood to re-offend through (i) a large-scale Amazon Mechanical\nTurk experiment, and (ii) in-depth interviews with domain experts. Our\nevaluations show that our method can help designers and users of these systems\nbetter understand and navigate algorithmic trade-offs. This paper contributes a\nnew way of providing designers the ability to understand and control the\noutcomes of algorithmic systems they are creating.",
    "published_date": "2019-10-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03061v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.02806v3",
    "title": "Learning De-biased Representations with Biased Representations",
    "authors": [
      "Hyojin Bahng",
      "Sanghyuk Chun",
      "Sangdoo Yun",
      "Jaegul Choo",
      "Seong Joon Oh"
    ],
    "author_ids": [],
    "abstract": "Many machine learning algorithms are trained and evaluated by splitting data\nfrom a single source into training and test sets. While such focus on\nin-distribution learning scenarios has led to interesting advancement, it has\nnot been able to tell if models are relying on dataset biases as shortcuts for\nsuccessful prediction (e.g., using snow cues for recognising snowmobiles),\nresulting in biased models that fail to generalise when the bias shifts to a\ndifferent class. The cross-bias generalisation problem has been addressed by\nde-biasing training data through augmentation or re-sampling, which are often\nprohibitive due to the data collection cost (e.g., collecting images of a\nsnowmobile on a desert) and the difficulty of quantifying or expressing biases\nin the first place. In this work, we propose a novel framework to train a\nde-biased representation by encouraging it to be different from a set of\nrepresentations that are biased by design. This tactic is feasible in many\nscenarios where it is much easier to define a set of biased representations\nthan to define and quantify bias. We demonstrate the efficacy of our method\nacross a variety of synthetic and real-world biases; our experiments show that\nthe method discourages models from taking bias shortcuts, resulting in improved\ngeneralisation. Source code is available at https://github.com/clovaai/rebias.",
    "published_date": "2019-10-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02806v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.02754v1",
    "title": "On Leveraging the Visual Modality for Neural Machine Translation",
    "authors": [
      "Vikas Raunak",
      "Sang Keun Choe",
      "Quanyang Lu",
      "Yi Xu",
      "Florian Metze"
    ],
    "author_ids": [],
    "abstract": "Leveraging the visual modality effectively for Neural Machine Translation\n(NMT) remains an open problem in computational linguistics. Recently, Caglayan\net al. posit that the observed gains are limited mainly due to the very simple,\nshort, repetitive sentences of the Multi30k dataset (the only multimodal MT\ndataset available at the time), which renders the source text sufficient for\ncontext. In this work, we further investigate this hypothesis on a new large\nscale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times\nlonger mean sentence length than Multi30k and no repetition. We propose and\nevaluate three novel fusion techniques, each of which is designed to ensure the\nutilization of visual context at different stages of the Sequence-to-Sequence\ntransduction pipeline, even under full linguistic context. However, we still\nobtain only marginal gains under full linguistic context and posit that visual\nembeddings extracted from deep vision models (ResNet for Multi30k, ResNext for\nHow2) do not lend themselves to increasing the discriminativeness between the\nvocabulary elements at token level prediction in NMT. We demonstrate this\nqualitatively by analyzing attention distribution and quantitatively through\nPrincipal Component Analysis, arriving at the conclusion that it is the quality\nof the visual embeddings rather than the length of sentences, which need to be\nimproved in existing MMT datasets.",
    "published_date": "2019-10-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.02321v1",
    "title": "The Impact of Data Preparation on the Fairness of Software Systems",
    "authors": [
      "Inês Valentim",
      "Nuno Lourenço",
      "Nuno Antunes"
    ],
    "author_ids": [],
    "abstract": "Machine learning models are widely adopted in scenarios that directly affect\npeople. The development of software systems based on these models raises\nsocietal and legal concerns, as their decisions may lead to the unfair\ntreatment of individuals based on attributes like race or gender. Data\npreparation is key in any machine learning pipeline, but its effect on fairness\nis yet to be studied in detail. In this paper, we evaluate how the fairness and\neffectiveness of the learned models are affected by the removal of the\nsensitive attribute, the encoding of the categorical attributes, and instance\nselection methods (including cross-validators and random undersampling). We\nused the Adult Income and the German Credit Data datasets, which are widely\nstudied and known to have fairness concerns. We applied each data preparation\ntechnique individually to analyse the difference in predictive performance and\nfairness, using statistical parity difference, disparate impact, and the\nnormalised prejudice index. The results show that fairness is affected by\ntransformations made to the training data, particularly in imbalanced datasets.\nRemoving the sensitive attribute is insufficient to eliminate all the\nunfairness in the predictions, as expected, but it is key to achieve fairer\nmodels. Additionally, the standard random undersampling with respect to the\ntrue labels is sometimes more prejudicial than performing no random\nundersampling.",
    "published_date": "2019-10-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02321v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03476v3",
    "title": "Classification As Decoder: Trading Flexibility For Control In Neural Dialogue",
    "authors": [
      "Sam Shleifer",
      "Manish Chablani",
      "Namit Katariya",
      "Anitha Kannan",
      "Xavier Amatriain"
    ],
    "author_ids": [],
    "abstract": "Generative seq2seq dialogue systems are trained to predict the next word in\ndialogues that have already occurred. They can learn from large unlabeled\nconversation datasets, build a deep understanding of conversational context,\nand generate a wide variety of responses. This flexibility comes at the cost of\ncontrol. Undesirable responses in the training data will be reproduced by the\nmodel at inference time, and longer generations often don't make sense. Instead\nof generating responses one word at a time, we train a classifier to choose\nfrom a predefined list of full responses. The classifier is trained on\n(conversation context, response class) pairs, where each response class is a\nnoisily labeled group of interchangeable responses. At inference, we generate\nthe exemplar response associated with the predicted response class. Experts can\nedit and improve these exemplar responses over time without retraining the\nclassifier or invalidating old training data. Human evaluation of 775 unseen\ndoctor/patient conversations shows that this tradeoff improves responses. Only\n12% of our discriminative approach's responses are worse than the doctor's\nresponse in the same conversational context, compared to 18% for the generative\nmodel. A discriminative model trained without any manual labeling of response\nclasses achieves equal performance to the generative model.",
    "published_date": "2019-10-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03476v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.02114v1",
    "title": "A Comparison Study on Nonlinear Dimension Reduction Methods with Kernel Variations: Visualization, Optimization and Classification",
    "authors": [
      "Katherine C. Kempfert",
      "Yishi Wang",
      "Cuixian Chen",
      "Samuel W. K. Wong"
    ],
    "author_ids": [],
    "abstract": "Because of high dimensionality, correlation among covariates, and noise\ncontained in data, dimension reduction (DR) techniques are often employed to\nthe application of machine learning algorithms. Principal Component Analysis\n(PCA), Linear Discriminant Analysis (LDA), and their kernel variants (KPCA,\nKLDA) are among the most popular DR methods. Recently, Supervised Kernel\nPrincipal Component Analysis (SKPCA) has been shown as another successful\nalternative. In this paper, brief reviews of these popular techniques are\npresented first. We then conduct a comparative performance study based on three\nsimulated datasets, after which the performance of the techniques are evaluated\nthrough application to a pattern recognition problem in face image analysis.\nThe gender classification problem is considered on MORPH-II and FG-NET, two\npopular longitudinal face aging databases. Several feature extraction methods\nare used, including biologically-inspired features (BIF), local binary patterns\n(LBP), histogram of oriented gradients (HOG), and the Active Appearance Model\n(AAM). After applications of DR methods, a linear support vector machine (SVM)\nis deployed with gender classification accuracy rates exceeding 95% on\nMORPH-II, competitive with benchmark results. A parallel computational approach\nis also proposed, attaining faster processing speeds and similar recognition\nrates on MORPH-II. Our computational approach can be applied to practical\ngender classification systems and generalized to other face analysis tasks,\nsuch as race classification and age prediction.",
    "published_date": "2019-10-04T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02114v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.02097v1",
    "title": "Group-based Fair Learning Leads to Counter-intuitive Predictions",
    "authors": [
      "Ofir Nachum",
      "Heinrich Jiang"
    ],
    "author_ids": [],
    "abstract": "A number of machine learning (ML) methods have been proposed recently to\nmaximize model predictive accuracy while enforcing notions of group parity or\nfairness across sub-populations. We propose a desirable property for these\nprocedures, slack-consistency: For any individual, the predictions of the model\nshould be monotonic with respect to allowed slack (i.e., maximum allowed\ngroup-parity violation). Such monotonicity can be useful for individuals to\nunderstand the impact of enforcing fairness on their predictions. Surprisingly,\nwe find that standard ML methods for enforcing fairness violate this basic\nproperty. Moreover, this undesirable behavior arises in situations agnostic to\nthe complexity of the underlying model or approximate optimizations, suggesting\nthat the simple act of incorporating a constraint can lead to drastically\nunintended behavior in ML. We present a simple theoretical method for enforcing\nslack-consistency, while encouraging further discussions on the unintended\nbehaviors potentially induced when enforcing group-based parity.",
    "published_date": "2019-10-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02097v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.02059v1",
    "title": "Fairness and Efficiency in DAG-based Cryptocurrencies",
    "authors": [
      "Georgios Birmpas",
      "Elias Koutsoupias",
      "Philip Lazos",
      "Francisco J. Marmolejo-Cossío"
    ],
    "author_ids": [],
    "abstract": "Bitcoin is a decentralised digital currency that serves as an alternative to\nexisting transaction systems based on an external central authority for\nsecurity. Although Bitcoin has many desirable properties, one of its\nfundamental shortcomings is its inability to process transactions at high\nrates. To address this challenge, many subsequent protocols either modify the\nrules of block acceptance (longest chain rule) and reward, or alter the\ngraphical structure of the public ledger from a tree to a directed acyclic\ngraph (DAG). Motivated by these approaches, we introduce a new general\nframework that captures ledger growth for a large class of DAG-based\nimplementations. With this in hand, and by assuming honest miner behaviour, we\n(experimentally) explore how different DAG-based protocols perform in terms of\nfairness, i.e., if the block reward of a miner is proportional to their hash\npower, as well as efficiency, i.e. what proportion of user transactions a\nledger deems valid after a certain length of time. Our results demonstrate\nfundamental structural limits on how well DAG-based ledger protocols cope with\na high transaction load. More specifically, we show that even in a scenario\nwhere every miner on the system is honest in terms of when they publish blocks,\nwhat they point to, and what transactions each block contains, fairness and\nefficiency of the ledger can break down at specific hash rates if miners have\ndiffering levels of connectivity to the P2P network sustaining the protocol.",
    "published_date": "2019-10-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02059v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.12937v2",
    "title": "Targeted sampling from massive block model graphs with personalized PageRank",
    "authors": [
      "Fan Chen",
      "Yini Zhang",
      "Karl Rohe"
    ],
    "author_ids": [],
    "abstract": "The paper provides statistical theory and intuition for personalized PageRank\n(called \"PPR\"): a popular technique that samples a small community from a\nmassive network. We study a setting where the entire network is expensive to\nobtain thoroughly or to maintain, but we can start from a seed node of interest\nand \"crawl\" the network to find other nodes through their connections. By\ncrawling the graph in a designed way, the PPR vector can be approximated\nwithout querying the entire massive graph, making it an alternative to snowball\nsampling. Using the degree-corrected stochastic block model, we study whether\nthe PPR vector can select nodes that belong to the same block as the seed node.\nWe provide a simple and interpretable form for the PPR vector, highlighting its\nbiases towards high degree nodes outside the target block. We examine a simple\nadjustment based on node degrees and establish consistency results for PPR\nclustering that allows for directed graphs. These results are enabled by recent\ntechnical advances showing the elementwise convergence of eigenvectors. We\nillustrate the method with the massive Twitter friendship graph, which we crawl\nby using the Twitter application programming interface. We find that the\nadjusted and unadjusted PPR techniques are complementary approaches, where the\nadjustment makes the results particularly localized around the seed node, and\nthat the bias adjustment greatly benefits from degree regularization.",
    "published_date": "2019-10-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CC",
      "cs.DL",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12937v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.01722v2",
    "title": "Constant State of Change: Engagement Inequality in Temporal Dynamic Networks",
    "authors": [
      "Hadar Miller",
      "Osnat Mokryn"
    ],
    "author_ids": [],
    "abstract": "The temporal changes in complex systems of interactions have excited the\nresearch community in recent years as they encompass understandings on their\ndynamics and evolution. From the collective dynamics of organizations and\nonline communities to the spreading of information and fake news, to name a\nfew, temporal dynamics are fundamental in the understanding of complex systems.\nIn this work, we quantify the level of engagement in dynamic complex systems of\ninteractions, modeled as networks. We focus on interaction networks for which\nthe dynamics of the interactions are coupled with that of the topology, such as\nonline messaging, forums, and emails. We define two indices to capture the\ntemporal level of engagement: the Temporal Network (edge) Intensity index, and\nthe Temporal Dominance Inequality index. Our surprising results are that these\nmeasures are stationary for most measured networks, regardless of vast\nfluctuations in the size of the networks in time. Moreover, more than 80% of\nweekly changes in the indices values are bounded by less than 10%. The indices\nare stable between the temporal evolution of a network but are different\nbetween networks, and a classifier can determine the network the temporal\nindices belong to with high success. We find an exception in the Enron\nmanagement email exchange during the year before its disintegration, in which\nboth indices show high volatility throughout the inspected period.",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "physics.data-an",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01722v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.01568v2",
    "title": "Incremental learning for the detection and classification of GAN-generated images",
    "authors": [
      "Francesco Marra",
      "Cristiano Saltori",
      "Giulia Boato",
      "Luisa Verdoliva"
    ],
    "author_ids": [],
    "abstract": "Current developments in computer vision and deep learning allow to\nautomatically generate hyper-realistic images, hardly distinguishable from real\nones. In particular, human face generation achieved a stunning level of\nrealism, opening new opportunities for the creative industry but, at the same\ntime, new scary scenarios where such content can be maliciously misused.\nTherefore, it is essential to develop innovative methodologies to automatically\ntell apart real from computer generated multimedia, possibly able to follow the\nevolution and continuous improvement of data in terms of quality and realism.\nIn the last few years, several deep learning-based solutions have been proposed\nfor this problem, mostly based on Convolutional Neural Networks (CNNs).\nAlthough results are good in controlled conditions, it is not clear how such\nproposals can adapt to real-world scenarios, where learning needs to\ncontinuously evolve as new types of generated data appear. In this work, we\ntackle this problem by proposing an approach based on incremental learning for\nthe detection and classification of GAN-generated images. Experiments on a\ndataset comprising images generated by several GAN-based architectures show\nthat the proposed method is able to correctly perform discrimination when new\nGANs are presented to the network",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01568v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.01565v1",
    "title": "On partisan bias in redistricting: computational complexity meets the science of gerrymandering",
    "authors": [
      "Tanima Chatterjee",
      "Bhaskar DasGupta"
    ],
    "author_ids": [],
    "abstract": "The topic of this paper is \"gerrymandering\", namely the curse of deliberate\ncreations of district maps with highly asymmetric electoral outcomes to\ndisenfranchise voters, and it has a long legal history. Measuring and\neliminating gerrymandering has enormous implications to sustain the backbone of\ndemocratic principles of a society. Although there is no dearth of legal briefs\ninvolving gerrymandering over many years, it is only more recently that\nmathematicians and applied computational researchers have started to\ninvestigate this topic. However, it has received relatively little attention so\nfar from the computational complexity researchers dealing with theoretical\nanalysis of computational complexity issues, such as computational hardness,\napproximability issues, etc. There could be many reasons for this, such as\ndescriptions of these problem non-CS non-math (often legal or political)\njournals that theoretical CS (TCS) people usually do not follow, or the lack of\ncoverage of these topics in TCS publication venues. One of our modest goals in\nwriting this article is to improve upon this situation by stimulating further\ninteractions between the gerrymandering and TCS researchers. To this effect,\nour main contributions are twofold: (1) we provide formalization of several\nmodels, related concepts, and corresponding problem statements using TCS\nframeworks from the descriptions of these problems as available in existing\nnon-TCS (perhaps legal) venues, and (2) we also provide computational\ncomplexity analysis of some versions of these problems, leaving other versions\nfor future research.\n  The goal of writing this article is not to have the final word on\ngerrymandering, but to introduce a series of concepts, models and problems to\nthe TCS community and to show that science of gerrymandering involves an\nintriguing set of partitioning problems involving geometric and combinatorial\noptimization.",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CC",
      "cs.CG",
      "cs.DM",
      "math.CO",
      "physics.soc-ph",
      "68Q17, 68Q25, 68W20, 68W25, 68W40, 90C59",
      "F.2.2; G.2.3; G.3; G.4; J.1; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01565v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.01465v2",
    "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics",
    "authors": [
      "Johannes Ackermann",
      "Volker Gabler",
      "Takayuki Osa",
      "Masashi Sugiyama"
    ],
    "author_ids": [],
    "abstract": "Many real world tasks require multiple agents to work together. Multi-agent\nreinforcement learning (RL) methods have been proposed in recent years to solve\nthese tasks, but current methods often fail to efficiently learn policies. We\nthus investigate the presence of a common weakness in single-agent RL, namely\nvalue function overestimation bias, in the multi-agent setting. Based on our\nfindings, we propose an approach that reduces this bias by using double\ncentralized critics. We evaluate it on six mixed cooperative-competitive tasks,\nshowing a significant advantage over current methods. Finally, we investigate\nthe application of multi-agent methods to high-dimensional robotic tasks and\nshow that our approach can be used to learn decentralized policies in this\ndomain.",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01465v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.01432v2",
    "title": "The Bouncer Problem: Challenges to Remote Explainability",
    "authors": [
      "Erwan Le Merrer",
      "Gilles Tredan"
    ],
    "author_ids": [],
    "abstract": "The concept of explainability is envisioned to satisfy society's demands for\ntransparency on machine learning decisions. The concept is simple: like humans,\nalgorithms should explain the rationale behind their decisions so that their\nfairness can be assessed. While this approach is promising in a local context\n(e.g. to explain a model during debugging at training time), we argue that this\nreasoning cannot simply be transposed in a remote context, where a trained\nmodel by a service provider is only accessible through its API. This is\nproblematic as it constitutes precisely the target use-case requiring\ntransparency from a societal perspective. Through an analogy with a club\nbouncer (which may provide untruthful explanations upon customer reject), we\nshow that providing explanations cannot prevent a remote service from lying\nabout the true reasons leading to its decisions. More precisely, we prove the\nimpossibility of remote explainability for single explanations, by constructing\nan attack on explanations that hides discriminatory features to the querying\nuser. We provide an example implementation of this attack. We then show that\nthe probability that an observer spots the attack, using several explanations\nfor attempting to find incoherences, is low in practical settings. This\nundermines the very concept of remote explainability in general.",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01432v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.01331v1",
    "title": "Optimal Joint Subcarrier and Power Allocation in NOMA is Strongly NP-Hard",
    "authors": [
      "Lou Salaun",
      "Chung Shue Chen",
      "Marceau Coupechoux"
    ],
    "author_ids": [],
    "abstract": "Non-orthogonal multiple access (NOMA) is a promising radio access technology\nfor 5G. It allows several users to transmit on the same frequency and time\nresource by performing power-domain multiplexing. At the receiver side,\nsuccessive interference cancellation (SIC) is applied to mitigate interference\namong the multiplexed signals. In this way, NOMA can outperform orthogonal\nmultiple access schemes used in conventional cellular networks in terms of\nspectral efficiency and allows more simultaneous users. This paper investigates\nthe computational complexity of joint subcarrier and power allocation problems\nin multi-carrier NOMA systems. We prove that these problems are strongly\nNP-hard for a large class of objective functions, namely the weighted\ngeneralized means of the individual data rates. This class covers the popular\nweighted sum-rate, proportional fairness, harmonic mean and max-min fairness\nutilities. Our results show that the optimal power and subcarrier allocation\ncannot be computed in polynomial time in the general case, unless P = NP.\nNevertheless, we present some tractable special cases and we show that they can\nbe solved efficiently.",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CC",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01331v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.01279v2",
    "title": "Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks",
    "authors": [
      "Haofan Wang",
      "Zifan Wang",
      "Mengnan Du",
      "Fan Yang",
      "Zijian Zhang",
      "Sirui Ding",
      "Piotr Mardziel",
      "Xia Hu"
    ],
    "author_ids": [],
    "abstract": "Recently, increasing attention has been drawn to the internal mechanisms of\nconvolutional neural networks, and the reason why the network makes specific\ndecisions. In this paper, we develop a novel post-hoc visual explanation method\ncalled Score-CAM based on class activation mapping. Unlike previous class\nactivation mapping based approaches, Score-CAM gets rid of the dependence on\ngradients by obtaining the weight of each activation map through its forward\npassing score on target class, the final result is obtained by a linear\ncombination of weights and activation maps. We demonstrate that Score-CAM\nachieves better visual performance and fairness for interpreting the decision\nmaking process. Our approach outperforms previous methods on both recognition\nand localization tasks, it also passes the sanity check. We also indicate its\napplication as debugging tools. Official code has been released.",
    "published_date": "2019-10-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01279v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.00910v3",
    "title": "Estimating Lower Limb Kinematics using a Reduced Wearable Sensor Count",
    "authors": [
      "Luke Sy",
      "Michael Raitor",
      "Michael Del Rosario",
      "Heba Khamis",
      "Lauren Kark",
      "Nigel H. Lovell",
      "Stephen J. Redmond"
    ],
    "author_ids": [],
    "abstract": "Goal: This paper presents an algorithm for accurately estimating pelvis,\nthigh, and shank kinematics during walking using only three wearable inertial\nsensors. Methods: The algorithm makes novel use of a constrained Kalman filter\n(CKF). The algorithm iterates through the prediction (kinematic equation),\nmeasurement (pelvis position pseudo-measurements, zero velocity update,\nflat-floor assumption, and covariance limiter), and constraint update\n(formulation of hinged knee joints and ball-and-socket hip joints). Results:\nEvaluation of the algorithm using an optical motion capture-based\nsensor-to-segment calibration on nine participants ($7$ men and $2$ women,\nweight $63.0 \\pm 6.8$ kg, height $1.70 \\pm 0.06$ m, age $24.6 \\pm 3.9$ years\nold), with no known gait or lower body biomechanical abnormalities, who walked\nwithin a $4 \\times 4$ m$^2$ capture area shows that it can track motion\nrelative to the mid-pelvis origin with mean position and orientation (no bias)\nroot-mean-square error (RMSE) of $5.21 \\pm 1.3$ cm and $16.1 \\pm 3.2^\\circ$,\nrespectively. The sagittal knee and hip joint angle RMSEs (no bias) were $10.0\n\\pm 2.9^\\circ$ and $9.9 \\pm 3.2^\\circ$, respectively, while the corresponding\ncorrelation coefficient (CC) values were $0.87 \\pm 0.08$ and $0.74 \\pm 0.12$.\nConclusion: The CKF-based algorithm was able to track the 3D pose of the\npelvis, thigh, and shanks using only three inertial sensors worn on the pelvis\nand shanks. Significance: Due to the Kalman-filter-based algorithm's low\ncomputation cost and the relative convenience of using only three wearable\nsensors, gait parameters can be computed in real-time and remotely for\nlong-term gait monitoring. Furthermore, the system can be used to inform\nreal-time gait assistive devices.",
    "published_date": "2019-10-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00910v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.00757v1",
    "title": "Quantifying Voter Biases in Online Platforms: An Instrumental Variable Approach",
    "authors": [
      "Himel Dev",
      "Karrie Karahalios",
      "Hari Sundaram"
    ],
    "author_ids": [],
    "abstract": "In content-based online platforms, use of aggregate user feedback (say, the\nsum of votes) is commonplace as the \"gold standard\" for measuring content\nquality. Use of vote aggregates, however, is at odds with the existing\nempirical literature, which suggests that voters are susceptible to different\nbiases -- reputation (e.g., of the poster), social influence (e.g., votes thus\nfar), and position (e.g., answer position). Our goal is to quantify, in an\nobservational setting, the degree of these biases in online platforms.\nSpecifically, what are the causal effects of different impression signals --\nsuch as the reputation of the contributing user, aggregate vote thus far, and\nposition of content -- on a participant's vote on content? We adopt an\ninstrumental variable (IV) framework to answer this question. We identify a set\nof candidate instruments, carefully analyze their validity, and then use the\nvalid instruments to reveal the effects of the impression signals on votes. Our\nempirical study using log data from Stack Exchange websites shows that the bias\nestimates from our IV approach differ from the bias estimates from the ordinary\nleast squares (OLS) method. In particular, OLS underestimates reputation bias\n(1.6--2.2x for gold badges) and position bias (up to 1.9x for the initial\nposition) and overestimates social influence bias (1.8--2.3x for initial\nvotes). The implications of our work include: redesigning user interface to\navoid voter biases; making changes to platforms' policy to mitigate voter\nbiases; detecting other forms of biases in online platforms.",
    "published_date": "2019-10-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00757v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.00321v1",
    "title": "Libra: Fair Order-Matching for Electronic Financial Exchanges",
    "authors": [
      "Vasilios Mavroudis",
      "Hayden Melton"
    ],
    "author_ids": [],
    "abstract": "While historically, economists have been primarily occupied with analyzing\nthe behaviour of the markets, electronic trading gave rise to a new class of\nunprecedented problems associated with market fairness, transparency and\nmanipulation. These problems stem from technical shortcomings that are not\naccounted for in the simple conceptual models used for theoretical market\nanalysis. They, thus, call for more pragmatic market design methodologies that\nconsider the various infrastructure complexities and their potential impact on\nthe market procedures. First, we formally define temporal fairness and then\nexplain why it is very difficult for order-matching policies to ensure it in\ncontinuous markets. Subsequently, we introduce a list of system requirements\nand evaluate existing \"fair\" market designs in various practical and\nadversarial scenarios. We conclude that they fail to retain their properties in\nthe presence of infrastructure inefficiencies and sophisticated technical\nmanipulation attacks. Based on these findings, we then introduce Libra, a\n\"fair\" policy that is resilient to gaming and tolerant of technical\ncomplications. Our security analysis shows that it is significantly more robust\nthan existing designs, while Libra's deployment (in a live foreign currency\nexchange) validated both its considerably low impact on the operation of the\nmarket and its ability to reduce speed-based predatory trading.",
    "published_date": "2019-10-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "q-fin.TR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00321v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1910.00292v2",
    "title": "Generalization in Generation: A closer look at Exposure Bias",
    "authors": [
      "Florian Schmidt"
    ],
    "author_ids": [],
    "abstract": "Exposure bias refers to the train-test discrepancy that seemingly arises when\nan autoregressive generative model uses only ground-truth contexts at training\ntime but generated ones at test time. We separate the contributions of the\nmodel and the learning framework to clarify the debate on consequences and\nreview proposed counter-measures. In this light, we argue that generalization\nis the underlying property to address and propose unconditional generation as\nits fundamental benchmark. Finally, we combine latent variable modeling with a\nrecent formulation of exploration in reinforcement learning to obtain a\nrigorous handling of true and generated contexts. Results on language modeling\nand variational sentence auto-encoding confirm the model's generalization\ncapability.",
    "published_date": "2019-10-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00292v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.00069v1",
    "title": "Tightening Bounds for Variational Inference by Revisiting Perturbation Theory",
    "authors": [
      "Robert Bamler",
      "Cheng Zhang",
      "Manfred Opper",
      "Stephan Mandt"
    ],
    "author_ids": [],
    "abstract": "Variational inference has become one of the most widely used methods in\nlatent variable modeling. In its basic form, variational inference employs a\nfully factorized variational distribution and minimizes its KL divergence to\nthe posterior. As the minimization can only be carried out approximately, this\napproximation induces a bias. In this paper, we revisit perturbation theory as\na powerful way of improving the variational approximation. Perturbation theory\nrelies on a form of Taylor expansion of the log marginal likelihood, vaguely in\nterms of the log ratio of the true posterior and its variational approximation.\nWhile first order terms give the classical variational bound, higher-order\nterms yield corrections that tighten it. However, traditional perturbation\ntheory does not provide a lower bound, making it inapt for stochastic\noptimization. In this paper, we present a similar yet alternative way of\nderiving corrections to the ELBO that resemble perturbation theory, but that\nresult in a valid bound. We show in experiments on Gaussian Processes and\nVariational Autoencoders that the new bounds are more mass covering, and that\nthe resulting posterior covariances are closer to the true posterior and lead\nto higher likelihoods on held-out data.",
    "published_date": "2019-09-30T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00069v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.13705v1",
    "title": "A Closer Look at Data Bias in Neural Extractive Summarization Models",
    "authors": [
      "Ming Zhong",
      "Danqing Wang",
      "Pengfei Liu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we take stock of the current state of summarization datasets\nand explore how different factors of datasets influence the generalization\nbehaviour of neural extractive summarization models. Specifically, we first\npropose several properties of datasets, which matter for the generalization of\nsummarization models. Then we build the connection between priors residing in\ndatasets and model designs, analyzing how different properties of datasets\ninfluence the choices of model structure design and training methods. Finally,\nby taking a typical dataset as an example, we rethink the process of the model\ndesign based on the experience of the above analysis. We demonstrate that when\nwe have a deep understanding of the characteristics of datasets, a simple\napproach can bring significant improvements to the existing state-of-the-art\nmodel.A",
    "published_date": "2019-09-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.13705v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.13446v2",
    "title": "Random Bias Initialization Improves Quantized Training",
    "authors": [
      "Xinlin Li",
      "Vahid Partovi Nia"
    ],
    "author_ids": [],
    "abstract": "Binary neural networks improve computationally efficiency of deep models with\na large margin. However, there is still a performance gap between a successful\nfull-precision training and binary training. We bring some insights about why\nthis accuracy drop exists and call for a better understanding of binary network\ngeometry. We start with analyzing full-precision neural networks with ReLU\nactivation and compare it with its binarized version. This comparison suggests\nto initialize networks with random bias, a counter-intuitive remedy.",
    "published_date": "2019-09-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.13446v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.13126v2",
    "title": "Feature Level Fusion from Facial Attributes for Face Recognition",
    "authors": [
      "Mohammad Rasool Izadi"
    ],
    "author_ids": [],
    "abstract": "We introduce a deep convolutional neural networks (CNN) architecture to\nclassify facial attributes and recognize face images simultaneously via a\nshared learning paradigm to improve the accuracy for facial attribute\nprediction and face recognition performance. In this method, we use facial\nattributes as an auxiliary source of information to assist CNN features\nextracted from the face images to improve the face recognition performance.\nSpecifically, we use a shared CNN architecture that jointly predicts facial\nattributes and recognize face images simultaneously via a shared learning\nparameters, and then we use facial attribute features an an auxiliary source of\ninformation concatenated by face features to increase the discrimination of the\nCNN for face recognition. This process assists the CNN classifier to better\nrecognize face images. The experimental results show that our model increases\nboth the face recognition and facial attribute prediction performance,\nespecially for the identity attributes such as gender and race. We evaluated\nour method on several standard datasets labeled by identities and face\nattributes and the results show that the proposed method outperforms\nstate-of-the-art face recognition models.",
    "published_date": "2019-09-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.13126v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.06144v2",
    "title": "What does it mean to solve the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems",
    "authors": [
      "Javier Sanchez-Monedero",
      "Lina Dencik",
      "Lilian Edwards"
    ],
    "author_ids": [],
    "abstract": "The ability to get and keep a job is a key aspect of participating in society\nand sustaining livelihoods. Yet the way decisions are made on who is eligible\nfor jobs, and why, are rapidly changing with the advent and growth in uptake of\nautomated hiring systems (AHSs) powered by data-driven tools. Key concerns\nabout such AHSs include the lack of transparency and potential limitation of\naccess to jobs for specific profiles. In relation to the latter, however,\nseveral of these AHSs claim to detect and mitigate discriminatory practices\nagainst protected groups and promote diversity and inclusion at work. Yet\nwhilst these tools have a growing user-base around the world, such claims of\nbias mitigation are rarely scrutinised and evaluated, and when done so, have\nalmost exclusively been from a US socio-legal perspective. In this paper, we\nintroduce a perspective outside the US by critically examining how three\nprominent automated hiring systems (AHSs) in regular use in the UK, HireVue,\nPymetrics and Applied, understand and attempt to mitigate bias and\ndiscrimination. Using publicly available documents, we describe how their tools\nare designed, validated and audited for bias, highlighting assumptions and\nlimitations, before situating these in the socio-legal context of the UK. The\nUK has a very different legal background to the US in terms not only of hiring\nand equality law, but also in terms of data protection (DP) law. We argue that\nthis might be important for addressing concerns about transparency and could\nmean a challenge to building bias mitigation into AHSs definitively capable of\nmeeting EU legal standards. This is significant as these AHSs, especially those\ndeveloped in the US, may obscure rather than improve systemic discrimination in\nthe workplace.",
    "published_date": "2019-09-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4; J.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.06144v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.13063v3",
    "title": "Training convolutional neural networks with cheap convolutions and online distillation",
    "authors": [
      "Jiao Xie",
      "Shaohui Lin",
      "Yichen Zhang",
      "Linkai Luo"
    ],
    "author_ids": [],
    "abstract": "The large memory and computation consumption in convolutional neural networks\n(CNNs) has been one of the main barriers for deploying them on resource-limited\nsystems. To this end, most cheap convolutions (e.g., group convolution,\ndepth-wise convolution, and shift convolution) have recently been used for\nmemory and computation reduction but with the specific architecture designing.\nFurthermore, it results in a low discriminability of the compressed networks by\ndirectly replacing the standard convolution with these cheap ones. In this\npaper, we propose to use knowledge distillation to improve the performance of\nthe compact student networks with cheap convolutions. In our case, the teacher\nis a network with the standard convolution, while the student is a simple\ntransformation of the teacher architecture without complicated redesigning. In\nparticular, we propose a novel online distillation method, which online\nconstructs the teacher network without pre-training and conducts mutual\nlearning between the teacher and student network, to improve the performance of\nthe student model. Extensive experiments demonstrate that the proposed approach\nachieves superior performance to simultaneously reduce memory and computation\noverhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and\nImageNet ILSVRC 2012, compared to the state-of-the-art CNN compression and\nacceleration methods. The codes are publicly available at\nhttps://github.com/EthanZhangYC/OD-cheap-convolution.",
    "published_date": "2019-09-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.13063v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.00411v7",
    "title": "Generating Fair Universal Representations using Adversarial Models",
    "authors": [
      "Peter Kairouz",
      "Jiachun Liao",
      "Chong Huang",
      "Maunil Vyas",
      "Monica Welfert",
      "Lalitha Sankar"
    ],
    "author_ids": [],
    "abstract": "We present a data-driven framework for learning fair universal\nrepresentations (FUR) that guarantee statistical fairness for any learning task\nthat may not be known a priori. Our framework leverages recent advances in\nadversarial learning to allow a data holder to learn representations in which a\nset of sensitive attributes are decoupled from the rest of the dataset. We\nformulate this as a constrained minimax game between an encoder and an\nadversary where the constraint ensures a measure of usefulness (utility) of the\nrepresentation. The resulting problem is that of censoring, i.e., finding a\nrepresentation that is least informative about the sensitive attributes given a\nutility constraint. For appropriately chosen adversarial loss functions, our\ncensoring framework precisely clarifies the optimal adversarial strategy\nagainst strong information-theoretic adversaries; it also achieves the fairness\nmeasure of demographic parity for the resulting constrained representations. We\nevaluate the performance of our proposed framework on both synthetic and\npublicly available datasets. For these datasets, we use two tradeoff measures:\ncensoring vs. representation fidelity and fairness vs. utility for downstream\ntasks, to amply demonstrate that multiple sensitive features can be effectively\ncensored even as the resulting fair representations ensure accuracy for\nmultiple downstream tasks.",
    "published_date": "2019-09-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00411v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.12868v1",
    "title": "Automatically Learning Data Augmentation Policies for Dialogue Tasks",
    "authors": [
      "Tong Niu",
      "Mohit Bansal"
    ],
    "author_ids": [],
    "abstract": "Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for\noptimal perturbation policies via a controller trained using performance\nrewards of a sampled policy on the target task, hence reducing data-level model\nbias. While being a powerful algorithm, their work has focused on computer\nvision tasks, where it is comparatively easy to apply imperceptible\nperturbations without changing an image's semantic meaning. In our work, we\nadapt AutoAugment to automatically discover effective perturbation policies for\nnatural language processing (NLP) tasks such as dialogue generation. We start\nwith a pool of atomic operations that apply subtle semantic-preserving\nperturbations to the source inputs of a dialogue task (e.g., different POS-tag\ntypes of stopword dropout, grammatical errors, and paraphrasing). Next, we\nallow the controller to learn more complex augmentation policies by searching\nover the space of the various combinations of these atomic operations.\nMoreover, we also explore conditioning the controller on the source inputs of\nthe target task, since certain strategies may not apply to inputs that do not\ncontain that strategy's required linguistic features. Empirically, we\ndemonstrate that both our input-agnostic and input-aware controllers discover\nuseful data augmentation policies, and achieve significant improvements over\nthe previous state-of-the-art, including trained on manually-designed policies.",
    "published_date": "2019-09-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12868v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.12838v2",
    "title": "Responsible AI by Design in Practice",
    "authors": [
      "Richard Benjamins",
      "Alberto Barbado",
      "Daniel Sierra"
    ],
    "author_ids": [],
    "abstract": "Recently, a lot of attention has been given to undesired consequences of\nArtificial Intelligence (AI), such as unfair bias leading to discrimination, or\nthe lack of explanations of the results of AI systems. There are several\nimportant questions to answer before AI can be deployed at scale in our\nbusinesses and societies. Most of these issues are being discussed by experts\nand the wider communities, and it seems there is broad consensus on where they\ncome from. There is, however, less consensus on, and experience with how to\npractically deal with those issues in organizations that develop and use AI,\nboth from a technical and organizational perspective. In this paper, we discuss\nthe practical case of a large organization that is putting in place a\ncompany-wide methodology to minimize the risk of undesired consequences of AI.\nWe hope that other organizations can learn from this and that our experience\ncontributes to making the best of AI while minimizing its risks.",
    "published_date": "2019-09-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "K.4.1, K.4.2, K.4.3",
      "K.4.1; K.4.2; K.4.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12838v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.12454v1",
    "title": "SoK: Blockchain Technology and Its Potential Use Cases",
    "authors": [
      "Scott Ruoti",
      "Ben Kaiser",
      "Arkady Yerukhimovich",
      "Jeremy Clark",
      "Robert Cunningham"
    ],
    "author_ids": [],
    "abstract": "Bitcoin's success has led to significant interest in its underlying\ncomponents, particularly Blockchain technology. Over 10 years after Bitcoin's\ninitial release, the community still suffers from a lack of clarity regarding\nwhat properties defines Blockchain technology, its relationship to similar\ntechnologies, and which of its proposed use-cases are tenable and which are\nlittle more than hype. In this paper we answer four common questions regarding\nBlockchain technology: (1) what exactly is Blockchain technology, (2) what\ncapabilities does it provide, and (3) what are good applications for Blockchain\ntechnology, and (4) how does it relate to other approache distributed\ntechnologies (e.g., distributed databases). We accomplish this goal by using\ngrounded theory (a structured approach to gathering and analyzing qualitative\ndata) to thoroughly analyze a large corpus of literature on Blockchain\ntechnology. This method enables us to answer the above questions while limiting\nresearcher bias, separating thought leadership from peddled hype and\nidentifying open research questions related to Blockchain technology. The\naudience for this paper is broad as it aims to help researchers in a variety of\nareas come to a better understanding of Blockchain technology and identify\nwhether it may be of use in their own research.",
    "published_date": "2019-09-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.CY",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12454v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.12243v2",
    "title": "Data Smashing 2.0: Sequence Likelihood (SL) Divergence For Fast Time Series Comparison",
    "authors": [
      "Yi Huang",
      "Ishanu Chattopadhyay"
    ],
    "author_ids": [],
    "abstract": "Recognizing subtle historical patterns is central to modeling and forecasting\nproblems in time series analysis. Here we introduce and develop a new approach\nto quantify deviations in the underlying hidden generators of observed data\nstreams, resulting in a new efficiently computable universal metric for time\nseries. The proposed metric is in the sense that we can compare and contrast\ndata streams regardless of where and how they are generated and without any\nfeature engineering step. The approach proposed in this paper is conceptually\ndistinct from our previous work on data smashing, and vastly improves\ndiscrimination performance and computing speed. The core idea here is the\ngeneralization of the notion of KL divergence often used to compare probability\ndistributions to a notion of divergence in time series. We call this the\nsequence likelihood (SL) divergence, which may be used to measure deviations\nwithin a well-defined class of discrete-valued stochastic processes. We devise\nefficient estimators of SL divergence from finite sample paths and subsequently\nformulate a universal metric useful for computing distance between time series\nproduced by hidden stochastic generators.",
    "published_date": "2019-09-26T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.MF"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12243v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.12051v2",
    "title": "The Implicit Bias of Depth: How Incremental Learning Drives Generalization",
    "authors": [
      "Daniel Gissin",
      "Shai Shalev-Shwartz",
      "Amit Daniely"
    ],
    "author_ids": [],
    "abstract": "A leading hypothesis for the surprising generalization of neural networks is\nthat the dynamics of gradient descent bias the model towards simple solutions,\nby searching through the solution space in an incremental order of complexity.\nWe formally define the notion of incremental learning dynamics and derive the\nconditions on depth and initialization for which this phenomenon arises in deep\nlinear models. Our main theoretical contribution is a dynamical depth\nseparation result, proving that while shallow models can exhibit incremental\nlearning dynamics, they require the initialization to be exponentially small\nfor these dynamics to present themselves. However, once the model becomes\ndeeper, the dependence becomes polynomial and incremental learning can arise in\nmore natural settings. We complement our theoretical findings by experimenting\nwith deep matrix sensing, quadratic neural networks and with binary\nclassification using diagonal and convolutional linear networks, showing all of\nthese models exhibit incremental learning.",
    "published_date": "2019-09-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12051v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.11869v1",
    "title": "This Thing Called Fairness: Disciplinary Confusion Realizing a Value in Technology",
    "authors": [
      "Deirdre K. Mulligan",
      "Joshua A. Kroll",
      "Nitin Kohli",
      "Richmond Y. Wong"
    ],
    "author_ids": [],
    "abstract": "The explosion in the use of software in important sociotechnical systems has\nrenewed focus on the study of the way technical constructs reflect policies,\nnorms, and human values. This effort requires the engagement of scholars and\npractitioners from many disciplines. And yet, these disciplines often\nconceptualize the operative values very differently while referring to them\nusing the same vocabulary. The resulting conflation of ideas confuses\ndiscussions about values in technology at disciplinary boundaries. In the\nservice of improving this situation, this paper examines the value of shared\nvocabularies, analytics, and other tools that facilitate conversations about\nvalues in light of these disciplinary specific conceptualizations, the role\nsuch tools play in furthering research and practice, outlines different\nconceptions of \"fairness\" deployed in discussions about computer systems, and\nprovides an analytic tool for interdisciplinary discussions and collaborations\naround the concept of fairness. We use a case study of risk assessments in\ncriminal justice applications to both motivate our effort--describing how\nconflation of different concepts under the banner of \"fairness\" led to\nunproductive confusion--and illustrate the value of the fairness analytic by\ndemonstrating how the rigorous analysis it enables can assist in identifying\nkey areas of theoretical, political, and practical misunderstanding or\ndisagreement, and where desired support alignment or collaboration in the\nabsence of consensus.",
    "published_date": "2019-09-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.HC",
      "H.1; I.2; J.7; K.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11869v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.11849v2",
    "title": "The Ant Swarm Neuro-Evolution Procedure for Optimizing Recurrent Networks",
    "authors": [
      "AbdElRahman A. ElSaid",
      "Alexander G. Ororbia",
      "Travis J. Desell"
    ],
    "author_ids": [],
    "abstract": "Hand-crafting effective and efficient structures for recurrent neural\nnetworks (RNNs) is a difficult, expensive, and time-consuming process. To\naddress this challenge, we propose a novel neuro-evolution algorithm based on\nant colony optimization (ACO), called ant swarm neuro-evolution (ASNE), for\ndirectly optimizing RNN topologies. The procedure selects from multiple modern\nrecurrent cell types such as Delta-RNN, GRU, LSTM, MGU and UGRNN cells, as well\nas recurrent connections which may span multiple layers and/or steps of time.\nIn order to introduce an inductive bias that encourages the formation of\nsparser synaptic connectivity patterns, we investigate several variations of\nthe core algorithm. We do so primarily by formulating different functions that\ndrive the underlying pheromone simulation process (which mimic L1 and L2\nregularization in standard machine learning) as well as by introducing ant\nagents with specialized roles (inspired by how real ant colonies operate),\ni.e., explorer ants that construct the initial feed forward structure and\nsocial ants which select nodes from the feed forward connections to\nsubsequently craft recurrent memory structures. We also incorporate a\nLamarckian strategy for weight initialization which reduces the number of\nbackpropagation epochs required to locally train candidate RNNs, speeding up\nthe neuro-evolution process. Our results demonstrate that the sparser RNNs\nevolved by ASNE significantly outperform traditional one and two layer\narchitectures consisting of modern memory cells, as well as the well-known NEAT\nalgorithm. Furthermore, we improve upon prior state-of-the-art results on the\ntime series dataset utilized in our experiments.",
    "published_date": "2019-09-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11849v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.11793v2",
    "title": "MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit",
    "authors": [
      "John Palowitch",
      "Bryan Perozzi"
    ],
    "author_ids": [],
    "abstract": "Are Graph Neural Networks (GNNs) fair? In many real world graphs, the\nformation of edges is related to certain node attributes (e.g. gender,\ncommunity, reputation). In this case, standard GNNs using these edges will be\nbiased by this information, as it is encoded in the structure of the adjacency\nmatrix itself. In this paper, we show that when metadata is correlated with the\nformation of node neighborhoods, unsupervised node embedding dimensions learn\nthis metadata. This bias implies an inability to control for important\ncovariates in real-world applications, such as recommendation systems. To solve\nthese issues, we introduce the Metadata-Orthogonal Node Embedding Training\n(MONET) unit, a general model for debiasing embeddings of nodes in a graph.\nMONET achieves this by ensuring that the node embeddings are trained on a\nhyperplane orthogonal to that of the node metadata. This effectively organizes\nunstructured embedding dimensions into an interpretable topology-only,\nmetadata-only division with no linear interactions. We illustrate the\neffectiveness of MONET though our experiments on a variety of real world\ngraphs, which shows that our method can learn and remove the effect of\narbitrary covariates in tasks such as preventing the leakage of political party\naffiliation in a blog network, and thwarting the gaming of embedding-based\nrecommendation systems.",
    "published_date": "2019-09-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11793v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.12931v4",
    "title": "Revenue allocation in Formula One: a pairwise comparison approach",
    "authors": [
      "Dóra Gréta Petróczy",
      "László Csató"
    ],
    "author_ids": [],
    "abstract": "A model is proposed to allocate Formula One World Championship prize money\namong the constructors. The methodology is based on pairwise comparison\nmatrices, allows for the use of any weighting method, and makes possible to\ntune the level of inequality. We introduce an axiom called scale invariance,\nwhich requires the ranking of the teams to be independent of the parameter\ncontrolling inequality. The eigenvector method is revealed to violate this\ncondition in our dataset, while the row geometric mean method always satisfies\nit. The revenue allocation is not influenced by the arbitrary valuation given\nto the race prizes in the official points scoring system of Formula One and\ntakes the intensity of pairwise preferences into account, contrary to the\nstandard Condorcet method. Our approach can be used to share revenues among\ngroups when group members are ranked several times.",
    "published_date": "2019-09-25T00:00:00",
    "year": 2019,
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC",
      "62F07, 90B50, 91B08"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12931v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.11555v2",
    "title": "Optimally Compressed Nonparametric Online Learning",
    "authors": [
      "Alec Koppel",
      "Amrit Singh Bedi",
      "Ketan Rajawat",
      "Brian M. Sadler"
    ],
    "author_ids": [],
    "abstract": "Batch training of machine learning models based on neural networks is now\nwell established, whereas to date streaming methods are largely based on linear\nmodels. To go beyond linear in the online setting, nonparametric methods are of\ninterest due to their universality and ability to stably incorporate new\ninformation via convexity or Bayes' Rule. Unfortunately, when used online,\nnonparametric methods suffer a \"curse of dimensionality\" which precludes their\nuse: their complexity scales at least with the time index. We survey online\ncompression tools which bring their memory under control and attain approximate\nconvergence. The asymptotic bias depends on a compression parameter that trades\noff memory and accuracy. Further, the applications to robotics, communications,\neconomics, and power are discussed, as well as extensions to multi-agent\nsystems.",
    "published_date": "2019-09-25T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11555v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.11414v1",
    "title": "Inequality is rising where social network segregation interacts with urban topology",
    "authors": [
      "Gergő Tóth",
      "Johannes Wachs",
      "Riccardo Di Clemente",
      "Ákos Jakobi",
      "Bence Ságvári",
      "János Kertész",
      "Balázs Lengyel"
    ],
    "author_ids": [],
    "abstract": "Social networks amplify inequalities due to fundamental mechanisms of social\ntie formation such as homophily and triadic closure. These forces sharpen\nsocial segregation reflected in network fragmentation. Yet, little is known\nabout what structural factors facilitate fragmentation. In this paper we use\nbig data from a widely-used online social network to demonstrate that there is\na significant relationship between social network fragmentation and income\ninequality in cities and towns. We find that the organization of the physical\nurban space has a stronger relationship with fragmentation than unequal access\nto education, political segregation, or the presence of ethnic and religious\nminorities. Fragmentation of social networks is significantly higher in towns\nin which residential neighborhoods are divided by physical barriers such as\nrivers and railroads and are relatively distant from the center of town. Towns\nin which amenities are spatially concentrated are also typically more socially\nsegregated. These relationships suggest how urban planning may be a useful\npoint of intervention to mitigate inequalities in the long run.",
    "published_date": "2019-09-25T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11414v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.11190v1",
    "title": "Mobile Phone Data for Children on the Move: Challenges and Opportunities",
    "authors": [
      "Vedran Sekara",
      "Elisa Omodei",
      "Laura Healy",
      "Jan Beise",
      "Claus Hansen",
      "Danzhen You",
      "Saskia Blume",
      "Manuel Garcia-Herranz"
    ],
    "author_ids": [],
    "abstract": "Today, 95% of the global population has 2G mobile phone coverage and the\nnumber of individuals who own a mobile phone is at an all time high. Mobile\nphones generate rich data on billions of people across different societal\ncontexts and have in the last decade helped redefine how we do research and\nbuild tools to understand society. As such, mobile phone data has the potential\nto revolutionize how we tackle humanitarian problems, such as the many suffered\nby refugees all over the world. While promising, mobile phone data and the new\ncomputational approaches bring both opportunities and challenges. Mobile phone\ntraces contain detailed information regarding people's whereabouts, social\nlife, and even financial standing. Therefore, developing and adopting\nstrategies that open data up to the wider humanitarian and international\ndevelopment community for analysis and research while simultaneously protecting\nthe privacy of individuals is of paramount importance. Here we outline the\nchallenging situation of children on the move and actions UNICEF is pushing in\nhelping displaced children and youth globally, and discuss opportunities where\nmobile phone data can be used. We identify three key challenges: data access,\ndata and algorithmic bias, and operationalization of research, which need to be\naddressed if mobile phone data is to be successfully applied in humanitarian\ncontexts.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11190v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.11166v1",
    "title": "Ethical Hacking for IoT Security: A First Look into Bug Bounty Programs and Responsible Disclosure",
    "authors": [
      "Aaron Yi Ding",
      "Gianluca Limon De Jesus",
      "Marijn Janssen"
    ],
    "author_ids": [],
    "abstract": "The security of the Internet of Things (IoT) has attracted much attention due\nto the growing number of IoT-oriented security incidents. IoT hardware and\nsoftware security vulnerabilities are exploited affecting many companies and\npersons. Since the causes of vulnerabilities go beyond pure technical measures,\nthere is a pressing demand nowadays to demystify IoT \"security complex\" and\ndevelop practical guidelines for both companies, consumers, and regulators. In\nthis paper, we present an initial study targeting an unexplored sphere in IoT\nby illuminating the potential of crowdsource ethical hacking approaches for\nenhancing IoT vulnerability management. We focus on Bug Bounty Programs (BBP)\nand Responsible Disclosure (RD), which stimulate hackers to report\nvulnerability in exchange for monetary rewards. We carried out a qualitative\ninvestigation supported by literature survey and expert interviews to explore\nhow BBP and RD can facilitate the practice of identifying, classifying,\nprioritizing, remediating, and mitigating IoT vulnerabilities in an effective\nand cost-efficient manner. Besides deriving tangible guidelines for IoT\nstakeholders, our study also sheds light on a systematic integration path to\ncombine BBP and RD with existing security practices (e.g., penetration test) to\nfurther boost overall IoT security.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11166v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.11117v1",
    "title": "Semi-supervised classification on graphs using explicit diffusion dynamics",
    "authors": [
      "Robert L. Peach",
      "Alexis Arnaudon",
      "Mauricio Barahona"
    ],
    "author_ids": [],
    "abstract": "Classification tasks based on feature vectors can be significantly improved\nby including within deep learning a graph that summarises pairwise\nrelationships between the samples. Intuitively, the graph acts as a conduit to\nchannel and bias the inference of class labels. Here, we study classification\nmethods that consider the graph as the originator of an explicit graph\ndiffusion. We show that appending graph diffusion to feature-based learning as\nan \\textit{a posteriori} refinement achieves state-of-the-art classification\naccuracy. This method, which we call Graph Diffusion Reclassification (GDR),\nuses overshooting events of a diffusive graph dynamics to reclassify individual\nnodes. The method uses intrinsic measures of node influence, which are distinct\nfor each node, and allows the evaluation of the relationship and importance of\nfeatures and graph for classification. We also present diff-GCN, a simple\nextension of Graph Convolutional Neural Network (GCN) architectures that\nleverages explicit diffusion dynamics, and allows the natural use of directed\ngraphs. To showcase our methods, we use benchmark datasets of documents with\nassociated citation data.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IR",
      "cs.SI",
      "physics.data-an",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11117v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.11074v1",
    "title": "Power Allocation in Cache-Aided NOMA Systems: Optimization and Deep Reinforcement Learning Approaches",
    "authors": [
      "Khai Nguyen Doan",
      "Mojtaba Vaezi",
      "Wonjae Shin",
      "H. Vincent Poor",
      "Hyundong Shin",
      "Tony Q. S. Quek"
    ],
    "author_ids": [],
    "abstract": "This work exploits the advantages of two prominent techniques in future\ncommunication networks, namely caching and non-orthogonal multiple access\n(NOMA). Particularly, a system with Rayleigh fading channels and cache-enabled\nusers is analyzed. It is shown that the caching-NOMA combination provides a new\nopportunity of cache hit which enhances the cache utility as well as the\neffectiveness of NOMA. Importantly, this comes without requiring users'\ncollaboration, and thus, avoids many complicated issues such as users' privacy\nand security, selfishness, etc. In order to optimize users' quality of service\nand, concurrently, ensure the fairness among users, the probability that all\nusers can decode the desired signals is maximized. In NOMA, a combination of\nmultiple messages are sent to users, and the defined objective is approached by\nfinding an appropriate power allocation for message signals. To address the\npower allocation problem, two novel methods are proposed. The first one is a\ndivide-and-conquer-based method for which closed-form expressions for the\noptimal resource allocation policy are derived, making this method simple and\nflexible to the system context. The second one is based on the deep\nreinforcement learning method that allows all users to share the full\nbandwidth. Finally, simulation results are provided to demonstrate the\neffectiveness of the proposed methods and to compare their performance.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.LG",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11074v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.10948v1",
    "title": "Microchain: A Hybrid Consensus Mechanism for Lightweight Distributed Ledger for IoT",
    "authors": [
      "Ronghua Xu",
      "Yu Chen",
      "Erik Blasch",
      "Genshe Chen"
    ],
    "author_ids": [],
    "abstract": "A blockchain and smart contract enabled security mechanism for IoT\napplications has been reported recently for urban, financial, and network\nservices. However, due to the power-intensive and a low-throughput consensus\nmechanism in existing blockchain, like Bitcoin and Ethereum, there are still\nchallenges in integrating blockchain technology into resource-constrained IoT\nplatforms. In this paper, Microchain, based on a hybrid Proof-of-Credit\n(PoC)-Voting-based Chain Finality (VCF) consensus protocol, is proposed to\nprovide a secure, scalable and lightweight distributed ledger for IoT systems.\nBy using a bias-resistant randomness protocol and a cryptographic sortition\nalgorithm, a random subset of nodes are selected as a final committee to\nperform the consensus protocol. The hybrid consensus mechanism relies on PoC, a\npure Proof of stake (PoS) protocol, to determine whether or not a participant\nis qualified to propose a block, given a fair initial distribution of the\ncredit assignment. The voting-based chain finality protocol is responsible for\nfinalizing a history of blocks by resolving conflicting checkpoint and\nselecting a unique chain. A proof-of-conception prototype is implemented and\ntested on a physical network environment. The experimental results verify that\nthe Micorchain is able to offer a partially decentralized, scalable and\nlightweight distributed ledger protocol for IoT applications.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10948v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.10925v1",
    "title": "Scalable Fair Division for 'At Most One' Preferences",
    "authors": [
      "Christian Kroer",
      "Alexander Peysakhovich"
    ],
    "author_ids": [],
    "abstract": "Allocating multiple scarce items across a set of individuals is an important\npractical problem. In the case of divisible goods and additive preferences a\nconvex program can be used to find the solution that maximizes Nash welfare\n(MNW). The MNW solution is equivalent to finding the equilibrium of a market\neconomy (aka. the competitive equilibrium from equal incomes, CEEI) and thus\nhas good properties such as Pareto optimality, envy-freeness, and incentive\ncompatibility in the large. Unfortunately, this equivalence (and nice\nproperties) breaks down for general preference classes. Motivated by real world\nproblems such as course allocation and recommender systems we study the case of\nadditive `at most one' (AMO) preferences - individuals want at most 1 of each\nitem and lotteries are allowed. We show that in this case the MNW solution is\nstill a convex program and importantly is a CEEI solution when the instance\ngets large but has a `low rank' structure. Thus a polynomial time algorithm can\nbe used to scale CEEI (which is in general PPAD-hard) for AMO preferences. We\nexamine whether the properties guaranteed in the limit hold approximately in\nfinite samples using several real datasets.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.MA",
      "econ.EM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10925v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.10698v1",
    "title": "Multi-scale discriminative Region Discovery for Weakly-Supervised Object Localization",
    "authors": [
      "Pei Lv",
      "Haiyu Yu",
      "Junxiao Xue",
      "Junjin Cheng",
      "Lisha Cui",
      "Bing Zhou",
      "Mingliang Xu",
      "Yi Yang"
    ],
    "author_ids": [],
    "abstract": "Localizing objects with weak supervision in an image is a key problem of the\nresearch in computer vision community. Many existing Weakly-Supervised Object\nLocalization (WSOL) approaches tackle this problem by estimating the most\ndiscriminative regions with feature maps (activation maps) obtained by Deep\nConvolutional Neural Network, that is, only the objects or parts of them with\nthe most discriminative response will be located. However, the activation maps\noften display different local maximum responses or relatively weak response\nwhen one image contains multiple objects with the same type or small objects.\nIn this paper, we propose a simple yet effective multi-scale discriminative\nregion discovery method to localize not only more integral objects but also as\nmany as possible with only image-level class labels. The gradient weights\nflowing into different convolutional layers of CNN are taken as the input of\nour method, which is different from previous methods only considering that of\nthe final convolutional layer. To mine more discriminative regions for the task\nof object localization, the multiple local maximum from the gradient weight\nmaps are leveraged to generate the localization map with a parallel sliding\nwindow. Furthermore, multi-scale localization maps from different convolutional\nlayers are fused to produce the final result. We evaluate the proposed method\nwith the foundation of VGGnet on the ILSVRC 2016, CUB-200-2011 and PASCAL VOC\n2012 datasets. On ILSVRC 2016, the proposed method yields the Top-1\nlocalization error of 48.65\\%, which outperforms previous results by 2.75\\%. On\nPASCAL VOC 2012, our approach achieve the highest localization accuracy of\n0.43. Even for CUB-200-2011 dataset, our method still achieves competitive\nresults.",
    "published_date": "2019-09-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10698v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.10549v1",
    "title": "Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Estimators for Reinforcement Learning",
    "authors": [
      "Gregory Farquhar",
      "Shimon Whiteson",
      "Jakob Foerster"
    ],
    "author_ids": [],
    "abstract": "Gradient-based methods for optimisation of objectives in stochastic settings\nwith unknown or intractable dynamics require estimators of derivatives. We\nderive an objective that, under automatic differentiation, produces\nlow-variance unbiased estimators of derivatives at any order. Our objective is\ncompatible with arbitrary advantage estimators, which allows the control of the\nbias and variance of any-order derivatives when using function approximation.\nFurthermore, we propose a method to trade off bias and variance of higher order\nderivatives by discounting the impact of more distant causal dependencies. We\ndemonstrate the correctness and utility of our objective in analytically\ntractable MDPs and in meta-reinforcement-learning for continuous control.",
    "published_date": "2019-09-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10549v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.10502v7",
    "title": "Weighted Envy-Freeness in Indivisible Item Allocation",
    "authors": [
      "Mithun Chakraborty",
      "Ayumi Igarashi",
      "Warut Suksompong",
      "Yair Zick"
    ],
    "author_ids": [],
    "abstract": "We introduce and analyze new envy-based fairness concepts for agents with\nweights that quantify their entitlements in the allocation of indivisible\nitems. We propose two variants of weighted envy-freeness up to one item (WEF1):\nstrong, where envy can be eliminated by removing an item from the envied\nagent's bundle, and weak, where envy can be eliminated either by removing an\nitem (as in the strong version) or by replicating an item from the envied\nagent's bundle in the envying agent's bundle. We show that for additive\nvaluations, an allocation that is both Pareto optimal and strongly WEF1 always\nexists and can be computed in pseudo-polynomial time; moreover, an allocation\nthat maximizes the weighted Nash social welfare may not be strongly WEF1, but\nalways satisfies the weak version of the property. Moreover, we establish that\na generalization of the round-robin picking sequence algorithm produces in\npolynomial time a strongly WEF1 allocation for an arbitrary number of agents;\nfor two agents, we can efficiently achieve both strong WEF1 and Pareto\noptimality by adapting the adjusted winner procedure. Our work highlights\nseveral aspects in which weighted fair division is richer and more challenging\nthan its unweighted counterpart.",
    "published_date": "2019-09-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.GT",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10502v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.10411v1",
    "title": "NLVR2 Visual Bias Analysis",
    "authors": [
      "Alane Suhr",
      "Yoav Artzi"
    ],
    "author_ids": [],
    "abstract": "NLVR2 (Suhr et al., 2019) was designed to be robust for language bias through\na data collection process that resulted in each natural language sentence\nappearing with both true and false labels. The process did not provide a\nsimilar measure of control for visual bias. This technical report analyzes the\npotential for visual bias in NLVR2. We show that some amount of visual bias\nlikely exists. Finally, we identify a subset of the test data that allows to\ntest for model performance in a way that is robust to such potential biases. We\nshow that the performance of existing models (Li et al., 2019; Tan and Bansal\n2019) is relatively robust to this potential bias. We propose to add the\nevaluation on this subset of the data to the NLVR2 evaluation protocol, and\nupdate the official release to include it. A notebook including an\nimplementation of the code used to replicate this analysis is available at\nhttp://nlvr.ai/NLVR2BiasAnalysis.html.",
    "published_date": "2019-09-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10411v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.10179v1",
    "title": "Design of Globally Exponentially Convergent Continuous Observers for Velocity Bias and State for Systems on Real Matrix Groups",
    "authors": [
      "Dong Eui Chang"
    ],
    "author_ids": [],
    "abstract": "We propose globally exponentially convergent continuous observers for\ninvariant kinematic systems on finite-dimensional matrix Lie groups. Such an\nobserver estimates, from measurements of landmarks, vectors and biased\nvelocity, both the system state and the unknown constant bias in velocity\nmeasurement, where the state belongs to the state-space Lie group and the\nvelocity to the Lie algebra of the Lie group. The main technique is to embed a\ngiven system defined on a matrix Lie group into Euclidean space and build\nobservers in the Euclidean space. The theory is illustrated with the special\nEuclidean group in three dimensions.",
    "published_date": "2019-09-23T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10179v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.12230v2",
    "title": "KnowBias: Detecting Political Polarity in Long Text Content",
    "authors": [
      "Aditya Saligrama"
    ],
    "author_ids": [],
    "abstract": "We introduce a classification scheme for detecting political bias in long\ntext content such as newspaper opinion articles. Obtaining long text data and\nannotations at sufficient scale for training is difficult, but it is relatively\neasy to extract political polarity from tweets through their authorship. We\ntrain on tweets and perform inference on articles. Universal sentence encoders\nand other existing methods that aim to address this domain-adaptation scenario\ndeliver inaccurate and inconsistent predictions on articles, which we show is\ndue to a difference in opinion concentration between tweets and articles. We\npropose a two-step classification scheme that uses a neutral detector trained\non tweets to remove neutral sentences from articles in order to align opinion\nconcentration and therefore improve accuracy on that domain. Our implementation\nis available for public use at https://knowbias.ml.",
    "published_date": "2019-09-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.12230v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.10055v1",
    "title": "A Probabilistic Graph Model for Trust Opinion Estimation in Online Social Networks",
    "authors": [
      "Luke Liu",
      "Qing Yang"
    ],
    "author_ids": [],
    "abstract": "Trust assessment plays a key role in many online applications, such as online\nmoney lending, product reviewing and active friending. Trust models usually\nemploy a group of parameters to represent the trust relation between a\ntrustor-trustee pair. These parameters are originated from the trustor's bias\nand opinion on the trustee. Naturally, these parameters can be regarded as a\nvector. To address this problem, we propose a framework to accurately convert\nthe single values to the parameters needed by 3VSL. The framework firstly\nemploys a probabilistic graph model (PGM) to derive the trustor's opinion and\nbias to his rating on the trustee.",
    "published_date": "2019-09-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10055v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.10005v2",
    "title": "Incremental Fairness in Two-Sided Market Platforms: On Smoothly Updating Recommendations",
    "authors": [
      "Gourab K Patro",
      "Abhijnan Chakraborty",
      "Niloy Ganguly",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "Major online platforms today can be thought of as two-sided markets with\nproducers and customers of goods and services. There have been concerns that\nover-emphasis on customer satisfaction by the platforms may affect the\nwell-being of the producers. To counter such issues, few recent works have\nattempted to incorporate fairness for the producers. However, these studies\nhave overlooked an important issue in such platforms -- to supposedly improve\ncustomer utility, the underlying algorithms are frequently updated, causing\nabrupt changes in the exposure of producers. In this work, we focus on the\nfairness issues arising out of such frequent updates, and argue for incremental\nupdates of the platform algorithms so that the producers have enough time to\nadjust (both logistically and mentally) to the change. However, naive\nincremental updates may become unfair to the customers. Thus focusing on\nrecommendations deployed on two-sided platforms, we formulate an ILP based\nonline optimization to deploy changes incrementally in n steps, where we can\nensure smooth transition of the exposure of items while guaranteeing a minimum\nutility for every customer. Evaluations over multiple real world datasets show\nthat our proposed mechanism for platform updates can be efficient and fair to\nboth the producers and the customers in two-sided platforms.",
    "published_date": "2019-09-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.10005v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.09903v1",
    "title": "A Multi-Strategy Approach to Overcoming Bias in Community Detection Evaluation",
    "authors": [
      "Jeancarlo Campos Leão",
      "Alberto H. F. Laender",
      "Pedro O. S. Vaz de Melo"
    ],
    "author_ids": [],
    "abstract": "Community detection is key to understand the structure of complex networks.\nHowever, the lack of appropriate evaluation strategies for this specific task\nmay produce biased and incorrect results that might invalidate further analyses\nor applications based on such networks. In this context, the main contribution\nof this paper is an approach that supports a robust quality evaluation when\ndetecting communities in real-world networks. In our approach, we use multiple\nstrategies that capture distinct aspects of the communities. The conclusion on\nthe quality of these communities is based on the consensus among the strategies\nadopted for the structural evaluation, as well as on the comparison with\ncommunities detected by different methods and with their existing ground\ntruths. In this way, our approach allows one to overcome biases in network\ndata, detection algorithms and evaluation metrics, thus providing more\nconsistent conclusions about the quality of the detected communities.\nExperiments conducted with several real and synthetic networks provided results\nthat show the effectiveness of our approach.",
    "published_date": "2019-09-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09903v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09839v1",
    "title": "Class Activation Map generation by Multiple Level Class Grouping and Orthogonal Constraint",
    "authors": [
      "Kaixu Huang",
      "Fanman Meng",
      "Hongliang Li",
      "Shuai Chen",
      "Qingbo Wu",
      "King N. Ngan"
    ],
    "author_ids": [],
    "abstract": "Class activation map (CAM) highlights regions of classes based on\nclassification network, which is widely used in weakly supervised tasks.\nHowever, it faces the problem that the class activation regions are usually\nsmall and local. Although several efforts paid to the second step (the CAM\ngeneration step) have partially enhanced the generation, we believe such\nproblem is also caused by the first step (training step), because single\nclassification model trained on the entire classes contains finite discriminate\ninformation that limits the object region extraction. To this end, this paper\nsolves CAM generation by using multiple classification models. To form multiple\nclassification networks that carry different discriminative information, we try\nto capture the semantic relationships between classes to form different\nsemantic levels of classification models. Specifically, hierarchical clustering\nbased on class relationships is used to form hierarchical clustering results,\nwhere the clustering levels are treated as semantic levels to form the\nclassification models. Moreover, a new orthogonal module and a two-branch based\nCAM generation method are proposed to generate class regions that are\northogonal and complementary. We use the PASCAL VOC 2012 dataset to verify the\nproposed method. Experimental results show that our approach improves the CAM\ngeneration.",
    "published_date": "2019-09-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09839v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09777v3",
    "title": "Generating Positive Bounding Boxes for Balanced Training of Object Detectors",
    "authors": [
      "Kemal Oksuz",
      "Baris Can Cam",
      "Emre Akbas",
      "Sinan Kalkan"
    ],
    "author_ids": [],
    "abstract": "Two-stage deep object detectors generate a set of regions-of-interest (RoI)\nin the first stage, then, in the second stage, identify objects among the\nproposed RoIs that sufficiently overlap with a ground truth (GT) box. The\nsecond stage is known to suffer from a bias towards RoIs that have low\nintersection-over-union (IoU) with the associated GT boxes. To address this\nissue, we first propose a sampling method to generate bounding boxes (BB) that\noverlap with a given reference box more than a given IoU threshold. Then, we\nuse this BB generation method to develop a positive RoI (pRoI) generator that\nproduces RoIs following any desired spatial or IoU distribution, for the\nsecond-stage. We show that our pRoI generator is able to simulate other\nsampling methods for positive examples such as hard example mining and prime\nsampling. Using our generator as an analysis tool, we show that (i) IoU\nimbalance has an adverse effect on performance, (ii) hard positive example\nmining improves the performance only for certain input IoU distributions, and\n(iii) the imbalance among the foreground classes has an adverse effect on\nperformance and that it can be alleviated at the batch level. Finally, we train\nFaster R-CNN using our pRoI generator and, compared to conventional training,\nobtain better or on-par performance for low IoUs and significant improvements\nwhen trained for higher IoUs for Pascal VOC and MS COCO datasets. The code is\navailable at: https://github.com/kemaloksuz/BoundingBoxGenerator.",
    "published_date": "2019-09-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09777v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09758v3",
    "title": "Empirical Analysis of Multi-Task Learning for Reducing Model Bias in Toxic Comment Detection",
    "authors": [
      "Ameya Vaidya",
      "Feng Mai",
      "Yue Ning"
    ],
    "author_ids": [],
    "abstract": "With the recent rise of toxicity in online conversations on social media\nplatforms, using modern machine learning algorithms for toxic comment detection\nhas become a central focus of many online applications. Researchers and\ncompanies have developed a variety of models to identify toxicity in online\nconversations, reviews, or comments with mixed successes. However, many\nexisting approaches have learned to incorrectly associate non-toxic comments\nthat have certain trigger-words (e.g. gay, lesbian, black, muslim) as a\npotential source of toxicity. In this paper, we evaluate several\nstate-of-the-art models with the specific focus of reducing model bias towards\nthese commonly-attacked identity groups. We propose a multi-task learning model\nwith an attention layer that jointly learns to predict the toxicity of a\ncomment as well as the identities present in the comments in order to reduce\nthis bias. We then compare our model to an array of shallow and deep-learning\nmodels using metrics designed especially to test for unintended model bias\nwithin these identity groups.",
    "published_date": "2019-09-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09758v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09633v1",
    "title": "Quantifying the Impact of Cognitive Biases in Question-Answering Systems",
    "authors": [
      "Keith Burghardt",
      "Tad Hogg",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "Crowdsourcing can identify high-quality solutions to problems; however,\nindividual decisions are constrained by cognitive biases. We investigate some\nof these biases in an experimental model of a question-answering system. In\nboth natural and controlled experiments, we observe a strong position bias in\nfavor of answers appearing earlier in a list of choices. This effect is\nenhanced by three cognitive factors: the attention an answer receives, its\nperceived popularity, and cognitive load, measured by the number of choices a\nuser has to process. While separately weak, these effects synergistically\namplify position bias and decouple user choices of best answers from their\nintrinsic quality. We end our paper by discussing the novel ways we can apply\nthese findings to substantially improve how high-quality answers are found in\nquestion-answering systems.",
    "published_date": "2019-09-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09633v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.09389v1",
    "title": "Sampling Bias in Deep Active Classification: An Empirical Study",
    "authors": [
      "Ameya Prabhu",
      "Charles Dognin",
      "Maneesh Singh"
    ],
    "author_ids": [],
    "abstract": "The exploding cost and time needed for data labeling and model training are\nbottlenecks for training DNN models on large datasets. Identifying smaller\nrepresentative data samples with strategies like active learning can help\nmitigate such bottlenecks. Previous works on active learning in NLP identify\nthe problem of sampling bias in the samples acquired by uncertainty-based\nquerying and develop costly approaches to address it. Using a large empirical\nstudy, we demonstrate that active set selection using the posterior entropy of\ndeep models like FastText.zip (FTZ) is robust to sampling biases and to various\nalgorithmic choices (query size and strategies) unlike that suggested by\ntraditional literature. We also show that FTZ based query strategy produces\nsample sets similar to those from more sophisticated approaches (e.g ensemble\nnetworks). Finally, we show the effectiveness of the selected samples by\ncreating tiny high-quality datasets, and utilizing them for fast and cheap\ntraining of large models. Based on the above, we propose a simple baseline for\ndeep active text classification that outperforms the state-of-the-art. We\nexpect the presented work to be useful and informative for dataset compression\nand for problems involving active, semi-supervised or online learning\nscenarios. Code and models are available at:\nhttps://github.com/drimpossible/Sampling-Bias-Active-Learning",
    "published_date": "2019-09-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09349v2",
    "title": "Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom",
    "authors": [
      "Juan Luis Gonzalez Bello",
      "Munchurl Kim"
    ],
    "author_ids": [],
    "abstract": "The 3D-zoom operation is the positive translation of the camera in the\nZ-axis, perpendicular to the image plane. In contrast, the optical zoom changes\nthe focal length and the digital zoom is used to enlarge a certain region of an\nimage to the original image size. In this paper, we are the first to formulate\nan unsupervised 3D-zoom learning problem where images with an arbitrary zoom\nfactor can be generated from a given single image. An unsupervised framework is\nconvenient, as it is a challenging task to obtain a 3D-zoom dataset of natural\nscenes due to the need for special equipment to ensure camera movement is\nrestricted to the Z-axis. In addition, the objects in the scenes should not\nmove when being captured, which hinders the construction of a large dataset of\noutdoor scenes. We present a novel unsupervised framework to learn how to\ngenerate arbitrarily 3D-zoomed versions of a single image, not requiring a\n3D-zoom ground truth, called the Deep 3D-Zoom Net. The Deep 3D-Zoom Net\nincorporates the following features: (i) transfer learning from a pre-trained\ndisparity estimation network via a back re-projection reconstruction loss; (ii)\na fully convolutional network architecture that models depth-image-based\nrendering (DIBR), taking into account high-frequency details without the need\nfor estimating the intermediate disparity; and (iii) incorporating a\ndiscriminator network that acts as a no-reference penalty for unnaturally\nrendered areas. Even though there is no baseline to fairly compare our results,\nour method outperforms previous novel view synthesis research in terms of\nrealistic appearance on large camera baselines. We performed extensive\nexperiments to verify the effectiveness of our method on the KITTI and\nCityscapes datasets.",
    "published_date": "2019-09-20T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09349v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09285v2",
    "title": "Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias",
    "authors": [
      "Asma Ghandeharioun",
      "Brian Eoff",
      "Brendan Jou",
      "Rosalind W. Picard"
    ],
    "author_ids": [],
    "abstract": "Supporting model interpretability for complex phenomena where annotators can\nlegitimately disagree, such as emotion recognition, is a challenging machine\nlearning task. In this work, we show that explicitly quantifying the\nuncertainty in such settings has interpretability benefits. We use a simple\nmodification of a classical network inference using Monte Carlo dropout to give\nmeasures of epistemic and aleatoric uncertainty. We identify a significant\ncorrelation between aleatoric uncertainty and human annotator disagreement\n($r\\approx.3$). Additionally, we demonstrate how difficult and subjective\ntraining samples can be identified using aleatoric uncertainty and how\nepistemic uncertainty can reveal data bias that could result in unfair\npredictions. We identify the total uncertainty as a suitable surrogate for\nmodel calibration, i.e. the degree we can trust model's predicted confidence.\nIn addition to explainability benefits, we observe modest performance boosts\nfrom incorporating model uncertainty.",
    "published_date": "2019-09-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09285v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09156v1",
    "title": "Learning to Conceal: A Deep Learning Based Method for Preserving Privacy and Avoiding Prejudice",
    "authors": [
      "Moshe Hanukoglu",
      "Nissan Goldberg",
      "Aviv Rovshitz",
      "Amos Azaria"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce a learning model able to conceals personal\ninformation (e.g. gender, age, ethnicity, etc.) from an image, while\nmaintaining any additional information present in the image (e.g. smile,\nhair-style, brightness). Our trained model is not provided the information that\nit is concealing, and does not try learning it either. Namely, we created a\nvariational autoencoder (VAE) model that is trained on a dataset including\nlabels of the information one would like to conceal (e.g. gender, ethnicity,\nage). These labels are directly added to the VAE's sampled latent vector. Due\nto the limited number of neurons in the latent vector and its appended noise,\nthe VAE avoids learning any relation between the given images and the given\nlabels, as those are given directly. Therefore, the encoded image lacks any of\nthe information one wishes to conceal. The encoding may be decoded back into an\nimage according to any provided properties (e.g. a 40 year old woman).\n  The proposed architecture can be used as a mean for privacy preserving and\ncan serve as an input to systems, which will become unbiased and not suffer\nfrom prejudice. We believe that privacy and discrimination are two of the most\nimportant aspects in which the community should try and develop methods to\nprevent misuse of technological advances.",
    "published_date": "2019-09-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09156v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09141v2",
    "title": "Causal Modeling for Fairness in Dynamical Systems",
    "authors": [
      "Elliot Creager",
      "David Madras",
      "Toniann Pitassi",
      "Richard Zemel"
    ],
    "author_ids": [],
    "abstract": "In many application areas---lending, education, and online recommenders, for\nexample---fairness and equity concerns emerge when a machine learning system\ninteracts with a dynamically changing environment to produce both immediate and\nlong-term effects for individuals and demographic groups. We discuss causal\ndirected acyclic graphs (DAGs) as a unifying framework for the recent\nliterature on fairness in such dynamical systems. We show that this formulation\naffords several new directions of inquiry to the modeler, where causal\nassumptions can be expressed and manipulated. We emphasize the importance of\ncomputing interventional quantities in the dynamical fairness setting, and show\nhow causal assumptions enable simulation (when environment dynamics are known)\nand off-policy estimation (when dynamics are unknown) of intervention on short-\nand long-term outcomes, at both the group and individual levels.",
    "published_date": "2019-09-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09141v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.09138v1",
    "title": "Uncovering Sociological Effect Heterogeneity using Machine Learning",
    "authors": [
      "Jennie E. Brand",
      "Jiahui Xu",
      "Bernard Koch",
      "Pablo Geraldo"
    ],
    "author_ids": [],
    "abstract": "Individuals do not respond uniformly to treatments, events, or interventions.\nSociologists routinely partition samples into subgroups to explore how the\neffects of treatments vary by covariates like race, gender, and socioeconomic\nstatus. In so doing, analysts determine the key subpopulations based on\ntheoretical priors. Data-driven discoveries are also routine, yet the analyses\nby which sociologists typically go about them are problematic and seldom move\nus beyond our expectations, and biases, to explore new meaningful subgroups.\nEmerging machine learning methods allow researchers to explore sources of\nvariation that they may not have previously considered, or envisaged. In this\npaper, we use causal trees to recursively partition the sample and uncover\nsources of treatment effect heterogeneity. We use honest estimation, splitting\nthe sample into a training sample to grow the tree and an estimation sample to\nestimate leaf-specific effects. Assessing a central topic in the social\ninequality literature, college effects on wages, we compare what we learn from\nconventional approaches for exploring variation in effects to causal trees.\nGiven our use of observational data, we use leaf-specific matching and\nsensitivity analyses to address confounding and offer interpretations of\neffects based on observed and unobserved heterogeneity. We encourage\nresearchers to follow similar practices in their work on variation in\nsociological effects.",
    "published_date": "2019-09-18T00:00:00",
    "year": 2019,
    "categories": [
      "stat.OT",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.09138v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.08518v3",
    "title": "Bias In, Bias Out? Evaluating the Folk Wisdom",
    "authors": [
      "Ashesh Rambachan",
      "Jonathan Roth"
    ],
    "author_ids": [],
    "abstract": "We evaluate the folk wisdom that algorithmic decision rules trained on data\nproduced by biased human decision-makers necessarily reflect this bias. We\nconsider a setting where training labels are only generated if a biased\ndecision-maker takes a particular action, and so \"biased\" training data arise\ndue to discriminatory selection into the training data. In our baseline model,\nthe more biased the decision-maker is against a group, the more the algorithmic\ndecision rule favors that group. We refer to this phenomenon as \"bias\nreversal.\" We then clarify the conditions that give rise to bias reversal.\nWhether a prediction algorithm reverses or inherits bias depends critically on\nhow the decision-maker affects the training data as well as the label used in\ntraining. We illustrate our main theoretical results in a simulation study\napplied to the New York City Stop, Question and Frisk dataset.",
    "published_date": "2019-09-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08518v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.08375v2",
    "title": "Advancing subgroup fairness via sleeping experts",
    "authors": [
      "Avrim Blum",
      "Thodoris Lykouris"
    ],
    "author_ids": [],
    "abstract": "We study methods for improving fairness to subgroups in settings with\noverlapping populations and sequential predictions. Classical notions of\nfairness focus on the balance of some property across different populations.\nHowever, in many applications the goal of the different groups is not to be\npredicted equally but rather to be predicted well. We demonstrate that the task\nof satisfying this guarantee for multiple overlapping groups is not\nstraightforward and show that for the simple objective of unweighted average of\nfalse negative and false positive rate, satisfying this for overlapping\npopulations can be statistically impossible even when we are provided\npredictors that perform well separately on each subgroup. On the positive side,\nwe show that when individuals are equally important to the different groups\nthey belong to, this goal is achievable; to do so, we draw a connection to the\nsleeping experts literature in online learning. Motivated by the one-sided\nfeedback in natural settings of interest, we extend our results to such a\nfeedback model. We also provide a game-theoretic interpretation of our results,\nexamining the incentives of participants to join the system and to provide the\nsystem full information about predictors they may possess. We end with several\ninteresting open problems concerning the strength of guarantees that can be\nachieved in a computationally efficient manner.",
    "published_date": "2019-09-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08375v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.08255v1",
    "title": "Towards Ethical Machines Via Logic Programming",
    "authors": [
      "Abeer Dyoub",
      "Stefania Costantini",
      "Francesca A. Lisi"
    ],
    "author_ids": [],
    "abstract": "Autonomous intelligent agents are playing increasingly important roles in our\nlives. They contain information about us and start to perform tasks on our\nbehalves. Chatbots are an example of such agents that need to engage in a\ncomplex conversations with humans. Thus, we need to ensure that they behave\nethically. In this work we propose a hybrid logic-based approach for ethical\nchatbots.",
    "published_date": "2019-09-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "I.2;I.2.3;I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08255v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.08081v1",
    "title": "A Distributed Fair Machine Learning Framework with Private Demographic Data Protection",
    "authors": [
      "Hui Hu",
      "Yijun Liu",
      "Zhen Wang",
      "Chao Lan"
    ],
    "author_ids": [],
    "abstract": "Fair machine learning has become a significant research topic with broad\nsocietal impact. However, most fair learning methods require direct access to\npersonal demographic data, which is increasingly restricted to use for\nprotecting user privacy (e.g. by the EU General Data Protection Regulation). In\nthis paper, we propose a distributed fair learning framework for protecting the\nprivacy of demographic data. We assume this data is privately held by a third\nparty, which can communicate with the data center (responsible for model\ndevelopment) without revealing the demographic information. We propose a\nprincipled approach to design fair learning methods under this framework,\nexemplify four methods and show they consistently outperform their existing\ncounterparts in both fairness and accuracy across three real-world data sets.\nWe theoretically analyze the framework, and prove it can learn models with high\nfairness or high accuracy, with their trade-offs balanced by a threshold\nvariable.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08081v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.08017v2",
    "title": "Verifying Reachability Properties in Markov Chains via Incremental Induction",
    "authors": [
      "Elizabeth Polgreen",
      "Martin Brain",
      "Martin Fraenzle",
      "Alessandro Abate"
    ],
    "author_ids": [],
    "abstract": "There is a scalability gap between probabilistic and non-probabilistic\nverification. Probabilistic model checking tools are based either on explicit\nengines or on (Multi-Terminal) Binary Decision Diagrams. These structures are\ncomplemented in areas of non-probabilistic verification by more scalable\ntechniques, such as IC3. We present a symbolic probabilistic model checking\nalgorithm based on IC3-like incremental construction of inductive clauses to\npartition the state space, interleaved with incremental construction of a\nsystem of linear inequalities. This paper compares our implementation to\nstandard quantitative verification alternatives: our experiments show that our\nalgorithm is a step to more scalable symbolic verification of rare events in\nfinite-state Markov chains.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08017v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.07945v1",
    "title": "ProtoGAN: Towards Few Shot Learning for Action Recognition",
    "authors": [
      "Sai Kumar Dwivedi",
      "Vikram Gupta",
      "Rahul Mitra",
      "Shuaib Ahmed",
      "Arjun Jain"
    ],
    "author_ids": [],
    "abstract": "Few-shot learning (FSL) for action recognition is a challenging task of\nrecognizing novel action categories which are represented by few instances in\nthe training data. In a more generalized FSL setting (G-FSL), both seen as well\nas novel action categories need to be recognized. Conventional classifiers\nsuffer due to inadequate data in FSL setting and inherent bias towards seen\naction categories in G-FSL setting. In this paper, we address this problem by\nproposing a novel ProtoGAN framework which synthesizes additional examples for\nnovel categories by conditioning a conditional generative adversarial network\nwith class prototype vectors. These class prototype vectors are learnt using a\nClass Prototype Transfer Network (CPTN) from examples of seen categories. Our\nsynthesized examples for a novel class are semantically similar to real\nexamples belonging to that class and is used to train a model exhibiting better\ngeneralization towards novel classes. We support our claim by performing\nextensive experiments on three datasets: UCF101, HMDB51 and Olympic-Sports. To\nthe best of our knowledge, we are the first to report the results for G-FSL and\nprovide a strong benchmark for future research. We also outperform the\nstate-of-the-art method in FSL for all the aforementioned datasets.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.07945v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.07913v2",
    "title": "Learning to Deceive with Attention-Based Explanations",
    "authors": [
      "Danish Pruthi",
      "Mansi Gupta",
      "Bhuwan Dhingra",
      "Graham Neubig",
      "Zachary C. Lipton"
    ],
    "author_ids": [],
    "abstract": "Attention mechanisms are ubiquitous components in neural architectures\napplied to natural language processing. In addition to yielding gains in\npredictive accuracy, attention weights are often claimed to confer\ninterpretability, purportedly useful both for providing insights to\npractitioners and for explaining why a model makes its decisions to\nstakeholders. We call the latter use of attention mechanisms into question by\ndemonstrating a simple method for training models to produce deceptive\nattention masks. Our method diminishes the total weight assigned to designated\nimpermissible tokens, even when the models can be shown to nevertheless rely on\nthese features to drive predictions. Across multiple models and tasks, our\napproach manipulates attention weights while paying surprisingly little cost in\naccuracy. Through a human study, we show that our manipulated attention-based\nexplanations deceive people into thinking that predictions from a model biased\nagainst gender minorities do not rely on the gender. Consequently, our results\ncast doubt on attention's reliability as a tool for auditing algorithms in the\ncontext of fairness and accountability.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.07913v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.07780v3",
    "title": "Preprocessing and Cutting Planes with Conflict Graphs",
    "authors": [
      "Samuel S. Brito",
      "Haroldo G. Santos"
    ],
    "author_ids": [],
    "abstract": "This paper addresses the development of conflict graph-based algorithms and\ndata structures into the COIN-OR Branch-and-Cut (CBC) solver, including: $(i)$\nan efficient infrastructure for the construction and manipulation of conflict\ngraphs; $(ii)$ a preprocessing routine based on a clique strengthening scheme\nthat can both reduce the number of constraints and produce stronger\nformulations; $(iii)$ a clique cut separator capable of obtaining dual bounds\nat the root node LP relaxation that are $19.65\\%$ stronger than those provided\nby the equivalent cut generator of a state-of-the-art commercial solver, $3.62$\ntimes better than those attained by the clique cut separator of the GLPK solver\nand $4.22$ times stronger than the dual bounds obtained by the clique\nseparation routine of the COIN-OR Cut Generation Library; and $(iv)$ an\nodd-cycle cut separator with a new lifting module to produce valid odd-wheel\ninequalities. The average gap closed by this new version of CBC was up to four\ntimes better than its previous version. Moreover, the number of mixed-integer\nprograms solved by CBC in a time limit of three hours was increased by\n$23.53\\%$.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "math.OC",
      "90C10",
      "G.1.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.07780v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.07700v1",
    "title": "Efficient, Fair and QoS-Aware Policies for Wirelessly Powered Communication Networks",
    "authors": [
      "Roohollah Rezaei",
      "Naeimeh Omidvar",
      "Mohammad Movahednasab",
      "Mohammad Reza Pakravan",
      "Sumei Sun",
      "Yong Liang Guan"
    ],
    "author_ids": [],
    "abstract": "Wireless power transfer (WPT) is a viable source of energy for wirelessly\npowered communication networks (WPCNs). In this paper, we first consider WPT\nfrom an energy access point (E-AP) to multiple energy receivers (E-Rs) to\nobtain the optimal policy that maximizes the WPT efficiency. For this purpose,\nwe formulate the problem of maximizing the total average received power of the\nE-Rs subject to the average and peak power level constraints of the E-AP. The\nformulated problem is a non-convex stochastic optimization problem. Using some\nstochastic optimization techniques, we tackle the challenges of this problem\nand derive a closed-form expression for the optimal solution, which requires\nthe explicit knowledge of the distribution of channel state information (CSI)\nin the network. We then propose a near-optimal algorithm that does not require\nany explicit knowledge of the CSI distribution and prove that the proposed\nalgorithm attains a near-optimal solution within a guaranteed gap to the\noptimal solution. We next consider fairness among the E-Rs and propose a\nquality of service (QoS) aware fair policy that maximizes a generic network\nutility function while guaranteeing the required QoS of each E-R. Finally, we\nstudy a practical wirelessly powered communication scenario in which the E-Rs\nutilize their energy harvested through WPT to transmit information to the E-AP.\nWe optimize the received information at the E-AP under its average and peak\ntransmission power constraints and the fairness constraints of the E-Rs.\nNumerical results show the significant performance of our proposed solutions\ncompared to the state-of-the-art baselines.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.07700v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.08982v1",
    "title": "AdaFair: Cumulative Fairness Adaptive Boosting",
    "authors": [
      "Vasileios Iosifidis",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "The widespread use of ML-based decision making in domains with high societal\nimpact such as recidivism, job hiring and loan credit has raised a lot of\nconcerns regarding potential discrimination. In particular, in certain cases it\nhas been observed that ML algorithms can provide different decisions based on\nsensitive attributes such as gender or race and therefore can lead to\ndiscrimination. Although, several fairness-aware ML approaches have been\nproposed, their focus has been largely on preserving the overall classification\naccuracy while improving fairness in predictions for both protected and\nnon-protected groups (defined based on the sensitive attribute(s)). The overall\naccuracy however is not a good indicator of performance in case of class\nimbalance, as it is biased towards the majority class. As we will see in our\nexperiments, many of the fairness-related datasets suffer from class imbalance\nand therefore, tackling fairness requires also tackling the imbalance problem.\n  To this end, we propose AdaFair, a fairness-aware classifier based on\nAdaBoost that further updates the weights of the instances in each boosting\nround taking into account a cumulative notion of fairness based upon all\ncurrent ensemble members, while explicitly tackling class-imbalance by\noptimizing the number of ensemble members for balanced classification error.\nOur experiments show that our approach can achieve parity in true positive and\ntrue negative rates for both protected and non-protected groups, while it\nsignificantly outperforms existing fairness-aware methods up to 25% in terms of\nbalanced error.",
    "published_date": "2019-09-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08982v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.07502v2",
    "title": "Automatic Detection and Classification of Cognitive Distortions in Mental Health Text",
    "authors": [
      "Benjamin Shickel",
      "Scott Siegel",
      "Martin Heesacker",
      "Sherry Benton",
      "Parisa Rashidi"
    ],
    "author_ids": [],
    "abstract": "In cognitive psychology, automatic and self-reinforcing irrational thought\npatterns are known as cognitive distortions. Left unchecked, patients\nexhibiting these types of thoughts can become stuck in negative feedback loops\nof unhealthy thinking, leading to inaccurate perceptions of reality commonly\nassociated with anxiety and depression. In this paper, we present a machine\nlearning framework for the automatic detection and classification of 15 common\ncognitive distortions in two novel mental health free text datasets collected\nfrom both crowdsourcing and a real-world online therapy program. When\ndifferentiating between distorted and non-distorted passages, our model\nachieved a weighted F1 score of 0.88. For classifying distorted passages into\none of 15 distortion categories, our model yielded weighted F1 scores of 0.68\nin the larger crowdsourced dataset and 0.45 in the smaller online counseling\ndataset, both of which outperformed random baseline metrics by a large margin.\nFor both tasks, we also identified the most discriminative words and phrases\nbetween classes to highlight common thematic elements for improving targeted\nand therapist-guided mental health treatment. Furthermore, we performed an\nexploratory analysis using unsupervised content-based clustering and topic\nmodeling algorithms as first efforts towards a data-driven perspective on the\nthematic relationship between similar cognitive distortions traditionally\ndeemed unique. Finally, we highlight the difficulties in applying mental\nhealth-based machine learning in a real-world setting and comment on the\nimplications and benefits of our framework for improving automated delivery of\ntherapeutic treatment in conjunction with traditional cognitive-behavioral\ntherapy.",
    "published_date": "2019-09-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.07502v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.06713v1",
    "title": "Human self-determination within algorithmic sociotechnical systems",
    "authors": [
      "Bogdana Rakova",
      "Rumman Chowdhury"
    ],
    "author_ids": [],
    "abstract": "In order to investigate the protection of human self-determination within\nalgorithmic sociotechnical systems, we study the relationships between the\nconcepts of mutability, bias, feedback loops, and power dynamics. We focus on\nthe interactions between people and algorithmic systems in the case of\nRecommender Systems (RS) and provide novel theoretical analysis informed by\nhuman-in-the-loop system design and Supervisory Control, in order to question\nthe dynamics in our interactions with RSs. We explore what meaningful\nreliability monitoring means in the context of RSs and elaborate on the need\nfor metrics that encompass human-algorithmic interaction. We derive a metric we\ncall a barrier-to-exit which is a proxy to the amount of effort a user needs to\nexpend in order for the system to recognize their change in preference. Our\ngoal is to highlight the assumptions and limitations of RSs and introduce a\nhuman-centered method of combating deterministic design.",
    "published_date": "2019-09-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.07370v2",
    "title": "Machine learning in healthcare -- a system's perspective",
    "authors": [
      "Awais Ashfaq",
      "Slawomir Nowaczyk"
    ],
    "author_ids": [],
    "abstract": "A consequence of the fragmented and siloed healthcare landscape is that\npatient care (and data) is split along multitude of different facilities and\ncomputer systems and enabling interoperability between these systems is hard.\nThe lack interoperability not only hinders continuity of care and burdens\nproviders, but also hinders effective application of Machine Learning (ML)\nalgorithms. Thus, most current ML algorithms, designed to understand patient\ncare and facilitate clinical decision-support, are trained on limited datasets.\nThis approach is analogous to the Newtonian paradigm of Reductionism in which a\nsystem is broken down into elementary components and a description of the whole\nis formed by understanding those components individually. A key limitation of\nthe reductionist approach is that it ignores the component-component\ninteractions and dynamics within the system which are often of prime\nsignificance in understanding the overall behaviour of complex adaptive systems\n(CAS). Healthcare is a CAS.\n  Though the application of ML on health data have shown incremental\nimprovements for clinical decision support, ML has a much a broader potential\nto restructure care delivery as a whole and maximize care value. However, this\nML potential remains largely untapped: primarily due to functional limitations\nof Electronic Health Records (EHR) and the inability to see the healthcare\nsystem as a whole. This viewpoint (i) articulates the healthcare as a complex\nsystem which has a biological and an organizational perspective, (ii) motivates\nwith examples, the need of a system's approach when addressing healthcare\nchallenges via ML and, (iii) emphasizes to unleash EHR functionality - while\nduly respecting all ethical and legal concerns - to reap full benefits of ML.",
    "published_date": "2019-09-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.07370v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.06429v1",
    "title": "Recommendation or Discrimination?: Quantifying Distribution Parity in Information Retrieval Systems",
    "authors": [
      "Rinat Khaziev",
      "Bryce Casavant",
      "Pearce Washabaugh",
      "Amy A. Winecoff",
      "Matthew Graham"
    ],
    "author_ids": [],
    "abstract": "Information retrieval (IR) systems often leverage query data to suggest\nrelevant items to users. This introduces the possibility of unfairness if the\nquery (i.e., input) and the resulting recommendations unintentionally correlate\nwith latent factors that are protected variables (e.g., race, gender, and age).\nFor instance, a visual search system for fashion recommendations may pick up on\nfeatures of the human models rather than fashion garments when generating\nrecommendations. In this work, we introduce a statistical test for\n\"distribution parity\" in the top-K IR results, which assesses whether a given\nset of recommendations is fair with respect to a specific protected variable.\nWe evaluate our test using both simulated and empirical results. First, using\nartificially biased recommendations, we demonstrate the trade-off between\nstatistically detectable bias and the size of the search catalog. Second, we\napply our test to a visual search system for fashion garments, specifically\ntesting for recommendation bias based on the skin tone of fashion models. Our\ndistribution parity test can help ensure that IR systems' results are fair and\nproduce a good experience for all users.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06429v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.06321v3",
    "title": "End-to-End Bias Mitigation by Modelling Biases in Corpora",
    "authors": [
      "Rabeeh Karimi Mahabadi",
      "Yonatan Belinkov",
      "James Henderson"
    ],
    "author_ids": [],
    "abstract": "Several recent studies have shown that strong natural language understanding\n(NLU) models are prone to relying on unwanted dataset biases without learning\nthe underlying task, resulting in models that fail to generalize to\nout-of-domain datasets and are likely to perform poorly in real-world\nscenarios. We propose two learning strategies to train neural models, which are\nmore robust to such biases and transfer better to out-of-domain datasets. The\nbiases are specified in terms of one or more bias-only models, which learn to\nleverage the dataset biases. During training, the bias-only models' predictions\nare used to adjust the loss of the base model to reduce its reliance on biases\nby down-weighting the biased examples and focusing the training on the hard\nexamples. We experiment on large-scale natural language inference and fact\nverification benchmarks, evaluating on out-of-domain datasets that are\nspecifically designed to assess the robustness of models against known biases\nin the training data. Results show that our debiasing methods greatly improve\nrobustness in all settings and better transfer to other textual entailment\ndatasets. Our code and data are publicly available in\n\\url{https://github.com/rabeehk/robust-nli}.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06321v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.03090v3",
    "title": "Instagram Fake and Automated Account Detection",
    "authors": [
      "Fatih Cagatay Akyon",
      "Esat Kalfaoglu"
    ],
    "author_ids": [],
    "abstract": "Fake engagement is one of the significant problems in Online Social Networks\n(OSNs) which is used to increase the popularity of an account in an inorganic\nmanner. The detection of fake engagement is crucial because it leads to loss of\nmoney for businesses, wrong audience targeting in advertising, wrong product\npredictions systems, and unhealthy social network environment. This study is\nrelated with the detection of fake and automated accounts which leads to fake\nengagement on Instagram. Prior to this work, there were no publicly available\ndataset for fake and automated accounts. For this purpose, two datasets have\nbeen published for the detection of fake and automated accounts. For the\ndetection of these accounts, machine learning algorithms like Naive Bayes,\nLogistic Regression, Support Vector Machines and Neural Networks are applied.\nAdditionally, for the detection of automated accounts, cost sensitive genetic\nalgorithm is proposed to handle the unnatural bias in the dataset. To deal with\nthe unevenness problem in the fake dataset, Smote-nc algorithm is implemented.\nFor the automated and fake account detection datasets, 86% and 96%\nclassification accuracies are obtained, respectively.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.03090v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.06108v1",
    "title": "Shallow Self-Learning for Reject Inference in Credit Scoring",
    "authors": [
      "Nikita Kozodoi",
      "Panagiotis Katsas",
      "Stefan Lessmann",
      "Luis Moreira-Matias",
      "Konstantinos Papakonstantinou"
    ],
    "author_ids": [],
    "abstract": "Credit scoring models support loan approval decisions in the financial\nservices industry. Lenders train these models on data from previously granted\ncredit applications, where the borrowers' repayment behavior has been observed.\nThis approach creates sample bias. The scoring model (i.e., classifier) is\ntrained on accepted cases only. Applying the resulting model to screen credit\napplications from the population of all borrowers degrades model performance.\nReject inference comprises techniques to overcome sampling bias through\nassigning labels to rejected cases. The paper makes two contributions. First,\nwe propose a self-learning framework for reject inference. The framework is\ngeared toward real-world credit scoring requirements through considering\ndistinct training regimes for iterative labeling and model training. Second, we\nintroduce a new measure to assess the effectiveness of reject inference\nstrategies. Our measure leverages domain knowledge to avoid artificial labeling\nof rejected cases during strategy evaluation. We demonstrate this approach to\noffer a robust and operational assessment of reject inference strategies.\nExperiments on a real-world credit scoring data set confirm the superiority of\nthe adjusted self-learning framework over regular self-learning and previous\nreject inference strategies. We also find strong evidence in favor of the\nproposed evaluation measure assessing reject inference strategies more\nreliably, raising the performance of the eventual credit scoring model.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.RM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06108v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.06092v2",
    "title": "A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces",
    "authors": [
      "Anne Lauscher",
      "Goran Glavaš",
      "Simone Paolo Ponzetto",
      "Ivan Vulić"
    ],
    "author_ids": [],
    "abstract": "Distributional word vectors have recently been shown to encode many of the\nhuman biases, most notably gender and racial biases, and models for attenuating\nsuch biases have consequently been proposed. However, existing models and\nstudies (1) operate on under-specified and mutually differing bias definitions,\n(2) are tailored for a particular bias (e.g., gender bias) and (3) have been\nevaluated inconsistently and non-rigorously. In this work, we introduce a\ngeneral framework for debiasing word embeddings. We operationalize the\ndefinition of a bias by discerning two types of bias specification: explicit\nand implicit. We then propose three debiasing models that operate on explicit\nor implicit bias specifications and that can be composed towards more robust\ndebiasing. Finally, we devise a full-fledged evaluation framework in which we\ncouple existing bias metrics with newly proposed ones. Experimental findings\nacross three embedding methods suggest that the proposed debiasing models are\nrobust and widely applicable: they often completely remove the bias both\nimplicitly and explicitly without degradation of semantic information encoded\nin any of the input distributional spaces. Moreover, we successfully transfer\ndebiasing models, by means of cross-lingual embedding spaces, and remove or\nattenuate biases in distributional word vector spaces of languages that lack\nreadily available bias specifications.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06092v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1910.12948v2",
    "title": "GENDIS: GENetic DIscovery of Shapelets",
    "authors": [
      "Gilles Vandewiele",
      "Femke Ongenae",
      "Filip De Turck"
    ],
    "author_ids": [],
    "abstract": "In the time series classification domain, shapelets are small time series\nthat are discriminative for a certain class. It has been shown that classifiers\nare able to achieve state-of-the-art results on a plethora of datasets by\ntaking as input distances from the input time series to different\ndiscriminative shapelets. Additionally, these shapelets can easily be\nvisualized and thus possess an interpretable characteristic, making them very\nappealing in critical domains, such as the health care domain, where\nlongitudinal data is ubiquitous. In this study, a new paradigm for shapelet\ndiscovery is proposed, which is based upon evolutionary computation. The\nadvantages of the proposed approach are that (i) it is gradient-free, which\ncould allow to escape from local optima more easily and to find suited\ncandidates more easily and supports non-differentiable objectives, (ii) no\nbrute-force search is required, which drastically reduces the computational\ncomplexity by several orders of magnitude, (iii) the total amount of shapelets\nand length of each of these shapelets are evolved jointly with the shapelets\nthemselves, alleviating the need to specify this beforehand, (iv) entire sets\nare evaluated at once as opposed to single shapelets, which results in smaller\nfinal sets with less similar shapelets that result in similar predictive\nperformances, and (v) discovered shapelets do not need to be a subsequence of\nthe input time series. We present the results of experiments which validate the\nenumerated advantages.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NE",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.12948v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.06362v1",
    "title": "Crank up the volume: preference bias amplification in collaborative recommendation",
    "authors": [
      "Kun Lin",
      "Nasim Sonboli",
      "Bamshad Mobasher",
      "Robin Burke"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are personalized: we expect the results given to a\nparticular user to reflect that user's preferences. Some researchers have\nstudied the notion of calibration, how well recommendations match users' stated\npreferences, and bias disparity the extent to which mis-calibration affects\ndifferent user groups. In this paper, we examine bias disparity over a range of\ndifferent algorithms and for different item categories and demonstrate\nsignificant differences between model-based and memory-based algorithms.",
    "published_date": "2019-09-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06362v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.05826v1",
    "title": "A chain rule for the quantum relative entropy",
    "authors": [
      "Kun Fang",
      "Omar Fawzi",
      "Renato Renner",
      "David Sutter"
    ],
    "author_ids": [],
    "abstract": "The chain rule for the classical relative entropy ensures that the relative\nentropy between probability distributions on multipartite systems can be\ndecomposed into a sum of relative entropies of suitably chosen conditional\ndistributions on the individual systems. Here, we prove a similar chain rule\ninequality for the quantum relative entropy in terms of channel relative\nentropies. The new chain rule allows us to solve an open problem in the context\nof asymptotic quantum channel discrimination: surprisingly, adaptive protocols\ncannot improve the error rate for asymmetric channel discrimination compared to\nnon-adaptive strategies. In addition, we give examples of quantum channels\nshowing that the channel relative entropy is not additive under the tensor\nproduct.",
    "published_date": "2019-09-12T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05826v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.05758v1",
    "title": "Geometric Rényi Divergence and its Applications in Quantum Channel Capacities",
    "authors": [
      "Kun Fang",
      "Hamza Fawzi"
    ],
    "author_ids": [],
    "abstract": "We present a systematic study of the geometric R\\'enyi divergence (GRD), also\nknown as the maximal R\\'enyi divergence, from the point of view of quantum\ninformation theory. We show that this divergence, together with its extension\nto channels, has many appealing structural properties. For example we prove a\nchain rule inequality that immediately implies the \"amortization collapse\" for\nthe geometric R\\'enyi divergence, addressing an open question by Berta et al.\n[arXiv:1808.01498, Equation (55)] in the area of quantum channel\ndiscrimination. As applications, we explore various channel capacity problems\nand construct new channel information measures based on the geometric R\\'enyi\ndivergence, sharpening the previously best-known bounds based on the\nmax-relative entropy while still keeping the new bounds single-letter\nefficiently computable. A plethora of examples are investigated and the\nimprovements are evident for almost all cases.",
    "published_date": "2019-09-12T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05758v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.05583v1",
    "title": "Minimizing Margin of Victory for Fair Political and Educational Districting",
    "authors": [
      "Ana-Andreea Stoica",
      "Abhijnan Chakraborty",
      "Palash Dey",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "In many practical scenarios, a population is divided into disjoint groups for\nbetter administration, e.g., electorates into political districts, employees\ninto departments, students into school districts, and so on. However, grouping\npeople arbitrarily may lead to biased partitions, raising concerns of\ngerrymandering in political districting, racial segregation in schools, etc. To\ncounter such issues, in this paper, we conceptualize such problems in a voting\nscenario, and propose FAIR DISTRICTING problem to divide a given set of people\nhaving preference over candidates into k groups such that the maximum margin of\nvictory of any group is minimized. We also propose the FAIR CONNECTED\nDISTRICTING problem which additionally requires each group to be connected. We\nshow that the FAIR DISTRICTING problem is NP-complete for plurality voting even\nif we have only 3 candidates but admits polynomial time algorithms if we assume\nk to be some constant or everyone can be moved to any group. In contrast, we\nshow that the FAIR CONNECTED DISTRICTING problem is NP-complete for plurality\nvoting even if we have only 2 candidates and k = 2. Finally, we propose\nheuristic algorithms for both the problems and show their effectiveness in UK\npolitical districting and in lowering racial segregation in public schools in\nthe US.",
    "published_date": "2019-09-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05583v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.05568v2",
    "title": "On the Effect of Observed Subject Biases in Apparent Personality Analysis from Audio-visual Signals",
    "authors": [
      "Ricardo Darío Pérez Principi",
      "Cristina Palmero",
      "Julio C. S. Jacques Junior",
      "Sergio Escalera"
    ],
    "author_ids": [],
    "abstract": "Personality perception is implicitly biased due to many subjective factors,\nsuch as cultural, social, contextual, gender and appearance. Approaches\ndeveloped for automatic personality perception are not expected to predict the\nreal personality of the target, but the personality external observers\nattributed to it. Hence, they have to deal with human bias, inherently\ntransferred to the training data. However, bias analysis in personality\ncomputing is an almost unexplored area. In this work, we study different\npossible sources of bias affecting personality perception, including emotions\nfrom facial expressions, attractiveness, age, gender, and ethnicity, as well as\ntheir influence on prediction ability for apparent personality estimation. To\nthis end, we propose a multi-modal deep neural network that combines raw audio\nand visual information alongside predictions of attribute-specific models to\nregress apparent personality. We also analyse spatio-temporal aggregation\nschemes and the effect of different time intervals on first impressions. We\nbase our study on the ChaLearn First Impressions dataset, consisting of\none-person conversational videos. Our model shows state-of-the-art results\nregressing apparent personality based on the Big-Five model. Furthermore, given\nthe interpretability nature of our network design, we provide an incremental\nanalysis on the impact of each possible source of bias on final network\npredictions.",
    "published_date": "2019-09-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05568v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.05460v3",
    "title": "Accelerating Column Generation via Flexible Dual Optimal Inequalities with Application to Entity Resolution",
    "authors": [
      "Vishnu Suresh Lokhande",
      "Shaofei Wang",
      "Maneesh Singh",
      "Julian Yarkony"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce a new optimization approach to Entity Resolution.\nTraditional approaches tackle entity resolution with hierarchical clustering,\nwhich does not benefit from a formal optimization formulation. In contrast, we\nmodel entity resolution as correlation-clustering, which we treat as a weighted\nset-packing problem and write as an integer linear program (ILP). In this case\nsources in the input data correspond to elements and entities in output data\ncorrespond to sets/clusters. We tackle optimization of weighted set packing by\nrelaxing integrality in our ILP formulation. The set of potential sets/clusters\ncan not be explicitly enumerated, thus motivating optimization via column\ngeneration. In addition to the novel formulation, we also introduce new dual\noptimal inequalities (DOI), that we call flexible dual optimal inequalities,\nwhich tightly lower-bound dual variables during optimization and accelerate\ncolumn generation. We apply our formulation to entity resolution (also called\nde-duplication of records), and achieve state-of-the-art accuracy on two\npopular benchmark datasets. The project page is available at the following url,\nhttps://github.com/lokhande-vishnu/EntityResolution",
    "published_date": "2019-09-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05460v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.05443v1",
    "title": "Feedback Learning for Improving the Robustness of Neural Networks",
    "authors": [
      "Chang Song",
      "Zuoguan Wang",
      "Hai Li"
    ],
    "author_ids": [],
    "abstract": "Recent research studies revealed that neural networks are vulnerable to\nadversarial attacks. State-of-the-art defensive techniques add various\nadversarial examples in training to improve models' adversarial robustness.\nHowever, these methods are not universal and can't defend unknown or\nnon-adversarial evasion attacks. In this paper, we analyze the model robustness\nin the decision space. A feedback learning method is then proposed, to\nunderstand how well a model learns and to facilitate the retraining process of\nremedying the defects. The evaluations according to a set of distance-based\ncriteria show that our method can significantly improve models' accuracy and\nrobustness against different types of evasion attacks. Moreover, we observe the\nexistence of inter-class inequality and propose to compensate it by changing\nthe proportions of examples generated in different classes.",
    "published_date": "2019-09-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05443v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.05380v1",
    "title": "Selecting Data to Clean for Fact Checking: Minimizing Uncertainty vs. Maximizing Surprise",
    "authors": [
      "Stavros Sintos",
      "Pankaj K. Agarwal",
      "Jun Yang"
    ],
    "author_ids": [],
    "abstract": "We study the optimization problem of selecting numerical quantities to clean\nin order to fact-check claims based on such data. Oftentimes, such claims are\ntechnically correct, but they can still mislead for two reasons. First, data\nmay contain uncertainty and errors. Second, data can be \"fished\" to advance\nparticular positions. In practice, fact-checkers cannot afford to clean all\ndata and must choose to clean what \"matters the most\" to checking a claim. We\nexplore alternative definitions of what \"matters the most\": one is to ascertain\nclaim qualities (by minimizing uncertainty in these measures), while an\nalternative is just to counter the claim (by maximizing the probability of\nfinding a counterargument). We show whether the two objectives align with each\nother, with important implications on when fact-checkers should exercise care\nin selective data cleaning, to avoid potential bias introduced by their desire\nto counter claims. We develop efficient algorithms for solving the various\nvariants of the optimization problem, showing significant improvements over\nnaive solutions. The problem is particularly challenging because the objectives\nin the fact-checking context are complex, non-linear functions over data. We\nobtain results that generalize to a large class of functions, with potential\napplications beyond fact-checking.",
    "published_date": "2019-09-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05380v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.05288v4",
    "title": "Contrastively Smoothed Class Alignment for Unsupervised Domain Adaptation",
    "authors": [
      "Shuyang Dai",
      "Yu Cheng",
      "Yizhe Zhang",
      "Zhe Gan",
      "Jingjing Liu",
      "Lawrence Carin"
    ],
    "author_ids": [],
    "abstract": "Recent unsupervised approaches to domain adaptation primarily focus on\nminimizing the gap between the source and the target domains through refining\nthe feature generator, in order to learn a better alignment between the two\ndomains. This minimization can be achieved via a domain classifier to detect\ntarget-domain features that are divergent from source-domain features. However,\nby optimizing via such domain classification discrepancy, ambiguous target\nsamples that are not smoothly distributed on the low-dimensional data manifold\nare often missed. To solve this issue, we propose a novel Contrastively\nSmoothed Class Alignment (CoSCA) model, that explicitly incorporates both\nintra- and inter-class domain discrepancy to better align ambiguous target\nsamples with the source domain. CoSCA estimates the underlying label hypothesis\nof target samples, and simultaneously adapts their feature representations by\noptimizing a proposed contrastive loss. In addition, Maximum Mean Discrepancy\n(MMD) is utilized to directly match features between source and target samples\nfor better global alignment. Experiments on several benchmark datasets\ndemonstrate that CoSCA can outperform state-of-the-art approaches for\nunsupervised domain adaptation by producing more discriminative features.",
    "published_date": "2019-09-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05288v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.05282v1",
    "title": "Algorithmic and Economic Perspectives on Fairness",
    "authors": [
      "David C. Parkes",
      "Rakesh V. Vohra",
      "other workshop participants"
    ],
    "author_ids": [],
    "abstract": "Algorithmic systems have been used to inform consequential decisions for at\nleast a century. Recidivism prediction dates back to the 1920s. Automated\ncredit scoring dates began in the middle of the last century, but the last\ndecade has witnessed an acceleration in the adoption of prediction algorithms.\nThey are deployed to screen job applicants for the recommendation of products,\npeople, and content, as well as in medicine (diagnostics and decision aids),\ncriminal justice, facial recognition, lending and insurance, and the allocation\nof public services. The prominence of algorithmic methods has led to concerns\nregarding their systematic unfairness in their treatment of those whose\nbehavior they are predicting. These concerns have found their way into the\npopular imagination through news accounts and general interest books. Even when\nthese algorithms are deployed in domains subject to regulation, it appears that\nexisting regulation is poorly equipped to deal with this issue. The word\n'fairness' in this context is a placeholder for three related equity concerns.\nFirst, such algorithms may systematically discriminate against individuals with\na common ethnicity, religion, or gender, irrespective of whether the relevant\ngroup enjoys legal protections. The second is that these algorithms fail to\ntreat people as individuals. Third, who gets to decide how algorithms are\ndesigned and deployed. These concerns are present when humans, unaided, make\npredictions.",
    "published_date": "2019-09-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05282v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.05242v1",
    "title": "Proceedings 12th Interaction and Concurrency Experience",
    "authors": [
      "Massimo Bartoletti",
      "Ludovic Henrio",
      "Anastasia Mavridou",
      "Alceste Scalas"
    ],
    "author_ids": [],
    "abstract": "This volume contains the proceedings of ICE'19, the 12th Interaction and\nConcurrency Experience, which was held in Copenhagen, Denmark on the 20th and\n21st of June 2019, as a satellite event of DisCoTec'19. The ICE workshop series\nfeatures a distinguishing review and selection procedure, allowing PC members\nto interact anonymously with authors. As in the past 11 editions, this\ninteraction considerably improved the accuracy of the feedback from the\nreviewers and the quality of accepted papers, and offered the basis for lively\ndiscussion during the workshop. The 2019 edition of ICE included double blind\nreviewing of original research papers, in order to increase fairness and avoid\nbias in reviewing. Each paper was reviewed by three PC members, and altogether\n9 papers were accepted for publication - plus 2 oral presentations which are\nnot part of this volume. We were proud to host 4 invited talks, by Dilian\nGurov, Fritz Henglein, Sophia Knight, and Hern\\'an Melgratti. The abstracts of\nthese talks are included in this volume together with the regular papers. Final\nversions of the contributions, taking into account the discussion at the\nworkshop, are included.",
    "published_date": "2019-09-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.PL",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05242v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.05167v3",
    "title": "FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and Transparency",
    "authors": [
      "Kacper Sokol",
      "Raul Santos-Rodriguez",
      "Peter Flach"
    ],
    "author_ids": [],
    "abstract": "Today, artificial intelligence systems driven by machine learning algorithms\ncan be in a position to take important, and sometimes legally binding,\ndecisions about our everyday lives. In many cases, however, these systems and\ntheir actions are neither regulated nor certified. To help counter the\npotential harm that such algorithms can cause we developed an open source\ntoolbox that can analyse selected fairness, accountability and transparency\naspects of the machine learning process: data (and their features), models and\npredictions, allowing to automatically and objectively report them to relevant\nstakeholders. In this paper we describe the design, scope, usage and impact of\nthis Python package, which is published under the 3-Clause BSD open source\nlicence.",
    "published_date": "2019-09-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05167v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04974v1",
    "title": "Computer-Aided Automated Detection of Gene-Controlled Social Actions of Drosophila",
    "authors": [
      "Khan Faraz",
      "Ahmed Bouridane",
      "Richard Jiang",
      "Tiancheng Xia",
      "Paul Chazot",
      "Abdel Ennaceur"
    ],
    "author_ids": [],
    "abstract": "Gene expression of social actions in Drosophilae has been attracting wide\ninterest from biologists, medical scientists and psychologists. Gene-edited\nDrosophilae have been used as a test platform for experimental investigation.\nFor example, Parkinson's genes can be embedded into a group of newly bred\nDrosophilae for research purpose. However, human observation of numerous tiny\nDrosophilae for a long term is an arduous work, and the dependence on human's\nacute perception is highly unreliable. As a result, an automated system of\nsocial action detection using machine learning has been highly demanded. In\nthis study, we propose to automate the detection and classification of two\ninnate aggressive actions demonstrated by Drosophilae. Robust keypoint\ndetection is achieved using selective spatio-temporal interest points (sSTIP)\nwhich are then described using the 3D Scale Invariant Feature Transform\n(3D-SIFT) descriptors. Dimensionality reduction is performed using Spectral\nRegression Kernel Discriminant Analysis (SR-KDA) and classification is done\nusing the nearest centre rule. The classification accuracy shown demonstrates\nthe feasibility of the proposed system.",
    "published_date": "2019-09-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04974v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.08093v1",
    "title": "Q-Learning Based Aerial Base Station Placement for Fairness Enhancement in Mobile Networks",
    "authors": [
      "Rozhina Ghanavi",
      "Maryam Sabbaghian",
      "Halim Yanikomeroglu"
    ],
    "author_ids": [],
    "abstract": "In this paper, we use an aerial base station (aerial-BS) to enhance fairness\nin a dynamic environment with user mobility. The problem of optimally placing\nthe aerial-BS is a non-deterministic polynomial-time hard (NP-hard) problem.\nMoreover, the network topology is subject to continuous changes due to the user\nmobility. These issues intensify the quest to develop an adaptive and fast\nalgorithm for 3D placement of the aerial-BS. To this end, we propose a method\nbased on reinforcement learning to achieve these goals. Simulation results show\nthat our method increases fairness among users in a reasonable computing time,\nwhile the solution is comparatively close to the optimal solution obtained by\nexhaustive search.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI",
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.08093v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04463v2",
    "title": "Algorithmic expedients for the S-labeling problem",
    "authors": [
      "Markus Sinnl"
    ],
    "author_ids": [],
    "abstract": "Graph labeling problems have been widely studied in the last decades and have\na vast area of application. In this work, we study the recently introduced\nS-labeling problem, in which the nodes get labeled using labels from 1 to |V |\nand for each edge the contribution to the objective function, called S-labeling\nnumber of the graph, is the minimum label of its end-nodes. The goal is to find\na labeling with minimum value. The problem is NP-hard for planar subcubic\ngraphs, although for many other graph classes the complexity status is still\nunknown. In this paper, we present different algorithmic approaches for\ntackling this problem: We develop an exact solution framework based on\nMixed-Integer Programming (MIP) which is enhanced with valid inequalities,\nstarting and primal heuristics and specialized branching rules. We show that\nour MIP formulation has no integrality gap for paths, cycles and perfect n-ary\ntrees, and, to the best of our knowledge, we give the first polynomial-time\nalgorithm for the problem on n-ary trees as well as a closed formula for the\nS-labeling number for such trees. Moreover, we also present a Lagrangian\nheuristic and a constraint programming approach. A computational study is\ncarried out in order to (i) investigate if there may be other special graph\nclasses, where our MIP formulation has no integrality gap, and (ii) assess the\neffectiveness of the proposed solution approaches for solving the problem on a\ndataset consisting of general graphs.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04463v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.05677v1",
    "title": "Raiders of the Lost Art",
    "authors": [
      "Anthony Bourached",
      "George Cann"
    ],
    "author_ids": [],
    "abstract": "Neural style transfer, first proposed by Gatys et al. (2015), can be used to\ncreate novel artistic work through rendering a content image in the form of a\nstyle image. We present a novel method of reconstructing lost artwork, by\napplying neural style transfer to x-radiographs of artwork with secondary\ninterior artwork beneath a primary exterior, so as to reconstruct lost artwork.\nFinally we reflect on AI art exhibitions and discuss the social, cultural,\nethical, and philosophical impact of these technical innovations.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.05677v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04826v1",
    "title": "Spam filtering on forums: A synthetic oversampling based approach for imbalanced data classification",
    "authors": [
      "Pratik Ratadiya",
      "Rahul Moorthy"
    ],
    "author_ids": [],
    "abstract": "Forums play an important role in providing a platform for community\ninteraction. The introduction of irrelevant content or spam by individuals for\ncommercial and social gains tends to degrade the professional experience\npresented to the forum users. Automated moderation of the relevancy of posted\ncontent is desired. Machine learning is used for text classification and finds\napplications in spam email detection, fraudulent transaction detection etc. The\nbalance of classes in training data is essential in the case of classification\nalgorithms to make the learning efficient and accurate. However, in the case of\nforums, the spam content is sparse compared to the relevant content giving rise\nto a bias towards the latter while training. A model trained on such biased\ndata will fail to classify a spam sample. An approach based on Synthetic\nMinority Over-sampling Technique(SMOTE) is presented in this paper to tackle\nimbalanced training data. It involves synthetically creating new minority class\nsamples from the existing ones until balance in data is achieved. The enhanced\ndata is then passed through various classifiers for which the performance is\nrecorded. The results were analyzed on the data of forums of Spoken Tutorial,\nIIT Bombay over standard performance metrics and revealed that models trained\nafter Synthetic Minority oversampling outperform the ones trained on imbalanced\ndata by substantial margins. An empirical comparison of the results obtained by\nboth SMOTE and without SMOTE for various supervised classification algorithms\nhave been presented in this paper. Synthetic oversampling proves to be a\ncritical technique for achieving uniform class distribution which in turn\nyields commendable results in text classification. The presented approach can\nbe further extended to content categorization on educational websites thus\nhelping to improve the overall digital learning experience.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04386v1",
    "title": "Attesting Biases and Discrimination using Language Semantics",
    "authors": [
      "Xavier Ferrer Aran",
      "Jose M. Such",
      "Natalia Criado"
    ],
    "author_ids": [],
    "abstract": "AI agents are increasingly deployed and used to make automated decisions that\naffect our lives on a daily basis. It is imperative to ensure that these\nsystems embed ethical principles and respect human values. We focus on how we\ncan attest to whether AI agents treat users fairly without discriminating\nagainst particular individuals or groups through biases in language. In\nparticular, we discuss human unconscious biases, how they are embedded in\nlanguage, and how AI systems inherit those biases by learning from and\nprocessing human language. Then, we outline a roadmap for future research to\nbetter understand and attest problematic AI biases derived from language.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CL",
      "68T50"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04386v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04344v1",
    "title": "A Meta-Learning Framework for Generalized Zero-Shot Learning",
    "authors": [
      "Vinay Kumar Verma",
      "Dhanajit Brahma",
      "Piyush Rai"
    ],
    "author_ids": [],
    "abstract": "Learning to classify unseen class samples at test time is popularly referred\nto as zero-shot learning (ZSL). If test samples can be from training (seen) as\nwell as unseen classes, it is a more challenging problem due to the existence\nof strong bias towards seen classes. This problem is generally known as\n\\emph{generalized} zero-shot learning (GZSL). Thanks to the recent advances in\ngenerative models such as VAEs and GANs, sample synthesis based approaches have\ngained considerable attention for solving this problem. These approaches are\nable to handle the problem of class bias by synthesizing unseen class samples.\nHowever, these ZSL/GZSL models suffer due to the following key limitations:\n$(i)$ Their training stage learns a class-conditioned generator using only\n\\emph{seen} class data and the training stage does not \\emph{explicitly} learn\nto generate the unseen class samples; $(ii)$ They do not learn a generic\noptimal parameter which can easily generalize for both seen and unseen class\ngeneration; and $(iii)$ If we only have access to a very few samples per seen\nclass, these models tend to perform poorly. In this paper, we propose a\nmeta-learning based generative model that naturally handles these limitations.\nThe proposed model is based on integrating model-agnostic meta learning with a\nWasserstein GAN (WGAN) to handle $(i)$ and $(iii)$, and uses a novel task\ndistribution to handle $(ii)$. Our proposed model yields significant\nimprovements on standard ZSL as well as more challenging GZSL setting. In ZSL\nsetting, our model yields 4.5\\%, 6.0\\%, 9.8\\%, and 27.9\\% relative improvements\nover the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets,\nrespectively.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04344v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04299v3",
    "title": "A Multistep Lyapunov Approach for Finite-Time Analysis of Biased Stochastic Approximation",
    "authors": [
      "Gang Wang",
      "Bingcong Li",
      "Georgios B. Giannakis"
    ],
    "author_ids": [],
    "abstract": "Motivated by the widespread use of temporal-difference (TD-) and Q-learning\nalgorithms in reinforcement learning, this paper studies a class of biased\nstochastic approximation (SA) procedures under a mild \"ergodic-like\" assumption\non the underlying stochastic noise sequence. Building upon a carefully designed\nmultistep Lyapunov function that looks ahead to several future updates to\naccommodate the stochastic perturbations (for control of the gradient bias), we\nprove a general result on the convergence of the iterates, and use it to derive\nnon-asymptotic bounds on the mean-square error in the case of constant\nstepsizes. This novel looking-ahead viewpoint renders finite-time analysis of\nbiased SA algorithms under a large family of stochastic perturbations possible.\nFor direct comparison with existing contributions, we also demonstrate these\nbounds by applying them to TD- and Q-learning with linear function\napproximation, under the practical Markov chain observation model. The\nresultant finite-time error bound for both the TD- as well as the Q-learning\nalgorithms is the first of its kind, in the sense that it holds i) for the\nunmodified versions (i.e., without making any modifications to the parameter\nupdates) using even nonlinear function approximators; as well as for Markov\nchains ii) under general mixing conditions and iii) starting from any initial\ndistribution, at least one of which has to be violated for existing results to\nbe applicable.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04299v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04276v4",
    "title": "NISER: Normalized Item and Session Representations to Handle Popularity Bias",
    "authors": [
      "Priyanka Gupta",
      "Diksha Garg",
      "Pankaj Malhotra",
      "Lovekesh Vig",
      "Gautam Shroff"
    ],
    "author_ids": [],
    "abstract": "The goal of session-based recommendation (SR) models is to utilize the\ninformation from past actions (e.g. item/product clicks) in a session to\nrecommend items that a user is likely to click next. Recently it has been shown\nthat the sequence of item interactions in a session can be modeled as\ngraph-structured data to better account for complex item transitions. Graph\nneural networks (GNNs) can learn useful representations for such\nsession-graphs, and have been shown to improve over sequential models such as\nrecurrent neural networks [14]. However, we note that these GNN-based\nrecommendation models suffer from popularity bias: the models are biased\ntowards recommending popular items, and fail to recommend relevant long-tail\nitems (less popular or less frequent items). Therefore, these models perform\npoorly for the less popular new items arriving daily in a practical online\nsetting. We demonstrate that this issue is, in part, related to the magnitude\nor norm of the learned item and session-graph representations (embedding\nvectors). We propose a training procedure that mitigates this issue by using\nnormalized representations. The models using normalized item and session-graph\nrepresentations perform significantly better: i. for the less popular long-tail\nitems in the offline setting, and ii. for the less popular newly introduced\nitems in the online setting. Furthermore, our approach significantly improves\nupon existing state-of-the-art on three benchmark datasets.",
    "published_date": "2019-09-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04276v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03977v2",
    "title": "Learning Fair Rule Lists",
    "authors": [
      "Ulrich Aïvodji",
      "Julien Ferry",
      "Sébastien Gambs",
      "Marie-José Huguet",
      "Mohamed Siala"
    ],
    "author_ids": [],
    "abstract": "As the use of black-box models becomes ubiquitous in high stake\ndecision-making systems, demands for fair and interpretable models are\nincreasing. While it has been shown that interpretable models can be as\naccurate as black-box models in several critical domains, existing fair\nclassification techniques that are interpretable by design often display poor\naccuracy/fairness tradeoffs in comparison with their non-interpretable\ncounterparts. In this paper, we propose FairCORELS, a fair classification\ntechnique interpretable by design, whose objective is to learn fair rule lists.\nOur solution is a multi-objective variant of CORELS, a branch-and-bound\nalgorithm to learn rule lists, that supports several statistical notions of\nfairness. Examples of such measures include statistical parity, equal\nopportunity and equalized odds. The empirical evaluation of FairCORELS on\nreal-world datasets demonstrates that it outperforms state-of-the-art fair\nclassification techniques that are interpretable by design while being\ncompetitive with non-interpretable ones.",
    "published_date": "2019-09-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03977v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03939v2",
    "title": "Deterministic Value-Policy Gradients",
    "authors": [
      "Qingpeng Cai",
      "Ling Pan",
      "Pingzhong Tang"
    ],
    "author_ids": [],
    "abstract": "Reinforcement learning algorithms such as the deep deterministic policy\ngradient algorithm (DDPG) has been widely used in continuous control tasks.\nHowever, the model-free DDPG algorithm suffers from high sample complexity. In\nthis paper we consider the deterministic value gradients to improve the sample\nefficiency of deep reinforcement learning algorithms. Previous works consider\ndeterministic value gradients with the finite horizon, but it is too myopic\ncompared with infinite horizon. We firstly give a theoretical guarantee of the\nexistence of the value gradients in this infinite setting. Based on this\ntheoretical guarantee, we propose a class of the deterministic value gradient\nalgorithm (DVG) with infinite horizon, and different rollout steps of the\nanalytical gradients by the learned model trade off between the variance of the\nvalue gradients and the model bias. Furthermore, to better combine the\nmodel-based deterministic value gradient estimators with the model-free\ndeterministic policy gradient estimator, we propose the deterministic\nvalue-policy gradient (DVPG) algorithm. We finally conduct extensive\nexperiments comparing DVPG with state-of-the-art methods on several standard\ncontinuous control benchmarks. Results demonstrate that DVPG substantially\noutperforms other baselines.",
    "published_date": "2019-09-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03939v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03783v1",
    "title": "Sample average approximation of CVaR-based Wardrop equilibrium in routing under uncertain costs",
    "authors": [
      "Ashish Cherukuri"
    ],
    "author_ids": [],
    "abstract": "This paper focuses on the class of routing games that have uncertain costs.\nAssuming that agents are risk-averse and select paths with minimum conditional\nvalue-at-risk (CVaR) associated to them, we define the notion of CVaR-based\nWardrop equilibrium (CWE). We focus on computing this equilibrium under the\ncondition that the distribution of the uncertainty is unknown and a set of\nindependent and identically distributed samples is available. To this end, we\ndefine the sample average approximation scheme where CWE is estimated with\nsolutions of a variational inequality problem involving sample average\napproximations of the CVaR. We establish two properties for this scheme. First,\nunder continuity of costs and boundedness of uncertainty, we prove asymptotic\nconsistency, establishing almost sure convergence of approximate equilibria to\nCWE as the sample size grows. Second, under the additional assumption of\nLipschitz cost, we prove exponential convergence where the probability of the\ndistance between an approximate solution and the CWE being smaller than any\nconstant approaches unity exponentially fast. Simulation example validates our\ntheoretical findings.",
    "published_date": "2019-09-09T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03783v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.03749v3",
    "title": "Learning Visual Dynamics Models of Rigid Objects using Relational Inductive Biases",
    "authors": [
      "Fabio Ferreira",
      "Lin Shao",
      "Tamim Asfour",
      "Jeannette Bohg"
    ],
    "author_ids": [],
    "abstract": "Endowing robots with human-like physical reasoning abilities remains\nchallenging. We argue that existing methods often disregard spatio-temporal\nrelations and by using Graph Neural Networks (GNNs) that incorporate a\nrelational inductive bias, we can shift the learning process towards exploiting\nrelations. In this work, we learn action-conditional forward dynamics models of\na simulated manipulation task from visual observations involving cluttered and\nirregularly shaped objects. We investigate two GNN approaches and empirically\nassess their capability to generalize to scenarios with novel and an increasing\nnumber of objects. The first, Graph Networks (GN) based approach, considers\nexplicitly defined edge attributes and not only does it consistently\nunderperform an auto-encoder baseline that we modified to predict future\nstates, our results indicate how different edge attributes can significantly\ninfluence the predictions. Consequently, we develop the Auto-Predictor that\ndoes not rely on explicitly defined edge attributes. It outperforms the\nbaseline and the GN-based models. Overall, our results show the sensitivity of\nGNN-based approaches to the task representation, the efficacy of relational\ninductive biases and advocate choosing lightweight approaches that implicitly\nreason about relations over ones that leave these decisions to human designers.",
    "published_date": "2019-09-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03749v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03704v1",
    "title": "Estimating Granger Causality with Unobserved Confounders via Deep Latent-Variable Recurrent Neural Network",
    "authors": [
      "Yuan Meng"
    ],
    "author_ids": [],
    "abstract": "Granger causality analysis, as one of the most popular time series causality\nmethods, has been widely used in the economics, neuroscience. However,\nunobserved confounders is a fundamental problem in the observational studies,\nwhich is still not solved for the non-linear Granger causality. The application\nworks often deal with this problem in virtue of the proxy variables, who can be\ntreated as a measure of the confounder with noise. But the proxy variables has\nbeen proved to be unreliable, because of the bias it may induce. In this paper,\nwe try to \"recover\" the unobserved confounders for the Granger causality. We\nuse a generative model with latent variable to build the relationship between\nthe unobserved confounders and the observed variables(tested variable and the\nproxy variables). The posterior distribution of the latent variable is adopted\nto represent the confounders distribution, which can be sampled to get the\nestimated confounders. We adopt the variational autoencoder to estimate the\nintractable posterior distribution. The recurrent neural network is applied to\nbuild the temporal relationship in the data. We evaluate our method in the\nsynthetic and semi-synthetic dataset. The result shows our estimated\nconfounders has a better performance than the proxy variables in the non-linear\nGranger causality with multiple proxies in the semi-synthetic dataset. But the\nperformances of the synthetic dataset and the different noise level of proxy\nseem terrible. Any advice can really help.",
    "published_date": "2019-09-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03704v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03618v2",
    "title": "Bias-Variance Games",
    "authors": [
      "Yiding Feng",
      "Ronen Gradwohl",
      "Jason Hartline",
      "Aleck Johnsen",
      "Denis Nekipelov"
    ],
    "author_ids": [],
    "abstract": "Firms engaged in electronic commerce increasingly rely on predictive\nanalytics via machine-learning algorithms to drive a wide array of managerial\ndecisions. The tuning of many standard machine learning algorithms can be\nunderstood as trading off bias (i.e., accuracy) with variance (i.e., precision)\nin the algorithm's predictions. The goal of this paper is to understand how\ncompetition between firms affects their strategic choice of such algorithms. To\nthis end, we model the interaction of two firms choosing learning algorithms as\na game and analyze its equilibria. Absent competition, players care only about\nthe magnitude of predictive error and not its source. In contrast, our main\nresult is that with competition, players prefer to incur error due to variance\nrather than due to bias, even at the cost of higher total error. In addition,\nwe show that competition can have counterintuitive implications -- for example,\nreducing the error incurred by a firm's algorithm can be harmful to that firm\n-- but we provide conditions under which such phenomena do not occur. In\naddition to our theoretical analysis, we also validate our insights by applying\nour metrics to several publicly available datasets.",
    "published_date": "2019-09-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03618v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03567v1",
    "title": "What You See Is What You Get? The Impact of Representation Criteria on Human Bias in Hiring",
    "authors": [
      "Andi Peng",
      "Besmira Nushi",
      "Emre Kiciman",
      "Kori Inkpen",
      "Siddharth Suri",
      "Ece Kamar"
    ],
    "author_ids": [],
    "abstract": "Although systematic biases in decision-making are widely documented, the ways\nin which they emerge from different sources is less understood. We present a\ncontrolled experimental platform to study gender bias in hiring by decoupling\nthe effect of world distribution (the gender breakdown of candidates in a\nspecific profession) from bias in human decision-making. We explore the\neffectiveness of \\textit{representation criteria}, fixed proportional display\nof candidates, as an intervention strategy for mitigation of gender bias by\nconducting experiments measuring human decision-makers' rankings for who they\nwould recommend as potential hires. Experiments across professions with varying\ngender proportions show that balancing gender representation in candidate\nslates can correct biases for some professions where the world distribution is\nskewed, although doing so has no impact on other professions where human\npersistent preferences are at play. We show that the gender of the\ndecision-maker, complexity of the decision-making task and over- and\nunder-representation of genders in the candidate slate can all impact the final\ndecision. By decoupling sources of bias, we can better isolate strategies for\nbias mitigation in human-in-the-loop systems.",
    "published_date": "2019-09-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03567v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03529v1",
    "title": "Generating Reliable Friends via Adversarial Training to Improve Social Recommendation",
    "authors": [
      "Junliang Yu",
      "Min Gao",
      "Hongzhi Yin",
      "Jundong Li",
      "Chongming Gao",
      "Qinyong Wang"
    ],
    "author_ids": [],
    "abstract": "Most of the recent studies of social recommendation assume that people share\nsimilar preferences with their friends and the online social relations are\nhelpful in improving traditional recommender systems. However, this assumption\nis often untenable as the online social networks are quite sparse and a\nmajority of users only have a small number of friends. Besides, explicit\nfriends may not share similar interests because of the randomness in the\nprocess of building social networks. Therefore, discovering a number of\nreliable friends for each user plays an important role in advancing social\nrecommendation. Unlike other studies which focus on extracting valuable\nexplicit social links, our work pays attention to identifying reliable friends\nin both the observed and unobserved social networks. Concretely, in this paper,\nwe propose an end-to-end social recommendation framework based on Generative\nAdversarial Nets (GAN). The framework is composed of two blocks: a generator\nthat is used to produce friends that can possibly enhance the social\nrecommendation model, and a discriminator that is responsible for assessing\nthese generated friends and ranking the items according to both the current\nuser and her friends' preferences. With the competition between the generator\nand the discriminator, our framework can dynamically and adaptively generate\nreliable friends who can perfectly predict the current user' preference at a\nspecific time. As a result, the sparsity and unreliability problems of explicit\nsocial relations can be mitigated and the social recommendation performance is\nsignificantly improved. Experimental studies on real-world datasets demonstrate\nthe superiority of our framework and verify the positive effects of the\ngenerated reliable friends.",
    "published_date": "2019-09-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03529v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03434v1",
    "title": "Order-free Learning Alleviating Exposure Bias in Multi-label Classification",
    "authors": [
      "Che-Ping Tsai",
      "Hung-Yi Lee"
    ],
    "author_ids": [],
    "abstract": "Multi-label classification (MLC) assigns multiple labels to each sample.\nPrior studies show that MLC can be transformed to a sequence prediction problem\nwith a recurrent neural network (RNN) decoder to model the label dependency.\nHowever, training a RNN decoder requires a predefined order of labels, which is\nnot directly available in the MLC specification. Besides, RNN thus trained\ntends to overfit the label combinations in the training set and have difficulty\ngenerating unseen label sequences. In this paper, we propose a new framework\nfor MLC which does not rely on a predefined label order and thus alleviates\nexposure bias. The experimental results on three multi-label classification\nbenchmark datasets show that our method outperforms competitive baselines by a\nlarge margin. We also find the proposed approach has a higher probability of\ngenerating label combinations not seen during training than the baseline\nmodels. The result shows that the proposed approach has better generalization\ncapability.",
    "published_date": "2019-09-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SD",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03434v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.04028v1",
    "title": "Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses",
    "authors": [
      "Matt Grenander",
      "Yue Dong",
      "Jackie Chi Kit Cheung",
      "Annie Louis"
    ],
    "author_ids": [],
    "abstract": "Sentence position is a strong feature for news summarization, since the lead\noften (but not always) summarizes the key points of the article. In this paper,\nwe show that recent neural systems excessively exploit this trend, which\nalthough powerful for many inputs, is also detrimental when summarizing\ndocuments where important content should be extracted from later parts of the\narticle. We propose two techniques to make systems sensitive to the importance\nof content in different parts of the article. The first technique employs\n'unbiased' data; i.e., randomly shuffled sentences of the source document, to\npretrain the model. The second technique uses an auxiliary ROUGE-based loss\nthat encourages the model to distribute importance scores throughout a document\nby mimicking sentence-level ROUGE scores on the training data. We show that\nthese techniques significantly improve the performance of a competitive\nreinforcement learning based extractive system, with the auxiliary loss being\nmore powerful than pretraining.",
    "published_date": "2019-09-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.04028v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03360v2",
    "title": "Episode-based Prototype Generating Network for Zero-Shot Learning",
    "authors": [
      "Yunlong Yu",
      "Zhong Ji",
      "Zhongfei Zhang",
      "Jungong Han"
    ],
    "author_ids": [],
    "abstract": "We introduce a simple yet effective episode-based training framework for\nzero-shot learning (ZSL), where the learning system requires to recognize\nunseen classes given only the corresponding class semantics. During training,\nthe model is trained within a collection of episodes, each of which is designed\nto simulate a zero-shot classification task. Through training multiple\nepisodes, the model progressively accumulates ensemble experiences on\npredicting the mimetic unseen classes, which will generalize well on the real\nunseen classes. Based on this training framework, we propose a novel generative\nmodel that synthesizes visual prototypes conditioned on the class semantic\nprototypes. The proposed model aligns the visual-semantic interactions by\nformulating both the visual prototype generation and the class semantic\ninference into an adversarial framework paired with a parameter-economic\nMulti-modal Cross-Entropy Loss to capture the discriminative information.\nExtensive experiments on four datasets under both traditional ZSL and\ngeneralized ZSL tasks show that our model outperforms the state-of-the-art\napproaches by large margins.",
    "published_date": "2019-09-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03360v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03343v4",
    "title": "Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts",
    "authors": [
      "Jack Merullo",
      "Luke Yeh",
      "Abram Handler",
      "Alvin Grissom II",
      "Brendan O'Connor",
      "Mohit Iyyer"
    ],
    "author_ids": [],
    "abstract": "Sports broadcasters inject drama into play-by-play commentary by building\nteam and player narratives through subjective analyses and anecdotes. Prior\nstudies based on small datasets and manual coding show that such theatrics\nevince commentator bias in sports broadcasts. To examine this phenomenon, we\nassemble FOOTBALL, which contains 1,455 broadcast transcripts from American\nfootball games across six decades that are automatically annotated with 250K\nplayer mentions and linked with racial metadata. We identify major confounding\nfactors for researchers examining racial bias in FOOTBALL, and perform a\ncomputational analysis that supports conclusions from prior social science\nstudies.",
    "published_date": "2019-09-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03343v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03290v1",
    "title": "Pretrained AI Models: Performativity, Mobility, and Change",
    "authors": [
      "Lav R. Varshney",
      "Nitish Shirish Keskar",
      "Richard Socher"
    ],
    "author_ids": [],
    "abstract": "The paradigm of pretrained deep learning models has recently emerged in\nartificial intelligence practice, allowing deployment in numerous societal\nsettings with limited computational resources, but also embedding biases and\nenabling unintended negative uses. In this paper, we treat pretrained models as\nobjects of study and discuss the ethical impacts of their sociological\nposition. We discuss how pretrained models are developed and compared under the\ncommon task framework, but that this may make self-regulation inadequate.\nFurther how pretrained models may have a performative effect on society that\nexacerbates biases. We then discuss how pretrained models move through actor\nnetworks as a kind of computationally immutable mobile, but that users also act\nas agents of technological change by reinterpreting them via fine-tuning and\ntransfer. We further discuss how users may use pretrained models in malicious\nways, drawing a novel connection between the responsible innovation and\nuser-centered innovation literatures. We close by discussing how this\nsociological understanding of pretrained models can inform AI governance\nframeworks for fairness, accountability, and transparency.",
    "published_date": "2019-09-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03290v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03166v1",
    "title": "Equalizing Recourse across Groups",
    "authors": [
      "Vivek Gupta",
      "Pegah Nokhiz",
      "Chitradeep Dutta Roy",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "The rise in machine learning-assisted decision-making has led to concerns\nabout the fairness of the decisions and techniques to mitigate problems of\ndiscrimination. If a negative decision is made about an individual (denying a\nloan, rejecting an application for housing, and so on) justice dictates that we\nbe able to ask how we might change circumstances to get a favorable decision\nthe next time. Moreover, the ability to change circumstances (a better\neducation, improved credentials) should not be limited to only those with\naccess to expensive resources. In other words, \\emph{recourse} for negative\ndecisions should be considered a desirable value that can be equalized across\n(demographically defined) groups. This paper describes how to build models that\nmake accurate predictions while still ensuring that the penalties for a\nnegative outcome do not disadvantage different groups disproportionately. We\nmeasure recourse as the distance of an individual from the decision boundary of\na classifier. We then introduce a regularized objective to minimize the\ndifference in recourse across groups. We explore linear settings and further\nextend recourse to non-linear settings as well as model-agnostic settings where\nthe exact distance from boundary cannot be calculated. Our results show that we\ncan successfully decrease the unfairness in recourse while maintaining\nclassifier performance.",
    "published_date": "2019-09-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03166v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03082v1",
    "title": "One Size Does Not Fit All: Multi-Scale, Cascaded RNNs for Radar Classification",
    "authors": [
      "Dhrubojyoti Roy",
      "Sangeeta Srivastava",
      "Aditya Kusupati",
      "Pranshu Jain",
      "Manik Varma",
      "Anish Arora"
    ],
    "author_ids": [],
    "abstract": "Edge sensing with micro-power pulse-Doppler radars is an emergent domain in\nmonitoring and surveillance with several smart city applications. Existing\nsolutions for the clutter versus multi-source radar classification task are\nlimited in terms of either accuracy or efficiency, and in some cases, struggle\nwith a trade-off between false alarms and recall of sources. We find that this\nproblem can be resolved by learning the classifier across multiple time-scales.\nWe propose a multi-scale, cascaded recurrent neural network architecture,\nMSC-RNN, comprised of an efficient multi-instance learning (MIL) Recurrent\nNeural Network (RNN) for clutter discrimination at a lower tier, and a more\ncomplex RNN classifier for source classification at the upper tier. By\ncontrolling the invocation of the upper RNN with the help of the lower tier\nconditionally, MSC-RNN achieves an overall accuracy of 0.972. Our approach\nholistically improves the accuracy and per-class recalls over ML models\nsuitable for radar inferencing. Notably, we outperform cross-domain handcrafted\nfeature engineering with time-domain deep feature learning, while also being up\nto $\\sim$3$\\times$ more efficient than a competitive solution.",
    "published_date": "2019-09-06T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03082v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03037v1",
    "title": "Quantized Fisher Discriminant Analysis",
    "authors": [
      "Benyamin Ghojogh",
      "Ali Saheb Pasand",
      "Fakhri Karray",
      "Mark Crowley"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a new subspace learning method, named Quantized Fisher\nDiscriminant Analysis (QFDA), which makes use of both machine learning and\ninformation theory. There is a lack of literature for combination of machine\nlearning and information theory and this paper tries to tackle this gap. QFDA\nfinds a subspace which discriminates the uniformly quantized images in the\nDiscrete Cosine Transform (DCT) domain at least as well as discrimination of\nnon-quantized images by Fisher Discriminant Analysis (FDA) while the images\nhave been compressed. This helps the user to throw away the original images and\nkeep the compressed images instead without noticeable loss of classification\naccuracy. We propose a cost function whose minimization can be interpreted as\nrate-distortion optimization in information theory. We also propose quantized\nFisherfaces for facial analysis in QFDA. Our experiments on AT&T face dataset\nand Fashion MNIST dataset show the effectiveness of this subspace learning\nmethod.",
    "published_date": "2019-09-06T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "eess.IV",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03037v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.03013v1",
    "title": "Approaching Machine Learning Fairness through Adversarial Network",
    "authors": [
      "Xiaoqian Wang",
      "Heng Huang"
    ],
    "author_ids": [],
    "abstract": "Fairness is becoming a rising concern w.r.t. machine learning model\nperformance. Especially for sensitive fields such as criminal justice and loan\ndecision, eliminating the prediction discrimination towards a certain group of\npopulation (characterized by sensitive features like race and gender) is\nimportant for enhancing the trustworthiness of model. In this paper, we present\na new general framework to improve machine learning fairness. The goal of our\nmodel is to minimize the influence of sensitive feature from the perspectives\nof both the data input and the predictive model. In order to achieve this goal,\nwe reformulate the data input by removing the sensitive information and\nstrengthen model fairness by minimizing the marginal contribution of the\nsensitive feature. We propose to learn the non-sensitive input via sampling\namong features and design an adversarial network to minimize the dependence\nbetween the reformulated input and the sensitive information. Extensive\nexperiments on three benchmark datasets suggest that our model achieve better\nresults than related state-of-the-art methods with respect to both fairness\nmetrics and prediction performance.",
    "published_date": "2019-09-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.03013v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.02939v1",
    "title": "Optimizing Generalized Rate Metrics through Game Equilibrium",
    "authors": [
      "Harikrishna Narasimhan",
      "Andrew Cotter",
      "Maya Gupta"
    ],
    "author_ids": [],
    "abstract": "We present a general framework for solving a large class of learning problems\nwith non-linear functions of classification rates. This includes problems where\none wishes to optimize a non-decomposable performance metric such as the\nF-measure or G-mean, and constrained training problems where the classifier\nneeds to satisfy non-linear rate constraints such as predictive parity\nfairness, distribution divergences or churn ratios. We extend previous\ntwo-player game approaches for constrained optimization to a game between three\nplayers to decouple the classifier rates from the non-linear objective, and\nseek to find an equilibrium of the game. Our approach generalizes many existing\nalgorithms, and makes possible new algorithms with more flexibility and tighter\nhandling of non-linear rate constraints. We provide convergence guarantees for\nconvex functions of rates, and show how our methodology can be extended to\nhandle sums of ratios of rates. Experiments on different fairness tasks confirm\nthe efficacy of our approach.",
    "published_date": "2019-09-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02939v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.02827v2",
    "title": "Master your Metrics with Calibration",
    "authors": [
      "Wissam Siblini",
      "Jordan Fréry",
      "Liyun He-Guelton",
      "Frédéric Oblé",
      "Yi-Qing Wang"
    ],
    "author_ids": [],
    "abstract": "Machine learning models deployed in real-world applications are often\nevaluated with precision-based metrics such as F1-score or AUC-PR (Area Under\nthe Curve of Precision Recall). Heavily dependent on the class prior, such\nmetrics make it difficult to interpret the variation of a model's performance\nover different subpopulations/subperiods in a dataset. In this paper, we\npropose a way to calibrate the metrics so that they can be made invariant to\nthe prior. We conduct a large number of experiments on balanced and imbalanced\ndata to assess the behavior of calibrated metrics and show that they improve\ninterpretability and provide a better control over what is really measured. We\ndescribe specific real-world use-cases where calibration is beneficial such as,\nfor instance, model monitoring in production, reporting, or fairness\nevaluation.",
    "published_date": "2019-09-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02827v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.02800v2",
    "title": "CrowdHub: Extending crowdsourcing platforms for the controlled evaluation of tasks designs",
    "authors": [
      "Jorge Ramírez",
      "Simone Degiacomi",
      "Davide Zanella",
      "Marcos Baez",
      "Fabio Casati",
      "Boualem Benatallah"
    ],
    "author_ids": [],
    "abstract": "We present CrowdHub, a tool for running systematic evaluations of task\ndesigns on top of crowdsourcing platforms. The goal is to support the\nevaluation process, avoiding potential experimental biases that, according to\nour empirical studies, can amount to 38% loss in the utility of the collected\ndataset in uncontrolled settings. Using CrowdHub, researchers can map their\nexperimental design and automate the complex process of managing task execution\nover time while controlling for returning workers and crowd demographics, thus\nreducing bias, increasing utility of collected data, and making more efficient\nuse of a limited pool of subjects.",
    "published_date": "2019-09-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02800v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.02670v1",
    "title": "In Plain Sight: Media Bias Through the Lens of Factual Reporting",
    "authors": [
      "Lisa Fan",
      "Marshall White",
      "Eva Sharma",
      "Ruisi Su",
      "Prafulla Kumar Choubey",
      "Ruihong Huang",
      "Lu Wang"
    ],
    "author_ids": [],
    "abstract": "The increasing prevalence of political bias in news media calls for greater\npublic awareness of it, as well as robust methods for its detection. While\nprior work in NLP has primarily focused on the lexical bias captured by\nlinguistic attributes such as word choice and syntax, other types of bias stem\nfrom the actual content selected for inclusion in the text. In this work, we\ninvestigate the effects of informational bias: factual content that can\nnevertheless be deployed to sway reader opinion. We first produce a new\ndataset, BASIL, of 300 news articles annotated with 1,727 bias spans and find\nevidence that informational bias appears in news articles more frequently than\nlexical bias. We further study our annotations to observe how informational\nbias surfaces in news articles by different media outlets. Lastly, a baseline\nmodel for informational bias prediction is presented by fine-tuning BERT on our\nlabeled data, indicating the challenges of the task and future directions.",
    "published_date": "2019-09-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02670v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.02236v1",
    "title": "Effective Domain Knowledge Transfer with Soft Fine-tuning",
    "authors": [
      "Zhichen Zhao",
      "Bowen Zhang",
      "Yuning Jiang",
      "Li Xu",
      "Lei Li",
      "Wei-Ying Ma"
    ],
    "author_ids": [],
    "abstract": "Convolutional neural networks require numerous data for training. Considering\nthe difficulties in data collection and labeling in some specific tasks,\nexisting approaches generally use models pre-trained on a large source domain\n(e.g. ImageNet), and then fine-tune them on these tasks. However, the datasets\nfrom source domain are simply discarded in the fine-tuning process. We argue\nthat the source datasets could be better utilized and benefit fine-tuning. This\npaper firstly introduces the concept of general discrimination to describe\nability of a network to distinguish untrained patterns, and then experimentally\ndemonstrates that general discrimination could potentially enhance the total\ndiscrimination ability on target domain. Furthermore, we propose a novel and\nlight-weighted method, namely soft fine-tuning. Unlike traditional fine-tuning\nwhich directly replaces optimization objective by a loss function on the target\ndomain, soft fine-tuning effectively keeps general discrimination by holding\nthe previous loss and removes it softly. By doing so, soft fine-tuning improves\nthe robustness of the network to data bias, and meanwhile accelerates the\nconvergence. We evaluate our approach on several visual recognition tasks.\nExtensive experimental results support that soft fine-tuning provides\nconsistent improvement on all evaluated tasks, and outperforms the\nstate-of-the-art significantly. Codes will be made available to the public.",
    "published_date": "2019-09-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02236v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.02224v2",
    "title": "Examining Gender Bias in Languages with Grammatical Gender",
    "authors": [
      "Pei Zhou",
      "Weijia Shi",
      "Jieyu Zhao",
      "Kuan-Hao Huang",
      "Muhao Chen",
      "Ryan Cotterell",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "Recent studies have shown that word embeddings exhibit gender bias inherited\nfrom the training corpora. However, most studies to date have focused on\nquantifying and mitigating such bias only in English. These analyses cannot be\ndirectly extended to languages that exhibit morphological agreement on gender,\nsuch as Spanish and French. In this paper, we propose new metrics for\nevaluating gender bias in word embeddings of these languages and further\ndemonstrate evidence of gender bias in bilingual embeddings which align these\nlanguages with English. Finally, we extend an existing approach to mitigate\ngender bias in word embeddings under both monolingual and bilingual settings.\nExperiments on modified Word Embedding Association Test, word similarity, word\ntranslation, and word pair translation tasks show that the proposed approaches\neffectively reduce the gender bias while preserving the utility of the\nembeddings.",
    "published_date": "2019-09-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02224v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.02156v1",
    "title": "Bidding Strategies with Gender Nondiscrimination: Constraints for Online Ad Auctions",
    "authors": [
      "Milad Nasr",
      "Michael Tschantz"
    ],
    "author_ids": [],
    "abstract": "Interactions between bids to show ads online can lead to an advertiser's ad\nbeing shown to more men than women even when the advertiser does not target\ntowards men. We design bidding strategies that advertisers can use to avoid\nsuch emergent discrimination without having to modify the auction mechanism. We\nmathematically analyze the strategies to determine the additional cost to the\nadvertiser for avoiding discrimination, proving our strategies to be optimal in\nsome settings. We use simulations to understand other settings.",
    "published_date": "2019-09-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02156v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.02119v2",
    "title": "Inductive-bias-driven Reinforcement Learning For Efficient Schedules in Heterogeneous Clusters",
    "authors": [
      "Subho S Banerjee",
      "Saurabh Jha",
      "Zbigniew T. Kalbarczyk",
      "Ravishankar K. Iyer"
    ],
    "author_ids": [],
    "abstract": "The problem of scheduling of workloads onto heterogeneous processors (e.g.,\nCPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\nsystem schedulers rely on application/system-specific heuristics that have to\nbe built on a case-by-case basis. Recent work has demonstrated ML techniques\nfor automating the heuristic search by using black-box approaches which require\nsignificant training data and time, which make them challenging to use in\npractice. This paper presents Symphony, a scheduling framework that addresses\nthe challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n(RL) model for scheduling, which inherently models the resource dependencies\nidentified from the system architecture; and (ii) a sampling-based technique to\ncompute the gradients of a Bayesian model without performing full probabilistic\ninference. Together, these techniques reduce both the amount of training data\nand the time required to produce scheduling policies that significantly\noutperform black-box approaches by up to 2.2x.",
    "published_date": "2019-09-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.02119v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.01931v2",
    "title": "Efron-Stein PAC-Bayesian Inequalities",
    "authors": [
      "Ilja Kuzborskij",
      "Csaba Szepesvári"
    ],
    "author_ids": [],
    "abstract": "We prove semi-empirical concentration inequalities for random variables which\nare given as possibly nonlinear functions of independent random variables.\nThese inequalities describe concentration of random variable in terms of the\ndata/distribution-dependent Efron-Stein (ES) estimate of its variance and they\ndo not require any additional assumptions on the moments. In particular, this\nallows us to state semi-empirical Bernstein type inequalities for general\nfunctions of unbounded random variables, which gives user-friendly\nconcentration bounds for cases where related methods (e.g. bounded differences)\nmight be more challenging to apply. We extend these results to Efron-Stein\nPAC-Bayesian inequalities which hold for arbitrary probability kernels that\ndefine a random, data-dependent choice of the function of interest. Finally, we\ndemonstrate a number of applications, including PAC-Bayesian generalization\nbounds for unbounded loss functions, empirical Bernstein type generalization\nbounds, new truncation-free bounds for off-policy evaluation with Weighted\nImportance Sampling (WIS), and off-policy PAC-Bayesian learning with WIS.",
    "published_date": "2019-09-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01931v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.01826v2",
    "title": "Status in flux: Unequal alliances can create power vacuums",
    "authors": [
      "John Bryden",
      "Eric Silverman",
      "Simon T. Powers"
    ],
    "author_ids": [],
    "abstract": "Human groups show a variety of leadership structures from no leader, to\nchanging leaders, to a single long-term leader. When a leader is deposed, the\npresence of a power vacuum can mean they are often quickly replaced. We lack an\nexplanation of how such phenomena can emerge from simple rules of interaction\nbetween individuals. Here, we model transitions between different phases of\nleadership structure. We find a novel class of group dynamical behaviour where\nthere is a single leader who is quickly replaced when they lose status,\ndemonstrating a power vacuum. The model uses a dynamic network of individuals\nwho non-coercively form and break alliances with one-another, with a key\nparameter modelling inequality in these alliances. We argue the model can\nexplain transitions in leadership structure in the Neolithic Era from\nrelatively equal hunter-gatherer societies, to groups with chieftains which\nchange over time, to groups with an institutionalised leader on a paternal\nlineage. Our model demonstrates how these transitions can be explained by the\nimpact of technological developments such as food storage and/or weapons, which\nmeant that alliances became more unequal. In general terms, our approach\nprovides a quantitative understanding of how technology and social norms can\naffect leadership dynamics and structures.",
    "published_date": "2019-09-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01826v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.01635v2",
    "title": "The polarization process of ferroelectric materials analyzed in the framework of variational inequalities",
    "authors": [
      "Astrid S. Pechstein",
      "Martin Meindlhumer",
      "Alexander Humer"
    ],
    "author_ids": [],
    "abstract": "We are concerned with the mathematical modeling of the polarization process\nin ferroelectric media. We assume that this dissipative process is governed by\ntwo constitutive functions, which are the free energy function and the\ndissipation function. The dissipation function, which is closely connected to\nthe dissipated energy, is usually non-differentiable. Thus, a minimization\ncondition for the overall energy includes the subdifferential of the\ndissipation function. This condition can also be formulated by way of a\nvariational inequality in the unknown fields strain, dielectric displacement,\nremanent polarization and remanent strain. We analyze the mathematical\nwell-posedness of this problem. We provide an existence and uniqueness result\nfor the time-discrete update equation. Under stronger assumptions, we can prove\nexistence of a solution to the time-dependent variational inequality. To solve\nthe discretized variational inequality, we use mixed finite elements, where\nmechanical displacement and dielectric displacement are unknowns, as well as\npolarization (and, if included in the model, remanent strain). It is then\npossible to satisfy Gauss' law of zero free charges exactly. We propose to\nregularize the dissipation function and solve for all unknowns at once in a\nsingle Newton iteration. We present numerical examples gained in the open\nsource software package Netgen/NGSolve.",
    "published_date": "2019-09-04T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 74F15"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01635v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.01562v2",
    "title": "Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons",
    "authors": [
      "Jie Hao",
      "Xing Wang",
      "Shuming Shi",
      "Jinfeng Zhang",
      "Zhaopeng Tu"
    ],
    "author_ids": [],
    "abstract": "Recent studies have shown that a hybrid of self-attention networks (SANs) and\nrecurrent neural networks (RNNs) outperforms both individual architectures,\nwhile not much is known about why the hybrid models work. With the belief that\nmodeling hierarchical structure is an essential complementary between SANs and\nRNNs, we propose to further enhance the strength of hybrid models with an\nadvanced variant of RNNs - Ordered Neurons LSTM (ON-LSTM), which introduces a\nsyntax-oriented inductive bias to perform tree-like composition. Experimental\nresults on the benchmark machine translation task show that the proposed\napproach outperforms both individual architectures and a standard hybrid model.\nFurther analyses on targeted linguistic evaluation and logical inference tasks\ndemonstrate that the proposed approach indeed benefits from a better modeling\nof hierarchical structure.",
    "published_date": "2019-09-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01562v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.01495v1",
    "title": "Cross-Cutting Political Awareness through Diverse News Recommendations",
    "authors": [
      "Bibek Paudel",
      "Abraham Bernstein"
    ],
    "author_ids": [],
    "abstract": "The suggestions generated by most existing recommender systems are known to\nsuffer from a lack of diversity, and other issues like popularity bias. As a\nresult, they have been observed to promote well-known \"blockbuster\" items, and\nto present users with \"more of the same\" choices that entrench their existing\nbeliefs and biases. This limits users' exposure to diverse viewpoints and\npotentially increases political polarization. To promote the diversity of\nviews, we developed a novel computational framework that can identify the\npolitical leanings of users and the news items they share on online social\nnetworks. Based on such information, our system can recommend news items that\npurposefully expose users to different viewpoints and increase the diversity of\ntheir information \"diet.\" Our research on recommendation diversity and\npolitical polarization helps us to develop algorithms that measure each user's\nreaction %to diverse viewpoints and adjust the recommendation accordingly. The\nresult is an approach that exposes users to a variety of political views and\nwill, hopefully, broaden their acceptance (not necessarily the agreement) of\nvarious opinions.",
    "published_date": "2019-09-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01495v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.01326v2",
    "title": "The Woman Worked as a Babysitter: On Biases in Language Generation",
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Premkumar Natarajan",
      "Nanyun Peng"
    ],
    "author_ids": [],
    "abstract": "We present a systematic study of biases in natural language generation (NLG)\nby analyzing text generated from prompts that contain mentions of different\ndemographic groups. In this work, we introduce the notion of the regard towards\na demographic, use the varying levels of regard towards different demographics\nas a defining metric for bias in NLG, and analyze the extent to which sentiment\nscores are a relevant proxy metric for regard. To this end, we collect\nstrategically-generated text from language models and manually annotate the\ntext with both sentiment and regard scores. Additionally, we build an automatic\nregard classifier through transfer learning, so that we can analyze biases in\nunseen text. Together, these methods reveal the extent of the biased nature of\nlanguage model generations. Our analysis provides a study of biases in NLG,\nbias metrics and correlated human judgments, and empirical evidence on the\nusefulness of our annotated dataset.",
    "published_date": "2019-09-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01326v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.01251v1",
    "title": "Avoiding Resentment Via Monotonic Fairness",
    "authors": [
      "Guy W. Cole",
      "Sinead A. Williamson"
    ],
    "author_ids": [],
    "abstract": "Classifiers that achieve demographic balance by explicitly using protected\nattributes such as race or gender are often politically or culturally\ncontroversial due to their lack of individual fairness, i.e. individuals with\nsimilar qualifications will receive different outcomes. Individually and group\nfair decision criteria can produce counter-intuitive results, e.g. that the\noptimal constrained boundary may reject intuitively better candidates due to\ndemographic imbalance in similar candidates. Both approaches can be seen as\nintroducing individual resentment, where some individuals would have received a\nbetter outcome if they either belonged to a different demographic class and had\nthe same qualifications, or if they remained in the same class but had\nobjectively worse qualifications (e.g. lower test scores). We show that both\nforms of resentment can be avoided by using monotonically constrained machine\nlearning models to create individually fair, demographically balanced\nclassifiers.",
    "published_date": "2019-09-03T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01251v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00982v1",
    "title": "Quantifying Infra-Marginality and Its Trade-off with Group Fairness",
    "authors": [
      "Arpita Biswas",
      "Siddharth Barman",
      "Amit Deshpande",
      "Amit Sharma"
    ],
    "author_ids": [],
    "abstract": "In critical decision-making scenarios, optimizing accuracy can lead to a\nbiased classifier, hence past work recommends enforcing group-based fairness\nmetrics in addition to maximizing accuracy. However, doing so exposes the\nclassifier to another kind of bias called infra-marginality. This refers to\nindividual-level bias where some individuals/subgroups can be worse off than\nunder simply optimizing for accuracy. For instance, a classifier implementing\nrace-based parity may significantly disadvantage women of the advantaged race.\nTo quantify this bias, we propose a general notion of $\\eta$-infra-marginality\nthat can be used to evaluate the extent of this bias. We prove theoretically\nthat, unlike other fairness metrics, infra-marginality does not have a\ntrade-off with accuracy: high accuracy directly leads to low infra-marginality.\nThis observation is confirmed through empirical analysis on multiple simulated\nand real-world datasets. Further, we find that maximizing group fairness often\nincreases infra-marginality, suggesting the consideration of both group-level\nfairness and individual-level infra-marginality. However, measuring\ninfra-marginality requires knowledge of the true distribution of\nindividual-level outcomes correctly and explicitly. We propose a practical\nmethod to measure infra-marginality, and a simple algorithm to maximize\ngroup-wise accuracy and avoid infra-marginality.",
    "published_date": "2019-09-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00982v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00871v3",
    "title": "It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution",
    "authors": [
      "Rowan Hall Maudslay",
      "Hila Gonen",
      "Ryan Cotterell",
      "Simone Teufel"
    ],
    "author_ids": [],
    "abstract": "This paper treats gender bias latent in word embeddings. Previous mitigation\nattempts rely on the operationalisation of gender bias as a projection over a\nlinear subspace. An alternative approach is Counterfactual Data Augmentation\n(CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by\nswapping all inherently-gendered words in the copy. We perform an empirical\ncomparison of these approaches on the English Gigaword and Wikipedia, and find\nthat whilst both successfully reduce direct bias and perform well in tasks\nwhich quantify embedding quality, CDA variants outperform projection-based\nmethods at the task of drawing non-biased gender analogies by an average of 19%\nacross both corpora. We propose two improvements to CDA: Counterfactual Data\nSubstitution (CDS), a variant of CDA in which potentially biased text is\nrandomly substituted to avoid duplication, and the Names Intervention, a novel\nname-pairing technique that vastly increases the number of words being treated.\nCDA/S with the Names Intervention is the only approach which is able to\nmitigate indirect gender bias: following debiasing, previously biased words are\nsignificantly less clustered according to gender (cluster purity is reduced by\n49%), thus improving on the state-of-the-art for bias mitigation.",
    "published_date": "2019-09-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00871v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.01866v1",
    "title": "Understanding Bias in Machine Learning",
    "authors": [
      "Jindong Gu",
      "Daniela Oelke"
    ],
    "author_ids": [],
    "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.",
    "published_date": "2019-09-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01866v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00597v1",
    "title": "Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection",
    "authors": [
      "Seunghyeon Kim",
      "Jaehoon Choi",
      "Taekyung Kim",
      "Changick Kim"
    ],
    "author_ids": [],
    "abstract": "Deep learning-based object detectors have shown remarkable improvements.\nHowever, supervised learning-based methods perform poorly when the train data\nand the test data have different distributions. To address the issue, domain\nadaptation transfers knowledge from the label-sufficient domain (source domain)\nto the label-scarce domain (target domain). Self-training is one of the\npowerful ways to achieve domain adaptation since it helps class-wise domain\nadaptation. Unfortunately, a naive approach that utilizes pseudo-labels as\nground-truth degenerates the performance due to incorrect pseudo-labels. In\nthis paper, we introduce a weak self-training (WST) method and adversarial\nbackground score regularization (BSR) for domain adaptive one-stage object\ndetection. WST diminishes the adverse effects of inaccurate pseudo-labels to\nstabilize the learning procedure. BSR helps the network extract discriminative\nfeatures for target backgrounds to reduce the domain shift. Two components are\ncomplementary to each other as BSR enhances discrimination between foregrounds\nand backgrounds, whereas WST strengthen class-wise discrimination. Experimental\nresults show that our approach effectively improves the performance of the\none-stage object detection in unsupervised domain adaptation setting.",
    "published_date": "2019-09-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00597v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00556v1",
    "title": "Phrase-Level Class based Language Model for Mandarin Smart Speaker Query Recognition",
    "authors": [
      "Yiheng Huang",
      "Liqiang He",
      "Lei Han",
      "Guangsen Wang",
      "Dan Su"
    ],
    "author_ids": [],
    "abstract": "The success of speech assistants requires precise recognition of a number of\nentities on particular contexts. A common solution is to train a class-based\nn-gram language model and then expand the classes into specific words or\nphrases. However, when the class has a huge list, e.g., more than 20 million\nsongs, a fully expansion will cause memory explosion. Worse still, the list\nitems in the class need to be updated frequently, which requires a dynamic\nmodel updating technique. In this work, we propose to train pruned language\nmodels for the word classes to replace the slots in the root n-gram. We further\npropose to use a novel technique, named Difference Language Model (DLM), to\ncorrect the bias from the pruned language models. Once the decoding graph is\nbuilt, we only need to recalculate the DLM when the entities in word classes\nare updated. Results show that the proposed method consistently and\nsignificantly outperforms the conventional approaches on all datasets, esp. for\nlarge lists, which the conventional approaches cannot handle.",
    "published_date": "2019-09-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00556v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00554v1",
    "title": "Analysis of Bias in Gathering Information Between User Attributes in News Application",
    "authors": [
      "Yoshifumi Seki",
      "Mitsuo Yoshida"
    ],
    "author_ids": [],
    "abstract": "In the process of information gathering on the web, confirmation bias is\nknown to exist, exemplified in phenomena such as echo chambers and filter\nbubbles. Our purpose is to reveal how people consume news and discuss these\nphenomena. In web services, we are able to use action logs of a service to\ninvestigate these phenomena. However, many existing studies about these\nphenomena are conducted via questionnaires, and there are few studies using\naction logs. In this paper, we attempt to discover biases of information\ngathering due to differences in user demographic attributes, such as age and\ngender, from the behavior log of the news distribution service. First, we\nsummarized the actions in the service for each user attribute and showed the\ndifference of user behavior depending on the attributes. Next, the degree of\ncorrelation between the attributes was measured using the correlation\ncoefficient, and a strong correlation was found to exist in the browsing\ntendency of the news articles between the attributes. Then, the bias of\nkeywords between attributes was discovered, keywords with bias in behavior\namong the attributes were found using parameters of regression analysis. Since\nthese discovered keywords are almost explainable by big news, our proposed\nmethod is effective in detecting biased keywords.",
    "published_date": "2019-09-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00554v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.00384v2",
    "title": "DeepHealth: Review and challenges of artificial intelligence in health informatics",
    "authors": [
      "Gloria Hyunjung Kwak",
      "Pan Hui"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence has provided us with an exploration of a whole new\nresearch era. As more data and better computational power become available, the\napproach is being implemented in various fields. The demand for it in health\ninformatics is also increasing, and we can expect to see the potential benefits\nof its applications in healthcare. It can help clinicians diagnose disease,\nidentify drug effects for each patient, understand the relationship between\ngenotypes and phenotypes, explore new phenotypes or treatment recommendations,\nand predict infectious disease outbreaks with high accuracy. In contrast to\ntraditional models, recent artificial intelligence approaches do not require\ndomain-specific data pre-processing, and it is expected that it will ultimately\nchange life in the future. Despite its notable advantages, there are some key\nchallenges on data (high dimensionality, heterogeneity, time dependency,\nsparsity, irregularity, lack of label, bias) and model (reliability,\ninterpretability, feasibility, security, scalability) for practical use. This\narticle presents a comprehensive review of research applying artificial\nintelligence in health informatics, focusing on the last seven years in the\nfields of medical imaging, electronic health records, genomics, sensing, and\nonline communication health, as well as challenges and promising directions for\nfuture research. We highlight ongoing popular approaches' research and identify\nseveral challenges in building models.",
    "published_date": "2019-09-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00384v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00250v2",
    "title": "A Note on New Bernstein-type Inequalities for the Log-likelihood Function of Bernoulli Variables",
    "authors": [
      "Yunpeng Zhao"
    ],
    "author_ids": [],
    "abstract": "We prove a new Bernstein-type inequality for the log-likelihood function of\nBernoulli variables. In contrast to classical Bernstein's inequality and\nHoeffding's inequality when applied to the log-likelihood, the new bound is\nindependent of the parameters of the Bernoulli variables and therefore does not\nblow up as the parameters approach 0 or 1. The new inequality strengthens\ncertain theoretical results on likelihood-based methods for community detection\nin networks and can be applied to other likelihood-based methods for binary\ndata.",
    "published_date": "2019-08-31T00:00:00",
    "year": 2019,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.TH",
      "60E15"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00250v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1909.00179v1",
    "title": "Boundary-Aware Feature Propagation for Scene Segmentation",
    "authors": [
      "Henghui Ding",
      "Xudong Jiang",
      "Ai Qun Liu",
      "Nadia Magnenat Thalmann",
      "Gang Wang"
    ],
    "author_ids": [],
    "abstract": "In this work, we address the challenging issue of scene segmentation. To\nincrease the feature similarity of the same object while keeping the feature\ndiscrimination of different objects, we explore to propagate information\nthroughout the image under the control of objects' boundaries. To this end, we\nfirst propose to learn the boundary as an additional semantic class to enable\nthe network to be aware of the boundary layout. Then, we propose unidirectional\nacyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs),\nwhich structurize the image via building graphic pixel-by-pixel connections, in\nan efficient and effective way. Furthermore, we propose a boundary-aware\nfeature propagation (BFP) module to harvest and propagate the local features\nwithin their regions isolated by the learned boundaries in the UAG-structured\nimage. The proposed BFP is capable of splitting the feature propagation into a\nset of semantic groups via building strong connections among the same segment\nregion but weak connections between different segment regions. Without bells\nand whistles, our approach achieves new state-of-the-art segmentation\nperformance on three challenging semantic segmentation datasets, i.e.,\nPASCAL-Context, CamVid, and Cityscapes.",
    "published_date": "2019-08-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00179v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.00066v3",
    "title": "Counterfactual Risk Assessments, Evaluation, and Fairness",
    "authors": [
      "Amanda Coston",
      "Alan Mishler",
      "Edward H. Kennedy",
      "Alexandra Chouldechova"
    ],
    "author_ids": [],
    "abstract": "Algorithmic risk assessments are increasingly used to help humans make\ndecisions in high-stakes settings, such as medicine, criminal justice and\neducation. In each of these cases, the purpose of the risk assessment tool is\nto inform actions, such as medical treatments or release conditions, often with\nthe aim of reducing the likelihood of an adverse event such as hospital\nreadmission or recidivism. Problematically, most tools are trained and\nevaluated on historical data in which the outcomes observed depend on the\nhistorical decision-making policy. These tools thus reflect risk under the\nhistorical policy, rather than under the different decision options that the\ntool is intended to inform. Even when tools are constructed to predict risk\nunder a specific decision, they are often improperly evaluated as predictors of\nthe target outcome.\n  Focusing on the evaluation task, in this paper we define counterfactual\nanalogues of common predictive performance and algorithmic fairness metrics\nthat we argue are better suited for the decision-making context. We introduce a\nnew method for estimating the proposed metrics using doubly robust estimation.\nWe provide theoretical results that show that only under strong conditions can\nfairness according to the standard metric and the counterfactual metric\nsimultaneously hold. Consequently, fairness-promoting methods that target\nparity in a standard fairness metric may --- and as we show empirically, do ---\ninduce greater imbalance in the counterfactual analogue. We provide empirical\ncomparisons on both synthetic data and a real world child welfare dataset to\ndemonstrate how the proposed method improves upon standard practice.",
    "published_date": "2019-08-30T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.00066v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.11723v1",
    "title": "Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization",
    "authors": [
      "Taehee Jung",
      "Dongyeop Kang",
      "Lucas Mentch",
      "Eduard Hovy"
    ],
    "author_ids": [],
    "abstract": "Despite the recent developments on neural summarization systems, the\nunderlying logic behind the improvements from the systems and its\ncorpus-dependency remains largely unexplored. Position of sentences in the\noriginal text, for example, is a well known bias for news summarization.\nFollowing in the spirit of the claim that summarization is a combination of\nsub-functions, we define three sub-aspects of summarization: position,\nimportance, and diversity and conduct an extensive analysis of the biases of\neach sub-aspect with respect to the domain of nine different summarization\ncorpora (e.g., news, academic papers, meeting minutes, movie script, books,\nposts). We find that while position exhibits substantial bias in news articles,\nthis is not the case, for example, with academic papers and meeting minutes.\nFurthermore, our empirical study shows that different types of summarization\nsystems (e.g., neural-based) are composed of different degrees of the\nsub-aspects. Our study provides useful lessons regarding consideration of\nunderlying sub-aspects when collecting a new summarization dataset or\ndeveloping a new system.",
    "published_date": "2019-08-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.11723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.11474v2",
    "title": "Detecting and Reducing Bias in a High Stakes Domain",
    "authors": [
      "Ruiqi Zhong",
      "Yanda Chen",
      "Desmond Patton",
      "Charlotte Selous",
      "Kathy McKeown"
    ],
    "author_ids": [],
    "abstract": "Gang-involved youth in cities such as Chicago sometimes post on social media\nto express their aggression towards rival gangs and previous research has\ndemonstrated that a deep learning approach can predict aggression and loss in\nposts. To address the possibility of bias in this sensitive application, we\ndeveloped an approach to systematically interpret the state of the art model.\nWe found, surprisingly, that it frequently bases its predictions on stop words\nsuch as \"a\" or \"on\", an approach that could harm social media users who have no\naggressive intentions. To tackle this bias, domain experts annotated the\nrationales, highlighting words that explain why a tweet is labeled as\n\"aggression\". These new annotations enable us to quantitatively measure how\njustified the model predictions are, and build models that drastically reduce\nbias. Our study shows that in high stake scenarios, accuracy alone cannot\nguarantee a good system and we need new evaluation methods.",
    "published_date": "2019-08-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.11474v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.11415v2",
    "title": "Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training",
    "authors": [
      "Zelun Wang",
      "Jyh-Charn Liu"
    ],
    "author_ids": [],
    "abstract": "In this paper we propose a deep neural network model with an encoder-decoder\narchitecture that translates images of math formulas into their LaTeX markup\nsequences. The encoder is a convolutional neural network (CNN) that transforms\nimages into a group of feature maps. To better capture the spatial\nrelationships of math symbols, the feature maps are augmented with 2D\npositional encoding before being unfolded into a vector. The decoder is a\nstacked bidirectional long short-term memory (LSTM) model integrated with the\nsoft attention mechanism, which works as a language model to translate the\nencoder output into a sequence of LaTeX tokens. The neural network is trained\nin two steps. The first step is token-level training using the\nMaximum-Likelihood Estimation (MLE) as the objective function. At completion of\nthe token-level training, the sequence-level training objective function is\nemployed to optimize the overall model based on the policy gradient algorithm\nfrom reinforcement learning. Our design also overcomes the exposure bias\nproblem by closing the feedback loop in the decoder during sequence-level\ntraining, i.e., feeding in the predicted token instead of the ground truth\ntoken at every time step. The model is trained and evaluated on the\nIM2LATEX-100K dataset and shows state-of-the-art performance on both\nsequence-based and image-based evaluation metrics.",
    "published_date": "2019-08-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.11415v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.11030v1",
    "title": "Towards Ethical Content-Based Detection of Online Influence Campaigns",
    "authors": [
      "Evan Crothers",
      "Nathalie Japkowicz",
      "Herna Viktor"
    ],
    "author_ids": [],
    "abstract": "The detection of clandestine efforts to influence users in online communities\nis a challenging problem with significant active development. We demonstrate\nthat features derived from the text of user comments are useful for identifying\nsuspect activity, but lead to increased erroneous identifications when keywords\nover-represented in past influence campaigns are present. Drawing on research\nin native language identification (NLI), we use \"named entity masking\" (NEM) to\ncreate sentence features robust to this shortcoming, while maintaining\ncomparable classification accuracy. We demonstrate that while NEM consistently\nreduces false positives when key named entities are mentioned, both masked and\nunmasked models exhibit increased false positive rates on English sentences by\nRussian native speakers, raising ethical considerations that should be\naddressed in future research.",
    "published_date": "2019-08-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.11030v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.11018v1",
    "title": "A cross-sectional study of social inequities in medical crowdfunding campaigns in the United States",
    "authors": [
      "Nora Kenworthy",
      "Zhihang Dong",
      "Anne Montgomery",
      "Emily Fuller",
      "Lauren Berliner"
    ],
    "author_ids": [],
    "abstract": "Americans are increasingly relying on crowdfunding to pay for the costs of\nhealthcare. In medical crowdfunding, online platforms allow individuals to\nappeal to social networks to request donations for health and medical needs.\nUsers are often told that success depends on how they organize and share their\ncampaigns to increase social network engagement. However, experts have\ncautioned that MCF could exacerbate health and social disparities by amplifying\nthe choices and biases of the crowd and leveraging these to determine who has\naccess to financial support for healthcare. To date, research on potential axes\nof disparity in MCF, and their impacts on fundraising outcomes, has been\nlimited. This paper presents an exploratory cross-sectional study of a\nrandomized sample of 637 MCF campaigns on the popular platform Gofundme, for\nwhich the race, gender, age, and relationships of campaigners and campaign\nrecipients were categorized alongside campaign characteristics and outcomes.\nOur analyses examine race, gender, and age disparities in MCF use, and tests\nhow these are associated with differential campaign outcomes. The results show\nsystemic disparities in MCF use and outcomes: non-white users are\nunder-represented. There is significant evidence of an additional digital care\nlabor burden on women organizers of campaigns, and marginalized race and gender\ngroups are associated with poorer fundraising outcomes. Outcomes are only\nminimally associated with campaign characteristics under users' control, such\nas photos, videos, and updates. These results corroborate widespread concerns\nhow technology fuels health inequities, and about how crowdfunding may be\ncreating an unequal and biased marketplace for those seeking financial support\nto access healthcare. Further research and better data access are needed to\nexplore these dynamics more deeply and inform policy for this largely\nunregulated industry.",
    "published_date": "2019-08-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.11018v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.11451v1",
    "title": "Fairness-Aware Process Mining",
    "authors": [
      "Mahnaz Sadat Qafari",
      "Wil van der Aalst"
    ],
    "author_ids": [],
    "abstract": "Process mining is a multi-purpose tool enabling organizations to improve\ntheir processes. One of the primary purposes of process mining is finding the\nroot causes of performance or compliance problems in processes. The usual way\nof doing so is by gathering data from the process event log and other sources\nand then applying some data mining and machine learning techniques. However,\nthe results of applying such techniques are not always acceptable. In many\nsituations, this approach is prone to making obvious or unfair diagnoses and\napplying them may result in conclusions that are unsurprising or even\ndiscriminating (e.g., blaming overloaded employees for delays). In this paper,\nwe present a solution to this problem by creating a fair classifier for such\nsituations. The undesired effects are removed at the expense of reduction on\nthe accuracy of the resulting classifier. We have implemented this method as a\nplug-in in ProM. Using the implemented plug-in on two real event logs, we\ndecreased the discrimination caused by the classifier, while losing a small\nfraction of its accuracy.",
    "published_date": "2019-08-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.11451v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.10763v2",
    "title": "Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual",
    "authors": [
      "He He",
      "Sheng Zha",
      "Haohan Wang"
    ],
    "author_ids": [],
    "abstract": "Statistical natural language inference (NLI) models are susceptible to\nlearning dataset bias: superficial cues that happen to associate with the label\non a particular dataset, but are not useful in general, e.g., negation words\nindicate contradiction. As exposed by several recent challenge datasets, these\nmodels perform poorly when such association is absent, e.g., predicting that \"I\nlove dogs\" contradicts \"I don't love cats\". Our goal is to design learning\nalgorithms that guard against known dataset bias. We formalize the concept of\ndataset bias under the framework of distribution shift and present a simple\ndebiasing algorithm based on residual fitting, which we call DRiFt. We first\nlearn a biased model that only uses features that are known to relate to\ndataset bias. Then, we train a debiased model that fits to the residual of the\nbiased model, focusing on examples that cannot be predicted well by biased\nfeatures only. We use DRiFt to train three high-performing NLI models on two\nbenchmark datasets, SNLI and MNLI. Our debiased models achieve significant\ngains over baseline models on two challenge test sets, while maintaining\nreasonable performance on the original test sets.",
    "published_date": "2019-08-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.10763v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.10703v1",
    "title": "Emotion Detection with Neural Personal Discrimination",
    "authors": [
      "Xiabing Zhou",
      "Zhongqing Wang",
      "Shoushan Li",
      "Guodong Zhou",
      "Min Zhang"
    ],
    "author_ids": [],
    "abstract": "There have been a recent line of works to automatically predict the emotions\nof posts in social media. Existing approaches consider the posts individually\nand predict their emotions independently. Different from previous researches,\nwe explore the dependence among relevant posts via the authors' backgrounds,\nsince the authors with similar backgrounds, e.g., gender, location, tend to\nexpress similar emotions. However, such personal attributes are not easy to\nobtain in most social media websites, and it is hard to capture\nattributes-aware words to connect similar people. Accordingly, we propose a\nNeural Personal Discrimination (NPD) approach to address above challenges by\ndetermining personal attributes from posts, and connecting relevant posts with\nsimilar attributes to jointly learn their emotions. In particular, we employ\nadversarial discriminators to determine the personal attributes, with attention\nmechanisms to aggregate attributes-aware words. In this way, social\ncorrelationship among different posts can be better addressed. Experimental\nresults show the usefulness of personal attributes, and the effectiveness of\nour proposed NPD approach in capturing such personal attributes with\nsignificant gains over the state-of-the-art models.",
    "published_date": "2019-08-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.10703v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.10414v2",
    "title": "Artificial Intelligence Fairness in the Context of Accessibility Research on Intelligent Systems for People who are Deaf or Hard of Hearing",
    "authors": [
      "Sushant Kafle",
      "Abraham Glasser",
      "Sedeeq Al-khazraji",
      "Larwan Berke",
      "Matthew Seita",
      "Matt Huenerfauth"
    ],
    "author_ids": [],
    "abstract": "We discuss issues of Artificial Intelligence (AI) fairness for people with\ndisabilities, with examples drawn from our research on human-computer\ninteraction (HCI) for AI-based systems for people who are Deaf or Hard of\nHearing (DHH). In particular, we discuss the need for inclusion of data from\npeople with disabilities in training sets, the lack of interpretability of AI\nsystems, ethical responsibilities of access technology researchers and\ncompanies, the need for appropriate evaluation metrics for AI-based access\ntechnologies (to determine if they are ready to be deployed and if they can be\ntrusted by users), and the ways in which AI systems influence human behavior\nand influence the set of abilities needed by users to successfully interact\nwith computing systems.",
    "published_date": "2019-08-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.10414v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09891v1",
    "title": "A Weakly Supervised Method for Instance Segmentation of Biological Cells",
    "authors": [
      "Fidel A. Guerrero-Peña",
      "Pedro D. Marrero Fernandez",
      "Tsang Ing Ren",
      "Alexandre Cunha"
    ],
    "author_ids": [],
    "abstract": "We present a weakly supervised deep learning method to perform instance\nsegmentation of cells present in microscopy images. Annotation of biomedical\nimages in the lab can be scarce, incomplete, and inaccurate. This is of concern\nwhen supervised learning is used for image analysis as the discriminative power\nof a learning model might be compromised in these situations. To overcome the\ncurse of poor labeling, our method focuses on three aspects to improve\nlearning: i) we propose a loss function operating in three classes to\nfacilitate separating adjacent cells and to drive the optimizer to properly\nclassify underrepresented regions; ii) a contour-aware weight map model is\nintroduced to strengthen contour detection while improving the network\ngeneralization capacity; and iii) we augment data by carefully modulating local\nintensities on edges shared by adjoining regions and to account for possibly\nweak signals on these edges. Generated probability maps are segmented using\ndifferent methods, with the watershed based one generally offering the best\nsolutions, specially in those regions where the prevalence of a single class is\nnot clear. The combination of these contributions allows segmenting individual\ncells on challenging images. We demonstrate our methods in sparse and crowded\ncell images, showing improvements in the learning process for a fixed network\narchitecture.",
    "published_date": "2019-08-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09891v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09812v4",
    "title": "Impact of Confirmation Bias on Competitive Information Spread in Social Networks",
    "authors": [
      "Yanbing Mao",
      "Emrah Akyol",
      "Naira Hovakimyan"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the impact of confirmation bias on competitive\ninformation spread in the cyber-social network that comprises individuals in a\nsocial network and competitive information sources in cyber layer. We formulate\nthe problem as a zero-sum game, which admits a unique Nash equilibrium in pure\nstrategies. We characterize the dependence of pure Nash equilibrium on the\npublic's innate opinions, the social network topology, as well as the\nparameters of confirmation bias. We uncover that confirmation bias moves the\nequilibrium towards the center only when the innate opinions are not neutral,\nand this move does not occur for the competitive information sources\nsimultaneously. Numerical examples in the context of well-known Krackhardt's\nadvice network are provided to demonstrate the correctness of theoretical\nresults.",
    "published_date": "2019-08-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09812v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.09745v1",
    "title": "A Semantics-Guided Class Imbalance Learning Model for Zero-Shot Classification",
    "authors": [
      "Zhong Ji",
      "Xuejie Yu",
      "Yunlong Yu",
      "Yanwei Pang",
      "Zhongfei Zhang"
    ],
    "author_ids": [],
    "abstract": "Zero-Shot Classification (ZSC) equips the learned model with the ability to\nrecognize the visual instances from the novel classes via constructing the\ninteractions between the visual and the semantic modalities. In contrast to the\ntraditional image classification, ZSC is easily suffered from the\nclass-imbalance issue since it is more concerned with the class-level knowledge\ntransfer capability. In the real world, the class samples follow a long-tailed\ndistribution, and the discriminative information in the sample-scarce seen\nclasses is hard to be transferred to the related unseen classes in the\ntraditional batch-based training manner, which degrades the overall\ngeneralization ability a lot. Towards alleviating the class imbalance issue in\nZSC, we propose a sample-balanced training process to encourage all training\nclasses to contribute equally to the learned model. Specifically, we randomly\nselect the same number of images from each class across all training classes to\nform a training batch to ensure that the sample-scarce classes contribute\nequally as those classes with sufficient samples during each iteration.\nConsidering that the instances from the same class differ in class\nrepresentativeness, we further develop an efficient semantics-guided feature\nfusion model to obtain discriminative class visual prototype for the following\nvisual-semantic interaction process via distributing different weights to the\nselected samples based on their class representativeness. Extensive experiments\non three imbalanced ZSC benchmark datasets for both the Traditional ZSC (TZSC)\nand the Generalized ZSC (GZSC) tasks demonstrate our approach achieves\npromising results especially for the unseen categories those are closely\nrelated to the sample-scarce seen categories.",
    "published_date": "2019-08-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09745v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09564v2",
    "title": "Towards Blockchain-enabled Searchable Encryption",
    "authors": [
      "Qiang Tang"
    ],
    "author_ids": [],
    "abstract": "Distributed Leger Technologies (DLTs), most notably Blockchain technologies,\nbring decentralised platforms that eliminate a single trusted third party and\navoid the notorious single point of failure vulnerability. Since Nakamoto's\nBitcoin cryptocurrency system, an enormous number of decentralised applications\nhave been proposed on top of these technologies, aiming at more transparency\nand trustworthiness than their traditional counterparts. These applications\nspread over a lot of areas, e.g. financial services, healthcare,\ntransportation, supply chain management, and cloud computing. While Blockchain\nbrings transparency and decentralised trust intuitively due to the consensus of\na (very large) group of nodes (or, miners), it introduces very subtle\nimplications for other desirable properties such as privacy. In this work, we\ndemonstrate these subtle implications for Blockchain-based searchable\nencryption solutions, which are one specific use case of cloud computing\nservices. These solutions rely on Blockchain to achieve both the standard\nprivacy property and the new fairness property, which requires that search\noperations are carried out faithfully and are rewarded accordingly. We show\nthat directly replacing the server in an existing searchable encryption\nsolution with a Blockchain will cause undesirable operational cost, privacy\nloss, and security vulnerabilities. The analysis results indicate that a\ndedicated server is still needed to achieve the desired privacy guarantee. To\nthis end, we propose two frameworks which can be instantiated based on most\nexisting searchable encryption schemes. Through analysing these two frameworks,\nwe affirmatively show that a carefully engineered Blockchain-based solution can\nachieve the desired fairness property while preserving the privacy guarantee of\nthe original searchable encryption scheme simultaneously.",
    "published_date": "2019-08-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09564v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.09472v2",
    "title": "On Inference of Network Topology and Confirmation Bias in Cyber-Social Networks",
    "authors": [
      "Yanbing Mao",
      "Emrah Akyol"
    ],
    "author_ids": [],
    "abstract": "This paper studies topology inference, from agent states, of a directed\ncyber-social network with opinion spreading dynamics model that explicitly\ntakes confirmation bias into account. The cyber-social network comprises a set\nof partially connected directed network of agents at the social level, and a\nset of information sources at the cyber layer. The necessary and sufficient\nconditions for the existence of exact inference solution are characterized. A\nmethod for exact inference, when it is possible, of entire network topology as\nwell as confirmation bias model parameters is proposed for the case where the\nbias mentioned earlier follows a piece-wise linear model. The particular case\nof no confirmation bias is analyzed in detail. For the setting where the model\nof confirmation bias is unknown, an algorithm that approximates the network\ntopology, building on the exact inference method, is presented. This algorithm\ncan exactly infer the weighted communication from the neighbors to the\nnon-followers of information sources. Numerical simulations demonstrate the\neffectiveness of the proposed methods for different scenarios.",
    "published_date": "2019-08-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09472v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.09369v3",
    "title": "On Measuring and Mitigating Biased Inferences of Word Embeddings",
    "authors": [
      "Sunipa Dev",
      "Tao Li",
      "Jeff Phillips",
      "Vivek Srikumar"
    ],
    "author_ids": [],
    "abstract": "Word embeddings carry stereotypical connotations from the text they are\ntrained on, which can lead to invalid inferences in downstream models that rely\non them. We use this observation to design a mechanism for measuring\nstereotypes using the task of natural language inference. We demonstrate a\nreduction in invalid inferences via bias mitigation strategies on static word\nembeddings (GloVe). Further, we show that for gender bias, these techniques\nextend to contextualized embeddings when applied selectively only to the static\ncomponents of contextualized embeddings (ELMo, BERT).",
    "published_date": "2019-08-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09369v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09323v3",
    "title": "Characterizing Safety: Minimal Barrier Functions from Scalar Comparison Systems",
    "authors": [
      "Rohit Konda",
      "Aaron D. Ames",
      "Samuel Coogan"
    ],
    "author_ids": [],
    "abstract": "Verifying set invariance has classical solutions stemming from the seminal\nwork by Nagumo, and defining sets via a smooth barrier function constraint\ninequality results in computable flow conditions for guaranteeing set\ninvariance. While a majority of these historic results on set invariance\nconsider flow conditions on the boundary, recent results on control barrier\nfunctions extended these conditions to the entire set, although they required\nregularity conditions on the barrier function. This paper fully characterizes\nset invariance through \\emph{minimal barrier functions} by directly appealing\nto a comparison result to define a flow condition over the entire domain of the\nsystem. A considerable benefit of this approach is the removal of regularity\nassumptions of the barrier function. This paper also outlines necessary and\nsufficient conditions for a valid differential inequality condition, giving the\nminimum conditions for this type of approach. We also show when minimal barrier\nfunctions are necessary and sufficient for set invariance.",
    "published_date": "2019-08-25T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09323v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.10209v2",
    "title": "Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes",
    "authors": [
      "Sameera Ramasinghe",
      "Salman Khan",
      "Nick Barnes",
      "Stephen Gould"
    ],
    "author_ids": [],
    "abstract": "Existing networks directly learn feature representations on 3D point clouds\nfor shape analysis. We argue that 3D point clouds are highly redundant and hold\nirregular (permutation-invariant) structure, which makes it difficult to\nachieve inter-class discrimination efficiently. In this paper, we propose a\ntwo-faceted solution to this problem that is seamlessly integrated in a single\n`Blended Convolution and Synthesis' layer. This fully differentiable layer\nperforms two critical tasks in succession. In the first step, it projects the\ninput 3D point clouds into a latent 3D space to synthesize a highly compact and\nmore inter-class discriminative point cloud representation. Since, 3D point\nclouds do not follow a Euclidean topology, standard 2/3D Convolutional Neural\nNetworks offer limited representation capability. Therefore, in the second\nstep, it uses a novel 3D convolution operator functioning inside the unit ball\n($\\mathbb{B}^3$) to extract useful volumetric features. We extensively derive\nformulae to achieve both translation and rotation of our novel convolution\nkernels. Finally, using the proposed techniques we present an extremely\nlight-weight, end-to-end architecture that achieves compelling results on 3D\nshape recognition and retrieval.",
    "published_date": "2019-08-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.10209v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09092v2",
    "title": "Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data",
    "authors": [
      "Dylan Slack",
      "Sorelle Friedler",
      "Emile Givental"
    ],
    "author_ids": [],
    "abstract": "Motivated by concerns surrounding the fairness effects of sharing and\ntransferring fair machine learning tools, we propose two algorithms: Fairness\nWarnings and Fair-MAML. The first is a model-agnostic algorithm that provides\ninterpretable boundary conditions for when a fairly trained model may not\nbehave fairly on similar but slightly different tasks within a given domain.\nThe second is a fair meta-learning approach to train models that can be quickly\nfine-tuned to specific tasks from only a few number of sample instances while\nbalancing fairness and accuracy. We demonstrate experimentally the individual\nutility of each model using relevant baselines and provide the first experiment\nto our knowledge of K-shot fairness, i.e. training a fair model on a new task\nwith only K data points. Then, we illustrate the usefulness of both algorithms\nas a combined method for training models from a few data points on new tasks\nwhile using Fairness Warnings as interpretable boundary conditions under which\nthe newly trained model may not be fair.",
    "published_date": "2019-08-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09092v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09072v1",
    "title": "Camera Pose Correction in SLAM Based on Bias Values of Map Points",
    "authors": [
      "Zhaobing Kang",
      "Wei Zou",
      "Zheng Zhu"
    ],
    "author_ids": [],
    "abstract": "Accurate camera pose estimation result is essential for visual SLAM (VSLAM).\nThis paper presents a novel pose correction method to improve the accuracy of\nthe VSLAM system. Firstly, the relationship between the camera pose estimation\nerror and bias values of map points is derived based on the optimized function\nin VSLAM. Secondly, the bias value of the map point is calculated by a\nstatistical method. Finally, the camera pose estimation error is compensated\naccording to the first derived relationship. After the pose correction,\nprocedures of the original system, such as the bundle adjustment (BA)\noptimization, can be executed as before. Compared with existing methods, our\nalgorithm is compact and effective and can be easily generalized to different\nVSLAM systems. Additionally, the robustness to system noise of our method is\nbetter than feature selection methods, due to all original system information\nis preserved in our algorithm while only a subset is employed in the latter.\nExperimental results on benchmark datasets show that our approach leads to\nconsiderable improvements over state-of-the-art algorithms for absolute pose\nestimation.",
    "published_date": "2019-08-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09072v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09066v1",
    "title": "Robust Regression via Deep Negative Correlation Learning",
    "authors": [
      "Le Zhang",
      "Zenglin Shi",
      "Ming-Ming Cheng",
      "Yun Liu",
      "Jia-Wang Bian",
      "Joey Tianyi Zhou",
      "Guoyan Zheng",
      "Zeng Zeng"
    ],
    "author_ids": [],
    "abstract": "Nonlinear regression has been extensively employed in many computer vision\nproblems (e.g., crowd counting, age estimation, affective computing). Under the\numbrella of deep learning, two common solutions exist i) transforming nonlinear\nregression to a robust loss function which is jointly optimizable with the deep\nconvolutional network, and ii) utilizing ensemble of deep networks. Although\nsome improved performance is achieved, the former may be lacking due to the\nintrinsic limitation of choosing a single hypothesis and the latter usually\nsuffers from much larger computational complexity. To cope with those issues,\nwe propose to regress via an efficient \"divide and conquer\" manner. The core of\nour approach is the generalization of negative correlation learning that has\nbeen shown, both theoretically and empirically, to work well for non-deep\nregression problems. Without extra parameters, the proposed method controls the\nbias-variance-covariance trade-off systematically and usually yields a deep\nregression ensemble where each base model is both \"accurate\" and \"diversified\".\nMoreover, we show that each sub-problem in the proposed method has less\nRademacher Complexity and thus is easier to optimize. Extensive experiments on\nseveral diverse and challenging tasks including crowd counting, personality\nanalysis, age estimation, and image super-resolution demonstrate the\nsuperiority over challenging baselines as well as the versatility of the\nproposed method.",
    "published_date": "2019-08-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.09041v2",
    "title": "A Center in Your Neighborhood: Fairness in Facility Location",
    "authors": [
      "Christopher Jung",
      "Sampath Kannan",
      "Neil Lutz"
    ],
    "author_ids": [],
    "abstract": "When selecting locations for a set of facilities, standard clustering\nalgorithms may place unfair burden on some individuals and neighborhoods. We\nformulate a fairness concept that takes local population densities into\naccount. In particular, given $k$ facilities to locate and a population of size\n$n$, we define the \"neighborhood radius\" of an individual $i$ as the minimum\nradius of a ball centered at $i$ that contains at least $n/k$ individuals. Our\nobjective is to ensure that each individual has a facility within at most a\nsmall constant factor of her neighborhood radius. We present several\ntheoretical results:\n  We show that optimizing this factor is NP-hard; we give an approximation\nalgorithm that guarantees a factor of at most 2 in all metric spaces; and we\nprove matching lower bounds in some metric spaces. We apply a variant of this\nalgorithm to real-world address data, showing that it is quite different from\nstandard clustering algorithms and outperforms them on our objective function\nand balances the load between facilities more evenly.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09041v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.08991v2",
    "title": "Football is becoming more predictable; Network analysis of 88 thousands matches in 11 major leagues",
    "authors": [
      "Victor Martins Maimone",
      "Taha Yasseri"
    ],
    "author_ids": [],
    "abstract": "In recent years excessive monetization of football and professionalism among\nthe players has been argued to have affected the quality of the match in\ndifferent ways. On the one hand, playing football has become a high-income\nprofession and the players are highly motivated; on the other hand, stronger\nteams have higher incomes and therefore afford better players leading to an\neven stronger appearance in tournaments that can make the game more imbalanced\nand hence predictable. To quantify and document this observation, in this work\nwe take a minimalist network science approach to measure the predictability of\nfootball over 26 years in major European leagues. We show that over time, the\ngames in major leagues have indeed become more predictable. We provide further\nsupport for this observation by showing that inequality between teams has\nincreased and the home-field advantage has been vanishing ubiquitously. We do\nnot include any direct analysis on the effects of monetization on football's\npredictability or therefore, lack of excitement, however, we propose several\nhypotheses which could be tested in future analyses.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.SI",
      "stat.OT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08991v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.08843v2",
    "title": "Fairness in Deep Learning: A Computational Perspective",
    "authors": [
      "Mengnan Du",
      "Fan Yang",
      "Na Zou",
      "Xia Hu"
    ],
    "author_ids": [],
    "abstract": "Deep learning is increasingly being used in high-stake decision making\napplications that affect individual lives. However, deep learning models might\nexhibit algorithmic discrimination behaviors with respect to protected groups,\npotentially posing negative impacts on individuals and society. Therefore,\nfairness in deep learning has attracted tremendous attention recently. We\nprovide a review covering recent progresses to tackle algorithmic fairness\nproblems of deep learning from the computational perspective. Specifically, we\nshow that interpretability can serve as a useful ingredient to diagnose the\nreasons that lead to algorithmic discrimination. We also discuss fairness\nmitigation approaches categorized according to three stages of deep learning\nlife-cycle, aiming to push forward the area of fairness in deep learning and\nbuild genuinely fair and reliable deep learning systems.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08843v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.08748v1",
    "title": "Multi-Tag Backscattering to MIMO Reader: Channel Estimation and Throughput Fairness",
    "authors": [
      "Deepak Mishra",
      "Erik G. Larsson"
    ],
    "author_ids": [],
    "abstract": "Green low power networking with the least requirement of dedicated radio\nresources is need of the hour which has led to the upsurge of backscatter\ncommunication (BSC) technology. However, this inherent potential of BSC is\nchallenged by hardware constraints of the underlying tags. We address this\ntimely concern by investigating the practical efficacy of\nmultiple-input-multiple-output (MIMO) technology in overcoming the fundamental\nlimitations of BSC. Specifically, we first introduce a novel least-squares\nbased channel estimation (CE) protocol for multi-tag BSC settings that takes\ncare of both the unintended ambient reflections and the inability of tags in\nperforming estimation by themselves. Then using it, a nontrivial low-complexity\nalgorithm is proposed to obtain the optimal transceiver designs for the\nmultiantenna reader to maximize the minimum value of the lower-bounded\nbackscattered throughput among the single-antenna semi-passive tags. Additional\nanalytical insights on both individually and jointly-optimal precoding vector\nand detector matrix at the reader are provided by exploring the\nasymptotically-optimal transceiver designs. Lastly detailed numerical\ninvestigation is carried out to validate the theoretical results and quantify\nthe practically realizable throughput fairness. Specifically, more than\nseven-fold increase in the common-backscattered-throughput among tags as\nachieved by the proposed designs over the relevant benchmarks corroborates\ntheir practical significance.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08748v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.08717v1",
    "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
    "authors": [
      "Mahault Garnerin",
      "Solange Rossato",
      "Laurent Besacier"
    ],
    "author_ids": [],
    "abstract": "This paper analyzes the gender representation in four major corpora of French\nbroadcast. These corpora being widely used within the speech processing\ncommunity, they are a primary material for training automatic speech\nrecognition (ASR) systems. As gender bias has been highlighted in numerous\nnatural language processing (NLP) applications, we study the impact of the\ngender imbalance in TV and radio broadcast on the performance of an ASR system.\nThis analysis shows that women are under-represented in our data in terms of\nspeakers and speech turns. We introduce the notion of speaker role to refine\nour analysis and find that women are even fewer within the Anchor category\ncorresponding to prominent speakers. The disparity of available data for both\ngender causes performance to decrease on women. However this global trend can\nbe counterbalanced for speaker who are used to speak in the media when\nsufficient amount of data is available.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08717v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.08702v4",
    "title": "A simple model suggesting economically rational sample-size choice drives irreproducibility",
    "authors": [
      "Oliver Braganza"
    ],
    "author_ids": [],
    "abstract": "Several systematic studies have suggested that a large fraction of published\nresearch is not reproducible. One probable reason for low reproducibility is\ninsufficient sample size, resulting in low power and low positive predictive\nvalue. It has been suggested that insufficient sample-size choice is driven by\na combination of scientific competition and 'positive publication bias'. Here\nwe formalize this intuition in a simple model, in which scientists choose\neconomically rational sample sizes, balancing the cost of experimentation with\nincome from publication. Specifically, assuming that a scientist's income\nderives only from 'positive' findings (positive publication bias) and that\nindividual samples cost a fixed amount, allows to leverage basic statistical\nformulas into an economic optimality prediction. We find that if effects have\ni) low base probability, ii) small effect size or iii) low grant income per\npublication, then the rational (economically optimal) sample size is small.\nFurthermore, for plausible distributions of these parameters we find a robust\nemergence of a bimodal distribution of obtained statistical power and low\noverall reproducibility rates, both matching empirical findings. Finally, we\nexplore conditional equivalence testing as a means to align economic incentives\nwith adequate sample sizes. Overall, the model describes a simple mechanism\nexplaining both the prevalence and the persistence of small sample sizes, and\nis well suited for empirical validation. It proposes economic rationality, or\neconomic pressures, as a principal driver of irreproducibility and suggests\nstrategies to change this.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "econ.GN",
      "cs.SY",
      "eess.SY",
      "q-fin.EC",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08702v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.09635v3",
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "authors": [
      "Ninareh Mehrabi",
      "Fred Morstatter",
      "Nripsuta Saxena",
      "Kristina Lerman",
      "Aram Galstyan"
    ],
    "author_ids": [],
    "abstract": "With the widespread use of AI systems and applications in our everyday lives,\nit is important to take fairness issues into consideration while designing and\nengineering these types of systems. Such systems can be used in many sensitive\nenvironments to make important and life-changing decisions; thus, it is crucial\nto ensure that the decisions do not reflect discriminatory behavior toward\ncertain groups or populations. We have recently seen work in machine learning,\nnatural language processing, and deep learning that addresses such challenges\nin different subdomains. With the commercialization of these systems,\nresearchers are becoming aware of the biases that these applications can\ncontain and have attempted to address them. In this survey we investigated\ndifferent real-world applications that have shown biases in various ways, and\nwe listed different sources of biases that can affect AI applications. We then\ncreated a taxonomy for fairness definitions that machine learning researchers\nhave defined in order to avoid the existing bias in AI systems. In addition to\nthat, we examined different domains and subdomains in AI showing what\nresearchers have observed with regard to unfair outcomes in the\nstate-of-the-art methods and how they have tried to address them. There are\nstill many future directions and solutions that can be taken to mitigate the\nproblem of bias in AI systems. We are hoping that this survey will motivate\nresearchers to tackle these issues in the near future by observing existing\nwork in their respective fields.",
    "published_date": "2019-08-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.09635v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.08452v1",
    "title": "A new measure of modularity density for community detection",
    "authors": [
      "Swathi M. Mula",
      "Gerardo Veltri"
    ],
    "author_ids": [],
    "abstract": "Using an intuitive concept of what constitutes a meaningful community, a\nnovel metric is formulated for detecting non-overlapping communities in\nundirected, weighted heterogeneous networks. This metric, modularity density,\nis shown to be superior to the versions of modularity density in present\nliterature. Compared to the previous versions of modularity density,\nmaximization of our metric is proven to be free from bias and better detect\nweakly-separated communities particularly in heterogeneous networks. In\naddition to these characteristics, the computational running time of our\nmodularity density is found to be on par or faster than that of the previous\nvariants. Our findings further reveal that community detection by maximization\nof our metric is mathematically related to partitioning a network by\nminimization of the normalized cut criterion.",
    "published_date": "2019-08-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08452v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.10313v1",
    "title": "Game-theoretic modeling of curtailment rules and network investments with distributed generation",
    "authors": [
      "Merlinda Andoni",
      "Valentin Robu",
      "Wolf-Gerrit Fruh",
      "David Flynn"
    ],
    "author_ids": [],
    "abstract": "Renewable energy has achieved high penetration rates in many areas, leading\nto curtailment, especially if existing network infrastructure is insufficient\nand energy generated cannot be exported. In this context, Distribution Network\nOperators (DNOs) face a significant knowledge gap about how to implement\ncurtailment rules that achieve desired operational objectives, but at the same\ntime minimise disruption and economic losses for renewable generators. In this\nwork, we study the properties of several curtailment rules widely used in UK\nrenewable energy projects, and their effect on the viability of renewable\ngeneration investment. Moreover, we propose a new curtailment rule which\nguarantees fair allocation of curtailment amongst all generators with minimal\ndisruption. Another key knowledge gap faced by DNOs is how to incentivise\nprivate network upgrades, especially in settings where several generators can\nuse the same line against the payment of a transmission fee. In this work, we\nprovide a solution to this problem by using tools from algorithmic game theory.\nSpecifically, this setting can be modelled as a Stackelberg game between the\nprivate transmission line investor and local renewable generators, who are\nrequired to pay a transmission fee to access the line. We provide a method for\ncomputing the empirical equilibrium of this game, using a model that captures\nthe stochastic nature of renewable energy generation and demand. Finally, we\nuse the practical setting of a grid reinforcement project from the UK and a\nlarge dataset of wind speed measurements and demand to validate our model. We\nshow that charging a transmission fee as a proportion of the feed-in tariff\nprice between 15%-75% would allow both investors to implement their projects\nand achieve desirable distribution of the profit.",
    "published_date": "2019-08-22T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.CY",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.10313v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.08939v3",
    "title": "AI and Accessibility: A Discussion of Ethical Considerations",
    "authors": [
      "Meredith Ringel Morris"
    ],
    "author_ids": [],
    "abstract": "According to the World Health Organization, more than one billion people\nworldwide have disabilities. The field of disability studies defines disability\nthrough a social lens; people are disabled to the extent that society creates\naccessibility barriers. AI technologies offer the possibility of removing many\naccessibility barriers; for example, computer vision might help people who are\nblind better sense the visual world, speech recognition and translation\ntechnologies might offer real time captioning for people who are hard of\nhearing, and new robotic systems might augment the capabilities of people with\nlimited mobility. Considering the needs of users with disabilities can help\ntechnologists identify high-impact challenges whose solutions can advance the\nstate of AI for all users; however, ethical challenges such as inclusivity,\nbias, privacy, error, expectation setting, simulated data, and social\nacceptability must be considered.",
    "published_date": "2019-08-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08939v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.07898v2",
    "title": "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets",
    "authors": [
      "Mor Geva",
      "Yoav Goldberg",
      "Jonathan Berant"
    ],
    "author_ids": [],
    "abstract": "Crowdsourcing has been the prevalent paradigm for creating natural language\nunderstanding datasets in recent years. A common crowdsourcing practice is to\nrecruit a small number of high-quality workers, and have them massively\ngenerate examples. Having only a few workers generate the majority of examples\nraises concerns about data diversity, especially when workers freely generate\nsentences. In this paper, we perform a series of experiments showing these\nconcerns are evident in three recent NLP datasets. We show that model\nperformance improves when training with annotator identifiers as features, and\nthat models are able to recognize the most productive annotators. Moreover, we\nshow that often models do not generalize well to examples from annotators that\ndid not contribute to the training set. Our findings suggest that annotator\nbias should be monitored during dataset creation, and that test set annotators\nshould be disjoint from training set annotators.",
    "published_date": "2019-08-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07898v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.07765v1",
    "title": "Dataset Growth in Medical Image Analysis Research",
    "authors": [
      "Yuval Landau",
      "Nahum Kiryati"
    ],
    "author_ids": [],
    "abstract": "Medical image analysis studies usually require medical image datasets for\ntraining, testing and validation of algorithms. The need is underscored by the\ndeep learning revolution and the dominance of machine learning in recent\nmedical image analysis research. Nevertheless, due to ethical and legal\nconstraints, commercial conflicts and the dependence on busy medical\nprofessionals, medical image analysis researchers have been described as \"data\nstarved\". Due to the lack of objective criteria for sufficiency of dataset\nsize, the research community implicitly sets ad-hoc standards by means of the\npeer review process. We hypothesize that peer review requires researchers to\nreport the use of ever-increasing datasets as one condition for acceptance of\ntheir work to reputable publication venues. To test this hypothesis, we scanned\nthe proceedings of the eminent MICCAI (Medical Image Computing and\nComputer-Assisted Intervention) conferences from 2011 to 2018. From a total of\n2136 articles, we focused on 907 papers involving human datasets of MRI\n(Magnetic Resonance Imaging), CT (Computed Tomography) and fMRI (functional\nMRI) images. For each modality, for each of the years 2011-2018 we calculated\nthe average, geometric mean and median number of human subjects used in that\nyear's MICCAI articles. The results corroborate the dataset growth hypothesis.\nSpecifically, the annual median dataset size in MICCAI articles has grown\nroughly 3-10 times from 2011 to 2018, depending on the imaging modality.\nStatistical analysis further supports the dataset growth hypothesis and reveals\nexponential growth of the geometric mean dataset size, with annual growth of\nabout 21% for MRI, 24% for CT and 31% for fMRI. In slight analogy to Moore's\nlaw, the results can provide guidance about trends in the expectations of the\nmedical image analysis community regarding dataset size.",
    "published_date": "2019-08-21T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.07924v3",
    "title": "Data Management for Causal Algorithmic Fairness",
    "authors": [
      "Babak Salimi",
      "Bill Howe",
      "Dan Suciu"
    ],
    "author_ids": [],
    "abstract": "Fairness is increasingly recognized as a critical component of machine\nlearning systems. However, it is the underlying data on which these systems are\ntrained that often reflects discrimination, suggesting a data management\nproblem. In this paper, we first make a distinction between associational and\ncausal definitions of fairness in the literature and argue that the concept of\nfairness requires causal reasoning. We then review existing works and identify\nfuture opportunities for applying data management techniques to causal\nalgorithmic fairness.",
    "published_date": "2019-08-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07924v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.07123v3",
    "title": "Estimating Attention Flow in Online Video Networks",
    "authors": [
      "Siqi Wu",
      "Marian-Andrei Rizoiu",
      "Lexing Xie"
    ],
    "author_ids": [],
    "abstract": "Online videos have shown tremendous increase in Internet traffic. Most video\nhosting sites implement recommender systems, which connect the videos into a\ndirected network and conceptually act as a source of pathways for users to\nnavigate. At present, little is known about how human attention is allocated\nover such large-scale networks, and about the impacts of the recommender\nsystems. In this paper, we first construct the Vevo network -- a YouTube video\nnetwork with 60,740 music videos interconnected by the recommendation links,\nand we collect their associated viewing dynamics. This results in a total of\n310 million views every day over a period of 9 weeks. Next, we present\nlarge-scale measurements that connect the structure of the recommendation\nnetwork and the video attention dynamics. We use the bow-tie structure to\ncharacterize the Vevo network and we find that its core component (23.1% of the\nvideos), which occupies most of the attention (82.6% of the views), is made out\nof videos that are mainly recommended among themselves. This is indicative of\nthe links between video recommendation and the inequality of attention\nallocation. Finally, we address the task of estimating the attention flow in\nthe video recommendation network. We propose a model that accounts for the\nnetwork effects for predicting video popularity, and we show it consistently\noutperforms the baselines. This model also identifies a group of artists\ngaining attention because of the recommendation network. Altogether, our\nobservations and our models provide a new set of tools to better understand the\nimpacts of recommender systems on collective social attention.",
    "published_date": "2019-08-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.HC",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07123v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.07009v4",
    "title": "Towards Reducing Biases in Combining Multiple Experts Online",
    "authors": [
      "Yi Sun",
      "Ivan Ramirez",
      "Alfredo Cuesta-Infante",
      "Kalyan Veeramachaneni"
    ],
    "author_ids": [],
    "abstract": "In many real life situations, including job and loan applications,\ngatekeepers must make justified and fair real-time decisions about a person's\nfitness for a particular opportunity. In this paper, we aim to accomplish\napproximate group fairness in an online stochastic decision-making process,\nwhere the fairness metric we consider is equalized odds. Our work follows from\nthe classical learning-from-experts scheme, assuming a finite set of\nclassifiers (human experts, rules, options, etc) that cannot be modified. We\nrun separate instances of the algorithm for each label class as well as\nsensitive groups, where the probability of choosing each instance is optimized\nfor both fairness and regret. Our theoretical results show that approximately\nequalized odds can be achieved without sacrificing much regret. We also\ndemonstrate the performance of the algorithm on real data sets commonly used by\nthe fairness community.",
    "published_date": "2019-08-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07009v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06874v1",
    "title": "Efficient Discovery of Expressive Multi-label Rules using Relaxed Pruning",
    "authors": [
      "Yannik Klein",
      "Michael Rapp",
      "Eneldo Loza Mencía"
    ],
    "author_ids": [],
    "abstract": "Being able to model correlations between labels is considered crucial in\nmulti-label classification. Rule-based models enable to expose such\ndependencies, e.g., implications, subsumptions, or exclusions, in an\ninterpretable and human-comprehensible manner. Albeit the number of possible\nlabel combinations increases exponentially with the number of available labels,\nit has been shown that rules with multiple labels in their heads, which are a\nnatural form to model local label dependencies, can be induced efficiently by\nexploiting certain properties of rule evaluation measures and pruning the label\nsearch space accordingly. However, experiments have revealed that multi-label\nheads are unlikely to be learned by existing methods due to their\nrestrictiveness. To overcome this limitation, we propose a plug-in approach\nthat relaxes the search space pruning used by existing methods in order to\nintroduce a bias towards larger multi-label heads resulting in more expressive\nrules. We further demonstrate the effectiveness of our approach empirically and\nshow that it does not come with drawbacks in terms of training time or\npredictive performance.",
    "published_date": "2019-08-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06874v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06708v1",
    "title": "Recommender Systems Fairness Evaluation via Generalized Cross Entropy",
    "authors": [
      "Yashar Deldjoo",
      "Vito Walter Anelli",
      "Hamed Zamani",
      "Alejandro Bellogin",
      "Tommaso Di Noia"
    ],
    "author_ids": [],
    "abstract": "Fairness in recommender systems has been considered with respect to sensitive\nattributes of users (e.g., gender, race) or items (e.g., revenue in a\nmultistakeholder setting). Regardless, the concept has been commonly\ninterpreted as some form of equality -- i.e., the degree to which the system is\nmeeting the information needs of all its users in an equal sense. In this\npaper, we argue that fairness in recommender systems does not necessarily imply\nequality, but instead it should consider a distribution of resources based on\nmerits and needs.\n  We present a probabilistic framework based on generalized cross entropy to\nevaluate fairness of recommender systems under this perspective, where we show\nthat the proposed framework is flexible and explanatory by allowing to\nincorporate domain knowledge (through an ideal fair distribution) that can help\nto understand which item or user aspects a recommendation algorithm is over- or\nunder-representing. Results on two real-world datasets show the merits of the\nproposed evaluation framework both in terms of user and item fairness.",
    "published_date": "2019-08-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06708v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.06361v1",
    "title": "Understanding Undesirable Word Embedding Associations",
    "authors": [
      "Kawin Ethayarajh",
      "David Duvenaud",
      "Graeme Hirst"
    ],
    "author_ids": [],
    "abstract": "Word embeddings are often criticized for capturing undesirable word\nassociations such as gender stereotypes. However, methods for measuring and\nremoving such biases remain poorly understood. We show that for any embedding\nmodel that implicitly does matrix factorization, debiasing vectors post hoc\nusing subspace projection (Bolukbasi et al., 2016) is, under certain\nconditions, equivalent to training on an unbiased corpus. We also prove that\nWEAT, the most common association test for word embeddings, systematically\noverestimates bias. Given that the subspace projection method is provably\neffective, we use it to derive a new measure of association called the\n$\\textit{relational inner product association}$ (RIPA). Experiments with RIPA\nreveal that, on average, skipgram with negative sampling (SGNS) does not make\nmost words any more gendered than they are in the training corpus. However, for\ngender-stereotyped words, SGNS actually amplifies the gender association in the\ncorpus.",
    "published_date": "2019-08-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06361v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.07333v1",
    "title": "Fairness Issues in AI Systems that Augment Sensory Abilities",
    "authors": [
      "Leah Findlater",
      "Steven Goodman",
      "Yuhang Zhao",
      "Shiri Azenkot",
      "Margot Hanley"
    ],
    "author_ids": [],
    "abstract": "Systems that augment sensory abilities are increasingly employing AI and\nmachine learning (ML) approaches, with applications ranging from object\nrecognition and scene description tools for blind users to sound awareness\ntools for d/Deaf users. However, unlike many other AI-enabled technologies,\nthese systems provide information that is already available to non-disabled\npeople. In this paper, we discuss unique AI fairness challenges that arise in\nthis context, including accessibility issues with data and models, ethical\nimplications in deciding what sensory information to convey to the user, and\nprivacy concerns both for the primary user and for others.",
    "published_date": "2019-08-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05885v1",
    "title": "Regression on imperfect class labels derived by unsupervised clustering",
    "authors": [
      "Rasmus Froberg Brøndum",
      "Thomas Yssing Michaelsen",
      "Martin Bøgsted"
    ],
    "author_ids": [],
    "abstract": "Outcome regressed on class labels identified by unsupervised clustering is\ncustom in many applications. However, it is common to ignore the\nmisclassification of class labels caused by the learning algorithm, which\npotentially leads to serious bias of the estimated effect parameters. Due to\nits generality we suggest to redress the situation by use of the simulation and\nextrapolation method. Performance is illustrated by simulated data from\nGaussian mixture models. Finally, we apply our method to a study which\nregressed overall survival on class labels derived from unsupervised clustering\nof gene expression data from bone marrow samples of multiple myeloma patients.",
    "published_date": "2019-08-16T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05885v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05884v1",
    "title": "GODS: Generalized One-class Discriminative Subspaces for Anomaly Detection",
    "authors": [
      "Jue Wang",
      "Anoop Cherian"
    ],
    "author_ids": [],
    "abstract": "One-class learning is the classic problem of fitting a model to data for\nwhich annotations are available only for a single class. In this paper, we\npropose a novel objective for one-class learning. Our key idea is to use a pair\nof orthonormal frames -- as subspaces -- to \"sandwich\" the labeled data via\noptimizing for two objectives jointly: i) minimize the distance between the\norigins of the two subspaces, and ii) to maximize the margin between the\nhyperplanes and the data, either subspace demanding the data to be in its\npositive and negative orthant respectively. Our proposed objective however\nleads to a non-convex optimization problem, to which we resort to Riemannian\noptimization schemes and derive an efficient conjugate gradient scheme on the\nStiefel manifold. To study the effectiveness of our scheme, we propose a new\ndataset~\\emph{Dash-Cam-Pose}, consisting of clips with skeleton poses of humans\nseated in a car, the task being to classify the clips as normal or abnormal;\nthe latter is when any human pose is out-of-position with regard to say an\nairbag deployment. Our experiments on the proposed Dash-Cam-Pose dataset, as\nwell as several other standard anomaly/novelty detection benchmarks demonstrate\nthe benefits of our scheme, achieving state-of-the-art one-class accuracy.",
    "published_date": "2019-08-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05884v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05841v1",
    "title": "Recurrent U-net: Deep learning to predict daily summertime ozone in the United States",
    "authors": [
      "Tai-Long He",
      "Dylan B. A. Jones",
      "Binxuan Huang",
      "Yuyang Liu",
      "Kazuyuki Miyazaki",
      "Zhe Jiang",
      "E. Charlie White",
      "Helen M. Worden",
      "John R. Worden"
    ],
    "author_ids": [],
    "abstract": "We use a hybrid deep learning model to predict June-July-August (JJA) daily\nmaximum 8-h average (MDA8) surface ozone concentrations in the US. A set of\nmeteorological fields from the ERA-Interim reanalysis as well as monthly mean\nNO$_x$ emissions from the Community Emissions Data System (CEDS) inventory are\nselected as predictors. Ozone measurements from the US Environmental Protection\nAgency (EPA) Air Quality System (AQS) from 1980 to 2009 are used to train the\nmodel, whereas data from 2010 to 2014 are used to evaluate the performance of\nthe model. The model captures well daily, seasonal and interannual variability\nin MDA8 ozone across the US. Feature maps show that the model captures\nteleconnections between MDA8 ozone and the meteorological fields, which are\nresponsible for driving the ozone dynamics. We used the model to evaluate\nrecent trends in NO$_x$ emissions in the US and found that the trend in the EPA\nemission inventory produced the largest negative bias in MDA8 ozone between\n2010-2016. The top-down emission trends from the Tropospheric Chemistry\nReanalysis (TCR-2), which is based on satellite observations, produced\npredictions in best agreement with observations. In urban regions, the trend in\nAQS NO$_2$ observations provided ozone predictions in agreement with\nobservations, whereas in rural regions the satellite-derived trends produced\nthe best agreement. In both rural and urban regions the EPA trend resulted in\nthe largest negative bias in predicted ozone. Our results suggest that the EPA\ninventory is overestimating the reductions in NO$_x$ emissions and that the\nsatellite-derived trend reflects the influence of reductions in NO$_x$\nemissions as well as changes in background NO$_x$. Our results demonstrate the\nsignificantly greater predictive capability that the deep learning model\nprovides over conventional atmospheric chemical transport models for air\nquality analyses.",
    "published_date": "2019-08-16T00:00:00",
    "year": 2019,
    "categories": [
      "physics.ao-ph",
      "cs.CV",
      "cs.LG",
      "physics.chem-ph",
      "physics.geo-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05841v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05783v3",
    "title": "Tackling Algorithmic Bias in Neural-Network Classifiers using Wasserstein-2 Regularization",
    "authors": [
      "Laurent Risser",
      "Alberto Gonzalez Sanz",
      "Quentin Vincenot",
      "Jean-Michel Loubes"
    ],
    "author_ids": [],
    "abstract": "The increasingly common use of neural network classifiers in industrial and\nsocial applications of image analysis has allowed impressive progress these\nlast years. Such methods are however sensitive to algorithmic bias, i.e. to an\nunder- or an over-representation of positive predictions or to higher\nprediction errors in specific subgroups of images. We then introduce in this\npaper a new method to temper the algorithmic bias in Neural-Network based\nclassifiers. Our method is Neural-Network architecture agnostic and scales well\nto massive training sets of images. It indeed only overloads the loss function\nwith a Wasserstein-2 based regularization term for which we back-propagate the\nimpact of specific output predictions using a new model, based on the Gateaux\nderivatives of the predictions distribution. This model is algorithmically\nreasonable and makes it possible to use our regularized loss with standard\nstochastic gradient-descent strategies. Its good behavior is assessed on the\nreference Adult census, MNIST, CelebA datasets.",
    "published_date": "2019-08-15T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05783v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06943v2",
    "title": "Resolving challenges in deep learning-based analyses of histopathological images using explanation methods",
    "authors": [
      "Miriam Hägele",
      "Philipp Seegerer",
      "Sebastian Lapuschkin",
      "Michael Bockmayr",
      "Wojciech Samek",
      "Frederick Klauschen",
      "Klaus-Robert Müller",
      "Alexander Binder"
    ],
    "author_ids": [],
    "abstract": "Deep learning has recently gained popularity in digital pathology due to its\nhigh prediction quality. However, the medical domain requires explanation and\ninsight for a better understanding beyond standard quantitative performance\nevaluation. Recently, explanation methods have emerged, which are so far still\nrarely used in medicine. This work shows their application to generate heatmaps\nthat allow to resolve common challenges encountered in deep learning-based\ndigital histopathology analyses. These challenges comprise biases typically\ninherent to histopathology data. We study binary classification tasks of tumor\ntissue discrimination in publicly available haematoxylin and eosin slides of\nvarious tumor entities and investigate three types of biases: (1) biases which\naffect the entire dataset, (2) biases which are by chance correlated with class\nlabels and (3) sampling biases. While standard analyses focus on patch-level\nevaluation, we advocate pixel-wise heatmaps, which offer a more precise and\nversatile diagnostic instrument and furthermore help to reveal biases in the\ndata. This insight is shown to not only detect but also to be helpful to remove\nthe effects of common hidden biases, which improves generalization within and\nacross datasets. For example, we could see a trend of improved area under the\nreceiver operating characteristic curve by 5% when reducing a labeling bias.\nExplanation techniques are thus demonstrated to be a helpful and highly\nrelevant tool for the development and the deployment phases within the life\ncycle of real-world applications in digital pathology.",
    "published_date": "2019-08-15T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06943v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05433v3",
    "title": "The Price of Connectivity in Fair Division",
    "authors": [
      "Xiaohui Bei",
      "Ayumi Igarashi",
      "Xinhang Lu",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We study the allocation of indivisible goods that form an undirected graph\nand quantify the loss of fairness when we impose a constraint that each agent\nmust receive a connected subgraph. Our focus is on well-studied fairness\nnotions including envy-freeness and maximin share fairness. We introduce the\nprice of connectivity to capture the largest gap between the graph-specific and\nthe unconstrained maximin share, and derive bounds on this quantity which are\ntight for large classes of graphs in the case of two agents and for paths and\nstars in the general case. For instance, with two agents we show that for\nbiconnected graphs it is possible to obtain at least $3/4$ of the maximin share\nwith connected allocations, while for the remaining graphs the guarantee is at\nmost $1/2$. In addition, we determine the optimal relaxation of envy-freeness\nthat can be obtained with each graph for two agents, and characterize the set\nof trees and complete bipartite graphs that always admit an allocation\nsatisfying envy-freeness up to one good (EF1) for three agents. Our work\ndemonstrates several applications of graph-theoretic tools and concepts to fair\ndivision problems.",
    "published_date": "2019-08-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05433v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.05429v1",
    "title": "Domain-adversarial Network Alignment",
    "authors": [
      "Huiting Hong",
      "Xin Li",
      "Yuangang Pan",
      "Ivor Tsang"
    ],
    "author_ids": [],
    "abstract": "Network alignment is a critical task to a wide variety of fields. Many\nexisting works leverage on representation learning to accomplish this task\nwithout eliminating domain representation bias induced by domain-dependent\nfeatures, which yield inferior alignment performance. This paper proposes a\nunified deep architecture (DANA) to obtain a domain-invariant representation\nfor network alignment via an adversarial domain classifier. Specifically, we\nemploy the graph convolutional networks to perform network embedding under the\ndomain adversarial principle, given a small set of observed anchors. Then, the\nsemi-supervised learning framework is optimized by maximizing a posterior\nprobability distribution of observed anchors and the loss of a domain\nclassifier simultaneously. We also develop a few variants of our model, such\nas, direction-aware network alignment, weight-sharing for directed networks and\nsimplification of parameter space. Experiments on three real-world social\nnetwork datasets demonstrate that our proposed approaches achieve\nstate-of-the-art alignment results.",
    "published_date": "2019-08-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05429v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.10187v1",
    "title": "A Machine Learning Approach for Smartphone-based Sensing of Roads and Driving Style",
    "authors": [
      "M. Ricardo Carlos"
    ],
    "author_ids": [],
    "abstract": "Road transportation is of critical importance for a nation, having profound\neffects in the economy, the health and life style of its people. With the\ngrowth of cities and populations come bigger demands for mobility and safety,\ncreating new problems and magnifying those of the past. New tools are needed to\nface the challenge, to keep roads in good conditions, their users safe, and\nminimize the impact on the environment.\n  This dissertation is concerned with road quality assessment and aggressive\ndriving, two important problems in road transportation, approached in the\ncontext of Intelligent Transportation Systems by using Machine Learning\ntechniques to analyze acceleration time series acquired with smartphone-based\nopportunistic sensing to automatically detect, classify, and characterize\nevents of interest.\n  Two aspects of road quality assessment are addressed: the detection and the\ncharacterization of road anomalies. For the first, the most widely cited works\nin the literature are compared and proposals capable of equal or better\nperformance are presented, removing the reliance on threshold values and\nreducing the computational cost and dimensionality of previous proposals. For\nthe second, new approaches for the estimation of pothole depth and the\nfunctional condition of speed reducers are showed. The new problem of pothole\ndepth ranking is introduced, using a learning-to-rank approach to sort\nacceleration signals by the depth of the potholes that they reflect.\n  The classification of aggressive driving maneuvers is done with automatic\nfeature extraction, finding characteristically shaped subsequences in the\nsignals as more effective discriminants than conventional descriptors\ncalculated over time windows.\n  Finally, all the previously mentioned tasks are combined to produce a robust\nroad transport evaluation platform.",
    "published_date": "2019-08-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.10187v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05757v1",
    "title": "Debiasing Personal Identities in Toxicity Classification",
    "authors": [
      "Apik Ashod Zorian",
      "Chandra Shekar Bikkanur"
    ],
    "author_ids": [],
    "abstract": "As Machine Learning models continue to be relied upon for making automated\ndecisions, the issue of model bias becomes more and more prevalent. In this\npaper, we approach training a text classifica-tion model and optimize on bias\nminimization by measuring not only the models performance on our dataset as a\nwhole, but also how it performs across different subgroups. This requires\nmeasuring per-formance independently for different demographic subgroups and\nmeasuring bias by comparing them to results from the rest of our data. We show\nhow unintended bias can be detected using these metrics and how removing bias\nfrom a dataset completely can result in worse results.",
    "published_date": "2019-08-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05757v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.05168v1",
    "title": "A Tour of Convolutional Networks Guided by Linear Interpreters",
    "authors": [
      "Pablo Navarrete Michelini",
      "Hanwen Liu",
      "Yunhua Lu",
      "Xingqun Jiang"
    ],
    "author_ids": [],
    "abstract": "Convolutional networks are large linear systems divided into layers and\nconnected by non-linear units. These units are the \"articulations\" that allow\nthe network to adapt to the input. To understand how a network manages to solve\na problem we must look at the articulated decisions in entirety. If we could\ncapture the actions of non-linear units for a particular input, we would be\nable to replay the whole system back and forth as if it was always linear. It\nwould also reveal the actions of non-linearities because the resulting linear\nsystem, a Linear Interpreter, depends on the input image. We introduce a\nhooking layer, called a LinearScope, which allows us to run the network and the\nlinear interpreter in parallel. Its implementation is simple, flexible and\nefficient. From here we can make many curious inquiries: how do these linear\nsystems look like? When the rows and columns of the transformation matrix are\nimages, how do they look like? What type of basis do these linear\ntransformations rely on? The answers depend on the problems presented, through\nwhich we take a tour to some popular architectures used for classification,\nsuper-resolution (SR) and image-to-image translation (I2I). For classification\nwe observe that popular networks use a pixel-wise vote per class strategy and\nheavily rely on bias parameters. For SR and I2I we find that CNNs use\nwavelet-type basis similar to the human visual system. For I2I we reveal\ncopy-move and template-creation strategies to generate outputs.",
    "published_date": "2019-08-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NA",
      "eess.IV",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05168v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.04930v1",
    "title": "Generalised Zero-Shot Learning with Domain Classification in a Joint Semantic and Visual Space",
    "authors": [
      "Rafael Felix",
      "Ben Harwood",
      "Michele Sasdelli",
      "Gustavo Carneiro"
    ],
    "author_ids": [],
    "abstract": "Generalised zero-shot learning (GZSL) is a classification problem where the\nlearning stage relies on a set of seen visual classes and the inference stage\naims to identify both the seen visual classes and a new set of unseen visual\nclasses. Critically, both the learning and inference stages can leverage a\nsemantic representation that is available for the seen and unseen classes. Most\nstate-of-the-art GZSL approaches rely on a mapping between latent visual and\nsemantic spaces without considering if a particular sample belongs to the set\nof seen or unseen classes. In this paper, we propose a novel GZSL method that\nlearns a joint latent representation that combines both visual and semantic\ninformation. This mitigates the need for learning a mapping between the two\nspaces. Our method also introduces a domain classification that estimates\nwhether a sample belongs to a seen or an unseen class. Our classifier then\ncombines a class discriminator with this domain classifier with the goal of\nreducing the natural bias that GZSL approaches have toward the seen classes.\nExperiments show that our method achieves state-of-the-art results in terms of\nharmonic mean, the area under the seen and unseen curve and unseen\nclassification accuracy on public GZSL benchmark data sets. Our code will be\navailable upon acceptance of this paper.",
    "published_date": "2019-08-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04930v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.04913v1",
    "title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age",
    "authors": [
      "Kimmo Kärkkäinen",
      "Jungseock Joo"
    ],
    "author_ids": [],
    "abstract": "Existing public face datasets are strongly biased toward Caucasian faces, and\nother races (e.g., Latino) are significantly underrepresented. This can lead to\ninconsistent model accuracy, limit the applicability of face analytic systems\nto non-White race groups, and adversely affect research findings based on such\nskewed data. To mitigate the race bias in these datasets, we construct a novel\nface image dataset, containing 108,501 images, with an emphasis of balanced\nrace composition in the dataset. We define 7 race groups: White, Black, Indian,\nEast Asian, Southeast Asian, Middle East, and Latino. Images were collected\nfrom the YFCC-100M Flickr dataset and labeled with race, gender, and age\ngroups. Evaluations were performed on existing face attribute datasets as well\nas novel image datasets to measure generalization performance. We find that the\nmodel trained from our dataset is substantially more accurate on novel datasets\nand the accuracy is consistent between race and gender groups.",
    "published_date": "2019-08-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04913v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06024v2",
    "title": "Tackling Online Abuse: A Survey of Automated Abuse Detection Methods",
    "authors": [
      "Pushkar Mishra",
      "Helen Yannakoudakis",
      "Ekaterina Shutova"
    ],
    "author_ids": [],
    "abstract": "Abuse on the Internet represents an important societal problem of our time.\nMillions of Internet users face harassment, racism, personal attacks, and other\ntypes of abuse on online platforms. The psychological effects of such abuse on\nindividuals can be profound and lasting. Consequently, over the past few years,\nthere has been a substantial research effort towards automated abuse detection\nin the field of natural language processing (NLP). In this paper, we present a\ncomprehensive survey of the methods that have been proposed to date, thus\nproviding a platform for further development of this area. We describe the\nexisting datasets and review the computational approaches to abuse detection,\nanalyzing their strengths and limitations. We discuss the main trends that\nemerge, highlight the challenges that remain, outline possible solutions, and\npropose guidelines for ethics and explainability",
    "published_date": "2019-08-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06024v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.04562v1",
    "title": "Null Space Analysis for Class-Specific Discriminant Learning",
    "authors": [
      "Jenni Raitoharju",
      "Alexandros Iosifidis"
    ],
    "author_ids": [],
    "abstract": "In this paper, we carry out null space analysis for Class-Specific\nDiscriminant Analysis (CSDA) and formulate a number of solutions based on the\nanalysis. We analyze both theoretically and experimentally the significance of\neach algorithmic step. The innate subspace dimensionality resulting from the\nproposed solutions is typically quite high and we discuss how the need for\nfurther dimensionality reduction changes the situation. Experimental evaluation\nof the proposed solutions shows that the straightforward extension of null\nspace analysis approaches to the class-specific setting can outperform the\nstandard CSDA method. Furthermore, by exploiting a recently proposed\nout-of-class scatter definition encoding the multi-modality of the negative\nclass naturally appearing in class-specific problems, null space projections\ncan lead to a performance comparable to or outperforming the most recent CSDA\nmethods.",
    "published_date": "2019-08-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.04556v4",
    "title": "Reinterpretation and Extension of Entropy Correction Terms for Residual Distribution and Discontinuous Galerkin Schemes: Application to Structure Preserving Discretization",
    "authors": [
      "Rémi Abgrall",
      "Philipp Öffner",
      "Hendrik Ranocha"
    ],
    "author_ids": [],
    "abstract": "For the general class of residual distribution (RD) schemes, including many\nfinite element (such as continuous/discontinuous Galerkin) and flux\nreconstruction methods, an approach to construct entropy conservative/\ndissipative semidiscretizations by adding suitable correction terms has been\nproposed by Abgrall (J.~Comp.~Phys. 372: pp. 640--666, 2018). In this work, the\ncorrection terms are characterized as solutions of certain optimization\nproblems and are adapted to the SBP-SAT framework, focusing on discontinuous\nGalerkin methods. Novel generalizations to entropy inequalities, multiple\nconstraints, and kinetic energy preservation for the Euler equations are\ndeveloped and tested in numerical experiments. For all of these optimization\nproblems, explicit solutions are provided. Additionally, the correction\napproach is applied for the first time to obtain a fully discrete entropy\nconservative/dissipative RD scheme. Here, the application of the deferred\ncorrection (DeC) method for the time integration is essential. This paper can\nbe seen as describing a systematic method to construct structure preserving\ndiscretization, at least for the considered example.",
    "published_date": "2019-08-13T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA",
      "65M12, 65M60, 65M70, 65M06"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04556v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.04336v3",
    "title": "Fairness and efficiency for probabilistic allocations with participation constraints",
    "authors": [
      "Federico Echenique",
      "Antonio Miralles",
      "Jun Zhang"
    ],
    "author_ids": [],
    "abstract": "We propose a notion of fairness for allocation problems in which different\nagents may have different reservation utilities, stemming from different\noutside options, or property rights. Fairness is usually understood as the\nabsence of envy, but this can be incompatible with reservation utilities. It is\npossible that Alice's envy of Bob's assignment cannot be remedied without\nviolating Bob's participation constraint. Instead, we seek to rule out {\\em\njustified envy}, defined as envy for which a remedy would not violate any\nagent's participation constraint. We show that fairness, meaning the absence of\njustified envy, can be achieved together with efficiency and individual\nrationality. We introduce a competitive equilibrium approach with\nprice-dependent incomes obtaining the desired properties.",
    "published_date": "2019-08-12T00:00:00",
    "year": 2019,
    "categories": [
      "econ.TH",
      "cs.GT",
      "91-XX"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04336v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.06866v1",
    "title": "Radio Resource Management for V2V Multihop Communication Considering Adjacent Channel Interference",
    "authors": [
      "Anver Hisham",
      "Erik G. Ström",
      "Fredrik Brännström"
    ],
    "author_ids": [],
    "abstract": "This paper investigate joint scheduling and power control for V2V multicast\nallowing multihop communication. The effects of both co-channel interference\nand adjacent channel interference are considered. First, we solve the problem\nwith the objective of maximizing the throughput and connectivity of vehicles in\nthe network. Then extend the same problem formulation to include the objective\nof minimizing the latency and the average age of information (AoI), which is\nthe age of the latest received message. In order to account for fairness, we\nalso show the problem formulation to maximize the worst-case throughput and\nconnectivity. All the problems are formulated as mixed Boolean linear\nprogramming problems, which allows computation of optimal solutions.\nFurthermore, we consider the error probability of a link failure in all the\nproblem formulations and accommodate the probability requirements for\nsatisfying a certain throughput/connectivity/latency/AoI. In order to support a\nlarge V2V network, a clustering algorithm is proposed whose computational\ncomplexity scale well with the network size. To handle the case of zero channel\ninformation at the scheduler, a multihop distributed scheduling scheme is\nproposed.",
    "published_date": "2019-08-12T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06866v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.06170v1",
    "title": "A Survey of Challenges and Opportunities in Sensing and Analytics for Cardiovascular Disorders",
    "authors": [
      "Nathan C. Hurley",
      "Erica S. Spatz",
      "Harlan M. Krumholz",
      "Roozbeh Jafari",
      "Bobak J. Mortazavi"
    ],
    "author_ids": [],
    "abstract": "Cardiovascular disorders account for nearly 1 in 3 deaths in the United\nStates. Care for these disorders are often determined during visits to acute\ncare facilities, such as hospitals. While the length of stay in these settings\nrepresents just a small proportion of patients' lives, they account for a\ndisproportionately large amount of decision making. To overcome this bias\ntowards data from acute care settings, there is a need for longitudinal\nmonitoring in patients with cardiovascular disorders. Longitudinal monitoring\ncan provide a more comprehensive picture of patient health, allowing for more\ninformed decision making. This work surveys the current field of sensing\ntechnologies and machine learning analytics that exist in the field of remote\nmonitoring for cardiovascular disorders. We highlight three primary needs in\nthe design of new smart health technologies: 1) the need for sensing technology\nthat can track longitudinal trends in signs and symptoms of the cardiovascular\ndisorder despite potentially infrequent, noisy, or missing data measurements;\n2) the need for new analytic techniques that model data captured in a\nlongitudinal, continual fashion to aid in the development of new risk\nprediction techniques and in tracking disease progression; and 3) the need for\nmachine learning techniques that are personalized and interpretable, allowing\nfor advancements in shared clinical decision making. We highlight these needs\nbased upon the current state-of-the-art in smart health technologies and\nanalytics and discuss the ample opportunities that exist in addressing all\nthree needs in the development of smart health technologies and analytics\napplied to the field of cardiovascular disorders and care.",
    "published_date": "2019-08-12T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06170v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.03957v1",
    "title": "Tensor Factorization with Label Information for Fake News Detection",
    "authors": [
      "Frosso Papanastasiou",
      "Georgios Katsimpras",
      "Georgios Paliouras"
    ],
    "author_ids": [],
    "abstract": "The buzz over the so-called \"fake news\" has created concerns about a\ndegenerated media environment and led to the need for technological solutions.\nAs the detection of fake news is increasingly considered a technological\nproblem, it has attracted considerable research. Most of these studies\nprimarily focus on utilizing information extracted from textual news content.\nIn contrast, we focus on detecting fake news solely based on structural\ninformation of social networks. We suggest that the underlying network\nconnections of users that share fake news are discriminative enough to support\nthe detection of fake news. Thereupon, we model each post as a network of\nfriendship interactions and represent a collection of posts as a\nmultidimensional tensor. Taking into account the available labeled data, we\npropose a tensor factorization method which associates the class labels of data\nsamples with their latent representations. Specifically, we combine a\nclassification error term with the standard factorization in a unified\noptimization process. Results on real-world datasets demonstrate that our\nproposed method is competitive against state-of-the-art methods by implementing\nan arguably simpler approach.",
    "published_date": "2019-08-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03957v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.03945v6",
    "title": "Robust Online Multi-target Visual Tracking using a HISP Filter with Discriminative Deep Appearance Learning",
    "authors": [
      "Nathanael L. Baisa"
    ],
    "author_ids": [],
    "abstract": "We propose a novel online multi-target visual tracker based on the recently\ndeveloped Hypothesized and Independent Stochastic Population (HISP) filter. The\nHISP filter combines advantages of traditional tracking approaches like MHT and\npoint-process-based approaches like PHD filter, and it has linear complexity\nwhile maintaining track identities. We apply this filter for tracking multiple\ntargets in video sequences acquired under varying environmental conditions and\ntargets density using a tracking-by-detection approach. We also adopt deep CNN\nappearance representation by training a verification-identification network\n(VerIdNet) on large-scale person re-identification data sets. We construct an\naugmented likelihood in a principled manner using this deep CNN appearance\nfeatures and spatio-temporal information. Furthermore, we solve the problem of\ntwo or more targets having identical label considering the weight propagated\nwith each confirmed hypothesis. Extensive experiments on MOT16 and MOT17\nbenchmark data sets show that our tracker significantly outperforms several\nstate-of-the-art trackers in terms of tracking accuracy.",
    "published_date": "2019-08-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03945v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06166v1",
    "title": "A Mulching Proposal",
    "authors": [
      "Os Keyes",
      "Jevan Hutson",
      "Meredith Durbin"
    ],
    "author_ids": [],
    "abstract": "he ethical implications of algorithmic systems have been much discussed in\nboth HCI and the broader community of those interested in technology design,\ndevelopment and policy. In this paper, we explore the application of one\nprominent ethical framework - Fairness, Accountability, and Transparency - to a\nproposed algorithm that resolves various societal issues around food security\nand population ageing. Using various standardised forms of algorithmic audit\nand evaluation, we drastically increase the algorithm's adherence to the FAT\nframework, resulting in a more ethical and beneficent system. We discuss how\nthis might serve as a guide to other researchers or practitioners looking to\nensure better ethical outcomes from algorithmic systems in their line of work.",
    "published_date": "2019-08-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06166v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.03637v2",
    "title": "Physical Layer Secret Key Generation in Static Environments",
    "authors": [
      "Nasser Aldaghri",
      "Hessam Mahdavifar"
    ],
    "author_ids": [],
    "abstract": "Two legitimate parties, referred to as Alice and Bob, wish to generate secret\nkeys from the wireless channel in the presence of an eavesdropper, referred to\nas Eve, in order to use such keys for encryption and decryption. In general,\nthe secret key rate highly depends on the coherence time of the channel. In\nparticular, a straightforward method of generating secret keys in static\nenvironments results in ultra-low rates. In order to resolve this problem, we\nintroduce a low-complexity method called induced randomness. In this method,\nAlice and Bob independently generate local randomness to be used together with\nthe uniqueness of the wireless channel coefficients in order to enable\nhigh-rate secret key generation. In this work, two scenarios are considered:\nfirst, when Alice and Bob share a direct communication channel, and second,\nwhen Alice and Bob do not have a direct link and communicate through an\nuntrusted relay. After exchanging the induced randomness, post-processing is\ndone by Alice and Bob to generate highly-correlated samples that are used for\nthe key generation. Such samples are then converted into bits, disparities\nbetween the sequences generated by Alice and Bob are mitigated, and the\nresulting sequences are then hashed to compensate for the information leakage\nto the eavesdropper and to allow consistency checking of the generated key bit\nsequences. We utilize semantic security measures and information-theoretic\ninequalities to upper bound the probability of successful eavesdropping attack\nin terms of the mutual information measures that can be numerically computed.\nGiven certain reasonable system parameters this bound is numerically evaluated\nto be $2^{-31}$ and $2^{-10.57}$ in the first and the second scenario,\nrespectively.",
    "published_date": "2019-08-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03637v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.03491v1",
    "title": "Bayesian Inference for Large Scale Image Classification",
    "authors": [
      "Jonathan Heek",
      "Nal Kalchbrenner"
    ],
    "author_ids": [],
    "abstract": "Bayesian inference promises to ground and improve the performance of deep\nneural networks. It promises to be robust to overfitting, to simplify the\ntraining procedure and the space of hyperparameters, and to provide a\ncalibrated measure of uncertainty that can enhance decision making, agent\nexploration and prediction fairness. Markov Chain Monte Carlo (MCMC) methods\nenable Bayesian inference by generating samples from the posterior distribution\nover model parameters. Despite the theoretical advantages of Bayesian inference\nand the similarity between MCMC and optimization methods, the performance of\nsampling methods has so far lagged behind optimization methods for large scale\ndeep learning tasks. We aim to fill this gap and introduce ATMC, an adaptive\nnoise MCMC algorithm that estimates and is able to sample from the posterior of\na neural network. ATMC dynamically adjusts the amount of momentum and noise\napplied to each parameter update in order to compensate for the use of\nstochastic gradients. We use a ResNet architecture without batch normalization\nto test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark\nand show that, despite the absence of batch normalization, ATMC outperforms a\nstrong optimization baseline in terms of both classification accuracy and test\nlog-likelihood. We show that ATMC is intrinsically robust to overfitting on the\ntraining data and that ATMC provides a better calibrated measure of uncertainty\ncompared to the optimization baseline.",
    "published_date": "2019-08-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03491v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.03454v3",
    "title": "Bias and variance reduction and denoising for CTF Estimation",
    "authors": [
      "Ayelet Heimowitz",
      "Joakim Andén",
      "Amit Singer"
    ],
    "author_ids": [],
    "abstract": "When using an electron microscope for imaging of particles embedded in\nvitreous ice, the objective lens will inevitably corrupt the projection images.\nThis corruption manifests as a band-pass filter on the micrograph. In addition,\nit causes the phase of several frequency bands to be flipped and distorts\nfrequency bands. As a precursor to compensating for this distortion, the\ncorrupting point spread function, which is termed the contrast transfer\nfunction (CTF) in reciprocal space, must be estimated. In this paper, we will\npresent a novel method for CTF estimation. Our method is based on the\nmulti-taper method for power spectral density estimation, which aims to reduce\nthe bias and variance of the estimator. Furthermore, we use known properties of\nthe CTF and of the background of the power spectrum to increase the accuracy of\nour estimation. We will show that the resulting estimates capture the\nzero-crossings of the CTF in the low-mid frequency range.",
    "published_date": "2019-08-09T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03454v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.04769v2",
    "title": "Graph Embedding Using Infomax for ASD Classification and Brain Functional Difference Detection",
    "authors": [
      "Xiaoxiao Li",
      "Nicha C. Dvornek",
      "Juntang Zhuang",
      "Pamela Ventola",
      "James Duncan"
    ],
    "author_ids": [],
    "abstract": "Significant progress has been made using fMRI to characterize the brain\nchanges that occur in ASD, a complex neuro-developmental disorder. However, due\nto the high dimensionality and low signal-to-noise ratio of fMRI, embedding\ninformative and robust brain regional fMRI representations for both graph-level\nclassification and region-level functional difference detection tasks between\nASD and healthy control (HC) groups is difficult. Here, we model the whole\nbrain fMRI as a graph, which preserves geometrical and temporal information and\nuse a Graph Neural Network (GNN) to learn from the graph-structured fMRI data.\nWe investigate the potential of including mutual information (MI) loss\n(Infomax), which is an unsupervised term encouraging large MI of each nodal\nrepresentation and its corresponding graph-level summarized representation to\nlearn a better graph embedding. Specifically, this work developed a pipeline\nincluding a GNN encoder, a classifier and a discriminator, which forces the\nencoded nodal representations to both benefit classification and reveal the\ncommon nodal patterns in a graph. We simultaneously optimize graph-level\nclassification loss and Infomax. We demonstrated that Infomax graph embedding\nimproves classification performance as a regularization term. Furthermore, we\nfound separable nodal representations of ASD and HC groups in prefrontal\ncortex, cingulate cortex, visual regions, and other social, emotional and\nexecution related brain regions. In contrast with GNN with classification loss\nonly, the proposed pipeline can facilitate training more robust ASD\nclassification models. Moreover, the separable nodal representations can detect\nthe functional differences between the two groups and contribute to revealing\nnew ASD biomarkers.",
    "published_date": "2019-08-09T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04769v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.03263v1",
    "title": "Trajectory-wise Control Variates for Variance Reduction in Policy Gradient Methods",
    "authors": [
      "Ching-An Cheng",
      "Xinyan Yan",
      "Byron Boots"
    ],
    "author_ids": [],
    "abstract": "Policy gradient methods have demonstrated success in reinforcement learning\ntasks that have high-dimensional continuous state and action spaces. However,\npolicy gradient methods are also notoriously sample inefficient. This can be\nattributed, at least in part, to the high variance in estimating the gradient\nof the task objective with Monte Carlo methods. Previous research has\nendeavored to contend with this problem by studying control variates (CVs) that\ncan reduce the variance of estimates without introducing bias, including the\nearly use of baselines, state dependent CVs, and the more recent state-action\ndependent CVs. In this work, we analyze the properties and drawbacks of\nprevious CV techniques and, surprisingly, we find that these works have\noverlooked an important fact that Monte Carlo gradient estimates are generated\nby trajectories of states and actions. We show that ignoring the correlation\nacross the trajectories can result in suboptimal variance reduction, and we\npropose a simple fix: a class of \"trajectory-wise\" CVs, that can further drive\ndown the variance. We show that constructing trajectory-wise CVs can be done\nrecursively and requires only learning state-action value functions like the\nprevious CVs for policy gradient. We further prove that the proposed\ntrajectory-wise CVs are optimal for variance reduction under reasonable\nassumptions.",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03263v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.03156v2",
    "title": "Optimal multiclass overfitting by sequence reconstruction from Hamming queries",
    "authors": [
      "Jayadev Acharya",
      "Ananda Theertha Suresh"
    ],
    "author_ids": [],
    "abstract": "A primary concern of excessive reuse of test datasets in machine learning is\nthat it can lead to overfitting. Multiclass classification was recently shown\nto be more resistant to overfitting than binary classification. In an open\nproblem of COLT 2019, Feldman, Frostig, and Hardt ask to characterize the\ndependence of the amount of overfitting bias with the number of classes $m$,\nthe number of accuracy queries $k$, and the number of examples in the dataset\n$n$. We resolve this problem and determine the amount of overfitting possible\nin multi-class classification. We provide computationally efficient algorithms\nthat achieve overfitting bias of $\\tilde{\\Theta}(\\max\\{\\sqrt{{k}/{(mn)}},\nk/n\\})$, matching the known upper bounds.",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03156v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06165v1",
    "title": "Oxford Handbook on AI Ethics Book Chapter on Race and Gender",
    "authors": [
      "Timnit Gebru"
    ],
    "author_ids": [],
    "abstract": "From massive face-recognition-based surveillance and machine-learning-based\ndecision systems predicting crime recidivism rates, to the move towards\nautomated health diagnostic systems, artificial intelligence (AI) is being used\nin scenarios that have serious consequences in people's lives. However, this\nrapid permeation of AI into society has not been accompanied by a thorough\ninvestigation of the sociopolitical issues that cause certain groups of people\nto be harmed rather than advantaged by it. For instance, recent studies have\nshown that commercial face recognition systems have much higher error rates for\ndark skinned women while having minimal errors on light skinned men. A 2016\nProPublica investigation uncovered that machine learning based tools that\nassess crime recidivism rates in the US are biased against African Americans.\nOther studies show that natural language processing tools trained on newspapers\nexhibit societal biases (e.g. finishing the analogy \"Man is to computer\nprogrammer as woman is to X\" by homemaker). At the same time, books such as\nWeapons of Math Destruction and Automated Inequality detail how people in lower\nsocioeconomic classes in the US are subjected to more automated decision making\ntools than those who are in the upper class. Thus, these tools are most often\nused on people towards whom they exhibit the most bias. While many technical\nsolutions have been proposed to alleviate bias in machine learning systems, we\nhave to take a holistic and multifaceted approach. This includes\nstandardization bodies determining what types of systems can be used in which\nscenarios, making sure that automated decision tools are created by people from\ndiverse backgrounds, and understanding the historical and political factors\nthat disadvantage certain groups who are subjected to these tools.",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06165v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.03106v3",
    "title": "\"Conservatives Overfit, Liberals Underfit\": The Social-Psychological Control of Affect and Uncertainty",
    "authors": [
      "Jesse Hoey",
      "Neil J. MacKinnon"
    ],
    "author_ids": [],
    "abstract": "The presence of artificial agents in human social networks is growing. From\nchatbots to robots, human experience in the developed world is moving towards a\nsocio-technical system in which agents can be technological or biological, with\nincreasingly blurred distinctions between. Given that emotion is a key element\nof human interaction, enabling artificial agents with the ability to reason\nabout affect is a key stepping stone towards a future in which technological\nagents and humans can work together. This paper presents work on building\nintelligent computational agents that integrate both emotion and cognition.\nThese agents are grounded in the well-established social-psychological Bayesian\nAffect Control Theory (BayesAct). The core idea of BayesAct is that humans are\nmotivated in their social interactions by affective alignment: they strive for\ntheir social experiences to be coherent at a deep, emotional level with their\nsense of identity and general world views as constructed through culturally\nshared symbols. This affective alignment creates cohesive bonds between group\nmembers, and is instrumental for collaborations to solidify as relational group\ncommitments. BayesAct agents are motivated in their social interactions by a\ncombination of affective alignment and decision theoretic reasoning, trading\nthe two off as a function of the uncertainty or unpredictability of the\nsituation. This paper provides a high-level view of dual process theories and\nadvances BayesAct as a plausible, computationally tractable model based in\nsocial-psychological theory. We introduce a revised BayesAct model that more\ndeeply integrates social-psychological theorising, and we demonstrate a\ncomponent of the model as being sufficient to account for cognitive biases\nabout fairness, dissonance and conformity. We show how the model can unify\ndifferent exploration strategies in reinforcement learning.",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03106v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.03093v3",
    "title": "ExtremeC3Net: Extreme Lightweight Portrait Segmentation Networks using Advanced C3-modules",
    "authors": [
      "Hyojin Park",
      "Lars Lowe Sjösund",
      "YoungJoon Yoo",
      "Jihwan Bang",
      "Nojun Kwak"
    ],
    "author_ids": [],
    "abstract": "Designing a lightweight and robust portrait segmentation algorithm is an\nimportant task for a wide range of face applications. However, the problem has\nbeen considered as a subset of the object segmentation problem. bviously,\nportrait segmentation has its unique requirements. First, because the portrait\nsegmentation is performed in the middle of a whole process of many realworld\napplications, it requires extremely lightweight models. Second, there has not\nbeen any public datasets in this domain that contain a sufficient number of\nimages with unbiased statistics. To solve the problems, we introduce a new\nextremely lightweight portrait segmentation model consisting of a two-branched\narchitecture based on the concentrated-comprehensive convolutions block. Our\nmethod reduces the number of parameters from 2.1M to 37.7K (around 98.2%\nreduction), while maintaining the accuracy within a 1% margin from the\nstate-of-the-art portrait segmentation method. In our qualitative and\nquantitative analysis on the EG1800 dataset, we show that our method\noutperforms various existing lightweight segmentation models. Second, we\npropose a simple method to create additional portrait segmentation data which\ncan improve accuracy on the EG1800 dataset. Also, we analyze the bias in public\ndatasets by additionally annotating race, gender, and age on our own. The\naugmented dataset, the additional annotations and code are available in\nhttps://github.com/HYOJINPARK/ExtPortraitSeg .",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.03093v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02983v5",
    "title": "Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning",
    "authors": [
      "Eric Arazo",
      "Diego Ortego",
      "Paul Albert",
      "Noel E. O'Connor",
      "Kevin McGuinness"
    ],
    "author_ids": [],
    "abstract": "Semi-supervised learning, i.e. jointly learning from labeled and unlabeled\nsamples, is an active research topic due to its key role on relaxing human\nsupervision. In the context of image classification, recent advances to learn\nfrom unlabeled samples are mainly focused on consistency regularization methods\nthat encourage invariant predictions for different perturbations of unlabeled\nsamples. We, conversely, propose to learn from unlabeled data by generating\nsoft pseudo-labels using the network predictions. We show that a naive\npseudo-labeling overfits to incorrect pseudo-labels due to the so-called\nconfirmation bias and demonstrate that mixup augmentation and setting a minimum\nnumber of labeled samples per mini-batch are effective regularization\ntechniques for reducing it. The proposed approach achieves state-of-the-art\nresults in CIFAR-10/100, SVHN, and Mini-ImageNet despite being much simpler\nthan other methods. These results demonstrate that pseudo-labeling alone can\noutperform consistency regularization methods, while the opposite was supposed\nin previous work. Source code is available at https://git.io/fjQsC.",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02983v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02946v1",
    "title": "Bootstrapping a stable computation token",
    "authors": [
      "Jason Teutsch",
      "Sami Mäkelä",
      "Surya Bakshi"
    ],
    "author_ids": [],
    "abstract": "We outline a token model for Truebit, a retrofitting, blockchain enhancement\nwhich enables secure, community-based computation. The model addresses the\nchallenge of stable task pricing, as raised in the Truebit whitepaper, without\nappealing to external oracles, exchanges, or hierarchical nodes. The system's\nsustainable economics and fair market pricing derive from a mintable token\nformat which leverages existing tokens for liquidity. Finally, we introduce a\ngovernance layer whose lifecycles culminates with permanent dissolution into\nutility tokens, thereby tending the network towards autonomous\ndecentralization.",
    "published_date": "2019-08-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.GT",
      "econ.TH",
      "68M12, 68M14, 91B24"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02946v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.02810v1",
    "title": "Debiasing Embeddings for Reduced Gender Bias in Text Classification",
    "authors": [
      "Flavien Prost",
      "Nithum Thain",
      "Tolga Bolukbasi"
    ],
    "author_ids": [],
    "abstract": "(Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can\ninherit gender bias from the data they were trained on. We investigate how this\nbias affects downstream classification tasks, using the case study of\noccupation classification (De-Arteaga et al.,2019). We show that traditional\ntechniques for debiasing embeddings can actually worsen the bias of the\ndownstream classifier by providing a less noisy channel for communicating\ngender information. With a relatively minor adjustment, however, we show how\nthese same techniques can be used to simultaneously reduce bias and maintain\nhigh classification accuracy.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02810v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02723v1",
    "title": "Advocacy Learning: Learning through Competition and Class-Conditional Representations",
    "authors": [
      "Ian Fox",
      "Jenna Wiens"
    ],
    "author_ids": [],
    "abstract": "We introduce advocacy learning, a novel supervised training scheme for\nattention-based classification problems. Advocacy learning relies on a\nframework consisting of two connected networks: 1) $N$ Advocates (one for each\nclass), each of which outputs an argument in the form of an attention map over\nthe input, and 2) a Judge, which predicts the class label based on these\narguments. Each Advocate produces a class-conditional representation with the\ngoal of convincing the Judge that the input example belongs to their class,\neven when the input belongs to a different class. Applied to several different\nclassification tasks, we show that advocacy learning can lead to small\nimprovements in classification accuracy over an identical supervised baseline.\nThough a series of follow-up experiments, we analyze when and how such\nclass-conditional representations improve discriminative performance. Though\nsomewhat counter-intuitive, a framework in which subnetworks are trained to\ncompetitively provide evidence in support of their class shows promise, in many\ncases performing on par with standard learning approaches. This provides a\nfoundation for further exploration into competition and class-conditional\nrepresentations in supervised learning.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02723v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02641v2",
    "title": "Paired-Consistency: An Example-Based Model-Agnostic Approach to Fairness Regularization in Machine Learning",
    "authors": [
      "Yair Horesh",
      "Noa Haas",
      "Elhanan Mishraky",
      "Yehezkel S. Resheff",
      "Shir Meir Lador"
    ],
    "author_ids": [],
    "abstract": "As AI systems develop in complexity it is becoming increasingly hard to\nensure non-discrimination on the basis of protected attributes such as gender,\nage, and race. Many recent methods have been developed for dealing with this\nissue as long as the protected attribute is explicitly available for the\nalgorithm. We address the setting where this is not the case (with either no\nexplicit protected attribute, or a large set of them). Instead, we assume the\nexistence of a fair domain expert capable of generating an extension to the\nlabeled dataset - a small set of example pairs, each having a different value\non a subset of protected variables, but judged to warrant a similar model\nresponse. We define a performance metric - paired consistency. Paired\nconsistency measures how close the output (assigned by a classifier or a\nregressor) is on these carefully selected pairs of examples for which fairness\ndictates identical decisions. In some cases consistency can be embedded within\nthe loss function during optimization and serve as a fairness regularizer, and\nin others it is a tool for fair model selection. We demonstrate our method\nusing the well studied Income Census dataset.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02641v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02632v3",
    "title": "Scene-based Factored Attention for Image Captioning",
    "authors": [
      "Chen Shen",
      "Rongrong Ji",
      "Fuhai Chen",
      "Xiaoshuai Sun",
      "Xiangming Li"
    ],
    "author_ids": [],
    "abstract": "Image captioning has attracted ever-increasing research attention in the\nmultimedia community. To this end, most cutting-edge works rely on an\nencoder-decoder framework with attention mechanisms, which have achieved\nremarkable progress. However, such a framework does not consider scene concepts\nto attend visual information, which leads to sentence bias in caption\ngeneration and defects the performance correspondingly. We argue that such\nscene concepts capture higher-level visual semantics and serve as an important\ncue in describing images. In this paper, we propose a novel scene-based\nfactored attention module for image captioning. Specifically, the proposed\nmodule first embeds the scene concepts into factored weights explicitly and\nattends the visual information extracted from the input image. Then, an\nadaptive LSTM is used to generate captions for specific scene types.\nExperimental results on Microsoft COCO benchmark show that the proposed\nscene-based attention module improves model performance a lot, which\noutperforms the state-of-the-art approaches under various evaluation metrics.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02632v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02627v1",
    "title": "Speculative Execution for Guided Visual Analytics",
    "authors": [
      "Fabian Sperrle",
      "Jürgen Bernard",
      "Michael Sedlmair",
      "Daniel Keim",
      "Mennatallah El-Assady"
    ],
    "author_ids": [],
    "abstract": "We propose the concept of Speculative Execution for Visual Analytics and\ndiscuss its effectiveness for model exploration and optimization. Speculative\nExecution enables the automatic generation of alternative, competing model\nconfigurations that do not alter the current model state unless explicitly\nconfirmed by the user. These alternatives are computed based on either user\ninteractions or model quality measures and can be explored using\ndelta-visualizations. By automatically proposing modeling alternatives, systems\nemploying Speculative Execution can shorten the gap between users and models,\nreduce the confirmation bias and speed up optimization processes. In this\npaper, we have assembled five application scenarios showcasing the potential of\nSpeculative Execution, as well as a potential for further research.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02627v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.02624v1",
    "title": "A 20-Year Community Roadmap for Artificial Intelligence Research in the US",
    "authors": [
      "Yolanda Gil",
      "Bart Selman"
    ],
    "author_ids": [],
    "abstract": "Decades of research in artificial intelligence (AI) have produced formidable\ntechnologies that are providing immense benefit to industry, government, and\nsociety. AI systems can now translate across multiple languages, identify\nobjects in images and video, streamline manufacturing processes, and control\ncars. The deployment of AI systems has not only created a trillion-dollar\nindustry that is projected to quadruple in three years, but has also exposed\nthe need to make AI systems fair, explainable, trustworthy, and secure. Future\nAI systems will rightfully be expected to reason effectively about the world in\nwhich they (and people) operate, handling complex tasks and responsibilities\neffectively and ethically, engaging in meaningful communication, and improving\ntheir awareness through experience.\n  Achieving the full potential of AI technologies poses research challenges\nthat require a radical transformation of the AI research enterprise,\nfacilitated by significant and sustained investment. These are the major\nrecommendations of a recent community effort coordinated by the Computing\nCommunity Consortium and the Association for the Advancement of Artificial\nIntelligence to formulate a Roadmap for AI research and development over the\nnext two decades.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02408v2",
    "title": "Analytical Performance Models for NoCs with Multiple Priority Traffic Classes",
    "authors": [
      "Sumit K. Mandal",
      "Raid Ayoub",
      "Michael Kishinevsky",
      "Umit Y. Ogras"
    ],
    "author_ids": [],
    "abstract": "Networks-on-chip (NoCs) have become the standard for interconnect solutions\nin industrial designs ranging from client CPUs to many-core\nchip-multiprocessors. Since NoCs play a vital role in system performance and\npower consumption, pre-silicon evaluation environments include cycle-accurate\nNoC simulators. Long simulations increase the execution time of evaluation\nframeworks, which are already notoriously slow, and prohibit design-space\nexploration. Existing analytical NoC models, which assume fair arbitration,\ncannot replace these simulations since industrial NoCs typically employ\npriority schedulers and multiple priority classes. To address this limitation,\nwe propose a systematic approach to construct priority-aware analytical\nperformance models using micro-architecture specifications and input traffic.\nOur approach consists of developing two novel transformations of queuing system\nand designing an algorithm which iteratively uses these two transformations to\nestimate end-to-end latency. Our approach decomposes the given NoC into\nindividual queues with modified service time to enable accurate and scalable\nlatency computations. Specifically, we introduce novel transformations along\nwith an algorithm that iteratively applies these transformations to decompose\nthe queuing system. Experimental evaluations using real architectures and\napplications show high accuracy of 97% and up to 2.5x speedup in full-system\nsimulation.",
    "published_date": "2019-08-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.PF",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02408v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.02338v2",
    "title": "Modelling Segmented Cardiotocography Time-Series Signals Using One-Dimensional Convolutional Neural Networks for the Early Detection of Abnormal Birth Outcomes",
    "authors": [
      "Paul Fergus",
      "Carl Chalmers",
      "Casimiro Curbelo Montanez",
      "Denis Reilly",
      "Paulo Lisboa",
      "Beth Pineles"
    ],
    "author_ids": [],
    "abstract": "Gynaecologists and obstetricians visually interpret cardiotocography (CTG)\ntraces using the International Federation of Gynaecology and Obstetrics (FIGO)\nguidelines to assess the wellbeing of the foetus during antenatal care. This\napproach has raised concerns among professionals with regards to inter- and\nintra-variability where clinical diagnosis only has a 30\\% positive predictive\nvalue when classifying pathological outcomes. Machine learning models, trained\nwith FIGO and other user derived features extracted from CTG traces, have been\nshown to increase positive predictive capacity and minimise variability. This\nis only possible however when class distributions are equal which is rarely the\ncase in clinical trials where case-control observations are heavily skewed in\nfavour of normal outcomes. Classes can be balanced using either synthetic data\nderived from resampled case training data or by decreasing the number of\ncontrol instances. However, this either introduces bias or removes valuable\ninformation. Concerns have also been raised regarding machine learning studies\nand their reliance on manually handcrafted features. While this has led to some\ninteresting results, deriving an optimal set of features is considered to be an\nart as well as a science and is often an empirical and time consuming process.\nIn this paper, we address both of these issues and propose a novel CTG analysis\nmethodology that a) splits CTG time-series signals into n-size windows with\nequal class distributions, and b) automatically extracts features from\ntime-series windows using a one dimensional convolutional neural network\n(1DCNN) and multilayer perceptron (MLP) ensemble. Collectively, the proposed\napproach normally distributes classes and removes the need to handcrafted\nfeatures from CTG traces.",
    "published_date": "2019-08-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02338v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02334v1",
    "title": "Predicted disease compositions of human gliomas estimated from multiparametric MRI can predict endothelial proliferation, tumor grade, and overall survival",
    "authors": [
      "Emily E Diller",
      "Sha Cao",
      "Beth Ey",
      "Robert Lober",
      "Jason G Parker"
    ],
    "author_ids": [],
    "abstract": "Background and Purpose: Biopsy is the main determinants of glioma clinical\nmanagement, but require invasive sampling that fail to detect relevant features\nbecause of tumor heterogeneity. The purpose of this study was to evaluate the\naccuracy of a voxel-wise, multiparametric MRI radiomic method to predict\nfeatures and develop a minimally invasive method to objectively assess\nneoplasms.\n  Methods: Multiparametric MRI were registered to T1-weighted gadolinium\ncontrast-enhanced data using a 12 degree-of-freedom affine model. The\nretrospectively collected MRI data included T1-weighted, T1-weighted gadolinium\ncontrast-enhanced, T2-weighted, fluid attenuated inversion recovery, and\nmulti-b-value diffusion-weighted acquired at 1.5T or 3.0T. Clinical experts\nprovided voxel-wise annotations for five disease states on a subset of patients\nto establish a training feature vector of 611,930 observations. Then, a\nk-nearest-neighbor (k-NN) classifier was trained using a 25% hold-out design.\nThe trained k-NN model was applied to 13,018,171 observations from seventeen\nhistologically confirmed glioma patients. Linear regression tested overall\nsurvival (OS) relationship to predicted disease compositions (PDC) and\ndiagnostic age (alpha = 0.05). Canonical discriminant analysis tested if PDC\nand diagnostic age could differentiate clinical, genetic, and microscopic\nfactors (alpha = 0.05).\n  Results: The model predicted voxel annotation class with a Dice similarity\ncoefficient of 94.34% +/- 2.98. Linear combinations of PDCs and diagnostic age\npredicted OS (p = 0.008), grade (p = 0.014), and endothelia proliferation (p =\n0.003); but fell short predicting gene mutations for TP53BP1 and IDH1.\n  Conclusions: This voxel-wise, multi-parametric MRI radiomic strategy holds\npotential as a non-invasive decision-making aid for clinicians managing\npatients with glioma.",
    "published_date": "2019-08-06T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "eess.IV",
      "physics.med-ph",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02334v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02269v4",
    "title": "Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning",
    "authors": [
      "Julien Roy",
      "Paul Barde",
      "Félix G. Harvey",
      "Derek Nowrouzezahrai",
      "Christopher Pal"
    ],
    "author_ids": [],
    "abstract": "In multi-agent reinforcement learning, discovering successful collective\nbehaviors is challenging as it requires exploring a joint action space that\ngrows exponentially with the number of agents. While the tractability of\nindependent agent-wise exploration is appealing, this approach fails on tasks\nthat require elaborate group strategies. We argue that coordinating the agents'\npolicies can guide their exploration and we investigate techniques to promote\nsuch an inductive bias. We propose two policy regularization methods: TeamReg,\nwhich is based on inter-agent action predictability and CoachReg that relies on\nsynchronized behavior selection. We evaluate each approach on four challenging\ncontinuous control tasks with sparse rewards that require varying levels of\ncoordination as well as on the discrete action Google Research Football\nenvironment. Our experiments show improved performance across many cooperative\nmulti-agent problems. Finally, we analyze the effects of our proposed methods\non the policies that our agents learn and show that our methods successfully\nenforce the qualities that we propose as proxies for coordinated behaviors.",
    "published_date": "2019-08-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.MA",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02269v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02123v1",
    "title": "Addressing Data Bias Problems for Chest X-ray Image Report Generation",
    "authors": [
      "Philipp Harzig",
      "Yan-Ying Chen",
      "Francine Chen",
      "Rainer Lienhart"
    ],
    "author_ids": [],
    "abstract": "Automatic medical report generation from chest X-ray images is one\npossibility for assisting doctors to reduce their workload. However, the\ndifferent patterns and data distribution of normal and abnormal cases can bias\nmachine learning models. Previous attempts did not focus on isolating the\ngeneration of the abnormal and normal sentences in order to increase the\nvariability of generated paragraphs. To address this, we propose to separate\nabnormal and normal sentence generation by using two different word LSTMs in a\nhierarchical LSTM model. We conduct an analysis on the distinctiveness of\ngenerated sentences compared to the BLEU score, which increases when less\ndistinct reports are generated. We hope our findings will help to encourage the\ndevelopment of new metrics to better verify methods of automatic medical report\ngeneration.",
    "published_date": "2019-08-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02123v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.01925v2",
    "title": "Attract or Distract: Exploit the Margin of Open Set",
    "authors": [
      "Qianyu Feng",
      "Guoliang Kang",
      "Hehe Fan",
      "Yi Yang"
    ],
    "author_ids": [],
    "abstract": "Open set domain adaptation aims to diminish the domain shift across domains,\nwith partially shared classes. There exist unknown target samples out of the\nknowledge of source domain. Compared to the close set setting, how to separate\nthe unknown (unshared) class from the known (shared) ones plays a key role.\nWhereas, previous methods did not emphasize the semantic structure of the open\nset data, which may introduce bias into the domain alignment and confuse the\nclassifier around the decision boundary. In this paper, we exploit the semantic\nstructure of open set data from two aspects: 1) Semantic Categorical Alignment,\nwhich aims to achieve good separability of target known classes by\ncategorically aligning the centroid of target with the source. 2)Semantic\nContrastive Mapping, which aims to push the unknown class away from the\ndecision boundary. Empirically, we demonstrate that our method performs\nfavourably against the state-of-the-art methods on representative benchmarks,\ne.g. Digit datasets and Office-31 datasets.",
    "published_date": "2019-08-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.01925v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.01875v2",
    "title": "Animal Wildlife Population Estimation Using Social Media Images Collections",
    "authors": [
      "Matteo Foglio",
      "Lorenzo Semeria",
      "Guido Muscioni",
      "Riccardo Pressiani",
      "Tanya Berger-Wolf"
    ],
    "author_ids": [],
    "abstract": "We are losing biodiversity at an unprecedented scale and in many cases, we do\nnot even know the basic data for the species. Traditional methods for wildlife\nmonitoring are inadequate. Development of new computer vision tools enables the\nuse of images as the source of information about wildlife. Social media is the\nrich source of wildlife images, which come with a huge bias, thus thwarting\ntraditional population size estimate approaches. Here, we present a new\nframework to take into account the social media bias when using this data\nsource to provide wildlife population size estimates. We show that,\nsurprisingly, this is a learnable and potentially solvable problem.",
    "published_date": "2019-08-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.01875v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.01718v2",
    "title": "Discovery of Bias and Strategic Behavior in Crowdsourced Performance Assessment",
    "authors": [
      "Yifei Huang",
      "Matt Shum",
      "Xi Wu",
      "Jason Zezhong Xiao"
    ],
    "author_ids": [],
    "abstract": "With the industry trend of shifting from a traditional hierarchical approach\nto flatter management structure, crowdsourced performance assessment gained\nmainstream popularity. One fundamental challenge of crowdsourced performance\nassessment is the risks that personal interest can introduce distortions of\nfacts, especially when the system is used to determine merit pay or promotion.\nIn this paper, we developed a method to identify bias and strategic behavior in\ncrowdsourced performance assessment, using a rich dataset collected from a\nprofessional service firm in China. We find a pattern of \"discriminatory\ngenerosity\" on the part of peer evaluation, where raters downgrade their peer\ncoworkers who have passed objective promotion requirements while overrating\ntheir peer coworkers who have not yet passed. This introduces two types of\nbiases: the first aimed against more competent competitors, and the other\nfavoring less eligible peers which can serve as a mask of the first bias. This\npaper also aims to bring angles of fairness-aware data mining to talent and\nmanagement computing. Historical decision records, such as performance ratings,\noften contain subjective judgment which is prone to bias and strategic\nbehavior. For practitioners of predictive talent analytics, it is important to\ninvestigate potential bias and strategic behavior underlying historical\ndecision records.",
    "published_date": "2019-08-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "econ.EM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.01718v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02598v3",
    "title": "Criteria for assessing grant applications: A systematic review",
    "authors": [
      "Sven E. Hug",
      "Mirjam Aeschbach"
    ],
    "author_ids": [],
    "abstract": "Criteria are an essential component of any procedure for assessing merit.\nYet, little is known about the criteria peers use in assessing grant\napplications. In this systematic review we therefore identify and synthesize\nstudies that examine grant peer review criteria in an empirical and inductive\nmanner. To facilitate the synthesis, we introduce a framework that classifies\nwhat is generally referred to as 'criterion' into an evaluated entity (i.e. the\nobject of evaluation) and an evaluation criterion (i.e. the dimension along\nwhich an entity is evaluated). In total, this synthesis includes 12 studies.\nTwo-thirds of these studies examine criteria in the medical and health\nsciences, while studies in other fields are scarce. Few studies compare\ncriteria across different fields, and none focus on criteria for\ninterdisciplinary research. We conducted a qualitative content analysis of the\n12 studies and thereby identified 15 evaluation criteria and 30 evaluated\nentities as well as the relations between them. Based on a network analysis, we\npropose a conceptualization that groups the identified evaluation criteria and\nevaluated entities into aims, means, and outcomes. We compare our results to\ncriteria found in studies on research quality and guidelines of funding\nagencies. Since peer review is often approached from a normative perspective,\nwe discuss our findings in relation to two normative positions, the fairness\ndoctrine and the ideal of impartiality. Our findings suggest that future\nstudies on criteria in grant peer review should focus on the applicant, include\ndata from non-Western countries, and examine fields other than the medical and\nhealth sciences.",
    "published_date": "2019-08-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02598v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1912.05616v1",
    "title": "Ensuring Liveness Properties of Distributed Systems: Open Problems",
    "authors": [
      "Rob van Glabbeek"
    ],
    "author_ids": [],
    "abstract": "Often fairness assumptions need to be made in order to establish liveness\nproperties of distributed systems, but in many situations they lead to false\nconclusions. This document presents a research agenda aiming at laying the\nfoundations of a theory of concurrency that is equipped to ensure liveness\nproperties of distributed systems without making fairness assumptions. This\ntheory will encompass process algebra, temporal logic and semantic models. The\nagenda also includes the development of a methodology and tools that allow\nsuccessful application of this theory to the specification, analysis and\nverification of realistic distributed systems. Contemporary process algebras\nand temporal logics fail to make distinctions between systems of which one has\na crucial liveness property and the other does not, at least when assuming\njustness, a strong progress property, but not assuming fairness. Setting up an\nalternative framework involves giving up on identifying strongly bisimilar\nsystems, inventing new induction principles, developing new axiomatic bases for\nprocess algebras and new congruence formats for operational semantics, and\ncreating matching treatments of time and probability. Even simple systems like\nfair schedulers or mutual exclusion protocols cannot be accurately specified in\nstandard process algebras (or Petri nets) in the absence of fairness\nassumptions. Hence the work involves the study of adequate language or model\nextensions, and their expressive power.",
    "published_date": "2019-08-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LO",
      "F.3.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.05616v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.01024v3",
    "title": "What is the Point of Fairness? Disability, AI and The Complexity of Justice",
    "authors": [
      "Cynthia L. Bennett",
      "Os Keyes"
    ],
    "author_ids": [],
    "abstract": "Work integrating conversations around AI and Disability is vital and valued,\nparticularly when done through a lens of fairness. Yet at the same time,\nanalyzing the ethical implications of AI for disabled people solely through the\nlens of a singular idea of \"fairness\" risks reinforcing existing power\ndynamics, either through reinforcing the position of existing medical\ngatekeepers, or promoting tools and techniques that benefit\notherwise-privileged disabled people while harming those who are rendered\noutliers in multiple ways. In this paper we present two case studies from\nwithin computer vision - a subdiscipline of AI focused on training algorithms\nthat can \"see\" - of technologies putatively intended to help disabled people\nbut, through failures to consider structural injustices in their design, are\nlikely to result in harms not addressed by a \"fairness\" framing of ethics.\nDrawing on disability studies and critical data science, we call on researchers\ninto AI ethics and disability to move beyond simplistic notions of fairness,\nand towards notions of justice.",
    "published_date": "2019-08-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.01024v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.00831v1",
    "title": "Bias Disparity in Collaborative Recommendation: Algorithmic Evaluation and Comparison",
    "authors": [
      "Masoud Mansoury",
      "Bamshad Mobasher",
      "Robin Burke",
      "Mykola Pechenizkiy"
    ],
    "author_ids": [],
    "abstract": "Research on fairness in machine learning has been recently extended to\nrecommender systems. One of the factors that may impact fairness is bias\ndisparity, the degree to which a group's preferences on various item categories\nfail to be reflected in the recommendations they receive. In some cases biases\nin the original data may be amplified or reversed by the underlying\nrecommendation algorithm. In this paper, we explore how different\nrecommendation algorithms reflect the tradeoff between ranking quality and bias\ndisparity. Our experiments include neighborhood-based, model-based, and\ntrust-aware recommendation algorithms.",
    "published_date": "2019-08-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00831v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.00699v1",
    "title": "Efficiency Fairness Tradeoff in Battery Sharing",
    "authors": [
      "Karan N. Chadha",
      "Ankur A. Kulkarni",
      "Jayakrishnan Nair"
    ],
    "author_ids": [],
    "abstract": "The increasing presence of decentralized renewable generation in the power\ngrid has motivated consumers to install batteries to save excess energy for\nfuture use. The high price of energy storage calls for a shared storage system,\nbut careful battery management is required so that the battery is operated in a\nmanner that is fair to all and as efficiently as possible. In this paper, we\nstudy the tradeoffs between efficiency and fairness in operating a shared\nbattery. We develop a framework based on constrained Markov decision processes\nto study both regimes, namely, optimizing efficiency under a hard fairness\nconstraint and optimizing fairness under hard efficiency constraint. Our\nresults show that there are fundamental limits to efficiency under fairness and\nvice-versa, and, in general, the two cannot be achieved simultaneously. We\ncharacterize these fundamental limits via absolute bounds on these quantities,\nand via the notion of price of fairness that we introduce in this paper.",
    "published_date": "2019-08-02T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY",
      "60J27, 90B22, 60F10"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00699v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.00403v2",
    "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots",
    "authors": [
      "Yating Wei",
      "Honghui Mei",
      "Ying Zhao",
      "Shuyue Zhou",
      "Bingru Lin",
      "Haojing Jiang",
      "Wei Chen"
    ],
    "author_ids": [],
    "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and\nmulti-device data analysis environments. A common method used for scaling is to\nenlarge or shrink the entire scatterplot together with the inside points\nsynchronously and proportionally. This process is called geometric scaling.\nHowever, geometric scaling of scatterplots may cause a perceptual bias, that\nis, the perceived and physical values of visual features may be dissociated\nwith respect to geometric scaling. For example, if a scatterplot is projected\nfrom a laptop to a large projector screen, then observers may feel that the\nscatterplot shown on the projector has fewer points than that viewed on the\nlaptop. This paper presents an evaluation study on the perceptual bias of\nvisual features in scatterplots caused by geometric scaling. The study focuses\non three fundamental visual features (i.e., numerosity, correlation, and\ncluster separation) and three hypotheses that are formulated on the basis of\nour experience. We carefully design three controlled experiments by using\nwell-prepared synthetic data and recruit participants to complete the\nexperiments on the basis of their subjective experience. With a detailed\nanalysis of the experimental results, we obtain a set of instructive findings.\nFirst, geometric scaling causes a bias that has a linear relationship with the\nscale ratio. Second, no significant difference exists between the biases\nmeasured from normally and uniformly distributed scatterplots. Third, changing\nthe point radius can correct the bias to a certain extent. These findings can\nbe used to inspire the design decisions of scatterplots in various scenarios.",
    "published_date": "2019-08-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00403v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.00308v1",
    "title": "MSnet: A BERT-based Network for Gendered Pronoun Resolution",
    "authors": [
      "Zili Wang"
    ],
    "author_ids": [],
    "abstract": "The pre-trained BERT model achieves a remarkable state of the art across a\nwide range of tasks in natural language processing. For solving the gender bias\nin gendered pronoun resolution task, I propose a novel neural network model\nbased on the pre-trained BERT. This model is a type of mention score classifier\nand uses an attention mechanism with no parameters to compute the contextual\nrepresentation of entity span, and a vector to represent the triple-wise\nsemantic similarity among the pronoun and the entities. In stage 1 of the\ngendered pronoun resolution task, a variant of this model, trained in the\nfine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the\n5-fold cross-validation of training set and 0.2795 in testing set. Besides,\nthis variant won the 2nd place with a score at 0.17289 in stage 2 of the task.\nThe code in this paper is available at:\nhttps://github.com/ziliwang/MSnet-for-Gendered-PronounResolution",
    "published_date": "2019-08-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00308v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.00176v2",
    "title": "FairSight: Visual Analytics for Fairness in Decision Making",
    "authors": [
      "Yongsu Ahn",
      "Yu-Ru Lin"
    ],
    "author_ids": [],
    "abstract": "Data-driven decision making related to individuals has become increasingly\npervasive, but the issue concerning the potential discrimination has been\nraised by recent studies. In response, researchers have made efforts to propose\nand implement fairness measures and algorithms, but those efforts have not been\ntranslated to the real-world practice of data-driven decision making. As such,\nthere is still an urgent need to create a viable tool to facilitate fair\ndecision making. We propose FairSight, a visual analytic system to address this\nneed; it is designed to achieve different notions of fairness in ranking\ndecisions through identifying the required actions -- understanding, measuring,\ndiagnosing and mitigating biases -- that together lead to fairer decision\nmaking. Through a case study and user study, we demonstrate that the proposed\nvisual analytic and diagnostic modules in the system are effective in\nunderstanding the fairness-aware decision pipeline and obtaining more fair\noutcomes.",
    "published_date": "2019-08-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00176v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.00160v2",
    "title": "Max-Min Fairness Design for MIMO Interference Channels: a Minorization-Maximization Approach",
    "authors": [
      "Mohammad Mahdi Naghsh",
      "Maryam Masjedi",
      "Arman Adibi",
      "Petre Stoica"
    ],
    "author_ids": [],
    "abstract": "We address the problem of linear precoder (beamformer) design in a\nmultiple-input multiple-output interference channel (MIMO-IC). The aim is to\ndesign the transmit covariance matrices in order to achieve max-min utility\nfairness for all users. The corresponding optimization problem is non-convex\nand NP-hard in general. We devise an efficient algorithm based on the\nminorization-maximization (MM) technique to obtain quality solutions to this\nproblem. The proposed method solves a second-order cone convex program (SOCP)\nat each iteration. We prove that the devised method converges to stationary\npoints of the problem. We also extend our algorithm to the case where there are\nuncertainties in the noise covariance matrices or channel state information\n(CSI). Simulation results show the effectiveness of the proposed method\ncompared with its main competitor.",
    "published_date": "2019-08-01T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.SI",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00160v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.02134v1",
    "title": "Adapting SQuaRE for Quality Assessment of Artificial Intelligence Systems",
    "authors": [
      "Hiroshi Kuwajima",
      "Fuyuki Ishikawa"
    ],
    "author_ids": [],
    "abstract": "More and more software practitioners are tackling towards industrial\napplications of artificial intelligence (AI) systems, especially those based on\nmachine learning (ML). However, many of existing principles and approaches to\ntraditional systems do not work effectively for the system behavior obtained by\ntraining not by logical design. In addition, unique kinds of requirements are\nemerging such as fairness and explainability. To provide clear guidance to\nunderstand and tackle these difficulties, we present an analysis on what\nquality concepts we should evaluate for AI systems. We base our discussion on\nISO/IEC 25000 series, known as SQuaRE, and identify how it should be adapted\nfor the unique nature of ML and $\\textit{Ethics guidelines for trustworthy AI}$\nfrom European Commission. We thus provide holistic insights for quality of AI\nsystems by incorporating the ML nature and AI ethics to the traditional\nsoftware quality concepts.",
    "published_date": "2019-07-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG",
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02134v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.04348v1",
    "title": "What's in the box? Explaining the black-box model through an evaluation of its interpretable features",
    "authors": [
      "Francesco Ventura",
      "Tania Cerquitelli"
    ],
    "author_ids": [],
    "abstract": "Algorithms are powerful and necessary tools behind a large part of the\ninformation we use every day. However, they may introduce new sources of bias,\ndiscrimination and other unfair practices that affect people who are unaware of\nit. Greater algorithm transparency is indispensable to provide more credible\nand reliable services. Moreover, requiring developers to design transparent\nalgorithm-driven applications allows them to keep the model accessible and\nhuman understandable, increasing the trust of end users. In this paper we\npresent EBAnO, a new engine able to produce prediction-local explanations for a\nblack-box model exploiting interpretable feature perturbations. EBAnO exploits\nthe hypercolumns representation together with the cluster analysis to identify\na set of interpretable features of images. Furthermore two indices have been\nproposed to measure the influence of input features on the final prediction\nmade by a CNN model. EBAnO has been preliminarily tested on a set of\nheterogeneous images. The results highlight the effectiveness of EBAnO in\nexplaining the CNN classification through the evaluation of interpretable\nfeatures influence.",
    "published_date": "2019-07-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T30, 68U10",
      "I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04348v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.13349v2",
    "title": "Competing Ratio Loss for Discriminative Multi-class Image Classification",
    "authors": [
      "Ke Zhang",
      "Xinsheng Wang",
      "Yurong Guo",
      "Zhenbing Zhao",
      "Zhanyu Ma",
      "Tony X. Han"
    ],
    "author_ids": [],
    "abstract": "The development of deep convolutional neural network architecture is critical\nto the improvement of image classification task performance. A lot of studies\nof image classification based on deep convolutional neural network focus on the\nnetwork structure to improve the image classification performance. Contrary to\nthese studies, we focus on the loss function. Cross-entropy Loss (CEL) is\nwidely used for training a multi-class classification deep convolutional neural\nnetwork. While CEL has been successfully implemented in image classification\ntasks, it only focuses on the posterior probability of correct class when the\nlabels of training images are one-hot. It cannot be discriminated against the\nclasses not belong to correct class (wrong classes) directly. In order to solve\nthe problem of CEL, we propose Competing Ratio Loss (CRL), which calculates the\nposterior probability ratio between the correct class and competing wrong\nclasses to better discriminate the correct class from competing wrong classes,\nincreasing the difference between the negative log likelihood of the correct\nclass and the negative log likelihood of competing wrong classes, widening the\ndifference between the probability of the correct class and the probabilities\nof wrong classes. To demonstrate the effectiveness of our loss function, we\nperform some sets of experiments on different types of image classification\ndatasets, including CIFAR, SVHN, CUB200- 2011, Adience and ImageNet datasets.\nThe experimental results show the effectiveness and robustness of our loss\nfunction on different deep convolutional neural network architectures and\ndifferent image classification tasks, such as fine-grained image\nclassification, hard face age estimation and large-scale image classification.",
    "published_date": "2019-07-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.13349v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.13286v3",
    "title": "The Unfairness of Popularity Bias in Recommendation",
    "authors": [
      "Himan Abdollahpouri",
      "Masoud Mansoury",
      "Robin Burke",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are known to suffer from the popularity bias problem:\npopular (i.e. frequently rated) items get a lot of exposure while less popular\nones are under-represented in the recommendations. Research in this area has\nbeen mainly focusing on finding ways to tackle this issue by increasing the\nnumber of recommended long-tail items or otherwise the overall catalog\ncoverage. In this paper, however, we look at this problem from the users'\nperspective: we want to see how popularity bias causes the recommendations to\ndeviate from what the user expects to get from the recommender system. We\ndefine three different groups of users according to their interest in popular\nitems (Niche, Diverse and Blockbuster-focused) and show the impact of\npopularity bias on the users in each group. Our experimental results on a movie\ndataset show that in many recommendation algorithms the recommendations the\nusers get are extremely concentrated on popular items even if a user is\ninterested in long-tail and non-popular items showing an extreme bias\ndisparity.",
    "published_date": "2019-07-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.13286v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.13242v2",
    "title": "Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking",
    "authors": [
      "Tianyang Xu",
      "Zhen-Hua Feng",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "author_ids": [],
    "abstract": "We propose a new Group Feature Selection method for Discriminative\nCorrelation Filters (GFS-DCF) based visual object tracking. The key innovation\nof the proposed method is to perform group feature selection across both\nchannel and spatial dimensions, thus to pinpoint the structural relevance of\nmulti-channel features to the filtering system. In contrast to the widely used\nspatial regularisation or feature selection methods, to the best of our\nknowledge, this is the first time that channel selection has been advocated for\nDCF-based tracking. We demonstrate that our GFS-DCF method is able to\nsignificantly improve the performance of a DCF tracker equipped with deep\nneural network features. In addition, our GFS-DCF enables joint feature\nselection and filter learning, achieving enhanced discrimination and\ninterpretability of the learned filters.\n  To further improve the performance, we adaptively integrate historical\ninformation by constraining filters to be smooth across temporal frames, using\nan efficient low-rank approximation. By design, specific\ntemporal-spatial-channel configurations are dynamically learned in the tracking\nprocess, highlighting the relevant features, and alleviating the performance\ndegrading impact of less discriminative representations and reducing\ninformation redundancy. The experimental results obtained on OTB2013, OTB2015,\nVOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its\nsuperiority over the state-of-the-art trackers. The code is publicly available\nat https://github.com/XU-TIANYANG/GFS-DCF.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.13242v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.06877v1",
    "title": "Decentralising power: how we are trying to keep CALLector ethical",
    "authors": [
      "Cathy Chua",
      "Hanieh Habibi",
      "Manny Rayner",
      "Nikos Tsourakis"
    ],
    "author_ids": [],
    "abstract": "We present a brief overview of the CALLector project, and consider ethical\nquestions arising from its overall goal of creating a social network to support\ncreation and use of online CALL resources. We argue that these questions are\nbest addressed in a decentralised, pluralistic open source architecture.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.06877v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.13158v1",
    "title": "Multi-stakeholder Recommendation and its Connection to Multi-sided Fairness",
    "authors": [
      "Himan Abdollahpouri",
      "Robin Burke"
    ],
    "author_ids": [],
    "abstract": "There is growing research interest in recommendation as a multi-stakeholder\nproblem, one where the interests of multiple parties should be taken into\naccount. This category subsumes some existing well-established areas of\nrecommendation research including reciprocal and group recommendation, but a\ndetailed taxonomy of different classes of multi-stakeholder recommender systems\nis still lacking. Fairness-aware recommendation has also grown as a research\narea, but its close connection with multi-stakeholder recommendation is not\nalways recognized. In this paper, we define the most commonly observed classes\nof multi-stakeholder recommender systems and discuss how different fairness\nconcerns may come into play in such systems.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.13158v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.13075v1",
    "title": "Pay attention to the activations: a modular attention mechanism for fine-grained image recognition",
    "authors": [
      "Pau Rodríguez López",
      "Diego Velazquez Dorta",
      "Guillem Cucurull Preixens",
      "Josep M. Gonfaus",
      "F. Xavier Roca Marva",
      "Jordi Gonzàlez Sabaté"
    ],
    "author_ids": [],
    "abstract": "Fine-grained image recognition is central to many multimedia tasks such as\nsearch, retrieval and captioning. Unfortunately, these tasks are still\nchallenging since the appearance of samples of the same class can be more\ndifferent than those from different classes. Attention has been typically\nimplemented in neural networks by selecting the most informative regions of the\nimage that improve classification. In contrast, in this paper, attention is not\napplied at the image level but to the convolutional feature activations. In\nessence, with our approach, the neural model learns to attend to lower-level\nfeature activations without requiring part annotations and uses those\nactivations to update and rectify the output likelihood distribution. The\nproposed mechanism is modular, architecture-independent and efficient in terms\nof both parameters and computation required. Experiments demonstrate that\nwell-known networks such as Wide Residual Networks and ResNeXt, when augmented\nwith our approach, systematically improve their classification accuracy and\nbecome more robust to changes in deformation and pose and to the presence of\nclutter. As a result, our proposal reaches state-of-the-art classification\naccuracies in CIFAR-10, the Adience gender recognition task, Stanford Dogs, and\nUEC-Food100 while obtaining competitive performance in ImageNet, CIFAR-100,\nCUB200 Birds, and Stanford Cars. In addition, we analyze the different\ncomponents of our model, showing that the proposed attention modules succeed in\nfinding the most discriminative regions of the image. Finally, as a proof of\nconcept, we demonstrate that with only local predictions, an augmented neural\nnetwork can successfully classify an image before reaching any fully connected\nlayer, thus reducing the computational amount up to 10%.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.13075v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.13054v2",
    "title": "Grid Saliency for Context Explanations of Semantic Segmentation",
    "authors": [
      "Lukas Hoyer",
      "Mauricio Munoz",
      "Prateek Katiyar",
      "Anna Khoreva",
      "Volker Fischer"
    ],
    "author_ids": [],
    "abstract": "Recently, there has been a growing interest in developing saliency methods\nthat provide visual explanations of network predictions. Still, the usability\nof existing methods is limited to image classification models. To overcome this\nlimitation, we extend the existing approaches to generate grid saliencies,\nwhich provide spatially coherent visual explanations for (pixel-level) dense\nprediction networks. As the proposed grid saliency allows to spatially\ndisentangle the object and its context, we specifically explore its potential\nto produce context explanations for semantic segmentation networks, discovering\nwhich context most influences the class predictions inside a target object\narea. We investigate the effectiveness of grid saliency on a synthetic dataset\nwith an artificially induced bias between objects and their context as well as\non the real-world Cityscapes dataset using state-of-the-art segmentation\nnetworks. Our results show that grid saliency can be successfully used to\nprovide easily interpretable context explanations and, moreover, can be\nemployed for detecting and localizing contextual biases present in the data.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.13054v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.12892v1",
    "title": "Increasing Shape Bias in ImageNet-Trained Networks Using Transfer Learning and Domain-Adversarial Methods",
    "authors": [
      "Francis Brochu"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNNs) have become the state-of-the-art method\nto learn from image data. However, recent research shows that they may include\na texture and colour bias in their representation, contrary to the intuition\nthat they learn the shapes of the image content and to human biological\nlearning. Thus, recent works have attempted to increase the shape bias in CNNs\nin order to train more robust and accurate networks on tasks. One such approach\nuses style-transfer in order to remove texture clues from the data. This work\nreproduces this methodology on four image classification datasets, as well as\nextends the method to use domain-adversarial training in order to further\nincrease the shape bias in the learned representation. The results show the\nproposed method increases the robustness and shape bias of the CNNs, while it\ndoes not provide a gain in accuracy.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12892v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1908.02121v1",
    "title": "What do the founders of online communities owe to their users?",
    "authors": [
      "Cathy Chua",
      "Manny Rayner"
    ],
    "author_ids": [],
    "abstract": "We discuss the organisation of internet communities, focusing on what we call\nthe principle of \"bait and switch\": founders of internet communities often find\nit advantageous to recruit members by promising inducements which are later not\nhonoured. We look at some of the dilemmas and ways of attempting to resolve\nthem through two paradigmatic examples, Wikispaces and Wordpress. Our analysis\nis to a large extent motivated by the demands of CALLector, a\nuniversity-centred social network we are in the process of establishing. We\nconsider the question of what ethical standards are imposed on universities\nengaged in this type of activity.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02121v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.12723v2",
    "title": "Euclidean Forward-Reverse Brascamp-Lieb Inequalities: Finiteness, Structure and Extremals",
    "authors": [
      "Thomas A. Courtade",
      "Jingbo Liu"
    ],
    "author_ids": [],
    "abstract": "A new proof is given for the fact that centered gaussian functions saturate\nthe Euclidean forward-reverse Brascamp-Lieb inequalities, extending the\nBrascamp-Lieb and Barthe theorems. A duality principle for best constants is\nalso developed, which generalizes the fact that the best constants in the\nBrascamp-Lieb and Barthe inequalities are equal. Finally, as the title hints,\nthe main results concerning finiteness, structure and gaussian-extremizability\nfor the Brascamp-Lieb inequality due to Bennett, Carbery, Christ and Tao are\ngeneralized to the setting of the forward-reverse Brascamp-Lieb inequality.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "math.FA",
      "cs.IT",
      "math.CA",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12723v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.12700v1",
    "title": "A Practical Guide for the Effective Evaluation of Twitter User Geolocation",
    "authors": [
      "Ahmed Mourad",
      "Falk Scholer",
      "Walid Magdy",
      "Mark Sanderson"
    ],
    "author_ids": [],
    "abstract": "Geolocating Twitter users---the task of identifying their home\nlocations---serves a wide range of community and business applications such as\nmanaging natural crises, journalism, and public health. Many approaches have\nbeen proposed for automatically geolocating users based on their tweets; at the\nsame time, various evaluation metrics have been proposed to measure the\neffectiveness of these approaches, making it challenging to understand which of\nthese metrics is the most suitable for this task. In this paper, we propose a\nguide for a standardized evaluation of Twitter user geolocation by analyzing\nfifteen models and two baselines in a controlled experimental setting. Models\nare evaluated using ten metrics over four geographic granularities. We use rank\ncorrelations to assess the effectiveness of these metrics.\n  Our results demonstrate that the choice of effectiveness metric can have a\nsubstantial impact on the conclusions drawn from a geolocation system\nexperiment, potentially leading experimenters to contradictory results about\nrelative effectiveness. We show that for general evaluations, a range of\nperformance metrics should be reported, to ensure that a complete picture of\nsystem effectiveness is conveyed. Given the global geographic coverage of this\ntask, we specifically recommend evaluation at micro versus macro levels to\nmeasure the impact of the bias in distribution over locations. Although a lot\nof complex geolocation algorithms have been applied in recent years, a majority\nclass baseline is still competitive at coarse geographic granularity. We\npropose a suite of statistical analysis tests, based on the employed metric, to\nensure that the results are not coincidental.",
    "published_date": "2019-07-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12700v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.12537v2",
    "title": "Towards Automatic Screening of Typical and Atypical Behaviors in Children With Autism",
    "authors": [
      "Andrew Cook",
      "Bappaditya Mandal",
      "Donna Berry",
      "Matthew Johnson"
    ],
    "author_ids": [],
    "abstract": "This paper has been withdrawn by the authors due to insufficient or\ndefinition error(s) in the ethics approval protocol.\n  Autism spectrum disorders (ASD) impact the cognitive, social, communicative\nand behavioral abilities of an individual. The development of new clinical\ndecision support systems is of importance in reducing the delay between\npresentation of symptoms and an accurate diagnosis. In this work, we contribute\na new database consisting of video clips of typical (normal) and atypical (such\nas hand flapping, spinning or rocking) behaviors, displayed in natural\nsettings, which have been collected from the YouTube video website. We propose\na preliminary non-intrusive approach based on skeleton keypoint identification\nusing pretrained deep neural networks on human body video clips to extract\nfeatures and perform body movement analysis that differentiates typical and\natypical behaviors of children. Experimental results on the newly contributed\ndatabase show that our platform performs best with decision tree as the\nclassifier when compared to other popular methodologies and offers a baseline\nagainst which alternate approaches may developed and tested.",
    "published_date": "2019-07-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12537v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.12475v2",
    "title": "Energy-Efficient Processing and Robust Wireless Cooperative Transmission for Edge Inference",
    "authors": [
      "Kai Yang",
      "Yuanming Shi",
      "Wei Yu",
      "Zhi Ding"
    ],
    "author_ids": [],
    "abstract": "Edge machine learning can deliver low-latency and private artificial\nintelligent (AI) services for mobile devices by leveraging computation and\nstorage resources at the network edge. This paper presents an energy-efficient\nedge processing framework to execute deep learning inference tasks at the edge\ncomputing nodes whose wireless connections to mobile devices are prone to\nchannel uncertainties. Aimed at minimizing the sum of computation and\ntransmission power consumption with probabilistic quality-of-service (QoS)\nconstraints, we formulate a joint inference tasking and downlink beamforming\nproblem that is characterized by a group sparse objective function. We provide\na statistical learning based robust optimization approach to approximate the\nhighly intractable probabilistic-QoS constraints by nonconvex quadratic\nconstraints, which are further reformulated as matrix inequalities with a\nrank-one constraint via matrix lifting. We design a reweighted power\nminimization approach by iteratively reweighted $\\ell_1$ minimization with\ndifference-of-convex-functions (DC) regularization and updating weights, where\nthe reweighted approach is adopted for enhancing group sparsity whereas the DC\nregularization is designed for inducing rank-one solutions. Numerical results\ndemonstrate that the proposed approach outperforms other state-of-the-art\napproaches.",
    "published_date": "2019-07-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12475v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.12468v3",
    "title": "Integer Programming, Constraint Programming, and Hybrid Decomposition Approaches to Discretizable Distance Geometry Problems",
    "authors": [
      "Moira MacNeil",
      "Merve Bodur"
    ],
    "author_ids": [],
    "abstract": "Given an integer dimension K and a simple, undirected graph G with positive\nedge weights, the Distance Geometry Problem (DGP) aims to find a realization\nfunction mapping each vertex to a coordinate in K-dimensional space such that\nthe distance between pairs of vertex coordinates is equal to the corresponding\nedge weights in G. The so-called discretization assumptions reduce the search\nspace of the realization to a finite discrete one which can be explored via the\nbranch-and-prune (BP) algorithm. Given a discretization vertex order in G, the\nBP algorithm constructs a binary tree where the nodes at a layer provide all\npossible coordinates of the vertex corresponding to that layer. The focus of\nthis paper is finding optimal BP trees for a class of Discretizable DGPs. More\nspecifically, we aim to find a discretization vertex order in G that yields a\nBP tree with the least number of branches. We propose an integer programming\nformulation and three constraint programming formulations that all\nsignificantly outperform the state-of-the-art cutting plane algorithm for this\nproblem. Moreover, motivated by the difficulty in solving instances with a\nlarge and low density input graph, we develop two hybrid decomposition\nalgorithms, strengthened by a set of valid inequalities, which further improve\nthe solvability of the problem.",
    "published_date": "2019-07-29T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12468v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.12059v1",
    "title": "Wasserstein Fair Classification",
    "authors": [
      "Ray Jiang",
      "Aldo Pacchiano",
      "Tom Stepleton",
      "Heinrich Jiang",
      "Silvia Chiappa"
    ],
    "author_ids": [],
    "abstract": "We propose an approach to fair classification that enforces independence\nbetween the classifier outputs and sensitive information by minimizing\nWasserstein-1 distances. The approach has desirable theoretical properties and\nis robust to specific choices of the threshold used to obtain class predictions\nfrom model outputs. We introduce different methods that enable hiding sensitive\ninformation at test time or have a simple and fast implementation. We show\nempirical performance against different fairness baselines on several benchmark\nfairness datasets.",
    "published_date": "2019-07-28T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12059v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.11672v2",
    "title": "Fairness-Efficiency Tradeoffs in Dynamic Fair Division",
    "authors": [
      "David Zeng",
      "Alexandros Psomas"
    ],
    "author_ids": [],
    "abstract": "We investigate the tradeoffs between fairness and efficiency when allocating\nindivisible items over time. Suppose T items arrive over time and must be\nallocated upon arrival, immediately and irrevocably, to one of n agents. Agent\ni assigns a value v_{it} in [0,1] to the t-th item to arrive and has an\nadditive valuation function. If the values are chosen by an adaptive adversary\nit is known that the algorithm that minimizes maximum pairwise envy simply\nallocates each item uniformly at random; the maximum pairwise envy is then\nsublinear in T. If the values are independently and identically drawn from an\nadversarially chosen distribution D, it is also known that, under some mild\nconditions on D, allocating to the agent with the highest value -- a Pareto\nefficient allocation -- is envy-free with high probability.\n  In this paper we study fairness-efficiency tradeoffs in this setting and\nprovide matching upper and lower bounds under a spectrum of progressively\nstronger adversaries. On one hand we show that, even against a non-adaptive\nadversary, there is no algorithm with sublinear maximum pairwise envy that\nPareto dominates the simple algorithm that allocates each item uniformly at\nrandom. On the other hand, under a slightly weaker adversary regime where item\nvalues are drawn from a known distribution and are independent with respect to\ntime, i.e. v_{it} is independent of v_{it'} but possibly correlated with\nv_{i't}, optimal (in isolation) efficiency is compatible with optimal (in\nisolation) fairness. That is, we give an algorithm that is Pareto efficient\nex-post and is simultaneously optimal with respect to fairness: for each pair\nof agents i and j, either i envies j by at most one item (a prominent fairness\nnotion), or $i$ does not envy j with high probability. En route, we prove a\nstructural result about allocations of divisible items that might be of\nindependent interest.",
    "published_date": "2019-07-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.11672v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.12393v2",
    "title": "To regulate or not: a social dynamics analysis of the race for AI supremacy",
    "authors": [
      "The Anh Han",
      "Luis Moniz Pereira",
      "Francisco C. Santos",
      "Tom Lenaerts"
    ],
    "author_ids": [],
    "abstract": "Rapid technological advancements in AI as well as the growing deployment of\nintelligent technologies in new application domains are currently driving the\ncompetition between businesses, nations and regions. This race for\ntechnological supremacy creates a complex ecology of choices that may lead to\nnegative consequences, in particular, when ethical and safety procedures are\nunderestimated or even ignored. As a consequence, different actors are urging\nto consider both the normative and social impact of these technological\nadvancements. As there is no easy access to data describing this AI race,\ntheoretical models are necessary to understand its dynamics, allowing for the\nidentification of when, how and which procedures need to be put in place to\nfavour outcomes beneficial for all. We show that, next to the risks of setbacks\nand being reprimanded for unsafe behaviour, the time-scale in which AI\nsupremacy can be achieved plays a crucial role. When this supremacy can be\nachieved in a short term, those who completely ignore the safety precautions\nare bound to win the race but at a cost to society, apparently requiring\nregulatory actions. Our analysis reveals that blindly imposing regulations may\nnot have anticipated effect as only for specific conditions a dilemma arises\nbetween what individually preferred and globally beneficial. Similar\nobservations can be made for the long-term development case. Yet different from\nthe short term situation, certain conditions require the promotion of\nrisk-taking as opposed to compliance to safety regulations in order to improve\nsocial welfare. These results remain robust when two or several actors are\ninvolved in the race and when collective rather than individual setbacks are\nproduced by risk-taking behaviour. When defining codes of conduct and\nregulatory policies for AI, a clear understanding about the time-scale of the\nrace is required.",
    "published_date": "2019-07-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12393v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.12652v1",
    "title": "How model accuracy and explanation fidelity influence user trust",
    "authors": [
      "Andrea Papenmeier",
      "Gwenn Englebienne",
      "Christin Seifert"
    ],
    "author_ids": [],
    "abstract": "Machine learning systems have become popular in fields such as marketing,\nfinancing, or data mining. While they are highly accurate, complex machine\nlearning systems pose challenges for engineers and users. Their inherent\ncomplexity makes it impossible to easily judge their fairness and the\ncorrectness of statistically learned relations between variables and classes.\nExplainable AI aims to solve this challenge by modelling explanations alongside\nwith the classifiers, potentially improving user trust and acceptance. However,\nusers should not be fooled by persuasive, yet untruthful explanations. We\ntherefore conduct a user study in which we investigate the effects of model\naccuracy and explanation fidelity, i.e. how truthfully the explanation\nrepresents the underlying model, on user trust. Our findings show that accuracy\nis more important for user trust than explainability. Adding an explanation for\na classification result can potentially harm trust, e.g. when adding\nnonsensical explanations. We also found that users cannot be tricked by\nhigh-fidelity explanations into having trust for a bad classifier. Furthermore,\nwe found a mismatch between observed (implicit) and self-reported (explicit)\ntrust.",
    "published_date": "2019-07-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12652v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.11019v4",
    "title": "Fair and Efficient Cake Division with Connected Pieces",
    "authors": [
      "Eshwar Ram Arunachaleswaran",
      "Siddharth Barman",
      "Rachitesh Kumar",
      "Nidhi Rathi"
    ],
    "author_ids": [],
    "abstract": "The classic cake-cutting problem provides a model for addressing fair and\nefficient allocation of a divisible, heterogeneous resource (metaphorically,\nthe cake) among agents with distinct preferences. Focusing on a standard\nformulation of cake cutting, in which each agent must receive a contiguous\npiece of the cake, this work establishes algorithmic and hardness results for\nmultiple fairness/efficiency measures.\n  First, we consider the well-studied notion of envy-freeness and develop an\nefficient algorithm that finds a cake division (with connected pieces) wherein\nthe envy is multiplicatively within a factor of 2+o(1). The same algorithm in\nfact achieves an approximation ratio of 3+o(1) for the problem of finding cake\ndivisions with as large a Nash social welfare (NSW) as possible. NSW is another\nstandard measure of fairness and this work also establishes a connection\nbetween envy-freeness and NSW: approximately envy-free cake divisions (with\nconnected pieces) always have near-optimal Nash social welfare. Furthermore, we\ndevelop an approximation algorithm for maximizing the $\\rho$-mean welfare--this\nunifying objective, with different values of $\\rho$, interpolates between\nnotions of fairness (NSW) and efficiency (average social welfare). Finally, we\ncomplement these algorithmic results by proving that maximizing NSW (and, in\ngeneral, the $\\rho$-mean welfare) is APX-hard in the cake-division context.",
    "published_date": "2019-07-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "F.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.11019v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1908.00625v1",
    "title": "Learning about spatial inequalities: Capturing the heterogeneity in the urban environment",
    "authors": [
      "J. Siqueira-Gay",
      "M. A. Giannotti",
      "M. Sester"
    ],
    "author_ids": [],
    "abstract": "Transportation systems can be conceptualized as an instrument of spreading\npeople and resources over the territory, playing an important role in\ndeveloping sustainable cities. The current rationale of transport provision is\nbased on population demand, disregarding land use and socioeconomic\ninformation. To meet the challenge to promote a more equitable resource\ndistribution, this work aims at identifying and describing patterns of urban\nservices supply, their accessibility, and household income. By using a\nmultidimensional approach, the spatial inequalities of a large city of the\nglobal south reveal that the low-income population has low access mainly to\nhospitals and cultural centers. A low-income group presents an intermediate\nlevel of accessibility to public schools and sports centers, evidencing the\ndiverse condition of citizens in the peripheries. These complex outcomes\ngenerated by the interaction of land use and public transportation emphasize\nthe importance of comprehensive methodological approaches to support decisions\nof urban projects, plans and programs. Reducing spatial inequalities,\nespecially providing services for deprived groups, is fundamental to promote\nthe sustainable use of resources and optimize the daily commuting.",
    "published_date": "2019-07-24T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.00625v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.11142v1",
    "title": "On the bias of H-scores for comparing biclusters, and how to correct it",
    "authors": [
      "Jacopo Di Iorio",
      "Francesca Chiaromonte",
      "Marzia A. Cremona"
    ],
    "author_ids": [],
    "abstract": "In the last two decades several biclustering methods have been developed as\nnew unsupervised learning techniques to simultaneously cluster rows and columns\nof a data matrix. These algorithms play a central role in contemporary machine\nlearning and in many applications, e.g. to computational biology and\nbioinformatics. The H-score is the evaluation score underlying the seminal\nbiclustering algorithm by Cheng and Church, as well as many other subsequent\nbiclustering methods. In this paper, we characterize a potentially troublesome\nbias in this score, that can distort biclustering results. We prove, both\nanalytically and by simulation, that the average H-score increases with the\nnumber of rows/columns in a bicluster. This makes the H-score, and hence all\nalgorithms based on it, biased towards small clusters. Based on our analytical\nproof, we are able to provide a straightforward way to correct this bias,\nallowing users to accurately compare biclusters.",
    "published_date": "2019-07-24T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.11142v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.10323v1",
    "title": "Fairness in Reinforcement Learning",
    "authors": [
      "Paul Weng"
    ],
    "author_ids": [],
    "abstract": "Decision support systems (e.g., for ecological conservation) and autonomous\nsystems (e.g., adaptive controllers in smart cities) start to be deployed in\nreal applications. Although their operations often impact many users or\nstakeholders, no fairness consideration is generally taken into account in\ntheir design, which could lead to completely unfair outcomes for some users or\nstakeholders. To tackle this issue, we advocate for the use of social welfare\nfunctions that encode fairness and present this general novel problem in the\ncontext of (deep) reinforcement learning, although it could possibly be\nextended to other machine learning tasks.",
    "published_date": "2019-07-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.10323v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.10228v2",
    "title": "Zero-shifting Technique for Deep Neural Network Training on Resistive Cross-point Arrays",
    "authors": [
      "Hyungjun Kim",
      "Malte Rasch",
      "Tayfun Gokmen",
      "Takashi Ando",
      "Hiroyuki Miyazoe",
      "Jae-Joon Kim",
      "John Rozen",
      "Seyoung Kim"
    ],
    "author_ids": [],
    "abstract": "A resistive memory device-based computing architecture is one of the\npromising platforms for energy-efficient Deep Neural Network (DNN) training\naccelerators. The key technical challenge in realizing such accelerators is to\naccumulate the gradient information without a bias. Unlike the digital numbers\nin software which can be assigned and accessed with desired accuracy, numbers\nstored in resistive memory devices can only be manipulated following the\nphysics of the device, which can significantly limit the training performance.\nTherefore, additional techniques and algorithm-level remedies are required to\nachieve the best possible performance in resistive memory device-based\naccelerators. In this paper, we analyze asymmetric conductance modulation\ncharacteristics in RRAM by Soft-bound synapse model and present an in-depth\nanalysis on the relationship between device characteristics and DNN model\naccuracy using a 3-layer DNN trained on the MNIST dataset. We show that the\nimbalance between up and down update leads to a poor network performance. We\nintroduce a concept of symmetry point and propose a zero-shifting technique\nwhich can compensate imbalance by programming the reference device and changing\nthe zero value point of the weight. By using this zero-shifting method, we show\nthat network performance dramatically improves for imbalanced synapse devices.",
    "published_date": "2019-07-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.ET",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.10228v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.10081v1",
    "title": "Multimodal Age and Gender Classification Using Ear and Profile Face Images",
    "authors": [
      "Dogucan Yaman",
      "Fevziye Irem Eyiokur",
      "Hazım Kemal Ekenel"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present multimodal deep neural network frameworks for age\nand gender classification, which take input a profile face image as well as an\near image. Our main objective is to enhance the accuracy of soft biometric\ntrait extraction from profile face images by additionally utilizing a promising\nbiometric modality: ear appearance. For this purpose, we provided end-to-end\nmultimodal deep learning frameworks. We explored different multimodal\nstrategies by employing data, feature, and score level fusion. To increase\nrepresentation and discrimination capability of the deep neural networks, we\nbenefited from domain adaptation and employed center loss besides softmax loss.\nWe conducted extensive experiments on the UND-F, UND-J2, and FERET datasets.\nExperimental results indicated that profile face images contain a rich source\nof information for age and gender classification. We found that the presented\nmultimodal system achieves very high age and gender classification accuracies.\nMoreover, we attained superior results compared to the state-of-the-art profile\nface image or ear image-based age and gender classification methods.",
    "published_date": "2019-07-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.10081v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09754v1",
    "title": "Controlling biases and diversity in diverse image-to-image translation",
    "authors": [
      "Yaxing Wang",
      "Abel Gonzalez-Garcia",
      "Joost van de Weijer",
      "Luis Herranz"
    ],
    "author_ids": [],
    "abstract": "The task of unpaired image-to-image translation is highly challenging due to\nthe lack of explicit cross-domain pairs of instances. We consider here diverse\nimage translation (DIT), an even more challenging setting in which an image can\nhave multiple plausible translations. This is normally achieved by explicitly\ndisentangling content and style in the latent representation and sampling\ndifferent styles codes while maintaining the image content. Despite the success\nof current DIT models, they are prone to suffer from bias. In this paper, we\nstudy the problem of bias in image-to-image translation. Biased datasets may\nadd undesired changes (e.g. change gender or race in face images) to the output\ntranslations as a consequence of the particular underlying visual distribution\nin the target domain. In order to alleviate the effects of this problem we\npropose the use of semantic constraints that enforce the preservation of\ndesired image properties. Our proposed model is a step towards unbiased diverse\nimage-to-image translation (UDIT), and results in less unwanted changes in the\ntranslated images while still performing the wanted transformation. Experiments\non several heavily biased datasets show the effectiveness of the proposed\ntechniques in different domains such as faces, objects, and scenes.",
    "published_date": "2019-07-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09754v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.10516v2",
    "title": "Achieving Fairness in the Stochastic Multi-armed Bandit Problem",
    "authors": [
      "Vishakha Patil",
      "Ganesh Ghalme",
      "Vineet Nair",
      "Y. Narahari"
    ],
    "author_ids": [],
    "abstract": "We study an interesting variant of the stochastic multi-armed bandit problem,\ncalled the Fair-SMAB problem, where each arm is required to be pulled for at\nleast a given fraction of the total available rounds. We investigate the\ninterplay between learning and fairness in terms of a pre-specified vector\ndenoting the fractions of guaranteed pulls. We define a fairness-aware regret,\ncalled $r$-Regret, that takes into account the above fairness constraints and\nnaturally extends the conventional notion of regret. Our primary contribution\nis characterizing a class of Fair-SMAB algorithms by two parameters: the\nunfairness tolerance and the learning algorithm used as a black-box. We provide\na fairness guarantee for this class that holds uniformly over time irrespective\nof the choice of the learning algorithm. In particular, when the learning\nalgorithm is UCB1, we show that our algorithm achieves $O(\\ln T)$ $r$-Regret.\nFinally, we evaluate the cost of fairness in terms of the conventional notion\nof regret.",
    "published_date": "2019-07-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.10516v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09615v1",
    "title": "Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems",
    "authors": [
      "Shalmali Joshi",
      "Oluwasanmi Koyejo",
      "Warut Vijitbenjaronk",
      "Been Kim",
      "Joydeep Ghosh"
    ],
    "author_ids": [],
    "abstract": "Machine learning based decision making systems are increasingly affecting\nhumans. An individual can suffer an undesirable outcome under such decision\nmaking systems (e.g. denied credit) irrespective of whether the decision is\nfair or accurate. Individual recourse pertains to the problem of providing an\nactionable set of changes a person can undertake in order to improve their\noutcome. We propose a recourse algorithm that models the underlying data\ndistribution or manifold. We then provide a mechanism to generate the smallest\nset of changes that will improve an individual's outcome. This mechanism can be\neasily used to provide recourse for any differentiable machine learning based\ndecision making system. Further, the resulting algorithm is shown to be\napplicable to both supervised classification and causal decision making\nsystems. Our work attempts to fill gaps in existing fairness literature that\nhave primarily focused on discovering and/or algorithmically enforcing fairness\nconstraints on decision making systems. This work also provides an alternative\napproach to generating counterfactual explanations.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09615v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09557v2",
    "title": "Relational Generalized Few-Shot Learning",
    "authors": [
      "Xiahan Shi",
      "Leonard Salewski",
      "Martin Schiegg",
      "Zeynep Akata",
      "Max Welling"
    ],
    "author_ids": [],
    "abstract": "Transferring learned models to novel tasks is a challenging problem,\nparticularly if only very few labeled examples are available. Although this\nfew-shot learning setup has received a lot of attention recently, most proposed\nmethods focus on discriminating novel classes only. Instead, we consider the\nextended setup of generalized few-shot learning (GFSL), where the model is\nrequired to perform classification on the joint label space consisting of both\npreviously seen and novel classes. We propose a graph-based framework that\nexplicitly models relationships between all seen and novel classes in the joint\nlabel space. Our model Graph-convolutional Global Prototypical Networks (GcGPN)\nincorporates these inter-class relations using graph-convolution in order to\nembed novel class representations into the existing space of previously seen\nclasses in a globally consistent manner. Our approach ensures both fast\nadaptation and global discrimination, which is the major challenge in GFSL. We\ndemonstrate the benefits of our model on two challenging benchmark datasets.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09557v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.11166v2",
    "title": "Body-worn triaxial accelerometer coherence and reliability related to static posturography in unilateral vestibular failure",
    "authors": [
      "M. Alessandrini",
      "A. Micarelli",
      "A. Viziano",
      "I. Pavone",
      "G. Costantini",
      "D. Casali",
      "F. Paolizzo",
      "G. Saggio"
    ],
    "author_ids": [],
    "abstract": "Due to the fact that no study to date has shown the experimental validity of\nACC-based measures of body sway with respect to posturography for subjects with\nvestibular deficits, the aim of the present study was: i) to develop and\nvalidate a practical tool that can allow clinicians to measure postural sway\nderangements in an otoneurological setting by ACC, and ii) to provide reliable,\nsensitive and accurate automatic analysis of sway that could help in\ndiscriminating unilateral vestibular failure (UVF) patients. Thus, a group of\n13 patients (seven females, 6 males; mean age 48.6 +/- 6.4 years) affected for\nat least 6 months by UVF and 13 matched healthy subjects were instructed to\nmaintain an upright position during a static forceplate-based posturography\n(FBP) acquisition while wearing a Movit sensor (by Captiks) with 3-D\naccelerometers mounted on the posterior trunk near the body centre of mass.\nPearson product moment correlation demonstrated a high level of correspondence\nof four time-domain and three frequency-domain measures extracted by ACC and\nFBP testing; in addition, t-test demonstrated that two ACC-based time- and\nfrequency-domain parameters were reliable measures in discriminating UVF\nsubjects. These aspects, overall, should further highlight the attention of\nclinicians and researchers to this kind of sway recording technique in the\nfield of otoneurological disorders by considering the possibility to enrich the\namount of quantitative and qualitative information useful for discrimination,\ndiagnosis and treatment of UVF. In conclusion, we believe the present ACC-based\nmeasurement of sway offers a patient-friendly, reliable, inexpensive and\nefficient alternative recording technique that is useful - together with\nclinical balance and mobility tests - in various circumstances, as well as in\noutcome studies involving diagnosis, follow-up and rehabilitation of UVF\npatients.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "physics.med-ph",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.11166v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.09328v1",
    "title": "A Conceptual Framework for Evaluating Fairness in Search",
    "authors": [
      "Anubrata Das",
      "Matthew Lease"
    ],
    "author_ids": [],
    "abstract": "While search efficacy has been evaluated traditionally on the basis of result\nrelevance, fairness of search has attracted recent attention. In this work, we\ndefine a notion of distributional fairness and provide a conceptual framework\nfor evaluating search results based on it. As part of this, we formulate a set\nof axioms which an ideal evaluation framework should satisfy for distributional\nfairness. We show how existing TREC test collections can be repurposed to study\nfairness, and we measure potential data bias to inform test collection design\nfor fair search. A set of analyses show metric divergence between relevance and\nfairness, and we describe a simple but flexible interpolation strategy for\nintegrating relevance and fairness into a single metric for optimization and\nevaluation.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09328v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.09245v1",
    "title": "Quadruplet Selection Methods for Deep Embedding Learning",
    "authors": [
      "Kaan Karaman",
      "Erhan Gundogdu",
      "Aykut Koc",
      "A. Aydin Alatan"
    ],
    "author_ids": [],
    "abstract": "Recognition of objects with subtle differences has been used in many\npractical applications, such as car model recognition and maritime vessel\nidentification. For discrimination of the objects in fine-grained detail, we\nfocus on deep embedding learning by using a multi-task learning framework, in\nwhich the hierarchical labels (coarse and fine labels) of the samples are\nutilized both for classification and a quadruplet-based loss function. In order\nto improve the recognition strength of the learned features, we present a novel\nfeature selection method specifically designed for four training samples of a\nquadruplet. By experiments, it is observed that the selection of very hard\nnegative samples with relatively easy positive ones from the same coarse and\nfine classes significantly increases some performance metrics in a fine-grained\ndataset when compared to selecting the quadruplet samples randomly. The feature\nembedding learned by the proposed method achieves favorable performance against\nits state-of-the-art counterparts.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09245v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09204v2",
    "title": "Domain Adaptation for One-Class Classification: Monitoring the Health of Critical Systems Under Limited Information",
    "authors": [
      "Gabriel Michau",
      "Olga Fink"
    ],
    "author_ids": [],
    "abstract": "The failure of a complex and safety critical industrial asset can have\nextremely high consequences. Close monitoring for early detection of abnormal\nsystem conditions is therefore required. Data-driven solutions to this problem\nhave been limited for two reasons: First, safety critical assets are designed\nand maintained to be highly reliable and faults are rare. Fault detection can\nthus not be solved with supervised learning. Second, complex industrial systems\nusually have long lifetime during which they face very different operating\nconditions. In the early life of the system, the collected data is probably not\nrepresentative of future operating conditions, making it challenging to train a\nrobust model.\n  In this paper, we propose a methodology to monitor the systems in their early\nlife. To do so, we enhance the training dataset with other units from a fleet,\nfor which longer observations are available. Since each unit has its own\nspecificity, we propose to extract features made independent of their origin by\nthree unsupervised feature alignment techniques. First, using a variational\nencoder, we impose a shared probabilistic encoder/decoder for both units.\nSecond, we introduce a new loss designed to conserve inter-point spacial\nrelationships between the input and the learned features. Last, we propose to\ntrain in an adversarial manner a discriminator on the origin of the features.\nOnce aligned, the features are fed to a one-class classifier to monitor the\nhealth of the system. By exploring the different combinations of the proposed\nalignment strategies, and by testing them on a real case study, a fleet\ncomposed of 112 power plants operated in different geographical locations and\nunder very different operating regimes, we demonstrate that this alignment is\nnecessary and beneficial.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09204v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09189v1",
    "title": "Comparative Evaluation of Multiagent Learning Algorithms in a Diverse Set of Ad Hoc Team Problems",
    "authors": [
      "Stefano V. Albrecht",
      "Subramanian Ramamoorthy"
    ],
    "author_ids": [],
    "abstract": "This paper is concerned with evaluating different multiagent learning (MAL)\nalgorithms in problems where individual agents may be heterogenous, in the\nsense of utilizing different learning strategies, without the opportunity for\nprior agreements or information regarding coordination. Such a situation arises\nin ad hoc team problems, a model of many practical multiagent systems\napplications. Prior work in multiagent learning has often been focussed on\nhomogeneous groups of agents, meaning that all agents were identical and a\npriori aware of this fact. Also, those algorithms that are specifically\ndesigned for ad hoc team problems are typically evaluated in teams of agents\nwith fixed behaviours, as opposed to agents which are adapting their\nbehaviours. In this work, we empirically evaluate five MAL algorithms,\nrepresenting major approaches to multiagent learning but originally developed\nwith the homogeneous setting in mind, to understand their behaviour in a set of\nad hoc team problems. All teams consist of agents which are continuously\nadapting their behaviours. The algorithms are evaluated with respect to a\ncomprehensive characterisation of repeated matrix games, using performance\ncriteria that include considerations such as attainment of equilibrium, social\nwelfare and fairness. Our main conclusion is that there is no clear winner.\nHowever, the comparative evaluation also highlights the relative strengths of\ndifferent algorithms with respect to the type of performance criteria, e.g.,\nsocial welfare vs. attainment of equilibrium.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09189v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09100v1",
    "title": "Reasoning about Social Choice and Games in Monadic Fixed-Point Logic",
    "authors": [
      "Ramit Das",
      "R. Ramanujam",
      "Sunil Simon"
    ],
    "author_ids": [],
    "abstract": "Whether it be in normal form games, or in fair allocations, or in voter\npreferences in voting systems, a certain pattern of reasoning is common. From a\nparticular profile, an agent or a group of agents may have an incentive to\nshift to a new one. This induces a natural graph structure that we call the\nimprovement graph on the strategy space of these systems. We suggest that the\nmonadic fixed-point logic with counting, an extension of monadic first-order\nlogic on graphs with fixed-point and counting quantifiers, is a natural\nspecification language on improvement graphs, and thus for a class of\nproperties that can be interpreted across these domains. The logic has an\nefficient model checking algorithm (in the size of the improvement graph).",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09100v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.12917v1",
    "title": "Covering up bias in CelebA-like datasets with Markov blankets: A post-hoc cure for attribute prior avoidance",
    "authors": [
      "Vinay Uday Prabhu",
      "Dian Ang Yap",
      "Alexander Wang",
      "John Whaley"
    ],
    "author_ids": [],
    "abstract": "Attribute prior avoidance entails subconscious or willful non-modeling of\n(meta)attributes that datasets are oft born with, such as the 40 semantic\nfacial attributes associated with the CelebA and CelebA-HQ datasets. The\nconsequences of this infirmity, we discover, are especially stark in\nstate-of-the-art deep generative models learned on these datasets that just\nmodel the pixel-space measurements, resulting in an inter-attribute bias-laden\nlatent space. This viscerally manifests itself when we perform face\nmanipulation experiments based on latent vector interpolations. In this paper,\nwe address this and propose a post-hoc solution that utilizes an Ising\nattribute prior learned in the attribute space and showcase its efficacy via\nqualitative experiments.",
    "published_date": "2019-07-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12917v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09013v1",
    "title": "Conscientious Classification: A Data Scientist's Guide to Discrimination-Aware Classification",
    "authors": [
      "Brian d'Alessandro",
      "Cathy O'Neil",
      "Tom LaGatta"
    ],
    "author_ids": [],
    "abstract": "Recent research has helped to cultivate growing awareness that machine\nlearning systems fueled by big data can create or exacerbate troubling\ndisparities in society. Much of this research comes from outside of the\npracticing data science community, leaving its members with little concrete\nguidance to proactively address these concerns. This article introduces issues\nof discrimination to the data science community on its own terms. In it, we\ntour the familiar data mining process while providing a taxonomy of common\npractices that have the potential to produce unintended discrimination. We also\nsurvey how discrimination is commonly measured, and suggest how familiar\ndevelopment processes can be augmented to mitigate systems' discriminatory\npotential. We advocate that data scientists should be intentional about\nmodeling and reducing discriminatory outcomes. Without doing so, their efforts\nwill result in perpetuating any systemic discrimination that may exist, but\nunder a misleading veil of data-driven objectivity.",
    "published_date": "2019-07-21T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09013v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.08968v2",
    "title": "Infant Mortality Prediction using Birth Certificate Data",
    "authors": [
      "Antonia Saravanou",
      "Clemens Noelke",
      "Nicholas Huntington",
      "Dolores Acevedo-Garcia",
      "Dimitrios Gunopulos"
    ],
    "author_ids": [],
    "abstract": "The Infant Mortality Rate (IMR) is the number of infants per 1000 that do not\nsurvive until their first birthday. It is an important metric providing\ninformation about infant health but it also measures the society's general\nhealth status. Despite the high level of prosperity in the U.S.A., the\ncountry's IMR is higher than that of many other developed countries.\nAdditionally, the U.S.A. exhibits persistent inequalities in the IMR across\ndifferent racial and ethnic groups. In this paper, we study the infant\nmortality prediction using features extracted from birth certificates. We are\ninterested in training classification models to decide whether an infant will\nsurvive or not. We focus on exploring and understanding the importance of\nfeatures in subsets of the population; we compare models trained for individual\nraces to general models. Our evaluation shows that our methodology outperforms\nstandard classification methods used by epidemiology researchers.",
    "published_date": "2019-07-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08968v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.08922v1",
    "title": "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990",
    "authors": [
      "Melvin Wevers"
    ],
    "author_ids": [],
    "abstract": "Contemporary debates on filter bubbles and polarization in public and social\nmedia raise the question to what extent news media of the past exhibited\nbiases. This paper specifically examines bias related to gender in six Dutch\nnational newspapers between 1950 and 1990. We measure bias related to gender by\ncomparing local changes in word embedding models trained on newspapers with\ndivergent ideological backgrounds. We demonstrate clear differences in gender\nbias and changes within and between newspapers over time. In relation to themes\nsuch as sexuality and leisure, we see the bias moving toward women, whereas,\ngenerally, the bias shifts in the direction of men, despite growing female\nemployment number and feminist movements. Even though Dutch society became less\nstratified ideologically (depillarization), we found an increasing divergence\nin gender bias between religious and social-democratic on the one hand and\nliberal newspapers on the other. Methodologically, this paper illustrates how\nword embeddings can be used to examine historical language change. Future work\nwill investigate how fine-tuning deep contextualized embedding models, such as\nELMO, might be used for similar tasks with greater contextual information.",
    "published_date": "2019-07-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08922v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.08873v1",
    "title": "Detecting Cyberbullying and Cyberaggression in Social Media",
    "authors": [
      "Despoina Chatzakou",
      "Ilias Leontiadis",
      "Jeremy Blackburn",
      "Emiliano De Cristofaro",
      "Gianluca Stringhini",
      "Athena Vakali",
      "Nicolas Kourtellis"
    ],
    "author_ids": [],
    "abstract": "Cyberbullying and cyberaggression are increasingly worrisome phenomena\naffecting people across all demographics. More than half of young social media\nusers worldwide have been exposed to such prolonged and/or coordinated digital\nharassment. Victims can experience a wide range of emotions, with negative\nconsequences such as embarrassment, depression, isolation from other community\nmembers, which embed the risk to lead to even more critical consequences, such\nas suicide attempts.\n  In this work, we take the first concrete steps to understand the\ncharacteristics of abusive behavior in Twitter, one of today's largest social\nmedia platforms. We analyze 1.2 million users and 2.1 million tweets, comparing\nusers participating in discussions around seemingly normal topics like the NBA,\nto those more likely to be hate-related, such as the Gamergate controversy, or\nthe gender pay inequality at the BBC station. We also explore specific\nmanifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in\none of the hate-related communities (Gamergate). We present a robust\nmethodology to distinguish bullies and aggressors from normal Twitter users by\nconsidering text, user, and network-based attributes. Using various\nstate-of-the-art machine learning algorithms, we classify these accounts with\nover 90% accuracy and AUC. Finally, we discuss the current status of Twitter\nuser accounts marked as abusive by our methodology, and study the performance\nof potential mechanisms that can be used by Twitter to suspend users in the\nfuture.",
    "published_date": "2019-07-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08873v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.08646v1",
    "title": "Fair quantile regression",
    "authors": [
      "Dana Yang",
      "John Lafferty",
      "David Pollard"
    ],
    "author_ids": [],
    "abstract": "Quantile regression is a tool for learning conditional distributions. In this\npaper we study quantile regression in the setting where a protected attribute\nis unavailable when fitting the model. This can lead to \"unfair'' quantile\nestimators for which the effective quantiles are very different for the\nsubpopulations defined by the protected attribute. We propose a procedure for\nadjusting the estimator on a heldout sample where the protected attribute is\navailable. The main result of the paper is an empirical process analysis\nshowing that the adjustment leads to a fair estimator for which the target\nquantiles are brought into balance, in a statistical sense that we call\n$\\sqrt{n}$-fairness. We illustrate the ideas and adjustment procedure on a\ndataset of 200,000 live births, where the objective is to characterize the\ndependence of the birth weights of the babies on demographic attributes of the\nbirth mother; the protected attribute is the mother's race.",
    "published_date": "2019-07-19T00:00:00",
    "year": 2019,
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08646v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.08419v1",
    "title": "Towards Efficient BLE Mesh: Design of an Autonomous Network Joining Algorithm",
    "authors": [
      "Clara Nieto-Taladriz",
      "Yuri Murillo",
      "Sofie Pollin"
    ],
    "author_ids": [],
    "abstract": "The Internet of Things (IoT) opens the doors to a digital revolution, but\nrequires a robust protocol to wirelessly interconnect a large number of\ndevices. Bluetooth Low Energy (BLE) Mesh emerges as a suitable candidate,\nsolving the range limitations of original BLE. Nevertheless, the main\nlimitation is shifted to the scatternet architecture: how the considerable\nnumber of end systems are interconnected to ensure the network efficiency and\nscalability. As of today, some timid solutions have emerged, though the most\nrelevant parameters for the best parent selection in BLE mesh network joining\nprocedures have not been identified yet. In this work, we perform a thorough\nexploration of different device and environmental parameters in a BLE mesh\ntestbed to analyze their impact in the overall network figures of merit, such\nas end-to-end delay and packet delivery ratio (PDR). According to the inferred\nrelevance, the second part of the work implements and measures the proposed\nparent selection algorithm. The implementation is based on the open source\nFruitymesh protocol and is used as a baseline. The results reflect an\nenhancement in the network scalability and fairness, accomplishing a delay and\nPDR improvement of 26% and 10% respectively, and the avoidance of saturated\nbranches of 24%.",
    "published_date": "2019-07-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08419v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.08340v2",
    "title": "Only Time Can Tell: Discovering Temporal Data for Temporal Modeling",
    "authors": [
      "Laura Sevilla-Lara",
      "Shengxin Zha",
      "Zhicheng Yan",
      "Vedanuj Goswami",
      "Matt Feiszli",
      "Lorenzo Torresani"
    ],
    "author_ids": [],
    "abstract": "Understanding temporal information and how the visual world changes over time\nis a fundamental ability of intelligent systems. In video understanding,\ntemporal information is at the core of many current challenges, including\ncompression, efficient inference, motion estimation or summarization. However,\nin current video datasets it has been observed that action classes can often be\nrecognized without any temporal information from a single frame of video. As a\nresult, both benchmarking and training in these datasets may give an\nunintentional advantage to models with strong image understanding capabilities,\nas opposed to those with strong temporal understanding. In this paper we\naddress this problem head on by identifying action classes where temporal\ninformation is actually necessary to recognize them and call these \"temporal\nclasses\". Selecting temporal classes using a computational method would bias\nthe process. Instead, we propose a methodology based on a simple and effective\nhuman annotation experiment. We remove just the temporal information by\nshuffling frames in time and measure if the action can still be recognized.\nClasses that cannot be recognized when frames are not in order are included in\nthe temporal Dataset. We observe that this set is statistically different from\nother static classes, and that performance in it correlates with a network's\nability to capture temporal information. Thus we use it as a benchmark on\ncurrent popular networks, which reveals a series of interesting facts. We also\nexplore the effect of training on the temporal dataset, and observe that this\nleads to better generalization in unseen classes, demonstrating the need for\nmore temporal data. We hope that the proposed dataset of temporal categories\nwill help guide future research in temporal modeling for better video\nunderstanding.",
    "published_date": "2019-07-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08340v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09006v1",
    "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
    "authors": [
      "Yibin Zheng",
      "Xi Wang",
      "Lei He",
      "Shifeng Pan",
      "Frank K. Soong",
      "Zhengqi Wen",
      "Jianhua Tao"
    ],
    "author_ids": [],
    "abstract": "Neural end-to-end TTS can generate very high-quality synthesized speech, and\neven close to human recording within similar domain text. However, it performs\nunsatisfactory when scaling it to challenging test sets. One concern is that\nthe encoder-decoder with attention-based network adopts autoregressive\ngenerative sequence model with the limitation of \"exposure bias\" To address\nthis issue, we propose two novel methods, which learn to predict future by\nimproving agreement between forward and backward decoding sequence. The first\none is achieved by introducing divergence regularization terms into model\ntraining objective to reduce the mismatch between two directional models,\nnamely L2R and R2L (which generates targets from left-to-right and\nright-to-left, respectively). While the second one operates on decoder-level\nand exploits the future information during decoding. In addition, we employ a\njoint training strategy to allow forward and backward decoding to improve each\nother in an interactive process. Experimental results show our proposed methods\nespecially the second one (bidirectional decoder regularization), leads a\nsignificantly improvement on both robustness and overall naturalness, as\noutperforming baseline (the revised version of Tacotron2) with a MOS gap of\n0.14 in a challenging test, and achieving close to human quality (4.42 vs. 4.49\nin MOS) on general test.",
    "published_date": "2019-07-18T00:00:00",
    "year": 2019,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09006v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07892v1",
    "title": "Global AI Ethics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence",
    "authors": [
      "Alexa Hagerty",
      "Igor Rubinov"
    ],
    "author_ids": [],
    "abstract": "The ethical implications and social impacts of artificial intelligence have\nbecome topics of compelling interest to industry, researchers in academia, and\nthe public. However, current analyses of AI in a global context are biased\ntoward perspectives held in the U.S., and limited by a lack of research,\nespecially outside the U.S. and Western Europe.\n  This article summarizes the key findings of a literature review of recent\nsocial science scholarship on the social impacts of AI and related technologies\nin five global regions. Our team of social science researchers reviewed more\nthan 800 academic journal articles and monographs in over a dozen languages.\n  Our review of the literature suggests that AI is likely to have markedly\ndifferent social impacts depending on geographical setting. Likewise,\nperceptions and understandings of AI are likely to be profoundly shaped by\nlocal cultural and social context.\n  Recent research in U.S. settings demonstrates that AI-driven technologies\nhave a pattern of entrenching social divides and exacerbating social\ninequality, particularly among historically-marginalized groups. Our literature\nreview indicates that this pattern exists on a global scale, and suggests that\nlow- and middle-income countries may be more vulnerable to the negative social\nimpacts of AI and less likely to benefit from the attendant gains.\n  We call for rigorous ethnographic research to better understand the social\nimpacts of AI around the world. Global, on-the-ground research is particularly\ncritical to identify AI systems that may amplify social inequality in order to\nmitigate potential harms. Deeper understanding of the social impacts of AI in\ndiverse social settings is a necessary precursor to the development,\nimplementation, and monitoring of responsible and beneficial AI technologies,\nand forms the basis for meaningful regulation of these technologies.",
    "published_date": "2019-07-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07892v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07874v1",
    "title": "A Study on the Prevalence of Human Values in Software Engineering Publications, 2015-2018",
    "authors": [
      "Harsha Perera",
      "Arif Nurwidyantoro",
      "Waqar Hussain",
      "Davoud Mougouei",
      "Jon Whittle",
      "Rifat Ara Shams",
      "Gillian Oliver"
    ],
    "author_ids": [],
    "abstract": "Failure to account for human values in software (e.g., equality and fairness)\ncan result in user dissatisfaction and negative socio-economic impact.\nEngineering these values in software, however, requires technical and\nmethodological support throughout the development life cycle. This paper\ninvestigates to what extent software engineering (SE) research has considered\nhuman values. We investigate the prevalence of human values in recent (2015 -\n2018) publications at some of the top-tier SE conferences and journals. We\nclassify SE publications, based on their relevance to different values, against\na widely used value structure adopted from social sciences. Our results show\nthat: (a) only a small proportion of the publications directly consider values,\nclassified as relevant publications; (b) for the majority of the values, very\nfew or no relevant publications were found; and (c) the prevalence of the\nrelevant publications was higher in SE conferences compared to SE journals.\nThis paper shares these and other insights that motivate research on human\nvalues in software engineering.",
    "published_date": "2019-07-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07874v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.07818v1",
    "title": "Decoding the Style and Bias of Song Lyrics",
    "authors": [
      "Manash Pratim Barman",
      "Amit Awekar",
      "Sambhav Kothari"
    ],
    "author_ids": [],
    "abstract": "The central idea of this paper is to gain a deeper understanding of song\nlyrics computationally. We focus on two aspects: style and biases of song\nlyrics. All prior works to understand these two aspects are limited to manual\nanalysis of a small corpus of song lyrics. In contrast, we analyzed more than\nhalf a million songs spread over five decades. We characterize the lyrics style\nin terms of vocabulary, length, repetitiveness, speed, and readability. We have\nobserved that the style of popular songs significantly differs from other\nsongs. We have used distributed representation methods and WEAT test to measure\nvarious gender and racial biases in the song lyrics. We have observed that\nbiases in song lyrics correlate with prior results on human subjects. This\ncorrelation indicates that song lyrics reflect the biases that exist in\nsociety. Increasing consumption of music and the effect of lyrics on human\nemotions makes this analysis important.",
    "published_date": "2019-07-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07818v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07739v1",
    "title": "Deep Multi-View Learning via Task-Optimal CCA",
    "authors": [
      "Heather D. Couture",
      "Roland Kwitt",
      "J. S. Marron",
      "Melissa Troester",
      "Charles M. Perou",
      "Marc Niethammer"
    ],
    "author_ids": [],
    "abstract": "Canonical Correlation Analysis (CCA) is widely used for multimodal data\nanalysis and, more recently, for discriminative tasks such as multi-view\nlearning; however, it makes no use of class labels. Recent CCA methods have\nstarted to address this weakness but are limited in that they do not\nsimultaneously optimize the CCA projection for discrimination and the CCA\nprojection itself, or they are linear only. We address these deficiencies by\nsimultaneously optimizing a CCA-based and a task objective in an end-to-end\nmanner. Together, these two objectives learn a non-linear CCA projection to a\nshared latent space that is highly correlated and discriminative. Our method\nshows a significant improvement over previous state-of-the-art (including deep\nsupervised approaches) for cross-view classification, regularization with a\nsecond view, and semi-supervised learning on real data.",
    "published_date": "2019-07-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07739v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07733v2",
    "title": "Quantum Codes of Maximal Distance and Highly Entangled Subspaces",
    "authors": [
      "Felix Huber",
      "Markus Grassl"
    ],
    "author_ids": [],
    "abstract": "We present new bounds on the existence of general quantum maximum distance\nseparable codes (QMDS): the length $n$ of all QMDS codes with local dimension\n$D$ and distance $d \\geq 3$ is bounded by $n \\leq D^2 + d - 2$. We obtain their\nweight distribution and present additional bounds that arise from Rains' shadow\ninequalities. Our main result can be seen as a generalization of bounds that\nare known for the two special cases of stabilizer QMDS codes and absolutely\nmaximally entangled states, and confirms the quantum MDS conjecture in the\nspecial case of distance-three codes. As the existence of QMDS codes is linked\nto that of highly entangled subspaces (in which every vector has uniform\n$r$-body marginals) of maximal dimension, our methods directly carry over to\naddress questions in multipartite entanglement.",
    "published_date": "2019-07-17T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07733v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.07493v1",
    "title": "Canada Protocol: an ethical checklist for the use of Artificial Intelligence in Suicide Prevention and Mental Health",
    "authors": [
      "Carl-Maria Mörch",
      "Abhishek Gupta",
      "Brian L. Mishara"
    ],
    "author_ids": [],
    "abstract": "Introduction: To improve current public health strategies in suicide\nprevention and mental health, governments, researchers and private companies\nincreasingly use information and communication technologies, and more\nspecifically Artificial Intelligence and Big Data. These technologies are\npromising but raise ethical challenges rarely covered by current legal systems.\nIt is essential to better identify, and prevent potential ethical risks.\nObjectives: The Canada Protocol - MHSP is a tool to guide and support\nprofessionals, users, and researchers using AI in mental health and suicide\nprevention. Methods: A checklist was constructed based upon ten international\nreports on AI and ethics and two guides on mental health and new technologies.\n329 recommendations were identified, of which 43 were considered as applicable\nto Mental Health and AI. The checklist was validated, using a two round Delphi\nConsultation. Results: 16 experts participated in the first round of the Delphi\nConsultation and 8 participated in the second round. Of the original 43 items,\n38 were retained. They concern five categories: \"Description of the Autonomous\nIntelligent System\" (n=8), \"Privacy and Transparency\" (n=8), \"Security\" (n=6),\n\"Health-Related Risks\" (n=8), \"Biases\" (n=8). The checklist was considered\nrelevant by most users, and could need versions tailored to each category of\ntarget users.",
    "published_date": "2019-07-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07493v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07475v2",
    "title": "Computational Human Dynamics",
    "authors": [
      "Márton Karsai"
    ],
    "author_ids": [],
    "abstract": "This thesis summarises my scientific contributions in the domain of network\nscience, human dynamics and computational social science. These contributions\nare associated to computer science, physics, statistics, and applied\nmathematics. The goal of this thesis is twofold, on one hand to write a concise\nsummary of my most interesting scientific contributions, and on the other hand\nto provide an up-to-date view and perspective about my field. I start my\ndissertation with an introduction to position the reader on the landscape of my\nfield and to put in perspective my contributions. In the second chapter I\nconcentrate on my works on bursty human dynamics, addressing heterogeneous\ntemporal characters of human actions and interactions. Next, I discuss my\ncontributions to the field of temporal networks and give a synthesises of my\nworks on various methods of the representation, characterisation, and modelling\nof time-varying structures. Finally, I discuss my works on the data-driven\nobservations and modelling of collective social phenomena. There, I summarise\nstudies on the static observations of emergent patterns of socioeconomic\ninequalities and their correlations with social-communication networks, and\nwith linguistic patterns. I also discuss dynamic observations and modelling of\nsocial contagion processes.",
    "published_date": "2019-07-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07475v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.09279v1",
    "title": "Almost Group Envy-free Allocation of Indivisible Goods and Chores",
    "authors": [
      "Haris Aziz",
      "Simon Rey"
    ],
    "author_ids": [],
    "abstract": "We consider a multi-agent resource allocation setting in which an agent's\nutility may decrease or increase when an item is allocated. We take the group\nenvy-freeness concept that is well-established in the literature and present\nstronger and relaxed versions that are especially suitable for the allocation\nof indivisible items. Of particular interest is a concept called group\nenvy-freeness up to one item (GEF1). We then present a clear taxonomy of the\nfairness concepts. We study which fairness concepts guarantee the existence of\na fair allocation under which preference domain. For two natural classes of\nadditive utilities, we design polynomial-time algorithms to compute a GEF1\nallocation. We also prove that checking whether a given allocation satisfies\nGEF1 is coNP-complete when there are either only goods, only chores or both.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09279v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07253v1",
    "title": "Fairness and Diversity in the Recommendation and Ranking of Participatory Media Content",
    "authors": [
      "Muskaan",
      "Mehak Preet Dhaliwal",
      "Aaditeshwar Seth"
    ],
    "author_ids": [],
    "abstract": "Online participatory media platforms that enable one-to-many communication\namong users, see a significant amount of user generated content and\nconsequently face a problem of being able to recommend a subset of this content\nto its users. We address the problem of recommending and ranking this content\nsuch that different viewpoints about a topic get exposure in a fair and diverse\nmanner. We build our model in the context of a voice-based participatory media\nplatform running in rural central India, for low-income and less-literate\ncommunities, that plays audio messages in a ranked list to users over a phone\ncall and allows them to contribute their own messages. In this paper, we\ndescribe our model and evaluate it using call-logs from the platform, to\ncompare the fairness and diversity performance of our model with the manual\neditorial processes currently being followed. Our models are generic and can be\nadapted and applied to other participatory media platforms as well.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07253v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.07237v1",
    "title": "FAHT: An Adaptive Fairness-aware Decision Tree Classifier",
    "authors": [
      "Wenbin Zhang",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "Automated data-driven decision-making systems are ubiquitous across a wide\nspread of online as well as offline services. These systems, depend on\nsophisticated learning algorithms and available data, to optimize the service\nfunction for decision support assistance. However, there is a growing concern\nabout the accountability and fairness of the employed models by the fact that\noften the available historic data is intrinsically discriminatory, i.e., the\nproportion of members sharing one or more sensitive attributes is higher than\nthe proportion in the population as a whole when receiving positive\nclassification, which leads to a lack of fairness in decision support system. A\nnumber of fairness-aware learning methods have been proposed to handle this\nconcern. However, these methods tackle fairness as a static problem and do not\ntake the evolution of the underlying stream population into consideration. In\nthis paper, we introduce a learning mechanism to design a fair classifier for\nonline stream based decision-making. Our learning model, FAHT (Fairness-Aware\nHoeffding Tree), is an extension of the well-known Hoeffding Tree algorithm for\ndecision tree induction over streams, that also accounts for fairness. Our\nexperiments show that our algorithm is able to deal with discrimination in\nstreaming environments, while maintaining a moderate predictive performance\nover the stream.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07237v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07228v1",
    "title": "Modeling Human Annotation Errors to Design Bias-Aware Systems for Social Stream Processing",
    "authors": [
      "Rahul Pandey",
      "Carlos Castillo",
      "Hemant Purohit"
    ],
    "author_ids": [],
    "abstract": "High-quality human annotations are necessary to create effective machine\nlearning systems for social media. Low-quality human annotations indirectly\ncontribute to the creation of inaccurate or biased learning systems. We show\nthat human annotation quality is dependent on the ordering of instances shown\nto annotators (referred as 'annotation schedule'), and can be improved by local\nchanges in the instance ordering provided to the annotators, yielding a more\naccurate annotation of the data stream for efficient real-time social media\nanalytics. We propose an error-mitigating active learning algorithm that is\nrobust with respect to some cases of human errors when deciding an annotation\nschedule. We validate the human error model and evaluate the proposed algorithm\nagainst strong baselines by experimenting on classification tasks of relevant\nsocial media posts during crises. According to these experiments, considering\nthe order in which data instances are presented to human annotators leads to\nboth an increase in accuracy for machine learning and awareness toward some\npotential biases in human learning that may affect the automated classifier.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07228v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.07223v1",
    "title": "Fairness-enhancing interventions in stream classification",
    "authors": [
      "Vasileios Iosifidis",
      "Thi Ngoc Han Tran",
      "Eirini Ntoutsi"
    ],
    "author_ids": [],
    "abstract": "The wide spread usage of automated data-driven decision support systems has\nraised a lot of concerns regarding accountability and fairness of the employed\nmodels in the absence of human supervision. Existing fairness-aware approaches\ntackle fairness as a batch learning problem and aim at learning a fair model\nwhich can then be applied to future instances of the problem. In many\napplications, however, the data comes sequentially and its characteristics\nmight evolve with time. In such a setting, it is counter-intuitive to \"fix\" a\n(fair) model over the data stream as changes in the data might incur changes in\nthe underlying model therefore, affecting its fairness. In this work, we\npropose fairness-enhancing interventions that modify the input data so that the\noutcome of any stream classifier applied to that data will be fair. Experiments\non real and synthetic data show that our approach achieves good predictive\nperformance and low discrimination scores over the course of the stream.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07223v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.09454v1",
    "title": "A Fog Computing Framework for Autonomous Driving Assist: Architecture, Experiments, and Challenges",
    "authors": [
      "Muthucumaru Maheswaran",
      "Tianzi Yang",
      "Salman Memon"
    ],
    "author_ids": [],
    "abstract": "Autonomous driving is expected to provide a range of far-reaching economic,\nenvironmental and safety benefits. In this study, we propose a fog computing\nbased framework to assist autonomous driving. Our framework relies on overhead\nviews from cameras and data streams from vehicle sensors to create a network of\ndistributed digital twins, called an edge twin, on fog machines. The edge twin\nwill be continuously updated with the locations of both autonomous and\nhuman-piloted vehicles on the road segments. The vehicle locations will be\nharvested from overhead cameras as well as location feeds from the vehicles\nthemselves. Although the edge twin can make fair road space allocations from a\nglobal viewpoint, there is a communication cost (delay) in reaching it from the\ncameras and vehicular sensors. To address this, we introduce a machine learning\nforecaster as a part of the edge twin which is responsible for predicting the\nfuture location of vehicles. Lastly, we introduce a box algorithm that will use\nthe forecasted values to create a hazard map for the road segment which would\nbe used by the framework to suggest safe manoeuvres for the autonomous vehicles\nsuch as lane changes and accelerations. We present the complete fog computing\nframework for autonomous driving assist and evaluate key portions of the\nproposed framework using simulations based on a real-world dataset of vehicle\nposition traces on a highway",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.09454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.08292v1",
    "title": "Compositional Deep Learning",
    "authors": [
      "Bruno Gavranović"
    ],
    "author_ids": [],
    "abstract": "Neural networks have become an increasingly popular tool for solving many\nreal-world problems. They are a general framework for differentiable\noptimization which includes many other machine learning approaches as special\ncases. In this thesis we build a category-theoretic formalism around a class of\nneural networks exemplified by CycleGAN. CycleGAN is a collection of neural\nnetworks, closed under composition, whose inductive bias is increased by\nenforcing composition invariants, i.e. cycle-consistencies. Inspired by\nFunctorial Data Migration, we specify the interconnection of these networks\nusing a categorical schema, and network instances as set-valued functors on\nthis schema. We also frame neural network architectures, datasets, models, and\na number of other concepts in a categorical setting and thus show a special\nclass of functors, rather than functions, can be learned using gradient\ndescent. We use the category-theoretic framework to conceive a novel neural\nnetwork architecture whose goal is to learn the task of object insertion and\nobject deletion in images with unpaired data. We test the architecture on three\ndifferent datasets and obtain promising results.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.CT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.08292v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.06809v1",
    "title": "Ethical Underpinnings in the Design and Management of ICT Projects",
    "authors": [
      "Aaditeshwar Seth"
    ],
    "author_ids": [],
    "abstract": "With a view towards understanding why undesirable outcomes often arise in ICT\nprojects, we draw attention to three aspects in this essay. First, we present\nseveral examples to show that incorporating an ethical framework in the design\nof an ICT system is not sufficient in itself, and that ethics need to guide the\ndeployment and ongoing management of the projects as well. We present a\nframework that brings together the objectives, design, and deployment\nmanagement of ICT projects as being shaped by a common underlying ethical\nsystem. Second, we argue that power-based equality should be incorporated as a\nkey underlying ethical value in ICT projects, to ensure that the project does\nnot reinforce inequalities in power relationships between the actors directly\nor indirectly associated with the project. We present a method to model ICT\nprojects to make legible its influence on the power relationships between\nvarious actors in the ecosystem. Third, we discuss that the ethical values\nunderlying any ICT project ultimately need to be upheld by the project teams,\nwhere certain factors like political ideologies or dispersed teams may affect\nthe rigour with which these ethical values are followed. These three aspects of\nhaving an ethical underpinning to the design and management of ICT projects,\nthe need for having a power-based equality principle for ICT projects, and the\nimportance of socialization of the project teams, needs increasing attention in\ntoday's age of ICT platforms where millions and billions of users interact on\nthe same platform but which are managed by only a few people.",
    "published_date": "2019-07-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06809v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.06430v1",
    "title": "A Causal Bayesian Networks Viewpoint on Fairness",
    "authors": [
      "Silvia Chiappa",
      "William S. Isaac"
    ],
    "author_ids": [],
    "abstract": "We offer a graphical interpretation of unfairness in a dataset as the\npresence of an unfair causal path in the causal Bayesian network representing\nthe data-generation mechanism. We use this viewpoint to revisit the recent\ndebate surrounding the COMPAS pretrial risk assessment tool and, more\ngenerally, to point out that fairness evaluation on a model requires careful\nconsiderations on the patterns of unfairness underlying the training data. We\nshow that causal Bayesian networks provide us with a powerful tool to measure\nunfairness in a dataset and to design fair models in complex unfairness\nscenarios.",
    "published_date": "2019-07-15T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.06360v1",
    "title": "The Elusive Model of Technology, Media, Social Development, and Financial Sustainability",
    "authors": [
      "Aaditeshwar Seth"
    ],
    "author_ids": [],
    "abstract": "We recount in this essay the decade-long story of Gram Vaani, a social\nenterprise with a vision to build appropriate ICTs (Information and\nCommunication Technologies) for participatory media in rural and low-income\nsettings, to bring about social development and community empowerment. Other\nsocial enterprises will relate to the learning gained and the strategic pivots\nthat Gram Vaani had to undertake to survive and deliver on its mission, while\nsearching for a robust financial sustainability model. While we believe the\nideal model still remains elusive, we conclude this essay with an open question\nabout the reason to differentiate between different kinds of enterprises -\ncommercial or social, for-profit or not-for-profit - and argue that all\nenterprises should have an ethical underpinning to their work.",
    "published_date": "2019-07-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06360v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.06338v1",
    "title": "A dynamic over games drives selfish agents to win-win outcomes",
    "authors": [
      "Seth Frey",
      "Curtis Atkisson"
    ],
    "author_ids": [],
    "abstract": "Understanding the evolution of human social systems requires flexible\nformalisms for the emergence of institutions. Although game theory is normally\nused to model interactions individually, larger spaces of games can be helpful\nfor modeling how interactions change. We introduce a framework for modeling\n\"institutional evolution,\" how individuals change the games they are placed in.\nWe contrast this with the more familiar within-game \"behavioral evolution\".\nStarting from an initial game, agents trace trajectories through game space by\nrepeatedly navigating to more preferable games until they converge on attractor\ngames that are preferred to all others. Agents choose between games on the\nbasis of their \"institutional preferences,\" which define between-game\ncomparisons in terms of game-level features such as stability, fairness, and\nefficiency. Computing institutional change trajectories over the two-player\nspace, we find that the attractors of self-interested economic agents\nover-represent fairness by 100% relative to baseline, even though those agents\nare indifferent to fairness. This seems to occur because fairness, as a game\nfeature, co-occurs with the self-serving features these agents do prefer. We\nthus present institutional evolution as a mechanism for encouraging the\nspontaneous emergence of cooperation among inherently selfish agents. We then\nextend these findings beyond two players, and to two other types of\nevolutionary agent: the relative fitness maximizing agent of evolutionary game\ntheory (who maximizes inequality), and the relative group fitness maximizing\nagent of multi-level/group selection theory (who minimizes inequality). This\nwork provides a flexible, testable formalism for modeling the interdependencies\nof behavioral and institutional evolutionary processes.",
    "published_date": "2019-07-15T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "nlin.AO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06338v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.06286v1",
    "title": "Autoencoding sensory substitution",
    "authors": [
      "Viktor Tóth",
      "Lauri Parkkonen"
    ],
    "author_ids": [],
    "abstract": "Tens of millions of people live blind, and their number is ever increasing.\nVisual-to-auditory sensory substitution (SS) encompasses a family of cheap,\ngeneric solutions to assist the visually impaired by conveying visual\ninformation through sound. The required SS training is lengthy: months of\neffort is necessary to reach a practical level of adaptation. There are two\nreasons for the tedious training process: the elongated substituting audio\nsignal, and the disregard for the compressive characteristics of the human\nhearing system. To overcome these obstacles, we developed a novel class of SS\nmethods, by training deep recurrent autoencoders for image-to-sound conversion.\nWe successfully trained deep learning models on different datasets to execute\nvisual-to-auditory stimulus conversion. By constraining the visual space, we\ndemonstrated the viability of shortened substituting audio signals, while\nproposing mechanisms, such as the integration of computational hearing models,\nto optimally convey visual features in the substituting stimulus as\nperceptually discernible auditory components. We tested our approach in two\nseparate cases. In the first experiment, the author went blindfolded for 5\ndays, while performing SS training on hand posture discrimination. The second\nexperiment assessed the accuracy of reaching movements towards objects on a\ntable. In both test cases, above-chance-level accuracy was attained after a few\nhours of training. Our novel SS architecture broadens the horizon of\nrehabilitation methods engineered for the visually impaired. Further\nimprovements on the proposed model shall yield hastened rehabilitation of the\nblind and a wider adaptation of SS devices as a consequence.",
    "published_date": "2019-07-14T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.NC",
      "cs.CV",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06286v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.06260v1",
    "title": "Counterfactual Reasoning for Fair Clinical Risk Prediction",
    "authors": [
      "Stephen Pfohl",
      "Tony Duan",
      "Daisy Yi Ding",
      "Nigam H. Shah"
    ],
    "author_ids": [],
    "abstract": "The use of machine learning systems to support decision making in healthcare\nraises questions as to what extent these systems may introduce or exacerbate\ndisparities in care for historically underrepresented and mistreated groups,\ndue to biases implicitly embedded in observational data in electronic health\nrecords. To address this problem in the context of clinical risk prediction\nmodels, we develop an augmented counterfactual fairness criteria to extend the\ngroup fairness criteria of equalized odds to an individual level. We do so by\nrequiring that the same prediction be made for a patient, and a counterfactual\npatient resulting from changing a sensitive attribute, if the factual and\ncounterfactual outcomes do not differ. We investigate the extent to which the\naugmented counterfactual fairness criteria may be applied to develop fair\nmodels for prolonged inpatient length of stay and mortality with observational\nelectronic health records data. As the fairness criteria is ill-defined without\nknowledge of the data generating process, we use a variational autoencoder to\nperform counterfactual inference in the context of an assumed causal graph.\nWhile our technique provides a means to trade off maintenance of fairness with\nreduction in predictive performance in the context of a learned generative\nmodel, further work is needed to assess the generality of this approach.",
    "published_date": "2019-07-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06260v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.06130v5",
    "title": "Quantifying the Vulnerabilities of the Online Public Square to Adversarial Manipulation Tactics",
    "authors": [
      "Bao Tran Truong",
      "Xiaodan Lou",
      "Alessandro Flammini",
      "Filippo Menczer"
    ],
    "author_ids": [],
    "abstract": "Social media, seen by some as the modern public square, is vulnerable to\nmanipulation. By controlling inauthentic accounts impersonating humans,\nmalicious actors can amplify disinformation within target communities. The\nconsequences of such operations are difficult to evaluate due to the challenges\nposed by collecting data and carrying out ethical experiments that would\ninfluence online communities. Here we use a social media model that simulates\ninformation diffusion in an empirical network to quantify the impacts of\nseveral adversarial manipulation tactics on the quality of content. We find\nthat the presence of influential accounts, a hallmark of social media,\nexacerbates the vulnerabilities of online communities to manipulation. Among\nthe explored tactics that bad actors can employ, infiltrating a community is\nthe most likely to make low-quality content go viral. Such harm can be further\ncompounded by inauthentic agents flooding the network with low-quality, yet\nappealing content, but is mitigated when bad actors focus on specific targets,\nsuch as influential or vulnerable individuals. These insights suggest\ncountermeasures that platforms could employ to increase the resilience of\nsocial media users to manipulation.",
    "published_date": "2019-07-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06130v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.06098v1",
    "title": "Seeker based Adaptive Guidance via Reinforcement Meta-Learning Applied to Asteroid Close Proximity Operations",
    "authors": [
      "Brian Gaudet",
      "Richard Linares",
      "Roberto Furfaro"
    ],
    "author_ids": [],
    "abstract": "Current practice for asteroid close proximity maneuvers requires extremely\naccurate characterization of the environmental dynamics and precise spacecraft\npositioning prior to the maneuver. This creates a delay of several months\nbetween the spacecraft's arrival and the ability to safely complete close\nproximity maneuvers. In this work we develop an adaptive integrated guidance,\nnavigation, and control system that can complete these maneuvers in\nenvironments with unknown dynamics, with initial conditions spanning a large\ndeployment region, and without a shape model of the asteroid. The system is\nimplemented as a policy optimized using reinforcement meta-learning. The\nspacecraft is equipped with an optical seeker that locks to either a terrain\nfeature, back-scattered light from a targeting laser, or an active beacon, and\nthe policy maps observations consisting of seeker angles and LIDAR range\nreadings directly to engine thrust commands. The policy implements a recurrent\nnetwork layer that allows the deployed policy to adapt real time to both\nenvironmental forces acting on the agent and internal disturbances such as\nactuator failure and center of mass variation. We validate the guidance system\nthrough simulated landing maneuvers in a six degrees-of-freedom simulator. The\nsimulator randomizes the asteroid's characteristics such as solar radiation\npressure, density, spin rate, and nutation angle, requiring the guidance and\ncontrol system to adapt to the environment. We also demonstrate robustness to\nactuator failure, sensor bias, and changes in the spacecraft's center of mass\nand inertia tensor. Finally, we suggest a concept of operations for asteroid\nclose proximity maneuvers that is compatible with the guidance system.",
    "published_date": "2019-07-13T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "astro-ph.IM",
      "cs.LG",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06098v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.06010v1",
    "title": "The Futility of Bias-Free Learning and Search",
    "authors": [
      "George D. Montanez",
      "Jonathan Hayase",
      "Julius Lauw",
      "Dominique Macias",
      "Akshay Trikha",
      "Julia Vendemiatti"
    ],
    "author_ids": [],
    "abstract": "Building on the view of machine learning as search, we demonstrate the\nnecessity of bias in learning, quantifying the role of bias (measured relative\nto a collection of possible datasets, or more generally, information resources)\nin increasing the probability of success. For a given degree of bias towards a\nfixed target, we show that the proportion of favorable information resources is\nstrictly bounded from above. Furthermore, we demonstrate that bias is a\nconserved quantity, such that no algorithm can be favorably biased towards many\ndistinct targets simultaneously. Thus bias encodes trade-offs. The probability\nof success for a task can also be measured geometrically, as the angle of\nagreement between what holds for the actual task and what is assumed by the\nalgorithm, represented in its bias. Lastly, finding a favorably biasing\ndistribution over a fixed set of information resources is provably difficult,\nunless the set of resources itself is already favorable with respect to the\ngiven task and algorithm.",
    "published_date": "2019-07-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06010v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.06001v1",
    "title": "The Two-Sided Game of Googol and Sample-Based Prophet Inequalities",
    "authors": [
      "José Correa",
      "Andrés Cristi",
      "Boris Epstein",
      "José A. Soto"
    ],
    "author_ids": [],
    "abstract": "The secretary problem or the game of Googol are classic models for online\nselection problems that have received significant attention in the last five\ndecades. We consider a variant of the problem and explore its connections to\ndata-driven online selection. Specifically, we are given $n$ cards with\narbitrary non-negative numbers written on both sides. The cards are randomly\nplaced on $n$ consecutive positions on a table, and for each card, the visible\nside is also selected at random. The player sees the visible side of all cards\nand wants to select the card with the maximum hidden value. To this end, the\nplayer flips the first card, sees its hidden value and decides whether to pick\nit or drop it and continue with the next card.\n  We study algorithms for two natural objectives. In the first one, as in the\nsecretary problem, the player wants to maximize the probability of selecting\nthe maximum hidden value. We show that this can be done with probability at\nleast $0.45292$. In the second one, similar to the prophet inequality, the\nplayer maximizes the expectation of the selected hidden value. We show a\nguarantee of at least $0.63518$ with respect to the expected maximum hidden\nvalue.\n  Our algorithms result from combining three basic strategies. One is to stop\nwhenever we see a value larger than the initial $n$ visible numbers. The second\none is to stop the first time the last flipped card's value is the largest of\nthe currently $n$ visible numbers in the table. And the third one is similar to\nthe latter but it additionally requires that the last flipped value is larger\nthan the value on the other side of its card.\n  We apply our results to the prophet secretary problem with unknown\ndistributions, but with access to a single sample from each distribution. Our\nguarantee improves upon $1-1/e$ for this problem, which is the currently best\nknown guarantee and only works for the i.i.d. case.",
    "published_date": "2019-07-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06001v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.05878v3",
    "title": "Composing Neural Learning and Symbolic Reasoning with an Application to Visual Discrimination",
    "authors": [
      "Adithya Murali",
      "Atharva Sehgal",
      "Paul Krogmeier",
      "P. Madhusudan"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of combining machine learning models to perform\nhigher-level cognitive tasks with clear specifications. We propose the novel\nproblem of Visual Discrimination Puzzles (VDP) that requires finding\ninterpretable discriminators that classify images according to a logical\nspecification. Humans can solve these puzzles with ease and they give robust,\nverifiable, and interpretable discriminators as answers. We propose a\ncompositional neurosymbolic framework that combines a neural network to detect\nobjects and relationships with a symbolic learner that finds interpretable\ndiscriminators. We create large classes of VDP datasets involving natural and\nartificial images and show that our neurosymbolic framework performs favorably\ncompared to several purely neural approaches.",
    "published_date": "2019-07-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.05878v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.05447v1",
    "title": "Grounding Value Alignment with Ethical Principles",
    "authors": [
      "Tae Wan Kim",
      "Thomas Donaldson",
      "John Hooker"
    ],
    "author_ids": [],
    "abstract": "An important step in the development of value alignment (VA) systems in AI is\nunderstanding how values can interrelate with facts. Designers of future VA\nsystems will need to utilize a hybrid approach in which ethical reasoning and\nempirical observation interrelate successfully in machine behavior. In this\narticle we identify two problems about this interrelation that have been\noverlooked by AI discussants and designers. The first problem is that many AI\ndesigners commit inadvertently a version of what has been called by moral\nphilosophers the \"naturalistic fallacy,\" that is, they attempt to derive an\n\"ought\" from an \"is.\" We illustrate when and why this occurs. The second\nproblem is that AI designers adopt training routines that fail fully to\nsimulate human ethical reasoning in the integration of ethical principles and\nfacts. Using concepts of quantified modal logic, we proceed to offer an\napproach that promises to simulate ethical reasoning in humans by connecting\nethical principles on the one hand and propositions about states of affairs on\nthe other.",
    "published_date": "2019-07-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.05447v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.05171v2",
    "title": "Privileged Features Distillation at Taobao Recommendations",
    "authors": [
      "Chen Xu",
      "Quan Li",
      "Junfeng Ge",
      "Jinyang Gao",
      "Xiaoyong Yang",
      "Changhua Pei",
      "Fei Sun",
      "Jian Wu",
      "Hanxiao Sun",
      "Wenwu Ou"
    ],
    "author_ids": [],
    "abstract": "Features play an important role in the prediction tasks of e-commerce\nrecommendations. To guarantee the consistency of off-line training and on-line\nserving, we usually utilize the same features that are both available. However,\nthe consistency in turn neglects some discriminative features. For example,\nwhen estimating the conversion rate (CVR), i.e., the probability that a user\nwould purchase the item if she clicked it, features like dwell time on the item\ndetailed page are informative. However, CVR prediction should be conducted for\non-line ranking before the click happens. Thus we cannot get such post-event\nfeatures during serving.\n  We define the features that are discriminative but only available during\ntraining as the privileged features. Inspired by the distillation techniques\nwhich bridge the gap between training and inference, in this work, we propose\nprivileged features distillation (PFD). We train two models, i.e., a student\nmodel that is the same as the original one and a teacher model that\nadditionally utilizes the privileged features. Knowledge distilled from the\nmore accurate teacher is transferred to the student to improve its accuracy.\nDuring serving, only the student part is extracted and it relies on no\nprivileged features. We conduct experiments on two fundamental prediction tasks\nat Taobao recommendations, i.e., click-through rate (CTR) at coarse-grained\nranking and CVR at fine-grained ranking. By distilling the interacted features\nthat are prohibited during serving for CTR and the post-event features for CVR,\nwe achieve significant improvements over their strong baselines. During the\non-line A/B tests, the click metric is improved by +5.0% in the CTR task. And\nthe conversion metric is improved by +2.3% in the CVR task. Besides, by\naddressing several issues of training PFD, we obtain comparable training speed\nas the baselines without any distillation.",
    "published_date": "2019-07-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.05171v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.05159v1",
    "title": "Fairness without Regret",
    "authors": [
      "Marcus Hutter"
    ],
    "author_ids": [],
    "abstract": "A popular approach of achieving fairness in optimization problems is by\nconstraining the solution space to \"fair\" solutions, which unfortunately\ntypically reduces solution quality. In practice, the ultimate goal is often an\naggregate of sub-goals without a unique or best way of combining them or which\nis otherwise only partially known. I turn this problem into a feature and\nsuggest to use a parametrized objective and vary the parameters within\nreasonable ranges to get a \"set\" of optimal solutions, which can then be\noptimized using secondary criteria such as fairness without compromising the\nprimary objective, i.e. without regret (societal cost).",
    "published_date": "2019-07-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.05159v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.05083v1",
    "title": "Cake Cutting on Graphs: A Discrete and Bounded Proportional Protocol",
    "authors": [
      "Xiaohui Bei",
      "Xiaoming Sun",
      "Hao Wu",
      "Jialin Zhang",
      "Zhijie Zhang",
      "Wei Zi"
    ],
    "author_ids": [],
    "abstract": "The classical cake cutting problem studies how to find fair allocations of a\nheterogeneous and divisible resource among multiple agents. Two of the most\ncommonly studied fairness concepts in cake cutting are proportionality and\nenvy-freeness. It is well known that a proportional allocation among $n$ agents\ncan be found efficiently via simple protocols [16]. For envy-freeness, in a\nrecent breakthrough, Aziz and Mackenzie [5] proposed a discrete and bounded\nenvy-free protocol for any number of players. However, the protocol suffers\nfrom high multiple-exponential query complexity and it remains open to find\nsimpler and more efficient envy-free protocols.\n  In this paper we consider a variation of the cake cutting problem by assuming\nan underlying graph over the agents whose edges describe their acquaintance\nrelationships, and agents evaluate their shares relatively to those of their\nneighbors. An allocation is called locally proportional if each agent thinks\nshe receives at least the average value over her neighbors. Local\nproportionality generalizes proportionality and is in an interesting middle\nground between proportionality and envy-freeness: its existence is guaranteed\nby that of an envy-free allocation, but no simple protocol is known to produce\nsuch a locally proportional allocation for general graphs. Previous works\nshowed locally proportional protocols for special classes of graphs, and it is\nlisted in both [1] and [8] as an open question to design simple locally\nproportional protocols for more general classes of graphs. In this paper we\ncompletely resolved this open question by presenting a discrete and bounded\nlocally proportional protocol for any given graphs. Our protocol has a query\ncomplexity of only single exponential, which is significantly smaller than the\nsix towers of $n$ query complexity of the envy-free protocol given in [5].",
    "published_date": "2019-07-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.05083v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.07766v1",
    "title": "Flatter is better: Percentile Transformations for Recommender Systems",
    "authors": [
      "Masoud Mansoury",
      "Robin Burke",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "It is well known that explicit user ratings in recommender systems are biased\ntowards high ratings, and that users differ significantly in their usage of the\nrating scale. Implementers usually compensate for these issues through rating\nnormalization or the inclusion of a user bias term in factorization models.\nHowever, these methods adjust only for the central tendency of users'\ndistributions. In this work, we demonstrate that lack of \\textit{flatness} in\nrating distributions is negatively correlated with recommendation performance.\nWe propose a rating transformation model that compensates for skew in the\nrating distribution as well as its central tendency by converting ratings into\npercentile values as a pre-processing step before recommendation generation.\nThis transformation flattens the rating distribution, better compensates for\ndifferences in rating distributions, and improves recommendation performance.\nWe also show a smoothed version of this transformation designed to yield more\nintuitive results for users with very narrow rating distributions. A\ncomprehensive set of experiments show improved ranking performance for these\npercentile transformations with state-of-the-art recommendation algorithms in\nfour real-world data sets.",
    "published_date": "2019-07-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.07766v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.05846v1",
    "title": "Exploring the context of course rankings on online academic forums",
    "authors": [
      "Taha Hassan",
      "Bob Edmison",
      "Larry Cox II",
      "Matthew Louvet",
      "Daron Williams"
    ],
    "author_ids": [],
    "abstract": "University students routinely use the tools provided by online course ranking\nforums to share and discuss their satisfaction with the quality of instruction\nand content in a wide variety of courses. Student perception of the efficacy of\npedagogies employed in a course is a reflection of a multitude of decisions by\nprofessors, instructional designers and university administrators. This\ncomplexity has motivated a large body of research on the utility, reliability,\nand behavioral correlates of course rankings. There is, however, little\ninvestigation of the (potential) implicit student bias on these forums towards\ndesirable course outcomes at the institution level. To that end, we examine the\nconnection between course outcomes (student-reported GPA) and the overall\nranking of the primary course instructor, as well as rating disparity by nature\nof course outcomes, based on data from two popular academic rating forums. Our\nexperiments with ranking data about over ten thousand courses taught at\nVirginia Tech and its 25 SCHEV-approved peer institutions indicate that there\nis a discernible albeit complex bias towards course outcomes in the professor\nratings registered by students.",
    "published_date": "2019-07-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.05846v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.04699v3",
    "title": "From Group Sparse Coding to Rank Minimization: A Novel Denoising Model for Low-level Image Restoration",
    "authors": [
      "Yunyi Li",
      "Guan Gui",
      "Xiefeng Cheng"
    ],
    "author_ids": [],
    "abstract": "Recently, low-rank matrix recovery theory has been emerging as a significant\nprogress for various image processing problems. Meanwhile, the group sparse\ncoding (GSC) theory has led to great successes in image restoration (IR)\nproblem with each group contains low-rank property. In this paper, we propose a\nnovel low-rank minimization based denoising model for IR tasks under the\nperspective of GSC, an important connection between our denoising model and\nrank minimization problem has been put forward. To overcome the bias problem\ncaused by convex nuclear norm minimization (NNM) for rank approximation, a more\ngeneralized and flexible rank relaxation function is employed, namely weighted\nnonconvex relaxation. Accordingly, an efficient iteratively-reweighted\nalgorithm is proposed to handle the resulting minimization problem combing with\nthe popular L_(1/2) and L_(2/3) thresholding operators. Finally, our proposed\ndenoising model is applied to IR problems via an alternating direction method\nof multipliers (ADMM) strategy. Typical IR experiments on image compressive\nsensing (CS), inpainting, deblurring and impulsive noise removal demonstrate\nthat our proposed method can achieve significantly higher PSNR/FSIM values than\nmany relevant state-of-the-art methods.",
    "published_date": "2019-07-10T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04699v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.04685v2",
    "title": "Assessing Transferability from Simulation to Reality for Reinforcement Learning",
    "authors": [
      "Fabio Muratore",
      "Michael Gienger",
      "Jan Peters"
    ],
    "author_ids": [],
    "abstract": "Learning robot control policies from physics simulations is of great interest\nto the robotics community as it may render the learning process faster,\ncheaper, and safer by alleviating the need for expensive real-world\nexperiments. However, the direct transfer of learned behavior from simulation\nto reality is a major challenge. Optimizing a policy on a slightly faulty\nsimulator can easily lead to the maximization of the `Simulation Optimization\nBias` (SOB). In this case, the optimizer exploits modeling errors of the\nsimulator such that the resulting behavior can potentially damage the robot. We\ntackle this challenge by applying domain randomization, i.e., randomizing the\nparameters of the physics simulations during learning. We propose an algorithm\ncalled Simulation-based Policy Optimization with Transferability Assessment\n(SPOTA) which uses an estimator of the SOB to formulate a stopping criterion\nfor training. The introduced estimator quantifies the over-fitting to the set\nof domains experienced while training. Our experimental results on two\ndifferent second order nonlinear systems show that the new simulation-based\npolicy search algorithm is able to learn a control policy exclusively from a\nrandomized simulator, which can be applied directly to real systems without any\nadditional training.",
    "published_date": "2019-07-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04685v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.04472v3",
    "title": "The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning",
    "authors": [
      "Suyun Liu",
      "Luis Nunes Vicente"
    ],
    "author_ids": [],
    "abstract": "Optimization of conflicting functions is of paramount importance in decision\nmaking, and real world applications frequently involve data that is uncertain\nor unknown, resulting in multi-objective optimization (MOO) problems of\nstochastic type. We study the stochastic multi-gradient (SMG) method, seen as\nan extension of the classical stochastic gradient method for single-objective\noptimization.\n  At each iteration of the SMG method, a stochastic multi-gradient direction is\ncalculated by solving a quadratic subproblem, and it is shown that this\ndirection is biased even when all individual gradient estimators are unbiased.\nWe establish rates to compute a point in the Pareto front, of order similar to\nwhat is known for stochastic gradient in both convex and strongly convex cases.\nThe analysis handles the bias in the multi-gradient and the unknown a priori\nweights of the limiting Pareto point.\n  The SMG method is framed into a Pareto-front type algorithm for the\ncomputation of the entire Pareto front. The Pareto-front SMG algorithm is\ncapable of robustly determining Pareto fronts for a number of synthetic test\nproblems. One can apply it to any stochastic MOO problem arising from\nsupervised machine learning, and we report results for logistic binary\nclassification where multiple objectives correspond to distinct-sources data\ngroups.",
    "published_date": "2019-07-10T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04472v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.04389v1",
    "title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference",
    "authors": [
      "Yonatan Belinkov",
      "Adam Poliak",
      "Stuart M. Shieber",
      "Benjamin Van Durme",
      "Alexander M. Rush"
    ],
    "author_ids": [],
    "abstract": "Popular Natural Language Inference (NLI) datasets have been shown to be\ntainted by hypothesis-only biases. Adversarial learning may help models ignore\nsensitive biases and spurious correlations in data. We evaluate whether\nadversarial learning can be used in NLI to encourage models to learn\nrepresentations free of hypothesis-only biases. Our analyses indicate that the\nrepresentations learned via adversarial learning may be less biased, with only\nsmall drops in NLI accuracy.",
    "published_date": "2019-07-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.04194v2",
    "title": "Adaptive Exploration for Unsupervised Person Re-Identification",
    "authors": [
      "Yuhang Ding",
      "Hehe Fan",
      "Mingliang Xu",
      "Yi Yang"
    ],
    "author_ids": [],
    "abstract": "Due to domain bias, directly deploying a deep person re-identification\n(re-ID) model trained on one dataset often achieves considerably poor accuracy\non another dataset. In this paper, we propose an Adaptive Exploration (AE)\nmethod to address the domain-shift problem for re-ID in an unsupervised manner.\nSpecifically, in the target domain, the re-ID model is inducted to 1) maximize\ndistances between all person images and 2) minimize distances between similar\nperson images. In the first case, by treating each person image as an\nindividual class, a non-parametric classifier with a feature memory is\nexploited to encourage person images to move far away from each other. In the\nsecond case, according to a similarity threshold, our method adaptively selects\nneighborhoods for each person image in the feature space. By treating these\nsimilar person images as the same class, the non-parametric classifier forces\nthem to stay closer. However, a problem of the adaptive selection is that, when\nan image has too many neighborhoods, it is more likely to attract other images\nas its neighborhoods. As a result, a minority of images may select a large\nnumber of neighborhoods while a majority of images have only a few\nneighborhoods. To address this issue, we additionally integrate a balance\nstrategy into the adaptive selection. We evaluate our methods with two\nprotocols. The first one is called \"target-only re-ID\", in which only the\nunlabeled target data is used for training. The second one is called \"domain\nadaptive re-ID\", in which both the source data and the target data are used\nduring training. Experimental results on large-scale re-ID datasets demonstrate\nthe effectiveness of our method. Our code has been released at\nhttps://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.",
    "published_date": "2019-07-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04194v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.04103v1",
    "title": "Historical comparison of gender inequality in scientific careers across countries and disciplines",
    "authors": [
      "Junming Huang",
      "Alexander J. Gates",
      "Roberta Sinatra",
      "Albert-Laszlo Barabasi"
    ],
    "author_ids": [],
    "abstract": "There is extensive, yet fragmented, evidence of gender differences in\nacademia suggesting that women are under-represented in most scientific\ndisciplines, publish fewer articles throughout a career, and their work\nacquires fewer citations. Here, we offer a comprehensive picture of\nlongitudinal gender discrepancies in performance through a bibliometric\nanalysis of academic careers by reconstructing the complete publication history\nof over 1.5 million gender-identified authors whose publishing career ended\nbetween 1955 and 2010, covering 83 countries and 13 disciplines. We find that,\nparadoxically, the increase of participation of women in science over the past\n60 years was accompanied by an increase of gender differences in both\nproductivity and impact. Most surprisingly though, we uncover two gender\ninvariants, finding that men and women publish at a comparable annual rate and\nhave equivalent career-wise impact for the same size body of work. Finally, we\ndemonstrate that differences in dropout rates and career length explain a large\nportion of the reported career-wise differences in productivity and impact.\nThis comprehensive picture of gender inequality in academia can help rephrase\nthe conversation around the sustainability of women's careers in academia, with\nimportant consequences for institutions and policy makers.",
    "published_date": "2019-07-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04103v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.04102v1",
    "title": "Quantifying Confounding Bias in Neuroimaging Datasets with Causal Inference",
    "authors": [
      "Christian Wachinger",
      "Benjamin Gutierrez Becker",
      "Anna Rieckmann",
      "Sebastian Pölsterl"
    ],
    "author_ids": [],
    "abstract": "Neuroimaging datasets keep growing in size to address increasingly complex\nmedical questions. However, even the largest datasets today alone are too small\nfor training complex machine learning models. A potential solution is to\nincrease sample size by pooling scans from several datasets. In this work, we\ncombine 12,207 MRI scans from 15 studies and show that simple pooling is often\nill-advised due to introducing various types of biases in the training data.\nFirst, we systematically define these biases. Second, we detect bias by\nexperimentally showing that scans can be correctly assigned to their respective\ndataset with 73.3% accuracy. Finally, we propose to tell causal from\nconfounding factors by quantifying the extent of confounding and causality in a\nsingle dataset using causal inference. We achieve this by finding the simplest\ngraphical model in terms of Kolmogorov complexity. As Kolmogorov complexity is\nnot directly computable, we employ the minimum description length to\napproximate it. We empirically show that our approach is able to estimate\nplausible causal relationships from real neuroimaging data.",
    "published_date": "2019-07-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.04102v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.03718v1",
    "title": "CobWeb: A Research Prototype for Exploring User Bias in Political Fact-Checking",
    "authors": [
      "Anubrata Das",
      "Kunjan Mehta",
      "Matthew Lease"
    ],
    "author_ids": [],
    "abstract": "The effect of user bias in fact-checking has not been explored extensively\nfrom a user-experience perspective. We estimate the user bias as a function of\nthe user's perceived reputation of the news sources (e.g., a user with liberal\nbeliefs may tend to trust liberal sources). We build an interface to\ncommunicate the role of estimated user bias in the context of a fact-checking\ntask. We also explore the utility of helping users visualize their detected\nlevel of bias. 80% of the users of our system find that the presence of an\nindicator for user bias is useful in judging the veracity of a political claim.",
    "published_date": "2019-07-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.03718v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.03644v2",
    "title": "Unsupervised Domain Alignment to Mitigate Low Level Dataset Biases",
    "authors": [
      "Kirthi Shankar Sivamani"
    ],
    "author_ids": [],
    "abstract": "Dataset bias is a well-known problem in the field of computer vision. The\npresence of implicit bias in any image collection hinders a model trained and\nvalidated on a particular dataset to yield similar accuracies when tested on\nother datasets. In this paper, we propose a novel debiasing technique to reduce\nthe effects of a biased training dataset. Our goal is to augment the training\ndata using a generative network by learning a non-linear mapping from the\nsource domain (training set) to the target domain (testing set) while retaining\ntraining set labels. The cycle consistency loss and adversarial loss for\ngenerative adversarial networks are used to learn the mapping. A structured\nsimilarity index (SSIM) loss is used to enforce label retention while\naugmenting the training set. Our methods and hypotheses are supported by\nquantitative comparisons with prior debiasing techniques. These comparisons\nshowcase the superiority of our method and its potential to mitigate the\neffects of dataset bias during the inference stage.",
    "published_date": "2019-07-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.03644v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.02956v1",
    "title": "The FACTS of Technology-Assisted Sensitivity Review",
    "authors": [
      "Graham McDonald",
      "Craig Macdonald",
      "Iadh Ounis"
    ],
    "author_ids": [],
    "abstract": "At least ninety countries implement Freedom of Information laws that state\nthat government documents must be made freely available, or opened, to the\npublic. However, many government documents contain sensitive information, such\nas personal or confidential information. Therefore, all government documents\nthat are opened to the public must first be reviewed to identify, and protect,\nany sensitive information. Historically, sensitivity review has been a\ncompletely manual process. However, with the adoption of born-digital\ndocuments, such as e-mail, human-only sensitivity review is not practical and\nthere is a need for new technologies to assist human sensitivity reviewers. In\nthis paper, we discuss how issues of fairness, accountability, confidentiality,\ntransparency and safety (FACTS) impact technology-assisted sensitivity review.\nMoreover, we outline some important areas of future FACTS research that will\nneed to be addressed within technology-assisted sensitivity review.",
    "published_date": "2019-07-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02956v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.02908v1",
    "title": "On Inductive Biases in Deep Reinforcement Learning",
    "authors": [
      "Matteo Hessel",
      "Hado van Hasselt",
      "Joseph Modayil",
      "David Silver"
    ],
    "author_ids": [],
    "abstract": "Many deep reinforcement learning algorithms contain inductive biases that\nsculpt the agent's objective and its interface to the environment. These\ninductive biases can take many forms, including domain knowledge and pretuned\nhyper-parameters. In general, there is a trade-off between generality and\nperformance when algorithms use such biases. Stronger biases can lead to faster\nlearning, but weaker biases can potentially lead to more general algorithms.\nThis trade-off is important because inductive biases are not free; substantial\neffort may be required to obtain relevant domain knowledge or to tune\nhyper-parameters effectively. In this paper, we re-examine several\ndomain-specific components that bias the objective and the environmental\ninterface of common deep reinforcement learning agents. We investigated whether\nthe performance deteriorates when these components are replaced with adaptive\nsolutions from the literature. In our experiments, performance sometimes\ndecreased with the adaptive components, as one might expect when comparing to\ncomponents crafted for the domain, but sometimes the adaptive components\nperformed better. We investigated the main benefit of having fewer\ndomain-specific components, by comparing the learning performance of the two\nsystems on a different set of continuous control problems, without additional\ntuning of either system. As hypothesized, the system with adaptive components\nperformed better on many of the new tasks.",
    "published_date": "2019-07-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02908v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.02761v1",
    "title": "Distributed User Clustering and Resource Allocation for Imperfect NOMA in Heterogeneous Networks",
    "authors": [
      "Abdulkadir Celik",
      "Ming-Cheng Tsai",
      "Redha M. Radaydeh",
      "Fawaz S. Al-Qahtani",
      "Mohamed-Slim Alouini"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a distributed cluster formation (CF) and resource\nallocation (RA) framework for non-ideal non-orthogonal multiple access (NOMA)\nschemes in heterogeneous networks. The imperfection of the underlying NOMA\nscheme is due to the receiver sensitivity and interference residue from\nnon-ideal successive interference cancellation (SIC), which is generally\ncharacterized by a fractional error factor (FEF). Our analytical findings first\nshow that several factors have a significant impact on the achievable NOMA\ngain. Then, we investigate fundamental limits on NOMA cluster size as a\nfunction of FEF levels, cluster bandwidth, and quality of service (QoS) demands\nof user equipments (UEs). Thereafter, a clustering algorithm is developed by\ntaking feasible cluster size and channel gain disparity of UEs into account.\nFinally, we develop a distributed alpha-fair RA framework where alpha governs\nthe trade-off between maximum throughput and proportional fairness objectives.\nBased on the derived closed-form optimal power levels, the proposed distributed\nsolution iteratively updates bandwidths, clusters, and UEs' transmission\npowers. Numerical results demonstrate that proposed solutions deliver a higher\nspectral and energy efficiency than traditionally adopted basic NOMA cluster\nsize of two. We also show that an imperfect NOMA cannot always provide better\nperformance than orthogonal multiple access under certain conditions. Finally,\nour numerical investigations reveal that NOMA gain is maximized under\ndownlink/uplink decoupled (DUDe) UE association.",
    "published_date": "2019-07-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02761v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.02583v3",
    "title": "Fair Division through Information Withholding",
    "authors": [
      "Hadi Hosseini",
      "Sujoy Sikdar",
      "Rohit Vaish",
      "Jun Wang",
      "Lirong Xia"
    ],
    "author_ids": [],
    "abstract": "Envy-freeness up to one good (EF1) is a well-studied fairness notion for\nindivisible goods that addresses pairwise envy by the removal of at most one\ngood. In the worst case, each pair of agents might require the (hypothetical)\nremoval of a different good, resulting in a weak aggregate guarantee. We study\nallocations that are nearly envy-free in aggregate, and define a novel fairness\nnotion based on information withholding. Under this notion, an agent can\nwithhold (or hide) some of the goods in its bundle and reveal the remaining\ngoods to the other agents. We observe that in practice, envy-freeness can be\nachieved by withholding only a small number of goods overall. We show that\nfinding allocations that withhold an optimal number of goods is computationally\nhard even for highly restricted classes of valuations. In contrast to the\nworst-case results, our experiments on synthetic and real-world preference data\nshow that existing algorithms for finding EF1 allocations withhold\nclose-to-optimal number of goods.",
    "published_date": "2019-07-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02583v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.02499v2",
    "title": "Sim2real transfer learning for 3D human pose estimation: motion to the rescue",
    "authors": [
      "Carl Doersch",
      "Andrew Zisserman"
    ],
    "author_ids": [],
    "abstract": "Synthetic visual data can provide practically infinite diversity and rich\nlabels, while avoiding ethical issues with privacy and bias. However, for many\ntasks, current models trained on synthetic data generalize poorly to real data.\nThe task of 3D human pose estimation is a particularly interesting example of\nthis sim2real problem, because learning-based approaches perform reasonably\nwell given real training data, yet labeled 3D poses are extremely difficult to\nobtain in the wild, limiting scalability. In this paper, we show that standard\nneural-network approaches, which perform poorly when trained on synthetic RGB\nimages, can perform well when the data is pre-processed to extract cues about\nthe person's motion, notably as optical flow and the motion of 2D keypoints.\nTherefore, our results suggest that motion can be a simple way to bridge a\nsim2real gap when video is available. We evaluate on the 3D Poses in the Wild\ndataset, the most challenging modern benchmark for 3D pose estimation, where we\nshow full 3D mesh recovery that is on par with state-of-the-art methods trained\non real 3D sequences, despite training only on synthetic humans from the\nSURREAL dataset.",
    "published_date": "2019-07-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02499v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.02437v1",
    "title": "Subsampling Bias and The Best-Discrepancy Systematic Cross Validation",
    "authors": [
      "Liang Guo",
      "Jianya Liu",
      "Ruodan Lu"
    ],
    "author_ids": [],
    "abstract": "Statistical machine learning models should be evaluated and validated before\nputting to work. Conventional k-fold Monte Carlo Cross-Validation (MCCV)\nprocedure uses a pseudo-random sequence to partition instances into k subsets,\nwhich usually causes subsampling bias, inflates generalization errors and\njeopardizes the reliability and effectiveness of cross-validation. Based on\nordered systematic sampling theory in statistics and low-discrepancy sequence\ntheory in number theory, we propose a new k-fold cross-validation procedure by\nreplacing a pseudo-random sequence with a best-discrepancy sequence, which\nensures low subsampling bias and leads to more precise\nExpected-Prediction-Error estimates. Experiments with 156 benchmark datasets\nand three classifiers (logistic regression, decision tree and naive bayes) show\nthat in general, our cross-validation procedure can extrude subsampling bias in\nthe MCCV by lowering the EPE around 7.18% and the variances around 26.73%. In\ncomparison, the stratified MCCV can reduce the EPE and variances of the MCCV\naround 1.58% and 11.85% respectively. The Leave-One-Out (LOO) can lower the EPE\naround 2.50% but its variances are much higher than the any other CV procedure.\nThe computational time of our cross-validation procedure is just 8.64% of the\nMCCV, 8.67% of the stratified MCCV and 16.72% of the LOO. Experiments also show\nthat our approach is more beneficial for datasets characterized by relatively\nsmall size and large aspect ratio. This makes our approach particularly\npertinent when solving bioscience classification problems. Our proposed\nsystematic subsampling technique could be generalized to other machine learning\nalgorithms that involve random subsampling mechanism.",
    "published_date": "2019-07-04T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO",
      "stat.ME",
      "62-07, 11J71, 62G09, 68T05"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02437v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.02242v2",
    "title": "Fair Kernel Regression via Fair Feature Embedding in Kernel Space",
    "authors": [
      "Austin Okray",
      "Hui Hu",
      "Chao Lan"
    ],
    "author_ids": [],
    "abstract": "In recent years, there have been significant efforts on mitigating unethical\ndemographic biases in machine learning methods. However, very little is done\nfor kernel methods. In this paper, we propose a new fair kernel regression\nmethod via fair feature embedding (FKR-F$^2$E) in kernel space. Motivated by\nprior works on feature selection in kernel space and feature processing for\nfair machine learning, we propose to learn fair feature embedding functions\nthat minimize demographic discrepancy of feature distributions in kernel space.\nCompared to the state-of-the-art fair kernel regression method and several\nbaseline methods, we show FKR-F$^2$E achieves significantly lower prediction\ndisparity across three real-world data sets.",
    "published_date": "2019-07-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02242v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.02230v1",
    "title": "Attention based Convolutional Recurrent Neural Network for Environmental Sound Classification",
    "authors": [
      "Zhichao Zhang",
      "Shugong Xu",
      "Tianhao Qiao",
      "Shunqing Zhang",
      "Shan Cao"
    ],
    "author_ids": [],
    "abstract": "Environmental sound classification (ESC) is a challenging problem due to the\ncomplexity of sounds. The ESC performance is heavily dependent on the\neffectiveness of representative features extracted from the environmental\nsounds. However, ESC often suffers from the semantically irrelevant frames and\nsilent frames. In order to deal with this, we employ a frame-level attention\nmodel to focus on the semantically relevant frames and salient frames.\nSpecifically, we first propose an convolutional recurrent neural network to\nlearn spectro-temporal features and temporal correlations. Then, we extend our\nconvolutional RNN model with a frame-level attention mechanism to learn\ndiscriminative feature representations for ESC. Experiments were conducted on\nESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness\nof the proposed method and achieved the state-of-the-art performance in terms\nof classification accuracy.",
    "published_date": "2019-07-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02230v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.02227v2",
    "title": "Toward Fairness in AI for People with Disabilities: A Research Roadmap",
    "authors": [
      "Anhong Guo",
      "Ece Kamar",
      "Jennifer Wortman Vaughan",
      "Hanna Wallach",
      "Meredith Ringel Morris"
    ],
    "author_ids": [],
    "abstract": "AI technologies have the potential to dramatically impact the lives of people\nwith disabilities (PWD). Indeed, improving the lives of PWD is a motivator for\nmany state-of-the-art AI systems, such as automated speech recognition tools\nthat can caption videos for people who are deaf and hard of hearing, or\nlanguage prediction algorithms that can augment communication for people with\nspeech or cognitive disabilities. However, widely deployed AI systems may not\nwork properly for PWD, or worse, may actively discriminate against them. These\nconsiderations regarding fairness in AI for PWD have thus far received little\nattention. In this position paper, we identify potential areas of concern\nregarding how several AI technology categories may impact particular disability\nconstituencies if care is not taken in their design, development, and testing.\nWe intend for this risk assessment of how various classes of AI might interact\nwith various classes of disability to provide a roadmap for future research\nthat is needed to gather data, test these hypotheses, and build more inclusive\nalgorithms.",
    "published_date": "2019-07-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.02227v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01860v5",
    "title": "Encoding high-cardinality string categorical variables",
    "authors": [
      "Patricio Cerda",
      "Gaël Varoquaux"
    ],
    "author_ids": [],
    "abstract": "Statistical models usually require vector representations of categorical\nvariables, using for instance one-hot encoding. This strategy breaks down when\nthe number of categories grows, as it creates high-dimensional feature vectors.\nAdditionally, for string entries, one-hot encoding does not capture information\nin their representation.Here, we seek low-dimensional encoding of\nhigh-cardinality string categorical variables. Ideally, these should be:\nscalable to many categories; interpretable to end users; and facilitate\nstatistical analysis. We introduce two encoding approaches for string\ncategories: a Gamma-Poisson matrix factorization on substring counts, and the\nmin-hash encoder, for fast approximation of string similarities. We show that\nmin-hash turns set inclusions into inequality relations that are easier to\nlearn. Both approaches are scalable and streamable. Experiments on real and\nsimulated data show that these methods improve supervised learning with\nhigh-cardinality categorical variables. We recommend the following: if\nscalability is central, the min-hash encoder is the best option as it does not\nrequire any data fit; if interpretability is important, the Gamma-Poisson\nfactorization is the best alternative, as it can be interpreted as one-hot\nencoding on inferred categories with informative feature names. Both models\nenable autoML on the original string entries as they remove the need for\nfeature engineering or data cleaning.",
    "published_date": "2019-07-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01860v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01845v5",
    "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
    "authors": [
      "Xiangxiang Chu",
      "Bo Zhang",
      "Ruijun Xu"
    ],
    "author_ids": [],
    "abstract": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online http://github.com/fairnas/FairNAS .",
    "published_date": "2019-07-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01845v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01766v2",
    "title": "Algorithms for Competitive Division of Chores",
    "authors": [
      "Simina Brânzei",
      "Fedor Sandomirskiy"
    ],
    "author_ids": [],
    "abstract": "We study the problem of allocating divisible bads (chores) among multiple\nagents with additive utilities when monetary transfers are not allowed. The\ncompetitive rule is known for its remarkable fairness and efficiency properties\nin the case of goods. This rule was extended to chores in prior work by\nBogomolnaia, Moulin, Sandomirskiy, and Yanovskaya (2017). The rule produces\nPareto optimal and envy-free allocations for both goods and chores. In the case\nof goods, the outcome of the competitive rule can be easily computed.\nCompetitive allocations solve the Eisenberg-Gale convex program; hence the\noutcome is unique and can be approximately found by standard gradient methods.\nAn exact algorithm that runs in polynomial time in the number of agents and\ngoods was given by Orlin (2010).\n  In the case of chores, the competitive rule does not solve any convex\noptimization problem; instead, competitive allocations correspond to local\nminima, local maxima, and saddle points of the Nash social welfare on the\nPareto frontier of the set of feasible utilities. The Pareto frontier may\ncontain many such points; consequently, the competitive rule's outcome is no\nlonger unique.\n  In this paper, we show that all the outcomes of the competitive rule for\nchores can be computed in strongly polynomial time if either the number of\nagents or the number of chores is fixed. The approach is based on a combination\nof three ideas: all consumption graphs of Pareto optimal allocations can be\nlisted in polynomial time; for a given consumption graph, a candidate for a\ncompetitive utility profile can be constructed via an explicit formula; each\ncandidate can be checked for competitiveness, and the allocation can be\nreconstructed using a maximum flow computation.\n  Our algorithm gives an approximately-fair allocation of indivisible chores by\nthe rounding technique of Barman and Krishnamurthy (2018).",
    "published_date": "2019-07-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DS",
      "econ.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01766v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.01728v1",
    "title": "Quickly Finding the Best Linear Model in High Dimensions",
    "authors": [
      "Yahya Sattar",
      "Samet Oymak"
    ],
    "author_ids": [],
    "abstract": "We study the problem of finding the best linear model that can minimize\nleast-squares loss given a data-set. While this problem is trivial in the low\ndimensional regime, it becomes more interesting in high dimensions where the\npopulation minimizer is assumed to lie on a manifold such as sparse vectors. We\npropose projected gradient descent (PGD) algorithm to estimate the population\nminimizer in the finite sample regime. We establish linear convergence rate and\ndata dependent estimation error bounds for PGD. Our contributions include: 1)\nThe results are established for heavier tailed sub-exponential distributions\nbesides sub-gaussian. 2) We directly analyze the empirical risk minimization\nand do not require a realizable model that connects input data and labels. 3)\nOur PGD algorithm is augmented to learn the bias terms which boosts the\nperformance. The numerical experiments validate our theoretical results.",
    "published_date": "2019-07-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01728v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01671v1",
    "title": "Quantifying Algorithmic Biases over Time",
    "authors": [
      "Vivek K. Singh",
      "Ishaan Singh"
    ],
    "author_ids": [],
    "abstract": "Algorithms now permeate multiple aspects of human lives and multiple recent\nresults have reported that these algorithms may have biases pertaining to\ngender, race, and other demographic characteristics. The metrics used to\nquantify such biases have still focused on a static notion of algorithms.\nHowever, algorithms evolve over time. For instance, Tay (a conversational bot\nlaunched by Microsoft) was arguably not biased at its launch but quickly became\nbiased, sexist, and racist over time. We suggest a set of intuitive metrics to\nstudy the variations in biases over time and present the results for a case\nstudy for genders represented in images resulting from a Twitter image search\nfor #Nurse and #Doctor over a period of 21 days. Results indicate that biases\nvary significantly over time and the direction of bias could appear to be\ndifferent on different days. Hence, one-shot measurements may not suffice for\nunderstanding algorithmic bias, thus motivating further work on studying biases\nin algorithms over time.",
    "published_date": "2019-07-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01671v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01439v2",
    "title": "Operationalizing Individual Fairness with Pairwise Fair Representations",
    "authors": [
      "Preethi Lahoti",
      "Krishna P. Gummadi",
      "Gerhard Weikum"
    ],
    "author_ids": [],
    "abstract": "We revisit the notion of individual fairness proposed by Dwork et al. A\ncentral challenge in operationalizing their approach is the difficulty in\neliciting a human specification of a similarity metric. In this paper, we\npropose an operationalization of individual fairness that does not rely on a\nhuman specification of a distance metric. Instead, we propose novel approaches\nto elicit and leverage side-information on equally deserving individuals to\ncounter subordination between social groups. We model this knowledge as a\nfairness graph, and learn a unified Pairwise Fair Representation (PFR) of the\ndata that captures both data-driven similarity between individuals and the\npairwise side-information in fairness graph. We elicit fairness judgments from\na variety of sources, including human judgments for two real-world datasets on\nrecidivism prediction (COMPAS) and violent neighborhood prediction (Crime &\nCommunities). Our experiments show that the PFR model for operationalizing\nindividual fairness is practically viable.",
    "published_date": "2019-07-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01439v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01342v1",
    "title": "The Ethical Dilemma when (not) Setting up Cost-based Decision Rules in Semantic Segmentation",
    "authors": [
      "Robin Chan",
      "Matthias Rottmann",
      "Radin Dardashti",
      "Fabian Hüger",
      "Peter Schlicht",
      "Hanno Gottschalk"
    ],
    "author_ids": [],
    "abstract": "Neural networks for semantic segmentation can be seen as statistical models\nthat provide for each pixel of one image a probability distribution on\npredefined classes. The predicted class is then usually obtained by the maximum\na-posteriori probability (MAP) which is known as Bayes rule in decision theory.\nFrom decision theory we also know that the Bayes rule is optimal regarding the\nsimple symmetric cost function. Therefore, it weights each type of confusion\nbetween two different classes equally, e.g., given images of urban street\nscenes there is no distinction in the cost function if the network confuses a\nperson with a street or a building with a tree. Intuitively, there might be\nconfusions of classes that are more important to avoid than others. In this\nwork, we want to raise awareness of the possibility of explicitly defining\nconfusion costs and the associated ethical difficulties if it comes down to\nproviding numbers. We define two cost functions from different extreme\nperspectives, an egoistic and an altruistic one, and show how safety relevant\nquantities like precision / recall and (segment-wise) false positive / negative\nrate change when interpolating between MAP, egoistic and altruistic decision\nrules.",
    "published_date": "2019-07-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "68T45, 62-07, 62C05"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01342v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01260v2",
    "title": "Predicting the Topical Stance of Media and Popular Twitter Users",
    "authors": [
      "Peter Stefanov",
      "Kareem Darwish",
      "Atanas Atanasov",
      "Preslav Nakov"
    ],
    "author_ids": [],
    "abstract": "Discovering the stances of media outlets and influential people on current,\ndebatable topics is important for social statisticians and policy makers. Many\nsupervised solutions exist for determining viewpoints, but manually annotating\ntraining data is costly. In this paper, we propose a cascaded method that uses\nunsupervised learning to ascertain the stance of Twitter users with respect to\na polarizing topic by leveraging their retweet behavior; then, it uses\nsupervised learning based on user labels to characterize both the general\npolitical leaning of online media and of popular Twitter users, as well as\ntheir stance with respect to the target polarizing topic. We evaluate the model\nby comparing its predictions to gold labels from the Media Bias/Fact Check\nwebsite, achieving 82.6% accuracy.",
    "published_date": "2019-07-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR",
      "91D30"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01260v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01058v1",
    "title": "Associative Embedding for Game-Agnostic Team Discrimination",
    "authors": [
      "Maxime Istasse",
      "Julien Moreau",
      "Christophe De Vleeschouwer"
    ],
    "author_ids": [],
    "abstract": "Assigning team labels to players in a sport game is not a trivial task when\nno prior is known about the visual appearance of each team. Our work builds on\na Convolutional Neural Network (CNN) to learn a descriptor, namely a pixel-wise\nembedding vector, that is similar for pixels depicting players from the same\nteam, and dissimilar when pixels correspond to distinct teams. The advantage of\nthis idea is that no per-game learning is needed, allowing efficient team\ndiscrimination as soon as the game starts. In principle, the approach follows\nthe associative embedding framework introduced in arXiv:1611.05424 to\ndifferentiate instances of objects. Our work is however different in that it\nderives the embeddings from a lightweight segmentation network and, more\nfundamentally, because it considers the assignment of the same embedding to\nunconnected pixels, as required by pixels of distinct players from the same\nteam. Excellent results, both in terms of team labelling accuracy and\ngeneralization to new games/arenas, have been achieved on panoramic views of a\nlarge variety of basketball games involving players interactions and\nocclusions. This makes our method a good candidate to integrate team separation\nin many CNN-based sport analytics pipelines.",
    "published_date": "2019-07-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01058v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01040v1",
    "title": "The Sensitivity of Counterfactual Fairness to Unmeasured Confounding",
    "authors": [
      "Niki Kilbertus",
      "Philip J. Ball",
      "Matt J. Kusner",
      "Adrian Weller",
      "Ricardo Silva"
    ],
    "author_ids": [],
    "abstract": "Causal approaches to fairness have seen substantial recent interest, both\nfrom the machine learning community and from wider parties interested in\nethical prediction algorithms. In no small part, this has been due to the fact\nthat causal models allow one to simultaneously leverage data and expert\nknowledge to remove discriminatory effects from predictions. However, one of\nthe primary assumptions in causal modeling is that you know the causal graph.\nThis introduces a new opportunity for bias, caused by misspecifying the causal\nmodel. One common way for misspecification to occur is via unmeasured\nconfounding: the true causal effect between variables is partially described by\nunobserved quantities. In this work we design tools to assess the sensitivity\nof fairness measures to this confounding for the popular class of non-linear\nadditive noise models (ANMs). Specifically, we give a procedure for computing\nthe maximum difference between two counterfactually fair predictors, where one\nhas become biased due to confounding. For the case of bivariate confounding our\ntechnique can be swiftly computed via a sequence of closed-form updates. For\nmultivariate confounding we give an algorithm that can be efficiently solved\nvia automatic differentiation. We demonstrate our new sensitivity analysis\ntools in real-world fairness scenarios to assess the bias arising from\nconfounding.",
    "published_date": "2019-07-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01040v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00943v1",
    "title": "Estimating brain age based on a healthy population with deep learning and structural MRI",
    "authors": [
      "Xinyang Feng",
      "Zachary C. Lipton",
      "Jie Yang",
      "Scott A. Small",
      "Frank A. Provenzano"
    ],
    "author_ids": [],
    "abstract": "Numerous studies have established that estimated brain age, as derived from\nstatistical models trained on healthy populations, constitutes a valuable\nbiomarker that is predictive of cognitive decline and various neurological\ndiseases. In this work, we curate a large-scale heterogeneous dataset (N =\n10,158, age range 18 - 97) of structural brain MRIs in a healthy population\nfrom multiple publicly-available sources, upon which we train a deep learning\nmodel for brain age estimation. The availability of the large-scale dataset\nenables a more uniform age distribution across adult life-span for effective\nage estimation with no bias toward certain age groups. We demonstrate that the\nage estimation accuracy, evaluated with mean absolute error (MAE) and\ncorrelation coefficient (r), outperforms previously reported methods in both a\nhold-out test set reflective of the custom population (MAE = 4.06 years, r =\n0.970) and an independent life-span evaluation dataset (MAE = 4.21 years, r =\n0.960) on which a previous study has evaluated. We further demonstrate the\nutility of the estimated age in life-span aging analysis of cognitive\nfunctions. Furthermore, we conduct extensive ablation tests and employ\nfeature-attribution techniques to analyze which regions contribute the most\npredictive value, demonstrating the prominence of the frontal lobe as well as\npattern shift across life-span. In summary, we achieve superior age estimation\nperformance confirming the efficacy of deep learning and the added utility of\ntraining with data both in larger number and more uniformly distributed than in\nprevious studies. We demonstrate the regional contribution to our brain age\npredictions through multiple routes and confirm the association of divergence\nbetween estimated and chronological brain age with neuropsychological measures.",
    "published_date": "2019-07-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "eess.IV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00943v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00612v1",
    "title": "One Network for Multi-Domains: Domain Adaptive Hashing with Intersectant Generative Adversarial Network",
    "authors": [
      "Tao He",
      "Yuan-Fang Li",
      "Lianli Gao",
      "Dongxiang Zhang",
      "Jingkuan Song"
    ],
    "author_ids": [],
    "abstract": "With the recent explosive increase of digital data, image recognition and\nretrieval become a critical practical application. Hashing is an effective\nsolution to this problem, due to its low storage requirement and high query\nspeed. However, most of past works focus on hashing in a single (source)\ndomain. Thus, the learned hash function may not adapt well in a new (target)\ndomain that has a large distributional difference with the source domain. In\nthis paper, we explore an end-to-end domain adaptive learning framework that\nsimultaneously and precisely generates discriminative hash codes and classifies\ntarget domain images. Our method encodes two domains images into a semantic\ncommon space, followed by two independent generative adversarial networks\narming at crosswise reconstructing two domains' images, reducing domain\ndisparity and improving alignment in the shared space. We evaluate our\nframework on {four} public benchmark datasets, all of which show that our\nmethod is superior to the other state-of-the-art methods on the tasks of object\nrecognition and image retrieval.",
    "published_date": "2019-07-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00612v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00510v1",
    "title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to Shine a Light on Workplace Sexism and Sexual Harassment",
    "authors": [
      "Amir Karami",
      "Suzanne C. Swan",
      "Cynthia Nicole White",
      "Kayla Ford"
    ],
    "author_ids": [],
    "abstract": "Objective: The goal of this study is to understand how people experience\nsexism and sexual harassment in the workplace by discovering themes in 2,362\nexperiences posted on the Everyday Sexism Project's website everydaysexism.com.\nMethod: This study used both quantitative and qualitative methods. The\nquantitative method was a computational framework to collect and analyze a\nlarge number of workplace sexual harassment experiences. The qualitative method\nwas the analysis of the topics generated by a text mining method. Results:\nTwenty-three topics were coded and then grouped into three overarching themes\nfrom the sex discrimination and sexual harassment literature. The Sex\nDiscrimination theme included experiences in which women were treated\nunfavorably due to their sex, such as being passed over for promotion, denied\nopportunities, paid less than men, and ignored or talked over in meetings. The\nSex Discrimination and Gender harassment theme included stories about sex\ndiscrimination and gender harassment, such as sexist hostility behaviors\nranging from insults and jokes invoking misogynistic stereotypes to bullying\nbehavior. The last theme, Unwanted Sexual Attention, contained stories\ndescribing sexual comments and behaviors used to degrade women. Unwanted\ntouching was the highest weighted topic, indicating how common it was for\nwebsite users to endure being touched, hugged or kissed, groped, and grabbed.\nConclusions: This study illustrates how researchers can use automatic processes\nto go beyond the limits of traditional research methods and investigate\nnaturally occurring large scale datasets on the internet to achieve a better\nunderstanding of everyday workplace sexism experiences.",
    "published_date": "2019-07-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.CL",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00510v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00498v4",
    "title": "Proof of Witness Presence: Blockchain Consensus for Augmented Democracy in Smart Cities",
    "authors": [
      "Evangelos Pournaras"
    ],
    "author_ids": [],
    "abstract": "Smart Cities evolve into complex and pervasive urban environments with a\ncitizens' mandate to meet sustainable development goals. Repositioning\ndemocratic values of citizens' choices in these complex ecosystems has turned\nout to be imperative in an era of social media filter bubbles, fake news and\nopportunities for manipulating electoral results with such means. This paper\nintroduces a new paradigm of augmented democracy that promises actively\nengaging citizens in a more informed decision-making augmented into public\nurban space. The proposed concept is inspired by a digital revive of the\nAncient Agora of Athens, an arena of public discourse, a Polis where citizens\nassemble to actively deliberate and collectively decide about public matters.\nThe core contribution of the proposed paradigm is the concept of proving\nwitness presence: making decision-making subject of providing secure evidence\nand testifying for choices made in the physical space. This paper shows how the\nchallenge of proving witness presence can be tackled with blockchain consensus\nto empower citizens' trust and overcome security vulnerabilities of GPS\nlocalization. Moreover, a novel platform for collective decision-making and\ncrowd-sensing in urban space is introduced: Smart Agora. It is shown how\nreal-time collective measurements over citizens' choices can be made in a fully\ndecentralized and privacy-preserving way. Witness presence is tested by\ndeploying a decentralized system for crowd-sensing the sustainable use of\ntransport means. Furthermore, witness presence of cycling risk is validated\nusing official accident data from public authorities, which are compared\nagainst wisdom of the crowd. The paramount role of dynamic consensus,\nself-governance and ethically aligned artificial intelligence in the augmented\ndemocracy paradigm is outlined.",
    "published_date": "2019-06-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00498v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00435v2",
    "title": "YouTube Chatter: Understanding Online Comments Discourse on Misinformative and Political YouTube Videos",
    "authors": [
      "Aarash Heydari",
      "Janny Zhang",
      "Shaan Appel",
      "Xinyi Wu",
      "Gireeja Ranade"
    ],
    "author_ids": [],
    "abstract": "We conduct a preliminary analysis of comments on political YouTube content\ncontaining misinformation in comparison to comments on trustworthy or\napolitical videos, labelling the bias and factual ratings of our channels\naccording to Media Bias Fact Check where applicable. One of our most\ninteresting discoveries is that especially-polarized or misinformative\npolitical channels (Left-Bias, Right-Bias, PragerU, Conspiracy-Pseudoscience,\nand Questionable Source) generate 7.5x more comments per view and 10.42x more\nreplies per view than apolitical or Pro-Science channels; in particular,\nConspiracy-Pseudoscience and Questionable Sources generate 8.3x more comments\nper view and 11.0x more replies per view than apolitical and Pro-Science\nchannels. We also compared average thread lengths, average comment lengths, and\nprofanity rates across channels, and present simple machine learning\nclassifiers for predicting the bias category of a video based on these\nstatistics.",
    "published_date": "2019-06-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00435v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00430v1",
    "title": "Requisite Variety in Ethical Utility Functions for AI Value Alignment",
    "authors": [
      "Nadisha-Marie Aliman",
      "Leon Kester"
    ],
    "author_ids": [],
    "abstract": "Being a complex subject of major importance in AI Safety research, value\nalignment has been studied from various perspectives in the last years.\nHowever, no final consensus on the design of ethical utility functions\nfacilitating AI value alignment has been achieved yet. Given the urgency to\nidentify systematic solutions, we postulate that it might be useful to start\nwith the simple fact that for the utility function of an AI not to violate\nhuman ethical intuitions, it trivially has to be a model of these intuitions\nand reflect their variety $ - $ whereby the most accurate models pertaining to\nhuman entities being biological organisms equipped with a brain constructing\nconcepts like moral judgements, are scientific models. Thus, in order to better\nassess the variety of human morality, we perform a transdisciplinary analysis\napplying a security mindset to the issue and summarizing variety-relevant\nbackground knowledge from neuroscience and psychology. We complement this\ninformation by linking it to augmented utilitarianism as a suitable ethical\nframework. Based on that, we propose first practical guidelines for the design\nof approximate ethical goal functions that might better capture the variety of\nhuman moral judgements. Finally, we conclude and address future possible\nchallenges.",
    "published_date": "2019-06-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00313v3",
    "title": "Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates",
    "authors": [
      "Houston Claure",
      "Yifang Chen",
      "Jignesh Modi",
      "Malte Jung",
      "Stefanos Nikolaidis"
    ],
    "author_ids": [],
    "abstract": "How should a robot that collaborates with multiple people decide upon the\ndistribution of resources (e.g. social attention, or parts needed for an\nassembly)? People are uniquely attuned to how resources are distributed. A\ndecision to distribute more resources to one team member than another might be\nperceived as unfair with potentially detrimental effects for trust. We\nintroduce a multi-armed bandit algorithm with fairness constraints, where a\nrobot distributes resources to human teammates of different skill levels. In\nthis problem, the robot does not know the skill level of each human teammate,\nbut learns it by observing their performance over time. We define fairness as a\nconstraint on the minimum rate that each human teammate is selected throughout\nthe task. We provide theoretical guarantees on performance and perform a\nlarge-scale user study, where we adjust the level of fairness in our algorithm.\nResults show that fairness in resource distribution has a significant effect on\nusers' trust in the system.",
    "published_date": "2019-06-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00313v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.00020v2",
    "title": "Training individually fair ML models with Sensitive Subspace Robustness",
    "authors": [
      "Mikhail Yurochkin",
      "Amanda Bower",
      "Yuekai Sun"
    ],
    "author_ids": [],
    "abstract": "We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases.",
    "published_date": "2019-06-28T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.00020v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.12307v3",
    "title": "Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study",
    "authors": [
      "Ville Vakkuri",
      "Kai-Kristian Kemell",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "Artificial intelligence (AI) is becoming increasingly widespread in system\ndevelopment endeavors. As AI systems affect various stakeholders due to their\nunique nature, the growing influence of these systems calls for ethical\nconsiderations. Academic discussion and practical examples of autonomous system\nfailures have highlighted the need for implementing ethics in software\ndevelopment. However, research on methods and tools for implementing ethics\ninto AI system design and development in practice is still lacking. This paper\nbegins to address this focal problem by providing elements needed for producing\na baseline for ethics in AI based software development. We do so by means of an\nindustrial multiple case study on AI systems development in the healthcare\nsector. Using a research model based on extant, conceptual AI ethics\nliterature, we explore the current state of practice out on the field in the\nabsence of formal methods and tools for ethically aligned design.",
    "published_date": "2019-06-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.12307v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.12304v4",
    "title": "Statistical Learning from Biased Training Samples",
    "authors": [
      "Stephan Clémençon",
      "Pierre Laforgue"
    ],
    "author_ids": [],
    "abstract": "With the deluge of digitized information in the Big Data era, massive\ndatasets are becoming increasingly available for learning predictive models.\nHowever, in many practical situations, the poor control of the data acquisition\nprocesses may naturally jeopardize the outputs of machine learning algorithms,\nand selection bias issues are now the subject of much attention in the\nliterature. The present article investigates how to extend Empirical Risk\nMinimization, the principal paradigm in statistical learning, when training\nobservations are generated from biased models, i.e., from distributions that\nare different from that in the test/prediction stage, and absolutely continuous\nwith respect to the latter. Precisely, we show how to build a \"nearly debiased\"\ntraining statistical population from biased samples and the related biasing\nfunctions, following in the footsteps of the approach originally proposed in\nVardi (1985). Furthermore, we study from a nonasymptotic perspective the\nperformance of minimizers of an empirical version of the risk computed from the\nstatistical population thus created. Remarkably, the learning rate achieved by\nthis procedure is of the same order as that attained in absence of selection\nbias. Beyond the theoretical guarantees, we also present experimental results\nsupporting the relevance of the algorithmic approach promoted in this paper.",
    "published_date": "2019-06-28T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.12304v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11813v2",
    "title": "Learning Fair Representations for Kernel Models",
    "authors": [
      "Zilong Tan",
      "Samuel Yeom",
      "Matt Fredrikson",
      "Ameet Talwalkar"
    ],
    "author_ids": [],
    "abstract": "Fair representations are a powerful tool for establishing criteria like\nstatistical parity, proxy non-discrimination, and equality of opportunity in\nlearned models. Existing techniques for learning these representations are\ntypically model-agnostic, as they preprocess the original data such that the\noutput satisfies some fairness criterion, and can be used with arbitrary\nlearning methods. In contrast, we demonstrate the promise of learning a\nmodel-aware fair representation, focusing on kernel-based models. We leverage\nthe classical Sufficient Dimension Reduction (SDR) framework to construct\nrepresentations as subspaces of the reproducing kernel Hilbert space (RKHS),\nwhose member functions are guaranteed to satisfy fairness. Our method supports\nseveral fairness criteria, continuous and discrete data, and multiple protected\nattributes. We further show how to calibrate the accuracy tradeoff by\ncharacterizing it in terms of the principal angles between subspaces of the\nRKHS. Finally, we apply our approach to obtain the first Fair Gaussian Process\n(FGP) prior for fair Bayesian learning, and show that it is competitive with,\nand in some cases outperforms, state-of-the-art methods on real data.",
    "published_date": "2019-06-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11813v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11711v1",
    "title": "Reducing Popularity Bias in Recommendation Over Time",
    "authors": [
      "Himan Abdollahpouri",
      "Robin Burke"
    ],
    "author_ids": [],
    "abstract": "Many recommendation algorithms suffer from popularity bias: a small number of\npopular items being recommended too frequently, while other items get\ninsufficient exposure. Research in this area so far has concentrated on a\none-shot representation of this bias, and on algorithms to improve the\ndiversity of individual recommendation lists. In this work, we take a\ntime-sensitive view of popularity bias, in which the algorithm assesses its\nlong-tail coverage at regular intervals, and compensates in the present moment\nfor omissions in the past. In particular, we present a temporal version of the\nwell-known xQuAD diversification algorithm adapted for long-tail\nrecommendation. Experimental results on two public datasets show that our\nmethod is more effective in terms of the long-tail coverage and accuracy\ntradeoff compared to some other existing approaches.",
    "published_date": "2019-06-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11711v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.11428v1",
    "title": "ELKPPNet: An Edge-aware Neural Network with Large Kernel Pyramid Pooling for Learning Discriminative Features in Semantic Segmentation",
    "authors": [
      "Xianwei Zheng",
      "Linxi Huan",
      "Hanjiang Xiong",
      "Jianya Gong"
    ],
    "author_ids": [],
    "abstract": "Semantic segmentation has been a hot topic across diverse research fields.\nAlong with the success of deep convolutional neural networks, semantic\nsegmentation has made great achievements and improvements, in terms of both\nurban scene parsing and indoor semantic segmentation. However, most of the\nstate-of-the-art models are still faced with a challenge in discriminative\nfeature learning, which limits the ability of a model to detect multi-scale\nobjects and to guarantee semantic consistency inside one object or distinguish\ndifferent adjacent objects with similar appearance. In this paper, a practical\nand efficient edge-aware neural network is presented for semantic segmentation.\nThis end-to-end trainable engine consists of a new encoder-decoder network, a\nlarge kernel spatial pyramid pooling (LKPP) block, and an edge-aware loss\nfunction. The encoder-decoder network was designed as a balanced structure to\nnarrow the semantic and resolution gaps in multi-level feature aggregation,\nwhile the LKPP block was constructed with a densely expanding receptive field\nfor multi-scale feature extraction and fusion. Furthermore, the new powerful\nedge-aware loss function is proposed to refine the boundaries directly from the\nsemantic segmentation prediction for more robust and discriminative features.\nThe effectiveness of the proposed model was demonstrated using Cityscapes,\nCamVid, and NYUDv2 benchmark datasets. The performance of the two structures\nand the edge-aware loss function in ELKPPNet was validated on the Cityscapes\ndataset, while the complete ELKPPNet was evaluated on the CamVid and NYUDv2\ndatasets. A comparative analysis with the state-of-the-art methods under the\nsame conditions confirmed the superiority of the proposed algorithm.",
    "published_date": "2019-06-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11428v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11333v1",
    "title": "Fairness criteria through the lens of directed acyclic graphical models",
    "authors": [
      "Benjamin R. Baer",
      "Daniel E. Gilbert",
      "Martin T. Wells"
    ],
    "author_ids": [],
    "abstract": "A substantial portion of the literature on fairness in algorithms proposes,\nanalyzes, and operationalizes simple formulaic criteria for assessing fairness.\nTwo of these criteria, Equalized Odds and Calibration by Group, have gained\nsignificant attention for their simplicity and intuitive appeal, but also for\ntheir incompatibility. This chapter provides a perspective on the meaning and\nconsequences of these and other fairness criteria using graphical models which\nreveals Equalized Odds and related criteria to be ultimately misleading. An\nassessment of various graphical models suggests that fairness criteria should\nultimately be case-specific and sensitive to the nature of the information the\nalgorithm processes.",
    "published_date": "2019-06-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11333v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.03834v1",
    "title": "Differences in Online Course Usage and IP Geolocation Bias by Local Economic Profile",
    "authors": [
      "Daniela Ganelin"
    ],
    "author_ids": [],
    "abstract": "Although Massive Online Open Courses (MOOCs) have the promise to make\nrigorous higher education accessible to everyone, prior research has shown that\nregistrants tend to come from backgrounds of higher socioeconomic status. In\nthis work, I study geographically granular economic patterns in registration\nfor HarvardX and MITx courses, and in the accuracy of identifying users'\nlocations from their IP addresses. Using ZIP Codes identified by the MaxMind IP\ngeolocation database, I find that per-capita registration rates correlate with\neconomic prosperity and population density. Comparing these ZIP Codes with\nuser-provided mailing addresses, I find evidence of bias in MaxMind\ngeolocation: it makes greater errors, both geographically and economically, for\nusers from more economically distressed areas; it disproportionately geolocates\nusers to prosperous areas; and it underestimates the regressive pattern in MOOC\nregistration. Similar economic biases may affect IP geolocation in other\nacademic, commercial, and legal contexts.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.03834v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.10717v1",
    "title": "Uncertainty-aware Model-based Policy Optimization",
    "authors": [
      "Tung-Long Vuong",
      "Kenneth Tran"
    ],
    "author_ids": [],
    "abstract": "Model-based reinforcement learning has the potential to be more sample\nefficient than model-free approaches. However, existing model-based methods are\nvulnerable to model bias, which leads to poor generalization and asymptotic\nperformance compared to model-free counterparts. In addition, they are\ntypically based on the model predictive control (MPC) framework, which not only\nis computationally inefficient at decision time but also does not enable policy\ntransfer due to the lack of an explicit policy representation. In this paper,\nwe propose a novel uncertainty-aware model-based policy optimization framework\nwhich solves those issues. In this framework, the agent simultaneously learns\nan uncertainty-aware dynamics model and optimizes the policy according to these\nlearned models. In the optimization step, the policy gradient is computed by\nautomatic differentiation through the models. With respect to sample efficiency\nalone, our approach shows promising results on challenging continuous control\nbenchmarks with competitive asymptotic performance and significantly lower\nsample complexity than state-of-the-art baselines.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10717v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.10673v3",
    "title": "Learning Fair and Transferable Representations",
    "authors": [
      "Luca Oneto",
      "Michele Donini",
      "Andreas Maurer",
      "Massimiliano Pontil"
    ],
    "author_ids": [],
    "abstract": "Developing learning methods which do not discriminate subgroups in the\npopulation is a central goal of algorithmic fairness. One way to reach this\ngoal is by modifying the data representation in order to meet certain fairness\nconstraints. In this work we measure fairness according to demographic parity.\nThis requires the probability of the possible model decisions to be independent\nof the sensitive information. We argue that the goal of imposing demographic\nparity can be substantially facilitated within a multitask learning setting. We\nleverage task similarities by encouraging a shared fair representation across\nthe tasks via low rank matrix factorization. We derive learning bounds\nestablishing that the learned representation transfers well to novel tasks both\nin terms of prediction performance and fairness metrics. We present experiments\non three real world datasets, showing that the proposed method outperforms\nstate-of-the-art approaches by a significant margin.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10673v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.10490v1",
    "title": "Age and gender bias in pedestrian detection algorithms",
    "authors": [
      "Martim Brandao"
    ],
    "author_ids": [],
    "abstract": "Pedestrian detection algorithms are important components of mobile robots,\nsuch as autonomous vehicles, which directly relate to human safety. Performance\ndisparities in these algorithms could translate into disparate impact in the\nform of biased accident outcomes. To evaluate the need for such concerns, we\ncharacterize the age and gender bias in the performance of state-of-the-art\npedestrian detection algorithms. Our analysis is based on the INRIA Person\nDataset extended with child, adult, male and female labels. We show that all of\nthe 24 top-performing methods of the Caltech Pedestrian Detection Benchmark\nhave higher miss rates on children. The difference is significant and we\nanalyse how it varies with the classifier, features and training data used by\nthe methods. Algorithms were also gender-biased on average but the performance\ndifferences were not significant. We discuss the source of the bias, the\nethical implications, possible technical solutions and barriers.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10490v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.10395v1",
    "title": "Quantitative Verification of Neural Networks And its Security Applications",
    "authors": [
      "Teodora Baluta",
      "Shiqi Shen",
      "Shweta Shinde",
      "Kuldeep S. Meel",
      "Prateek Saxena"
    ],
    "author_ids": [],
    "abstract": "Neural networks are increasingly employed in safety-critical domains. This\nhas prompted interest in verifying or certifying logically encoded properties\nof neural networks. Prior work has largely focused on checking existential\nproperties, wherein the goal is to check whether there exists any input that\nviolates a given property of interest. However, neural network training is a\nstochastic process, and many questions arising in their analysis require\nprobabilistic and quantitative reasoning, i.e., estimating how many inputs\nsatisfy a given property. To this end, our paper proposes a novel and\nprincipled framework to quantitative verification of logical properties\nspecified over neural networks. Our framework is the first to provide PAC-style\nsoundness guarantees, in that its quantitative estimates are within a\ncontrollable and bounded error from the true count. We instantiate our\nalgorithmic framework by building a prototype tool called NPAQ that enables\nchecking rich properties over binarized neural networks. We show how emerging\nsecurity analyses can utilize our framework in 3 concrete point applications:\nquantifying robustness to adversarial inputs, efficacy of trojan attacks, and\nfairness/bias of given neural networks.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10395v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.10379v1",
    "title": "Blocking Mechanism of Porn Website in India: Claim and Truth",
    "authors": [
      "Saurabh Pandey",
      "Harish Sharma"
    ],
    "author_ids": [],
    "abstract": "In last few years, the addiction of internet is apparently recognized as the\nserious threat to the health of society. This internet addiction gives an\nimpetus to pornographic addiction because most of the pornographic content is\naccessible through internet. There have been ethical concerns on blocking the\ncontents over internet. In India Uttarakhand High court has taken initiative\nfor the blocking of pornographic content over internet. Technocrats are coming\nup with various innovative mechanisms to block the content over internet with\nvarious techniques, although long ago in 2015. The Supreme Court of India has\nalready asked to block some of the websites but it could not be materialized.\nThe focus of this research paper is to review the effectiveness of blocking\nexisting web content blocking mechanism of pornographic websites in Indian\ncontext.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10379v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.10378v1",
    "title": "Peril v. Promise: IoT and the Ethical Imaginaries",
    "authors": [
      "Funda Ustek-Spilda",
      "Alison Powell",
      "Irina Shklovski",
      "Sebastian Lehuede"
    ],
    "author_ids": [],
    "abstract": "The future scenarios often associated with Internet of Things (IoT) oscillate\nbetween the peril of IoT for the future of humanity and the promises for an\never-connected and efficient future. Such a dichotomous positioning creates\nproblems not only for expanding the field of application of the technology, but\nalso ensuring ethical and responsible design and production. As part of VirtEU\n(Values and Ethics in Innovation for Responsible Technology in Europe) (EU\nHorizon 2020 FP7), we have conducted ethnographic research into the main hubs\nof IoT in Europe, such as London, Amsterdam, Barcelona and Belgrade, with\ndevelopers and designers of IoT to identify the challenges they face in their\nday-to-day work. In this paper, we focus on the IoT and the ethical imaginaries\nexplore the practical challenges IoT developers face when they are designing,\nproducing and marketing IoT technologies. We argue that top-down ethical\nframeworks that overlook the situated capabilities of developers or the\nsolutionist approaches that treat ethical issues as technical problems are\nunlikely to provide an alternative to the dichotomous imaginary for the future.",
    "published_date": "2019-06-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10378v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.11668v1",
    "title": "Artificial Intelligence: the global landscape of ethics guidelines",
    "authors": [
      "Anna Jobin",
      "Marcello Ienca",
      "Effy Vayena"
    ],
    "author_ids": [],
    "abstract": "In the last five years, private companies, research institutions as well as\npublic sector organisations have issued principles and guidelines for ethical\nAI, yet there is debate about both what constitutes \"ethical AI\" and which\nethical requirements, technical standards and best practices are needed for its\nrealization. To investigate whether a global agreement on these questions is\nemerging, we mapped and analyzed the current corpus of principles and\nguidelines on ethical AI. Our results reveal a global convergence emerging\naround five ethical principles (transparency, justice and fairness,\nnon-maleficence, responsibility and privacy), with substantive divergence in\nrelation to how these principles are interpreted; why they are deemed\nimportant; what issue, domain or actors they pertain to; and how they should be\nimplemented. Our findings highlight the importance of integrating\nguideline-development efforts with substantive ethical analysis and adequate\nimplementation strategies.",
    "published_date": "2019-06-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11668v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.10086v7",
    "title": "Analyzing CART",
    "authors": [
      "Jason M. Klusowski"
    ],
    "author_ids": [],
    "abstract": "Decision trees with binary splits are popularly constructed using\nClassification and Regression Trees (CART) methodology. For binary\nclassification and regression models, this approach recursively divides the\ndata into two near-homogenous daughter nodes according to a split point that\nmaximizes the reduction in sum of squares error (the impurity) along a\nparticular variable. This paper aims to study the bias and adaptive properties\nof regression trees constructed with CART. In doing so, we derive an\ninteresting connection between the bias and the mean decrease in impurity (MDI)\nmeasure of variable importance---a tool widely used for model\ninterpretability---defined as the sum of impurity reductions over all\nnon-terminal nodes in the tree. In particular, we show that the probability\ncontent of a terminal subnode for a variable is small when the MDI for that\nvariable is large and that this relationship is exponential---confirming\ntheoretically that decision trees with CART have small bias and are adaptive to\nsignal strength and direction. Finally, we apply these individual tree bounds\nto tree ensembles and show consistency of Breiman's random forests. The context\nis surprisingly general and applies to a wide variety of multivariable data\ngenerating distributions and regression functions. The main technical tool is\nan exact characterization of the conditional probability content of the\ndaughter nodes arising from an optimal split, in terms of the partial\ndependence function and reduction in impurity.",
    "published_date": "2019-06-24T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.10086v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09948v6",
    "title": "The dual-process approach to human sociality: Meta-analytic evidence for a theory of internalized heuristics for self-preservation",
    "authors": [
      "Valerio Capraro"
    ],
    "author_ids": [],
    "abstract": "Which social decisions are influenced by intuitive processes? Which by\ndeliberative processes? The dual-process approach to human sociality has\nemerged in the last decades as a vibrant and exciting area of research. Yet, a\nperspective that integrates empirical and theoretical work is lacking. This\nreview and meta-analysis synthesizes the existing literature on the cognitive\nbasis of cooperation, altruism, truth-telling, positive and negative\nreciprocity, and deontology, and develops a framework that organizes the\nexperimental regularities. The meta-analytic results suggest that intuition\nfavours a set of heuristics that are related to the instinct for\nself-preservation: people avoid being harmed, avoid harming others (especially\nwhen there is a risk of harm to themselves), and are averse to disadvantageous\ninequalities. Finally, this paper highlights some key research questions to\nfurther advance our understanding of the cognitive foundations of human\nsociality.",
    "published_date": "2019-06-24T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09948v6",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.09740v2",
    "title": "Gaze-Contingent Ocular Parallax Rendering for Virtual Reality",
    "authors": [
      "Robert Konrad",
      "Anastasios Angelopoulos",
      "Gordon Wetzstein"
    ],
    "author_ids": [],
    "abstract": "Immersive computer graphics systems strive to generate perceptually realistic\nuser experiences. Current-generation virtual reality (VR) displays are\nsuccessful in accurately rendering many perceptually important effects,\nincluding perspective, disparity, motion parallax, and other depth cues. In\nthis article, we introduce ocular parallax rendering, a technology that\naccurately renders small amounts of gaze-contingent parallax capable of\nimproving depth perception and realism in VR. Ocular parallax describes the\nsmall amounts of depth-dependent image shifts on the retina that are created as\nthe eye rotates. The effect occurs because the centers of rotation and\nprojection of the eye are not the same. We study the perceptual implications of\nocular parallax rendering by designing and conducting a series of user\nexperiments. Specifically, we estimate perceptual detection and discrimination\nthresholds for this effect and demonstrate that it is clearly visible in most\nVR applications. Additionally, we show that ocular parallax rendering provides\nan effective ordinal depth cue and it improves the impression of realistic\ndepth in VR.",
    "published_date": "2019-06-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GR",
      "cs.HC",
      "J.4; I.3.7; H.5.1"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09740v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.09713v2",
    "title": "Penalty Bidding Mechanisms for Allocating Resources and Overcoming Present Bias",
    "authors": [
      "Hongyao Ma",
      "Reshef Meir",
      "David C. Parkes",
      "Elena Wu-Yan"
    ],
    "author_ids": [],
    "abstract": "From skipped exercise classes to last-minute cancellation of dentist\nappointments, underutilization of reserved resources abounds. Likely reasons\ninclude uncertainty about the future, further exacerbated by present bias. In\nthis paper, we unite resource allocation and commitment devices through the\ndesign of contingent payment mechanisms, and propose the two-bid\npenalty-bidding mechanism. This extends an earlier mechanism proposed by Ma et\nal. (2019), assigning the resources based on willingness to accept a no-show\npenalty, while also allowing each participant to increase her own penalty in\norder to counter present bias. We establish a simple dominant strategy\nequilibrium, regardless of an agent's level of present bias or degree of\n\"sophistication\". Via simulations, we show that the proposed mechanism\nsubstantially improves utilization and achieves higher welfare and better\nequity in comparison with mechanisms used in practice and mechanisms that\noptimize welfare in the absence of present bias.",
    "published_date": "2019-06-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09713v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.09688v3",
    "title": "Transfer of Machine Learning Fairness across Domains",
    "authors": [
      "Candice Schumann",
      "Xuezhi Wang",
      "Alex Beutel",
      "Jilin Chen",
      "Hai Qian",
      "Ed H. Chi"
    ],
    "author_ids": [],
    "abstract": "If our models are used in new or unexpected cases, do we know if they will\nmake fair predictions? Previously, researchers developed ways to debias a model\nfor a single problem domain. However, this is often not how models are trained\nand used in practice. For example, labels and demographics (sensitive\nattributes) are often hard to observe, resulting in auxiliary or synthetic data\nto be used for training, and proxies of the sensitive attribute to be used for\nevaluation of fairness. A model trained for one setting may be picked up and\nused in many others, particularly as is common with pre-training and cloud\nAPIs. Despite the pervasiveness of these complexities, remarkably little work\nin the fairness literature has theoretically examined these issues. We frame\nall of these settings as domain adaptation problems: how can we use what we\nhave learned in a source domain to debias in a new target domain, without\ndirectly debiasing on the target domain as if it is a completely new problem?\nWe offer new theoretical guarantees of improving fairness across domains, and\noffer a modeling approach to transfer to data-sparse target domains. We give\nempirical results validating the theory and showing that these modeling\napproaches can improve fairness metrics with less data.",
    "published_date": "2019-06-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09688v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09635v1",
    "title": "Investigating Biases in Textual Entailment Datasets",
    "authors": [
      "Shawn Tan",
      "Yikang Shen",
      "Chin-wei Huang",
      "Aaron Courville"
    ],
    "author_ids": [],
    "abstract": "The ability to understand logical relationships between sentences is an\nimportant task in language understanding. To aid in progress for this task,\nresearchers have collected datasets for machine learning and evaluation of\ncurrent systems. However, like in the crowdsourced Visual Question Answering\n(VQA) task, some biases in the data inevitably occur. In our experiments, we\nfind that performing classification on just the hypotheses on the SNLI dataset\nyields an accuracy of 64%. We analyze the bias extent in the SNLI and the\nMultiNLI dataset, discuss its implication, and propose a simple method to\nreduce the biases in the datasets.",
    "published_date": "2019-06-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09635v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09624v1",
    "title": "On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",
    "authors": [
      "Rohin Shah",
      "Noah Gundotra",
      "Pieter Abbeel",
      "Anca D. Dragan"
    ],
    "author_ids": [],
    "abstract": "Our goal is for agents to optimize the right reward function, despite how\ndifficult it is for us to specify what that is. Inverse Reinforcement Learning\n(IRL) enables us to infer reward functions from demonstrations, but it usually\nassumes that the expert is noisily optimal. Real people, on the other hand,\noften have systematic biases: risk-aversion, myopia, etc. One option is to try\nto characterize these biases and account for them explicitly during learning.\nBut in the era of deep learning, a natural suggestion researchers make is to\navoid mathematical models of human behavior that are fraught with specific\nassumptions, and instead use a purely data-driven approach. We decided to put\nthis to the test -- rather than relying on assumptions about which specific\nbias the demonstrator has when planning, we instead learn the demonstrator's\nplanning algorithm that they use to generate demonstrations, as a\ndifferentiable planner. Our exploration yielded mixed findings: on the one\nhand, learning the planner can lead to better reward inference than relying on\nthe wrong assumption; on the other hand, this benefit is dwarfed by the loss we\nincur by going from an exact to a differentiable planner. This suggest that at\nleast for the foreseeable future, agents need a middle ground between the\nflexibility of data-driven methods and the useful bias of known human biases.\nCode is available at https://tinyurl.com/learningbiases.",
    "published_date": "2019-06-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09624v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09613v4",
    "title": "The Cost of a Reductions Approach to Private Fair Optimization",
    "authors": [
      "Daniel Alabi"
    ],
    "author_ids": [],
    "abstract": "Through the lens of information-theoretic reductions, we examine a reductions\napproach to fair optimization and learning where a black-box optimizer is used\nto learn a fair model for classification or regression. Quantifying the\ncomplexity, both statistically and computationally, of making such models\nsatisfy the rigorous definition of differential privacy is our end goal. We\nresolve a few open questions and show applicability to fair machine learning,\nhypothesis testing, and to optimizing non-standard measures of classification\nloss. Furthermore, our sample complexity bounds are tight amongst all\nstrategies that jointly minimize a composition of functions.\n  The reductions approach to fair optimization can be abstracted as the\nconstrained group-objective optimization problem where we aim to optimize an\nobjective that is a function of losses of individual groups, subject to some\nconstraints. We give the first polynomial-time algorithms to solve the problem\nwith $(\\epsilon, 0)$ or $(\\epsilon, \\delta)$ differential privacy guarantees\nwhen defined on a convex decision set (for example, the $\\ell_P$ unit ball)\nwith convex constraints and losses. Accompanying information-theoretic lower\nbounds for the problem are presented. In addition, compared to a previous\nmethod for ensuring differential privacy subject to a relaxed form of the\nequalized odds fairness constraint, the $(\\epsilon, \\delta)$-differentially\nprivate algorithm we present provides asymptotically better sample complexity\nguarantees, resulting in an exponential improvement in certain parameter\nregimes. We introduce a class of bounded divergence linear optimizers, which\ncould be of independent interest, and specialize to pure and approximate\ndifferential privacy.",
    "published_date": "2019-06-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09613v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09575v2",
    "title": "Accelerating Primal Solution Findings for Mixed Integer Programs Based on Solution Prediction",
    "authors": [
      "Jian-Ya Ding",
      "Chao Zhang",
      "Lei Shen",
      "Shengyin Li",
      "Bing Wang",
      "Yinghui Xu",
      "Le Song"
    ],
    "author_ids": [],
    "abstract": "Mixed Integer Programming (MIP) is one of the most widely used modeling\ntechniques for combinatorial optimization problems. In many applications, a\nsimilar MIP model is solved on a regular basis, maintaining remarkable\nsimilarities in model structures and solution appearances but differing in\nformulation coefficients. This offers the opportunity for machine learning\nmethods to explore the correlations between model structures and the resulting\nsolution values. To address this issue, we propose to represent an MIP instance\nusing a tripartite graph, based on which a Graph Convolutional Network (GCN) is\nconstructed to predict solution values for binary variables. The predicted\nsolutions are used to generate a local branching type cut which can be either\ntreated as a global (invalid) inequality in the formulation resulting in a\nheuristic approach to solve the MIP, or as a root branching rule resulting in\nan exact approach. Computational evaluations on 8 distinct types of MIP\nproblems show that the proposed framework improves the primal solution finding\nperformance significantly on a state-of-the-art open-source MIP solver.",
    "published_date": "2019-06-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09575v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09531v2",
    "title": "Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting",
    "authors": [
      "Aditya Grover",
      "Jiaming Song",
      "Alekh Agarwal",
      "Kenneth Tran",
      "Ashish Kapoor",
      "Eric Horvitz",
      "Stefano Ermon"
    ],
    "author_ids": [],
    "abstract": "A learned generative model often produces biased statistics relative to the\nunderlying data distribution. A standard technique to correct this bias is\nimportance sampling, where samples from the model are weighted by the\nlikelihood ratio under model and true distributions. When the likelihood ratio\nis unknown, it can be estimated by training a probabilistic classifier to\ndistinguish samples from the two distributions. We employ this likelihood-free\nimportance weighting method to correct for the bias in generative models. We\nfind that this technique consistently improves standard goodness-of-fit metrics\nfor evaluating the sample quality of state-of-the-art deep generative models,\nsuggesting reduced bias. Finally, we demonstrate its utility on representative\napplications in a) data augmentation for classification using generative\nadversarial networks, and b) model-based policy evaluation using off-policy\ndata.",
    "published_date": "2019-06-23T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09531v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09437v2",
    "title": "A Unifying Framework for Variance Reduction Algorithms for Finding Zeroes of Monotone Operators",
    "authors": [
      "Xun Zhang",
      "William B. Haskell",
      "Zhisheng Ye"
    ],
    "author_ids": [],
    "abstract": "It is common to encounter large-scale monotone inclusion problems where the\nobjective has a finite sum structure. We develop a general framework for\nvariance-reduced forward-backward splitting algorithms for this problem. This\nframework includes a number of existing deterministic and variance-reduced\nalgorithms for function minimization as special cases, and it is also\napplicable to more general problems such as saddle-point problems and\nvariational inequalities. With a carefully constructed Lyapunov function, we\nshow that the algorithms covered by our framework enjoy a linear convergence\nrate in expectation under mild assumptions. We further consider Catalyst\nacceleration and asynchronous implementation to reduce the algorithmic\ncomplexity and computation time. We apply our proposed framework to a policy\nevaluation problem and a strongly monotone two-player game, both of which fall\noutside of function minimization.",
    "published_date": "2019-06-22T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09437v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09355v3",
    "title": "An Incentive Security Model to Provide Fairness for Peer-to-Peer Networks",
    "authors": [
      "Samaneh Berenjian",
      "Saeed Hajizadeh",
      "Reza Ebrahimi Atani"
    ],
    "author_ids": [],
    "abstract": "Peer-to-Peer networks are designed to rely on resources of their own users.\nTherefore, resource management plays an important role in P2P protocols.\nTherefore, resource management plays an important role in P2P protocols. Early\nP2P networks did not use proper mechanisms to manage fairness. However, after\nseeing difficulties and rise of freeloaders in networks like Gnutella, the\nimportance of providing fairness for users have become apparent. In this paper,\nwe propose an incentive based security model which leads to a network\ninfrastructure that lightens the work of Seeders and makes Leechers to\ncontribute more. This method is able to prevent betrayals in Leecher-to-Leecher\ntransactions and more importantly, helps Seeders to be treated more fairly.\nThis is what other incentive methods such as Bittorrent are incapable of doing.\nAdditionally, by getting help from cryptography and combining it with our\nmethod, it is also possible to achieve secure channels, immune to spying, next\nto a fair network. The simulation results clearly show that how our proposed\napproach can overcome free-riding issue. In addition, our findings revealed\nthat our approach is able to provide an appropriate level of fairness for the\nusers and can decrease the download time.",
    "published_date": "2019-06-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09355v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1907.03827v1",
    "title": "FairST: Equitable Spatial and Temporal Demand Prediction for New Mobility Systems",
    "authors": [
      "An Yan",
      "Bill Howe"
    ],
    "author_ids": [],
    "abstract": "Emerging transportation modes, including car-sharing, bike-sharing, and\nride-hailing, are transforming urban mobility but have been shown to reinforce\nsocioeconomic inequities. Spatiotemporal demand prediction models for these new\nmobility regimes must therefore consider fairness as a first-class design\nrequirement. We present FairST, a fairness-aware model for predicting demand\nfor new mobility systems. Our approach utilizes 1D, 2D and 3D convolutions to\nintegrate various urban features and learn the spatial-temporal dynamics of a\nmobility system, but we include fairness metrics as a form of regularization to\nmake the predictions more equitable across demographic groups. We propose two\nnovel spatiotemporal fairness metrics, a region-based fairness gap (RFG) and an\nindividual-based fairness gap (IFG). Both quantify equity in a spatiotemporal\ncontext, but vary by whether demographics are labeled at the region level (RFG)\nor whether population distribution information is available (IFG). Experimental\nresults on real bike share and ride share datasets demonstrate the\neffectiveness of the proposed model: FairST not only reduces the fairness gap\nby more than 80%, but can surprisingly achieve better accuracy than\nstate-of-the-art yet fairness-oblivious methods including LSTMs, ConvLSTMs, and\n3D CNN.",
    "published_date": "2019-06-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.03827v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09218v4",
    "title": "FlipTest: Fairness Testing via Optimal Transport",
    "authors": [
      "Emily Black",
      "Samuel Yeom",
      "Matt Fredrikson"
    ],
    "author_ids": [],
    "abstract": "We present FlipTest, a black-box technique for uncovering discrimination in\nclassifiers. FlipTest is motivated by the intuitive question: had an individual\nbeen of a different protected status, would the model have treated them\ndifferently? Rather than relying on causal information to answer this question,\nFlipTest leverages optimal transport to match individuals in different\nprotected groups, creating similar pairs of in-distribution samples. We show\nhow to use these instances to detect discrimination by constructing a\n\"flipset\": the set of individuals whose classifier output changes\npost-translation, which corresponds to the set of people who may be harmed\nbecause of their group membership. To shed light on why the model treats a\ngiven subgroup differently, FlipTest produces a \"transparency report\": a\nranking of features that are most associated with the model's behavior on the\nflipset. Evaluating the approach on three case studies, we show that this\nprovides a computationally inexpensive way to identify subgroups that may be\nharmed by model discrimination, including in cases where the model satisfies\ngroup fairness criteria.",
    "published_date": "2019-06-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09218v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09208v3",
    "title": "Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices",
    "authors": [
      "Manish Raghavan",
      "Solon Barocas",
      "Jon Kleinberg",
      "Karen Levy"
    ],
    "author_ids": [],
    "abstract": "There has been rapidly growing interest in the use of algorithms in hiring,\nespecially as a means to address or mitigate bias. Yet, to date, little is\nknown about how these methods are used in practice. How are algorithmic\nassessments built, validated, and examined for bias? In this work, we document\nand analyze the claims and practices of companies offering algorithms for\nemployment assessment. In particular, we identify vendors of algorithmic\npre-employment assessments (i.e., algorithms to screen candidates), document\nwhat they have disclosed about their development and validation procedures, and\nevaluate their practices, focusing particularly on efforts to detect and\nmitigate bias. Our analysis considers both technical and legal perspectives.\nTechnically, we consider the various choices vendors make regarding data\ncollection and prediction targets, and explore the risks and trade-offs that\nthese choices pose. We also discuss how algorithmic de-biasing techniques\ninterface with, and create challenges for, antidiscrimination law.",
    "published_date": "2019-06-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09208v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.08976v1",
    "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    "authors": [
      "Tony Sun",
      "Andrew Gaut",
      "Shirlyn Tang",
      "Yuxin Huang",
      "Mai ElSherief",
      "Jieyu Zhao",
      "Diba Mirza",
      "Elizabeth Belding",
      "Kai-Wei Chang",
      "William Yang Wang"
    ],
    "author_ids": [],
    "abstract": "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in\npopularity, it becomes increasingly vital to recognize the role they play in\nshaping societal biases and stereotypes. Although NLP models have shown success\nin modeling various applications, they propagate and may even amplify gender\nbias found in text corpora. While the study of bias in artificial intelligence\nis not new, methods to mitigate gender bias in NLP are relatively nascent. In\nthis paper, we review contemporary studies on recognizing and mitigating gender\nbias in NLP. We discuss gender bias based on four forms of representation bias\nand analyze methods recognizing gender bias. Furthermore, we discuss the\nadvantages and drawbacks of existing gender debiasing methods. Finally, we\ndiscuss future studies for recognizing and mitigating gender bias in NLP.",
    "published_date": "2019-06-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08976v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11049v1",
    "title": "Unsupervised Phoneme and Word Discovery from Multiple Speakers using Double Articulation Analyzer and Neural Network with Parametric Bias",
    "authors": [
      "Ryo Nakashima",
      "Ryo Ozaki",
      "Tadahiro Taniguchi"
    ],
    "author_ids": [],
    "abstract": "This paper describes a new unsupervised machine learning method for\nsimultaneous phoneme and word discovery from multiple speakers. Human infants\ncan acquire knowledge of phonemes and words from interactions with his/her\nmother as well as with others surrounding him/her. From a computational\nperspective, phoneme and word discovery from multiple speakers is a more\nchallenging problem than that from one speaker because the speech signals from\ndifferent speakers exhibit different acoustic features. This paper proposes an\nunsupervised phoneme and word discovery method that simultaneously uses\nnonparametric Bayesian double articulation analyzer (NPB-DAA) and deep sparse\nautoencoder with parametric bias in hidden layer (DSAE-PBHL). We assume that an\ninfant can recognize and distinguish speakers based on certain other features,\ne.g., visual face recognition. DSAE-PBHL is aimed to be able to subtract\nspeaker-dependent acoustic features and extract speaker-independent features.\nAn experiment demonstrated that DSAE-PBHL can subtract distributed\nrepresentations of acoustic signals, enabling extraction based on the types of\nphonemes rather than on the speakers. Another experiment demonstrated that a\ncombination of NPB-DAA and DSAE-PB outperformed the available methods in\nphoneme and word discovery tasks involving speech signals with Japanese vowel\nsequences from multiple speakers.",
    "published_date": "2019-06-21T00:00:00",
    "year": 2019,
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11049v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.08916v1",
    "title": "Understanding and Classifying Cultural Music Using Melodic Features Case Of Hindustani, Carnatic And Turkish Music",
    "authors": [
      "Amruta Vidwans",
      "Prateek Verma",
      "Preeti Rao"
    ],
    "author_ids": [],
    "abstract": "We present a melody based classification of musical styles by exploiting the\npitch and energy based characteristics derived from the audio signal. Three\nprominent musical styles were chosen which have improvisation as integral part\nwith similar melodic principles, theme, and structure of concerts namely,\nHindustani, Carnatic and Turkish music. Listeners of one or more of these\ngenres can discriminate between these based on the melodic contour alone.\nListening tests were carried out using melodic attributes alone, on similar\nmelodic pieces with respect to raga/makam, and removing any instrumentation cue\nto validate our hypothesis that style distinction is evident in the melody. Our\nmethod is based on finding a set of highly discriminatory features, derived\nfrom musicology, to capture distinct characteristics of the melodic contour.\nBehavior in terms of transitions of the pitch contour, the presence of\nmicro-tonal notes and the nature of variations in the vocal energy are\nexploited. The automatically classified style labels are found to correlate\nwell with subjective listening judgments. This was verified by using\nstatistical tests to compare the labels from subjective and objective\njudgments. The melody based features, when combined with timbre based features,\nwere seen to improve the classification performance.",
    "published_date": "2019-06-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08916v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.08732v2",
    "title": "Multi-Category Fairness in Sponsored Search Auctions",
    "authors": [
      "Shuchi Chawla",
      "Christina Ilvento",
      "Meena Jagadeesan"
    ],
    "author_ids": [],
    "abstract": "Fairness in advertising is a topic of particular concern motivated by\ntheoretical and empirical observations in both the computer science and\neconomics literature. We examine the problem of fairness in advertising for\ngeneral purpose platforms that service advertisers from many different\ncategories. First, we propose inter-category and intra-category fairness\ndesiderata that take inspiration from individual fairness and envy-freeness.\nSecond, we investigate the \"platform utility\" (a proxy for the quality of the\nallocation) achievable by mechanisms satisfying these desiderata. More\nspecifically, we compare the utility of fair mechanisms against the unfair\noptimal, and we show by construction that our fairness desiderata are\ncompatible with utility. That is, we construct a family of fair mechanisms with\nhigh utility that perform close to optimally within a class of fair mechanisms.\nOur mechanisms also enjoy nice implementation properties including\nmetric-obliviousness, which allows the platform to produce fair allocations\nwithout needing to know the specifics of the fairness requirements.",
    "published_date": "2019-06-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08732v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.08611v2",
    "title": "More Efficient Policy Learning via Optimal Retargeting",
    "authors": [
      "Nathan Kallus"
    ],
    "author_ids": [],
    "abstract": "Policy learning can be used to extract individualized treatment regimes from\nobservational data in healthcare, civics, e-commerce, and beyond. One big\nhurdle to policy learning is a commonplace lack of overlap in the data for\ndifferent actions, which can lead to unwieldy policy evaluation and poorly\nperforming learned policies. We study a solution to this problem based on\nretargeting, that is, changing the population on which policies are optimized.\nWe first argue that at the population level, retargeting may induce little to\nno bias. We then characterize the optimal reference policy and retargeting\nweights in both binary-action and multi-action settings. We do this in terms of\nthe asymptotic efficient estimation variance of the new learning objective.\nExtensive empirical results in a simulation study and a case study of\npersonalized job counseling demonstrate that retargeting is a fairly easy way\nto significantly improve any policy learning procedure applied to observational\ndata.",
    "published_date": "2019-06-20T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08611v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.08484v4",
    "title": "Coresets for Clustering with Fairness Constraints",
    "authors": [
      "Lingxiao Huang",
      "Shaofeng H. -C. Jiang",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "In a recent work, [19] studied the following \"fair\" variants of classical\nclustering problems such as $k$-means and $k$-median: given a set of $n$ data\npoints in $\\mathbb{R}^d$ and a binary type associated to each data point, the\ngoal is to cluster the points while ensuring that the proportion of each type\nin each cluster is roughly the same as its underlying proportion. Subsequent\nwork has focused on either extending this setting to when each data point has\nmultiple, non-disjoint sensitive types such as race and gender [6], or to\naddress the problem that the clustering algorithms in the above work do not\nscale well. The main contribution of this paper is an approach to clustering\nwith fairness constraints that involve multiple, non-disjoint types, that is\nalso scalable. Our approach is based on novel constructions of coresets: for\nthe $k$-median objective, we construct an $\\varepsilon$-coreset of size\n$O(\\Gamma k^2 \\varepsilon^{-d})$ where $\\Gamma$ is the number of distinct\ncollections of groups that a point may belong to, and for the $k$-means\nobjective, we show how to construct an $\\varepsilon$-coreset of size $O(\\Gamma\nk^3\\varepsilon^{-d-1})$. The former result is the first known coreset\nconstruction for the fair clustering problem with the $k$-median objective, and\nthe latter result removes the dependence on the size of the full dataset as in\n[39] and generalizes it to multiple, non-disjoint types. Plugging our coresets\ninto existing algorithms for fair clustering such as [5] results in the fastest\nalgorithms for several cases. Empirically, we assess our approach over the\n\\textbf{Adult}, \\textbf{Bank}, \\textbf{Diabetes} and \\textbf{Athlete} dataset,\nand show that the coreset sizes are much smaller than the full dataset. We also\nachieve a speed-up to recent fair clustering algorithms [5,6] by incorporating\nour coreset construction.",
    "published_date": "2019-06-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.CG",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08484v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.08430v1",
    "title": "Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects",
    "authors": [
      "Gabriel Grand",
      "Yonatan Belinkov"
    ],
    "author_ids": [],
    "abstract": "Visual question answering (VQA) models have been shown to over-rely on\nlinguistic biases in VQA datasets, answering questions \"blindly\" without\nconsidering visual context. Adversarial regularization (AdvReg) aims to address\nthis issue via an adversary sub-network that encourages the main model to learn\na bias-free representation of the question. In this work, we investigate the\nstrengths and shortcomings of AdvReg with the goal of better understanding how\nit affects inference in VQA models. Despite achieving a new state-of-the-art on\nVQA-CP, we find that AdvReg yields several undesirable side-effects, including\nunstable gradients and sharply reduced performance on in-domain examples. We\ndemonstrate that gradual introduction of regularization during training helps\nto alleviate, but not completely solve, these issues. Through error analyses,\nwe observe that AdvReg improves generalization to binary questions, but impairs\nperformance on questions with heterogeneous answer distributions.\nQualitatively, we also find that regularized models tend to over-rely on visual\nfeatures, while ignoring important linguistic cues in the question. Our results\nsuggest that AdvReg requires further refinement before it can be considered a\nviable bias mitigation technique for VQA.",
    "published_date": "2019-06-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08430v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.08379v1",
    "title": "Considerations for the Interpretation of Bias Measures of Word Embeddings",
    "authors": [
      "Inom Mirzaev",
      "Anthony Schulte",
      "Michael Conover",
      "Sam Shah"
    ],
    "author_ids": [],
    "abstract": "Word embedding spaces are powerful tools for capturing latent semantic\nrelationships between terms in corpora, and have become widely popular for\nbuilding state-of-the-art natural language processing algorithms. However,\nstudies have shown that societal biases present in text corpora may be\nincorporated into the word embedding spaces learned from them. Thus, there is\nan ethical concern that human-like biases contained in the corpora and their\nderived embedding spaces might be propagated, or even amplified with the usage\nof the biased embedding spaces in downstream applications. In an attempt to\nquantify these biases so that they may be better understood and studied,\nseveral bias metrics have been proposed. We explore the statistical properties\nof these proposed measures in the context of their cited applications as well\nas their supposed utilities. We find that there are caveats to the simple\ninterpretation of these metrics as proposed. We find that the bias metric\nproposed by Bolukbasi et al. 2016 is highly sensitive to embedding\nhyper-parameter selection, and that in many cases, the variance due to the\nselection of some hyper-parameters is greater than the variance in the metric\ndue to corpus selection, while in fewer cases the bias rankings of corpora vary\nwith hyper-parameter selection. In light of these observations, it may be the\ncase that bias estimates should not be thought to directly measure the\nproperties of the underlying corpus, but rather the properties of the specific\nembedding spaces in question, particularly in the context of hyper-parameter\nselections used to generate them. Hence, bias metrics of spaces generated with\ndiffering hyper-parameters should be compared only with explicit consideration\nof the embedding-learning algorithms particular configurations.",
    "published_date": "2019-06-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.08379v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07993v1",
    "title": "Ethical Interviews in Software Engineering",
    "authors": [
      "Per Erik Strandberg"
    ],
    "author_ids": [],
    "abstract": "Background: Despite a long history, numerous laws and regulations, ethics\nremains an unnatural topic for many software engineering researchers. Poor\nresearch ethics may lead to mistrust of research results, lost funding and\nretraction of publications. A core principle for research ethics is\nconfidentiality, and anonymization is a standard approach to guarantee it. Many\nguidelines for qualitative software engineering research, and for qualitative\nresearch in general, exist, but these do not penetrate how and why to anonymize\ninterview data. Aims: In this paper we aim to identify ethical guidelines for\nsoftware engineering interview studies involving industrial practitioners.\nMethod: By learning from previous experiences and listening to the authority of\nexisting guidelines in the more mature field of medicine as well as in software\nengineering, a comprehensive set of checklists for interview studies was\ndistilled. Results: The elements of an interview study were identified and\nethical considerations and recommendations for each step were produced, in\nparticular with respect to anonymization. Important ethical principles are:\nconsent, beneficence, confidentiality, scientific value, researcher skill,\njustice, respect for law, and ethical reviews. Conclusions: The most important\ncontribution of this study is the set of checklists for ethical interview\nstudies. Future work is needed to refine these guidelines with respect to legal\naspects and ethical boards.",
    "published_date": "2019-06-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07993v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.07946v1",
    "title": "Ethically Aligned Design of Autonomous Systems: Industry viewpoint and an empirical study",
    "authors": [
      "Ville Vakkuri",
      "Kai-Kristian Kemell",
      "Joni Kultanen",
      "Mikko Siponen",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "Progress in the field of artificial intelligence has been accelerating\nrapidly in the past two decades. Various autonomous systems from purely digital\nones to autonomous vehicles are being developed and deployed out on the field.\nAs these systems exert a growing impact on society, ethics in relation to\nartificial intelligence and autonomous systems have recently seen growing\nattention among the academia. However, the current literature on the topic has\nfocused almost exclusively on theory and more specifically on conceptualization\nin the area. To widen the body of knowledge in the area, we conduct an\nempirical study on the current state of practice in artificial intelligence\nethics. We do so by means of a multiple case study of five case companies, the\nresults of which indicate a gap between research and practice in the area.\nBased on our findings we propose ways to tackle the gap.",
    "published_date": "2019-06-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07946v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07858v3",
    "title": "Adversarial training approach for local data debiasing",
    "authors": [
      "Ulrich Aïvodji",
      "François Bidet",
      "Sébastien Gambs",
      "Rosin Claude Ngueveu",
      "Alain Tapp"
    ],
    "author_ids": [],
    "abstract": "The widespread use of automated decision processes in many areas of our\nsociety raises serious ethical issues concerning the fairness of the process\nand the possible resulting discriminations. In this work, we propose a novel\napproach called GANsan whose objective is to prevent the possibility of any\ndiscrimination i.e., direct and indirect) based on a sensitive attribute by\nremoving the attribute itself as well as the existing correlations with the\nremaining attributes. Our sanitization algorithm GANsan is partially inspired\nby the powerful framework of generative adversarial networks (in particular the\nCycle-GANs), which offers a flexible way to learn a distribution empirically or\nto translate between two different distributions.\n  In contrast to prior work, one of the strengths of our approach is that the\nsanitization is performed in the same space as the original data by only\nmodifying the other attributes as little as possible and thus preserving the\ninterpretability of the sanitized data. As a consequence, once the sanitizer is\ntrained, it can be applied to new data, such as for instance, locally by an\nindividual on his profile before releasing it. Finally, experiments on a real\ndataset demonstrate the effectiveness of the proposed approach as well as the\nachievable trade-off between fairness and utility.",
    "published_date": "2019-06-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07858v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07794v1",
    "title": "Convolutional neural network models for cancer type prediction based on gene expression",
    "authors": [
      "Milad Mostavi",
      "Yu-Chiao Chiu",
      "Yufei Huang",
      "Yidong Chen"
    ],
    "author_ids": [],
    "abstract": "Background Precise prediction of cancer types is vital for cancer diagnosis\nand therapy. Important cancer marker genes can be inferred through predictive\nmodel. Several studies have attempted to build machine learning models for this\ntask however none has taken into consideration the effects of tissue of origin\nthat can potentially bias the identification of cancer markers. Results In this\npaper, we introduced several Convolutional Neural Network (CNN) models that\ntake unstructured gene expression inputs to classify tumor and non-tumor\nsamples into their designated cancer types or as normal. Based on different\ndesigns of gene embeddings and convolution schemes, we implemented three CNN\nmodels: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and\ntested on combined 10,340 samples of 33 cancer types and 731 matched normal\ntissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent\nprediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal).\nFurthermore, we interpreted one of the models, known as 1D-CNN model, with a\nguided saliency technique and identified a total of 2,090 cancer markers (108\nper class). The concordance of differential expression of these markers between\nthe cancer type they represent and others is confirmed. In breast cancer, for\ninstance, our model identified well-known markers, such as GATA3 and ESR1.\nFinally, we extended the 1D-CNN model for prediction of breast cancer subtypes\nand achieved an average accuracy of 88.42% among 5 subtypes. The codes can be\nfound at https://github.com/chenlabgccri/CancerTypePrediction.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.GN",
      "cs.LG",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07794v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07625v3",
    "title": "Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data",
    "authors": [
      "David Borland",
      "Wenyuan Wang",
      "Jonathan Zhang",
      "Joshua Shrestha",
      "David Gotz"
    ],
    "author_ids": [],
    "abstract": "The collection of large, complex datasets has become common across a wide\nvariety of domains. Visual analytics tools increasingly play a key role in\nexploring and answering complex questions about these large datasets. However,\nmany visualizations are not designed to concurrently visualize the large number\nof dimensions present in complex datasets (e.g. tens of thousands of distinct\ncodes in an electronic health record system). This fact, combined with the\nability of many visual analytics systems to enable rapid, ad-hoc specification\nof groups, or cohorts, of individuals based on a small subset of visualized\ndimensions, leads to the possibility of introducing selection bias--when the\nuser creates a cohort based on a specified set of dimensions, differences\nacross many other unseen dimensions may also be introduced. These unintended\nside effects may result in the cohort no longer being representative of the\nlarger population intended to be studied, which can negatively affect the\nvalidity of subsequent analyses. We present techniques for selection bias\ntracking and visualization that can be incorporated into high-dimensional\nexploratory visual analytics systems, with a focus on medical data with\nexisting data hierarchies. These techniques include: (1) tree-based cohort\nprovenance and visualization, with a user-specified baseline cohort that all\nother cohorts are compared against, and visual encoding of the drift for each\ncohort, which indicates where selection bias may have occurred, and (2) a set\nof visualizations, including a novel icicle-plot based visualization, to\ncompare in detail the per-dimension differences between the baseline and a\nuser-specified focus cohort. These techniques are integrated into a medical\ntemporal event sequence visual analytics tool. We present example use cases and\nreport findings from domain expert user interviews.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07625v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1911.08657v1",
    "title": "On the Side Effects of Automation in IoT: Complacency and Comfort vs. Relapse and Distrust",
    "authors": [
      "D. Casado-Mansilla",
      "P. Garaizar",
      "A. Irizar-Arrieta",
      "D. López-de-Ipiña"
    ],
    "author_ids": [],
    "abstract": "Automation through IoT brings with it a whole new set of philosophical and\nethical implications that we barely began to address. However, it is widely\nconsidered by many scholars as the panacea to overcoming the majority of\nsocietal issues. The case of energy efficiency as an action for tackling\nclimate change is not different: demand-response proposals or occupancy-driven\nenergy management systems crowd the current research agenda on energy\nefficiency. However, there are still very few studies that have reported the\neffects of automation in the mid or long term beyond energy reduction (e.g.\nemotional feelings derived to interact with automation, complacency to the\ndevices or perceived value of the automation throughout the time). In this\nworkshop article, we report scientific evidence of a study conducted in ten\nworkplaces during more than one year where we found that automating some\nelectronic devices of common use (i.e. moving away or preventing subjects from\nthe control of these devices) in favour of comfort and energy efficiency, is\nassociated with a reduction of the users' confidence in science and technology\nas a mean to solve all environmental current problems and reduce the\nwillingness of people to act in favor of the environment.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.08657v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.07414v2",
    "title": "A Unified Speaker Adaptation Method for Speech Synthesis using Transcribed and Untranscribed Speech with Backpropagation",
    "authors": [
      "Hieu-Thi Luong",
      "Junichi Yamagishi"
    ],
    "author_ids": [],
    "abstract": "By representing speaker characteristic as a single fixed-length vector\nextracted solely from speech, we can train a neural multi-speaker speech\nsynthesis model by conditioning the model on those vectors. This model can also\nbe adapted to unseen speakers regardless of whether the transcript of\nadaptation data is available or not. However, this setup restricts the speaker\ncomponent to just a single bias vector, which in turn limits the performance of\nadaptation process. In this study, we propose a novel speech synthesis model,\nwhich can be adapted to unseen speakers by fine-tuning part of or all of the\nnetwork using either transcribed or untranscribed speech. Our methodology\nessentially consists of two steps: first, we split the conventional acoustic\nmodel into a speaker-independent (SI) linguistic encoder and a speaker-adaptive\n(SA) acoustic decoder; second, we train an auxiliary acoustic encoder that can\nbe used as a substitute for the linguistic encoder whenever linguistic features\nare unobtainable. The results of objective and subjective evaluations show that\nadaptation using either transcribed or untranscribed speech with our\nmethodology achieved a reasonable level of performance with an extremely\nlimited amount of data and greatly improved performance with more data.\nSurprisingly, adaptation with untranscribed speech surpassed the transcribed\ncounterpart in the subjective test, which reveals the limitations of the\nconventional acoustic model and hints at potential directions for improvements.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07414v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07339v2",
    "title": "Reputation Systems -- Fair allocation of points to the editors in the collaborative community",
    "authors": [
      "Shubhendra Pal Singhal"
    ],
    "author_ids": [],
    "abstract": "In this paper we are trying to determine a scheme for the fair allocation of\npoints to the contributors of the collaborative community. The major problem of\nfair allocation of points among the contributors is that we have to analyze the\nimprovement in the versions of an article. Lets say there is a contribution of\nmajor change in content which is relevant vs the contribution of adding a\nsingle comma. Every contributor cannot be given the same points in such a case.\nThere are many ways which can be used like number of changes in a new version.\nThat might seem relevant but it becomes irrelevant in terms of correct content\ncontribution and other significant changes. There is no AI system too which can\ndetect such a change and award the points accordingly. So this problem of\nallocation of points to the contributors is presented by an algorithm with a\ntheoretical proof. It relies on the interactive interaction of the users in the\nsystem which is trivial in case of big system design economies.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE",
      "cs.SI",
      "65-05"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07339v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07337v1",
    "title": "Measuring Bias in Contextualized Word Representations",
    "authors": [
      "Keita Kurita",
      "Nidhi Vyas",
      "Ayush Pareek",
      "Alan W Black",
      "Yulia Tsvetkov"
    ],
    "author_ids": [],
    "abstract": "Contextual word embeddings such as BERT have achieved state of the art\nperformance in numerous NLP tasks. Since they are optimized to capture the\nstatistical properties of training data, they tend to pick up on and amplify\nsocial stereotypes present in the data as well. In this study, we (1)~propose a\ntemplate-based method to quantify bias in BERT; (2)~show that this method\nobtains more consistent results in capturing social biases than the traditional\ncosine based method; and (3)~conduct a case study, evaluating gender bias in a\ndownstream task of Gender Pronoun Resolution. Although our case study focuses\non gender bias, the proposed technique is generalizable to unveiling other\nbiases, including in multiclass settings, such as racial and religious biases.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07337v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07322v2",
    "title": "Whole-Body Control with (Self) Collision Avoidance using Vector Field Inequalities",
    "authors": [
      "Juan José Quiroz-Omaña",
      "Bruno Vilhena Adorno"
    ],
    "author_ids": [],
    "abstract": "This work uses vector field inequalities (VFI) to prevent robot\nself-collisions and collisions with the workspace. Differently from previous\napproaches, the method is suitable for both velocity and torque-actuated\nrobots. We propose a new distance function and its corresponding Jacobian in\norder to generate a VFI to limit the angle between two Pl\\\"ucker lines. This\nnew VFI is used to prevent both undesired end-effector orientations and\nviolation of joints limits. The proposed method is evaluated in a realistic\nsimulation and on a real humanoid robot, showing that all constraints are\nrespected while the robot performs a manipulation task.",
    "published_date": "2019-06-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07322v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.07046v2",
    "title": "Bounded Expectation of Label Assignment: Dataset Annotation by Supervised Splitting with Bias-Reduction Techniques",
    "authors": [
      "Alyssa Herbst",
      "Bert Huang"
    ],
    "author_ids": [],
    "abstract": "Annotating large unlabeled datasets can be a major bottleneck for machine\nlearning applications. We introduce a scheme for inferring labels of unlabeled\ndata at a fraction of the cost of labeling the entire dataset. Our scheme,\nbounded expectation of label assignment (BELA), greedily queries an oracle (or\nhuman labeler) and partitions a dataset to find data subsets that have mostly\nthe same label. BELA can then infer labels by majority vote of the known labels\nin each subset. BELA determines whether to split or label from a subset by\nmaximizing a lower bound on the expected number of correctly labeled examples.\nOur approach differs from existing hierarchical labeling schemes by using\nsupervised models to partition the data, therefore avoiding reliance on\nunsupervised clustering methods that may not accurately group data by label. We\ndesign BELA with strategies to avoid bias that could be introduced through this\nadaptive partitioning. We evaluate BELA on three datasets and find that it\noutperforms existing strategies for adaptive labeling.",
    "published_date": "2019-06-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07046v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1909.11752v1",
    "title": "Challenges of Designing and Developing Tangible Interfaces for Mental Well-being",
    "authors": [
      "Kieran Woodward",
      "Eiman Kanjo",
      "David Brown"
    ],
    "author_ids": [],
    "abstract": "Mental well-being technologies possess many qualities that give them the\npotential to help people receive assessment and treatment who may otherwise not\nreceive help due to fear of stigma or lack of resources. The combination of\nadvances in sensors, microcontrollers and machine learning is leading to the\nemergence of dedicated tangible interfaces to monitor and promote positive\nmental well-being. However, there are key technical, ergonomic and aesthetic\nchallenges to be overcome in order to make these interfaces effective and\nrespond to users' needs. In this paper, the barriers to develop mental\nwell-being tangible interfaces are discussed by identifying and examining the\nrecent technological challenges machine learning, sensors, microcontrollers and\nbatteries create.User-oriented challenges that face the development of mental\nwell-being technologies are then considered ranging from user engagement during\nco-design and trials to ethical and privacy concerns.",
    "published_date": "2019-06-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11752v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06766v2",
    "title": "Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias",
    "authors": [
      "Stéphane d'Ascoli",
      "Levent Sagun",
      "Joan Bruna",
      "Giulio Biroli"
    ],
    "author_ids": [],
    "abstract": "Despite the phenomenal success of deep neural networks in a broad range of\nlearning tasks, there is a lack of theory to understand the way they work. In\nparticular, Convolutional Neural Networks (CNNs) are known to perform much\nbetter than Fully-Connected Networks (FCNs) on spatially structured data: the\narchitectural structure of CNNs benefits from prior knowledge on the features\nof the data, for instance their translation invariance. The aim of this work is\nto understand this fact through the lens of dynamics in the loss landscape.\n  We introduce a method that maps a CNN to its equivalent FCN (denoted as\neFCN). Such an embedding enables the comparison of CNN and FCN training\ndynamics directly in the FCN space. We use this method to test a new training\nprotocol, which consists in training a CNN, embedding it to FCN space at a\ncertain ``relax time'', then resuming the training in FCN space. We observe\nthat for all relax times, the deviation from the CNN subspace is small, and the\nfinal performance reached by the eFCN is higher than that reachable by a\nstandard FCN of same architecture. More surprisingly, for some intermediate\nrelax times, the eFCN outperforms the CNN it stemmed, by combining the prior\ninformation of the CNN and the expressivity of the FCN in a complementary way.\nThe practical interest of our protocol is limited by the very large size of the\nhighly sparse eFCN. However, it offers interesting insights into the\npersistence of architectural bias under stochastic gradient dynamics. It shows\nthe existence of some rare basins in the FCN loss landscape associated with\nvery good generalization. These can only be accessed thanks to the CNN prior,\nwhich helps navigate the landscape during the early stages of optimization.",
    "published_date": "2019-06-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06766v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06725v2",
    "title": "Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good",
    "authors": [
      "Xuewei Wang",
      "Weiyan Shi",
      "Richard Kim",
      "Yoojung Oh",
      "Sijia Yang",
      "Jingwen Zhang",
      "Zhou Yu"
    ],
    "author_ids": [],
    "abstract": "Developing intelligent persuasive conversational agents to change people's\nopinions and actions for social good is the frontier in advancing the ethical\ndevelopment of automated dialogue systems. To do so, the first step is to\nunderstand the intricate organization of strategic disclosures and appeals\nemployed in human persuasion conversations. We designed an online persuasion\ntask where one participant was asked to persuade the other to donate to a\nspecific charity. We collected a large dataset with 1,017 dialogues and\nannotated emerging persuasion strategies from a subset. Based on the\nannotation, we built a baseline classifier with context information and\nsentence-level features to predict the 10 persuasion strategies used in the\ncorpus. Furthermore, to develop an understanding of personalized persuasion\nprocesses, we analyzed the relationships between individuals' demographic and\npsychological backgrounds including personality, morality, value systems, and\ntheir willingness for donation. Then, we analyzed which types of persuasion\nstrategies led to a greater amount of donation depending on the individuals'\npersonal backgrounds. This work lays the ground for developing a personalized\npersuasive dialogue system.",
    "published_date": "2019-06-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06725v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06668v2",
    "title": "Principles alone cannot guarantee ethical AI",
    "authors": [
      "Brent Mittelstadt"
    ],
    "author_ids": [],
    "abstract": "AI Ethics is now a global topic of discussion in academic and policy circles.\nAt least 84 public-private initiatives have produced statements describing\nhigh-level principles, values, and other tenets to guide the ethical\ndevelopment, deployment, and governance of AI. According to recent\nmeta-analyses, AI Ethics has seemingly converged on a set of principles that\nclosely resemble the four classic principles of medical ethics. Despite the\ninitial credibility granted to a principled approach to AI Ethics by the\nconnection to principles in medical ethics, there are reasons to be concerned\nabout its future impact on AI development and governance. Significant\ndifferences exist between medicine and AI development that suggest a principled\napproach in the latter may not enjoy success comparable to the former. Compared\nto medicine, AI development lacks (1) common aims and fiduciary duties, (2)\nprofessional history and norms, (3) proven methods to translate principles into\npractice, and (4) robust legal and professional accountability mechanisms.\nThese differences suggest we should not yet celebrate consensus around\nhigh-level principles that hide deep political and normative disagreement.",
    "published_date": "2019-06-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06668v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06652v1",
    "title": "Staggered DG method for coupling of the Stokes and Darcy-Forchheimer problems",
    "authors": [
      "Lina Zhao",
      "Eric Chung",
      "Eun-Jae Park",
      "Guanyu Zhou"
    ],
    "author_ids": [],
    "abstract": "In this paper we develop a staggered discontinuous Galerkin method for the\nStokes and Darcy-Forchheimer problems coupled with the\n\\Red{Beavers-Joseph-Saffman} conditions. The method is defined by imposing\nstaggered continuity for all the variables involved and the interface\nconditions are enforced by switching the roles of the variables met on the\ninterface, which eliminate the hassle of introducing additional variables. This\nmethod can be flexibly applied to rough grids such as the highly distorted\ngrids and the polygonal grids. In addition, the method allows nonmatching grids\non the interface thanks to the special inclusion of the interface conditions,\nwhich is highly appreciated from a practical point of view. A new discrete\ntrace inequality and a generalized Poincar\\'{e}-Friedrichs inequality are\nproved, which enables us to prove the optimal convergence estimates under\nreasonable regularity assumptions. Finally, several numerical experiments are\ngiven to illustrate the performances of the proposed method, and the numerical\nresults indicate that the proposed method is accurate and efficient, in\naddition, it is a good candidate for practical applications.",
    "published_date": "2019-06-16T00:00:00",
    "year": 2019,
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06652v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.06613v1",
    "title": "The Price of Local Fairness in Multistage Selection",
    "authors": [
      "Vitalii Emelianov",
      "George Arvanitakis",
      "Nicolas Gast",
      "Krishna Gummadi",
      "Patrick Loiseau"
    ],
    "author_ids": [],
    "abstract": "The rise of algorithmic decision making led to active researches on how to\ndefine and guarantee fairness, mostly focusing on one-shot decision making. In\nseveral important applications such as hiring, however, decisions are made in\nmultiple stage with additional information at each stage. In such cases,\nfairness issues remain poorly understood.\n  In this paper we study fairness in $k$-stage selection problems where\nadditional features are observed at every stage. We first introduce two\nfairness notions, local (per stage) and global (final stage) fairness, that\nextend the classical fairness notions to the $k$-stage setting. We propose a\nsimple model based on a probabilistic formulation and show that the locally and\nglobally fair selections that maximize precision can be computed via a linear\nprogram. We then define the price of local fairness to measure the loss of\nprecision induced by local constraints; and investigate theoretically and\nempirically this quantity. In particular, our experiments show that the price\nof local fairness is generally smaller when the sensitive attribute is observed\nat the first stage; but globally fair selections are more locally fair when the\nsensitive attribute is observed at the second stage---hence in both cases it is\noften possible to have a selection that has a small price of local fairness and\nis close to locally fair.",
    "published_date": "2019-06-15T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06613v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.09324v1",
    "title": "Automatic Conditional Generation of Personalized Social Media Short Texts",
    "authors": [
      "Ziwen Wang",
      "Jie Wang",
      "Haiqian Gu",
      "Fei Su",
      "Bojin Zhuang"
    ],
    "author_ids": [],
    "abstract": "Automatic text generation has received much attention owing to rapid\ndevelopment of deep neural networks. In general, text generation systems based\non statistical language model will not consider anthropomorphic\ncharacteristics, which results in machine-like generated texts. To fill the\ngap, we propose a conditional language generation model with Big Five\nPersonality (BFP) feature vectors as input context, which writes human-like\nshort texts. The short text generator consists of a layer of long short memory\nnetwork (LSTM), where a BFP feature vector is concatenated as one part of input\nfor each cell. To enable supervised training generation model, a text\nclassification model based convolution neural network (CNN) has been used to\nprepare BFP-tagged Chinese micro-blog corpora. Validated by a BFP linguistic\ncomputational model, our generated Chinese short texts exhibit discriminative\npersonality styles, which are also syntactically correct and semantically\nsmooth with appropriate emoticons. With combination of natural language\ngeneration with psychological linguistics, our proposed BFP-dependent text\ngeneration model can be widely used for individualization in machine\ntranslation, image caption, dialogue generation and so on.",
    "published_date": "2019-06-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.09324v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06480v1",
    "title": "RECAL: Reuse of Established CNN classifer Apropos unsupervised Learning paradigm",
    "authors": [
      "Jayasree Saha",
      "Jayanta Mukhopadhyay"
    ],
    "author_ids": [],
    "abstract": "Recently, clustering with deep network framework has attracted attention of\nseveral researchers in the computer vision community. Deep framework gains\nextensive attention due to its efficiency and scalability towards large-scale\nand high-dimensional data. In this paper, we transform supervised CNN\nclassifier architecture into an unsupervised clustering model, called RECAL,\nwhich jointly learns discriminative embedding subspace and cluster labels.\nRECAL is made up of feature extraction layers which are convolutional, followed\nby unsupervised classifier layers which is fully connected. A multinomial\nlogistic regression function (softmax) stacked on top of classifier layers. We\ntrain this network using stochastic gradient descent (SGD) optimizer. However,\nthe successful implementation of our model is revolved around the design of\nloss function. Our loss function uses the heuristics that true partitioning\nentails lower entropy given that the class distribution is not heavily skewed.\nThis is a trade-off between the situations of \"skewed distribution\" and\n\"low-entropy\". To handle this, we have proposed classification entropy and\nclass entropy which are the two components of our loss function. In this\napproach, size of the mini-batch should be kept high. Experimental results\nindicate the consistent and competitive behavior of our model for clustering\nwell-known digit, multi-viewed object and face datasets. Morever, we use this\nmodel to generate unsupervised patch segmentation for multi-spectral LISS-IV\nimages. We observe that it is able to distinguish built-up area, wet land,\nvegetation and waterbody from the underlying scene.",
    "published_date": "2019-06-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06480v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06439v3",
    "title": "Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias",
    "authors": [
      "Remi Denton",
      "Ben Hutchinson",
      "Margaret Mitchell",
      "Timnit Gebru",
      "Andrew Zaldivar"
    ],
    "author_ids": [],
    "abstract": "Facial analysis models are increasingly used in applications that have\nserious impacts on people's lives, ranging from authentication to surveillance\ntracking. It is therefore critical to develop techniques that can reveal\nunintended biases in facial classifiers to help guide the ethical use of facial\nanalysis technology. This work proposes a framework called \\textit{image\ncounterfactual sensitivity analysis}, which we explore as a proof-of-concept in\nanalyzing a smiling attribute classifier trained on faces of celebrities. The\nframework utilizes counterfactuals to examine how a classifier's prediction\nchanges if a face characteristic slightly changes. We leverage recent advances\nin generative adversarial networks to build a realistic generative model of\nface images that affords controlled manipulation of specific image\ncharacteristics. We then introduce a set of metrics that measure the effect of\nmanipulating a specific property on the output of the trained classifier.\nEmpirically, we find several different factors of variation that affect the\npredictions of the smiling classifier. This proof-of-concept demonstrates\npotential ways generative models can be leveraged for fine-grained analysis of\nbias and fairness.",
    "published_date": "2019-06-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06439v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06425v1",
    "title": "Principled Frameworks for Evaluating Ethics in NLP Systems",
    "authors": [
      "Shrimai Prabhumoye",
      "Elijah Mayfield",
      "Alan W Black"
    ],
    "author_ids": [],
    "abstract": "We critique recent work on ethics in natural language processing. Those\ndiscussions have focused on data collection, experimental design, and\ninterventions in modeling. But we argue that we ought to first understand the\nframeworks of ethics that are being used to evaluate the fairness and justice\nof algorithmic systems. Here, we begin that discussion by outlining\ndeontological ethics, and envision a research agenda prioritized by it.",
    "published_date": "2019-06-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06425v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06361v2",
    "title": "Online Allocation and Pricing: Constant Regret via Bellman Inequalities",
    "authors": [
      "Alberto Vera",
      "Siddhartha Banerjee",
      "Itai Gurvich"
    ],
    "author_ids": [],
    "abstract": "We develop a framework for designing simple and efficient policies for a\nfamily of online allocation and pricing problems, that includes online packing,\nbudget-constrained probing, dynamic pricing, and online contextual bandits with\nknapsacks. In each case, we evaluate the performance of our policies in terms\nof their regret (i.e., additive gap) relative to an offline controller that is\nendowed with more information than the online controller. Our framework is\nbased on Bellman Inequalities, which decompose the loss of an algorithm into\ntwo distinct sources of error: (1) arising from computational tractability\nissues, and (2) arising from estimation/prediction of random trajectories.\nBalancing these errors guides the choice of benchmarks, and leads to policies\nthat are both tractable and have strong performance guarantees. In particular,\nin all our examples, we demonstrate constant-regret policies that only require\nre-solving an LP in each period, followed by a simple greedy action-selection\nrule; thus, our policies are practical as well as provably near optimal.",
    "published_date": "2019-06-14T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.DS",
      "cs.LG",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06361v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06310v3",
    "title": "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving",
    "authors": [
      "Yurong You",
      "Yan Wang",
      "Wei-Lun Chao",
      "Divyansh Garg",
      "Geoff Pleiss",
      "Bharath Hariharan",
      "Mark Campbell",
      "Kilian Q. Weinberger"
    ],
    "author_ids": [],
    "abstract": "Detecting objects such as cars and pedestrians in 3D plays an indispensable\nrole in autonomous driving. Existing approaches largely rely on expensive LiDAR\nsensors for accurate depth information. While recently pseudo-LiDAR has been\nintroduced as a promising alternative, at a much lower cost based solely on\nstereo images, there is still a notable performance gap. In this paper we\nprovide substantial advances to the pseudo-LiDAR framework through improvements\nin stereo depth estimation. Concretely, we adapt the stereo network\narchitecture and loss function to be more aligned with accurate depth\nestimation of faraway objects --- currently the primary weakness of\npseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely\nsparse LiDAR sensors, which alone provide insufficient information for 3D\ndetection, to de-bias our depth estimation. We propose a depth-propagation\nalgorithm, guided by the initial depth estimates, to diffuse these few exact\nmeasurements across the entire depth map. We show on the KITTI object detection\nbenchmark that our combined approach yields substantial improvements in depth\nestimation and stereo-based 3D object detection --- outperforming the previous\nstate-of-the-art detection accuracy for faraway objects by 40%. Our code is\navailable at https://github.com/mileyan/Pseudo_Lidar_V2.",
    "published_date": "2019-06-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06310v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06026v3",
    "title": "Adversarial Robustness Assessment: Why both $L_0$ and $L_\\infty$ Attacks Are Necessary",
    "authors": [
      "Shashank Kotyan",
      "Danilo Vasconcellos Vargas"
    ],
    "author_ids": [],
    "abstract": "There exists a vast number of adversarial attacks and defences for machine\nlearning algorithms of various types which makes assessing the robustness of\nalgorithms a daunting task. To make matters worse, there is an intrinsic bias\nin these adversarial algorithms. Here, we organise the problems faced: a) Model\nDependence, b) Insufficient Evaluation, c) False Adversarial Samples, and d)\nPerturbation Dependent Results). Based on this, we propose a model agnostic\ndual quality assessment method, together with the concept of robustness levels\nto tackle them. We validate the dual quality assessment on state-of-the-art\nneural networks (WideResNet, ResNet, AllConv, DenseNet, NIN, LeNet and CapsNet)\nas well as adversarial defences for image classification problem. We further\nshow that current networks and defences are vulnerable at all levels of\nrobustness. The proposed robustness assessment reveals that depending on the\nmetric used (i.e., $L_0$ or $L_\\infty$), the robustness may vary significantly.\nHence, the duality should be taken into account for a correct evaluation.\nMoreover, a mathematical derivation, as well as a counter-example, suggest that\n$L_1$ and $L_2$ metrics alone are not sufficient to avoid spurious adversarial\nsamples. Interestingly, the threshold attack of the proposed assessment is a\nnovel $L_\\infty$ black-box adversarial method which requires even less\nperturbation than the One-Pixel Attack (only $12\\%$ of One-Pixel Attack's\namount of perturbation) to achieve similar results.\n  Code is available at http://bit.ly/DualQualityAssessment.",
    "published_date": "2019-06-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "cs.NE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06026v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.05993v1",
    "title": "Conceptor Debiasing of Word Representations Evaluated on WEAT",
    "authors": [
      "Saket Karve",
      "Lyle Ungar",
      "João Sedoc"
    ],
    "author_ids": [],
    "abstract": "Bias in word embeddings such as Word2Vec has been widely investigated, and\nmany efforts made to remove such bias. We show how to use conceptors debiasing\nto post-process both traditional and contextualized word embeddings. Our\nconceptor debiasing can simultaneously remove racial and gender biases and,\nunlike standard debiasing methods, can make effect use of heterogeneous lists\nof biased words. We show that conceptor debiasing diminishes racial and gender\nbias of word representations as measured using the Word Embedding Association\nTest (WEAT) of Caliskan et al. (2017).",
    "published_date": "2019-06-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05993v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.06836v3",
    "title": "Multi-type Resource Allocation with Partial Preferences",
    "authors": [
      "Haibin Wang",
      "Sujoy Sikdar",
      "Xiaoxi Guo",
      "Lirong Xia",
      "Yongzhi Cao",
      "Hanpin Wang"
    ],
    "author_ids": [],
    "abstract": "We propose multi-type probabilistic serial (MPS) and multi-type random\npriority (MRP) as extensions of the well known PS and RP mechanisms to the\nmulti-type resource allocation problem (MTRA) with partial preferences. In our\nsetting, there are multiple types of divisible items, and a group of agents who\nhave partial order preferences over bundles consisting of one item of each\ntype. We show that for the unrestricted domain of partial order preferences, no\nmechanism satisfies both sd-efficiency and sd-envy-freeness. Notwithstanding\nthis impossibility result, our main message is positive: When agents'\npreferences are represented by acyclic CP-nets, MPS satisfies sd-efficiency,\nsd-envy-freeness, ordinal fairness, and upper invariance, while MRP satisfies\nex-post-efficiency, sd-strategy-proofness, and upper invariance, recovering the\nproperties of PS and RP.",
    "published_date": "2019-06-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.06836v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.05496v1",
    "title": "An image-driven machine learning approach to kinetic modeling of a discontinuous precipitation reaction",
    "authors": [
      "Elizabeth Kautz",
      "Wufei Ma",
      "Saumyadeep Jana",
      "Arun Devaraj",
      "Vineet Joshi",
      "Bülent Yener",
      "Daniel Lewis"
    ],
    "author_ids": [],
    "abstract": "Micrograph quantification is an essential component of several materials\nscience studies. Machine learning methods, in particular convolutional neural\nnetworks, have previously demonstrated performance in image recognition tasks\nacross several disciplines (e.g. materials science, medical imaging, facial\nrecognition). Here, we apply these well-established methods to develop an\napproach to microstructure quantification for kinetic modeling of a\ndiscontinuous precipitation reaction in a case study on the uranium-molybdenum\nsystem. Prediction of material processing history based on image data\n(classification), calculation of area fraction of phases present in the\nmicrographs (segmentation), and kinetic modeling from segmentation results were\nperformed. Results indicate that convolutional neural networks represent\nmicrostructure image data well, and segmentation using the k-means clustering\nalgorithm yields results that agree well with manually annotated images.\nClassification accuracies of original and segmented images are both 94\\% for a\n5-class classification problem. Kinetic modeling results agree well with\npreviously reported data using manual thresholding. The image quantification\nand kinetic modeling approach developed and presented here aims to reduce\nresearcher bias introduced into the characterization process, and allows for\nleveraging information in limited image data sets.",
    "published_date": "2019-06-13T00:00:00",
    "year": 2019,
    "categories": [
      "physics.app-ph",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05496v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.05478v3",
    "title": "Robust and interpretable blind image denoising via bias-free convolutional neural networks",
    "authors": [
      "Sreyas Mohan",
      "Zahra Kadkhodaie",
      "Eero P. Simoncelli",
      "Carlos Fernandez-Granda"
    ],
    "author_ids": [],
    "abstract": "Deep convolutional networks often append additive constant (\"bias\") terms to\ntheir convolution operations, enabling a richer repertoire of functional\nmappings. Biases are also used to facilitate training, by subtracting mean\nresponse over batches of training images (a component of \"batch\nnormalization\"). Recent state-of-the-art blind denoising methods (e.g., DnCNN)\nseem to require these terms for their success. Here, however, we show that\nthese networks systematically overfit the noise levels for which they are\ntrained: when deployed at noise levels outside the training range, performance\ndegrades dramatically. In contrast, a bias-free architecture -- obtained by\nremoving the constant terms in every layer of the network, including those used\nfor batch normalization-- generalizes robustly across noise levels, while\npreserving state-of-the-art performance within the training range. Locally, the\nbias-free network acts linearly on the noisy image, enabling direct analysis of\nnetwork behavior via standard linear-algebraic tools. These analyses provide\ninterpretations of network functionality in terms of nonlinear adaptive\nfiltering, and projection onto a union of low-dimensional subspaces, connecting\nthe learning-based method to more traditional denoising methodology.",
    "published_date": "2019-06-13T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05478v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07576v1",
    "title": "The Dynamics of Handwriting Improves the Automated Diagnosis of Dysgraphia",
    "authors": [
      "Konrad Zolna",
      "Thibault Asselborn",
      "Caroline Jolly",
      "Laurence Casteran",
      "Marie-Ange~Nguyen-Morel",
      "Wafa Johal",
      "Pierre Dillenbourg"
    ],
    "author_ids": [],
    "abstract": "Handwriting disorder (termed dysgraphia) is a far from a singular problem as\nnearly 8.6% of the population in France is considered dysgraphic. Moreover,\nresearch highlights the fundamental importance to detect and remediate these\nhandwriting difficulties as soon as possible as they may affect a child's\nentire life, undermining performance and self-confidence in a wide variety of\nschool activities. At the moment, the detection of handwriting difficulties is\nperformed through a standard test called BHK. This detection, performed by\ntherapists, is laborious because of its high cost and subjectivity. We present\na digital approach to identify and characterize handwriting difficulties via a\nRecurrent Neural Network model (RNN). The child under investigation is asked to\nwrite on a graphics tablet all the letters of the alphabet as well as the ten\ndigits. Once complete, the RNN delivers a diagnosis in a few milliseconds and\ndemonstrates remarkable efficiency as it correctly identifies more than 90% of\nchildren diagnosed as dysgraphic using the BHK test. The main advantage of our\ntablet-based system is that it captures the dynamic features of writing --\nsomething a human expert, such as a teacher, is unable to do. We show that\nincorporating the dynamic information available by the use of tablet is highly\nbeneficial to our digital test to discriminate between typically-developing and\ndysgraphic children.",
    "published_date": "2019-06-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07576v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.05330v3",
    "title": "Pairwise Fairness for Ranking and Regression",
    "authors": [
      "Harikrishna Narasimhan",
      "Andrew Cotter",
      "Maya Gupta",
      "Serena Wang"
    ],
    "author_ids": [],
    "abstract": "We present pairwise fairness metrics for ranking models and regression models\nthat form analogues of statistical fairness notions such as equal opportunity,\nequal accuracy, and statistical parity. Our pairwise formulation supports both\ndiscrete protected groups, and continuous protected attributes. We show that\nthe resulting training problems can be efficiently and effectively solved using\nexisting constrained optimization and robust optimization techniques developed\nfor fair classification. Experiments illustrate the broad applicability and\ntrade-offs of these methods.",
    "published_date": "2019-06-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05330v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.05301v4",
    "title": "Learning Curves for Deep Neural Networks: A Gaussian Field Theory Perspective",
    "authors": [
      "Omry Cohen",
      "Or Malka",
      "Zohar Ringel"
    ],
    "author_ids": [],
    "abstract": "In the past decade, deep neural networks (DNNs) came to the fore as the\nleading machine learning algorithms for a variety of tasks. Their raise was\nfounded on market needs and engineering craftsmanship, the latter based more on\ntrial and error than on theory. While still far behind the application\nforefront, the theoretical study of DNNs has recently made important\nadvancements in analyzing the highly over-parameterized regime where some exact\nresults have been obtained. Leveraging these ideas and adopting a more\nphysics-like approach, here we construct a versatile field-theory formalism for\nsupervised deep learning, involving renormalization group, Feynman diagrams and\nreplicas. In particular we show that our approach leads to highly accurate\npredictions of learning curves of truly deep DNNs trained on polynomial\nregression tasks and that these predictions can be used for efficient\nhyper-parameter optimization. In addition, they explain how DNNs generalize\nwell despite being highly over-parameterized, this due to an entropic bias to\nsimple functions which, for the case of fully-connected DNNs with data sampled\non the hypersphere, are low order polynomials in the input vector. Being a\ncomplex interacting system of artificial neurons, we believe that such tools\nand methodologies borrowed from condensed matter physics would prove essential\nfor obtaining an accurate quantitative understanding of deep learning.",
    "published_date": "2019-06-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.NE",
      "physics.data-an",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05301v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.05253v1",
    "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
    "authors": [
      "Benjamin Eysenbach",
      "Ruslan Salakhutdinov",
      "Sergey Levine"
    ],
    "author_ids": [],
    "abstract": "The history of learning for control has been an exciting back and forth\nbetween two broad classes of algorithms: planning and reinforcement learning.\nPlanning algorithms effectively reason over long horizons, but assume access to\na local policy and distance metric over collision-free paths. Reinforcement\nlearning excels at learning policies and the relative values of states, but\nfails to plan over long horizons. Despite the successes of each method in\nvarious domains, tasks that require reasoning over long horizons with limited\nfeedback and high-dimensional observations remain exceedingly challenging for\nboth planning and reinforcement learning algorithms. Frustratingly, these sorts\nof tasks are potentially the most useful, as they are simple to design (a human\nonly need to provide an example goal state) and avoid reward shaping, which can\nbias the agent towards finding a sub-optimal solution. We introduce a general\ncontrol algorithm that combines the strengths of planning and reinforcement\nlearning to effectively solve these tasks. Our aim is to decompose the task of\nreaching a distant goal state into a sequence of easier tasks, each of which\ncorresponds to reaching a subgoal. Planning algorithms can automatically find\nthese waypoints, but only if provided with suitable abstractions of the\nenvironment -- namely, a graph consisting of nodes and edges. Our main insight\nis that this graph can be constructed via reinforcement learning, where a\ngoal-conditioned value function provides edge weights, and nodes are taken to\nbe previously seen observations in a replay buffer. Using graph search over our\nreplay buffer, we can automatically generate this sequence of subgoals, even in\nimage-based environments. Our algorithm, search on the replay buffer (SoRB),\nenables agents to solve sparse reward tasks over one hundred steps, and\ngeneralizes substantially better than standard RL algorithms.",
    "published_date": "2019-06-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05253v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11872v2",
    "title": "The Worrisome Impact of an Inter-rater Bias on Neural Network Training",
    "authors": [
      "Or Shwartzman",
      "Harel Gazit",
      "Ilan Shelef",
      "Tammy Riklin-Raviv"
    ],
    "author_ids": [],
    "abstract": "The problem of inter-rater variability is often discussed in the context of\nmanual labeling of medical images. The emergence of data-driven approaches such\nas Deep Neural Networks (DNNs) brought this issue of raters' disagreement to\nthe front-stage. In this paper, we highlight the issue of inter-rater bias as\nopposed to random inter-observer variability and demonstrate its influence on\nDNN training, leading to different segmentation results for the same input\nimages. In fact, lower overlap scores are obtained between the outputs of a DNN\ntrained on annotations of one rater and tested on another. Moreover, we\ndemonstrate that inter-rater bias in the training examples is amplified and\nbecomes more consistent, considering the segmentation predictions of the DNNs'\ntest data. We support our findings by showing that a classifier-DNN trained to\ndistinguish between raters based on their manual annotations performs better\nwhen the automatic segmentation predictions rather than the actual raters'\nannotations were tested. For this study, we used two different datasets: the\nISBI 2015 Multiple Sclerosis (MS) challenge dataset, including MRI scans each\nwith annotations provided by two raters with different levels of expertise; and\nIntracerebral Hemorrhage (ICH) CT scans with manual and semi-manual\nsegmentations. The results obtained allow us to underline a worrisome clinical\nimplication of a DNN bias induced by an inter-rater bias during training.\nSpecifically, we present a consistent underestimate of MS-lesion loads when\ncalculated from segmentation predictions of a DNN trained on input provided by\nthe less experienced rater. In the same manner, the differences in ICH volumes\ncalculated based on outputs of identical DNNs, each trained on annotations from\na different source are more consistent and larger than the differences in\nvolumes between the manual and semi-manual annotations used for training.",
    "published_date": "2019-06-12T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11872v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04899v2",
    "title": "Prophet Inequalities on the Intersection of a Matroid and a Graph",
    "authors": [
      "Jackie Baek",
      "Will Ma"
    ],
    "author_ids": [],
    "abstract": "We consider prophet inequalities in a setting where agents correspond to both\nelements in a matroid and vertices in a graph. A set of agents is feasible if\nthey form both an independent set in the matroid and an independent set in the\ngraph. Our main result is an ex-ante 1/(2d+2)-prophet inequality, where d is a\ngraph parameter upper-bounded by the maximum size of an independent set in the\nneighborhood of any vertex.\n  We establish this result through a framework that sets both dynamic prices\nfor elements in the matroid (using the method of balanced thresholds), and\nstatic but discriminatory prices for vertices in the graph (motivated by recent\ndevelopments in approximate dynamic programming). The threshold for accepting\nan agent is then the sum of these two prices.\n  We show that for graphs induced by a certain family of interval-scheduling\nconstraints, the value of d is 1. Our framework thus provides the first\nconstant-factor prophet inequality when there are both matroid-independence\nconstraints and interval-scheduling constraints. It also unifies and improves\nseveral results from the literature, leading to a 1/2-prophet inequality when\nagents have XOS valuation functions over a set of items and use them for a\nfinite interval duration, and more generally, a 1/(d+1)-prophet inequality when\nthese items each require a bundle of d resources to procure.",
    "published_date": "2019-06-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04899v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.05684v1",
    "title": "Understanding artificial intelligence ethics and safety",
    "authors": [
      "David Leslie"
    ],
    "author_ids": [],
    "abstract": "A remarkable time of human promise has been ushered in by the convergence of\nthe ever-expanding availability of big data, the soaring speed and stretch of\ncloud computing platforms, and the advancement of increasingly sophisticated\nmachine learning algorithms. Innovations in AI are already leaving a mark on\ngovernment by improving the provision of essential social goods and services\nfrom healthcare, education, and transportation to food supply, energy, and\nenvironmental management. These bounties are likely just the start. The\nprospect that progress in AI will help government to confront some of its most\nurgent challenges is exciting, but legitimate worries abound. As with any new\nand rapidly evolving technology, a steep learning curve means that mistakes and\nmiscalculations will be made and that both unanticipated and harmful impacts\nwill occur.\n  This guide, written for department and delivery leads in the UK public sector\nand adopted by the British Government in its publication, 'Using AI in the\nPublic Sector,' identifies the potential harms caused by AI systems and\nproposes concrete, operationalisable measures to counteract them. It stresses\nthat public sector organisations can anticipate and prevent these potential\nharms by stewarding a culture of responsible innovation and by putting in place\ngovernance processes that support the design and implementation of ethical,\nfair, and safe AI systems. It also highlights the need for algorithmically\nsupported outcomes to be interpretable by their users and made understandable\nto decision subjects in clear, non-technical, and accessible ways. Finally, it\nbuilds out a vision of human-centred and context-sensitive implementation that\ngives a central role to communication, evidence-based reasoning, situational\nawareness, and moral justifiability.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05684v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04836v1",
    "title": "Unmasking Bias in News",
    "authors": [
      "Javier Sánchez-Junquera",
      "Paolo Rosso",
      "Manuel Montes-y-Gómez",
      "Simone Paolo Ponzetto"
    ],
    "author_ids": [],
    "abstract": "We present experiments on detecting hyperpartisanship in news using a\n'masking' method that allows us to assess the role of style vs. content for the\ntask at hand. Our results corroborate previous research on this task in that\ntopic related features yield better results than stylistic ones. We\nadditionally show that competitive results can be achieved by simply including\nhigher-length n-grams, which suggests the need to develop more challenging\ndatasets and tasks that address implicit and more subtle forms of bias.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04836v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04721v3",
    "title": "Data-Free Quantization Through Weight Equalization and Bias Correction",
    "authors": [
      "Markus Nagel",
      "Mart van Baalen",
      "Tijmen Blankevoort",
      "Max Welling"
    ],
    "author_ids": [],
    "abstract": "We introduce a data-free quantization method for deep neural networks that\ndoes not require fine-tuning or hyperparameter selection. It achieves\nnear-original model performance on common computer vision architectures and\ntasks. 8-bit fixed-point quantization is essential for efficient inference on\nmodern deep learning hardware. However, quantizing models to run in 8-bit is a\nnon-trivial task, frequently leading to either significant performance\nreduction or engineering time spent on training a network to be amenable to\nquantization. Our approach relies on equalizing the weight ranges in the\nnetwork by making use of a scale-equivariance property of activation functions.\nIn addition the method corrects biases in the error that are introduced during\nquantization. This improves quantization accuracy performance, and can be\napplied to many common computer vision architectures with a straight forward\nAPI call. For common architectures, such as the MobileNet family, we achieve\nstate-of-the-art quantized model performance. We further show that the method\nalso extends to other computer vision architectures and tasks such as semantic\nsegmentation and object detection.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04721v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04713v1",
    "title": "Automatic brain tissue segmentation in fetal MRI using convolutional neural networks",
    "authors": [
      "N. Khalili",
      "N. Lessmann",
      "E. Turk",
      "N. Claessens",
      "R. de Heus",
      "T. Kolk",
      "M. A. Viergever",
      "M. J. N. L. Benders",
      "I. Isgum"
    ],
    "author_ids": [],
    "abstract": "MR images of fetuses allow clinicians to detect brain abnormalities in an\nearly stage of development. The cornerstone of volumetric and morphologic\nanalysis in fetal MRI is segmentation of the fetal brain into different tissue\nclasses. Manual segmentation is cumbersome and time consuming, hence automatic\nsegmentation could substantially simplify the procedure. However, automatic\nbrain tissue segmentation in these scans is challenging owing to artifacts\nincluding intensity inhomogeneity, caused in particular by spontaneous fetal\nmovements during the scan. Unlike methods that estimate the bias field to\nremove intensity inhomogeneity as a preprocessing step to segmentation, we\npropose to perform segmentation using a convolutional neural network that\nexploits images with synthetically introduced intensity inhomogeneity as data\naugmentation. The method first uses a CNN to extract the intracranial volume.\nThereafter, another CNN with the same architecture is employed to segment the\nextracted volume into seven brain tissue classes: cerebellum, basal ganglia and\nthalami, ventricular cerebrospinal fluid, white matter, brain stem, cortical\ngray matter and extracerebral cerebrospinal fluid. To make the method\napplicable to slices showing intensity inhomogeneity artifacts, the training\ndata was augmented by applying a combination of linear gradients with random\noffsets and orientations to image slices without artifacts.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04713v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04598v1",
    "title": "Joint Subspace Recovery and Enhanced Locality Driven Robust Flexible Discriminative Dictionary Learning",
    "authors": [
      "Zhao Zhang",
      "Jiahuan Ren",
      "Weiming Jiang",
      "Zheng Zhang",
      "Richang Hong",
      "Shuicheng Yan",
      "Meng Wang"
    ],
    "author_ids": [],
    "abstract": "We propose a joint subspace recovery and enhanced locality based robust\nflexible label consistent dictionary learning method called Robust Flexible\nDiscriminative Dictionary Learning (RFDDL). RFDDL mainly improves the data\nrepresentation and classification abilities by enhancing the robust property to\nsparse errors and encoding the locality, reconstruction error and label\nconsistency more accurately. First, for the robustness to noise and sparse\nerrors in data and atoms, RFDDL aims at recovering the underlying clean data\nand clean atom subspaces jointly, and then performs DL and encodes the locality\nin the recovered subspaces. Second, to enable the data sampled from a nonlinear\nmanifold to be handled potentially and obtain the accurate reconstruction by\navoiding the overfitting, RFDDL minimizes the reconstruction error in a\nflexible manner. Third, to encode the label consistency accurately, RFDDL\ninvolves a discriminative flexible sparse code error to encourage the\ncoefficients to be soft. Fourth, to encode the locality well, RFDDL defines the\nLaplacian matrix over recovered atoms, includes label information of atoms in\nterms of intra-class compactness and inter-class separation, and associates\nwith group sparse codes and classifier to obtain the accurate discriminative\nlocality-constrained coefficients and classifier. Extensive results on public\ndatabases show the effectiveness of our RFDDL.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04598v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04488v2",
    "title": "Optimizing Pipelined Computation and Communication for Latency-Constrained Edge Learning",
    "authors": [
      "Nicolas Skatchkovsky",
      "Osvaldo Simeone"
    ],
    "author_ids": [],
    "abstract": "Consider a device that is connected to an edge processor via a communication\nchannel. The device holds local data that is to be offloaded to the edge\nprocessor so as to train a machine learning model, e.g., for regression or\nclassification. Transmission of the data to the learning processor, as well as\ntraining based on Stochastic Gradient Descent (SGD), must be both completed\nwithin a time limit. Assuming that communication and computation can be\npipelined, this letter investigates the optimal choice for the packet payload\nsize, given the overhead of each data packet transmission and the ratio between\nthe computation and the communication rates. This amounts to a tradeoff between\nbias and variance, since communicating the entire data set first reduces the\nbias of the training process but it may not leave sufficient time for learning.\nAnalytical bounds on the expected optimality gap are derived so as to enable an\neffective optimization, which is validated in numerical results.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.DC",
      "cs.IT",
      "cs.NI",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04488v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11881v2",
    "title": "Explicit Disentanglement of Appearance and Perspective in Generative Models",
    "authors": [
      "Nicki Skafte Detlefsen",
      "Søren Hauberg"
    ],
    "author_ids": [],
    "abstract": "Disentangled representation learning finds compact, independent and\neasy-to-interpret factors of the data. Learning such has been shown to require\nan inductive bias, which we explicitly encode in a generative model of images.\nSpecifically, we propose a model with two latent spaces: one that represents\nspatial transformations of the input data, and another that represents the\ntransformed data. We find that the latter naturally captures the intrinsic\nappearance of the data. To realize the generative model, we propose a\nVariationally Inferred Transformational Autoencoder (VITAE) that incorporates a\nspatial transformer into a variational autoencoder. We show how to perform\ninference in the model efficiently by carefully designing the encoders and\nrestricting the transformation class to be diffeomorphic. Empirically, our\nmodel separates the visual style from digit type on MNIST, separates shape and\npose in images of human bodies and facial features from facial shape on CelebA.",
    "published_date": "2019-06-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11881v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04177v2",
    "title": "Estimating Causal Effects of Tone in Online Debates",
    "authors": [
      "Dhanya Sridhar",
      "Lise Getoor"
    ],
    "author_ids": [],
    "abstract": "Statistical methods applied to social media posts shed light on the dynamics\nof online dialogue. For example, users' wording choices predict their\npersuasiveness and users adopt the language patterns of other dialogue\nparticipants. In this paper, we estimate the causal effect of reply tones in\ndebates on linguistic and sentiment changes in subsequent responses. The\nchallenge for this estimation is that a reply's tone and subsequent responses\nare confounded by the users' ideologies on the debate topic and their emotions.\nTo overcome this challenge, we learn representations of ideology using\ngenerative models of text. We study debates from 4Forums and compare annotated\ntones of replying such as emotional versus factual, or reasonable versus\nattacking. We show that our latent confounder representation reduces bias in\nATE estimation. Our results suggest that factual and asserting tones affect\ndialogue and provide a methodology for estimating causal effects from text.",
    "published_date": "2019-06-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04177v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.04066v1",
    "title": "Stretching the Effectiveness of MLE from Accuracy to Bias for Pairwise Comparisons",
    "authors": [
      "Jingyan Wang",
      "Nihar B. Shah",
      "R. Ravi"
    ],
    "author_ids": [],
    "abstract": "A number of applications (e.g., AI bot tournaments, sports, peer grading,\ncrowdsourcing) use pairwise comparison data and the Bradley-Terry-Luce (BTL)\nmodel to evaluate a given collection of items (e.g., bots, teams, students,\nsearch results). Past work has shown that under the BTL model, the widely-used\nmaximum-likelihood estimator (MLE) is minimax-optimal in estimating the item\nparameters, in terms of the mean squared error. However, another important\ndesideratum for designing estimators is fairness. In this work, we consider\nfairness modeled by the notion of bias in statistics. We show that the MLE\nincurs a suboptimal rate in terms of bias. We then propose a simple\nmodification to the MLE, which \"stretches\" the bounding box of the\nmaximum-likelihood optimizer by a small constant factor from the underlying\nground truth domain. We show that this simple modification leads to an improved\nrate in bias, while maintaining minimax-optimality in the mean squared error.\nIn this manner, our proposed class of estimators provably improves fairness\nrepresented by bias without loss in accuracy.",
    "published_date": "2019-06-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.04066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03843v2",
    "title": "Learning Fair Naive Bayes Classifiers by Discovering and Eliminating Discrimination Patterns",
    "authors": [
      "YooJung Choi",
      "Golnoosh Farnadi",
      "Behrouz Babaki",
      "Guy Van den Broeck"
    ],
    "author_ids": [],
    "abstract": "As machine learning is increasingly used to make real-world decisions, recent\nresearch efforts aim to define and ensure fairness in algorithmic decision\nmaking. Existing methods often assume a fixed set of observable features to\ndefine individuals, but lack a discussion of certain features not being\nobserved at test time. In this paper, we study fairness of naive Bayes\nclassifiers, which allow partial observations. In particular, we introduce the\nnotion of a discrimination pattern, which refers to an individual receiving\ndifferent classifications depending on whether some sensitive attributes were\nobserved. Then a model is considered fair if it has no such pattern. We propose\nan algorithm to discover and mine for discrimination patterns in a naive Bayes\nclassifier, and show how to learn maximum likelihood parameters subject to\nthese fairness constraints. Our approach iteratively discovers and eliminates\ndiscrimination patterns until a fair model is learned. An empirical evaluation\non three real-world datasets demonstrates that we can remove exponentially many\ndiscrimination patterns by only adding a small fraction of them as constraints.",
    "published_date": "2019-06-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03843v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03842v3",
    "title": "Analyzing the Role of Model Uncertainty for Electronic Health Records",
    "authors": [
      "Michael W. Dusenberry",
      "Dustin Tran",
      "Edward Choi",
      "Jonas Kemp",
      "Jeremy Nixon",
      "Ghassen Jerfel",
      "Katherine Heller",
      "Andrew M. Dai"
    ],
    "author_ids": [],
    "abstract": "In medicine, both ethical and monetary costs of incorrect predictions can be\nsignificant, and the complexity of the problems often necessitates increasingly\ncomplex models. Recent work has shown that changing just the random seed is\nenough for otherwise well-tuned deep neural networks to vary in their\nindividual predicted probabilities. In light of this, we investigate the role\nof model uncertainty methods in the medical domain. Using RNN ensembles and\nvarious Bayesian RNNs, we show that population-level metrics, such as AUC-PR,\nAUC-ROC, log-likelihood, and calibration error, do not capture model\nuncertainty. Meanwhile, the presence of significant variability in\npatient-specific predictions and optimal decisions motivates the need for\ncapturing model uncertainty. Understanding the uncertainty for individual\npatients is an area with clear clinical impact, such as determining when a\nmodel decision is likely to be brittle. We further show that RNNs with only\nBayesian embeddings can be a more efficient way to capture model uncertainty\ncompared to ensembles, and we analyze how model uncertainty is impacted across\nindividual input features and patient subgroups.",
    "published_date": "2019-06-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03842v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03708v1",
    "title": "Note on the bias and variance of variational inference",
    "authors": [
      "Chin-Wei Huang",
      "Aaron Courville"
    ],
    "author_ids": [],
    "abstract": "In this note, we study the relationship between the variational gap and the\nvariance of the (log) likelihood ratio. We show that the gap can be upper\nbounded by some form of dispersion measure of the likelihood ratio, which\nsuggests the bias of variational inference can be reduced by making the\ndistribution of the likelihood ratio more concentrated, such as via averaging\nand variance reduction.",
    "published_date": "2019-06-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03708v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03695v1",
    "title": "Gendered Pronoun Resolution using BERT and an extractive question answering formulation",
    "authors": [
      "Rakesh Chada"
    ],
    "author_ids": [],
    "abstract": "The resolution of ambiguous pronouns is a longstanding challenge in Natural\nLanguage Understanding. Recent studies have suggested gender bias among\nstate-of-the-art coreference resolution systems. As an example, Google AI\nLanguage team recently released a gender-balanced dataset and showed that\nperformance of these coreference resolvers is significantly limited on the\ndataset. In this paper, we propose an extractive question answering (QA)\nformulation of pronoun resolution task that overcomes this limitation and shows\nmuch lower gender bias (0.99) on their dataset. This system uses fine-tuned\nrepresentations from the pre-trained BERT model and outperforms the existing\nbaseline by a significant margin (22.2% absolute improvement in F1 score)\nwithout using any hand-engineered features. This QA framework is equally\nperformant even without the knowledge of the candidate antecedents of the\npronoun. An ensemble of QA and BERT-based multiple choice and sequence\nclassification models further improves the F1 (23.3% absolute improvement upon\nthe baseline). This ensemble model was submitted to the shared task for the 1st\nACL workshop on Gender Bias for Natural Language Processing. It ranked 9th on\nthe final official leaderboard. Source code is available at\nhttps://github.com/rakeshchada/corefqa",
    "published_date": "2019-06-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03695v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03691v1",
    "title": "Interpreting Age Effects of Human Fetal Brain from Spontaneous fMRI using Deep 3D Convolutional Neural Networks",
    "authors": [
      "Xiangrui Li",
      "Jasmine Hect",
      "Moriah Thomason",
      "Dongxiao Zhu"
    ],
    "author_ids": [],
    "abstract": "Understanding human fetal neurodevelopment is of great clinical importance as\nabnormal development is linked to adverse neuropsychiatric outcomes after\nbirth. Recent advances in functional Magnetic Resonance Imaging (fMRI) have\nprovided new insight into development of the human brain before birth, but\nthese studies have predominately focused on brain functional connectivity (i.e.\nFisher z-score), which requires manual processing steps for feature extraction\nfrom fMRI images. Deep learning approaches (i.e., Convolutional Neural\nNetworks) have achieved remarkable success on learning directly from image\ndata, yet have not been applied on fetal fMRI for understanding fetal\nneurodevelopment. Here, we bridge this gap by applying a novel application of\ndeep 3D CNN to fetal blood oxygen-level dependence (BOLD) resting-state fMRI\ndata. Specifically, we test a supervised CNN framework as a data-driven\napproach to isolate variation in fMRI signals that relate to younger v.s. older\nfetal age groups. Based on the learned CNN, we further perform sensitivity\nanalysis to identify brain regions in which changes in BOLD signal are strongly\nassociated with fetal brain age. The findings demonstrate that deep CNNs are a\npromising approach for identifying spontaneous functional patterns in fetal\nbrain activity that discriminate age groups. Further, we discovered that\nregions that most strongly differentiate groups are largely bilateral, share\nsimilar distribution in older and younger age groups, and are areas of\nheightened metabolic activity in early human development.",
    "published_date": "2019-06-09T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03691v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03559v1",
    "title": "The Implicit Bias of AdaGrad on Separable Data",
    "authors": [
      "Qian Qian",
      "Xiaoyuan Qian"
    ],
    "author_ids": [],
    "abstract": "We study the implicit bias of AdaGrad on separable linear classification\nproblems. We show that AdaGrad converges to a direction that can be\ncharacterized as the solution of a quadratic optimization problem with the same\nfeasible set as the hard SVM problem. We also give a discussion about how\ndifferent choices of the hyperparameters of AdaGrad might impact this\ndirection. This provides a deeper understanding of why adaptive methods do not\nseem to have the generalization ability as good as gradient descent does in\npractice.",
    "published_date": "2019-06-09T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03559v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03518v1",
    "title": "Maximum Weighted Loss Discrepancy",
    "authors": [
      "Fereshte Khani",
      "Aditi Raghunathan",
      "Percy Liang"
    ],
    "author_ids": [],
    "abstract": "Though machine learning algorithms excel at minimizing the average loss over\na population, this might lead to large discrepancies between the losses across\ngroups within the population. To capture this inequality, we introduce and\nstudy a notion we call maximum weighted loss discrepancy (MWLD), the maximum\n(weighted) difference between the loss of a group and the loss of the\npopulation. We relate MWLD to group fairness notions and robustness to\ndemographic shifts. We then show MWLD satisfies the following three properties:\n1) It is statistically impossible to estimate MWLD when all groups have equal\nweights. 2) For a particular family of weighting functions, we can estimate\nMWLD efficiently. 3) MWLD is related to loss variance, a quantity that arises\nin generalization bounds. We estimate MWLD with different weighting functions\non four common datasets from the fairness literature. We finally show that loss\nvariance regularization can halve the loss variance of a classifier and hence\nreduce MWLD without suffering a significant drop in accuracy.",
    "published_date": "2019-06-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03518v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03485v3",
    "title": "Learning Individual Causal Effects from Networked Observational Data",
    "authors": [
      "Ruocheng Guo",
      "Jundong Li",
      "Huan Liu"
    ],
    "author_ids": [],
    "abstract": "Convenient access to observational data enables us to learn causal effects\nwithout randomized experiments. This research direction draws increasing\nattention in research areas such as economics, healthcare, and education. For\nexample, we can study how a medicine (the treatment) causally affects the\nhealth condition (the outcome) of a patient using existing electronic health\nrecords. To validate causal effects learned from observational data, we have to\ncontrol confounding bias -- the influence of variables which causally influence\nboth the treatment and the outcome. Existing work along this line\noverwhelmingly relies on the unconfoundedness assumption that there do not\nexist unobserved confounders. However, this assumption is untestable and can\neven be untenable. In fact, an important fact ignored by the majority of\nprevious work is that observational data can come with network information that\ncan be utilized to infer hidden confounders. For example, in an observational\nstudy of the individual-level treatment effect of a medicine, instead of\nrandomized experiments, the medicine is often assigned to each individual based\non a series of factors. Some of the factors (e.g., socioeconomic status) can be\nchallenging to measure and therefore become hidden confounders. Fortunately,\nthe socioeconomic status of an individual can be reflected by whom she is\nconnected in social networks. With this fact in mind, we aim to exploit the\nnetwork information to recognize patterns of hidden confounders which would\nfurther allow us to learn valid individual causal effects from observational\ndata. In this work, we propose a novel causal inference framework, the network\ndeconfounder, which learns representations to unravel patterns of hidden\nconfounders from the network information. Empirically, we perform extensive\nexperiments to validate the effectiveness of the network deconfounder on\nvarious datasets.",
    "published_date": "2019-06-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03485v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03292v3",
    "title": "On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset",
    "authors": [
      "Muhammad Waleed Gondal",
      "Manuel Wüthrich",
      "Đorđe Miladinović",
      "Francesco Locatello",
      "Martin Breidt",
      "Valentin Volchkov",
      "Joel Akpo",
      "Olivier Bachem",
      "Bernhard Schölkopf",
      "Stefan Bauer"
    ],
    "author_ids": [],
    "abstract": "Learning meaningful and compact representations with disentangled semantic\naspects is considered to be of key importance in representation learning. Since\nreal-world data is notoriously costly to collect, many recent state-of-the-art\ndisentanglement models have heavily relied on synthetic toy data-sets. In this\npaper, we propose a novel data-set which consists of over one million images of\nphysical 3D objects with seven factors of variation, such as object color,\nshape, size and position. In order to be able to control all the factors of\nvariation precisely, we built an experimental platform where the objects are\nbeing moved by a robotic arm. In addition, we provide two more datasets which\nconsist of simulations of the experimental setup. These datasets provide for\nthe first time the possibility to systematically investigate how well different\ndisentanglement methods perform on real data in comparison to simulation, and\nhow simulated data can be leveraged to build better representations of the real\nworld. We provide a first experimental study of these questions and our results\nindicate that learned models transfer poorly, but that model and hyperparameter\nselection is an effective means of transferring information to the real world.",
    "published_date": "2019-06-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03292v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03284v3",
    "title": "Equalized odds postprocessing under imperfect group information",
    "authors": [
      "Pranjal Awasthi",
      "Matthäus Kleindessner",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "Most approaches aiming to ensure a model's fairness with respect to a\nprotected attribute (such as gender or race) assume to know the true value of\nthe attribute for every data point. In this paper, we ask to what extent\nfairness interventions can be effective even when only imperfect information\nabout the protected attribute is available. In particular, we study the\nprominent equalized odds postprocessing method of Hardt et al. (2016) under a\nperturbation of the attribute. We identify conditions on the perturbation that\nguarantee that the bias of a classifier is reduced even by running equalized\nodds with the perturbed attribute. We also study the error of the resulting\nclassifier. We empirically observe that under our identified conditions most\noften the error does not suffer from a perturbation of the protected attribute.\nFor a special case, we formally prove this observation to be true.",
    "published_date": "2019-06-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03284v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.03193v1",
    "title": "Fighting Quantization Bias With Bias",
    "authors": [
      "Alexander Finkelstein",
      "Uri Almog",
      "Mark Grobman"
    ],
    "author_ids": [],
    "abstract": "Low-precision representation of deep neural networks (DNNs) is critical for\nefficient deployment of deep learning application on embedded platforms,\nhowever, converting the network to low precision degrades its performance.\nCrucially, networks that are designed for embedded applications usually suffer\nfrom increased degradation since they have less redundancy. This is most\nevident for the ubiquitous MobileNet architecture which requires a costly\nquantization-aware training cycle to achieve acceptable performance when\nquantized to 8-bits. In this paper, we trace the source of the degradation in\nMobileNets to a shift in the mean activation value. This shift is caused by an\ninherent bias in the quantization process which builds up across layers,\nshifting all network statistics away from the learned distribution. We show\nthat this phenomenon happens in other architectures as well. We propose a\nsimple remedy - compensating for the quantization induced shift by adding a\nconstant to the additive bias term of each channel. We develop two simple\nmethods for estimating the correction constants - one using iterative\nevaluation of the quantized network and one where the constants are set using a\nshort training phase. Both methods are fast and require only a small amount of\nunlabeled data, making them appealing for rapid deployment of neural networks.\nUsing the above methods we are able to match the performance of training-based\nquantization of MobileNets at a fraction of the cost.",
    "published_date": "2019-06-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.03193v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02881v1",
    "title": "Vertex Classification on Weighted Networks",
    "authors": [
      "Hayden Helm",
      "Joshua Vogelstein",
      "Carey Priebe"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a discrimination technique for vertices in a weighted\nnetwork. We assume that the edge weights and adjacencies in the network are\nconditionally independent and that both sources of information encode class\nmembership information. In particular, we introduce a edge weight distribution\nmatrix to the standard K-Block Stochastic Block Model to model weighted\nnetworks. This allows us to develop simple yet powerful extensions of\nclassification techniques using the spectral embedding of the unweighted\nadjacency matrix. We consider two assumptions on the edge weight distributions\nand propose classification procedures in both settings. We show the\neffectiveness of the proposed classifiers by comparing them to quadratic\ndiscriminant analysis following the spectral embedding of a transformed\nweighted network. Moreover, we discuss and show how the methods perform when\nthe edge weights do not encode class membership information.",
    "published_date": "2019-06-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.SI",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02881v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02775v1",
    "title": "Fair Division Without Disparate Impact",
    "authors": [
      "Alexander Peysakhovich",
      "Christian Kroer"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of dividing items between individuals in a way that\nis fair both in the sense of distributional fairness and in the sense of not\nhaving disparate impact across protected classes. An important existing\nmechanism for distributionally fair division is competitive equilibrium from\nequal incomes (CEEI). Unfortunately, CEEI will not, in general, respect\ndisparate impact constraints. We consider two types of disparate impact\nmeasures: requiring that allocations be similar across protected classes and\nrequiring that average utility levels be similar across protected classes. We\nmodify the standard CEEI algorithm in two ways: equitable equilibrium from\nequal incomes, which removes disparate impact in allocations, and competitive\nequilibrium from equitable incomes which removes disparate impact in attained\nutility levels. We show analytically that removing disparate impact in outcomes\nbreaks several of CEEI's desirable properties such as envy, regret, Pareto\noptimality, and incentive compatibility. By contrast, we can remove disparate\nimpact in attained utility levels without affecting these properties. Finally,\nwe experimentally evaluate the tradeoffs between efficiency, equity, and\ndisparate impact in a recommender-system based market.",
    "published_date": "2019-06-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02775v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02589v1",
    "title": "Flexibly Fair Representation Learning by Disentanglement",
    "authors": [
      "Elliot Creager",
      "David Madras",
      "Jörn-Henrik Jacobsen",
      "Marissa A. Weis",
      "Kevin Swersky",
      "Toniann Pitassi",
      "Richard Zemel"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of learning representations that achieve group and\nsubgroup fairness with respect to multiple sensitive attributes. Taking\ninspiration from the disentangled representation learning literature, we\npropose an algorithm for learning compact representations of datasets that are\nuseful for reconstruction and prediction, but are also \\emph{flexibly fair},\nmeaning they can be easily modified at test time to achieve subgroup\ndemographic parity with respect to multiple sensitive attributes and their\nconjunctions. We show empirically that the resulting encoder---which does not\nrequire the sensitive attributes for inference---enables the adaptation of a\nsingle representation to a variety of fair classification tasks with new target\nlabels and subgroup definitions.",
    "published_date": "2019-06-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02589v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02164v2",
    "title": "Data preprocessing to mitigate bias: A maximum entropy based approach",
    "authors": [
      "L. Elisa Celis",
      "Vijay Keswani",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "Data containing human or social attributes may over- or under-represent\ngroups with respect to salient social attributes such as gender or race, which\ncan lead to biases in downstream applications. This paper presents an\nalgorithmic framework that can be used as a data preprocessing method towards\nmitigating such bias. Unlike prior work, it can efficiently learn distributions\nover large domains, controllably adjust the representation rates of protected\ngroups and achieve target fairness metrics such as statistical parity, yet\nremains close to the empirical distribution induced by the given dataset. Our\napproach leverages the principle of maximum entropy - amongst all distributions\nsatisfying a given set of constraints, we should choose the one closest in\nKL-divergence to a given prior. While maximum entropy distributions can\nsuccinctly encode distributions over large domains, they can be difficult to\ncompute. Our main contribution is an instantiation of this framework for our\nset of constraints and priors, which encode our bias mitigation goals, and that\nruns in time polynomial in the dimension of the data. Empirically, we observe\nthat samples from the learned distribution have desired representation rates\nand statistical rates, and when used for training a classifier incurs only a\nslight loss in accuracy while maintaining fairness properties.",
    "published_date": "2019-06-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02164v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02076v2",
    "title": "On the use of Pairwise Distance Learning for Brain Signal Classification with Limited Observations",
    "authors": [
      "David Calhas",
      "Enrique Romero",
      "Rui Henriques"
    ],
    "author_ids": [],
    "abstract": "The increasing access to brain signal data using electroencephalography\ncreates new opportunities to study electrophysiological brain activity and\nperform ambulatory diagnoses of neuronal diseases. This work proposes a\npairwise distance learning approach for Schizophrenia classification relying on\nthe spectral properties of the signal. Given the limited number of observations\n(i.e. the case and/or control individuals) in clinical trials, we propose a\nSiamese neural network architecture to learn a discriminative feature space\nfrom pairwise combinations of observations per channel. In this way, the\nmultivariate order of the signal is used as a form of data augmentation,\nfurther supporting the network generalization ability. Convolutional layers\nwith parameters learned under a cosine contrastive loss are proposed to\nadequately explore spectral images derived from the brain signal. Results on a\ncase-control population show that the features extracted using the proposed\nneural network lead to an improved Schizophrenia diagnosis (+10pp in accuracy\nand sensitivity) against spectral features, thus suggesting the existence of\nnon-trivial, discriminative electrophysiological brain patterns.",
    "published_date": "2019-06-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.NE",
      "eess.SP",
      "q-bio.NC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02076v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01908v1",
    "title": "Empirical Risk Minimization under Random Censorship: Theory and Practice",
    "authors": [
      "Guillaume Ausset",
      "Stéphan Clémençon",
      "François Portier"
    ],
    "author_ids": [],
    "abstract": "We consider the classic supervised learning problem, where a continuous\nnon-negative random label $Y$ (i.e. a random duration) is to be predicted based\nupon observing a random vector $X$ valued in $\\mathbb{R}^d$ with $d\\geq 1$ by\nmeans of a regression rule with minimum least square error. In various\napplications, ranging from industrial quality control to public health through\ncredit risk analysis for instance, training observations can be right censored,\nmeaning that, rather than on independent copies of $(X,Y)$, statistical\nlearning relies on a collection of $n\\geq 1$ independent realizations of the\ntriplet $(X, \\; \\min\\{Y,\\; C\\},\\; \\delta)$, where $C$ is a nonnegative r.v.\nwith unknown distribution, modeling censorship and $\\delta=\\mathbb{I}\\{Y\\leq\nC\\}$ indicates whether the duration is right censored or not. As ignoring\ncensorship in the risk computation may clearly lead to a severe underestimation\nof the target duration and jeopardize prediction, we propose to consider a\nplug-in estimate of the true risk based on a Kaplan-Meier estimator of the\nconditional survival function of the censorship $C$ given $X$, referred to as\nKaplan-Meier risk, in order to perform empirical risk minimization. It is\nestablished, under mild conditions, that the learning rate of minimizers of\nthis biased/weighted empirical risk functional is of order\n$O_{\\mathbb{P}}(\\sqrt{\\log(n)/n})$ when ignoring model bias issues inherent to\nplug-in estimation, as can be attained in absence of censorship. Beyond\ntheoretical results, numerical experiments are presented in order to illustrate\nthe relevance of the approach developed.",
    "published_date": "2019-06-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01908v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02688v2",
    "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings",
    "authors": [
      "Vihari Piratla",
      "Sunita Sarawagi",
      "Soumen Chakrabarti"
    ],
    "author_ids": [],
    "abstract": "Given a small corpus $\\mathcal D_T$ pertaining to a limited set of focused\ntopics, our goal is to train embeddings that accurately capture the sense of\nwords in the topic in spite of the limited size of $\\mathcal D_T$. These\nembeddings may be used in various tasks involving $\\mathcal D_T$. A popular\nstrategy in limited data settings is to adapt pre-trained embeddings $\\mathcal\nE$ trained on a large corpus. To correct for sense drift, fine-tuning,\nregularization, projection, and pivoting have been proposed recently. Among\nthese, regularization informed by a word's corpus frequency performed well, but\nwe improve upon it using a new regularizer based on the stability of its\ncooccurrence with other words. However, a thorough comparison across ten\ntopics, spanning three tasks, with standardized settings of hyper-parameters,\nreveals that even the best embedding adaptation strategies provide small gains\nbeyond well-tuned baselines, which many earlier comparisons ignored. In a bold\ndeparture from adapting pretrained embeddings, we propose using $\\mathcal D_T$\nto probe, attend to, and borrow fragments from any large, topic-rich source\ncorpus (such as Wikipedia), which need not be the corpus used to pretrain\nembeddings. This step is made scalable and practical by suitable indexing. We\nreach the surprising conclusion that even limited corpus augmentation is more\nuseful than adapting embeddings, which suggests that non-dominant sense\ninformation may be irrevocably obliterated from pretrained embeddings and\ncannot be salvaged by adaptation.",
    "published_date": "2019-06-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02688v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01747v1",
    "title": "Balanced Ranking with Diversity Constraints",
    "authors": [
      "Ke Yang",
      "Vasilis Gkatzelis",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "Many set selection and ranking algorithms have recently been enhanced with\ndiversity constraints that aim to explicitly increase representation of\nhistorically disadvantaged populations, or to improve the overall\nrepresentativeness of the selected set. An unintended consequence of these\nconstraints, however, is reduced in-group fairness: the selected candidates\nfrom a given group may not be the best ones, and this unfairness may not be\nwell-balanced across groups.\n  In this paper we study this phenomenon using datasets that comprise multiple\nsensitive attributes. We then introduce additional constraints, aimed at\nbalancing the \\in-group fairness across groups, and formalize the induced\noptimization problems as integer linear programs. Using these programs, we\nconduct an experimental evaluation with real datasets, and quantify the\nfeasible trade-offs between balance and overall performance in the presence of\ndiversity constraints.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01747v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.02687v3",
    "title": "Manifold-regression to predict from MEG/EEG brain signals without source modeling",
    "authors": [
      "David Sabbagh",
      "Pierre Ablin",
      "Gael Varoquaux",
      "Alexandre Gramfort",
      "Denis A. Engemann"
    ],
    "author_ids": [],
    "abstract": "Magnetoencephalography and electroencephalography (M/EEG) can reveal neuronal\ndynamics non-invasively in real-time and are therefore appreciated methods in\nmedicine and neuroscience. Recent advances in modeling brain-behavior\nrelationships have highlighted the effectiveness of Riemannian geometry for\nsummarizing the spatially correlated time-series from M/EEG in terms of their\ncovariance. However, after artefact-suppression, M/EEG data is often rank\ndeficient which limits the application of Riemannian concepts. In this article,\nwe focus on the task of regression with rank-reduced covariance matrices. We\nstudy two Riemannian approaches that vectorize the M/EEG covariance\nbetween-sensors through projection into a tangent space. The Wasserstein\ndistance readily applies to rank-reduced data but lacks affine-invariance. This\ncan be overcome by finding a common subspace in which the covariance matrices\nare full rank, enabling the affine-invariant geometric distance. We\ninvestigated the implications of these two approaches in synthetic generative\nmodels, which allowed us to control estimation bias of a linear model for\nprediction. We show that Wasserstein and geometric distances allow perfect\nout-of-sample prediction on the generative models. We then evaluated the\nmethods on real data with regard to their effectiveness in predicting age from\nM/EEG covariance matrices. The findings suggest that the data-driven Riemannian\nmethods outperform different sensor-space estimators and that they get close to\nthe performance of biophysics-driven source-localization model that requires\nMRI acquisitions and tedious data processing. Our study suggests that the\nproposed Riemannian methods can serve as fundamental building-blocks for\nautomated large-scale analysis of M/EEG.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.02687v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01681v1",
    "title": "Learning dynamic polynomial proofs",
    "authors": [
      "Alhussein Fawzi",
      "Mateusz Malinowski",
      "Hamza Fawzi",
      "Omar Fawzi"
    ],
    "author_ids": [],
    "abstract": "Polynomial inequalities lie at the heart of many mathematical disciplines. In\nthis paper, we consider the fundamental computational task of automatically\nsearching for proofs of polynomial inequalities. We adopt the framework of\nsemi-algebraic proof systems that manipulate polynomial inequalities via\nelementary inference rules that infer new inequalities from the premises. These\nproof systems are known to be very powerful, but searching for proofs remains a\nmajor difficulty. In this work, we introduce a machine learning based method to\nsearch for a dynamic proof within these proof systems. We propose a deep\nreinforcement learning framework that learns an embedding of the polynomials\nand guides the choice of inference rules, taking the inherent symmetries of the\nproblem as an inductive bias. We compare our approach with powerful and\nwidely-studied linear programming hierarchies based on static proof systems,\nand show that our method reduces the size of the linear program by several\norders of magnitude while also improving performance. These results hence pave\nthe way towards augmenting powerful and well-studied semi-algebraic proof\nsystems with machine learning guiding strategies for enhancing the expressivity\nof such proof systems.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CC",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01681v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01562v2",
    "title": "Privacy-preserving Crowd-guided AI Decision-making in Ethical Dilemmas",
    "authors": [
      "Teng Wang",
      "Jun Zhao",
      "Han Yu",
      "Jinyan Liu",
      "Xinyu Yang",
      "Xuebin Ren",
      "Shuyu Shi"
    ],
    "author_ids": [],
    "abstract": "With the rapid development of artificial intelligence (AI), ethical issues\nsurrounding AI have attracted increasing attention. In particular, autonomous\nvehicles may face moral dilemmas in accident scenarios, such as staying the\ncourse resulting in hurting pedestrians or swerving leading to hurting\npassengers. To investigate such ethical dilemmas, recent studies have adopted\npreference aggregation, in which each voter expresses her/his preferences over\ndecisions for the possible ethical dilemma scenarios, and a centralized system\naggregates these preferences to obtain the winning decision. Although a useful\nmethodology for building ethical AI systems, such an approach can potentially\nviolate the privacy of voters since moral preferences are sensitive information\nand their disclosure can be exploited by malicious parties. In this paper, we\nreport a first-of-its-kind privacy-preserving crowd-guided AI decision-making\napproach in ethical dilemmas. We adopt the notion of differential privacy to\nquantify privacy and consider four granularities of privacy protection by\ntaking voter-/record-level privacy protection and centralized/distributed\nperturbation into account, resulting in four approaches VLCP, RLCP, VLDP, and\nRLDP. Moreover, we propose different algorithms to achieve these privacy\nprotection granularities, while retaining the accuracy of the learned moral\npreference model. Specifically, VLCP and RLCP are implemented with the data\naggregator setting a universal privacy parameter and perturbing the averaged\nmoral preference to protect the privacy of voters' data. VLDP and RLDP are\nimplemented in such a way that each voter perturbs her/his local moral\npreference with a personalized privacy parameter. Extensive experiments on both\nsynthetic and real data demonstrate that the proposed approach can achieve high\naccuracy of preference aggregation while protecting individual voter's privacy.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01562v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01552v1",
    "title": "Assessing Disparate Impacts of Personalized Interventions: Identifiability and Bounds",
    "authors": [
      "Nathan Kallus",
      "Angela Zhou"
    ],
    "author_ids": [],
    "abstract": "Personalized interventions in social services, education, and healthcare\nleverage individual-level causal effect predictions in order to give the best\ntreatment to each individual or to prioritize program interventions for the\nindividuals most likely to benefit. While the sensitivity of these domains\ncompels us to evaluate the fairness of such policies, we show that actually\nauditing their disparate impacts per standard observational metrics, such as\ntrue positive rates, is impossible since ground truths are unknown. Whether our\ndata is experimental or observational, an individual's actual outcome under an\nintervention different than that received can never be known, only predicted\nbased on features. We prove how we can nonetheless point-identify these\nquantities under the additional assumption of monotone treatment response,\nwhich may be reasonable in many applications. We further provide a sensitivity\nanalysis for this assumption by means of sharp partial-identification bounds\nunder violations of monotonicity of varying strengths. We show how to use our\nresults to audit personalized interventions using partially-identified ROC and\nxROC curves and demonstrate this in a case study of a French job training\ndataset.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01552v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01440v1",
    "title": "Tracing Antisemitic Language Through Diachronic Embedding Projections: France 1789-1914",
    "authors": [
      "Rocco Tripodi",
      "Massimo Warglien",
      "Simon Levis Sullam",
      "Deborah Paci"
    ],
    "author_ids": [],
    "abstract": "We investigate some aspects of the history of antisemitism in France, one of\nthe cradles of modern antisemitism, using diachronic word embeddings. We\nconstructed a large corpus of French books and periodicals issues that contain\na keyword related to Jews and performed a diachronic word embedding over the\n1789-1914 period. We studied the changes over time in the semantic spaces of 4\ntarget words and performed embedding projections over 6 streams of antisemitic\ndiscourse. This allowed us to track the evolution of antisemitic bias in the\nreligious, economic, socio-politic, racial, ethic and conspiratorial domains.\nProjections show a trend of growing antisemitism, especially in the years\nstarting in the mid-80s and culminating in the Dreyfus affair. Our analysis\nalso allows us to highlight the peculiar adverse bias towards Judaism in the\nbroader context of other religions.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01440v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01314v2",
    "title": "Example-Guided Style Consistent Image Synthesis from Semantic Labeling",
    "authors": [
      "Miao Wang",
      "Guo-Ye Yang",
      "Ruilong Li",
      "Run-Ze Liang",
      "Song-Hai Zhang",
      "Peter. M. Hall",
      "Shi-Min Hu"
    ],
    "author_ids": [],
    "abstract": "Example-guided image synthesis aims to synthesize an image from a semantic\nlabel map and an exemplary image indicating style. We use the term \"style\" in\nthis problem to refer to implicit characteristics of images, for example: in\nportraits \"style\" includes gender, racial identity, age, hairstyle; in full\nbody pictures it includes clothing; in street scenes, it refers to weather and\ntime of day and such like. A semantic label map in these cases indicates facial\nexpression, full body pose, or scene segmentation. We propose a solution to the\nexample-guided image synthesis problem using conditional generative adversarial\nnetworks with style consistency. Our key contributions are (i) a novel style\nconsistency discriminator to determine whether a pair of images are consistent\nin style; (ii) an adaptive semantic consistency loss; and (iii) a training data\nsampling strategy, for synthesizing style-consistent results to the exemplar.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01314v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01986v1",
    "title": "Nonatomic Aggregative Games with Infinitely Many Types",
    "authors": [
      "Paulin Jacquot",
      "Cheng Wan"
    ],
    "author_ids": [],
    "abstract": "We define and analyze the notion of variational Wardrop equilibrium for\nnonatomic aggregative games with an infinity of players types. These equilibria\nare characterized through an infinite-dimensional variational inequality. We\nshow, under monotonicity conditions, a convergence theorem enables to\napproximate such an equilibrium with arbitrary precision. To this end, we\nintroduce a sequence of nonatomic games with a finite number of players types,\nwhich approximates the initial game. We show the existence of a symmetric\nWardrop equilibrium in each of these games. We prove that those symmetric\nequilibria converge to an equilibrium of the infinite game, and that they can\nbe computed as solutions of finite-dimensional variational inequalities. The\nmodel is illustrated through an example from smart grids: the description of a\nlarge population of electricity consumers by a parametric distribution gives a\nnonatomic game with an infinity of different players types, with actions\nsubject to coupling constraints.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01986v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.01160v1",
    "title": "Transfer Learning with intelligent training data selection for prediction of Alzheimer's Disease",
    "authors": [
      "Naimul Mefraz Khan",
      "Marcia Hon",
      "Nabila Abraham"
    ],
    "author_ids": [],
    "abstract": "Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI\nthrough machine learning has been a subject of intense research in recent\nyears. Recent success of deep learning in computer vision has progressed such\nresearch further. However, common limitations with such algorithms are reliance\non a large number of training images, and requirement of careful optimization\nof the architecture of deep networks. In this paper, we attempt solving these\nissues with transfer learning, where the state-of-the-art VGG architecture is\ninitialized with pre-trained weights from large benchmark datasets consisting\nof natural images. The network is then fine-tuned with layer-wise tuning, where\nonly a pre-defined group of layers are trained on MRI images. To shrink the\ntraining data size, we employ image entropy to select the most informative\nslices. Through experimentation on the ADNI dataset, we show that with training\nsize of 10 to 20 times smaller than the other contemporary methods, we reach\nstate-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC\nclassification problems, with a 4% and a 7% increase in accuracy over the\nstate-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide\ndetailed analysis of the effect of the intelligent training data selection\nmethod, changing the training size, and changing the number of layers to be\nfine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate\nhow the proposed model focuses on discriminative image regions that are\nneuropathologically relevant, and can help the healthcare practitioner in\ninterpreting the model's decision making process.",
    "published_date": "2019-06-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01160v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00839v1",
    "title": "Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling",
    "authors": [
      "Sandeep Attree"
    ],
    "author_ids": [],
    "abstract": "This paper presents a strong set of results for resolving gendered ambiguous\npronouns on the Gendered Ambiguous Pronouns shared task. The model presented\nhere draws upon the strengths of state-of-the-art language and coreference\nresolution models, and introduces a novel evidence-based deep learning\narchitecture. Injecting evidence from the coreference models compliments the\nbase architecture, and analysis shows that the model is not hindered by their\nweaknesses, specifically gender bias. The modularity and simplicity of the\narchitecture make it very easy to extend for further improvement and applicable\nto other NLP problems. Evaluation on GAP test data results in a\nstate-of-the-art performance at 92.5% F1 (gender bias of 0.97), edging closer\nto the human performance of 96.6%. The end-to-end solution presented here\nplaced 1st in the Kaggle competition, winning by a significant lead. The code\nis available at https://github.com/sattree/gap.",
    "published_date": "2019-06-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00839v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00742v1",
    "title": "Gender-preserving Debiasing for Pre-trained Word Embeddings",
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ],
    "author_ids": [],
    "abstract": "Word embeddings learnt from massive text collections have demonstrated\nsignificant levels of discriminative biases such as gender, racial or ethnic\nbiases, which in turn bias the down-stream NLP applications that use those word\nembeddings. Taking gender-bias as a working example, we propose a debiasing\nmethod that preserves non-discriminative gender-related information, while\nremoving stereotypical discriminative gender biases from pre-trained word\nembeddings. Specifically, we consider four types of information:\n\\emph{feminine}, \\emph{masculine}, \\emph{gender-neutral} and\n\\emph{stereotypical}, which represent the relationship between gender vs. bias,\nand propose a debiasing method that (a) preserves the gender-related\ninformation in feminine and masculine words, (b) preserves the neutrality in\ngender-neutral words, and (c) removes the biases from stereotypical words.\nExperimental results on several previously proposed benchmark datasets show\nthat our proposed method can debias pre-trained word embeddings better than\nexisting SoTA methods proposed for debiasing word embeddings while preserving\ngender-related but non-discriminative information.",
    "published_date": "2019-06-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00742v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.01161v2",
    "title": "Resolving Gendered Ambiguous Pronouns with BERT",
    "authors": [
      "Matei Ionita",
      "Yury Kashnitsky",
      "Ken Krige",
      "Vladimir Larin",
      "Denis Logvinenko",
      "Atanas Atanasov"
    ],
    "author_ids": [],
    "abstract": "Pronoun resolution is part of coreference resolution, the task of pairing an\nexpression to its referring entity. This is an important task for natural\nlanguage understanding and a necessary component of machine translation\nsystems, chat bots and assistants. Neural machine learning systems perform far\nfrom ideally in this task, reaching as low as 73% F1 scores on modern benchmark\ndatasets. Moreover, they tend to perform better for masculine pronouns than for\nfeminine ones. Thus, the problem is both challenging and important for NLP\nresearchers and practitioners. In this project, we describe our BERT-based\napproach to solving the problem of gender-balanced pronoun resolution. We are\nable to reach 92% F1 score and a much lower gender bias on the benchmark\ndataset shared by Google AI Language team.",
    "published_date": "2019-06-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.01161v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00591v1",
    "title": "Evaluating Gender Bias in Machine Translation",
    "authors": [
      "Gabriel Stanovsky",
      "Noah A. Smith",
      "Luke Zettlemoyer"
    ],
    "author_ids": [],
    "abstract": "We present the first challenge set and evaluation protocol for the analysis\nof gender bias in machine translation (MT). Our approach uses two recent\ncoreference resolution datasets composed of English sentences which cast\nparticipants into non-stereotypical gender roles (e.g., \"The doctor asked the\nnurse to help her in the operation\"). We devise an automatic gender bias\nevaluation method for eight target languages with grammatical gender, based on\nmorphological analysis (e.g., the use of female inflection for the word\n\"doctor\"). Our analyses show that four popular industrial MT systems and two\nrecent state-of-the-art academic MT models are significantly prone to\ngender-biased translation errors for all tested target languages. Our data and\ncode are made publicly available.",
    "published_date": "2019-06-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00591v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00569v1",
    "title": "Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards",
    "authors": [
      "Anmol Kagrecha",
      "Jayakrishnan Nair",
      "Krishna Jagannathan"
    ],
    "author_ids": [],
    "abstract": "Classical multi-armed bandit problems use the expected value of an arm as a\nmetric to evaluate its goodness. However, the expected value is a risk-neutral\nmetric. In many applications like finance, one is interested in balancing the\nexpected return of an arm (or portfolio) with the risk associated with that\nreturn. In this paper, we consider the problem of selecting the arm that\noptimizes a linear combination of the expected reward and the associated\nConditional Value at Risk (CVaR) in a fixed budget best-arm identification\nframework. We allow the reward distributions to be unbounded or even\nheavy-tailed. For this problem, our goal is to devise algorithms that are\nentirely distribution oblivious, i.e., the algorithm is not aware of any\ninformation on the reward distributions, including bounds on the moments/tails,\nor the suboptimality gaps across arms.\n  In this paper, we provide a class of such algorithms with provable upper\nbounds on the probability of incorrect identification. In the process, we\ndevelop a novel estimator for the CVaR of unbounded (including heavy-tailed)\nrandom variables and prove a concentration inequality for the same, which could\nbe of independent interest. We also compare the error bounds for our\ndistribution oblivious algorithms with those corresponding to standard\nnon-oblivious algorithms. Finally, numerical experiments reveal that our\nalgorithms perform competitively when compared with non-oblivious algorithms,\nsuggesting that distribution obliviousness can be realised in practice without\nincurring a significant loss of performance.",
    "published_date": "2019-06-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00569v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00451v1",
    "title": "Exact inference in structured prediction",
    "authors": [
      "Kevin Bello",
      "Jean Honorio"
    ],
    "author_ids": [],
    "abstract": "Structured prediction can be thought of as a simultaneous prediction of\nmultiple labels. This is often done by maximizing a score function on the space\nof labels, which decomposes as a sum of pairwise and unary potentials. The\nabove is naturally modeled with a graph, where edges and vertices are related\nto pairwise and unary potentials, respectively. We consider the generative\nprocess proposed by Globerson et al. and apply it to general connected graphs.\nWe analyze the structural conditions of the graph that allow for the exact\nrecovery of the labels. Our results show that exact recovery is possible and\nachievable in polynomial time for a large class of graphs. In particular, we\nshow that graphs that are bad expanders can be exactly recovered by adding\nsmall edge perturbations coming from the Erd\\H{o}s-R\\'enyi model. Finally, as a\nbyproduct of our analysis, we provide an extension of Cheeger's inequality.",
    "published_date": "2019-06-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00451v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.07261v1",
    "title": "Stability and non-linear dynamics of Dual congestion control schemes with two delays",
    "authors": [
      "Abuthahir Abuthahir",
      "Gaurav Raina"
    ],
    "author_ids": [],
    "abstract": "In this paper, we analyze some local stability and local bifurcation\nproperties of the Proportionally fair, TCP fair, and the Delay-based dual\nalgorithms in the presence of two distinct time delays. In particular, our\nfocus is on the interplay between different notions of fairness, stability, and\nbifurcation theoretic properties. Different notions of fairness give rise to\ndifferent non-linear models for the class of Dual algorithms. One can devise\nconditions for local stability, for each of these models, but such conditions\ndo not offer clear design recommendations on which fairness criteria is\ndesirable. With a bifurcation-theoretic analysis, we have to take non-linear\nterms into consideration, which helps to learn additional dynamical properties\nof the various systems. In the case of TCP fair and Delay dual algorithms, with\ntwo delays, we present evidence that they can undergo a sub-critical Hopf\nbifurcation, which has not been previously revealed through analysis of the\nsingle delay variants of these algorithms. A sub-critical Hopf bifurcation can\nresult in either large amplitude limit cycles or unstable limit cycles, and\nhence should be avoided in engineering applications. In the case of the\nProportionally fair algorithm, we provide strong evidence to suggest that all\none should expect is the occurrence of a super-critical Hopf bifurcation, which\nleads to stable limit cycles with small amplitude. Thus, from a design\nperspective, our analysis favors the use of Proportional fairness in the class\nof dual congestion control algorithms. To best of our knowledge, this is the\nfirst study that presents evidence to suggest that fluid models representing\nInternet congestion control algorithms may undergo a sub-critical Hopf\nbifurcation.",
    "published_date": "2019-06-02T00:00:00",
    "year": 2019,
    "categories": [
      "nlin.CD",
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07261v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.00389v4",
    "title": "Disparate Vulnerability to Membership Inference Attacks",
    "authors": [
      "Bogdan Kulynych",
      "Mohammad Yaghini",
      "Giovanni Cherubin",
      "Michael Veale",
      "Carmela Troncoso"
    ],
    "author_ids": [],
    "abstract": "A membership inference attack (MIA) against a machine-learning model enables\nan attacker to determine whether a given data record was part of the model's\ntraining data or not. In this paper, we provide an in-depth study of the\nphenomenon of disparate vulnerability against MIAs: unequal success rate of\nMIAs against different population subgroups. We first establish necessary and\nsufficient conditions for MIAs to be prevented, both on average and for\npopulation subgroups, using a notion of distributional generalization. Second,\nwe derive connections of disparate vulnerability to algorithmic fairness and to\ndifferential privacy. We show that fairness can only prevent disparate\nvulnerability against limited classes of adversaries. Differential privacy\nbounds disparate vulnerability but can significantly reduce the accuracy of the\nmodel. We show that estimating disparate vulnerability to MIAs by na\\\"ively\napplying existing attacks can lead to overestimation. We then establish which\nattacks are suitable for estimating disparate vulnerability, and provide a\nstatistical framework for doing so reliably. We conduct experiments on\nsynthetic and real-world data finding statistically significant evidence of\ndisparate vulnerability in realistic settings. The code is available at\nhttps://github.com/spring-epfl/disparate-vulnerability",
    "published_date": "2019-06-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00389v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00957v3",
    "title": "Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules",
    "authors": [
      "Niklas W. A. Gebauer",
      "Michael Gastegger",
      "Kristof T. Schütt"
    ],
    "author_ids": [],
    "abstract": "Deep learning has proven to yield fast and accurate predictions of\nquantum-chemical properties to accelerate the discovery of novel molecules and\nmaterials. As an exhaustive exploration of the vast chemical space is still\ninfeasible, we require generative models that guide our search towards systems\nwith desired properties. While graph-based models have previously been\nproposed, they are restricted by a lack of spatial information such that they\nare unable to recognize spatial isomerism and non-bonded interactions. Here, we\nintroduce a generative neural network for 3d point sets that respects the\nrotational invariance of the targeted structures. We apply it to the generation\nof molecules and demonstrate its ability to approximate the distribution of\nequilibrium structures using spatial metrics as well as established measures\nfrom chemoinformatics. As our model is able to capture the complex relationship\nbetween 3d geometry and electronic properties, we bias the distribution of the\ngenerator towards molecules with a small HOMO-LUMO gap - an important property\nfor the design of organic solar cells.",
    "published_date": "2019-06-02T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00957v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00346v2",
    "title": "Pre-training of Graph Augmented Transformers for Medication Recommendation",
    "authors": [
      "Junyuan Shang",
      "Tengfei Ma",
      "Cao Xiao",
      "Jimeng Sun"
    ],
    "author_ids": [],
    "abstract": "Medication recommendation is an important healthcare application. It is\ncommonly formulated as a temporal prediction task. Hence, most existing works\nonly utilize longitudinal electronic health records (EHRs) from a small number\nof patients with multiple visits ignoring a large number of patients with a\nsingle visit (selection bias). Moreover, important hierarchical knowledge such\nas diagnosis hierarchy is not leveraged in the representation learning process.\nTo address these challenges, we propose G-BERT, a new model to combine the\npower of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder\nRepresentations from Transformers) for medical code representation and\nmedication recommendation. We use GNNs to represent the internal hierarchical\nstructures of medical codes. Then we integrate the GNN representation into a\ntransformer-based visit encoder and pre-train it on EHR data from patients only\nwith a single visit. The pre-trained visit encoder and representation are then\nfine-tuned for downstream predictive tasks on longitudinal EHRs from patients\nwith multiple visits. G-BERT is the first to bring the language model\npre-training schema into the healthcare domain and it achieved state-of-the-art\nperformance on the medication recommendation task.",
    "published_date": "2019-06-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00346v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00285v2",
    "title": "Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination",
    "authors": [
      "Nathan Kallus",
      "Xiaojie Mao",
      "Angela Zhou"
    ],
    "author_ids": [],
    "abstract": "The increasing impact of algorithmic decisions on people's lives compels us\nto scrutinize their fairness and, in particular, the disparate impacts that\nostensibly-color-blind algorithms can have on different groups. Examples\ninclude credit decisioning, hiring, advertising, criminal justice, personalized\nmedicine, and targeted policymaking, where in some cases legislative or\nregulatory frameworks for fairness exist and define specific protected classes.\nIn this paper we study a fundamental challenge to assessing disparate impacts\nin practice: protected class membership is often not observed in the data. This\nis particularly a problem in lending and healthcare. We consider the use of an\nauxiliary dataset, such as the US census, to construct models that predict the\nprotected class from proxy variables, such as surname and geolocation. We show\nthat even with such data, a variety of common disparity measures are generally\nunidentifiable, providing a new perspective on the documented biases of popular\nproxy-based methods. We provide exact characterizations of the\ntightest-possible set of all possible true disparities that are consistent with\nthe data (and possibly any assumptions). We further provide optimization-based\nalgorithms for computing and visualizing these sets and statistical tools to\nassess sampling uncertainty. Together, these enable reliable and robust\nassessments of disparities -- an important tool when disparity assessment can\nhave far-reaching policy implications. We demonstrate this in two case studies\nwith real data: mortgage lending and personalized medicine dosing.",
    "published_date": "2019-06-01T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00285v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00250v2",
    "title": "Metric Learning for Individual Fairness",
    "authors": [
      "Christina Ilvento"
    ],
    "author_ids": [],
    "abstract": "There has been much discussion recently about how fairness should be measured\nor enforced in classification. Individual Fairness [Dwork, Hardt, Pitassi,\nReingold, Zemel, 2012], which requires that similar individuals be treated\nsimilarly, is a highly appealing definition as it gives strong guarantees on\ntreatment of individuals. Unfortunately, the need for a task-specific\nsimilarity metric has prevented its use in practice. In this work, we propose a\nsolution to the problem of approximating a metric for Individual Fairness based\non human judgments. Our model assumes that we have access to a human fairness\narbiter, who can answer a limited set of queries concerning similarity of\nindividuals for a particular task, is free of explicit biases and possesses\nsufficient domain knowledge to evaluate similarity. Our contributions include\ndefinitions for metric approximation relevant for Individual Fairness,\nconstructions for approximations from a limited number of realistic queries to\nthe arbiter on a sample of individuals, and learning procedures to construct\nhypotheses for metric approximations which generalize to unseen samples under\ncertain assumptions of learnability of distance threshold functions.",
    "published_date": "2019-06-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00250v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00186v1",
    "title": "On the Fairness Performance of NOMA-based Wireless Powered Communication Networks",
    "authors": [
      "Yong Liu",
      "Xuehan Chen",
      "Lin X. Cai",
      "Qingchun Chen",
      "Ruoting Gong",
      "Dong Tang"
    ],
    "author_ids": [],
    "abstract": "The near-far problem causes severe throughput unfairness in wireless powered\ncommunication networks (WPCN). In this paper, we exploit non-orthogonal\nmultiple access (NOMA) technology and propose a fairness-aware NOMA-based\nscheduling scheme to mitigate the near-far effect and to enhance the max-min\nfairness. Specifically, we sort all users according to their channel conditions\nand divide them into two groups, the interference group with high channel gains\nand the noninterference group with low channel gains. The power station (PS)\nconcurrently transmits energy signals with the data transmissions of the users\nin the interference group. Thus, the users in the noninterference group can\nharvest more energy and achieve a higher throughput, while the users in the\ninterference group degrade their performance due to the interfering signals\nfrom the PS. We then apply order statistic theory to analyze the achievable\nrates of ordered users, based on which all users are appropriately grouped for\nNOMA transmission to achieve the max-min fairness of the system. Meanwhile, the\noptimal number of interfered users that determines the set of users in each\ngroup, is derived. Our simulation results validate the significant improvement\nof both network fairness and throughput via the fairness-aware NOMA-based\nscheduling scheme.",
    "published_date": "2019-06-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00186v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1906.00150v5",
    "title": "Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks",
    "authors": [
      "Joonyoung Yi",
      "Juhyuk Lee",
      "Kwang Joon Kim",
      "Sung Ju Hwang",
      "Eunho Yang"
    ],
    "author_ids": [],
    "abstract": "Handling missing data is one of the most fundamental problems in machine\nlearning. Among many approaches, the simplest and most intuitive way is zero\nimputation, which treats the value of a missing entry simply as zero. However,\nmany studies have experimentally confirmed that zero imputation results in\nsuboptimal performances in training neural networks. Yet, none of the existing\nwork has explained what brings such performance degradations. In this paper, we\nintroduce the variable sparsity problem (VSP), which describes a phenomenon\nwhere the output of a predictive model largely varies with respect to the rate\nof missingness in the given input, and show that it adversarially affects the\nmodel performance. We first theoretically analyze this phenomenon and propose a\nsimple yet effective technique to handle missingness, which we refer to as\nSparsity Normalization (SN), that directly targets and resolves the VSP. We\nfurther experimentally validate SN on diverse benchmark datasets, to show that\ndebiasing the effect of input-level sparsity improves the performance and\nstabilizes the training of neural networks.",
    "published_date": "2019-06-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00150v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00128v1",
    "title": "Achieving Fairness in Determining Medicaid Eligibility through Fairgroup Construction",
    "authors": [
      "Boli Fang",
      "Miao Jiang",
      "Jerry Shen"
    ],
    "author_ids": [],
    "abstract": "Effective complements to human judgment, artificial intelligence techniques\nhave started to aid human decisions in complicated social problems across the\nworld. In the context of United States for instance, automated ML/DL\nclassification models offer complements to human decisions in determining\nMedicaid eligibility. However, given the limitations in ML/DL model design,\nthese algorithms may fail to leverage various factors for decision making,\nresulting in improper decisions that allocate resources to individuals who may\nnot be in the most need. In view of such an issue, we propose in this paper the\nmethod of \\textit{fairgroup construction}, based on the legal doctrine of\n\\textit{disparate impact}, to improve the fairness of regressive classifiers.\nExperiments on American Community Survey dataset demonstrate that our method\ncould be easily adapted to a variety of regressive classification models to\nboost their fairness in deciding Medicaid Eligibility, while maintaining high\nlevels of classification accuracy.",
    "published_date": "2019-06-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00128v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.00066v3",
    "title": "Optimized Score Transformation for Consistent Fair Classification",
    "authors": [
      "Dennis Wei",
      "Karthikeyan Natesan Ramamurthy",
      "Flavio du Pin Calmon"
    ],
    "author_ids": [],
    "abstract": "This paper considers fair probabilistic binary classification where the\noutputs of primary interest are predicted probabilities, commonly referred to\nas scores. We formulate the problem of transforming scores to satisfy fairness\nconstraints that are linear in conditional means of scores while minimizing a\ncross-entropy objective. The formulation can be applied directly to\npost-process classifier outputs and we also explore a pre-processing extension,\nthus allowing maximum freedom in selecting a classification algorithm. We\nderive a closed-form expression for the optimal transformed scores and a convex\noptimization problem for the transformation parameters. In the population\nlimit, the transformed score function is the fairness-constrained minimizer of\ncross-entropy with respect to the true conditional probability of the outcome.\nIn the finite sample setting, we propose a method called FairScoreTransformer\nto approach this solution using a combination of standard probabilistic\nclassifiers and ADMM. We provide several consistency and finite-sample\nguarantees for FairScoreTransformer, relating to the transformation parameters\nand transformed score function that it obtains. Comprehensive experiments\ncomparing to 10 existing methods show that FairScoreTransformer has advantages\nfor score-based metrics such as Brier score and AUC while remaining competitive\nfor binary label-based metrics such as accuracy.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00066v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13662v2",
    "title": "On the Fairness of Disentangled Representations",
    "authors": [
      "Francesco Locatello",
      "Gabriele Abbati",
      "Tom Rainforth",
      "Stefan Bauer",
      "Bernhard Schölkopf",
      "Olivier Bachem"
    ],
    "author_ids": [],
    "abstract": "Recently there has been a significant interest in learning disentangled\nrepresentations, as they promise increased interpretability, generalization to\nunseen scenarios and faster learning on downstream tasks. In this paper, we\ninvestigate the usefulness of different notions of disentanglement for\nimproving the fairness of downstream prediction tasks based on representations.\nWe consider the setting where the goal is to predict a target variable based on\nthe learned representation of high-dimensional observations (such as images)\nthat depend on both the target variable and an \\emph{unobserved} sensitive\nvariable. We show that in this setting both the optimal and empirical\npredictions can be unfair, even if the target variable and the sensitive\nvariable are independent. Analyzing the representations of more than\n\\num{12600} trained state-of-the-art disentangled models, we observe that\nseveral disentanglement scores are consistently correlated with increased\nfairness, suggesting that disentanglement may be a useful property to encourage\nfairness when sensitive variables are not observed.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13662v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13651v3",
    "title": "Principal Fairness: Removing Bias via Projections",
    "authors": [
      "Aris Anagnostopoulos",
      "Luca Becchetti",
      "Adriano Fazzone",
      "Cristina Menghini",
      "Chris Schwiegelshohn"
    ],
    "author_ids": [],
    "abstract": "Reducing hidden bias in the data and ensuring fairness in algorithmic data\nanalysis has recently received significant attention. We complement several\nrecent papers in this line of research by introducing a general method to\nreduce bias in the data through random projections in a \"fair\" subspace.\n  We apply this method to densest subgraph problem. For densest subgraph, our\napproach based on fair projections allows to recover both theoretically and\nempirically an almost optimal, fair, dense subgraph hidden in the input data.\nWe also show that, under the small set expansion hypothesis, approximating this\nproblem beyond a factor of 2 is NP-hard and we show a polynomial time algorithm\nwith a matching approximation bound.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13651v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13613v2",
    "title": "Regression Networks for Meta-Learning Few-Shot Classification",
    "authors": [
      "Arnout Devos",
      "Matthias Grossglauser"
    ],
    "author_ids": [],
    "abstract": "We propose regression networks for the problem of few-shot classification,\nwhere a classifier must generalize to new classes not seen in the training set,\ngiven only a small number of examples of each class. In high dimensional\nembedding spaces the direction of data generally contains richer information\nthan magnitude. Next to this, state-of-the-art few-shot metric methods that\ncompare distances with aggregated class representations, have shown superior\nperformance. Combining these two insights, we propose to meta-learn\nclassification of embedded points by regressing the closest approximation in\nevery class subspace while using the regression error as a distance metric.\nSimilarly to recent approaches for few-shot learning, regression networks\nreflect a simple inductive bias that is beneficial in this limited-data regime\nand they achieve excellent results, especially when more aggregate class\nrepresentations can be formed with multiple shots.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13613v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13367v2",
    "title": "PAC-Bayes Un-Expected Bernstein Inequality",
    "authors": [
      "Zakaria Mhammedi",
      "Peter D. Grunwald",
      "Benjamin Guedj"
    ],
    "author_ids": [],
    "abstract": "We present a new PAC-Bayesian generalization bound. Standard bounds contain a\n$\\sqrt{L_n \\cdot \\KL/n}$ complexity term which dominates unless $L_n$, the\nempirical error of the learning algorithm's randomized predictions, vanishes.\nWe manage to replace $L_n$ by a term which vanishes in many more situations,\nessentially whenever the employed learning algorithm is sufficiently stable on\nthe dataset at hand. Our new bound consistently beats state-of-the-art bounds\nboth on a toy example and on UCI datasets (with large enough $n$).\nTheoretically, unlike existing bounds, our new bound can be expected to\nconverge to $0$ faster whenever a Bernstein/Tsybakov condition holds, thus\nconnecting PAC-Bayesian generalization and {\\em excess risk\\/} bounds---for the\nlatter it has long been known that faster convergence can be obtained under\nBernstein conditions. Our main technical tool is a new concentration inequality\nwhich is like Bernstein's but with $X^2$ taken outside its expectation.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13367v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13364v1",
    "title": "Can We Derive Explicit and Implicit Bias from Corpus?",
    "authors": [
      "Bo Wang",
      "Baixiang Xue",
      "Anthony G. Greenwald"
    ],
    "author_ids": [],
    "abstract": "Language is a popular resource to mine speakers' attitude bias, supposing\nthat speakers' statements represent their bias on concepts. However, psychology\nstudies show that people's explicit bias in statements can be different from\ntheir implicit bias in mind. Although both explicit and implicit bias are\nuseful for different applications, current automatic techniques do not\ndistinguish them. Inspired by psychological measurements of explicit and\nimplicit bias, we develop an automatic language-based technique to reproduce\npsychological measurements on large population. By connecting each\npsychological measurement with the statements containing the certain\ncombination of special words, we derive explicit and implicit bias by\nunderstanding the sentiment of corresponding category of statements. Extensive\nexperiments on English and Chinese serious media (Wikipedia) and non-serious\nmedia (social media) show that our method successfully reproduce the\nsmall-scale psychological observations on large population and achieve new\nfindings.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13364v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13358v1",
    "title": "Multi-modal Discriminative Model for Vision-and-Language Navigation",
    "authors": [
      "Haoshuo Huang",
      "Vihan Jain",
      "Harsh Mehta",
      "Jason Baldridge",
      "Eugene Ie"
    ],
    "author_ids": [],
    "abstract": "Vision-and-Language Navigation (VLN) is a natural language grounding task\nwhere agents have to interpret natural language instructions in the context of\nvisual scenes in a dynamic environment to achieve prescribed navigation goals.\nSuccessful agents must have the ability to parse natural language of varying\nlinguistic styles, ground them in potentially unfamiliar scenes, plan and react\nwith ambiguous environmental feedback. Generalization ability is limited by the\namount of human annotated data. In particular, \\emph{paired} vision-language\nsequence data is expensive to collect. We develop a discriminator that\nevaluates how well an instruction explains a given path in VLN task using\nmulti-modal alignment. Our study reveals that only a small fraction of the\nhigh-quality augmented data from \\citet{Fried:2018:Speaker}, as scored by our\ndiscriminator, is useful for training VLN agents with similar performance on\npreviously unseen environments. We also show that a VLN agent warm-started with\npre-trained components from the discriminator outperforms the benchmark success\nrates of 35.5 by 10\\% relative measure on previously unseen environments.",
    "published_date": "2019-05-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13358v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13350v1",
    "title": "Threshold-Based Retrieval and Textual Entailment Detection on Legal Bar Exam Questions",
    "authors": [
      "Sabine Wehnert",
      "Sayed Anisul Hoque",
      "Wolfram Fenske",
      "Gunter Saake"
    ],
    "author_ids": [],
    "abstract": "Getting an overview over the legal domain has become challenging, especially\nin a broad, international context. Legal question answering systems have the\npotential to alleviate this task by automatically retrieving relevant legal\ntexts for a specific statement and checking whether the meaning of the\nstatement can be inferred from the found documents. We investigate a\ncombination of the BM25 scoring method of Elasticsearch with word embeddings\ntrained on English translations of the German and Japanese civil law. For this,\nwe define criteria which select a dynamic number of relevant documents\naccording to threshold scores. Exploiting two deep learning classifiers and\ntheir respective prediction bias with a threshold-based answer inclusion\ncriterion has shown to be beneficial for the textual entailment task, when\ncompared to the baseline.",
    "published_date": "2019-05-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13350v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13260v1",
    "title": "Large Scale Incremental Learning",
    "authors": [
      "Yue Wu",
      "Yinpeng Chen",
      "Lijuan Wang",
      "Yuancheng Ye",
      "Zicheng Liu",
      "Yandong Guo",
      "Yun Fu"
    ],
    "author_ids": [],
    "abstract": "Modern machine learning suffers from catastrophic forgetting when learning\nnew classes incrementally. The performance dramatically degrades due to the\nmissing data of old classes. Incremental learning methods have been proposed to\nretain the knowledge acquired from the old classes, by using knowledge\ndistilling and keeping a few exemplars from the old classes. However, these\nmethods struggle to scale up to a large number of classes. We believe this is\nbecause of the combination of two factors: (a) the data imbalance between the\nold and new classes, and (b) the increasing number of visually similar classes.\nDistinguishing between an increasing number of visually similar classes is\nparticularly challenging, when the training data is unbalanced. We propose a\nsimple and effective method to address this data imbalance issue. We found that\nthe last fully connected layer has a strong bias towards the new classes, and\nthis bias can be corrected by a linear model. With two bias parameters, our\nmethod performs remarkably well on two large datasets: ImageNet (1000 classes)\nand MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms\nby 11.1% and 13.2% respectively.",
    "published_date": "2019-05-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13260v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1907.01101v1",
    "title": "A Simulation Study of Social-Networking-Driven Smart Recommendations for Internet of Vehicles",
    "authors": [
      "Kashif Zia",
      "Arshad Muhammad",
      "Dinesh Kumar Saini"
    ],
    "author_ids": [],
    "abstract": "Social aspects of connectivity and information dispersion are often ignored\nwhile weighing the potential of Internet of Things (IoT). In the specialized\ndomain of Internet of Vehicles (IoV), Social IoV (SIoV) is introduced\nrealization its importance. Assuming a more commonly acceptable standardization\nof Big Data generated by IoV, the social dimensions enabling its fruitful usage\nremains a challenge. In this paper, an agent-based model of information sharing\nbetween vehicles for context-aware recommendations is presented. The model\nadheres to social dimensions as that of human society. Some important\nhypotheses are tested under reasonable connectivity and data constraints. The\nsimulation results reveal that closure of social ties and its timing impacts\ndispersion of novel information (necessary for a recommender system)\nsubstantially. It was also observed that as the network evolves as a result of\nincremental interactions, recommendations guaranteeing a fair distribution of\nvehicles across equally good competitors is not possible.",
    "published_date": "2019-05-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.01101v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1906.11891v1",
    "title": "Characterizing Bias in Classifiers using Generative Models",
    "authors": [
      "Daniel McDuff",
      "Shuang Ma",
      "Yale Song",
      "Ashish Kapoor"
    ],
    "author_ids": [],
    "abstract": "Models that are learned from real-world data are often biased because the\ndata used to train them is biased. This can propagate systemic human biases\nthat exist and ultimately lead to inequitable treatment of people, especially\nminorities. To characterize bias in learned classifiers, existing approaches\nrely on human oracles labeling real-world examples to identify the \"blind\nspots\" of the classifiers; these are ultimately limited due to the human labor\nrequired and the finite nature of existing image examples. We propose a\nsimulation-based approach for interrogating classifiers using generative\nadversarial models in a systematic manner. We incorporate a progressive\nconditional generative model for synthesizing photo-realistic facial images and\nBayesian Optimization for an efficient interrogation of independent facial\nimage classification systems. We show how this approach can be used to\nefficiently characterize racial and gender biases in commercial systems.",
    "published_date": "2019-05-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.11891v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12843v1",
    "title": "Fair Regression: Quantitative Definitions and Reduction-based Algorithms",
    "authors": [
      "Alekh Agarwal",
      "Miroslav Dudík",
      "Zhiwei Steven Wu"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the prediction of a real-valued target, such as a\nrisk score or recidivism rate, while guaranteeing a quantitative notion of\nfairness with respect to a protected attribute such as gender or race. We call\nthis class of problems \\emph{fair regression}. We propose general schemes for\nfair regression under two notions of fairness: (1) statistical parity, which\nasks that the prediction be statistically independent of the protected\nattribute, and (2) bounded group loss, which asks that the prediction error\nrestricted to any protected group remain below some pre-determined level. While\nwe only study these two notions of fairness, our schemes are applicable to\narbitrary Lipschitz-continuous losses, and so they encompass least-squares\nregression, logistic regression, quantile regression, and many other tasks. Our\nschemes only require access to standard risk minimization algorithms (such as\nstandard classification or least-squares regression) while providing\ntheoretical guarantees on the optimality and fairness of the obtained\nsolutions. In addition to analyzing theoretical properties of our schemes, we\nempirically demonstrate their ability to uncover fairness--accuracy frontiers\non several standard datasets.",
    "published_date": "2019-05-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12843v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12801v2",
    "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
    "authors": [
      "Yusu Qian",
      "Urwa Muaz",
      "Ben Zhang",
      "Jae Won Hyun"
    ],
    "author_ids": [],
    "abstract": "Gender bias exists in natural language datasets which neural language models\ntend to learn, resulting in biased text generation. In this research, we\npropose a debiasing approach based on the loss function modification. We\nintroduce a new term to the loss function which attempts to equalize the\nprobabilities of male and female words in the output. Using an array of bias\nevaluation metrics, we provide empirical evidence that our approach\nsuccessfully mitigates gender bias in language models without increasing\nperplexity. In comparison to existing debiasing strategies, data augmentation,\nand word embedding debiasing, our method performs better in several aspects,\nespecially in reducing gender bias in occupation words. Finally, we introduce a\ncombination of data augmentation and our approach, and show that it outperforms\nexisting strategies in all bias evaluation metrics.",
    "published_date": "2019-05-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12801v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12760v1",
    "title": "Batch weight for domain adaptation with mass shift",
    "authors": [
      "Mikołaj Bińkowski",
      "R Devon Hjelm",
      "Aaron Courville"
    ],
    "author_ids": [],
    "abstract": "Unsupervised domain transfer is the task of transferring or translating\nsamples from a source distribution to a different target distribution. Current\nsolutions unsupervised domain transfer often operate on data on which the modes\nof the distribution are well-matched, for instance have the same frequencies of\nclasses between source and target distributions. However, these models do not\nperform well when the modes are not well-matched, as would be the case when\nsamples are drawn independently from two different, but related, domains. This\nmode imbalance is problematic as generative adversarial networks (GANs), a\nsuccessful approach in this setting, are sensitive to mode frequency, which\nresults in a mismatch of semantics between source samples and generated samples\nof the target distribution. We propose a principled method of re-weighting\ntraining samples to correct for such mass shift between the transferred\ndistributions, which we call batch-weight. We also provide rigorous\nprobabilistic setting for domain transfer and new simplified objective for\ntraining transfer networks, an alternative to complex, multi-component loss\nfunctions used in the current state-of-the art image-to-image translation\nmodels. The new objective stems from the discrimination of joint distributions\nand enforces cycle-consistency in an abstract, high-level, rather than\npixel-wise, sense. Lastly, we experimentally show the effectiveness of the\nproposed methods in several image-to-image translation tasks.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12760v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12744v2",
    "title": "Fair Decision Making using Privacy-Protected Data",
    "authors": [
      "Satya Kuppam",
      "Ryan Mckenna",
      "David Pujol",
      "Michael Hay",
      "Ashwin Machanavajjhala",
      "Gerome Miklau"
    ],
    "author_ids": [],
    "abstract": "Data collected about individuals is regularly used to make decisions that\nimpact those same individuals. We consider settings where sensitive personal\ndata is used to decide who will receive resources or benefits. While it is well\nknown that there is a tradeoff between protecting privacy and the accuracy of\ndecisions, we initiate a first-of-its-kind study into the impact of formally\nprivate mechanisms (based on differential privacy) on fair and equitable\ndecision-making. We empirically investigate novel tradeoffs on two real-world\ndecisions made using U.S. Census data (allocation of federal funds and\nassignment of voting rights benefits) as well as a classic apportionment\nproblem. Our results show that if decisions are made using an\n$\\epsilon$-differentially private version of the data, under strict privacy\nconstraints (smaller $\\epsilon$), the noise added to achieve privacy may\ndisproportionately impact some groups over others. We propose novel measures of\nfairness in the context of randomized differentially private algorithms and\nidentify a range of causes of outcome disparities.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12744v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.12728v1",
    "title": "Fairness and Missing Values",
    "authors": [
      "Fernando Martínez-Plumed",
      "Cèsar Ferri",
      "David Nieves",
      "José Hernández-Orallo"
    ],
    "author_ids": [],
    "abstract": "The causes underlying unfair decision making are complex, being internalised\nin different ways by decision makers, other actors dealing with data and\nmodels, and ultimately by the individuals being affected by these decisions.\nOne frequent manifestation of all these latent causes arises in the form of\nmissing values: protected groups are more reluctant to give information that\ncould be used against them, delicate information for some groups can be erased\nby human operators, or data acquisition may simply be less complete and\nsystematic for minority groups. As a result, missing values and bias in data\nare two phenomena that are tightly coupled. However, most recent techniques,\nlibraries and experimental results dealing with fairness in machine learning\nhave simply ignored missing data. In this paper, we claim that fairness\nresearch should not miss the opportunity to deal properly with missing data. To\nsupport this claim, (1) we analyse the sources of missing data and bias, and we\nmap the common causes, (2) we find that rows containing missing values are\nusually fairer than the rest, which should not be treated as the uncomfortable\nugly data that different techniques and libraries get rid of at the first\noccasion, and (3) we study the trade-off between performance and fairness when\nthe rows with missing values are used (either because the technique deals with\nthem directly or by imputation methods). We end the paper with a series of\nrecommended procedures about what to do with missing data when aiming for fair\ndecision making.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12728v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12616v3",
    "title": "Defending Against Neural Fake News",
    "authors": [
      "Rowan Zellers",
      "Ari Holtzman",
      "Hannah Rashkin",
      "Yonatan Bisk",
      "Ali Farhadi",
      "Franziska Roesner",
      "Yejin Choi"
    ],
    "author_ids": [],
    "abstract": "Recent progress in natural language generation has raised dual-use concerns.\nWhile applications like summarization and translation are positive, the\nunderlying technology also might enable adversaries to generate neural fake\nnews: targeted propaganda that closely mimics the style of real news.\n  Modern computer security relies on careful threat modeling: identifying\npotential threats and vulnerabilities from an adversary's point of view, and\nexploring potential mitigations to these threats. Likewise, developing robust\ndefenses against neural fake news requires us first to carefully investigate\nand characterize the risks of these models. We thus present a model for\ncontrollable text generation called Grover. Given a headline like `Link Found\nBetween Vaccines and Autism,' Grover can generate the rest of the article;\nhumans find these generations to be more trustworthy than human-written\ndisinformation.\n  Developing robust verification techniques against generators like Grover is\ncritical. We find that best current discriminators can classify neural fake\nnews from real, human-written, news with 73% accuracy, assuming access to a\nmoderate level of training data. Counterintuitively, the best defense against\nGrover turns out to be Grover itself, with 92% accuracy, demonstrating the\nimportance of public release of strong generators. We investigate these results\nfurther, showing that exposure bias -- and sampling strategies that alleviate\nits effects -- both leave artifacts that similar discriminators can pick up on.\nWe conclude by discussing ethical issues regarding the technology, and plan to\nrelease Grover publicly, helping pave the way for better detection of neural\nfake news.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12616v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12614v4",
    "title": "Unsupervised Model Selection for Variational Disentangled Representation Learning",
    "authors": [
      "Sunny Duan",
      "Loic Matthey",
      "Andre Saraiva",
      "Nicholas Watters",
      "Christopher P. Burgess",
      "Alexander Lerchner",
      "Irina Higgins"
    ],
    "author_ids": [],
    "abstract": "Disentangled representations have recently been shown to improve fairness,\ndata efficiency and generalisation in simple supervised and reinforcement\nlearning tasks. To extend the benefits of disentangled representations to more\ncomplex domains and practical applications, it is important to enable\nhyperparameter tuning and model selection of existing unsupervised approaches\nwithout requiring access to ground truth attribute labels, which are not\navailable for most datasets. This paper addresses this problem by introducing a\nsimple yet robust and reliable method for unsupervised disentangled model\nselection. Our approach, Unsupervised Disentanglement Ranking (UDR), leverages\nthe recent theoretical results that explain why variational autoencoders\ndisentangle (Rolinek et al, 2019), to quantify the quality of disentanglement\nby performing pairwise comparisons between trained model representations. We\nshow that our approach performs comparably to the existing supervised\nalternatives across 5,400 models from six state of the art unsupervised\ndisentangled representation learning model classes. Furthermore, we show that\nthe ranking produced by our approach correlates well with the final task\nperformance on two different domains.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12614v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12528v2",
    "title": "Glancing Through Massive Binary Radio Lenses: Hardware-Aware Interferometry With 1-Bit Sensors",
    "authors": [
      "Manuel S. Stein"
    ],
    "author_ids": [],
    "abstract": "Energy consumption and hardware cost of signal digitization together with the\nmanagement of the resulting data volume form serious issues for high-rate\nmeasurement systems with multiple sensors. Switching to binary sensing\nfront-ends results in a resource-efficient layout but is commonly associated\nwith significant distortion due to the nonlinear signal acquisition. In\nparticular, for applications that require to solve high-resolution processing\ntasks under extreme conditions, it is a widely held belief that low-complexity\n$1$-bit analog-to-digital conversion leads to unacceptable performance\ndegradation. In the Big Science context of low-frequency radio astronomy, we\npropose a telescope architecture based on simplistic binary sampling, precise\nprobabilistic modeling, and likelihood-oriented data processing. The main\nprinciples, building blocks, and advantages of such a radio telescope system,\nwhich we refer to as The Massive Binary Radio Lenses, are sketched. The open\nengineering science questions which have to be answered before building a\nprototype are outlined. We set sail for the academic technology study by\nderiving a statistical algorithm for interferometric imaging from binary array\nmeasurements. The method aims at extracting the full discriminative information\nabout the spatial power distribution embedded in a binary sensor data stream\nwithout bias. Radio measurements obtained with LOFAR are used to test the\ndeveloped imaging technique and discuss visual and quantitative results. These\nassessments shed light on the fact that binary radio telescopes are suited for\nsurveying the universe.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "astro-ph.IM",
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12528v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.12516v1",
    "title": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "authors": [
      "Thomas Davidson",
      "Debasmita Bhattacharya",
      "Ingmar Weber"
    ],
    "author_ids": [],
    "abstract": "Technologies for abusive language detection are being developed and applied\nwith little consideration of their potential biases. We examine racial bias in\nfive different sets of Twitter data annotated for hate speech and abusive\nlanguage. We train classifiers on these datasets and compare the predictions of\nthese classifiers on tweets written in African-American English with those\nwritten in Standard American English. The results show evidence of systematic\nracial bias in all datasets, as classifiers trained on them tend to predict\nthat tweets written in African-American English are abusive at substantially\nhigher rates. If these abusive language detection systems are used in the field\nthey will therefore have a disproportionate negative impact on African-American\nsocial media users. Consequently, these systems may discriminate against the\ngroups who are often the targets of the abuse we are trying to detect.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12516v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12498v1",
    "title": "Image-to-Image Translation with Multi-Path Consistency Regularization",
    "authors": [
      "Jianxin Lin",
      "Yingce Xia",
      "Yijun Wang",
      "Tao Qin",
      "Zhibo Chen"
    ],
    "author_ids": [],
    "abstract": "Image translation across different domains has attracted much attention in\nboth machine learning and computer vision communities. Taking the translation\nfrom source domain $\\mathcal{D}_s$ to target domain $\\mathcal{D}_t$ as an\nexample, existing algorithms mainly rely on two kinds of loss for training: One\nis the discrimination loss, which is used to differentiate images generated by\nthe models and natural images; the other is the reconstruction loss, which\nmeasures the difference between an original image and the reconstructed version\nthrough $\\mathcal{D}_s\\to\\mathcal{D}_t\\to\\mathcal{D}_s$ translation. In this\nwork, we introduce a new kind of loss, multi-path consistency loss, which\nevaluates the differences between direct translation\n$\\mathcal{D}_s\\to\\mathcal{D}_t$ and indirect translation\n$\\mathcal{D}_s\\to\\mathcal{D}_a\\to\\mathcal{D}_t$ with $\\mathcal{D}_a$ as an\nauxiliary domain, to regularize training. For multi-domain translation (at\nleast, three) which focuses on building translation models between any two\ndomains, at each training iteration, we randomly select three domains, set them\nrespectively as the source, auxiliary and target domains, build the multi-path\nconsistency loss and optimize the network. For two-domain translation, we need\nto introduce an additional auxiliary domain and construct the multi-path\nconsistency loss. We conduct various experiments to demonstrate the\neffectiveness of our proposed methods, including face-to-face translation,\npaint-to-photo translation, and de-raining/de-noising translation.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12498v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12487v1",
    "title": "Food for thought: Ethical considerations of user trust in computer vision",
    "authors": [
      "Kaylen J. Pfisterer",
      "Jennifer Boger",
      "Alexander Wong"
    ],
    "author_ids": [],
    "abstract": "In computer vision research, especially when novel applications of tools are\ndeveloped, ethical implications around user perceptions of trust in the\nunderlying technology should be considered and supported. Here, we describe an\nexample of the incorporation of such considerations within the long-term care\nsector for tracking resident food and fluid intake. We highlight our recent\nuser study conducted to develop a Goldilocks quality horizontal prototype\ndesigned to support trust cues in which perceived trust in our horizontal\nprototype was higher than the existing system in place. We discuss the\nimportance and need for user engagement as part of ongoing computer\nvision-driven technology development and describe several important factors\nrelated to trust that are relevant to developing decision-making tools.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12487v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12213v5",
    "title": "Where is the Information in a Deep Neural Network?",
    "authors": [
      "Alessandro Achille",
      "Giovanni Paolini",
      "Stefano Soatto"
    ],
    "author_ids": [],
    "abstract": "Whatever information a deep neural network has gleaned from training data is\nencoded in its weights. How this information affects the response of the\nnetwork to future data remains largely an open question. Indeed, even defining\nand measuring information entails some subtleties, since a trained network is a\ndeterministic map, so standard information measures can be degenerate. We\nmeasure information in a neural network via the optimal trade-off between\naccuracy of the response and complexity of the weights, measured by their\ncoding length. Depending on the choice of code, the definition can reduce to\nstandard measures such as Shannon Mutual Information and Fisher Information.\nHowever, the more general definition allows us to relate information to\ngeneralization and invariance, through a novel notion of effective information\nin the activations of a deep network. We establish a novel relation between the\ninformation in the weights and the effective information in the activations,\nand use this result to show that models with low (information) complexity not\nonly generalize better, but are bound to learn invariant representations of\nfuture inputs. These relations hinge not only on the architecture of the model,\nbut also on how it is trained, highlighting the complex inter-dependency\nbetween the class of functions implemented by deep neural networks, the loss\nfunction used for training them from finite data, and the inductive bias\nimplicit in the optimization.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12213v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12173v2",
    "title": "On the Inductive Bias of Neural Tangent Kernels",
    "authors": [
      "Alberto Bietti",
      "Julien Mairal"
    ],
    "author_ids": [],
    "abstract": "State-of-the-art neural networks are heavily over-parameterized, making the\noptimization algorithm a crucial ingredient for learning predictive models with\ngood generalization properties. A recent line of work has shown that in a\ncertain over-parameterized regime, the learning dynamics of gradient descent\nare governed by a certain kernel obtained at initialization, called the neural\ntangent kernel. We study the inductive bias of learning in such a regime by\nanalyzing this kernel and the corresponding function space (RKHS). In\nparticular, we study smoothness, approximation, and stability properties of\nfunctions with finite norm, including stability to image deformations in the\ncase of convolutional networks, and compare to other known kernels for similar\narchitectures.",
    "published_date": "2019-05-29T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12173v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11931v2",
    "title": "Adversarial Domain Adaptation Being Aware of Class Relationships",
    "authors": [
      "Zeya Wang",
      "Baoyu Jing",
      "Yang Ni",
      "Nanqing Dong",
      "Pengtao Xie",
      "Eric P. Xing"
    ],
    "author_ids": [],
    "abstract": "Adversarial training is a useful approach to promote the learning of\ntransferable representations across the source and target domains, which has\nbeen widely applied for domain adaptation (DA) tasks based on deep neural\nnetworks. Until very recently, existing adversarial domain adaptation (ADA)\nmethods ignore the useful information from the label space, which is an\nimportant factor accountable for the complicated data distributions associated\nwith different semantic classes. Especially, the inter-class semantic\nrelationships have been rarely considered and discussed in the current work of\ntransfer learning. In this paper, we propose a novel relationship-aware\nadversarial domain adaptation (RADA) algorithm, which first utilizes a single\nmulti-class domain discriminator to enforce the learning of inter-class\ndependency structure during domain-adversarial training and then aligns this\nstructure with the inter-class dependencies that are characterized from\ntraining the label predictor on source domain. Specifically, we impose a\nregularization term to penalize the structure discrepancy between the\ninter-class dependencies respectively estimated from domain discriminator and\nlabel predictor. Through this alignment, our proposed method makes the\nadversarial domain adaptation aware of the class relationships. Empirical\nstudies show that the incorporation of class relationships significantly\nimproves the performance on benchmark datasets.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11931v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11924v1",
    "title": "Paper Matching with Local Fairness Constraints",
    "authors": [
      "Ari Kobren",
      "Barna Saha",
      "Andrew McCallum"
    ],
    "author_ids": [],
    "abstract": "Automatically matching reviewers to papers is a crucial step of the peer\nreview process for venues receiving thousands of submissions. Unfortunately,\ncommon paper matching algorithms often construct matchings suffering from two\ncritical problems: (1) the group of reviewers assigned to a paper do not\ncollectively possess sufficient expertise, and (2) reviewer workloads are\nhighly skewed. In this paper, we propose a novel local fairness formulation of\npaper matching that directly addresses both of these issues. Since optimizing\nour formulation is not always tractable, we introduce two new algorithms,\nFairIR and FairFlow, for computing fair matchings that approximately optimize\nthe new formulation. FairIR solves a relaxation of the local fairness\nformulation and then employs a rounding technique to construct a valid matching\nthat provably maximizes the objective and only compromises on fairness with\nrespect to reviewer loads and papers by a small constant. In contrast, FairFlow\nis not provably guaranteed to produce fair matchings, however it can be 2x as\nefficient as FairIR and an order of magnitude faster than matching algorithms\nthat directly optimize for fairness. Empirically, we demonstrate that both\nFairIR and FairFlow improve fairness over standard matching algorithms on real\nconference data. Moreover, in comparison to state-of-the-art matching\nalgorithms that optimize for fairness only, FairIR achieves higher objective\nscores, FairFlow achieves competitive fairness, and both are capable of more\nevenly allocating reviewers.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11924v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.11742v3",
    "title": "Overlearning Reveals Sensitive Attributes",
    "authors": [
      "Congzheng Song",
      "Vitaly Shmatikov"
    ],
    "author_ids": [],
    "abstract": "\"Overlearning\" means that a model trained for a seemingly simple objective\nimplicitly learns to recognize attributes and concepts that are (1) not part of\nthe learning objective, and (2) sensitive from a privacy or bias perspective.\nFor example, a binary gender classifier of facial images also learns to\nrecognize races\\textemdash even races that are not represented in the training\ndata\\textemdash and identities.\n  We demonstrate overlearning in several vision and NLP models and analyze its\nharmful consequences. First, inference-time representations of an overlearned\nmodel reveal sensitive attributes of the input, breaking privacy protections\nsuch as model partitioning. Second, an overlearned model can be \"re-purposed\"\nfor a different, privacy-violating task even in the absence of the original\ntraining data.\n  We show that overlearning is intrinsic for some tasks and cannot be prevented\nby censoring unwanted attributes. Finally, we investigate where, when, and why\noverlearning happens during model training.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11742v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11684v1",
    "title": "On Measuring Gender Bias in Translation of Gender-neutral Pronouns",
    "authors": [
      "Won Ik Cho",
      "Ji Won Kim",
      "Seok Min Kim",
      "Nam Soo Kim"
    ],
    "author_ids": [],
    "abstract": "Ethics regarding social bias has recently thrown striking issues in natural\nlanguage processing. Especially for gender-related topics, the need for a\nsystem that reduces the model bias has grown in areas such as image captioning,\ncontent recommendation, and automated employment. However, detection and\nevaluation of gender bias in the machine translation systems are not yet\nthoroughly investigated, for the task being cross-lingual and challenging to\ndefine. In this paper, we propose a scheme for making up a test set that\nevaluates the gender bias in a machine translation system, with Korean, a\nlanguage with gender-neutral pronouns. Three word/phrase sets are primarily\nconstructed, each incorporating positive/negative expressions or occupations;\nall the terms are gender-independent or at least not biased to one side\nseverely. Then, additional sentence lists are constructed concerning formality\nof the pronouns and politeness of the sentences. With the generated sentence\nset of size 4,236 in total, we evaluate gender bias in conventional machine\ntranslation systems utilizing the proposed measure, which is termed here as\ntranslation gender bias index (TGBI). The corpus and the code for evaluation is\navailable on-line.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11684v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11672v4",
    "title": "Invertible generative models for inverse problems: mitigating representation error and dataset bias",
    "authors": [
      "Muhammad Asim",
      "Max Daniels",
      "Oscar Leong",
      "Ali Ahmed",
      "Paul Hand"
    ],
    "author_ids": [],
    "abstract": "Trained generative models have shown remarkable performance as priors for\ninverse problems in imaging -- for example, Generative Adversarial Network\npriors permit recovery of test images from 5-10x fewer measurements than\nsparsity priors. Unfortunately, these models may be unable to represent any\nparticular image because of architectural choices, mode collapse, and bias in\nthe training dataset. In this paper, we demonstrate that invertible neural\nnetworks, which have zero representation error by design, can be effective\nnatural signal priors at inverse problems such as denoising, compressive\nsensing, and inpainting. Given a trained generative model, we study the\nempirical risk formulation of the desired inverse problem under a\nregularization that promotes high likelihood images, either directly by\npenalization or algorithmically by initialization. For compressive sensing,\ninvertible priors can yield higher accuracy than sparsity priors across almost\nall undersampling ratios, and due to their lack of representation error,\ninvertible priors can yield better reconstructions than GAN priors for images\nthat have rare features of variation within the biased training set, including\nout-of-distribution natural images. We additionally compare performance for\ncompressive sensing to unlearned methods, such as the deep decoder, and we\nestablish theoretical bounds on expected recovery error in the case of a linear\ninvertible model.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11672v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11985v6",
    "title": "Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals underreported bias types",
    "authors": [
      "David Rozado"
    ],
    "author_ids": [],
    "abstract": "This work describes a large-scale analysis of sentiment associations in\npopular word embedding models along the lines of gender and ethnicity but also\nalong the less frequently studied dimensions of socioeconomic status, age,\nsexual orientation, religious sentiment and political leanings. Consistent with\nprevious scholarly literature, this work has found systemic bias against given\nnames popular among African-Americans in most embedding models examined. Gender\nbias in embedding models however appears to be multifaceted and often reversed\nin polarity to what has been regularly reported. Interestingly, using the\ncommon operationalization of the term bias in the fairness literature, novel\ntypes of so far unreported bias types in word embedding models have also been\nidentified. Specifically, the popular embedding models analyzed here display\nnegative biases against middle and working-class socioeconomic status, male\nchildren, senior citizens, plain physical appearance, Islamic religious faith,\nnon-religiosity and conservative political orientation. Reasons for the\nparadoxical underreporting of these bias types in the relevant literature are\nprobably manifold but widely held blind spots when searching for algorithmic\nbias and a lack of widespread technical jargon to unambiguously describe a\nvariety of algorithmic associations could conceivably be playing a role. The\ncausal origins for the multiplicity of loaded associations attached to distinct\ndemographic groups within embedding models are often unclear but the\nheterogeneity of said associations and their potential multifactorial roots\nraises doubts about the validity of grouping them all under the umbrella term\nbias. Richer and more fine-grained terminology as well as a more comprehensive\nexploration of the bias landscape could help the fairness epistemic community\nto characterize and neutralize algorithmic discrimination more efficiently.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11985v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11612v2",
    "title": "Average Bias and Polynomial Sources",
    "authors": [
      "Arnab Bhattacharyya",
      "Philips George John",
      "Suprovat Ghoshal",
      "Raghu Meka"
    ],
    "author_ids": [],
    "abstract": "We identify a new notion of pseudorandomness for randomness sources, which we\ncall the average bias. Given a distribution $Z$ over $\\{0,1\\}^n$, its average\nbias is: $b_{\\text{av}}(Z) =2^{-n} \\sum_{c \\in \\{0,1\\}^n} |\\mathbb{E}_{z \\sim\nZ}(-1)^{\\langle c, z\\rangle}|$. A source with average bias at most $2^{-k}$ has\nmin-entropy at least $k$, and so low average bias is a stronger condition than\nhigh min-entropy. We observe that the inner product function is an extractor\nfor any source with average bias less than $2^{-n/2}$.\n  The notion of average bias especially makes sense for polynomial sources,\ni.e., distributions sampled by low-degree $n$-variate polynomials over\n$\\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see\nthat min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show\nthat for quadratic sources, min-entropy $k$ implies that the average bias is at\nmost $2^{-\\Omega(\\sqrt{k})}$. We use this relation to design dispersers for\nseparable quadratic sources with a min-entropy guarantee.",
    "published_date": "2019-05-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CC",
      "cs.DM",
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11612v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.11519v1",
    "title": "Open Platforms for Artificial Intelligence for Social Good: Common Patterns as a Pathway to True Impact",
    "authors": [
      "Kush R. Varshney",
      "Aleksandra Mojsilovic"
    ],
    "author_ids": [],
    "abstract": "The AI for social good movement has now reached a state in which a large\nnumber of one-off demonstrations have illustrated that partnerships of AI\npractitioners and social change organizations are possible and can address\nproblems faced in sustainable development. In this paper, we discuss how moving\nfrom demonstrations to true impact on humanity will require a different course\nof action, namely open platforms containing foundational AI capabilities to\nsupport common needs of multiple organizations working in similar topical\nareas. We lend credence to this proposal by describing three example patterns\nof social good problems and their AI-based solutions: natural language\nprocessing for making sense of international development reports, causal\ninference for providing guidance to vulnerable individuals, and\ndiscrimination-aware classification for supporting unbiased allocation\ndecisions. We argue that the development of such platforms will be possible\nthrough convenings of social change organizations, AI companies, and\ngrantmaking foundations.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11519v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.12425v2",
    "title": "Near-optimal Optimistic Reinforcement Learning using Empirical Bernstein Inequalities",
    "authors": [
      "Aristide Tossou",
      "Debabrota Basu",
      "Christos Dimitrakakis"
    ],
    "author_ids": [],
    "abstract": "We study model-based reinforcement learning in an unknown finite\ncommunicating Markov decision process. We propose a simple algorithm that\nleverages a variance based confidence interval. We show that the proposed\nalgorithm, UCRL-V, achieves the optimal regret\n$\\tilde{\\mathcal{O}}(\\sqrt{DSAT})$ up to logarithmic factors, and so our work\ncloses a gap with the lower bound without additional assumptions on the MDP. We\nperform experiments in a variety of environments that validates the theoretical\nbounds as well as prove UCRL-V to be better than the state-of-the-art\nalgorithms.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12425v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.13134v2",
    "title": "FairSearch: A Tool For Fairness in Ranked Search Results",
    "authors": [
      "Meike Zehlike",
      "Tom Sühr",
      "Carlos Castillo",
      "Ivan Kitanovski"
    ],
    "author_ids": [],
    "abstract": "Ranked search results and recommendations have become the main mechanism by\nwhich we find content, products, places, and people online. With hiring,\nselecting, purchasing, and dating being increasingly mediated by algorithms,\nrankings may determine career and business opportunities, educational\nplacement, access to benefits, and even social and reproductive success. It is\ntherefore of societal and ethical importance to ask whether search results can\ndemote, marginalize, or exclude individuals of unprivileged groups or promote\nproducts with undesired features. In this paper we present FairSearch, the\nfirst fair open source search API to provide fairness notions in ranked search\nresults. We implement two algorithms from the fair ranking literature, namely\nFA*IR (Zehlike et al., 2017) and DELTR (Zehlike and Castillo, 2018) and provide\nthem as stand-alone libraries in Python and Java. Additionally we implement\ninterfaces to Elasticsearch for both algorithms, that use the aforementioned\nJava libraries and are then provided as Elasticsearch plugins. Elasticsearch is\na well-known search engine API based on Apache Lucene. With our plugins we\nenable search engine developers who wish to ensure fair search results of\ndifferent styles to easily integrate DELTR and FA*IR into their existing\nElasticsearch environment.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.13134v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.11474v2",
    "title": "Infusing domain knowledge in AI-based \"black box\" models for better explainability with application in bankruptcy prediction",
    "authors": [
      "Sheikh Rabiul Islam",
      "William Eberle",
      "Sid Bundy",
      "Sheikh Khaled Ghafoor"
    ],
    "author_ids": [],
    "abstract": "Although \"black box\" models such as Artificial Neural Networks, Support\nVector Machines, and Ensemble Approaches continue to show superior performance\nin many disciplines, their adoption in the sensitive disciplines (e.g.,\nfinance, healthcare) is questionable due to the lack of interpretability and\nexplainability of the model. In fact, future adoption of \"black box\" models is\ndifficult because of the recent rule of \"right of explanation\" by the European\nUnion where a user can ask for an explanation behind an algorithmic decision,\nand the newly proposed bill by the US government, the \"Algorithmic\nAccountability Act\", which would require companies to assess their machine\nlearning systems for bias and discrimination and take corrective measures. Top\nBankruptcy Prediction Models are A.I.-based and are in need of better\nexplainability -the extent to which the internal working mechanisms of an AI\nsystem can be explained in human terms. Although explainable artificial\nintelligence is an emerging field of research, infusing domain knowledge for\nbetter explainability might be a possible solution. In this work, we\ndemonstrate a way to collect and infuse domain knowledge into a \"black box\"\nmodel for bankruptcy prediction. Our understanding from the experiments reveals\nthat infused domain knowledge makes the output from the black box model more\ninterpretable and explainable.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11474v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11361v1",
    "title": "Efficient candidate screening under multiple tests and implications for fairness",
    "authors": [
      "Lee Cohen",
      "Zachary C. Lipton",
      "Yishay Mansour"
    ],
    "author_ids": [],
    "abstract": "When recruiting job candidates, employers rarely observe their underlying\nskill level directly. Instead, they must administer a series of interviews\nand/or collate other noisy signals in order to estimate the worker's skill.\nTraditional economics papers address screening models where employers access\nworker skill via a single noisy signal. In this paper, we extend this\ntheoretical analysis to a multi-test setting, considering both Bernoulli and\nGaussian models. We analyze the optimal employer policy both when the employer\nsets a fixed number of tests per candidate and when the employer can set a\ndynamic policy, assigning further tests adaptively based on results from the\nprevious tests. To start, we characterize the optimal policy when employees\nconstitute a single group, demonstrating some interesting trade-offs.\nSubsequently, we address the multi-group setting, demonstrating that when the\nnoise levels vary across groups, a fundamental impossibility emerges whereby we\ncannot administer the same number of tests, subject candidates to the same\ndecision rule, and yet realize the same outcomes in both groups.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11361v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11260v3",
    "title": "Achieving Fairness in Stochastic Multi-armed Bandit Problem",
    "authors": [
      "Vishakha Patil",
      "Ganesh Ghalme",
      "Vineet Nair",
      "Y. Narahari"
    ],
    "author_ids": [],
    "abstract": "We study an interesting variant of the stochastic multi-armed bandit problem,\ncalled the Fair-SMAB problem, where each arm is required to be pulled for at\nleast a given fraction of the total available rounds. We investigate the\ninterplay between learning and fairness in terms of a pre-specified vector\ndenoting the fractions of guaranteed pulls. We define a fairness-aware regret,\ncalled r-Regret, that takes into account the above fairness constraints and\nnaturally extends the conventional notion of regret. Our primary contribution\nis characterizing a class of Fair-SMAB algorithms by two parameters: the\nunfairness tolerance and learning algorithm used as a black-box. We provide a\nfairness guarantee for this class that holds uniformly over time irrespective\nof the choice of the learning algorithm. In particular, when the learning\nalgorithm is UCB1, we show that our algorithm achieves O(log(T)) r-Regret.\nFinally, we evaluate the cost of fairness in terms of the conventional notion\nof regret.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11260v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.11219v1",
    "title": "Automated Ground Truth Estimation of Vulnerable Road Users in Automotive Radar Data Using GNSS",
    "authors": [
      "Nicolas Scheiner",
      "Nils Appenrodt",
      "Jürgen Dickmann",
      "Bernhard Sick"
    ],
    "author_ids": [],
    "abstract": "Annotating automotive radar data is a difficult task. This article presents\nan automated way of acquiring data labels which uses a highly accurate and\nportable global navigation satellite system (GNSS). The proposed system is\ndiscussed besides a revision of other label acquisitions techniques and a\nproblem description of manual data annotation. The article concludes with a\nsystematic comparison of conventional hand labeling and automatic data\nacquisition. The results show clear advantages of the proposed method without a\nrelevant loss in labeling accuracy. Minor changes can be observed in the\nmeasured radar data, but the so introduced bias of the GNSS reference is\nclearly outweighed by the indisputable time savings. Beside data annotation,\nthe proposed system can also provide a ground truth for validating object\ntracking or other automated driving system applications.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.11219v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10985v2",
    "title": "AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",
    "authors": [
      "Jeff Clune"
    ],
    "author_ids": [],
    "abstract": "Perhaps the most ambitious scientific quest in human history is the creation\nof general artificial intelligence, which roughly means AI that is as smart or\nsmarter than humans. The dominant approach in the machine learning community is\nto attempt to discover each of the pieces required for intelligence, with the\nimplicit assumption that some future group will complete the Herculean task of\nfiguring out how to combine all of those pieces into a complex thinking\nmachine. I call this the \"manual AI approach\". This paper describes another\nexciting path that ultimately may be more successful at producing general AI.\nIt is based on the clear trend in machine learning that hand-designed solutions\neventually are replaced by more effective, learned solutions. The idea is to\ncreate an AI-generating algorithm (AI-GA), which automatically learns how to\nproduce general AI. Three Pillars are essential for the approach: (1)\nmeta-learning architectures, (2) meta-learning the learning algorithms\nthemselves, and (3) generating effective learning environments. I argue that\neither approach could produce general AI first, and both are scientifically\nworthwhile irrespective of which is the fastest path. Because both are\npromising, yet the ML community is currently committed to the manual approach,\nI argue that our community should increase its research investment in the AI-GA\napproach. To encourage such research, I describe promising work in each of the\nThree Pillars. I also discuss AI-GA-specific safety and ethical considerations.\nBecause it it may be the fastest path to general AI and because it is\ninherently scientifically interesting to understand the conditions in which a\nsimple algorithm can produce general AI (as happened on Earth where Darwinian\nevolution produced human intelligence), I argue that the pursuit of AI-GAs\nshould be considered a new grand challenge of computer science research.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10985v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10957v3",
    "title": "Enhancing Item Response Theory for Cognitive Diagnosis",
    "authors": [
      "Song Cheng",
      "Qi Liu"
    ],
    "author_ids": [],
    "abstract": "Cognitive diagnosis is a fundamental and crucial task in many educational\napplications, e.g., computer adaptive test and cognitive assignments. Item\nResponse Theory (IRT) is a classical cognitive diagnosis method which can\nprovide interpretable parameters (i.e., student latent trait, question\ndiscrimination, and difficulty) for analyzing student performance. However,\ntraditional IRT ignores the rich information in question texts, cannot diagnose\nknowledge concept proficiency, and it is inaccurate to diagnose the parameters\nfor the questions which only appear several times. To this end, in this paper,\nwe propose a general Deep Item Response Theory (DIRT) framework to enhance\ntraditional IRT for cognitive diagnosis by exploiting semantic representation\nfrom question texts with deep learning. In DIRT, we first use a proficiency\nvector to represent students' proficiency in knowledge concepts and embed\nquestion texts and knowledge concepts to dense vectors by Word2Vec. Then, we\ndesign a deep diagnosis module to diagnose parameters in traditional IRT by\ndeep learning techniques. Finally, with the diagnosed parameters, we input them\ninto the logistic-like formula of IRT to predict student performance. Extensive\nexperimental results on real-world data clearly demonstrate the effectiveness\nand interpretation power of DIRT framework.",
    "published_date": "2019-05-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10957v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10881v3",
    "title": "Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection",
    "authors": [
      "Pan Li",
      "Eli Chien",
      "Olgica Milenkovic"
    ],
    "author_ids": [],
    "abstract": "Landing probabilities (LP) of random walks (RW) over graphs encode rich\ninformation regarding graph topology. Generalized PageRanks (GPR), which\nrepresent weighted sums of LPs of RWs, utilize the discriminative power of LP\nfeatures to enable many graph-based learning studies. Previous work in the area\nhas mostly focused on evaluating suitable weights for GPRs, and only a few\nstudies so far have attempted to derive the optimal weights of GRPs for a given\napplication. We take a fundamental step forward in this direction by using\nrandom graph models to better our understanding of the behavior of GPRs. In\nthis context, we provide a rigorous non-asymptotic analysis for the convergence\nof LPs and GPRs to their mean-field values on edge-independent random graphs.\nAlthough our theoretical results apply to many problem settings, we focus on\nthe task of seed-expansion community detection over stochastic block models.\nThere, we find that the predictive power of LPs decreases significantly slower\nthan previously reported based on asymptotic findings. Given this result, we\npropose a new GPR, termed Inverse PR (IPR), with LP weights that increase for\nthe initial few steps of the walks. Extensive experiments on both synthetic and\nreal, large-scale networks illustrate the superiority of IPR compared to other\nGPRs for seeded community detection.",
    "published_date": "2019-05-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10881v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10870v2",
    "title": "Equal Opportunity and Affirmative Action via Counterfactual Predictions",
    "authors": [
      "Yixin Wang",
      "Dhanya Sridhar",
      "David M. Blei"
    ],
    "author_ids": [],
    "abstract": "Machine learning (ML) can automate decision-making by learning to predict\ndecisions from historical data. However, these predictors may inherit\ndiscriminatory policies from past decisions and reproduce unfair decisions. In\nthis paper, we propose two algorithms that adjust fitted ML predictors to make\nthem fair. We focus on two legal notions of fairness: (a) providing equal\nopportunity (EO) to individuals regardless of sensitive attributes and (b)\nrepairing historical disadvantages through affirmative action (AA). More\ntechnically, we produce fair EO and AA predictors by positing a causal model\nand considering counterfactual decisions. We prove that the resulting\npredictors are theoretically optimal in predictive performance while satisfying\nfairness. We evaluate the algorithms, and the trade-offs between accuracy and\nfairness, on datasets about admissions, income, credit and recidivism.",
    "published_date": "2019-05-26T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10870v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10713v3",
    "title": "Field-aware Calibration: A Simple and Empirically Strong Method for Reliable Probabilistic Predictions",
    "authors": [
      "Feiyang Pan",
      "Xiang Ao",
      "Pingzhong Tang",
      "Min Lu",
      "Dapeng Liu",
      "Lei Xiao",
      "Qing He"
    ],
    "author_ids": [],
    "abstract": "It is often observed that the probabilistic predictions given by a machine\nlearning model can disagree with averaged actual outcomes on specific subsets\nof data, which is also known as the issue of miscalibration. It is responsible\nfor the unreliability of practical machine learning systems. For example, in\nonline advertising, an ad can receive a click-through rate prediction of 0.1\nover some population of users where its actual click rate is 0.15. In such\ncases, the probabilistic predictions have to be fixed before the system can be\ndeployed.\n  In this paper, we first introduce a new evaluation metric named field-level\ncalibration error that measures the bias in predictions over the sensitive\ninput field that the decision-maker concerns. We show that existing post-hoc\ncalibration methods have limited improvements in the new field-level metric and\nother non-calibration metrics such as the AUC score. To this end, we propose\nNeural Calibration, a simple yet powerful post-hoc calibration method that\nlearns to calibrate by making full use of the field-aware information over the\nvalidation set. We present extensive experiments on five large-scale datasets.\nThe results showed that Neural Calibration significantly improves against\nuncalibrated predictions in common metrics such as the negative log-likelihood,\nBrier score and AUC, as well as the proposed field-level calibration error.",
    "published_date": "2019-05-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10713v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10674v4",
    "title": "Compositional Fairness Constraints for Graph Embeddings",
    "authors": [
      "Avishek Joey Bose",
      "William L. Hamilton"
    ],
    "author_ids": [],
    "abstract": "Learning high-quality node embeddings is a key building block for machine\nlearning models that operate on graph data, such as social networks and\nrecommender systems. However, existing graph embedding techniques are unable to\ncope with fairness constraints, e.g., ensuring that the learned representations\ndo not correlate with certain attributes, such as age or gender. Here, we\nintroduce an adversarial framework to enforce fairness constraints on graph\nembeddings. Our approach is compositional---meaning that it can flexibly\naccommodate different combinations of fairness constraints during inference.\nFor instance, in the context of social recommendations, our framework would\nallow one user to request that their recommendations are invariant to both\ntheir age and gender, while also allowing another user to request invariance to\njust their age. Experiments on standard knowledge graph and recommender system\nbenchmarks highlight the utility of our proposed framework.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10674v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10660v2",
    "title": "An Algorithmic Framework for Fairness Elicitation",
    "authors": [
      "Christopher Jung",
      "Michael Kearns",
      "Seth Neel",
      "Aaron Roth",
      "Logan Stapleton",
      "Zhiwei Steven Wu"
    ],
    "author_ids": [],
    "abstract": "We consider settings in which the right notion of fairness is not captured by\nsimple mathematical definitions (such as equality of error rates across\ngroups), but might be more complex and nuanced and thus require elicitation\nfrom individual or collective stakeholders. We introduce a framework in which\npairs of individuals can be identified as requiring (approximately) equal\ntreatment under a learned model, or requiring ordered treatment such as\n\"applicant Alice should be at least as likely to receive a loan as applicant\nBob\". We provide a provably convergent and oracle efficient algorithm for\nlearning the most accurate model subject to the elicited fairness constraints,\nand prove generalization bounds for both accuracy and fairness. This algorithm\ncan also combine the elicited constraints with traditional statistical fairness\nnotions, thus \"correcting\" or modifying the latter by the former. We report\npreliminary findings of a behavioral study of our framework using human-subject\nfairness constraints elicited on the COMPAS criminal recidivism dataset.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10660v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10656v1",
    "title": "Equitable Allocations of Indivisible Goods",
    "authors": [
      "Rupert Freeman",
      "Sujoy Sikdar",
      "Rohit Vaish",
      "Lirong Xia"
    ],
    "author_ids": [],
    "abstract": "In fair division, equitability dictates that each participant receives the\nsame level of utility. In this work, we study equitable allocations of\nindivisible goods among agents with additive valuations. While prior work has\nstudied (approximate) equitability in isolation, we consider equitability in\nconjunction with other well-studied notions of fairness and economic\nefficiency. We show that the Leximin algorithm produces an allocation that\nsatisfies equitability up to any good and Pareto optimality. We also give a\nnovel algorithm that guarantees Pareto optimality and equitability up to one\ngood in pseudopolynomial time. Our experiments on real-world preference data\nreveal that approximate envy-freeness, approximate equitability, and Pareto\noptimality can often be achieved simultaneously.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10656v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.10617v10",
    "title": "Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?",
    "authors": [
      "Tianxing He",
      "Jingzhao Zhang",
      "Zhiming Zhou",
      "James Glass"
    ],
    "author_ids": [],
    "abstract": "Exposure bias has been regarded as a central problem for auto-regressive\nlanguage models (LM). It claims that teacher forcing would cause the test-time\ngeneration to be incrementally distorted due to the training-generation\ndiscrepancy. Although a lot of algorithms have been proposed to avoid teacher\nforcing and therefore alleviate exposure bias, there is little work showing how\nserious the exposure bias problem actually is. In this work, we focus on the\ntask of open-ended language generation, propose metrics to quantify the impact\nof exposure bias in the aspects of quality, diversity, and consistency. Our key\nintuition is that if we feed ground-truth data prefixes (instead of prefixes\ngenerated by the model itself) into the model and ask it to continue the\ngeneration, the performance should become much better because the\ntraining-generation discrepancy in the prefix is removed. Both automatic and\nhuman evaluations are conducted in our experiments. On the contrary to the\npopular belief in exposure bias, we find that the the distortion induced by the\nprefix discrepancy is limited, and does not seem to be incremental during the\ngeneration. Moreover, our analysis reveals an interesting self-recovery ability\nof the LM, which we hypothesize to be countering the harmful effects from\nexposure bias.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10617v10",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10607v2",
    "title": "Average Individual Fairness: Algorithms, Generalization and Experiments",
    "authors": [
      "Michael Kearns",
      "Aaron Roth",
      "Saeed Sharifi-Malvajerdi"
    ],
    "author_ids": [],
    "abstract": "We propose a new family of fairness definitions for classification problems\nthat combine some of the best properties of both statistical and individual\nnotions of fairness. We posit not only a distribution over individuals, but\nalso a distribution over (or collection of) classification tasks. We then ask\nthat standard statistics (such as error or false positive/negative rates) be\n(approximately) equalized across individuals, where the rate is defined as an\nexpectation over the classification tasks. Because we are no longer averaging\nover coarse groups (such as race or gender), this is a semantically meaningful\nindividual-level constraint. Given a sample of individuals and classification\nproblems, we design an oracle-efficient algorithm (i.e. one that is given\naccess to any standard, fairness-free learning heuristic) for the fair\nempirical risk minimization task. We also show that given sufficiently many\nsamples, the ERM solution generalizes in two directions: both to new\nindividuals, and to new classification tasks, drawn from their corresponding\ndistributions. Finally we implement our algorithm and empirically verify its\neffectiveness.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10607v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10572v1",
    "title": "Joint Label Prediction based Semi-Supervised Adaptive Concept Factorization for Robust Data Representation",
    "authors": [
      "Zhao Zhang",
      "Yan Zhang",
      "Guangcan Liu",
      "Jinhui Tang",
      "Shuicheng Yan",
      "Meng Wang"
    ],
    "author_ids": [],
    "abstract": "Constrained Concept Factorization (CCF) yields the enhanced representation\nability over CF by incorporating label information as additional constraints,\nbut it cannot classify and group unlabeled data appropriately. Minimizing the\ndifference between the original data and its reconstruction directly can enable\nCCF to model a small noisy perturbation, but is not robust to gross sparse\nerrors. Besides, CCF cannot preserve the manifold structures in new\nrepresentation space explicitly, especially in an adaptive manner. In this\npaper, we propose a joint label prediction based Robust Semi-Supervised\nAdaptive Concept Factorization (RS2ACF) framework. To obtain robust\nrepresentation, RS2ACF relaxes the factorization to make it simultaneously\nstable to small entrywise noise and robust to sparse errors. To enrich prior\nknowledge to enhance the discrimination, RS2ACF clearly uses class information\nof labeled data and more importantly propagates it to unlabeled data by jointly\nlearning an explicit label indicator for unlabeled data. By the label\nindicator, RS2ACF can ensure the unlabeled data of the same predicted label to\nbe mapped into the same class in feature space. Besides, RS2ACF incorporates\nthe joint neighborhood reconstruction error over the new representations and\npredicted labels of both labeled and unlabeled data, so the manifold structures\ncan be preserved explicitly and adaptively in the representation space and\nlabel space at the same time. Owing to the adaptive manner, the tricky process\nof determining the neighborhood size or kernel width can be avoided. Extensive\nresults on public databases verify that our RS2ACF can deliver state-of-the-art\ndata representation, compared with other related methods.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10572v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10546v3",
    "title": "Protecting the Protected Group: Circumventing Harmful Fairness",
    "authors": [
      "Omer Ben-Porat",
      "Fedor Sandomirskiy",
      "Moshe Tennenholtz"
    ],
    "author_ids": [],
    "abstract": "Machine Learning (ML) algorithms shape our lives. Banks use them to determine\nif we are good borrowers; IT companies delegate them recruitment decisions;\npolice apply ML for crime-prediction, and judges base their verdicts on ML.\nHowever, real-world examples show that such automated decisions tend to\ndiscriminate against protected groups. This potential discrimination generated\na huge hype both in media and in the research community. Quite a few formal\nnotions of fairness were proposed, which take a form of constraints a \"fair\"\nalgorithm must satisfy. We focus on scenarios where fairness is imposed on a\nself-interested party (e.g., a bank that maximizes its revenue). We find that\nthe disadvantaged protected group can be worse off after imposing a fairness\nconstraint. We introduce a family of \\textit{Welfare-Equalizing} fairness\nconstraints that equalize per-capita welfare of protected groups, and include\n\\textit{Demographic Parity} and \\textit{Equal Opportunity} as particular cases.\nIn this family, we characterize conditions under which the fairness constraint\nhelps the disadvantaged group. We also characterize the structure of the\noptimal \\textit{Welfare-Equalizing} classifier for the self-interested party,\nand provide an algorithm to compute it. Overall, our\n\\textit{Welfare-Equalizing} fairness approach provides a unified framework for\ndiscussing fairness in classification in the presence of a self-interested\nparty.",
    "published_date": "2019-05-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10546v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10360v1",
    "title": "The advantages of multiple classes for reducing overfitting from test set reuse",
    "authors": [
      "Vitaly Feldman",
      "Roy Frostig",
      "Moritz Hardt"
    ],
    "author_ids": [],
    "abstract": "Excessive reuse of holdout data can lead to overfitting. However, there is\nlittle concrete evidence of significant overfitting due to holdout reuse in\npopular multiclass benchmarks today. Known results show that, in the\nworst-case, revealing the accuracy of $k$ adaptively chosen classifiers on a\ndata set of size $n$ allows to create a classifier with bias of\n$\\Theta(\\sqrt{k/n})$ for any binary prediction problem. We show a new upper\nbound of $\\tilde O(\\max\\{\\sqrt{k\\log(n)/(mn)},k/n\\})$ on the worst-case bias\nthat any attack can achieve in a prediction problem with $m$ classes. Moreover,\nwe present an efficient attack that achieve a bias of $\\Omega(\\sqrt{k/(m^2\nn)})$ and improves on previous work for the binary setting ($m=2$). We also\npresent an inefficient attack that achieves a bias of $\\tilde\\Omega(k/n)$.\nComplementing our theoretical work, we give new practical attacks to\nstress-test multiclass benchmarks by aiming to create as large a bias as\npossible with a given number of queries. Our experiments show that the\nadditional uncertainty of prediction with a large number of classes indeed\nmitigates the effect of our best attacks.\n  Our work extends developments in understanding overfitting due to adaptive\ndata analysis to multiclass prediction problems. It also bears out the\nsurprising fact that multiclass prediction problems are significantly more\nrobust to overfitting when reusing a test (or holdout) dataset. This offers an\nexplanation as to why popular multiclass prediction benchmarks, such as\nImageNet, may enjoy a longer lifespan than what intuition from literature on\nbinary classification suggests.",
    "published_date": "2019-05-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10360v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10264v1",
    "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks",
    "authors": [
      "Yaoyu Zhang",
      "Zhi-Qin John Xu",
      "Tao Luo",
      "Zheng Ma"
    ],
    "author_ids": [],
    "abstract": "It remains a puzzle that why deep neural networks (DNNs), with more\nparameters than samples, often generalize well. An attempt of understanding\nthis puzzle is to discover implicit biases underlying the training process of\nDNNs, such as the Frequency Principle (F-Principle), i.e., DNNs often fit\ntarget functions from low to high frequencies. Inspired by the F-Principle, we\npropose an effective model of linear F-Principle (LFP) dynamics which\naccurately predicts the learning results of two-layer ReLU neural networks\n(NNs) of large widths. This LFP dynamics is rationalized by a linearized mean\nfield residual dynamics of NNs. Importantly, the long-time limit solution of\nthis LFP dynamics is equivalent to the solution of a constrained optimization\nproblem explicitly minimizing an FP-norm, in which higher frequencies of\nfeasible solutions are more heavily penalized. Using this optimization\nformulation, an a priori estimate of the generalization error bound is\nprovided, revealing that a higher FP-norm of the target function increases the\ngeneralization error. Overall, by explicitizing the implicit bias of the\nF-Principle as an explicit penalty for two-layer NNs, our work makes a step\ntowards a quantitative understanding of the learning and generalization of\ngeneral DNNs.",
    "published_date": "2019-05-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML",
      "68Q32, 68T01",
      "I.2.6"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10264v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.09972v1",
    "title": "Generative Adversarial Networks for Mitigating Biases in Machine Learning Systems",
    "authors": [
      "Adel Abusitta",
      "Esma Aïmeur",
      "Omar Abdel Wahab"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a new framework for mitigating biases in machine\nlearning systems. The problem of the existing mitigation approaches is that\nthey are model-oriented in the sense that they focus on tuning the training\nalgorithms to produce fair results, while overlooking the fact that the\ntraining data can itself be the main reason for biased outcomes. Technically\nspeaking, two essential limitations can be found in such model-based\napproaches: 1) the mitigation cannot be achieved without degrading the accuracy\nof the machine learning models, and 2) when the data used for training are\nlargely biased, the training time automatically increases so as to find\nsuitable learning parameters that help produce fair results. To address these\nshortcomings, we propose in this work a new framework that can largely mitigate\nthe biases and discriminations in machine learning systems while at the same\ntime enhancing the prediction accuracy of these systems. The proposed framework\nis based on conditional Generative Adversarial Networks (cGANs), which are used\nto generate new synthetic fair data with selective properties from the original\ndata. We also propose a framework for analyzing data biases, which is important\nfor understanding the amount and type of data that need to be synthetically\nsampled and labeled for each population group. Experimental results show that\nthe proposed solution can efficiently mitigate different types of biases, while\nat the same time enhancing the prediction accuracy of the underlying machine\nlearning model.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09972v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.09947v2",
    "title": "Affirmative Action Policies for Top-k Candidates Selection, With an Application to the Design of Policies for University Admissions",
    "authors": [
      "Michael Mathioudakis",
      "Carlos Castillo",
      "Giorgio Barnabo",
      "Sergio Celis"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of designing affirmative action policies for\nselecting the top-k candidates from a pool of applicants. We assume that for\neach candidate we have socio-demographic attributes and a series of variables\nthat serve as indicators of future performance (e.g., results on standardized\ntests). We further assume that we have access to historical data including the\nactual performance of previously selected candidates. Critically, performance\ninformation is only available for candidates who were selected under some\nprevious selection policy.\n  In this work we assume that due to legal requirements or voluntary\ncommitments, an organization wants to increase the presence of people from\ndisadvantaged socio-demographic groups among the selected candidates. Hence, we\nseek to design an affirmative action or positive action policy. This policy has\ntwo concurrent objectives: (i) to select candidates who, given what can be\nlearnt from historical data, are more likely to perform well, and (ii) to\nselect candidates in a way that increases the representation of disadvantaged\nsocio-demographic groups.\n  Our motivating application is the design of university admission policies to\nbachelor's degrees. We use a causal model as a framework to describe several\nfamilies of policies (changing component weights, giving bonuses, and enacting\nquotas), and compare them both theoretically and through extensive\nexperimentation on a large real-world dataset containing thousands of\nuniversity applicants. Our paper is the first to place the problem of\naffirmative-action policy design within the framework of algorithmic fairness.\nOur empirical results indicate that simple policies could favor the admission\nof disadvantaged groups without significantly compromising on the quality of\naccepted candidates.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09947v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.09945v1",
    "title": "Multifaceted Privacy: How to Express Your Online Persona without Revealing Your Sensitive Attributes",
    "authors": [
      "Victor Zakhary",
      "Ishani Gupta",
      "Rey Tang",
      "Amr El Abbadi"
    ],
    "author_ids": [],
    "abstract": "Recent works in social network stream analysis show that a user's online\npersona attributes (e.g., gender, ethnicity, political interest, location,\netc.) can be accurately inferred from the topics the user writes about or\nengages with. Attribute and preference inferences have been widely used to\nserve personalized recommendations, directed ads, and to enhance the user\nexperience in social networks. However, revealing a user's sensitive attributes\ncould represent a privacy threat to some individuals. Microtargeting\n(e.g.,Cambridge Analytica scandal), surveillance, and discriminating ads are\nexamples of threats to user privacy caused by sensitive attribute inference. In\nthis paper, we propose Multifaceted privacy, a novel privacy model that aims to\nobfuscate a user's sensitive attributes while publicly preserving the user's\npublic persona. To achieve multifaceted privacy, we build Aegis, a prototype\nclient-centric social network stream processing system that helps preserve\nmultifaceted privacy, and thus allowing social network users to freely express\ntheir online personas without revealing their sensitive attributes of choice.\nAegis allows social network users to control which persona attributes should be\npublicly revealed and which ones should be kept private. For this, Aegis\ncontinuously suggests topics and hashtags to social network users to post in\norder to obfuscate their sensitive attributes and hence confuse content-based\nsensitive attribute inferences. The suggested topics are carefully chosen to\npreserve the user's publicly revealed persona attributes while hiding their\nprivate sensitive persona attributes. Our experiments show that adding as few\nas 0 to 4 obfuscation posts (depending on how revealing the original post is)\nsuccessfully hides the user specified sensitive attributes without changing the\nuser's public persona attributes.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09945v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.09866v2",
    "title": "Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor",
    "authors": [
      "Malvina Nissim",
      "Rik van Noord",
      "Rob van der Goot"
    ],
    "author_ids": [],
    "abstract": "Analogies such as \"man is to king as woman is to X\" are often used to\nillustrate the amazing power of word embeddings. Concurrently, they have also\nbeen used to expose how strongly human biases are encoded in vector spaces\nbuilt on natural language, like \"man is to computer programmer as woman is to\nhomemaker\". Recent work has shown that analogies are in fact not such a\ndiagnostic for bias, and other methods have been proven to be more apt to the\ntask. However, beside the intrinsic problems with the analogy task as a bias\ndetection tool, in this paper we show that a series of issues related to how\nanalogies have been implemented and used might have yielded a distorted picture\nof bias in word embeddings. Human biases are present in word embeddings and\nneed to be addressed. Analogies, though, are probably not the right tool to do\nso. Also, the way they have been most often used has exacerbated some possibly\nnon-existing biases and perhaps hid others. Because they are still widely\npopular, and some of them have become classics within and outside the NLP\ncommunity, we deem it important to provide a series of clarifications that\nshould put well-known, and potentially new cases into the right perspective.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09866v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.09735v1",
    "title": "Digital Normativity: A challenge for human subjectivization and free will",
    "authors": [
      "Éric Fourneret",
      "Blaise Yvert"
    ],
    "author_ids": [],
    "abstract": "Over the past decade, artificial intelligence has demonstrated its efficiency\nin many different applications and a huge number of algorithms have become\ncentral and ubiquitous in our life. Their growing interest is essentially based\non their capability to synthesize and process large amounts of data, and to\nhelp humans making decisions in a world of increasing complexity. Yet, the\neffectiveness of algorithms in bringing more and more relevant recommendations\nto humans may start to compete with human-alone decisions based on values other\nthan pure efficacy. Here, we examine this tension in light of the emergence of\nseveral forms of digital normativity, and analyze how this normative role of AI\nmay influence the ability of humans to remain subject of their life. The advent\nof AI technology imposes a need to achieve a balance between concrete material\nprogress and progress of the mind to avoid any form of servitude. It has become\nessential that an ethical reflection accompany the current developments of\nintelligent algorithms beyond the sole question of their social acceptability.\nSuch reflection should be anchored where AI technologies are being developed as\nwell as in educational programs where their implications can be explained.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09735v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.09826v1",
    "title": "A model of brain morphological changes related to aging and Alzheimer's disease from cross-sectional assessments",
    "authors": [
      "Raphaël Sivera",
      "Hervé Delingette",
      "Marco Lorenzi",
      "Xavier Pennec",
      "Nicholas Ayache"
    ],
    "author_ids": [],
    "abstract": "In this study we propose a deformation-based framework to jointly model the\ninfluence of aging and Alzheimer's disease (AD) on the brain morphological\nevolution. Our approach combines a spatio-temporal description of both\nprocesses into a generative model. A reference morphology is deformed along\nspecific trajectories to match subject specific morphologies. It is used to\ndefine two imaging progression markers: 1) a morphological age and 2) a disease\nscore. These markers can be computed locally in any brain region. The approach\nis evaluated on brain structural magnetic resonance images (MRI) from the ADNI\ndatabase. The generative model is first estimated on a control population,\nthen, for each subject, the markers are computed for each acquisition. The\nlongitudinal evolution of these markers is then studied in relation with the\nclinical diagnosis of the subjects and used to generate possible morphological\nevolution. In the model, the morphological changes associated with normal aging\nare mainly found around the ventricles, while the Alzheimer's disease specific\nchanges are more located in the temporal lobe and the hippocampal area. The\nstatistical analysis of these markers highlights differences between clinical\nconditions even though the inter-subject variability is quiet high. In this\ncontext, the model can be used to generate plausible morphological trajectories\nassociated with the disease. Our method gives two interpretable scalar imaging\nbiomarkers assessing the effects of aging and disease on brain morphology at\nthe individual and population level. These markers confirm an acceleration of\napparent aging for Alzheimer's subjects and can help discriminate clinical\nconditions even in prodromal stages. More generally, the joint modeling of\nnormal and pathological evolutions shows promising results to describe\nage-related brain diseases over long time scales.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.NC",
      "cs.CV",
      "eess.IV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09826v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.10171v1",
    "title": "Perceptions of Gender Diversity's impact on mood in software development teams",
    "authors": [
      "Kelly Blincoe",
      "Olga Springer",
      "Michal R. Wrobel"
    ],
    "author_ids": [],
    "abstract": "Recent studies show that gender diversity in IT teams has a positive impact\non the software development process. However, there is still a great gender\ninequality. The aim of our study was to examine how the working atmosphere\ndepends on the gender differentiation of IT teams. The analysis of the results\nof the interviews and questionnaires showed that the atmosphere in\ngender-differentiated teams is more pleasant compared to purely male ones. The\npaper also discusses the problem of gender discrimination, which, according to\nthe results of the study, unfortunately still exists and affects the working\natmosphere. Finally, we looked at ways to reduce the gender inequity, where it\nturned out that soft approaches such as dedicated training, workshops to show\nthe human face of the IT industry are preferred.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.10171v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.12471v1",
    "title": "Complexity Analysis of Approaching Clinical Psychiatry with Predictive Analytics and Neural Networks",
    "authors": [
      "Soaad Hossain"
    ],
    "author_ids": [],
    "abstract": "As the emerging field of predictive analytics in psychiatry generated and\ncontinues to generate massive interest overtime with its major promises to\npositively change and revolutionize clinical psychiatry, health care and\nmedical professionals are greatly looking forward to its integration and\napplication into psychiatry. However, by directly applying predictive analytics\nto the practice of psychiatry, this could cause detrimental damage to those\nthat use predictive analytics through creating or worsening existing medical\nissues. In both cases, medical ethics issues arise, and need to be addressed.\nThis paper will use literature to provide descriptions of selected stages in\nthe treatment of mental disorders and phases in a predictive analytics project,\napproach mental disorder diagnoses using predictive models that rely on neural\nnetworks, analyze the complexities in clinical psychiatry, neural networks and\npredictive analytics, and conclude with emphasizing and elaborating on\nlimitations and medical ethics issues of applying neural networks and\npredictive analytics to clinical psychiatry.",
    "published_date": "2019-05-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "68Uxx"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12471v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.09350v2",
    "title": "The tradeoff between the utility and risk of location data and implications for public good",
    "authors": [
      "Dan Calacci",
      "Alex Berke",
      "Kent Larson",
      "Alex",
      "Pentland"
    ],
    "author_ids": [],
    "abstract": "High-resolution individual geolocation data passively collected from mobile\nphones is increasingly sold in private markets and shared with researchers.\nThis data poses significant security, privacy, and ethical risks: it's been\nshown that users can be re-identified in such datasets, and its collection\nrarely involves their full consent or knowledge. This data is valuable to\nprivate firms (e.g. targeted marketing) but also presents clear value as a\npublic good. Recent public interest research has demonstrated that\nhigh-resolution location data can more accurately measure segregation in cities\nand provide inexpensive transit modeling. But as data is aggregated to mitigate\nits re-identifiability risk, its value as a good diminishes. How do we rectify\nthe clear security and safety risks of this data, its high market value, and\nits potential as a resource for public good? We extend the recently proposed\nconcept of a tradeoff curve that illustrates the relationship between dataset\nutility and privacy. We then hypothesize how this tradeoff differs between\nprivate market use and its potential use for public good. We further provide\nreal-world examples of how high resolution location data, aggregated to varying\ndegrees of privacy protection, can be used in the public sphere and how it is\ncurrently used by private firms.",
    "published_date": "2019-05-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09350v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.09345v1",
    "title": "KPynq: A Work-Efficient Triangle-Inequality based K-means on FPGA",
    "authors": [
      "Yuke Wang",
      "Zhaorui Zeng",
      "Boyuan Feng",
      "Lei Deng",
      "Yufei Ding"
    ],
    "author_ids": [],
    "abstract": "K-means is a popular but computation-intensive algorithm for unsupervised\nlearning. To address this issue, we present KPynq, a work-efficient\ntriangle-inequality based K-means on FPGA for handling large-size,\nhigh-dimension datasets. KPynq leverages an algorithm-level optimization to\nbalance the performance and computation irregularity, and a hardware\narchitecture design to fully exploit the pipeline and parallel processing\ncapability of various FPGAs. In the experiment, KPynq consistently outperforms\nthe CPU-based standard K-means in terms of its speedup (up to 4.2x) and\nsignificant energy-efficiency (up to 218x).",
    "published_date": "2019-05-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09345v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.09151v1",
    "title": "Rank-based persistence",
    "authors": [
      "Mattia G. Bergomi",
      "Pietro Vertechi"
    ],
    "author_ids": [],
    "abstract": "Persistence has proved to be a valuable tool to analyze real world data\nrobustly. Several approaches to persistence have been attempted over time, some\ntopological in flavor, based on the vector space-valued homology functor, other\ncombinatorial, based on arbitrary set-valued functors. To unify the study of\ntopological and combinatorial persistence in a common categorical framework, we\ngive axioms for a generalized rank function on objects in a target category, so\nthat functors to that category induce persistence functions. We port the\ninterleaving and bottleneck distances to this novel framework and generalize\nclassical equalities and inequalities. Unlike sets and vector spaces, in many\ncategories the rank of an object does not identify it up to isomorphism: to\npreserve information about the structure of persistence modules, we define\ncolorable ranks, persistence diagrams and prove the equality between\nmulticolored bottleneck distance and interleaving distance in semisimple\nAbelian categories. To illustrate our framework in practice, we give examples\nof multicolored persistent homology on filtered topological spaces with a group\naction and labeled point cloud data.",
    "published_date": "2019-05-22T00:00:00",
    "year": 2019,
    "categories": [
      "math.AT",
      "cs.CG",
      "math.CT",
      "18E10, 18A35, 55N35, 68U05"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09151v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.09105v1",
    "title": "Demographic Differentials in Facebook Usage Around the World",
    "authors": [
      "Sofia Gil-Clavel",
      "Emilio Zagheni"
    ],
    "author_ids": [],
    "abstract": "We use data from the Facebook Advertisement Platform to study patterns of\ndemographic disparities in usage of Facebook across countries. We address three\nmain questions: (1) How does Facebook usage differ by age and by gender around\nthe world? (2) How does the size of friendship networks vary by age and by\ngender? (3) What are the demographic characteristics of specific subgroups of\nFacebook users? We find that in countries in North America and northern Europe,\npatterns of Facebook usage differ little between older people and younger\nadults. In Asian countries, which have high levels of gender inequality,\ndifferences in Facebook adoption by gender disappear at older ages, possibly as\na result of selectivity. We also observe that across countries, women tend to\nhave larger networks of close friends than men, and that female users who are\nliving away from their hometown are more likely to engage in Facebook use than\ntheir male counterparts, regardless of their region and age group. Our findings\ncontextualize recent research on gender gaps in online usage, and offer new\ninsights into some of the nuances of demographic differentials in the adoption\nand the use of digital technologies.",
    "published_date": "2019-05-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.09105v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.08998v1",
    "title": "Bias estimation in sensor networks",
    "authors": [
      "Mingming Shi",
      "Claudio De Persis",
      "Pietro Tesi",
      "Nima Monshizadeh"
    ],
    "author_ids": [],
    "abstract": "This paper investigates the problem of estimating biases affecting relative\nstate measurements in a sensor network. Each sensor measures the relative\nstates of its neighbors and this measurement is corrupted by a constant bias.\nWe analyse under what conditions on the network topology and the maximum number\nof biased sensors the biases can be correctly estimated. We show that for\nnon-bipartite graphs the biases can always be determined even when all the\nsensors are corrupted, while for bipartite graphs more than half of the sensors\nshould be unbiased to ensure the correctness of the bias estimation. If the\nbiases are heterogeneous, then the number of unbiased sensors can be reduced to\ntwo. Based on these conditions, we propose some algorithms to estimate the\nbiases.",
    "published_date": "2019-05-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08998v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.08881v1",
    "title": "An adaptive approach to real-time estimation of vehicle sideslip, road bank angles and sensor bias",
    "authors": [
      "Yi-Wen Liao",
      "Francesco Borrelli"
    ],
    "author_ids": [],
    "abstract": "Robust estimation of vehicle sideslip angle is essential for stability\ncontrol applications. However, the direct measurement of sideslip angle is\nexpensive for production vehicles. This paper presents a novel sideslip\nestimation algorithm which relies only on sensors available on passenger and\ncommercial vehicles. The proposed method uses both kinematics and dynamics\nvehicle models to construct extended Kalman filter observers. The estimate\nrelies on the results provided from the dynamics model observer where the tire\ncornering stiffness parameters are updated using the information provided from\nthe kinematics model observer. The stability property of the proposed algorithm\nis discussed and proven. Finally, multiple experimental tests are conducted to\nverify its performance in practice. The results show that the proposed approach\nprovides smooth and accurate sideslip angle estimation. In addition, our novel\nalgorithm provides reliable estimates of bank angles and lateral acceleration\nsensor bias.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.SY",
      "math.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08881v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.08871v2",
    "title": "Measuring the effects of confounders in medical supervised classification problems: the Confounding Index (CI)",
    "authors": [
      "Elisa Ferrari",
      "Alessandra Retico",
      "Davide Bacciu"
    ],
    "author_ids": [],
    "abstract": "Over the years, there has been growing interest in using Machine Learning\ntechniques for biomedical data processing. When tackling these tasks, one needs\nto bear in mind that biomedical data depends on a variety of characteristics,\nsuch as demographic aspects (age, gender, etc) or the acquisition technology,\nwhich might be unrelated with the target of the analysis. In supervised tasks,\nfailing to match the ground truth targets with respect to such characteristics,\ncalled confounders, may lead to very misleading estimates of the predictive\nperformance. Many strategies have been proposed to handle confounders, ranging\nfrom data selection, to normalization techniques, up to the use of training\nalgorithm for learning with imbalanced data. However, all these solutions\nrequire the confounders to be known a priori. To this aim, we introduce a novel\nindex that is able to measure the confounding effect of a data attribute in a\nbias-agnostic way. This index can be used to quantitatively compare the\nconfounding effects of different variables and to inform correction methods\nsuch as normalization procedures or ad-hoc-prepared learning algorithms. The\neffectiveness of this index is validated on both simulated data and real-world\nneuroimaging data.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08871v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08868v3",
    "title": "Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution",
    "authors": [
      "Yinchuan Xu",
      "Junlin Yang"
    ],
    "author_ids": [],
    "abstract": "Gender bias has been found in existing coreference resolvers. In order to\neliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns\n(GAP) has been released and the best baseline model achieves only 66.9% F1.\nBidirectional Encoder Representations from Transformers (BERT) has broken\nseveral NLP task records and can be used on GAP dataset. However, fine-tune\nBERT on a specific task is computationally expensive. In this paper, we propose\nan end-to-end resolver by combining pre-trained BERT with Relational Graph\nConvolutional Network (R-GCN). R-GCN is used for digesting structural syntactic\ninformation and learning better task-specific embeddings. Empirical results\ndemonstrate that, under explicit syntactic supervision and without the need to\nfine tune BERT, R-GCN's embeddings outperform the original BERT embeddings on\nthe coreference task. Our work significantly improves the snippet-context\nbaseline F1 score on GAP dataset from 66.9% to 80.3%. We participated in the\n2019 GAP Coreference Shared Task, and our codes are available online.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08868v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08780v1",
    "title": "A Comparative Analysis of Distributional Term Representations for Author Profiling in Social Media",
    "authors": [
      "Miguel Á. Álvarez-Carmona",
      "Esaú Villatoro-Tello",
      "Manuel Montes-y-Gómez",
      "Luis Villaseñor-Pienda"
    ],
    "author_ids": [],
    "abstract": "Author Profiling (AP) aims at predicting specific characteristics from a\ngroup of authors by analyzing their written documents. Many research has been\nfocused on determining suitable features for modeling writing patterns from\nauthors. Reported results indicate that content-based features continue to be\nthe most relevant and discriminant features for solving this task. Thus, in\nthis paper, we present a thorough analysis regarding the appropriateness of\ndifferent distributional term representations (DTR) for the AP task. In this\nregard, we introduce a novel framework for supervised AP using these\nrepresentations and, supported on it. We approach a comparative analysis of\nrepresentations such as DOR, TCOR, SSR, and word2vec in the AP problem. We also\ncompare the performance of the DTRs against classic approaches including\npopular topic-based methods. The obtained results indicate that DTRs are\nsuitable for solving the AP task in social media domains as they achieve\ncompetitive results while providing meaningful interpretability.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08780v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08707v3",
    "title": "Lagrangian uncertainty quantification and information inequalities for stochastic flows",
    "authors": [
      "Michal Branicki",
      "Kenneth Uda"
    ],
    "author_ids": [],
    "abstract": "We develop a systematic information-theoretic framework for quantification\nand mitigation of error in probabilistic Lagrangian (i.e., path-based)\npredictions which are obtained from dynamical systems generated by uncertain\n(Eulerian) vector fields. This work is motivated by the desire to improve\nLagrangian predictions in complex dynamical systems based either on\nanalytically simplified or data-driven models. We derive a hierarchy of general\ninformation bounds on uncertainty in estimates of statistical observables\n$\\mathbb{E}^{\\nu}[f]$, evaluated on trajectories of the approximating dynamical\nsystem, relative to the \"true'' observables $\\mathbb{E}^{\\mu}[f]$ in terms of\ncertain $\\varphi$-divergences, $\\mathcal{D}_\\varphi(\\mu\\|\\nu)$, which quantify\ndiscrepancies between probability measures $\\mu$ associated with the original\ndynamics and their approximations $\\nu$. We then derive two distinct bounds on\n$\\mathcal{D}_\\varphi(\\mu\\|\\nu)$ itself in terms of the Eulerian fields. This\nnew framework provides a rigorous way for quantifying and mitigating\nuncertainty in Lagrangian predictions due to Eulerian model error.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.DS",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08707v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.08594v1",
    "title": "Geometric Estimation of Multivariate Dependency",
    "authors": [
      "Salimeh Yasaei Sekeh",
      "Alfred O. Hero"
    ],
    "author_ids": [],
    "abstract": "This paper proposes a geometric estimator of dependency between a pair of\nmultivariate samples. The proposed estimator of dependency is based on a\nrandomly permuted geometric graph (the minimal spanning tree) over the two\nmultivariate samples. This estimator converges to a quantity that we call the\ngeometric mutual information (GMI), which is equivalent to the Henze-Penrose\ndivergence [1] between the joint distribution of the multivariate samples and\nthe product of the marginals. The GMI has many of the same properties as\nstandard MI but can be estimated from empirical data without density\nestimation; making it scalable to large datasets. The proposed empirical\nestimator of GMI is simple to implement, involving the construction of an MST\nspanning over both the original data and a randomly permuted version of this\ndata. We establish asymptotic convergence of the estimator and convergence\nrates of the bias and variance for smooth multivariate density functions\nbelonging to a H\\\"{o}lder class. We demonstrate the advantages of our proposed\ngeometric dependency estimator in a series of experiments.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08594v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08586v1",
    "title": "Marginalized Average Attentional Network for Weakly-Supervised Learning",
    "authors": [
      "Yuan Yuan",
      "Yueming Lyu",
      "Xi Shen",
      "Ivor W. Tsang",
      "Dit-Yan Yeung"
    ],
    "author_ids": [],
    "abstract": "In weakly-supervised temporal action localization, previous works have failed\nto locate dense and integral regions for each entire action due to the\noverestimation of the most salient regions. To alleviate this issue, we propose\na marginalized average attentional network (MAAN) to suppress the dominant\nresponse of the most salient regions in a principled manner. The MAAN employs a\nnovel marginalized average aggregation (MAA) module and learns a set of latent\ndiscriminative probabilities in an end-to-end fashion. MAA samples multiple\nsubsets from the video snippet features according to a set of latent\ndiscriminative probabilities and takes the expectation over all the averaged\nsubset features. Theoretically, we prove that the MAA module with learned\nlatent discriminative probabilities successfully reduces the difference in\nresponses between the most salient regions and the others. Therefore, MAAN is\nable to generate better class activation sequences and identify dense and\nintegral action regions in the videos. Moreover, we propose a fast algorithm to\nreduce the complexity of constructing MAA from O($2^T$) to O($T^2$). Extensive\nexperiments on two large-scale video datasets show that our MAAN achieves\nsuperior performance on weakly-supervised temporal action localization",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08586v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08495v1",
    "title": "Exploring Bias in GAN-based Data Augmentation for Small Samples",
    "authors": [
      "Mengxiao Hu",
      "Jinlong Li"
    ],
    "author_ids": [],
    "abstract": "For machine learning task, lacking sufficient samples mean the trained model\nhas low confidence to approach the ground truth function. Until recently, after\nthe generative adversarial networks (GAN) had been proposed, we see the hope of\nsmall samples data augmentation (DA) with realistic fake data, and many works\nvalidated the viability of GAN-based DA. Although most of the works pointed out\nhigher accuracy can be achieved using GAN-based DA, some researchers stressed\nthat the fake data generated from GAN has inherent bias, and in this paper, we\nexplored when the bias is so low that it cannot hurt the performance, we set\nexperiments to depict the bias in different GAN-based DA setting, and from the\nresults, we design a pipeline to inspect specific dataset is\nefficiently-augmentable with GAN-based DA or not. And finally, depending on our\ntrial to reduce the bias, we proposed some advice to mitigate bias in GAN-based\nDA application.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08495v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08464v1",
    "title": "Robustness Against Outliers For Deep Neural Networks By Gradient Conjugate Priors",
    "authors": [
      "Pavel Gurevich",
      "Hannes Stuke"
    ],
    "author_ids": [],
    "abstract": "We analyze a new robust method for the reconstruction of probability\ndistributions of observed data in the presence of output outliers. It is based\non a so-called gradient conjugate prior (GCP) network which outputs the\nparameters of a prior. By rigorously studying the dynamics of the GCP learning\nprocess, we derive an explicit formula for correcting the obtained variance of\nthe marginal distribution and removing the bias caused by outliers in the\ntraining set. Assuming a Gaussian (input-dependent) ground truth distribution\ncontaminated with a proportion $\\varepsilon$ of outliers, we show that the\nfitted mean is in a $c e^{-1/\\varepsilon}$-neighborhood of the ground truth\nmean and the corrected variance is in a $b\\varepsilon$-neighborhood of the\nground truth variance, whereas the uncorrected variance of the marginal\ndistribution can even be infinite. We explicitly find $b$ as a function of the\noutput of the GCP network, without a priori knowledge of the outliers (possibly\ninput-dependent) distribution. Experiments with synthetic and real-world data\nsets indicate that the GCP network fitted with a standard optimizer outperforms\nother robust methods for regression.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08464v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.08388v1",
    "title": "Exploring the Fairness and Resource Distribution in an Apache Mesos Environment",
    "authors": [
      "Pankaj Saha",
      "Angel Beltre",
      "Madhusudhan Govindaraju"
    ],
    "author_ids": [],
    "abstract": "Apache Mesos, a cluster-wide resource manager, is widely deployed in massive\nscale at several Clouds and Data Centers. Mesos aims to provide high cluster\nutilization via fine grained resource co-scheduling and resource fairness among\nmultiple users through Dominant Resource Fairness (DRF) based allocation. DRF\ntakes into account different resource types (CPU, Memory, Disk I/O) requested\nby each application and determines the share of each cluster resource that\ncould be allocated to the applications. Mesos has adopted a two-level\nscheduling policy: (1) DRF to allocate resources to competing frameworks and\n(2) task level scheduling by each framework for the resources allocated during\nthe previous step. We have conducted experiments in a local Mesos cluster when\nused with frameworks such as Apache Aurora, Marathon, and our own framework\nScylla, to study resource fairness and cluster utilization. Experimental\nresults show how informed decision regarding second level scheduling policy of\nframeworks and attributes like offer holding period, offer refusal cycle and\ntask arrival rate can reduce unfair resource distribution. Bin-Packing\nscheduling policy on Scylla with Marathon can reduce unfair allocation from\n38\\% to 3\\%. By reducing unused free resources in offers we bring down the\nunfairness from to 90\\% to 28\\%. We also show the effect of task arrival rate\nto reduce the unfairness from 23\\% to 7\\%.",
    "published_date": "2019-05-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.PF",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08388v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.07923v1",
    "title": "Transmitter Classification With Supervised Deep Learning",
    "authors": [
      "Cyrille Morin",
      "Leonardo Cardoso",
      "Jakob Hoydis",
      "Jean-Marie Gorce",
      "Thibaud Vial"
    ],
    "author_ids": [],
    "abstract": "Hardware imperfections in RF transmitters introduce features that can be used\nto identify a specific transmitter amongst others. Supervised deep learning has\nshown good performance in this task but using datasets not applicable to real\nworld situations where topologies evolve over time. To remedy this, the work\nrests on a series of datasets gathered in the Future Internet of Things /\nCognitive Radio Testbed [4] (FIT/CorteXlab) to train a convolutional neural\nnetwork (CNN), where focus has been given to reduce channel bias that has\nplagued previous works and constrained them to a constant environment or to\nsimulations. The most challenging scenarios provide the trained neural network\nwith resilience and show insight on the best signal type to use for\nidentification , namely packet preamble. The generated datasets are published\non the Machine Learning For Communications Emerging Technologies Initiatives\nweb site 4 in the hope that they serve as stepping stones for future progress\nin the area. The community is also invited to reproduce the studied scenarios\nand results by generating new datasets in FIT/CorteXlab.",
    "published_date": "2019-05-20T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.LG",
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07923v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07857v1",
    "title": "CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",
    "authors": [
      "Shubham Sharma",
      "Jette Henderson",
      "Joydeep Ghosh"
    ],
    "author_ids": [],
    "abstract": "As artificial intelligence plays an increasingly important role in our\nsociety, there are ethical and moral obligations for both businesses and\nresearchers to ensure that their machine learning models are designed,\ndeployed, and maintained responsibly. These models need to be rigorously\naudited for fairness, robustness, transparency, and interpretability. A variety\nof methods have been developed that focus on these issues in isolation,\nhowever, managing these methods in conjunction with model development can be\ncumbersome and timeconsuming. In this paper, we introduce a unified and\nmodel-agnostic approach to address these issues: Counterfactual Explanations\nfor Robustness, Transparency, Interpretability, and Fairness of Artificial\nIntelligence models (CERTIFAI). Unlike previous methods in this domain,\nCERTIFAI is a general tool that can be applied to any black-box model and any\ntype of input data. Given a model and an input instance, CERTIFAI uses a custom\ngenetic algorithm to generate counterfactuals: instances close to the input\nthat change the prediction of the model. We demonstrate how these\ncounterfactuals can be used to examine issues of robustness, interpretability,\ntransparency, and fairness. Additionally, we introduce CERScore, the first\nblack-box model robustness score that performs comparably to methods that have\naccess to model internals.",
    "published_date": "2019-05-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07857v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07831v3",
    "title": "Testing DNN Image Classifiers for Confusion & Bias Errors",
    "authors": [
      "Yuchi Tian",
      "Ziyuan Zhong",
      "Vicente Ordonez",
      "Gail Kaiser",
      "Baishakhi Ray"
    ],
    "author_ids": [],
    "abstract": "Image classifiers are an important component of today's software, from\nconsumer and business applications to safety-critical domains. The advent of\nDeep Neural Networks (DNNs) is the key catalyst behind such wide-spread\nsuccess. However, wide adoption comes with serious concerns about the\nrobustness of software systems dependent on DNNs for image classification, as\nseveral severe erroneous behaviors have been reported under sensitive and\ncritical circumstances. We argue that developers need to rigorously test their\nsoftware's image classifiers and delay deployment until acceptable. We present\nan approach to testing image classifier robustness based on class property\nviolations.\n  We found that many of the reported erroneous cases in popular DNN image\nclassifiers occur because the trained models confuse one class with another or\nshow biases towards some classes over others. These bugs usually violate some\nclass properties of one or more of those classes. Most DNN testing techniques\nfocus on per-image violations, so fail to detect class-level confusions or\nbiases.\n  We developed a testing technique to automatically detect class-based\nconfusion and bias errors in DNN-driven image classification software. We\nevaluated our implementation, DeepInspect, on several popular image classifiers\nwith precision up to 100% (avg.~72.6%) for confusion errors, and up to 84.3%\n(avg.~66.8%) for bias errors. DeepInspect found hundreds of classification\nmistakes in widely-used models, many exposing errors indicating confusion or\nbias.",
    "published_date": "2019-05-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07831v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07817v2",
    "title": "Spatio-Temporal Adversarial Learning for Detecting Unseen Falls",
    "authors": [
      "Shehroz S. Khan",
      "Jacob Nogas",
      "Alex Mihailidis"
    ],
    "author_ids": [],
    "abstract": "Fall detection is an important problem from both the health and machine\nlearning perspective. A fall can lead to severe injuries, long term impairments\nor even death in some cases. In terms of machine learning, it presents a\nseverely class imbalance problem with very few or no training data for falls\nowing to the fact that falls occur rarely. In this paper, we take an alternate\nphilosophy to detect falls in the absence of their training data, by training\nthe classifier on only the normal activities (that are available in abundance)\nand identifying a fall as an anomaly. To realize such a classifier, we use an\nadversarial learning framework, which comprises of a spatio-temporal\nautoencoder for reconstructing input video frames and a spatio-temporal\nconvolution network to discriminate them against original video frames. 3D\nconvolutions are used to learn spatial and temporal features from the input\nvideo frames. The adversarial learning of the spatio-temporal autoencoder will\nenable reconstructing the normal activities of daily living efficiently; thus,\nrendering detecting unseen falls plausible within this framework. We tested the\nperformance of the proposed framework on camera sensing modalities that may\npreserve an individual's privacy (fully or partially), such as thermal and\ndepth camera. Our results on three publicly available datasets show that the\nproposed spatio-temporal adversarial framework performed better than other\nbaseline frame based (or spatial) adversarial learning methods.",
    "published_date": "2019-05-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07817v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07473v2",
    "title": "Adaptively Truncating Backpropagation Through Time to Control Gradient Bias",
    "authors": [
      "Christopher Aicher",
      "Nicholas J. Foti",
      "Emily B. Fox"
    ],
    "author_ids": [],
    "abstract": "Truncated backpropagation through time (TBPTT) is a popular method for\nlearning in recurrent neural networks (RNNs) that saves computation and memory\nat the cost of bias by truncating backpropagation after a fixed number of lags.\nIn practice, choosing the optimal truncation length is difficult: TBPTT will\nnot converge if the truncation length is too small, or will converge slowly if\nit is too large. We propose an adaptive TBPTT scheme that converts the problem\nfrom choosing a temporal lag to one of choosing a tolerable amount of gradient\nbias. For many realistic RNNs, the TBPTT gradients decay geometrically in\nexpectation for large lags; under this condition, we can control the bias by\nvarying the truncation length adaptively. For RNNs with smooth activation\nfunctions, we prove that this bias controls the convergence rate of SGD with\nbiased gradients for our non-convex loss. Using this theory, we develop a\npractical method for adaptively estimating the truncation length during\ntraining. We evaluate our adaptive TBPTT method on synthetic data and language\nmodeling tasks and find that our adaptive TBPTT ameliorates the computational\npitfalls of fixed TBPTT.",
    "published_date": "2019-05-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07473v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07446v1",
    "title": "Limitations and Biases in Facial Landmark Detection -- An Empirical Study on Older Adults with Dementia",
    "authors": [
      "Azin Asgarian",
      "Shun Zhao",
      "Ahmed B. Ashraf",
      "M. Erin Browne",
      "Kenneth M. Prkachin",
      "Alex Mihailidis",
      "Thomas Hadjistavropoulos",
      "Babak Taati"
    ],
    "author_ids": [],
    "abstract": "Accurate facial expression analysis is an essential step in various clinical\napplications that involve physical and mental health assessments of older\nadults (e.g. diagnosis of pain or depression). Although remarkable progress has\nbeen achieved toward developing robust facial landmark detection methods,\nstate-of-the-art methods still face many challenges when encountering\nuncontrolled environments, different ranges of facial expressions, and\ndifferent demographics of the population. A recent study has revealed that the\nhealth status of individuals can also affect the performance of facial landmark\ndetection methods on front views of faces. In this work, we investigate this\nmatter in a much greater context using seven facial landmark detection methods.\nWe perform our evaluation not only on frontal faces but also on profile faces\nand in various regions of the face. Our results shed light on limitations of\nthe existing methods and challenges of applying these methods in clinical\nsettings by indicating: 1) a significant difference between the performance of\nstate-of-the-art when tested on the profile or frontal faces of individuals\nwith vs. without dementia; 2) insights on the existing bias for all regions of\nthe face; and 3) the presence of this bias despite re-training/fine-tuning with\nvarious configurations of six datasets.",
    "published_date": "2019-05-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07446v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07360v4",
    "title": "Contrastive Fairness in Machine Learning",
    "authors": [
      "Tapabrata Chakraborti",
      "Arijit Patra",
      "Alison Noble"
    ],
    "author_ids": [],
    "abstract": "Was it fair that Harry was hired but not Barry? Was it fair that Pam was\nfired instead of Sam? How can one ensure fairness when an intelligent algorithm\ntakes these decisions instead of a human? How can one ensure that the decisions\nwere taken based on merit and not on protected attributes like race or sex?\nThese are the questions that must be answered now that many decisions in real\nlife can be made through machine learning. However research in fairness of\nalgorithms has focused on the counterfactual questions \"what if?\" or \"why?\",\nwhereas in real life most subjective questions of consequence are contrastive:\n\"why this but not that?\". We introduce concepts and mathematical tools using\ncausal inference to address contrastive fairness in algorithmic decision-making\nwith illustrative examples.",
    "published_date": "2019-05-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07360v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07152v1",
    "title": "An Empirical View on Content Provider Fairness",
    "authors": [
      "Jan Rüth",
      "Ike Kunze",
      "Oliver Hohlfeld"
    ],
    "author_ids": [],
    "abstract": "Congestion control is an indispensable component of transport protocols to\nprevent congestion collapse. As such, it distributes the available bandwidth\namong all competing flows, ideally in a fair manner. However, there exists a\nconstantly evolving set of congestion control algorithms, each addressing\ndifferent performance needs and providing the potential for custom\nparametrizations. In particular, content providers such as CDNs are known to\ntune TCP stacks for performance gains. In this paper, we thus empirically\ninvestigate if current Internet traffic generated by content providers still\nadheres to the conventional understanding of fairness. Our study compares\nfairness properties of testbed hosts to actual traffic of six major content\nproviders subject to different bandwidths, RTTs, queue sizes, and queueing\ndisciplines in a home-user setting. We find that some employed congestion\ncontrol algorithms lead to significantly asymmetric bandwidth shares, however,\nAQMs such as FQ_CoDel are able to alleviate such unfairness.",
    "published_date": "2019-05-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07152v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.08095v1",
    "title": "Control Theory Meets POMDPs: A Hybrid Systems Approach",
    "authors": [
      "Mohamadreza Ahmadi",
      "Nils Jansen",
      "Bo Wu",
      "Ufuk Topcu"
    ],
    "author_ids": [],
    "abstract": "Partially observable Markov decision processes (POMDPs) provide a modeling\nframework for a variety of sequential decision making under uncertainty\nscenarios in artificial intelligence (AI). Since the states are not directly\nobservable in a POMDP, decision making has to be performed based on the output\nof a Bayesian filter (continuous beliefs). Hence, POMDPs are often\ncomputationally intractable to solve exactly and researchers resort to\napproximate methods often using discretizations of the continuous belief space.\nThese approximate solutions are, however, prone to discretization errors, which\nhas made POMDPs ineffective in applications, wherein guarantees for safety,\noptimality, or performance are required. To overcome the complexity challenge\nof POMDPs, we apply notions from control theory. The goal is to determine the\nreachable belief space of a POMDP, that is, the set of all possible evolutions\ngiven an initial belief distribution over the states and a set of actions and\nobservations. We begin by casting the problem of analyzing a POMDP into\nanalyzing the behavior of a discrete-time switched system. For estimating the\nreachable belief space, we find over-approximations in terms of sub-level sets\nof Lyapunov functions. Furthermore, in order to verify safety and optimality\nrequirements of a given POMDP, we formulate a barrier certificate theorem,\nwherein we show that if there exists a barrier certificate satisfying a set of\ninequalities along with the belief update equation of the POMDP, the safety and\noptimality properties are guaranteed to hold. In both cases, we show how the\ncalculations can be decomposed into smaller problems that can be solved in\nparallel. The conditions we formulate can be computationally implemented as a\nset of sum-of-squares programs. We illustrate the applicability of our method\nby addressing two problems in active ad scheduling and machine teaching.",
    "published_date": "2019-05-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08095v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.07026v2",
    "title": "Fairness in Machine Learning with Tractable Models",
    "authors": [
      "Michael Varley",
      "Vaishak Belle"
    ],
    "author_ids": [],
    "abstract": "Machine Learning techniques have become pervasive across a range of different\napplications, and are now widely used in areas as disparate as recidivism\nprediction, consumer credit-risk analysis and insurance pricing. The prevalence\nof machine learning techniques has raised concerns about the potential for\nlearned algorithms to become biased against certain groups. Many definitions\nhave been proposed in the literature, but the fundamental task of reasoning\nabout probabilistic events is a challenging one, owing to the intractability of\ninference.\n  The focus of this paper is taking steps towards the application of tractable\nmodels to fairness. Tractable probabilistic models have emerged that guarantee\nthat conditional marginal can be computed in time linear in the size of the\nmodel. In particular, we show that sum product networks (SPNs) enable an\neffective technique for determining the statistical relationships between\nprotected attributes and other training variables. If a subset of these\ntraining variables are found by the SPN to be independent of the training\nattribute then they can be considered `safe' variables, from which we can train\na classification model without concern that the resulting classifier will\nresult in disparate outcomes for different demographic groups.\n  Our initial experiments on the `German Credit' data set indicate that this\nprocessing technique significantly reduces disparate treatment of male and\nfemale credit applicants, with a small reduction in classification accuracy\ncompared to state of the art. We will also motivate the concept of \"fairness\nthrough percentile equivalence\", a new definition predicated on the notion that\nindividuals at the same percentile of their respective distributions should be\ntreated equivalently, and this prevents unfair penalisation of those\nindividuals who lie at the extremities of their respective distributions.",
    "published_date": "2019-05-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07026v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.06649v1",
    "title": "What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue",
    "authors": [
      "Laura Aina",
      "Carina Silberer",
      "Matthijs Westera",
      "Ionut-Teodor Sorodoc",
      "Gemma Boleda"
    ],
    "author_ids": [],
    "abstract": "Humans use language to refer to entities in the external world. Motivated by\nthis, in recent years several models that incorporate a bias towards learning\nentity representations have been proposed. Such entity-centric models have\nshown empirical success, but we still know little about why. In this paper we\nanalyze the behavior of two recently proposed entity-centric models in a\nreferential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4).\nWe show that these models outperform the state of the art on this task, and\nthat they do better on lower frequency entities than a counterpart model that\nis not entity-centric, with the same model size. We argue that making models\nentity-centric naturally fosters good architectural decisions. However, we also\nshow that these models do not really build entity representations and that they\nmake poor use of linguistic context. These negative results underscore the need\nfor model analysis, to test whether the motivations for particular\narchitectures are borne out in how models behave when deployed.",
    "published_date": "2019-05-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06649v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.06618v4",
    "title": "On the Fairness of Time-Critical Influence Maximization in Social Networks",
    "authors": [
      "Junaid Ali",
      "Mahmoudreza Babaei",
      "Abhijnan Chakraborty",
      "Baharan Mirzasoleiman",
      "Krishna P. Gummadi",
      "Adish Singla"
    ],
    "author_ids": [],
    "abstract": "Influence maximization has found applications in a wide range of real-world\nproblems, for instance, viral marketing of products in an online social\nnetwork, and information propagation of valuable information such as job\nvacancy advertisements and health-related information. While existing\nalgorithmic techniques usually aim at maximizing the total number of people\ninfluenced, the population often comprises several socially salient groups,\ne.g., based on gender or race. As a result, these techniques could lead to\ndisparity across different groups in receiving important information.\nFurthermore, in many of these applications, the spread of influence is\ntime-critical, i.e., it is only beneficial to be influenced before a time\ndeadline. As we show in this paper, the time-criticality of the information\ncould further exacerbate the disparity of influence across groups. This\ndisparity, introduced by algorithms aimed at maximizing total influence, could\nhave far-reaching consequences, impacting people's prosperity and putting\nminority groups at a big disadvantage. In this work, we propose a notion of\ngroup fairness in time-critical influence maximization. We introduce surrogate\nobjective functions to solve the influence maximization problem under fairness\nconsiderations. By exploiting the submodularity structure of our objectives, we\nprovide computationally efficient algorithms with guarantees that are effective\nin enforcing fairness during the propagation process. We demonstrate the\neffectiveness of our approach through synthetic and real-world experiments.",
    "published_date": "2019-05-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06618v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.06289v1",
    "title": "A Human-Centered Approach to Interactive Machine Learning",
    "authors": [
      "Kory W. Mathewson"
    ],
    "author_ids": [],
    "abstract": "The interactive machine learning (IML) community aims to augment humans'\nability to learn and make decisions over time through the development of\nautomated decision-making systems. This interaction represents a collaboration\nbetween multiple intelligent systems---humans and machines. A lack of\nappropriate consideration for the humans involved can lead to problematic\nsystem behaviour, and issues of fairness, accountability, and transparency.\nThis work presents a human-centred thinking approach to applying IML methods.\nThis guide is intended to be used by AI practitioners who incorporate human\nfactors in their work. These practitioners are responsible for the health,\nsafety, and well-being of interacting humans. An obligation of responsibility\nfor public interaction means acting with integrity, honesty, fairness, and\nabiding by applicable legal statutes. With these values and principles in mind,\nwe as a research community can better achieve the collective goal of augmenting\nhuman ability. This practical guide aims to support many of the responsible\ndecisions necessary throughout iterative design, development, and dissemination\nof IML systems.",
    "published_date": "2019-05-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06289v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.06221v4",
    "title": "Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets",
    "authors": [
      "Guanhua Zhang",
      "Bing Bai",
      "Jian Liang",
      "Kun Bai",
      "Shiyu Chang",
      "Mo Yu",
      "Conghui Zhu",
      "Tiejun Zhao"
    ],
    "author_ids": [],
    "abstract": "Natural Language Sentence Matching (NLSM) has gained substantial attention\nfrom both academics and the industry, and rich public datasets contribute a lot\nto this process. However, biased datasets can also hurt the generalization\nperformance of trained models and give untrustworthy evaluation results. For\nmany NLSM datasets, the providers select some pairs of sentences into the\ndatasets, and this sampling procedure can easily bring unintended pattern,\ni.e., selection bias. One example is the QuoraQP dataset, where some\ncontent-independent naive features are unreasonably predictive. Such features\nare the reflection of the selection bias and termed as the leakage features. In\nthis paper, we investigate the problem of selection bias on six NLSM datasets\nand find that four out of them are significantly biased. We further propose a\ntraining and evaluation framework to alleviate the bias. Experimental results\non QuoraQP suggest that the proposed framework can improve the generalization\nability of trained models, and give more trustworthy evaluation results for\nreal-world adoptions.",
    "published_date": "2019-05-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06221v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.06144v2",
    "title": "Feedback MPC for Torque-Controlled Legged Robots",
    "authors": [
      "Ruben Grandia",
      "Farbod Farshidian",
      "René Ranftl",
      "Marco Hutter"
    ],
    "author_ids": [],
    "abstract": "The computational power of mobile robots is currently insufficient to achieve\ntorque level whole-body Model Predictive Control (MPC) at the update rates\nrequired for complex dynamic systems such as legged robots. This problem is\ncommonly circumvented by using a fast tracking controller to compensate for\nmodel errors between updates. In this work, we show that the feedback policy\nfrom a Differential Dynamic Programming (DDP) based MPC algorithm is a viable\nalternative to bridge the gap between the low MPC update rate and the actuation\ncommand rate. We propose to augment the DDP approach with a relaxed barrier\nfunction to address inequality constraints arising from the friction cone. A\nfrequency-dependent cost function is used to reduce the sensitivity to\nhigh-frequency model errors and actuator bandwidth limits. We demonstrate that\nour approach can find stable locomotion policies for the torque-controlled\nquadruped, ANYmal, both in simulation and on hardware.",
    "published_date": "2019-05-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06144v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.06876v2",
    "title": "From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",
    "authors": [
      "Jessica Morley",
      "Luciano Floridi",
      "Libby Kinsey",
      "Anat Elhalal"
    ],
    "author_ids": [],
    "abstract": "The debate about the ethical implications of Artificial Intelligence dates\nfrom the 1960s. However, in recent years symbolic AI has been complemented and\nsometimes replaced by Neural Networks and Machine Learning techniques. This has\nvastly increased its potential utility and impact on society, with the\nconsequence that the ethical debate has gone mainstream. Such debate has\nprimarily focused on principles - the what of AI ethics - rather than on\npractices, the how. Awareness of the potential issues is increasing at a fast\nrate, but the AI community's ability to take action to mitigate the associated\nrisks is still at its infancy. Therefore, our intention in presenting this\nresearch is to contribute to closing the gap between principles and practices\nby constructing a typology that may help practically-minded developers apply\nethics at each stage of the pipeline, and to signal to researchers where\nfurther work is needed. The focus is exclusively on Machine Learning, but it is\nhoped that the results of this research may be easily applicable to other\nbranches of AI. The article outlines the research method for creating this\ntypology, the initial findings, and provides a summary of future research\nneeds.",
    "published_date": "2019-05-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06876v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.05961v1",
    "title": "Demographic Inference and Representative Population Estimates from Multilingual Social Media Data",
    "authors": [
      "Zijian Wang",
      "Scott A. Hale",
      "David Adelani",
      "Przemyslaw A. Grabowicz",
      "Timo Hartmann",
      "Fabian Flöck",
      "David Jurgens"
    ],
    "author_ids": [],
    "abstract": "Social media provide access to behavioural data at an unprecedented scale and\ngranularity. However, using these data to understand phenomena in a broader\npopulation is difficult due to their non-representativeness and the bias of\nstatistical inference tools towards dominant languages and groups. While\ndemographic attribute inference could be used to mitigate such bias, current\ntechniques are almost entirely monolingual and fail to work in a global\nenvironment. We address these challenges by combining multilingual demographic\ninference with post-stratification to create a more representative population\nsample. To learn demographic attributes, we create a new multimodal deep neural\narchitecture for joint classification of age, gender, and organization-status\nof social media users that operates in 32 languages. This method substantially\noutperforms current state of the art while also reducing algorithmic bias. To\ncorrect for sampling biases, we propose fully interpretable multilevel\nregression methods that estimate inclusion probabilities from inferred joint\npopulation counts and ground-truth population counts. In a large experiment\nover multilingual heterogeneous European regions, we show that our demographic\ninference and bias correction together allow for more accurate estimates of\npopulations and make a significant step towards representative social sensing\nin downstream applications with multilingual social media.",
    "published_date": "2019-05-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.05961v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.05892v2",
    "title": "Pareto-Optimal Allocation of Transactive Energy at Market Equilibrium in Distribution Systems: A Constrained Vector Optimization Approach",
    "authors": [
      "Ahmad Khaled Zarabie",
      "Sanjoy Das"
    ],
    "author_ids": [],
    "abstract": "In a grid constrained transactive distribution system market, distribution\nlocational marginal pricing DLMP is influenced by the distance from the\nsubstation to an energy user, thereby causing households that are further away\nfrom the substation to be charged more. The Jain index of fairness, which has\nbeen recently applied to alleviate this undesirable effect of inefficient\nenergy allocations, is used in this research to quantify fairness. It is shown\nthat the Jain index is strictly quasi-concave. A bilevel distributed mechanism\nis proposed, where at the lower level, auction mechanisms are invoked\nsimultaneously at each aggregator to obtain energy costs under market\nequilibrium conditions. A constrained multi gradient ascent algorithm,\nAugmented Lagrangian Multigradient Approach ALMA, is proposed for\nimplementation at the upper level to attain energy allocations that represent\ntradeoffs between efficiency and fairness. Theoretical issues pertaining to\nALMA as a generic algorithm for constrained vector optimization are considered.\nIt is shown that when the objectives are restricted to be strictly quasi\nconcave functions and if the feasible region is convex, ALMA converges towards\nglobal Pareto optimality. The overall effectiveness of the proposed approach is\nconfirmed through a set of MATLAB simulations implemented on a modified IEEE\n37-bus system platform.",
    "published_date": "2019-05-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CE",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.05892v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.08031v1",
    "title": "Power of the Few: Analyzing the Impact of Influential Users in Collaborative Recommender Systems",
    "authors": [
      "Farzad Eskandanian",
      "Nasim Sonboli",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Like other social systems, in collaborative filtering a small number of\n\"influential\" users may have a large impact on the recommendations of other\nusers, thus affecting the overall behavior of the system. Identifying\ninfluential users and studying their impact on other users is an important\nproblem because it provides insight into how small groups can inadvertently or\nintentionally affect the behavior of the system as a whole. Modeling these\ninfluences can also shed light on patterns and relationships that would\notherwise be difficult to discern, hopefully leading to more transparency in\nhow the system generates personalized content. In this work we first formalize\nthe notion of \"influence\" in collaborative filtering using an Influence\nDiscrimination Model. We then empirically identify and characterize influential\nusers and analyze their impact on the system under different underlying\nrecommendation algorithms and across three different recommendation domains:\njob, movie and book recommendations. Insights from these experiments can help\nin designing systems that are not only optimized for accuracy, but are also\ntuned to mitigate the impact of influential users when it might lead to\npotential imbalance or unfairness in the system's outcomes.",
    "published_date": "2019-05-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.08031v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.05786v2",
    "title": "Software Engineering for Fairness: A Case Study with Hyperparameter Optimization",
    "authors": [
      "Joymallya Chakraborty",
      "Tianpei Xia",
      "Fahmid M. Fahid",
      "Tim Menzies"
    ],
    "author_ids": [],
    "abstract": "We assert that it is the ethical duty of software engineers to strive to\nreduce software discrimination. This paper discusses how that might be done.\nThis is an important topic since machine learning software is increasingly\nbeing used to make decisions that affect people's lives. Potentially, the\napplication of that software will result in fairer decisions because (unlike\nhumans) machine learning software is not biased. However, recent results show\nthat the software within many data mining packages exhibits \"group\ndiscrimination\"; i.e. their decisions are inappropriately affected by\n\"protected attributes\"(e.g., race, gender, age, etc.).\n  There has been much prior work on validating the fairness of machine-learning\nmodels (by recognizing when such software discrimination exists). But after\ndetection, comes mitigation. What steps can ethical software engineers take to\nreduce discrimination in the software they produce?\n  This paper shows that making \\textit{fairness} as a goal during\nhyperparameter optimization can (a) preserve the predictive power of a model\nlearned from a data miner while also (b) generates fairer results. To the best\nof our knowledge, this is the first application of hyperparameter optimization\nas a tool for software engineers to generate fairer software.",
    "published_date": "2019-05-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.05786v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.06995v1",
    "title": "Making ethical decisions for the immersive web",
    "authors": [
      "Diane Hosfelt"
    ],
    "author_ids": [],
    "abstract": "Mixed reality (MR) ethics occupies a space that intersects with web ethics,\nemerging tech ethics, healthcare ethics and product ethics (among others). This\npaper focuses on how we can build an immersive web that encourages ethical\ndevelopment and usage. The technology is beyond emerging (footnote: generally,\nthe ethics of emerging technologies are focused on ethical assessments of\nresearch and innovation), but not quite entrenched. We're still in a position\nto intervene in the development process, instead of attempting to retrofit\nethical decisions into an established design. While we have a wider range of\ndata to analyze than most emerging technologies, we're still in a much more\nspeculative state than entrenched technologies. This space is a challenge and\nan opportunity.",
    "published_date": "2019-05-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06995v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.05498v5",
    "title": "Bias-Reduced Hindsight Experience Replay with Virtual Goal Prioritization",
    "authors": [
      "Binyamin Manela",
      "Armin Biess"
    ],
    "author_ids": [],
    "abstract": "Hindsight Experience Replay (HER) is a multi-goal reinforcement learning\nalgorithm for sparse reward functions. The algorithm treats every failure as a\nsuccess for an alternative (virtual) goal that has been achieved in the\nepisode. Virtual goals are randomly selected, irrespective of which are most\ninstructive for the agent. In this paper, we present two improvements over the\nexisting HER algorithm. First, we prioritize virtual goals from which the agent\nwill learn more valuable information. We call this property the instructiveness\nof the virtual goal and define it by a heuristic measure, which expresses how\nwell the agent will be able to generalize from that virtual goal to actual\ngoals. Secondly, we reduce existing bias in HER by the removal of misleading\nsamples. To test our algorithms, we built two challenging environments with\nsparse reward functions. Our empirical results in both environments show vast\nimprovement in the final success rate and sample efficiency when compared to\nthe original HER algorithm. A video showing experimental results is available\nat https://youtu.be/3cZwfK8Nfps .",
    "published_date": "2019-05-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.05498v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.05286v1",
    "title": "Friendship Paradox Biases Perceptions in Directed Networks",
    "authors": [
      "Nazanin Alipourfard",
      "Buddhika Nettasinghe",
      "Andres Abeliuk",
      "Vikram Krishnamurthy",
      "Kristina Lerman"
    ],
    "author_ids": [],
    "abstract": "How popular a topic or an opinion appears to be in a network can be very\ndifferent from its actual popularity. For example, in an online network of a\nsocial media platform, the number of people who mention a topic in their\nposts---i.e., its global popularity---can be dramatically different from how\npeople see it in their social feeds---i.e., its perceived popularity---where\nthe feeds aggregate their friends' posts. We trace the origin of this\ndiscrepancy to the friendship paradox in directed networks, which states that\npeople are less popular than their friends (or followers) are, on average. We\nidentify conditions on network structure that give rise to this perception\nbias, and validate the findings empirically using data from Twitter. Within\nmessages posted by Twitter users in our sample, we identify topics that appear\nmore frequently within the users' social feeds, than they do globally, i.e.,\namong all posts. In addition, we present a polling algorithm that leverages the\nfriendship paradox to obtain a statistically efficient estimate of a topic's\nglobal prevalence from biased perceptions of individuals. We characterize the\nbias of the polling estimate, provide an upper bound for its variance, and\nvalidate the algorithm's efficiency through synthetic polling experiments on\nour Twitter data. Our paper elucidates the non-intuitive ways in which the\nstructure of directed networks can distort social perceptions and resulting\nbehaviors.",
    "published_date": "2019-05-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.05286v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.04910v2",
    "title": "The Price of Fairness for Indivisible Goods",
    "authors": [
      "Xiaohui Bei",
      "Xinhang Lu",
      "Pasin Manurangsi",
      "Warut Suksompong"
    ],
    "author_ids": [],
    "abstract": "We investigate the efficiency of fair allocations of indivisible goods using\nthe well-studied price of fairness concept. Previous work has focused on\nclassical fairness notions such as envy-freeness, proportionality, and\nequitability. However, these notions cannot always be satisfied for indivisible\ngoods, leading to certain instances being ignored in the analysis. In this\npaper, we focus instead on notions with guaranteed existence, including\nenvy-freeness up to one good (EF1), balancedness, maximum Nash welfare (MNW),\nand leximin. We also introduce the concept of strong price of fairness, which\ncaptures the efficiency loss in the worst fair allocation as opposed to that in\nthe best fair allocation as in the price of fairness. We mostly provide tight\nor asymptotically tight bounds on the worst-case efficiency loss for\nallocations satisfying these notions, for both the price of fairness and the\nstrong price of fairness.",
    "published_date": "2019-05-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.04910v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.04854v2",
    "title": "DotSCN: Group Re-identification via Domain-Transferred Single and Couple Representation Learning",
    "authors": [
      "Ziling Huang",
      "Zheng Wang",
      "Chung-Chi Tsai",
      "Shin'ichi Satoh",
      "Chia-Wen Lin"
    ],
    "author_ids": [],
    "abstract": "Group re-identification (G-ReID) is an important yet less-studied task. Its\nchallenges not only lie in appearance changes of individuals which have been\nwell-investigated in general person re-identification (ReID), but also derive\nfrom group layout and membership changes. So the key task of G-ReID is to learn\nrepresentations robust to such changes. To address this issue, we propose a\nTransferred Single and Couple Representation Learning Network (TSCN). Its\nmerits are two aspects: 1) Due to the lack of labelled training samples,\nexisting G-ReID methods mainly rely on unsatisfactory hand-crafted features. To\ngain the superiority of deep learning models, we treat a group as multiple\npersons and transfer the domain of a labeled ReID dataset to a G-ReID target\ndataset style to learn single representations. 2) Taking into account the\nneighborhood relationship in a group, we further propose learning a novel\ncouple representation between two group members, that achieves more\ndiscriminative power in G-ReID tasks. In addition, an unsupervised weight\nlearning method is exploited to adaptively fuse the results of different views\ntogether according to result patterns. Extensive experimental results\ndemonstrate the effectiveness of our approach that significantly outperforms\nstate-of-the-art methods by 11.7\\% CMC-1 on the Road Group dataset and by\n39.0\\% CMC-1 on the DukeMCMT dataset.",
    "published_date": "2019-05-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.04854v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.04749v2",
    "title": "A Benchmark Study of Machine Learning Models for Online Fake News Detection",
    "authors": [
      "Junaed Younus Khan",
      "Md. Tawkat Islam Khondaker",
      "Sadia Afroz",
      "Gias Uddin",
      "Anindya Iqbal"
    ],
    "author_ids": [],
    "abstract": "The proliferation of fake news and its propagation on social media has become\na major concern due to its ability to create devastating impacts. Different\nmachine learning approaches have been suggested to detect fake news. However,\nmost of those focused on a specific type of news (such as political) which\nleads us to the question of dataset-bias of the models used. In this research,\nwe conducted a benchmark study to assess the performance of different\napplicable machine learning approaches on three different datasets where we\naccumulated the largest and most diversified one. We explored a number of\nadvanced pre-trained language models for fake news detection along with the\ntraditional and deep learning ones and compared their performances from\ndifferent aspects for the first time to the best of our knowledge. We find that\nBERT and similar pre-trained models perform the best for fake news detection,\nespecially with very small dataset. Hence, these models are significantly\nbetter option for languages with limited electronic contents, i.e., training\ndata. We also carried out several analysis based on the models' performance,\narticle's topic, article's length, and discussed different lessons learned from\nthem. We believe that this benchmark study will help the research community to\nexplore further and news sites/blogs to select the most appropriate fake news\ndetection method.",
    "published_date": "2019-05-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.04749v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.04438v1",
    "title": "Group Fairness in Committee Selection",
    "authors": [
      "Yu Cheng",
      "Zhihao Jiang",
      "Kamesh Munagala",
      "Kangning Wang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study fairness in committee selection problems. We consider\na general notion of fairness via stability: A committee is stable if no\ncoalition of voters can deviate and choose a committee of proportional size, so\nthat all these voters strictly prefer the new committee to the existing one.\nOur main contribution is to extend this definition to stability of a\ndistribution (or lottery) over committees. We consider two canonical voter\npreference models: the Approval Set setting where each voter approves a set of\ncandidates and prefers committees with larger intersection with this set; and\nthe Ranking setting where each voter ranks committees based on how much she\nlikes her favorite candidate in a committee. Our main result is to show that\nstable lotteries always exist for these canonical preference models.\nInterestingly, given preferences of voters over committees, the procedure for\ncomputing an approximately stable lottery is the same for both models and\ntherefore extends to the setting where some voters have the former preference\nstructure and others have the latter. Our existence proof uses the\nprobabilistic method and a new large deviation inequality that may be of\nindependent interest.",
    "published_date": "2019-05-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.04438v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.06134v1",
    "title": "Recommending Dream Jobs in a Biased Real World",
    "authors": [
      "Nadia Fawaz"
    ],
    "author_ids": [],
    "abstract": "Machine learning models learn what we teach them to learn. Machine learning\nis at the heart of recommender systems. If a machine learning model is trained\non biased data, the resulting recommender system may reflect the biases in its\nrecommendations. Biases arise at different stages in a recommender system, from\nexisting societal biases in the data such as the professional gender gap, to\nbiases introduced by the data collection or modeling processes. These biases\nimpact the performance of various components of recommender systems, from\noffline training, to evaluation and online serving of recommendations in\nproduction systems. Specific techniques can help reduce bias at each stage of a\nrecommender system. Reducing bias in our recommender systems is crucial to\nsuccessfully recommending dream jobs to hundreds of millions members worldwide,\nwhile being true to LinkedIn's vision: \"To create economic opportunity for\nevery member of the global workforce\".",
    "published_date": "2019-05-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06134v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.04337v1",
    "title": "Learning in structured MDPs with convex cost functions: Improved regret bounds for inventory management",
    "authors": [
      "Shipra Agrawal",
      "Randy Jia"
    ],
    "author_ids": [],
    "abstract": "We consider a stochastic inventory control problem under censored demands,\nlost sales, and positive lead times. This is a fundamental problem in inventory\nmanagement, with significant literature establishing near-optimality of a\nsimple class of policies called ``base-stock policies'' for the underlying\nMarkov Decision Process (MDP), as well as convexity of long run average-cost\nunder those policies. We consider the relatively less studied problem of\ndesigning a learning algorithm for this problem when the underlying demand\ndistribution is unknown. The goal is to bound regret of the algorithm when\ncompared to the best base-stock policy. We utilize the convexity properties and\na newly derived bound on bias of base-stock policies to establish a connection\nto stochastic convex bandit optimization.\n  Our main contribution is a learning algorithm with a regret bound of\n$\\tilde{O}(L\\sqrt{T}+D)$ for the inventory control problem. Here $L$ is the\nfixed and known lead time, and $D$ is an unknown parameter of the demand\ndistribution described roughly as the number of time steps needed to generate\nenough demand for depleting one unit of inventory. Notably, even though the\nstate space of the underlying MDP is continuous and $L$-dimensional, our regret\nbounds depend linearly on $L$. Our results significantly improve the previously\nbest known regret bounds for this problem where the dependence on $L$ was\nexponential and many further assumptions on demand distribution were required.\nThe techniques presented here may be of independent interest for other settings\nthat involve large structured MDPs but with convex cost functions.",
    "published_date": "2019-05-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.04337v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.03488v2",
    "title": "Projections onto the canonical simplex with additional linear inequalities",
    "authors": [
      "L. Adam",
      "V. Mácha"
    ],
    "author_ids": [],
    "abstract": "We consider the distributionally robust optimization and show that computing\nthe distributional worst-case is equivalent to computing the projection onto\nthe canonical simplex with additional linear inequality. We consider several\ndistance functions to measure the distance of distributions. We write the\nprojections as optimization problems and show that they are equivalent to\nfinding a zero of real-valued functions. We prove that these functions possess\nnice properties such as monotonicity or convexity. We design optimization\nmethods with guaranteed convergence and derive their theoretical complexity. We\ndemonstrate that our methods have (almost) linear observed complexity.",
    "published_date": "2019-05-09T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03488v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.03403v1",
    "title": "Fairness across Network Positions in Cyberbullying Detection Algorithms",
    "authors": [
      "Vivek Singh",
      "Connor Hofenbitzer"
    ],
    "author_ids": [],
    "abstract": "Cyberbullying, which often has a deeply negative impact on the victim, has\ngrown as a serious issue in Online Social Networks. Recently, researchers have\ncreated automated machine learning algorithms to detect Cyberbullying using\nsocial and textual features. However, the very algorithms that are intended to\nfight off one threat (cyberbullying) may inadvertently be falling prey to\nanother important threat (bias of the automatic detection algorithms). This is\nexacerbated by the fact that while the current literature on algorithmic\nfairness has multiple empirical results, metrics, and algorithms for countering\nbias across immediately observable demographic characteristics (e.g. age, race,\ngender), there have been no efforts at empirically quantifying the variation in\nalgorithmic performance based on the network role or position of individuals.\nWe audit an existing cyberbullying algorithm using Twitter data for disparity\nin detection performance based on the network centrality of the potential\nvictim and then demonstrate how this disparity can be countered using an\nEqualized Odds post-processing technique. The results pave way for more\naccurate and fair cyberbullying detection algorithms.",
    "published_date": "2019-05-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03403v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.03314v1",
    "title": "Entrofy Your Cohort: A Data Science Approach to Candidate Selection",
    "authors": [
      "D. Huppenkothen",
      "B. McFee",
      "L. Norén"
    ],
    "author_ids": [],
    "abstract": "Selecting a cohort from a set of candidates is a common task within and\nbeyond academia. Admitting students, awarding grants, choosing speakers for a\nconference are situations where human biases may affect the make-up of the\nfinal cohort. We propose a new algorithm, Entrofy, designed to be part of a\nlarger decision making strategy aimed at making cohort selection as just,\nquantitative, transparent, and accountable as possible. We suggest this\nalgorithm be embedded in a two-step selection procedure. First, all application\nmaterials are stripped of markers of identity that could induce conscious or\nsub-conscious bias. During blind review, the committee selects all applicants,\nsubmissions, or other entities that meet their merit-based criteria. This often\nyields a cohort larger than the admissible number. In the second stage, the\ntarget cohort can be chosen from this meritorious pool via a new algorithm and\nsoftware tool. Entrofy optimizes differences across an assignable set of\ncategories selected by the human committee. Criteria could include gender,\nacademic discipline, experience with certain technologies, or other\nquantifiable characteristics. The Entrofy algorithm yields the computational\nmaximization of diversity by solving the tie-breaking problem with provable\nperformance guarantees. We show how Entrofy selects cohorts according to\npre-determined characteristics in simulated sets of applications and\ndemonstrate its use in a case study. This cohort selection process allows human\njudgment to prevail when assessing merit, but assigns the assessment of\ndiversity to a computational process less likely to be beset by human bias.\nImportantly, the stage at which diversity assessments occur is fully\ntransparent and auditable with Entrofy. Splitting merit and diversity\nconsiderations into their own assessment stages makes it easier to explain why\na given candidate was selected or rejected.",
    "published_date": "2019-05-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "astro-ph.IM",
      "physics.ed-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03314v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.03015v1",
    "title": "An Entropy Power Inequality for Discrete Random Variables",
    "authors": [
      "Ehsan Nekouei",
      "Mikael Skoglund",
      "Karl Henrik Johansson"
    ],
    "author_ids": [],
    "abstract": "Let $\\mathsf{N}_{\\rm d}\\left[X\\right]=\\frac{1}{2\\pi {\\rm e}}{\\rm\ne}^{2\\mathsf{H}\\left[X\\right]}$ denote the entropy power of the discrete random\nvariable $X$ where $\\mathsf{H}\\left[X\\right]$ denotes the discrete entropy of\n$X$. In this paper, we show that for two independent discrete random variables\n$X$ and $Y$, the entropy power inequality $\\mathsf{N}_{\\rm\nd}\\left[X\\right]+\\mathsf{N}_{\\rm d}\\left[Y\\right]\\leq 2 \\mathsf{N}_{\\rm\nd}\\left[X+Y\\right]$ holds and it can be tight. The basic idea behind the proof\nis to perturb the discrete random variables using suitably designed continuous\nrandom variables. Then, the continuous entropy power inequality is applied to\nthe sum of the perturbed random variables and the resulting lower bound is\noptimized.",
    "published_date": "2019-05-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03015v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.02941v1",
    "title": "Robust Federated Training via Collaborative Machine Teaching using Trusted Instances",
    "authors": [
      "Yufei Han",
      "Xiangliang Zhang"
    ],
    "author_ids": [],
    "abstract": "Federated learning performs distributed model training using local data\nhosted by agents. It shares only model parameter updates for iterative\naggregation at the server. Although it is privacy-preserving by design,\nfederated learning is vulnerable to noise corruption of local agents, as\ndemonstrated in the previous study on adversarial data poisoning threat against\nfederated learning systems. Even a single noise-corrupted agent can bias the\nmodel training. In our work, we propose a collaborative and privacy-preserving\nmachine teaching paradigm with multiple distributed teachers, to improve\nrobustness of the federated training process against local data corruption. We\nassume that each local agent (teacher) have the resources to verify a small\nportions of trusted instances, which may not by itself be adequate for\nlearning. In the proposed collaborative machine teaching method, these trusted\ninstances guide the distributed agents to jointly select a compact while\ninformative training subset from data hosted by their own. Simultaneously, the\nagents learn to add changes of limited magnitudes into the selected data\ninstances, in order to improve the testing performances of the federally\ntrained model despite of the training data corruption. Experiments on toy and\nreal data demonstrate that our approach can identify training set bugs\neffectively and suggest appropriate changes to the labels. Our algorithm is a\nstep toward trustworthy machine learning.",
    "published_date": "2019-05-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02941v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.02813v1",
    "title": "Fixing Inclusivity Bugs for Information Processing Styles and Learning Styles",
    "authors": [
      "Zoe Steine-Hanson",
      "Claudia Hilderbrand",
      "Lara Letaw",
      "Jillian Emard",
      "Christopher Perdriau",
      "Christopher Mendez",
      "Margaret Burnett",
      "Anita Sarma"
    ],
    "author_ids": [],
    "abstract": "Most software systems today do not support cognitive diversity. Further,\nbecause of differences in problem-solving styles that cluster by gender,\nsoftware that poorly supports cognitive diversity can also embed gender biases.\nTo help software professionals fix gender bias \"bugs\" related to people's\nproblem-solving styles for information processing and learning of new software\nwe collected inclusivity fixes from three sources. The first two are empirical\nstudies we conducted: a heuristics-driven user study and a field research\nindustry study. The third is data that we obtained about a before/after user\nstudy of inclusivity bugs. The resulting seven potential inclusivity fixes show\nhow to debug software to be more inclusive for diverse problem-solving styles.",
    "published_date": "2019-05-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02813v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.03857v1",
    "title": "FASS: A Fairness-Aware Approach for Concurrent Service Selection with Constraints",
    "authors": [
      "Songyuan Li",
      "Jiwei Huang",
      "Bo Cheng",
      "Lizhen Cui",
      "Yuliang Shi"
    ],
    "author_ids": [],
    "abstract": "The increasing momentum of service-oriented architecture has led to the\nemergence of divergent delivered services, where service selection is meritedly\nrequired to obtain the target service fulfilling the requirements from both\nusers and service providers. Despite many existing works have extensively\nhandled the issue of service selection, it remains an open question in the case\nwhere requests from multiple users are performed simultaneously by a certain\nset of shared candidate services. Meanwhile, there exist some constraints\nenforced on the context of service selection, e.g. service placement location\nand contracts between users and service providers. In this paper, we focus on\nthe QoS-aware service selection with constraints from a fairness aspect, with\nthe objective of achieving max-min fairness across multiple service requests\nsharing candidate service sets. To be more specific, we study the problem of\nfairly selecting services from shared candidate sets while service providers\nare self-motivated to offer better services with higher QoS values. We\nformulate this problem as a lexicographical maximization problem, which is far\nfrom trivial to deal with practically due to its inherently multi-objective and\ndiscrete nature. A fairness-aware algorithm for concurrent service selection\n(FASS) is proposed, whose basic idea is to iteratively solve the\nsingle-objective subproblems by transforming them into linear programming\nproblems. Experimental results based on real-world datasets also validate the\neffectiveness and practicality of our proposed approach.",
    "published_date": "2019-05-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03857v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.02535v1",
    "title": "F-measure Maximizing Logistic Regression",
    "authors": [
      "Masaaki Okabe",
      "Jun Tsuchida",
      "Hiroshi Yadohisa"
    ],
    "author_ids": [],
    "abstract": "Logistic regression is a widely used method in several fields. When applying\nlogistic regression to imbalanced data, for which majority classes dominate\nover minority classes, all class labels are estimated as `majority class.' In\nthis article, we use an F-measure optimization method to improve the\nperformance of logistic regression applied to imbalanced data. While many\nF-measure optimization methods adopt a ratio of the estimators to approximate\nthe F-measure, the ratio of the estimators tends to have more bias than when\nthe ratio is directly approximated. Therefore, we employ an approximate\nF-measure for estimating the relative density ratio. In addition, we define a\nrelative F-measure and approximate the relative F-measure. We show an algorithm\nfor a logistic regression weighted approximated relative to the F-measure. The\nexperimental results using real world data demonstrated that our proposed\nmethod is an efficient algorithm to improve the performance of logistic\nregression applied to imbalanced data.",
    "published_date": "2019-05-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02535v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.02366v3",
    "title": "What Do People See in a Twenty-Second Glimpse of Bivariate Vector Field Visualizations?",
    "authors": [
      "Henan Zhao",
      "Garnett W. Bryant",
      "Wesley Griffin",
      "Judith E. Terrill",
      "Jian Chen"
    ],
    "author_ids": [],
    "abstract": "Little is known about how people learn from a brief glimpse of\nthree-dimensional (3D) bivariate vector field visualizations and about how well\nvisual features can guide behavior. Here we report empirical study results on\nthe use of color, texture, and length to guide viewing of bivariate glyphs:\nthese three visual features are mapped to the first integer variable (v1) and\nlength to the second quantitative variable (v2). Participants performed two\ntasks within 20 seconds: (1) MAX: find the largest v2 when v1 is fixed; (2)\nSEARCH: find a specific bivariate variable shown on the screen in a vector\nfield. Our first study with eighteen participants performing these tasks showed\nthat the randomized vector positions, although they lessened viewers' ability\nto group vectors, did not reduce task accuracy compared to structured vector\nfields. This result may support that these color, texture, and length can\nprovide to a certain degree, guide viewers' attention to task-relevant regions.\nThe second study measured eye movement to quantify viewers' behaviors with\nthree-errors (scanning, recognition, and decision errors) and one-behavior\n(refixation) metrics. Our results showed two dominant search strategies:\ndrilling and scanning. Coloring tended to restrict eye movement to the\ntask-relevant regions of interest, enabling drilling. Length tended to support\nscanners who quickly wandered around at different v1 levels. Drillers had\nsignificantly less errors than scanners and the error rates for color and\ntexture were also lowest. And length had limited discrimination power than\ncolor and texture as a 3D visual guidance. Our experiment results may suggest\nthat using categorical visual feature could help obtain the global structure of\na vector field visualization. We provide the first benchmark of the attention\ncost of seeing a bivariate vector on average about 5 items per second.",
    "published_date": "2019-05-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02366v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.02272v1",
    "title": "On bias in social reviews of university courses",
    "authors": [
      "Taha Hassan"
    ],
    "author_ids": [],
    "abstract": "University course ranking forums are a popular means of disseminating\ninformation about satisfaction with the quality of course content and\ninstruction, especially with undergraduate students. A variety of policy\ndecisions by university administrators, instructional designers and teaching\nstaff affect how students perceive the efficacy of pedagogies employed in a\ngiven course, in class and online. While there is a large body of research on\nqualitative driving factors behind the use of academic rating sites, there is\nlittle investigation of the (potential) implicit student bias on said forums\ntowards desirable course outcomes at the institution level. To that end, we\nexamine the connection between course outcomes (student-reported GPA) and the\noverall ranking of the primary course instructor, as well as rating disparity\nby nature of course outcomes, for several hundred courses taught at Virginia\nTech based on data collected from a popular academic rating forum. We also\nreplicate our analysis for several public universities across the US. Our\nexperiments indicate that there is a discernible albeit complex bias towards\ncourse outcomes in the professor ratings registered by students.",
    "published_date": "2019-05-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02272v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.02693v4",
    "title": "3D Packing for Self-Supervised Monocular Depth Estimation",
    "authors": [
      "Vitor Guizilini",
      "Rares Ambrus",
      "Sudeep Pillai",
      "Allan Raventos",
      "Adrien Gaidon"
    ],
    "author_ids": [],
    "abstract": "Although cameras are ubiquitous, robotic platforms typically rely on active\nsensors like LiDAR for direct 3D perception. In this work, we propose a novel\nself-supervised monocular depth estimation method combining geometry with a new\ndeep network, PackNet, learned only from unlabeled monocular videos. Our\narchitecture leverages novel symmetrical packing and unpacking blocks to\njointly learn to compress and decompress detail-preserving representations\nusing 3D convolutions. Although self-supervised, our method outperforms other\nself, semi, and fully supervised methods on the KITTI benchmark. The 3D\ninductive bias in PackNet enables it to scale with input resolution and number\nof parameters without overfitting, generalizing better on out-of-domain data\nsuch as the NuScenes dataset. Furthermore, it does not require large-scale\nsupervised pretraining on ImageNet and can run in real-time. Finally, we\nrelease DDAD (Dense Depth for Automated Driving), a new urban driving dataset\nwith more challenging and accurate depth evaluation, thanks to longer-range and\ndenser ground-truth depth generated from high-density LiDARs mounted on a fleet\nof self-driving cars operating world-wide.",
    "published_date": "2019-05-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02693v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.01788v1",
    "title": "Statistically Discriminative Sub-trajectory Mining",
    "authors": [
      "Vo Nguyen Le Duy",
      "Takuto Sakuma",
      "Taiju Ishiyama",
      "Hiroki Toda",
      "Kazuya Nishi",
      "Masayuki Karasuyama",
      "Yuta Okubo",
      "Masayuki Sunaga",
      "Yasuo Tabei",
      "Ichiro Takeuchi"
    ],
    "author_ids": [],
    "abstract": "We study the problem of discriminative sub-trajectory mining. Given two\ngroups of trajectories, the goal of this problem is to extract moving patterns\nin the form of sub-trajectories which are more similar to sub-trajectories of\none group and less similar to those of the other. We propose a new method\ncalled Statistically Discriminative Sub-trajectory Mining (SDSM) for this\nproblem. An advantage of the SDSM method is that the statistical significance\nof the extracted sub-trajectories are properly controlled in the sense that the\nprobability of finding a false positive sub-trajectory is smaller than a\nspecified significance threshold alpha (e.g., 0.05), which is indispensable\nwhen the method is used in scientific or social studies under noisy\nenvironment. Finding such statistically discriminative sub-trajectories from\nmassive trajectory dataset is both computationally and statistically\nchallenging. In the SDSM method, we resolve the difficulties by introducing a\ntree representation among sub-trajectories and running an efficient\npermutation-based statistical inference method on the tree. To the best of our\nknowledge, SDSM is the first method that can efficiently extract statistically\ndiscriminative sub-trajectories from massive trajectory dataset. We illustrate\nthe effectiveness and scalability of the SDSM method by applying it to a\nreal-world dataset with 1,000,000 trajectories which contains 16,723,602,505\nsub-trajectories.",
    "published_date": "2019-05-06T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01788v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.01778v1",
    "title": "Same Influenza, Different Responses: Social Media Can Sense a Regional Spectrum of Symptoms",
    "authors": [
      "Siqing Shan",
      "Yingwei Jia",
      "Jichang Zhao"
    ],
    "author_ids": [],
    "abstract": "Influenza is an acute respiratory infection caused by a virus. It is highly\ncontagious and rapidly mutative. However, its epidemiological characteristics\nare conventionally collected in terms of outpatient records. In fact, the\nsubjective bias of the doctor emphasizes exterior signs, and the necessity of\nface-to-face inquiry results in an inaccurate and time-consuming manner of data\ncollection and aggregation. Accordingly, the inferred spectrum of syndromes can\nbe incomplete and lagged. With a massive number of users being sensors, online\nsocial media can indeed provide an alternative approach. Voluntary reports in\nTwitter and its variants can deliver not only exterior signs but also interior\nfeelings such as emotions. These sophisticated signals can further be\nefficiently collected and aggregated in a real-time manner, and a comprehensive\nspectrum of syndromes could thus be inferred. Taking Weibo as an example, it is\nconfirmed that a regional spectrum of symptoms can be credibly sensed. Aside\nfrom the differences in symptoms and treatment incentives between northern and\nsouthern China, it is also surprising that patients in the south are more\noptimistic, while those in the north demonstrate more intense emotions. The\ndifferences sensed from Weibo can even help improve the performance of\nregressions in monitoring influenza. Our results suggest that self-reports from\nsocial media can be profound supplements to the existing clinic-based systems\nfor influenza surveillance.",
    "published_date": "2019-05-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01778v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.01670v1",
    "title": "A Polynomial Time Algorithm for Fair Resource Allocation in Resource Exchange",
    "authors": [
      "Xiang Yan",
      "Wei Zhu"
    ],
    "author_ids": [],
    "abstract": "The rapid growth of wireless and mobile Internet has led to wide applications\nof exchanging resources over network, in which how to fairly allocate resources\nhas become a critical challenge. To motivate sharing, a BD Mechanism is\nproposed for resource allocation, which is based on a combinatorial structure\ncalled bottleneck decomposition. The mechanism has been shown with properties\nof fairness, economic efficiency [17], and truthfulness against two kinds of\nstrategic behaviors [2,3]. Unfortunately, the crux on how to compute a\nbottleneck decomposition of any graph is remain untouched. In this paper, we\nfocus on the computation of bottleneck decomposition to fill the blanks and\nprove that the bottleneck decomposition of a network $G = (V, E; w_v)$ can be\ncomputed in $O(n^6 log(nU))$, where $n = |V|$ and $U = max_{v\\in V} w_v$. Based\non the bottleneck decomposition, a fair allocation in resource exchange system\ncan be obtained in polynomial time. In addition, our work completes the\ncomputation of a market equilibrium and its relationship to two concepts of\nfairness in resource exchange.",
    "published_date": "2019-05-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01670v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.01574v1",
    "title": "A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling",
    "authors": [
      "Qi Wang",
      "Junyu Gao",
      "Yuan Yuan"
    ],
    "author_ids": [],
    "abstract": "Street scene understanding is an essential task for autonomous driving. One\nimportant step towards this direction is scene labeling, which annotates each\npixel in the images with a correct class label. Although many approaches have\nbeen developed, there are still some weak points. Firstly, many methods are\nbased on the hand-crafted features whose image representation ability is\nlimited. Secondly, they can not label foreground objects accurately due to the\ndataset bias. Thirdly, in the refinement stage, the traditional Markov Random\nFiled (MRF) inference is prone to over smoothness. For improving the above\nproblems, this paper proposes a joint method of priori convolutional neural\nnetworks at superpixel level (called as ``priori s-CNNs'') and soft restricted\ncontext transfer. Our contributions are threefold: (1) A priori s-CNNs model\nthat learns priori location information at superpixel level is proposed to\ndescribe various objects discriminatingly; (2) A hierarchical data augmentation\nmethod is presented to alleviate dataset bias in the priori s-CNNs training\nstage, which improves foreground objects labeling significantly; (3) A soft\nrestricted MRF energy function is defined to improve the priori s-CNNs model's\nlabeling performance and reduce the over smoothness at the same time. The\nproposed approach is verified on CamVid dataset (11 classes) and SIFT Flow\nStreet dataset (16 classes) and achieves competitive performance.",
    "published_date": "2019-05-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01574v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.06417v4",
    "title": "Ethically Aligned Design: An empirical evaluation of the RESOLVEDD-strategy in Software and Systems development context",
    "authors": [
      "Ville Vakkuri",
      "Kai-Kristian Kemell",
      "Pekka Abrahamsson"
    ],
    "author_ids": [],
    "abstract": "Use of artificial intelligence (AI) in human contexts calls for ethical\nconsiderations for the design and development of AI-based systems. However,\nlittle knowledge currently exists on how to provide useful and tangible tools\nthat could help software developers and designers implement ethical\nconsiderations into practice. In this paper, we empirically evaluate a method\nthat enables ethically aligned design in a decision-making process. Though this\nmethod, titled the RESOLVEDD-strategy, originates from the field of business\nethics, it is being applied in other fields as well. We tested the\nRESOLVEDD-strategy in a multiple case study of five student projects where the\nuse of ethical tools was given as one of the design requirements. A key finding\nfrom the study indicates that simply the presence of an ethical tool has an\neffect on ethical consideration, creating more responsibility even in instances\nwhere the use of the tool is not intrinsically motivated.",
    "published_date": "2019-05-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.06417v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.01351v1",
    "title": "In Defense of Synthetic Data",
    "authors": [
      "Luke Rodriguez",
      "Bill Howe"
    ],
    "author_ids": [],
    "abstract": "Synthetic datasets have long been thought of as second-rate, to be used only\nwhen \"real\" data collected directly from the real world is unavailable. But\nthis perspective assumes that raw data is clean, unbiased, and trustworthy,\nwhich it rarely is. Moreover, the benefits of synthetic data for privacy and\nfor bias correction are becoming increasingly important in any domain that\nworks with people. Curated synthetic datasets - synthetic data derived from\nminimal perturbations of real data - enable early stage product development and\ncollaboration, protect privacy, afford reproducibility, increase dataset\ndiversity in research, and protect disadvantaged groups from problematic\ninferences on the original data that reflects systematic discrimination. Rather\nthan representing a departure from the true state of the world, in this paper\nwe argue that properly generated synthetic data is a step towards responsible\nand equitable research and development of machine learning systems.",
    "published_date": "2019-05-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01351v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.01347v2",
    "title": "Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets",
    "authors": [
      "Chris Dulhanty",
      "Alexander Wong"
    ],
    "author_ids": [],
    "abstract": "The ImageNet dataset ushered in a flood of academic and industry interest in\ndeep learning for computer vision applications. Despite its significant impact,\nthere has not been a comprehensive investigation into the demographic\nattributes of images contained within the dataset. Such a study could lead to\nnew insights on inherent biases within ImageNet, particularly important given\nit is frequently used to pretrain models for a wide variety of computer vision\ntasks. In this work, we introduce a model-driven framework for the automatic\nannotation of apparent age and gender attributes in large-scale image datasets.\nUsing this framework, we conduct the first demographic audit of the 2012\nImageNet Large Scale Visual Recognition Challenge (ILSVRC) subset of ImageNet\nand the \"person\" hierarchical category of ImageNet. We find that 41.62% of\nfaces in ILSVRC appear as female, 1.71% appear as individuals above the age of\n60, and males aged 15 to 29 account for the largest subgroup with 27.11%. We\nnote that the presented model-driven framework is not fair for all\nintersectional groups, so annotation are subject to bias. We present this work\nas the starting point for future development of unbiased annotation models and\nfor the study of downstream effects of imbalances in the demographics of\nImageNet. Code and annotations are available at:\nhttp://bit.ly/ImageNetDemoAudit",
    "published_date": "2019-05-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01347v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.01304v1",
    "title": "Efficient Discrete Supervised Hashing for Large-scale Cross-modal Retrieval",
    "authors": [
      "Tao Yao",
      "Xiangwei Kong",
      "Lianshan Yan",
      "Wenjing Tang",
      "Qi Tian"
    ],
    "author_ids": [],
    "abstract": "Supervised cross-modal hashing has gained increasing research interest on\nlarge-scale retrieval task owning to its satisfactory performance and\nefficiency. However, it still has some challenging issues to be further\nstudied: 1) most of them fail to well preserve the semantic correlations in\nhash codes because of the large heterogenous gap; 2) most of them relax the\ndiscrete constraint on hash codes, leading to large quantization error and\nconsequent low performance; 3) most of them suffer from relatively high memory\ncost and computational complexity during training procedure, which makes them\nunscalable. In this paper, to address above issues, we propose a supervised\ncross-modal hashing method based on matrix factorization dubbed Efficient\nDiscrete Supervised Hashing (EDSH). Specifically, collective matrix\nfactorization on heterogenous features and semantic embedding with class labels\nare seamlessly integrated to learn hash codes. Therefore, the feature based\nsimilarities and semantic correlations can be both preserved in hash codes,\nwhich makes the learned hash codes more discriminative. Then an efficient\ndiscrete optimal algorithm is proposed to handle the scalable issue. Instead of\nlearning hash codes bit-by-bit, hash codes matrix can be obtained directly\nwhich is more efficient. Extensive experimental results on three public\nreal-world datasets demonstrate that EDSH produces a superior performance in\nboth accuracy and scalability over some existing cross-modal hashing methods.",
    "published_date": "2019-05-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.MM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01304v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.00873v1",
    "title": "Strong converse bounds in quantum network information theory: distributed hypothesis testing and source coding",
    "authors": [
      "Hao-Chung Cheng",
      "Nilanjana Datta",
      "Cambyse Rouzé"
    ],
    "author_ids": [],
    "abstract": "We consider a distributed quantum hypothesis testing problem with\ncommunication constraints, in which the two hypotheses correspond to two\ndifferent states of a bipartite quantum system, multiple identical copies of\nwhich are shared between Alice and Bob. They are allowed to perform local\noperations on their respective systems and send quantum information to Charlie\nat limited rates. By doing measurements on the systems that he receives,\nCharlie needs to infer which of the two different states the original bipartite\nstate was in, that is, which of the two hypotheses is true. We prove that the\nStein exponent for this problem is given by a regularized quantum relative\nentropy. The latter reduces to a single letter formula when the alternative\nhypothesis consists of the products of the marginals of the null hypothesis,\nand there is no rate constraint imposed on Bob. Our proof relies on certain\nproperties of the so-called quantum information bottleneck function.\n  The second part of this paper concerns the general problem of finding finite\nblocklength strong converse bounds in quantum network information theory. In\nthe classical case, the analogue of this problem has been reformulated in terms\nof the so-called image size characterization problem. Here, we extend this\nproblem to the classical-quantum setting and prove a second order strong\nconverse bound for it. As a by-product, we obtain a similar bound for the Stein\nexponent for distributed hypothesis testing in the special case in which the\nbipartite system is a classical-quantum system, as well as for the task of\nquantum source coding with compressed classical side information. Our proofs\nuse a recently developed tool from quantum functional inequalities, namely, the\ntensorization property of reverse hypercontractivity for the quantum\ndepolarizing semigroup.",
    "published_date": "2019-05-02T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00873v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.00724v2",
    "title": "KnowBias: A Novel AI Method to Detect Polarity in Online Content",
    "authors": [
      "Aditya Saligrama"
    ],
    "author_ids": [],
    "abstract": "We propose a novel training and inference method for detecting political bias\nin long text content such as newspaper opinion articles. Obtaining long text\ndata and annotations at sufficient scale for training is difficult, but it is\nrelatively easy to extract political polarity from tweets through their\nauthorship; as such, we train on tweets and perform inference on articles.\nUniversal sentence encoders and other existing methods that aim to address this\ndomain-adaptation scenario deliver inaccurate and inconsistent predictions on\narticles, which we show is due to a difference in opinion concentration between\ntweets and articles. We propose a two-step classification scheme that utilizes\na neutral detector trained on tweets to remove neutral sentences from articles\nin order to align opinion concentration and therefore improve accuracy on that\ndomain.\n  We evaluate our two-step approach using a variety of test suites, including a\nset of tweets and long-form articles where annotations were crowd-sourced to\ndecrease label noise, measuring accuracy and Spearman-rho rank correlation. In\npractice, KnowBias achieves a high accuracy of 86 (rho = 0.65) on these tweets\nand 75 (rho = 0.69) on long-form articles. While we validate our method on\npolitical bias, our scheme is general and can be readily applied to other\nsettings, where there exist such domain mismatches between source and target\ndomains. Our implementation is available for public use at https://knowbias.ml.",
    "published_date": "2019-05-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00724v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.02014v2",
    "title": "Noncommutative versions of inequalities in quantum information theory",
    "authors": [
      "Ali Dadkhah",
      "Mohammad Sal Moslehian",
      "Kenjiro Yanagi"
    ],
    "author_ids": [],
    "abstract": "In this paper, we aim to replace in the definitions of covariance and\ncorrelation the usual trace {\\rm Tr} by a tracial positive map between unital\n$C^*$-algebras and to replace the functions $x^{\\alpha}$ and $x^{1-\\alpha}$ by\nfunctions $f$ and $g$ satisfying some mild conditions. These allow us to define\nthe generalized covariance, the generalized variance, the generalized\ncorrelation and the generalized Wigner--Yanase--Dyson skew information related\nto the tracial positive maps and functions $f$ and $g$. We persent a\ngeneralization of Heisenberg's uncertainty relation in the noncommutative\nframework. We extend some inequalities and properties for the generalized\ncorrelation and the generalized Wigner--Yanase--Dyson skew information.\nFurthermore, we extend some inequalities for the generalized skew information\nsuch as uncertainty relation and the relation between the generalized variance\nand the generalized skew information.",
    "published_date": "2019-05-02T00:00:00",
    "year": 2019,
    "categories": [
      "math.OA",
      "cs.IT",
      "math.FA",
      "math.IT",
      "46L05, 47A63, 81P15"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02014v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.00603v2",
    "title": "A Mathematical Justification for Exponentially Distributed NLOS Bias",
    "authors": [
      "Christopher E. O'Lone",
      "Harpreet S. Dhillon",
      "R. Michael Buehrer"
    ],
    "author_ids": [],
    "abstract": "In the past few decades, the localization literature has seen many models\nattempting to characterize the non-line-of-sight (NLOS) bias error commonly\nexperienced in range measurements. These models have either been based on\nspecific measurement data or chosen due to attractive features of a particular\ndistribution, yet to date, none have been backed by rigorous analysis.\nLeveraging tools from stochastic geometry, this paper attempts to fill this\nvoid by providing the first analytical backing for an NLOS bias error model.\nUsing a Boolean model to statistically characterize the random locations,\norientations, and sizes of reflectors, and assuming first-order (i.e.,\nsingle-bounce) reflections, the distance traversed by the first-arriving NLOS\npath is characterized. Under these assumptions, this analysis reveals that NLOS\nbias exhibits an exponential form and can in fact be well approximated by an\nexponential distribution -- a result consistent with previous NLOS bias error\nmodels in the literature. This analytically derived distribution is then\ncompared to a common exponential model from the literature, revealing this\ndistribution to be a close match in some cases and a lower bound in others.\nLastly, the assumptions under which these results were derived suggest this\nmodel is aptly suited to characterize NLOS bias in 5G millimeter wave systems\nas well.",
    "published_date": "2019-05-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00603v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.00593v1",
    "title": "Directing DNNs Attention for Facial Attribution Classification using Gradient-weighted Class Activation Mapping",
    "authors": [
      "Xi Yang",
      "Bojian Wu",
      "Issei Sato",
      "Takeo Igarashi"
    ],
    "author_ids": [],
    "abstract": "Deep neural networks (DNNs) have a high accuracy on image classification\ntasks. However, DNNs trained by such dataset with co-occurrence bias may rely\non wrong features while making decisions for classification. It will greatly\naffect the transferability of pre-trained DNNs. In this paper, we propose an\ninteractive method to direct classifiers paying attentions to the regions that\nare manually specified by the users, in order to mitigate the influence of\nco-occurrence bias. We test on CelebA dataset, the pre-trained AlexNet is\nfine-tuned to focus on the specific facial attributes based on the results of\nGrad-CAM.",
    "published_date": "2019-05-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00593v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.00569v2",
    "title": "Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness",
    "authors": [
      "Xueru Zhang",
      "Mohammad Mahdi Khalili",
      "Cem Tekin",
      "Mingyan Liu"
    ],
    "author_ids": [],
    "abstract": "Machine Learning (ML) models trained on data from multiple demographic groups\ncan inherit representation disparity (Hashimoto et al., 2018) that may exist in\nthe data: the model may be less favorable to groups contributing less to the\ntraining process; this in turn can degrade population retention in these groups\nover time, and exacerbate representation disparity in the long run. In this\nstudy, we seek to understand the interplay between ML decisions and the\nunderlying group representation, how they evolve in a sequential framework, and\nhow the use of fairness criteria plays a role in this process. We show that the\nrepresentation disparity can easily worsen over time under a natural user\ndynamics (arrival and departure) model when decisions are made based on a\ncommonly used objective and fairness criteria, resulting in some groups\ndiminishing entirely from the sample pool in the long run. It highlights the\nfact that fairness criteria have to be defined while taking into consideration\nthe impact of decisions on user dynamics. Toward this end, we explain how a\nproper fairness criterion can be selected based on a general user dynamics\nmodel.",
    "published_date": "2019-05-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00569v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.00457v2",
    "title": "Truthful Aggregation of Budget Proposals",
    "authors": [
      "Rupert Freeman",
      "David M. Pennock",
      "Dominik Peters",
      "Jennifer Wortman Vaughan"
    ],
    "author_ids": [],
    "abstract": "We consider a participatory budgeting problem in which each voter submits a\nproposal for how to divide a single divisible resource (such as money or time)\namong several possible alternatives (such as public projects or activities) and\nthese proposals must be aggregated into a single aggregate division. Under\n$\\ell_1$ preferences -- for which a voter's disutility is given by the $\\ell_1$\ndistance between the aggregate division and the division he or she most prefers\n-- the social welfare-maximizing mechanism, which minimizes the average\n$\\ell_1$ distance between the outcome and each voter's proposal, is incentive\ncompatible (Goel et al. 2016). However, it fails to satisfy the natural\nfairness notion of proportionality, placing too much weight on majority\npreferences. Leveraging a connection between market prices and the generalized\nmedian rules of Moulin (1980), we introduce the independent markets mechanism,\nwhich is both incentive compatible and proportional. We unify the social\nwelfare-maximizing mechanism and the independent markets mechanism by defining\na broad class of moving phantom mechanisms that includes both. We show that\nevery moving phantom mechanism is incentive compatible. Finally, we\ncharacterize the social welfare-maximizing mechanism as the unique\nPareto-optimal mechanism in this class, suggesting an inherent tradeoff between\nPareto optimality and proportionality.",
    "published_date": "2019-05-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00457v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.00164v1",
    "title": "On a conditional inequality in Kolmogorov complexity and its applications in communication complexity",
    "authors": [
      "Andrei Romashchenko",
      "Marius Zimand"
    ],
    "author_ids": [],
    "abstract": "Romashchenko and Zimand~\\cite{rom-zim:c:mutualinfo} have shown that if we\npartition the set of pairs $(x,y)$ of $n$-bit strings into combinatorial\nrectangles, then $I(x:y) \\geq I(x:y \\mid t(x,y)) - O(\\log n)$, where $I$\ndenotes mutual information in the Kolmogorov complexity sense, and $t(x,y)$ is\nthe rectangle containing $(x,y)$. We observe that this inequality can be\nextended to coverings with rectangles which may overlap. The new inequality\nessentially states that in case of a covering with combinatorial rectangles,\n  $I(x:y) \\geq I(x:y \\mid t(x,y)) - \\log \\rho - O(\\log n)$, where $t(x,y)$ is\nany rectangle containing $(x,y)$ and $\\rho$ is the thickness of the covering,\nwhich is the maximum number of rectangles that overlap. We discuss applications\nto communication complexity of protocols that are nondeterministic, or\nrandomized, or Arthur-Merlin, and also to the information complexity of\ninteractive protocols.",
    "published_date": "2019-05-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CC",
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00164v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.00147v1",
    "title": "Fair Classification and Social Welfare",
    "authors": [
      "Lily Hu",
      "Yiling Chen"
    ],
    "author_ids": [],
    "abstract": "Now that machine learning algorithms lie at the center of many resource\nallocation pipelines, computer scientists have been unwittingly cast as partial\nsocial planners. Given this state of affairs, important questions follow. What\nis the relationship between fairness as defined by computer scientists and\nnotions of social welfare? In this paper, we present a welfare-based analysis\nof classification and fairness regimes. We translate a loss minimization\nprogram into a social welfare maximization problem with a set of implied\nwelfare weights on individuals and groups--weights that can be analyzed from a\ndistribution justice lens. In the converse direction, we ask what the space of\npossible labelings is for a given dataset and hypothesis class. We provide an\nalgorithm that answers this question with respect to linear hyperplanes in\n$\\mathbb{R}^d$ that runs in $O(n^dd)$. Our main findings on the relationship\nbetween fairness criteria and welfare center on sensitivity analyses of\nfairness-constrained empirical risk minimization programs. We characterize the\nranges of $\\Delta \\epsilon$ perturbations to a fairness parameter $\\epsilon$\nthat yield better, worse, and neutral outcomes in utility for individuals and\nby extension, groups. We show that applying more strict fairness criteria that\nare codified as parity constraints, can worsen welfare outcomes for both\ngroups. More generally, always preferring \"more fair\" classifiers does not\nabide by the Pareto Principle---a fundamental axiom of social choice theory and\nwelfare economics. Recent work in machine learning has rallied around these\nnotions of fairness as critical to ensuring that algorithmic systems do not\nhave disparate negative impact on disadvantaged social groups. By showing that\nthese constraints often fail to translate into improved outcomes for these\ngroups, we cast doubt on their effectiveness as a means to ensure justice.",
    "published_date": "2019-05-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00147v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.01989v3",
    "title": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search",
    "authors": [
      "Sahin Cem Geyik",
      "Stuart Ambler",
      "Krishnaram Kenthapadi"
    ],
    "author_ids": [],
    "abstract": "We present a framework for quantifying and mitigating algorithmic bias in\nmechanisms designed for ranking individuals, typically used as part of\nweb-scale search and recommendation systems. We first propose complementary\nmeasures to quantify bias with respect to protected attributes such as gender\nand age. We then present algorithms for computing fairness-aware re-ranking of\nresults. For a given search or recommendation task, our algorithms seek to\nachieve a desired distribution of top ranked results with respect to one or\nmore protected attributes. We show that such a framework can be tailored to\nachieve fairness criteria such as equality of opportunity and demographic\nparity depending on the choice of the desired distribution. We evaluate the\nproposed algorithms via extensive simulations over different parameter choices,\nand study the effect of fairness-aware ranking on both bias and utility\nmeasures. We finally present the online A/B testing results from applying our\nframework towards representative ranking in LinkedIn Talent Search, and discuss\nthe lessons learned in practice. Our approach resulted in tremendous\nimprovement in the fairness metrics (nearly three fold increase in the number\nof search queries with representative results) without affecting the business\nmetrics, which paved the way for deployment to 100% of LinkedIn Recruiter users\nworldwide. Ours is the first large-scale deployed framework for ensuring\nfairness in the hiring domain, with the potential positive impact for more than\n630M LinkedIn members.",
    "published_date": "2019-04-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01989v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.13341v1",
    "title": "Learning Fair Representations via an Adversarial Framework",
    "authors": [
      "Rui Feng",
      "Yang Yang",
      "Yuehan Lyu",
      "Chenhao Tan",
      "Yizhou Sun",
      "Chunping Wang"
    ],
    "author_ids": [],
    "abstract": "Fairness has become a central issue for our research community as\nclassification algorithms are adopted in societally critical domains such as\nrecidivism prediction and loan approval. In this work, we consider the\npotential bias based on protected attributes (e.g., race and gender), and\ntackle this problem by learning latent representations of individuals that are\nstatistically indistinguishable between protected groups while sufficiently\npreserving other information for classification. To do that, we develop a\nminimax adversarial framework with a generator to capture the data distribution\nand generate latent representations, and a critic to ensure that the\ndistributions across different protected groups are similar. Our framework\nprovides a theoretical guarantee with respect to statistical parity and\nindividual fairness. Empirical results on four real-world datasets also show\nthat the learned representation can effectively be used for classification\ntasks such as credit risk prediction while obstructing information related to\nprotected groups, especially when removing protected attributes is not\nsufficient for fair classification.",
    "published_date": "2019-04-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.13341v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.13335v3",
    "title": "Adversarial Balancing-based Representation Learning for Causal Effect Inference with Observational Data",
    "authors": [
      "Xin Du",
      "Lei Sun",
      "Wouter Duivesteijn",
      "Alexander Nikolaev",
      "Mykola Pechenizkiy"
    ],
    "author_ids": [],
    "abstract": "Learning causal effects from observational data greatly benefits a variety of\ndomains such as health care, education and sociology. For instance, one could\nestimate the impact of a new drug on specific individuals to assist the clinic\nplan and improve the survival rate. In this paper, we focus on studying the\nproblem of estimating Conditional Average Treatment Effect (CATE) from\nobservational data. The challenges for this problem are two-fold: on the one\nhand, we have to derive a causal estimator to estimate the causal quantity from\nobservational data, where there exists confounding bias; on the other hand, we\nhave to deal with the identification of CATE when the distribution of\ncovariates in treatment and control groups are imbalanced. To overcome these\nchallenges, we propose a neural network framework called Adversarial\nBalancing-based representation learning for Causal Effect Inference (ABCEI),\nbased on the recent advances in representation learning. To ensure the\nidentification of CATE, ABCEI uses adversarial learning to balance the\ndistributions of covariates in treatment and control groups in the latent\nrepresentation space, without any assumption on the form of the treatment\nselection/assignment function. In addition, during the representation learning\nand balancing process, highly predictive information from the original\ncovariate space might be lost. ABCEI can tackle this information loss problem\nby preserving useful information for predicting causal effects under the\nregularization of a mutual information estimator. The experimental results show\nthat ABCEI is robust against treatment selection bias, and matches/outperforms\nthe state-of-the-art approaches. Our experiments show promising results on\nseveral datasets, representing different health care domains among others.",
    "published_date": "2019-04-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.13335v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.13316v1",
    "title": "On Social Machines for Algorithmic Regulation",
    "authors": [
      "Nello Cristianini",
      "Teresa Scantamburlo"
    ],
    "author_ids": [],
    "abstract": "Autonomous mechanisms have been proposed to regulate certain aspects of\nsociety and are already being used to regulate business organisations. We take\nseriously recent proposals for algorithmic regulation of society, and we\nidentify the existing technologies that can be used to implement them, most of\nthem originally introduced in business contexts. We build on the notion of\n'social machine' and we connect it to various ongoing trends and ideas,\nincluding crowdsourced task-work, social compiler, mechanism design, reputation\nmanagement systems, and social scoring. After showing how all the building\nblocks of algorithmic regulation are already well in place, we discuss possible\nimplications for human autonomy and social order. The main contribution of this\npaper is to identify convergent social and technical trends that are leading\ntowards social regulation by algorithms, and to discuss the possible social,\npolitical, and ethical consequences of taking this path.",
    "published_date": "2019-04-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.13316v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.00501v1",
    "title": "The role of artificial intelligence in achieving the Sustainable Development Goals",
    "authors": [
      "Ricardo Vinuesa",
      "Hossein Azizpour",
      "Iolanda Leite",
      "Madeline Balaam",
      "Virginia Dignum",
      "Sami Domisch",
      "Anna Felländer",
      "Simone Langhans",
      "Max Tegmark",
      "Francesco Fuso Nerini"
    ],
    "author_ids": [],
    "abstract": "The emergence of artificial intelligence (AI) and its progressively wider\nimpact on many sectors across the society requires an assessment of its effect\non sustainable development. Here we analyze published evidence of positive or\nnegative impacts of AI on the achievement of each of the 17 goals and 169\ntargets of the 2030 Agenda for Sustainable Development. We find that AI can\nsupport the achievement of 128 targets across all SDGs, but it may also inhibit\n58 targets. Notably, AI enables new technologies that improve efficiency and\nproductivity, but it may also lead to increased inequalities among and within\ncountries, thus hindering the achievement of the 2030 Agenda. The fast\ndevelopment of AI needs to be supported by appropriate policy and regulation.\nOtherwise, it would lead to gaps in transparency, accountability, safety and\nethical standards of AI-based technology, which could be detrimental towards\nthe development and sustainable use of AI. Finally, there is a lack of research\nassessing the medium- and long-term impacts of AI. It is therefore essential to\nreinforce the global debate regarding the use of AI and to develop the\nnecessary regulatory insight and oversight for AI-based technologies.",
    "published_date": "2019-04-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00501v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.00003v3",
    "title": "How to Find New Characteristic-Dependent Linear Rank Inequalities using Binary Matrices as a Guide",
    "authors": [
      "Victor Peña",
      "Humberto Sarria"
    ],
    "author_ids": [],
    "abstract": "In Linear Algebra over finite fields, a characteristic-dependent linear rank\ninequality is a linear inequality that holds by ranks of subspaces of a vector\nspace over a finite field of determined characteristic, and does not in general\nhold over other characteristics. In this paper, we show a method to produce\nthese inequalities using binary matrices with suitable ranks over different\nfields. In particular, for each $n\\geq7$, we produce $2\\left\\lfloor\n\\frac{n-1}{2}\\right\\rfloor -4$ characteristic-dependent linear rank\ninequalities over $n$ variables. Many of the inequalities obtained are new but\nsome of them imply the inequalities presented in [1,9].",
    "published_date": "2019-04-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT",
      "68P30"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00003v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.12919v2",
    "title": "Female citation impact superiority 1996-2018 in six out of seven English-speaking nations",
    "authors": [
      "Mike Thelwall"
    ],
    "author_ids": [],
    "abstract": "Efforts to combat continuing gender inequalities in academia need to be\ninformed by evidence about where differences occur. Citations are relevant as\npotential evidence in appointment and promotion decisions, but it is unclear\nwhether there have been historical gender differences in average citation\nimpact that might explain the current shortfall of senior female academics.\nThis study investigates the evolution of gender differences in citation impact\n1996-2018 for six million articles from seven large English-speaking nations:\nAustralia, Canada, Ireland, Jamaica, New Zealand, UK, and the USA. The results\nshow that a small female citation advantage has been the norm over time for all\nthese countries except the USA, where there has been no practical difference.\nThe female citation advantage is largest, and statistically significant in most\nyears, for Australia and the UK. This suggests that any academic bias against\nciting female authored research cannot explain current employment inequalities.\nNevertheless, comparisons using recent citation data, or avoiding it\naltogether, during appointments or promotion may disadvantage females in some\ncountries by underestimating the likely impact of their work, especially in the\nlong term.",
    "published_date": "2019-04-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12919v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.12704v2",
    "title": "Cramér-Rao-type Bound and Stam's Inequality for Discrete Random Variables",
    "authors": [
      "Tomohiro Nishiyama"
    ],
    "author_ids": [],
    "abstract": "The variance and the entropy power of a continuous random variable are\nbounded from below by the reciprocal of its Fisher information through the\nCram\\'{e}r-Rao bound and the Stam's inequality respectively. In this note, we\nintroduce the Fisher information for discrete random variables and derive the\ndiscrete Cram\\'{e}r-Rao-type bound and the discrete Stam's inequality.",
    "published_date": "2019-04-29T00:00:00",
    "year": 2019,
    "categories": [
      "math.ST",
      "cs.IT",
      "math.IT",
      "math.PR",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12704v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.12534v3",
    "title": "Casting Geometric Constraints in Semantic Segmentation as Semi-Supervised Learning",
    "authors": [
      "Sinisa Stekovic",
      "Friedrich Fraundorfer",
      "Vincent Lepetit"
    ],
    "author_ids": [],
    "abstract": "We propose a simple yet effective method to learn to segment new indoor\nscenes from video frames: State-of-the-art methods trained on one dataset, even\nas large as the SUNRGB-D dataset, can perform poorly when applied to images\nthat are not part of the dataset, because of the dataset bias, a common\nphenomenon in computer vision. To make semantic segmentation more useful in\npractice, one can exploit geometric constraints. Our main contribution is to\nshow that these constraints can be cast conveniently as semi-supervised terms,\nwhich enforce the fact that the same class should be predicted for the\nprojections of the same 3D location in different images. This is interesting as\nwe can exploit general existing techniques developed for semi-supervised\nlearning to efficiently incorporate the constraints. We show that this approach\ncan efficiently and accurately learn to segment target sequences of ScanNet and\nour own target sequences using only annotations from SUNRGB-D, and geometric\nrelations between the video frames of target sequences.",
    "published_date": "2019-04-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12534v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.12470v5",
    "title": "Teaching AI, Ethics, Law and Policy",
    "authors": [
      "Asher Wilk"
    ],
    "author_ids": [],
    "abstract": "The cyberspace and development of intelligent systems using Artificial\nIntelligence (AI) creates new challenges to computer professionals, data\nscientists, regulators and policy makers. For example, self-driving cars raise\nnew technical, ethical, legal and public policy issues. This paper proposes a\ncourse named Computers, Ethics, Law, and Public Policy, and suggests a\ncurriculum for such a course. This paper presents ethical, legal, and public\npolicy issues relevant to building and using intelligent systems.",
    "published_date": "2019-04-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SE",
      "K.4; K.4.1; K.5; K.5.2; K.3.2; K.7.4; I.2; I.2.9"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12470v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.12465v1",
    "title": "Asymmetric Impurity Functions, Class Weighting, and Optimal Splits for Binary Classification Trees",
    "authors": [
      "David Zimmermann"
    ],
    "author_ids": [],
    "abstract": "We investigate how asymmetrizing an impurity function affects the choice of\noptimal node splits when growing a decision tree for binary classification. In\nparticular, we relax the usual axioms of an impurity function and show how\nskewing an impurity function biases the optimal splits to isolate points of a\nparticular class when splitting a node. We give a rigorous definition of this\nnotion, then give a necessary and sufficient condition for such a bias to hold.\nWe also show that the technique of class weighting is equivalent to applying a\nspecific transformation to the impurity function, and tie all these notions\ntogether for a class of impurity functions that includes the entropy and Gini\nimpurity. We also briefly discuss cost-insensitive impurity functions and give\na characterization of such functions.",
    "published_date": "2019-04-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12465v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.12352v1",
    "title": "Entropy inequalities and exponential decay of correlations for unique Gibbs measures on trees",
    "authors": [
      "Andrei Alpeev"
    ],
    "author_ids": [],
    "abstract": "In a recent paper by A. Backhausz, B. Gerencs\\'er and V. Harangi, it was\nshown that factors of independent identically distributed random processes\n(IID) on trees obey certain geometry-driven inequalities. In particular, the\nmutual information shared between two vertices decays exponentially, and there\nis an explicit bound for this decay. In this note we show that all of these\ninequalities could be verbatim translated to the setting of factors of\nprocesses driven by unique Gibbs measures. As a consequence, we show that\ncorrelations decay exponentially for unique Gibbs measures on trees.",
    "published_date": "2019-04-28T00:00:00",
    "year": 2019,
    "categories": [
      "math.PR",
      "cs.IT",
      "math.DS",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12352v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.12118v1",
    "title": "Incremental personalized E-mail spam filter using novel TFDCR feature selection with dynamic feature update",
    "authors": [
      "Gopi Sanghani",
      "Ketan Kotecha"
    ],
    "author_ids": [],
    "abstract": "Communication through e-mails remains to be highly formalized, conventional\nand indispensable method for the exchange of information over the Internet. An\never-increasing ratio and adversary nature of spam e-mails have posed a great\nmany challenges such as uneven class distribution, unequal error cost, frequent\nchange of content and personalized context-sensitive discrimination. In this\nresearch, we propose a novel and distinctive approach to develop an incremental\npersonalized e-mail spam filter. The proposed work is described using three\nsignificant contributions. First, we applied a novel term frequency difference\nand category ratio based feature selection function TFDCR to select the most\ndiscriminating features irrespective of the number of samples in each class.\nSecond, an incremental learning model is used which enables the classifier to\nupdate the discriminant function dynamically. Third, a heuristic function\ncalled selectionRankWeight is introduced to upgrade the existing feature set\nthat determines new features carrying strong discriminating ability from an\nincoming set of e-mails. Three public e-mail datasets possessing different\ncharacteristics are used to evaluate the filter performance. Experiments are\nconducted to compare the feature selection efficiency of TFDCR and to observe\nthe filter performance under both the batch and the incremental learning mode.\nThe results demonstrate the superiority of TFDCR as the most effective f eature\nselection function. The incremental learning model incorporating dynamic\nfeature update function overcomes the problem of drifting concepts. The\nproposed filter validates its efficiency and feasibility by substantially\nimproving the classification accuracy and reducing the false positive error of\nmisclassifying legitimate e-mail as spam.",
    "published_date": "2019-04-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12118v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.12025v2",
    "title": "Polyhedral Properties of the Induced Cluster Subgraphs",
    "authors": [
      "Seyedmohammadhossein Hosseinian",
      "Sergiy Butenko"
    ],
    "author_ids": [],
    "abstract": "A cluster graph is a graph whose every connected component is a complete\ngraph. Given a simple undirected graph $G$, a subset of vertices inducing a\ncluster graph is called an independent union of cliques (IUC), and the IUC\npolytope associated with $G$ is defined as the convex hull of the incidence\nvectors of all IUCs in the graph. The {\\sc Maximum IUC} problem, which is to\nfind a maximum-cardinality IUC in a graph, finds applications in network-based\ndata analysis. In this paper, we derive several families of facet-defining\nvalid inequalities for the IUC polytope. We also give a complete description of\nthis polytope for some special classes of graphs. We establish computational\ncomplexity of the separation problem for most of the considered families of\nvalid inequalities and explore the effectiveness of employing the corresponding\ncutting planes in an integer (linear) programming framework for the {\\sc\nMaximum IUC} problem through computational experiments.",
    "published_date": "2019-04-26T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.DM",
      "90C57 (Primary) 90C35, 90C10, 90C27, 05C (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12025v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.11886v2",
    "title": "Recommending research articles to consumers of online vaccination information",
    "authors": [
      "Eliza Harrison",
      "Paige Martin",
      "Didi Surian",
      "Adam G. Dunn"
    ],
    "author_ids": [],
    "abstract": "Online health communications often provide biased interpretations of evidence\nand have unreliable links to the source research. We tested the feasibility of\na tool for matching webpages to their source evidence. From 207,538 eligible\nvaccination-related PubMed articles, we evaluated several approaches using\n3,573 unique links to webpages from Altmetric. We evaluated methods for ranking\nthe source articles for vaccine-related research described on webpages,\ncomparing simple baseline feature representation and dimensionality reduction\napproaches to those augmented with canonical correlation analysis (CCA).\nPerformance measures included the median rank of the correct source article;\nthe percentage of webpages for which the source article was correctly ranked\nfirst (recall@1); and the percentage ranked within the top 50 candidate\narticles (recall@50). While augmenting baseline methods using CCA generally\nimproved results, no CCA-based approach outperformed a baseline method, which\nranked the correct source article first for over one quarter of webpages and in\nthe top 50 for more than half. Tools to help people identify evidence-based\nsources for the content they access on vaccination-related webpages are\npotentially feasible and may support the prevention of bias and\nmisrepresentation of research in news and social media.",
    "published_date": "2019-04-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.11886v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.11815v1",
    "title": "Producing Corpora of Medieval and Premodern Occitan",
    "authors": [
      "Jean-Baptiste Camps",
      "Gilles Guilhem Couffignal"
    ],
    "author_ids": [],
    "abstract": "At a time when the quantity of - more or less freely - available data is\nincreasing significantly, thanks to digital corpora, editions or libraries, the\ndevelopment of data mining tools or deep learning methods allows researchers to\nbuild a corpus of study tailored for their research, to enrich their data and\nto exploit them.Open optical character recognition (OCR) tools can be adapted\nto old prints, incunabula or even manuscripts, with usable results, allowing\nthe rapid creation of textual corpora. The alternation of training and\ncorrection phases makes it possible to improve the quality of the results by\nrapidly accumulating raw text data. These can then be structured, for example\nin XML/TEI, and enriched.The enrichment of the texts with graphic or linguistic\nannotations can also be automated. These processes, known to linguists and\nfunctional for modern languages, present difficulties for languages such as\nMedieval Occitan, due in part to the absence of big enough lemmatized corpora.\nSuggestions for the creation of tools adapted to the considerable spelling\nvariation of ancient languages will be presented, as well as experiments for\nthe lemmatization of Medieval and Premodern Occitan.These techniques open the\nway for many exploitations. The much desired increase in the amount of\navailable quality texts and data makes it possible to improve digital philology\nmethods, if everyone takes the trouble to make their data freely available\nonline and reusable.By exposing different technical solutions and some\nmicro-analyses as examples, this paper aims to show part of what digital\nphilology can offer to researchers in the Occitan domain, while recalling the\nethical issues on which such practices are based.",
    "published_date": "2019-04-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.11815v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.11783v2",
    "title": "Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors",
    "authors": [
      "Anne Lauscher",
      "Goran Glavaš"
    ],
    "author_ids": [],
    "abstract": "Word embeddings have recently been shown to reflect many of the pronounced\nsocietal biases (e.g., gender bias or racial bias). Existing studies are,\nhowever, limited in scope and do not investigate the consistency of biases\nacross relevant dimensions like embedding models, types of texts, and different\nlanguages. In this work, we present a systematic study of biases encoded in\ndistributional word vector spaces: we analyze how consistent the bias effects\nare across languages, corpora, and embedding models. Furthermore, we analyze\nthe cross-lingual biases encoded in bilingual embedding spaces, indicative of\nthe effects of bias transfer encompassed in cross-lingual transfer of NLP\nmodels. Our study yields some unexpected findings, e.g., that biases can be\nemphasized or downplayed by different embedding models or that user-generated\ncontent may be less biased than encyclopedic text. We hope our work catalyzes\nbias research in NLP and informs the development of bias reduction techniques.",
    "published_date": "2019-04-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.11783v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.11454v1",
    "title": "Reward-Based Deception with Cognitive Bias",
    "authors": [
      "Bo Wu",
      "Murat Cubuktepe",
      "Suda Bharadwaj",
      "Ufuk Topcu"
    ],
    "author_ids": [],
    "abstract": "Deception plays a key role in adversarial or strategic interactions for the\npurpose of self-defence and survival. This paper introduces a general framework\nand solution to address deception. Most existing approaches for deception\nconsider obfuscating crucial information to rational adversaries with abundant\nmemory and computation resources. In this paper, we consider deceiving\nadversaries with bounded rationality and in terms of expected rewards. This\nproblem is commonly encountered in many applications especially involving human\nadversaries. Leveraging the cognitive bias of humans in reward evaluation under\nstochastic outcomes, we introduce a framework to optimally assign resources of\na limited quantity to optimally defend against human adversaries. Modeling such\ncognitive biases follows the so-called prospect theory from behavioral\npsychology literature. Then we formulate the resource allocation problem as a\nsignomial program to minimize the defender's cost in an environment modeled as\na Markov decision process. We use police patrol hour assignment as an\nillustrative example and provide detailed simulation results based on\nreal-world data.",
    "published_date": "2019-04-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.LO",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.11454v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.03038v2",
    "title": "Maximin share allocations on cycles",
    "authors": [
      "Zbigniew Lonc",
      "Miroslaw Truszczynski"
    ],
    "author_ids": [],
    "abstract": "The problem of fair division of indivisible goods is a fundamental problem of\nsocial choice. Recently, the problem was extended to the case when goods form a\ngraph and the goal is to allocate goods to agents so that each agent's bundle\nforms a connected subgraph. For the maximin share fairness criterion\nresearchers proved that if goods form a tree, allocations offering each agent a\nbundle of at least her maximin share value always exist. Moreover, they can be\nfound in polynomial time. We consider here the problem of maximin share\nallocations of goods on a cycle. Despite the simplicity of the graph, the\nproblem turns out to be significantly harder than its tree version. We present\ncases when maximin share allocations of goods on cycles exist and provide\nresults on allocations guaranteeing each agent a certain portion of her maximin\nshare. We also study algorithms for computing maximin share allocations of\ngoods on cycles.",
    "published_date": "2019-04-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.DM",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03038v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.11150v1",
    "title": "A Deeper Look at Facial Expression Dataset Bias",
    "authors": [
      "Shan Li",
      "Weihong Deng"
    ],
    "author_ids": [],
    "abstract": "Datasets play an important role in the progress of facial expression\nrecognition algorithms, but they may suffer from obvious biases caused by\ndifferent cultures and collection conditions. To look deeper into this bias, we\nfirst conduct comprehensive experiments on dataset recognition and crossdataset\ngeneralization tasks, and for the first time explore the intrinsic causes of\nthe dataset discrepancy. The results quantitatively verify that current\ndatasets have a strong buildin bias and corresponding analyses indicate that\nthe conditional probability distributions between source and target datasets\nare different. However, previous researches are mainly based on shallow\nfeatures with limited discriminative ability under the assumption that the\nconditional distribution remains unchanged across domains. To address these\nissues, we further propose a novel deep Emotion-Conditional Adaption Network\n(ECAN) to learn domain-invariant and discriminative feature representations,\nwhich can match both the marginal and the conditional distributions across\ndomains simultaneously. In addition, the largely ignored expression class\ndistribution bias is also addressed by a learnable re-weighting parameter, so\nthat the training and testing domains can share similar class distribution.\nExtensive cross-database experiments on both lab-controlled datasets (CK+,\nJAFFE, MMI and Oulu-CASIA) and real-world databases (AffectNet, FER2013, RAF-DB\n2.0 and SFEW 2.0) demonstrate that our ECAN can yield competitive performances\nacross various facial expression transfer tasks and outperform the\nstate-of-theart methods.",
    "published_date": "2019-04-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.11150v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.12631v1",
    "title": "Detecting inter-sectional accuracy differences in driver drowsiness detection algorithms",
    "authors": [
      "Mkhuseli Ngxande",
      "Jule-Raymond Tapamo",
      "Michael Burke"
    ],
    "author_ids": [],
    "abstract": "Convolutional Neural Networks (CNNs) have been used successfully across a\nbroad range of areas including data mining, object detection, and in business.\nThe dominance of CNNs follows a breakthrough by Alex Krizhevsky which showed\nimprovements by dramatically reducing the error rate obtained in a general\nimage classification task from 26.2% to 15.4%. In road safety, CNNs have been\napplied widely to the detection of traffic signs, obstacle detection, and lane\ndeparture checking. In addition, CNNs have been used in data mining systems\nthat monitor driving patterns and recommend rest breaks when appropriate. This\npaper presents a driver drowsiness detection system and shows that there are\npotential social challenges regarding the application of these techniques, by\nhighlighting problems in detecting dark-skinned driver's faces. This is a\nparticularly important challenge in African contexts, where there are more\ndark-skinned drivers. Unfortunately, publicly available datasets are often\ncaptured in different cultural contexts, and therefore do not cover all\nethnicities, which can lead to false detections or racially biased models. This\nwork evaluates the performance obtained when training convolutional neural\nnetwork models on commonly used driver drowsiness detection datasets and\ntesting on datasets specifically chosen for broader representation. Results\nshow that models trained using publicly available datasets suffer extensively\nfrom over-fitting, and can exhibit racial bias, as shown by testing on a more\nrepresentative dataset. We propose a novel visualisation technique that can\nassist in identifying groups of people where there might be the potential of\ndiscrimination, using Principal Component Analysis (PCA) to produce a grid of\nfaces sorted by similarity, and combining these with a model accuracy overlay.",
    "published_date": "2019-04-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12631v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.10239v2",
    "title": "Ethics of Artificial Intelligence Demarcations",
    "authors": [
      "Anders Braarud Hanssen",
      "Stefano Nichele"
    ],
    "author_ids": [],
    "abstract": "In this paper we present a set of key demarcations, particularly important\nwhen discussing ethical and societal issues of current AI research and\napplications. Properly distinguishing issues and concerns related to Artificial\nGeneral Intelligence and weak AI, between symbolic and connectionist AI, AI\nmethods, data and applications are prerequisites for an informed debate. Such\ndemarcations would not only facilitate much-needed discussions on ethics on\ncurrent AI technologies and research. In addition sufficiently establishing\nsuch demarcations would also enhance knowledge-sharing and support rigor in\ninterdisciplinary research between technical and social sciences.",
    "published_date": "2019-04-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.10239v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.10117v1",
    "title": "Learning Actor Relation Graphs for Group Activity Recognition",
    "authors": [
      "Jianchao Wu",
      "Limin Wang",
      "Li Wang",
      "Jie Guo",
      "Gangshan Wu"
    ],
    "author_ids": [],
    "abstract": "Modeling relation between actors is important for recognizing group activity\nin a multi-person scene. This paper aims at learning discriminative relation\nbetween actors efficiently using deep models. To this end, we propose to build\na flexible and efficient Actor Relation Graph (ARG) to simultaneously capture\nthe appearance and position relation between actors. Thanks to the Graph\nConvolutional Network, the connections in ARG could be automatically learned\nfrom group activity videos in an end-to-end manner, and the inference on ARG\ncould be efficiently performed with standard matrix operations. Furthermore, in\npractice, we come up with two variants to sparsify ARG for more effective\nmodeling in videos: spatially localized ARG and temporal randomized ARG. We\nperform extensive experiments on two standard group activity recognition\ndatasets: the Volleyball dataset and the Collective Activity dataset, where\nstate-of-the-art performance is achieved on both datasets. We also visualize\nthe learned actor graphs and relation features, which demonstrate that the\nproposed ARG is able to capture the discriminative relation information for\ngroup activity recognition.",
    "published_date": "2019-04-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.10117v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.10761v1",
    "title": "Data Cleaning for Accurate, Fair, and Robust Models: A Big Data - AI Integration Approach",
    "authors": [
      "Ki Hyun Tae",
      "Yuji Roh",
      "Young Hun Oh",
      "Hyunsu Kim",
      "Steven Euijong Whang"
    ],
    "author_ids": [],
    "abstract": "The wide use of machine learning is fundamentally changing the software\ndevelopment paradigm (a.k.a. Software 2.0) where data becomes a first-class\ncitizen, on par with code. As machine learning is used in sensitive\napplications, it becomes imperative that the trained model is accurate, fair,\nand robust to attacks. While many techniques have been proposed to improve the\nmodel training process (in-processing approach) or the trained model itself\n(post-processing), we argue that the most effective method is to clean the root\ncause of error: the data the model is trained on (pre-processing).\nHistorically, there are at least three research communities that have been\nseparately studying this problem: data management, machine learning (model\nfairness), and security. Although a significant amount of research has been\ndone by each community, ultimately the same datasets must be preprocessed, and\nthere is little understanding how the techniques relate to each other and can\npossibly be integrated. We contend that it is time to extend the notion of data\ncleaning for modern machine learning needs. We identify dependencies among the\ndata preprocessing techniques and propose MLClean, a unified data cleaning\nframework that integrates the techniques and helps train accurate and fair\nmodels. This work is part of a broader trend of Big data -- Artificial\nIntelligence (AI) integration.",
    "published_date": "2019-04-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.10761v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.10044v1",
    "title": "UDFNet: Unsupervised Disparity Fusion with Adversarial Networks",
    "authors": [
      "Can Pu",
      "Robert B. Fisher"
    ],
    "author_ids": [],
    "abstract": "Existing disparity fusion methods based on deep learning achieve\nstate-of-the-art performance, but they require ground truth disparity data to\ntrain. As far as I know, this is the first time an unsupervised disparity\nfusion not using ground truth disparity data has been proposed. In this paper,\na mathematical model for disparity fusion is proposed to guide an adversarial\nnetwork to train effectively without ground truth disparity data. The initial\ndisparity maps are inputted from the left view along with auxiliary information\n(gradient, left & right intensity image) into the refiner and the refiner is\ntrained to output the refined disparity map registered on the left view. The\nrefined left disparity map and left intensity image are used to reconstruct a\nfake right intensity image. Finally, the fake and real right intensity images\n(from the right stereo vision camera) are fed into the discriminator. In the\nmodel, the refiner is trained to output a refined disparity value close to the\nweighted sum of the disparity inputs for global initialisation. Then, three\nrefinement principles are adopted to refine the results further. (1) The\nreconstructed intensity error between the fake and real right intensity image\nis minimised. (2) The similarities between the fake and real right image in\ndifferent receptive fields are maximised. (3) The refined disparity map is\nsmoothed based on the corresponding intensity image. The adversarial networks'\narchitectures are effective for the fusion task. The fusion time using the\nproposed network is small. The network can achieve 90 fps using Nvidia Geforce\nGTX 1080Ti on the Kitti2015 dataset when the input resolution is 1242 * 375\n(Width * Height) without downsampling and cropping. The accuracy of this work\nis equal to (or better than) the state-of-the-art supervised methods.",
    "published_date": "2019-04-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.10044v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.09942v2",
    "title": "Tracking and Improving Information in the Service of Fairness",
    "authors": [
      "Sumegha Garg",
      "Michael P. Kim",
      "Omer Reingold"
    ],
    "author_ids": [],
    "abstract": "As algorithmic prediction systems have become widespread, fears that these\nsystems may inadvertently discriminate against members of underrepresented\npopulations have grown. With the goal of understanding fundamental principles\nthat underpin the growing number of approaches to mitigating algorithmic\ndiscrimination, we investigate the role of information in fair prediction. A\ncommon strategy for decision-making uses a predictor to assign individuals a\nrisk score; then, individuals are selected or rejected on the basis of this\nscore. In this work, we study a formal framework for measuring the information\ncontent of predictors. Central to this framework is the notion of a refinement,\nfirst studied by Degroot and Fienberg. Intuitively, a refinement of a predictor\n$z$ increases the overall informativeness of the predictions without losing the\ninformation already contained in $z$. We show that increasing information\ncontent through refinements improves the downstream selection rules across a\nwide range of fairness measures (e.g. true positive rates, false positive\nrates, selection rates). In turn, refinements provide a simple but effective\ntool for reducing disparity in treatment and impact without sacrificing the\nutility of the predictions. Our results suggest that in many applications, the\nperceived \"cost of fairness\" results from an information disparity across\npopulations, and thus, may be avoided with improved information.",
    "published_date": "2019-04-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09942v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.09667v1",
    "title": "Scheduling to Approximate Minimization Objectives on Identical Machines",
    "authors": [
      "Benjamin Moseley"
    ],
    "author_ids": [],
    "abstract": "This paper considers scheduling on identical machines. The scheduling\nobjective considered in this paper generalizes most scheduling minimization\nproblems. In the problem, there are $n$ jobs and each job $j$ is associated\nwith a monotonically increasing function $g_j$. The goal is to design a\nschedule that minimizes $\\sum_{j \\in [n]} g_{j}(C_j)$ where $C_j$ is the\ncompletion time of job $j$ in the schedule. An $O(1)$-approximation is known\nfor the single machine case. On multiple machines, this paper shows that if the\nscheduler is required to be either non-migratory or non-preemptive then any\nalgorithm has an unbounded approximation ratio. Using preemption and migration,\nthis paper gives a $O(\\log \\log nP)$-approximation on multiple machines, the\nfirst result on multiple machines. These results imply the first non-trivial\npositive results for several special cases of the problem considered, such as\nthroughput minimization and tardiness.\n  Natural linear programs known for the problem have a poor integrality gap.\nThe results are obtained by strengthening a natural linear program for the\nproblem with a set of covering inequalities we call job cover inequalities.\nThis linear program is rounded to an integral solution by building on\nquasi-uniform sampling and rounding techniques.",
    "published_date": "2019-04-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09667v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.09601v4",
    "title": "MiniMax Entropy Network: Learning Category-Invariant Features for Domain Adaptation",
    "authors": [
      "Chaofan Tao",
      "Fengmao Lv",
      "Lixin Duan",
      "Min Wu"
    ],
    "author_ids": [],
    "abstract": "How to effectively learn from unlabeled data from the target domain is\ncrucial for domain adaptation, as it helps reduce the large performance gap due\nto domain shift or distribution change. In this paper, we propose an\neasy-to-implement method dubbed MiniMax Entropy Networks (MMEN) based on\nadversarial learning. Unlike most existing approaches which employ a generator\nto deal with domain difference, MMEN focuses on learning the categorical\ninformation from unlabeled target samples with the help of labeled source\nsamples. Specifically, we set an unfair multi-class classifier named\ncategorical discriminator, which classifies source samples accurately but be\nconfused about the categories of target samples. The generator learns a common\nsubspace that aligns the unlabeled samples based on the target pseudo-labels.\nFor MMEN, we also provide theoretical explanations to show that the learning of\nfeature alignment reduces domain mismatch at the category level. Experimental\nresults on various benchmark datasets demonstrate the effectiveness of our\nmethod over existing state-of-the-art baselines.",
    "published_date": "2019-04-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09601v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.09212v1",
    "title": "Risk Convergence of Centered Kernel Ridge Regression with Large Dimensional Data",
    "authors": [
      "Khalil Elkhalil",
      "Abla Kammoun",
      "Xiangliang Zhang",
      "Mohamed-Slim Alouini",
      "Tareq Al-Naffouri"
    ],
    "author_ids": [],
    "abstract": "This paper carries out a large dimensional analysis of a variation of kernel\nridge regression that we call \\emph{centered kernel ridge regression} (CKRR),\nalso known in the literature as kernel ridge regression with offset. This\nmodified technique is obtained by accounting for the bias in the regression\nproblem resulting in the old kernel ridge regression but with \\emph{centered}\nkernels. The analysis is carried out under the assumption that the data is\ndrawn from a Gaussian distribution and heavily relies on tools from random\nmatrix theory (RMT). Under the regime in which the data dimension and the\ntraining size grow infinitely large with fixed ratio and under some mild\nassumptions controlling the data statistics, we show that both the empirical\nand the prediction risks converge to a deterministic quantities that describe\nin closed form fashion the performance of CKRR in terms of the data statistics\nand dimensions. Inspired by this theoretical result, we subsequently build a\nconsistent estimator of the prediction risk based on the training data which\nallows to optimally tune the design parameters. A key insight of the proposed\nanalysis is the fact that asymptotically a large class of kernels achieve the\nsame minimum prediction risk. This insight is validated with both synthetic and\nreal data.",
    "published_date": "2019-04-19T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09212v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.09863v1",
    "title": "Optimizing Throughput Fairness of Cluster-based Cooperation in Underlay Cognitive WPCNs",
    "authors": [
      "Lina Yuan",
      "Suzhi Bi",
      "Xiaohui Lin",
      "Hui Wang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider a secondary wireless powered communication network\n(WPCN) underlaid to a primary point-to-point communication link. The WPCN\nconsists of a multi-antenna hybrid access point (HAP) that transfers wireless\nenergy to a cluster of low-power wireless devices (WDs) and receives sensing\ndata from them. To tackle the inherent severe user unfairness problem in WPCN,\nwe consider a cluster-based cooperation where a WD acts as the cluster head\nthat relays the information of the other WDs. Besides, we apply energy\nbeamforming technique to balance the dissimilar energy consumptions of the WDs\nto further improve the fairness. However, the use of energy beamforming and\ncluster-based cooperation may introduce more severe interference to the primary\nsystem than the WDs transmit independently. To guarantee the performance of\nprimary system, we consider an interference-temperature constraint to the\nprimary system and derive the throughput performance of each WD under the peak\ninterference-temperature constraint. To achieve maximum throughput fairness, we\njointly optimize the energy beamforming design, the transmit time allocation\namong the HAP and the WDs, and the transmit power allocation of each WD to\nmaximize the minimum data rate achievable among the WDs (the max-min\nthroughput). We show that the non-convex joint optimization problem can be\ntransformed to a convex one and then be efficiently solved using off-the-shelf\nconvex algorithms. Moreover, we simulate under practical network setups and\nshow that the proposed method can effectively improve the throughput fairness\nof the secondary WPCN, meanwhile guaranteeing the communication quality of the\nprimary network.",
    "published_date": "2019-04-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09863v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.09099v1",
    "title": "AMNet: Deep Atrous Multiscale Stereo Disparity Estimation Networks",
    "authors": [
      "Xianzhi Du",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "author_ids": [],
    "abstract": "In this paper, a new deep learning architecture for stereo disparity\nestimation is proposed. The proposed atrous multiscale network (AMNet) adopts\nan efficient feature extractor with depthwise-separable convolutions and an\nextended cost volume that deploys novel stereo matching costs on the deep\nfeatures. A stacked atrous multiscale network is proposed to aggregate rich\nmultiscale contextual information from the cost volume which allows for\nestimating the disparity with high accuracy at multiple scales. AMNet can be\nfurther modified to be a foreground-background aware network, FBA-AMNet, which\nis capable of discriminating between the foreground and the background objects\nin the scene at multiple scales. An iterative multitask learning method is\nproposed to train FBA-AMNet end-to-end. The proposed disparity estimation\nnetworks, AMNet and FBA-AMNet, show accurate disparity estimates and advance\nthe state of the art on the challenging Middlebury, KITTI 2012, KITTI 2015, and\nSceneflow stereo disparity estimation benchmarks.",
    "published_date": "2019-04-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09099v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.09092v1",
    "title": "Weakly Supervised Adversarial Domain Adaptation for Semantic Segmentation in Urban Scenes",
    "authors": [
      "Qi Wang",
      "Junyu Gao",
      "Xuelong Li"
    ],
    "author_ids": [],
    "abstract": "Semantic segmentation, a pixel-level vision task, is developed rapidly by\nusing convolutional neural networks (CNNs). Training CNNs requires a large\namount of labeled data, but manually annotating data is difficult. For\nemancipating manpower, in recent years, some synthetic datasets are released.\nHowever, they are still different from real scenes, which causes that training\na model on the synthetic data (source domain) cannot achieve a good performance\non real urban scenes (target domain). In this paper, we propose a weakly\nsupervised adversarial domain adaptation to improve the segmentation\nperformance from synthetic data to real scenes, which consists of three deep\nneural networks. To be specific, a detection and segmentation (\"DS\" for short)\nmodel focuses on detecting objects and predicting segmentation map; a\npixel-level domain classifier (\"PDC\" for short) tries to distinguish the image\nfeatures from which domains; an object-level domain classifier (\"ODC\" for\nshort) discriminates the objects from which domains and predicts the objects\nclasses. PDC and ODC are treated as the discriminators, and DS is considered as\nthe generator. By adversarial learning, DS is supposed to learn\ndomain-invariant features. In experiments, our proposed method yields the new\nrecord of mIoU metric in the same problem.",
    "published_date": "2019-04-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09092v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1905.03043v4",
    "title": "Topology comparison of Twitter diffusion networks effectively reveals misleading information",
    "authors": [
      "Francesco Pierri",
      "Carlo Piccardi",
      "Stefano Ceri"
    ],
    "author_ids": [],
    "abstract": "In recent years, malicious information had an explosive growth in social\nmedia, with serious social and political backlashes. Recent important studies,\nfeaturing large-scale analyses, have produced deeper knowledge about this\nphenomenon, showing that misleading information spreads faster, deeper and more\nbroadly than factual information on social media, where echo chambers,\nalgorithmic and human biases play an important role in diffusion networks.\nFollowing these directions, we explore the possibility of classifying news\narticles circulating on social media based exclusively on a topological\nanalysis of their diffusion networks. To this aim we collected a large dataset\nof diffusion networks on Twitter pertaining to news articles published on two\ndistinct classes of sources, namely outlets that convey mainstream, reliable\nand objective information and those that fabricate and disseminate various\nkinds of misleading articles, including false news intended to harm, satire\nintended to make people laugh, click-bait news that may be entirely factual or\nrumors that are unproven. We carried out an extensive comparison of these\nnetworks using several alignment-free approaches including basic network\nproperties, centrality measures distributions, and network distances. We\naccordingly evaluated to what extent these techniques allow to discriminate\nbetween the networks associated to the aforementioned news domains. Our results\nhighlight that the communities of users spreading mainstream news, compared to\nthose sharing misleading news, tend to shape diffusion networks with subtle yet\nsystematic differences which might be effectively employed to identify\nmisleading and harmful information.",
    "published_date": "2019-04-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03043v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.08818v1",
    "title": "(De)Constructing Bias on Skin Lesion Datasets",
    "authors": [
      "Alceu Bissoto",
      "Michel Fornaciali",
      "Eduardo Valle",
      "Sandra Avila"
    ],
    "author_ids": [],
    "abstract": "Melanoma is the deadliest form of skin cancer. Automated skin lesion analysis\nplays an important role for early detection. Nowadays, the ISIC Archive and the\nAtlas of Dermoscopy dataset are the most employed skin lesion sources to\nbenchmark deep-learning based tools. However, all datasets contain biases,\noften unintentional, due to how they were acquired and annotated. Those biases\ndistort the performance of machine-learning models, creating spurious\ncorrelations that the models can unfairly exploit, or, contrarily destroying\ncogent correlations that the models could learn. In this paper, we propose a\nset of experiments that reveal both types of biases, positive and negative, in\nexisting skin lesion datasets. Our results show that models can correctly\nclassify skin lesion images without clinically-meaningful information:\ndisturbingly, the machine-learning model learned over images where no\ninformation about the lesion remains, presents an accuracy above the AI\nbenchmark curated with dermatologists' performances. That strongly suggests\nspurious correlations guiding the models. We fed models with additional\nclinically meaningful information, which failed to improve the results even\nslightly, suggesting the destruction of cogent correlations. Our main findings\nraise awareness of the limitations of models trained and evaluated in small\ndatasets such as the ones we evaluated, and may suggest future guidelines for\nmodels intended for real-world deployment.",
    "published_date": "2019-04-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08818v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.08783v1",
    "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings",
    "authors": [
      "Christine Basta",
      "Marta R. Costa-jussà",
      "Noe Casas"
    ],
    "author_ids": [],
    "abstract": "Gender bias is highly impacting natural language processing applications.\nWord embeddings have clearly been proven both to keep and amplify gender biases\nthat are present in current data sources. Recently, contextualized word\nembeddings have enhanced previous word embedding techniques by computing word\nvector representations dependent on the sentence they appear in.\n  In this paper, we study the impact of this conceptual change in the word\nembedding computation in relation with gender bias. Our analysis includes\ndifferent measures previously applied in the literature to standard word\nembeddings. Our findings suggest that contextualized word embeddings are less\nbiased than standard ones even when the latter are debiased.",
    "published_date": "2019-04-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08783v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.08654v2",
    "title": "Analytical Methods for Interpretable Ultradense Word Embeddings",
    "authors": [
      "Philipp Dufter",
      "Hinrich Schütze"
    ],
    "author_ids": [],
    "abstract": "Word embeddings are useful for a wide variety of tasks, but they lack\ninterpretability. By rotating word spaces, interpretable dimensions can be\nidentified while preserving the information contained in the embeddings without\nany loss. In this work, we investigate three methods for making word spaces\ninterpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and\nDensRay, a new method we propose. In contrast to Densifier, DensRay can be\ncomputed in closed form, is hyperparameter-free and thus more robust than\nDensifier. We evaluate the three methods on lexicon induction and set-based\nword analogy. In addition we provide qualitative insights as to how\ninterpretable word spaces can be used for removing gender bias from embeddings.",
    "published_date": "2019-04-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08654v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.12628v1",
    "title": "Computational Attention System for Children, Adults and Elderly",
    "authors": [
      "Onkar Krishna",
      "Kiyoharu Aizawa",
      "Go Irie"
    ],
    "author_ids": [],
    "abstract": "The existing computational visual attention systems have focused on the\nobjective to basically simulate and understand the concept of visual attention\nsystem in adults. Consequently, the impact of observer's age in scene viewing\nbehavior has rarely been considered. This study quantitatively analyzed the\nage-related differences in gaze landings during scene viewing for three\ndifferent class of images: naturals, man-made, and fractals. Observer's of\ndifferent age-group have shown different scene viewing tendencies independent\nto the class of the image viewed. Several interesting observations are drawn\nfrom the results. First, gaze landings for man-made dataset showed that whereas\nchild observers focus more on the scene foreground, i.e., locations that are\nnear, elderly observers tend to explore the scene background, i.e., locations\nfarther in the scene. Considering this result a framework is proposed in this\npaper to quantitatively measure the depth bias tendency across age groups.\nSecond, the quantitative analysis results showed that children exhibit the\nlowest exploratory behavior level but the highest central bias tendency among\nthe age groups and across the different scene categories. Third,\ninter-individual similarity metrics reveal that an adult had significantly\nlower gaze consistency with children and elderly compared to other adults for\nall the scene categories. Finally, these analysis results were consequently\nleveraged to develop a more accurate age-adapted saliency model independent to\nthe image type. The prediction accuracy suggests that our model fits better to\nthe collected eye-gaze data of the observers belonging to different age groups\nthan the existing models do.",
    "published_date": "2019-04-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.12628v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.08279v1",
    "title": "Interpreting Adversarial Examples with Attributes",
    "authors": [
      "Sadaf Gulshad",
      "Jan Hendrik Metzen",
      "Arnold Smeulders",
      "Zeynep Akata"
    ],
    "author_ids": [],
    "abstract": "Deep computer vision systems being vulnerable to imperceptible and carefully\ncrafted noise have raised questions regarding the robustness of their\ndecisions. We take a step back and approach this problem from an orthogonal\ndirection. We propose to enable black-box neural networks to justify their\nreasoning both for clean and for adversarial examples by leveraging attributes,\ni.e. visually discriminative properties of objects. We rank attributes based on\ntheir class relevance, i.e. how the classification decision changes when the\ninput is visually slightly perturbed, as well as image relevance, i.e. how well\nthe attributes can be localized on both clean and perturbed images. We present\ncomprehensive experiments for attribute prediction, adversarial example\ngeneration, adversarially robust learning, and their qualitative and\nquantitative analysis using predicted attributes on three benchmark datasets.",
    "published_date": "2019-04-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08279v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.08928v2",
    "title": "Class specific or shared? A cascaded dictionary learning framework for image classification",
    "authors": [
      "Yan-Jiang Wang",
      "Shuai Shao",
      "Rui Xu",
      "Werifeng Liu",
      "Bao-Di Liu"
    ],
    "author_ids": [],
    "abstract": "Dictionary learning methods can be split into: i) class specific dictionary\nlearning ii) class shared dictionary learning. The difference between the two\ncategories is how to use discriminative information. With the first category,\nsamples of different classes are mapped into different subspaces, which leads\nto some redundancy with the class specific base vectors. While for the second\ncategory, the samples in each specific class can not be described accurately.\nIn this paper, we first propose a novel class shared dictionary learning method\nnamed label embedded dictionary learning (LEDL). It is the improvement based on\nLCKSVD, which is easier to find out the optimal solution. Then we propose a\nnovel framework named cascaded dictionary learning framework (CDLF) to combine\nthe specific dictionary learning with shared dictionary learning to describe\nthe feature to boost the performance of classification sufficiently. Extensive\nexperimental results on six benchmark datasets illustrate that our methods are\ncapable of achieving superior performance compared to several state-of-art\nclassification algorithms.",
    "published_date": "2019-04-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08928v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.08066v1",
    "title": "Collaboration Analysis Using Deep Learning",
    "authors": [
      "Zhang Guo",
      "Kevin Yu",
      "Rebecca Pearlman",
      "Nassir Navab",
      "Roghayeh Barmaki"
    ],
    "author_ids": [],
    "abstract": "The analysis of the collaborative learning process is one of the growing\nfields of education research, which has many different analytic solutions. In\nthis paper, we provided a new solution to improve automated collaborative\nlearning analyses using deep neural networks. Instead of using self-reported\nquestionnaires, which are subject to bias and noise, we automatically extract\ngroup-working information by object recognition results using Mask R-CNN\nmethod. This process is based on detecting the people and other objects from\npictures and video clips of the collaborative learning process, then evaluate\nthe mobile learning performance using the collaborative indicators. We tested\nour approach to automatically evaluate the group-work collaboration in a\ncontrolled study of thirty-three dyads while performing an anatomy body\npainting intervention. The results indicate that our approach recognizes the\ndifferences of collaborations among teams of treatment and control groups in\nthe case study. This work introduces new methods for automated quality\nprediction of collaborations among human-human interactions using computer\nvision techniques.",
    "published_date": "2019-04-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CV",
      "68U10"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08066v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.07911v1",
    "title": "REPAIR: Removing Representation Bias by Dataset Resampling",
    "authors": [
      "Yi Li",
      "Nuno Vasconcelos"
    ],
    "author_ids": [],
    "abstract": "Modern machine learning datasets can have biases for certain representations\nthat are leveraged by algorithms to achieve high performance without learning\nto solve the underlying task. This problem is referred to as \"representation\nbias\". The question of how to reduce the representation biases of a dataset is\ninvestigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure\nis proposed. This formulates bias minimization as an optimization problem,\nseeking a weight distribution that penalizes examples easy for a classifier\nbuilt on a given feature representation. Bias reduction is then equated to\nmaximizing the ratio between the classification loss on the reweighted dataset\nand the uncertainty of the ground-truth class labels. This is a minimax problem\nthat REPAIR solves by alternatingly updating classifier parameters and dataset\nresampling weights, using stochastic gradient descent. An experimental set-up\nis also introduced to measure the bias of any dataset for a given\nrepresentation, and the impact of this bias on the performance of recognition\nmodels. Experiments with synthetic and action recognition data show that\ndataset REPAIR can significantly reduce representation bias, and lead to\nimproved generalization of models trained on REPAIRed datasets. The tools used\nfor characterizing representation bias, and the proposed dataset REPAIR\nalgorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.",
    "published_date": "2019-04-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.07911v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.07659v1",
    "title": "Semantically Aligned Bias Reducing Zero Shot Learning",
    "authors": [
      "Akanksha Paul",
      "Narayanan C. Krishnan",
      "Prateek Munjal"
    ],
    "author_ids": [],
    "abstract": "Zero shot learning (ZSL) aims to recognize unseen classes by exploiting\nsemantic relationships between seen and unseen classes. Two major problems\nfaced by ZSL algorithms are the hubness problem and the bias towards the seen\nclasses. Existing ZSL methods focus on only one of these problems in the\nconventional and generalized ZSL setting. In this work, we propose a novel\napproach, Semantically Aligned Bias Reducing (SABR) ZSL, which focuses on\nsolving both the problems. It overcomes the hubness problem by learning a\nlatent space that preserves the semantic relationship between the labels while\nencoding the discriminating information about the classes. Further, we also\npropose ways to reduce the bias of the seen classes through a simple\ncross-validation process in the inductive setting and a novel weak transfer\nconstraint in the transductive setting. Extensive experiments on three\nbenchmark datasets suggest that the proposed model significantly outperforms\nexisting state-of-the-art algorithms by ~1.5-9% in the conventional ZSL setting\nand by ~2-14% in the generalized ZSL for both the inductive and transductive\nsettings.",
    "published_date": "2019-04-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.07659v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.07633v1",
    "title": "HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning",
    "authors": [
      "Oguzhan Gencoglu",
      "Mark van Gils",
      "Esin Guldogan",
      "Chamin Morikawa",
      "Mehmet Süzen",
      "Mathias Gruber",
      "Jussi Leinonen",
      "Heikki Huttunen"
    ],
    "author_ids": [],
    "abstract": "Recent advancements in machine learning research, i.e., deep learning,\nintroduced methods that excel conventional algorithms as well as humans in\nseveral complex tasks, ranging from detection of objects in images and speech\nrecognition to playing difficult strategic games. However, the current\nmethodology of machine learning research and consequently, implementations of\nthe real-world applications of such algorithms, seems to have a recurring\nHARKing (Hypothesizing After the Results are Known) issue. In this work, we\nelaborate on the algorithmic, economic and social reasons and consequences of\nthis phenomenon. We present examples from current common practices of\nconducting machine learning research (e.g. avoidance of reporting negative\nresults) and failure of generalization ability of the proposed algorithms and\ndatasets in actual real-life usage. Furthermore, a potential future trajectory\nof machine learning research and development from the perspective of\naccountable, unbiased, ethical and privacy-aware algorithmic decision making is\ndiscussed. We would like to emphasize that with this discussion we neither\nclaim to provide an exhaustive argumentation nor blame any specific institution\nor individual on the raised issues. This is simply a discussion put forth by\nus, insiders of the machine learning field, reflecting on us.",
    "published_date": "2019-04-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.07633v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.07536v1",
    "title": "Selection Bias in News Coverage: Learning it, Fighting it",
    "authors": [
      "Dylan Bourgeois",
      "Jeremie Rappaz",
      "Karl Aberer"
    ],
    "author_ids": [],
    "abstract": "News entities must select and filter the coverage they broadcast through\ntheir respective channels since the set of world events is too large to be\ntreated exhaustively. The subjective nature of this filtering induces biases\ndue to, among other things, resource constraints, editorial guidelines,\nideological affinities, or even the fragmented nature of the information at a\njournalist's disposal. The magnitude and direction of these biases are,\nhowever, widely unknown. The absence of ground truth, the sheer size of the\nevent space, or the lack of an exhaustive set of absolute features to measure\nmake it difficult to observe the bias directly, to characterize the leaning's\nnature and to factor it out to ensure a neutral coverage of the news. In this\nwork, we introduce a methodology to capture the latent structure of media's\ndecision process on a large scale. Our contribution is multi-fold. First, we\nshow media coverage to be predictable using personalization techniques, and\nevaluate our approach on a large set of events collected from the GDELT\ndatabase. We then show that a personalized and parametrized approach not only\nexhibits higher accuracy in coverage prediction, but also provides an\ninterpretable representation of the selection bias. Last, we propose a method\nable to select a set of sources by leveraging the latent representation. These\nselected sources provide a more diverse and egalitarian coverage, all while\nretaining the most actively covered events.",
    "published_date": "2019-04-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.07536v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.07496v2",
    "title": "Discriminative Ridge Machine: A Classifier for High-Dimensional Data or Imbalanced Data",
    "authors": [
      "Chong Peng",
      "Qiang Cheng"
    ],
    "author_ids": [],
    "abstract": "We introduce a discriminative regression approach to supervised\nclassification in this paper. It estimates a representation model while\naccounting for discriminativeness between classes, thereby enabling accurate\nderivation of categorical information. This new type of regression models\nextends existing models such as ridge, lasso, and group lasso through\nexplicitly incorporating discriminative information. As a special case we focus\non a quadratic model that admits a closed-form analytical solution. The\ncorresponding classifier is called discriminative regression machine (DRM).\nThree iterative algorithms are further established for the DRM to enhance the\nefficiency and scalability for real applications. Our approach and the\nalgorithms are applicable to general types of data including images,\nhigh-dimensional data, and imbalanced data. We compare the DRM with currently\nstate-of-the-art classifiers. Our extensive experimental results show superior\nperformance of the DRM and confirm the effectiveness of the proposed approach.",
    "published_date": "2019-04-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.07496v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.07441v1",
    "title": "fMRI Based Cerebral Instantaneous Parameters for Automatic Alzheimer's, Mild Cognitive Impairment and Healthy Subject Classification",
    "authors": [
      "Esmaeil Seraj",
      "Mehran Yazdi",
      "Nastaran Shahparian"
    ],
    "author_ids": [],
    "abstract": "Automatic identification and categorization of Alzheimer's patients and the\nability to distinguish between different levels of this disease can be very\nhelpful to the research community in this field, since other non-automatic\napproaches are very time-consuming and are highly dependent on experts'\nexperience. Herein, we propose the utility of cerebral instantaneous phase and\nenvelope information in order to discriminate between Alzheimer's patients, MCI\nsubjects and healthy normal individuals from functional magnetic resonance\nimaging (fMRI) data. To this end, after performing the region-of-interest (ROI)\nanalysis on fMRI data, different features covering power, entropy and coherency\naspects of data are derived from instantaneous phase and envelope sequences of\nROI signals. Various sets of features are calculated and fed to a sequential\nforward floating feature selection (SFFFS) to choose the most discriminative\nand informative sets of features. A Student's t-test has been used to select\nthe most relevant features from chosen sets. Finally, a K-NN classifier is used\nto distinguish between classes in a three-class categorization problem. The\nreported performance in overall accuracy using fMRI data of 111 combined\nsubjects, is 80.1% with 80.0% Sensitivity to both Alzheimer's and Normal\ncategories distinction and is comparable to the state-of-the-art approaches\nrecently proposed in this regard. The significance of obtained results was\nstatistically confirmed by evaluating through standard classification\nperformance indicators. The obtained results illustrate that introduced\nanalytic phase and envelope feature indexes derived from the ROI signals are\nsignificantly discriminative in distinguishing between Alzheimer's patients and\nNormal healthy subject.",
    "published_date": "2019-04-16T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SP",
      "cs.CE",
      "eess.IV",
      "q-bio.NC",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.07441v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1905.01139v1",
    "title": "Performance Evaluation of LTE-CommSense System for Discrimination of Presence of Multiple Objects in Outdoor Environment",
    "authors": [
      "Santu Sardar",
      "Amit K. Mishra",
      "Mohammed Zafar Ali Khan"
    ],
    "author_ids": [],
    "abstract": "LTE-CommSense is a novel instrumentation scheme which analyzes channel\naffected reference signals of LTE downlink signal to obtain knowledge about the\nenvironmental change. This work presents the characterization of LTE-CommSense\ninstrument to detect presence or absence of objects in outdoor environment.\nAdditionally, we analyze its capability of detecting and distinguishing when\nmultiple objects are present. For performance evaluation and characterization\nof this instrument, we derive object detection accuracy, FAR, FRR and\nresolution which we believe are the most important figures of merit in this\ncase. As the operation of LTE-CommSense is to detect events instead of objects,\nwe redefine the concept of resolution for LTE-CommSense. Two different\nproposals to represent the redefined resolution viz. Neyman Pearson principle\nbased and Cramer Rao principle based resolution are presented here. All the\nperformance metrics are derived using practical data captured using an SDR\nplatform modeled as a LTE-CommSense receiver. We observe that, LTE-CommSense\nprovides better performance in detecting presence or absence of objects at near\nrange.",
    "published_date": "2019-04-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI",
      "eess.SP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.01139v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.06996v1",
    "title": "SR-GAN: Semantic Rectifying Generative Adversarial Network for Zero-shot Learning",
    "authors": [
      "Zihan Ye",
      "Fan Lyu",
      "Linyan Li",
      "Qiming Fu",
      "Jinchang Ren",
      "Fuyuan Hu"
    ],
    "author_ids": [],
    "abstract": "The existing Zero-Shot learning (ZSL) methods may suffer from the vague class\nattributes that are highly overlapped for different classes. Unlike these\nmethods that ignore the discrimination among classes, in this paper, we propose\nto classify unseen image by rectifying the semantic space guided by the visual\nspace. First, we pre-train a Semantic Rectifying Network (SRN) to rectify\nsemantic space with a semantic loss and a rectifying loss. Then, a Semantic\nRectifying Generative Adversarial Network (SR-GAN) is built to generate\nplausible visual feature of unseen class from both semantic feature and\nrectified semantic feature. To guarantee the effectiveness of rectified\nsemantic features and synthetic visual features, a pre-reconstruction and a\npost reconstruction networks are proposed, which keep the consistency between\nvisual feature and semantic feature. Experimental results demonstrate that our\napproach significantly outperforms the state-of-the-arts on four benchmark\ndatasets.",
    "published_date": "2019-04-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.06996v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.06834v1",
    "title": "An Empirical Investigation of Global and Local Normalization for Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search",
    "authors": [
      "Kartik Goyal",
      "Chris Dyer",
      "Taylor Berg-Kirkpatrick"
    ],
    "author_ids": [],
    "abstract": "Globally normalized neural sequence models are considered superior to their\nlocally normalized equivalents because they may ameliorate the effects of label\nbias. However, when considering high-capacity neural parametrizations that\ncondition on the whole input sequence, both model classes are theoretically\nequivalent in terms of the distributions they are capable of representing.\nThus, the practical advantage of global normalization in the context of modern\nneural methods remains unclear. In this paper, we attempt to shed light on this\nproblem through an empirical study. We extend an approach for search-aware\ntraining via a continuous relaxation of beam search (Goyal et al., 2017b) in\norder to enable training of globally normalized recurrent sequence models\nthrough simple backpropagation. We then use this technique to conduct an\nempirical study of the interaction between global normalization, high-capacity\nencoders, and search-aware optimization. We observe that in the context of\ninexact search, globally normalized neural models are still more effective than\ntheir locally normalized counterparts. Further, since our training approach is\nsensitive to warm-starting with pre-trained models, we also propose a novel\ninitialization strategy based on self-normalization for pre-training globally\nnormalized models. We perform analysis of our approach on two tasks: CCG\nsupertagging and Machine Translation, and demonstrate the importance of global\nnormalization under different conditions while using search-aware training.",
    "published_date": "2019-04-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.06834v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.06635v1",
    "title": "Localizing Discriminative Visual Landmarks for Place Recognition",
    "authors": [
      "Zhe Xin",
      "Yinghao Cai",
      "Tao Lu",
      "Xiaoxia Xing",
      "Shaojun Cai",
      "Jixiang Zhang",
      "Yiping Yang",
      "Yanqing Wang"
    ],
    "author_ids": [],
    "abstract": "We address the problem of visual place recognition with perceptual changes.\nThe fundamental problem of visual place recognition is generating robust image\nrepresentations which are not only insensitive to environmental changes but\nalso distinguishable to different places. Taking advantage of the feature\nextraction ability of Convolutional Neural Networks (CNNs), we further\ninvestigate how to localize discriminative visual landmarks that positively\ncontribute to the similarity measurement, such as buildings and vegetations. In\nparticular, a Landmark Localization Network (LLN) is designed to indicate which\nregions of an image are used for discrimination. Detailed experiments are\nconducted on open source datasets with varied appearance and viewpoint changes.\nThe proposed approach achieves superior performance against state-of-the-art\nmethods.",
    "published_date": "2019-04-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.06635v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.06562v2",
    "title": "Tensorization of the strong data processing inequality for quantum chi-square divergences",
    "authors": [
      "Yu Cao",
      "Jianfeng Lu"
    ],
    "author_ids": [],
    "abstract": "It is well-known that any quantum channel $\\mathcal{E}$ satisfies the data\nprocessing inequality (DPI), with respect to various divergences, e.g., quantum\n$\\chi^2_{\\kappa}$divergences and quantum relative entropy. More specifically,\nthe data processing inequality states that the divergence between two arbitrary\nquantum states $\\rho$ and $\\sigma$ does not increase under the action of any\nquantum channel $\\mathcal{E}$. For a fixed channel $\\mathcal{E}$ and a state\n$\\sigma$, the divergence between output states $\\mathcal{E}(\\rho)$ and\n$\\mathcal{E}(\\sigma)$ might be strictly smaller than the divergence between\ninput states $\\rho$ and $\\sigma$, which is characterized by the strong data\nprocessing inequality (SDPI). Among various input states $\\rho$, the largest\nvalue of the rate of contraction is known as the SDPI constant. An important\nand widely studied property for classical channels is that SDPI constants\ntensorize. In this paper, we extend the tensorization property to the quantum\nregime: we establish the tensorization of SDPIs for the quantum\n$\\chi^2_{\\kappa_{1/2}}$ divergence for arbitrary quantum channels and also for\na family of $\\chi^2_{\\kappa}$ divergences (with $\\kappa \\ge \\kappa_{1/2}$) for\narbitrary quantum-classical channels.",
    "published_date": "2019-04-13T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.06562v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.06505v1",
    "title": "dipIQ: Blind Image Quality Assessment by Learning-to-Rank Discriminable Image Pairs",
    "authors": [
      "Kede Ma",
      "Wentao Liu",
      "Tongliang Liu",
      "Zhou Wang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Objective assessment of image quality is fundamentally important in many\nimage processing tasks. In this work, we focus on learning blind image quality\nassessment (BIQA) models which predict the quality of a digital image with no\naccess to its original pristine-quality counterpart as reference. One of the\nbiggest challenges in learning BIQA models is the conflict between the gigantic\nimage space (which is in the dimension of the number of image pixels) and the\nextremely limited reliable ground truth data for training. Such data are\ntypically collected via subjective testing, which is cumbersome, slow, and\nexpensive. Here we first show that a vast amount of reliable training data in\nthe form of quality-discriminable image pairs (DIP) can be obtained\nautomatically at low cost by exploiting large-scale databases with diverse\nimage content. We then learn an opinion-unaware BIQA (OU-BIQA, meaning that no\nsubjective opinions are used for training) model using RankNet, a pairwise\nlearning-to-rank (L2R) algorithm, from millions of DIPs, each associated with a\nperceptual uncertainty level, leading to a DIP inferred quality (dipIQ) index.\nExtensive experiments on four benchmark IQA databases demonstrate that dipIQ\noutperforms state-of-the-art OU-BIQA models. The robustness of dipIQ is also\nsignificantly improved as confirmed by the group MAximum Differentiation (gMAD)\ncompetition method. Furthermore, we extend the proposed framework by learning\nmodels with ListNet (a listwise L2R algorithm) on quality-discriminable image\nlists (DIL). The resulting DIL Inferred Quality (dilIQ) index achieves an\nadditional performance gain.",
    "published_date": "2019-04-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.06505v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.08338v1",
    "title": "OCKELM+: Kernel Extreme Learning Machine based One-class Classification using Privileged Information (or KOC+: Kernel Ridge Regression or Least Square SVM with zero bias based One-class Classification using Privileged Information)",
    "authors": [
      "Chandan Gautam",
      "Aruna Tiwari",
      "M. Tanveer"
    ],
    "author_ids": [],
    "abstract": "Kernel method-based one-class classifier is mainly used for outlier or\nnovelty detection. In this letter, kernel ridge regression (KRR) based\none-class classifier (KOC) has been extended for learning using privileged\ninformation (LUPI). LUPI-based KOC method is referred to as KOC+. This\nprivileged information is available as a feature with the dataset but only for\ntraining (not for testing). KOC+ utilizes the privileged information\ndifferently compared to normal feature information by using a so-called\ncorrection function. Privileged information helps KOC+ in achieving better\ngeneralization performance which is exhibited in this letter by testing the\nclassifiers with and without privileged information. Existing and proposed\nclassifiers are evaluated on the datasets from UCI machine learning repository\nand also on MNIST dataset. Moreover, experimental results evince the advantage\nof KOC+ over KOC and support vector machine (SVM) based one-class classifiers.",
    "published_date": "2019-04-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.08338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.06490v1",
    "title": "Towards Self-similarity Consistency and Feature Discrimination for Unsupervised Domain Adaptation",
    "authors": [
      "Chao Chen",
      "Zhihang Fu",
      "Zhihong Chen",
      "Zhaowei Cheng",
      "Xinyu Jin",
      "Xian-Sheng Hua"
    ],
    "author_ids": [],
    "abstract": "Recent advances in unsupervised domain adaptation mainly focus on learning\nshared representations by global distribution alignment without considering\nclass information across domains. The neglect of class information, however,\nmay lead to partial alignment (or even misalignment) and poor generalization\nperformance. For comprehensive alignment, we argue that the similarities across\ndifferent features in the source domain should be consistent with that of in\nthe target domain. Based on this assumption, we propose a new domain\ndiscrepancy metric, i.e., Self-similarity Consistency (SSC), to enforce the\nfeature structure being consistent across domains. The renowned correlation\nalignment (CORAL) is proven to be a special case, and a sub-optimal measure of\nour proposed SSC. Furthermore, we also propose to mitigate the side effect of\nthe partial alignment and misalignment by incorporating the discriminative\ninformation of the deep representations. Specifically, an embarrassingly simple\nand effective feature norm constraint is exploited to enlarge the discrepancy\nof inter-class samples. It relieves the requirements of strict alignment when\nperforming adaptation, therefore improving the adaptation performance\nsignificantly. Extensive experiments on visual domain adaptation tasks\ndemonstrate the effectiveness of our proposed SSC metric and feature\ndiscrimination approach.",
    "published_date": "2019-04-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.06490v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05419v4",
    "title": "FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning",
    "authors": [
      "Ángel Alexander Cabrera",
      "Will Epperson",
      "Fred Hohman",
      "Minsuk Kahng",
      "Jamie Morgenstern",
      "Duen Horng Chau"
    ],
    "author_ids": [],
    "abstract": "The growing capability and accessibility of machine learning has led to its\napplication to many real-world domains and data about people. Despite the\nbenefits algorithmic systems may bring, models can reflect, inject, or\nexacerbate implicit and explicit societal biases into their outputs,\ndisadvantaging certain demographic subgroups. Discovering which biases a\nmachine learning model has introduced is a great challenge, due to the numerous\ndefinitions of fairness and the large number of potentially impacted subgroups.\nWe present FairVis, a mixed-initiative visual analytics system that integrates\na novel subgroup discovery technique for users to audit the fairness of machine\nlearning models. Through FairVis, users can apply domain knowledge to generate\nand investigate known subgroups, and explore suggested and similar subgroups.\nFairVis' coordinated views enable users to explore a high-level overview of\nsubgroup performance and subsequently drill down into detailed investigation of\nspecific subgroups. We show how FairVis helps to discover biases in two real\ndatasets used in predicting income and recidivism. As a visual analytics system\ndevoted to discovering bias in machine learning, FairVis demonstrates how\ninteractive visualization may help data scientists and the general public\nunderstand and create more equitable algorithmic systems.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05419v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05375v2",
    "title": "Scanner Invariant Representations for Diffusion MRI Harmonization",
    "authors": [
      "Daniel Moyer",
      "Greg Ver Steeg",
      "Chantal M. W. Tax",
      "Paul M. Thompson"
    ],
    "author_ids": [],
    "abstract": "Purpose: In the present work we describe the correction of diffusion-weighted\nMRI for site and scanner biases using a novel method based on invariant\nrepresentation.\n  Theory and Methods: Pooled imaging data from multiple sources are subject to\nvariation between the sources. Correcting for these biases has become very\nimportant as imaging studies increase in size and multi-site cases become more\ncommon. We propose learning an intermediate representation invariant to\nsite/protocol variables, a technique adapted from information theory-based\nalgorithmic fairness; by leveraging the data processing inequality, such a\nrepresentation can then be used to create an image reconstruction that is\nuninformative of its original source, yet still faithful to underlying\nstructures. To implement this, we use a deep learning method based on\nvariational auto-encoders (VAE) to construct scanner invariant encodings of the\nimaging data.\n  Results: To evaluate our method, we use training data from the 2018 MICCAI\nComputational Diffusion MRI (CDMRI) Challenge Harmonization dataset. Our\nproposed method shows improvements on independent test data relative to a\nrecently published baseline method on each subtask, mapping data from three\ndifferent scanning contexts to and from one separate target scanning context.\n  Conclusion: As imaging studies continue to grow, the use of pooled multi-site\nimaging will similarly increase. Invariant representation presents a strong\ncandidate for the harmonization of these data.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "eess.IV",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05375v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05254v4",
    "title": "Attraction-Repulsion clustering with applications to fairness",
    "authors": [
      "Eustasio del Barrio",
      "Hristo Inouzhe",
      "Jean-Michel Loubes"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of diversity enhancing clustering, i.e, developing\nclustering methods which produce clusters that favour diversity with respect to\na set of protected attributes such as race, sex, age, etc. In the context of\nfair clustering, diversity plays a major role when fairness is understood as\ndemographic parity. To promote diversity, we introduce perturbations to the\ndistance in the unprotected attributes that account for protected attributes in\na way that resembles attraction-repulsion of charged particles in Physics.\nThese perturbations are defined through dissimilarities with a tractable\ninterpretation. Cluster analysis based on attraction-repulsion dissimilarities\npenalizes homogeneity of the clusters with respect to the protected attributes\nand leads to an improvement in diversity. An advantage of our approach, which\nfalls into a pre-processing set-up, is its compatibility with a wide variety of\nclustering methods and whit non-Euclidean data. We illustrate the use of our\nprocedures with both synthetic and real data and provide discussion about the\nrelation between diversity, fairness, and cluster structure. Our procedures are\nimplemented in an R package freely available at\nhttps://github.com/HristoInouzhe/AttractionRepulsionClustering.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "62H30, 68T10"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05254v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05773v5",
    "title": "Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy Images Using Color Balancing on Convolutional Neural Networks",
    "authors": [
      "Kamran Kowsari",
      "Rasoul Sali",
      "Marium N. Khan",
      "William Adorno",
      "S. Asad Ali",
      "Sean R. Moore",
      "Beatrice C. Amadi",
      "Paul Kelly",
      "Sana Syed",
      "Donald E. Brown"
    ],
    "author_ids": [],
    "abstract": "Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of\nmalnutrition and adversely impact normal childhood development. CD is an\nautoimmune disorder that is prevalent worldwide and is caused by an increased\nsensitivity to gluten. Gluten exposure destructs the small intestinal\nepithelial barrier, resulting in nutrient mal-absorption and childhood\nunder-nutrition. EE also results in barrier dysfunction but is thought to be\ncaused by an increased vulnerability to infections. EE has been implicated as\nthe predominant cause of under-nutrition, oral vaccine failure, and impaired\ncognitive development in low-and-middle-income countries. Both conditions\nrequire a tissue biopsy for diagnosis, and a major challenge of interpreting\nclinical biopsy images to differentiate between these gastrointestinal diseases\nis striking histopathologic overlap between them. In the current study, we\npropose a convolutional neural network (CNN) to classify duodenal biopsy images\nfrom subjects with CD, EE, and healthy controls. We evaluated the performance\nof our proposed model using a large cohort containing 1000 biopsy images. Our\nevaluations show that the proposed model achieves an area under ROC of 0.99,\n1.00, and 0.97 for CD, EE, and healthy controls, respectively. These results\ndemonstrate the discriminative power of the proposed model in duodenal biopsies\nclassification.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05773v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05233v1",
    "title": "What's in a Name? Reducing Bias in Bios without Access to Protected Attributes",
    "authors": [
      "Alexey Romanov",
      "Maria De-Arteaga",
      "Hanna Wallach",
      "Jennifer Chayes",
      "Christian Borgs",
      "Alexandra Chouldechova",
      "Sahin Geyik",
      "Krishnaram Kenthapadi",
      "Anna Rumshisky",
      "Adam Tauman Kalai"
    ],
    "author_ids": [],
    "abstract": "There is a growing body of work that proposes methods for mitigating bias in\nmachine learning systems. These methods typically rely on access to protected\nattributes such as race, gender, or age. However, this raises two significant\nchallenges: (1) protected attributes may not be available or it may not be\nlegal to use them, and (2) it is often desirable to simultaneously consider\nmultiple protected attributes, as well as their intersections. In the context\nof mitigating bias in occupation classification, we propose a method for\ndiscouraging correlation between the predicted probability of an individual's\ntrue occupation and a word embedding of their name. This method leverages the\nsocietal biases that are encoded in word embeddings, eliminating the need for\naccess to protected attributes. Crucially, it only requires access to\nindividuals' names at training time and not at deployment time. We evaluate two\nvariations of our proposed method using a large-scale dataset of online\nbiographies. We find that both variations simultaneously reduce race and gender\nbiases, with almost no reduction in the classifier's overall true positive\nrate.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05233v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05216v4",
    "title": "Dungeons for Science: Mapping Belief Places and Spaces",
    "authors": [
      "Aaron Dant",
      "Philip Feldman",
      "Wayne Lutters"
    ],
    "author_ids": [],
    "abstract": "Tabletop fantasy role-playing games (TFRPGs) have existed in offline and\nonline contexts for many decades, yet are rarely featured in scientific\nliterature. This paper presents a case study where TFRPGs were used to generate\nand collect data for maps of belief environments using fiction co-created by\nmultiple small groups of online tabletop gamers. The affordances of TFRPGs\nallowed us to collect repeatable, targeted data in online field conditions.\nThese data not only included terms that allowed us to build our maps, but also\nto explore nuanced ethical problems from a situated, collaborative perspective.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.SI",
      "J.4; H.5.1; H.5.3; I.3.0; K.4.3; K.8.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05216v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.05081v2",
    "title": "Relative-perfectness of discrete gradient vector fields and multi-parameter persistent homology",
    "authors": [
      "Claudia Landi",
      "Sara Scaramuccia"
    ],
    "author_ids": [],
    "abstract": "The combination of persistent homology and discrete Morse theory has proven\nvery effective in visualizing and analyzing big and heterogeneous data. Indeed,\ntopology provides computable and coarse summaries of data independently from\nspecific coordinate systems and does so robustly to noise. Moreover, the\ngeometric content of a discrete gradient vector field is very useful for\nvisualization purposes. The specific case of multivariate data still demands\nfor further investigations, on the one hand, for computational reasons, it is\nimportant to reduce the necessary amount of data to be processed. On the other\nhand, for analysis reasons, the multivariate case requires the detection and\ninterpretation of the possible interdepedance among data components. To this\nend, in this paper we introduce and study a notion of perfectness for discrete\ngradient vector fields with respect to multi-parameter persistent homology,\ncalled relative-perfectness. As a natural generalization of usual perfectness\nin Morse theory for homology, relative-perfectness entails having the least\nnumber of critical cells relevant for multi-parameter persistence. As a first\ncontribution, we support our definition of relative-perfectness by generalizing\nMorse inequalities to the filtration structure where homology groups involved\nare relative with respect to subsequent sublevel sets. In order to allow for an\ninterpretation of critical cells in $2$-parameter persistence, our second\ncontribution consists of two inequalities bounding Betti tables of persistence\nmodules from above and below, via the number of critical cells. Our last result\nis the proof that existing algorithms based on local homotopy expansions allow\nfor efficient computability over simplicial complexes up to dimension $2$.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CG",
      "55N35, 55U10, 37B35, 13D02"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05081v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.05020v2",
    "title": "Imitating Targets from all sides: An Unsupervised Transfer Learning method for Person Re-identification",
    "authors": [
      "Jiajie Tian",
      "Zhu Teng",
      "Rui Li",
      "Yan Li",
      "Baopeng Zhang",
      "Jianping Fan"
    ],
    "author_ids": [],
    "abstract": "Person re-identification (Re-ID) models usually show a limited performance\nwhen they are trained on one dataset and tested on another dataset due to the\ninter-dataset bias (e.g. completely different identities and backgrounds) and\nthe intra-dataset difference (e.g. camera invariance). In terms of this issue,\ngiven a labelled source training set and an unlabelled target training set, we\npropose an unsupervised transfer learning method characterized by 1) bridging\ninter-dataset bias and intra-dataset difference via a proposed ImitateModel\nsimultaneously; 2) regarding the unsupervised person Re-ID problem as a\nsemi-supervised learning problem formulated by a dual classification loss to\nlearn a discriminative representation across domains; 3) exploiting the\nunderlying commonality across different domains from the class-style space to\nimprove the generalization ability of re-ID models. Extensive experiments are\nconducted on two widely employed benchmarks, including Market-1501 and\nDukeMTMC-reID, and experimental results demonstrate that the proposed method\ncan achieve a competitive performance against other state-of-the-art\nunsupervised Re-ID approaches.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05020v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05005v1",
    "title": "Person Re-identification with Metric Learning using Privileged Information",
    "authors": [
      "Xun Yang",
      "Meng Wang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Despite the promising progress made in recent years, person re-identification\nremains a challenging task due to complex variations in human appearances from\ndifferent camera views. This paper presents a logistic discriminant metric\nlearning method for this challenging problem. Different with most existing\nmetric learning algorithms, it exploits both original data and auxiliary data\nduring training, which is motivated by the new machine learning paradigm -\nLearning Using Privileged Information. Such privileged information is a kind of\nauxiliary knowledge which is only available during training. Our goal is to\nlearn an optimal distance function by constructing a locally adaptive decision\nrule with the help of privileged information. We jointly learn two distance\nmetrics by minimizing the empirical loss penalizing the difference between the\ndistance in the original space and that in the privileged space. In our\nsetting, the distance in the privileged space functions as a local decision\nthreshold, which guides the decision making in the original space like a\nteacher. The metric learned from the original space is used to compute the\ndistance between a probe image and a gallery image during testing. In addition,\nwe extend the proposed approach to a multi-view setting which is able to\nexplore the complementation of multiple feature representations. In the\nmulti-view setting, multiple metrics corresponding to different original\nfeatures are jointly learned, guided by the same privileged information.\nBesides, an effective iterative optimization scheme is introduced to\nsimultaneously optimize the metrics and the assigned metric weights. Experiment\nresults on several widely-used datasets demonstrate that the proposed approach\nis superior to global decision threshold based methods and outperforms most\nstate-of-the-art results.",
    "published_date": "2019-04-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05005v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.04631v1",
    "title": "CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion",
    "authors": [
      "Takuhiro Kaneko",
      "Hirokazu Kameoka",
      "Kou Tanaka",
      "Nobukatsu Hojo"
    ],
    "author_ids": [],
    "abstract": "Non-parallel voice conversion (VC) is a technique for learning the mapping\nfrom source to target speech without relying on parallel data. This is an\nimportant task, but it has been challenging due to the disadvantages of the\ntraining conditions. Recently, CycleGAN-VC has provided a breakthrough and\nperformed comparably to a parallel VC method without relying on any extra data,\nmodules, or time alignment procedures. However, there is still a large gap\nbetween the real target and converted speech, and bridging this gap remains a\nchallenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved\nversion of CycleGAN-VC incorporating three new techniques: an improved\nobjective (two-step adversarial losses), improved generator (2-1-2D CNN), and\nimproved discriminator (PatchGAN). We evaluated our method on a non-parallel VC\ntask and analyzed the effect of each technique in detail. An objective\nevaluation showed that these techniques help bring the converted feature\nsequence closer to the target in terms of both global and local structures,\nwhich we assess by using Mel-cepstral distortion and modulation spectra\ndistance, respectively. A subjective evaluation showed that CycleGAN-VC2\noutperforms CycleGAN-VC in terms of naturalness and similarity for every\nspeaker pair, including intra-gender and inter-gender pairs.",
    "published_date": "2019-04-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04631v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.04608v1",
    "title": "Hyper-Parameter Tuning for the (1+(λ,λ)) GA",
    "authors": [
      "Nguyen Dang",
      "Carola Doerr"
    ],
    "author_ids": [],
    "abstract": "It is known that the $(1+(\\lambda,\\lambda))$~Genetic Algorithm (GA) with\nself-adjusting parameter choices achieves a linear expected optimization time\non OneMax if its hyper-parameters are suitably chosen. However, it is not very\nwell understood how the hyper-parameter settings influences the overall\nperformance of the $(1+(\\lambda,\\lambda))$~GA. Analyzing such multi-dimensional\ndependencies precisely is at the edge of what running time analysis can offer.\nTo make a step forward on this question, we present an in-depth empirical study\nof the self-adjusting $(1+(\\lambda,\\lambda))$~GA and its hyper-parameters. We\nshow, among many other results, that a 15\\% reduction of the average running\ntime is possible by a slightly different setup, which allows non-identical\noffspring population sizes of mutation and crossover phase, and more\nflexibility in the choice of mutation rate and crossover bias --a\ngeneralization which may be of independent interest. We also show indication\nthat the parametrization of mutation rate and crossover bias derived by\ntheoretical means for the static variant of the $(1+(\\lambda,\\lambda))$~GA\nextends to the non-static case.",
    "published_date": "2019-04-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04608v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.04555v1",
    "title": "Assessing Capsule Networks With Biased Data",
    "authors": [
      "Bruno Ferrarini",
      "Shoaib Ehsan",
      "Adrien Bartoli",
      "Aleš Leonardis",
      "Klaus D. McDonald-Maier"
    ],
    "author_ids": [],
    "abstract": "Machine learning based methods achieves impressive results in object\nclassification and detection. Utilizing representative data of the visual world\nduring the training phase is crucial to achieve good performance with such data\ndriven approaches. However, it not always possible to access bias-free datasets\nthus, robustness to biased data is a desirable property for a learning system.\nCapsule Networks have been introduced recently and their tolerance to biased\ndata has received little attention. This paper aims to fill this gap and\nproposes two experimental scenarios to assess the tolerance to imbalanced\ntraining data and to determine the generalization performance of a model with\nunfamiliar affine transformations of the images. This paper assesses dynamic\nrouting and EM routing based Capsule Networks and proposes a comparison with\nConvolutional Neural Networks in the two tested scenarios. The presented\nresults provide new insights into the behaviour of capsule networks.",
    "published_date": "2019-04-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "00B25"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04555v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.04363v1",
    "title": "A Robust Visual System for Small Target Motion Detection Against Cluttered Moving Backgrounds",
    "authors": [
      "Hongxin Wang",
      "Jigen Peng",
      "Xuqiang Zheng",
      "Shigang Yue"
    ],
    "author_ids": [],
    "abstract": "Monitoring small objects against cluttered moving backgrounds is a huge\nchallenge to future robotic vision systems. As a source of inspiration, insects\nare quite apt at searching for mates and tracking prey -- which always appear\nas small dim speckles in the visual field. The exquisite sensitivity of insects\nfor small target motion, as revealed recently, is coming from a class of\nspecific neurons called small target motion detectors (STMDs). Although a few\nSTMD-based models have been proposed, these existing models only use motion\ninformation for small target detection and cannot discriminate small targets\nfrom small-target-like background features (named as fake features). To address\nthis problem, this paper proposes a novel visual system model (STMD+) for small\ntarget motion detection, which is composed of four subsystems -- ommatidia,\nmotion pathway, contrast pathway and mushroom body. Compared to existing\nSTMD-based models, the additional contrast pathway extracts directional\ncontrast from luminance signals to eliminate false positive background motion.\nThe directional contrast and the extracted motion information by the motion\npathway are integrated in the mushroom body for small target discrimination.\nExtensive experiments showed the significant and consistent improvements of the\nproposed visual system model over existing STMD-based models against fake\nfeatures.",
    "published_date": "2019-04-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04363v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.04317v2",
    "title": "$\\mathcal{G}$-softmax: Improving Intra-class Compactness and Inter-class Separability of Features",
    "authors": [
      "Yan Luo",
      "Yongkang Wong",
      "Mohan Kankanhalli",
      "Qi Zhao"
    ],
    "author_ids": [],
    "abstract": "Intra-class compactness and inter-class separability are crucial indicators\nto measure the effectiveness of a model to produce discriminative features,\nwhere intra-class compactness indicates how close the features with the same\nlabel are to each other and inter-class separability indicates how far away the\nfeatures with different labels are. In this work, we investigate intra-class\ncompactness and inter-class separability of features learned by convolutional\nnetworks and propose a Gaussian-based softmax ($\\mathcal{G}$-softmax) function\nthat can effectively improve intra-class compactness and inter-class\nseparability. The proposed function is simple to implement and can easily\nreplace the softmax function. We evaluate the proposed $\\mathcal{G}$-softmax\nfunction on classification datasets (i.e., CIFAR-10, CIFAR-100, and Tiny\nImageNet) and on multi-label classification datasets (i.e., MS COCO and\nNUS-WIDE). The experimental results show that the proposed\n$\\mathcal{G}$-softmax function improves the state-of-the-art models across all\nevaluated datasets. In addition, analysis of the intra-class compactness and\ninter-class separability demonstrates the advantages of the proposed function\nover the softmax function, which is consistent with the performance\nimprovement. More importantly, we observe that high intra-class compactness and\ninter-class separability are linearly correlated to average precision on MS\nCOCO and NUS-WIDE. This implies that improvement of intra-class compactness and\ninter-class separability would lead to improvement of average precision.",
    "published_date": "2019-04-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04317v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.04205v5",
    "title": "Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions",
    "authors": [
      "Hoel Kervadec",
      "Jose Dolz",
      "Jing Yuan",
      "Christian Desrosiers",
      "Eric Granger",
      "Ismail Ben Ayed"
    ],
    "author_ids": [],
    "abstract": "This study investigates imposing hard inequality constraints on the outputs\nof convolutional neural networks (CNN) during training. Several recent works\nshowed that the theoretical and practical advantages of Lagrangian optimization\nover simple penalties do not materialize in practice when dealing with modern\nCNNs involving millions of parameters. Therefore, constrained CNNs are\ntypically handled with penalties. We propose *log-barrier extensions*, which\napproximate Lagrangian optimization of constrained-CNN problems with a sequence\nof unconstrained losses. Unlike standard interior-point and log-barrier\nmethods, our formulation does not need an initial feasible solution. The\nproposed extension yields an upper bound on the duality gap -- generalizing the\nresult of standard log-barriers -- and yielding sub-optimality certificates for\nfeasible solutions. While sub-optimality is not guaranteed for non-convex\nproblems, this result shows that log-barrier extensions are a principled way to\napproximate Lagrangian optimization for constrained CNNs via implicit dual\nvariables. We report weakly supervised image segmentation experiments, with\nvarious constraints, showing that our formulation outperforms substantially the\nexisting constrained-CNN methods, in terms of accuracy, constraint satisfaction\nand training stability, more so when dealing with a large number of\nconstraints.",
    "published_date": "2019-04-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04205v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03885v1",
    "title": "Referring to Objects in Videos using Spatio-Temporal Identifying Descriptions",
    "authors": [
      "Peratham Wiriyathammabhum",
      "Abhinav Shrivastava",
      "Vlad I. Morariu",
      "Larry S. Davis"
    ],
    "author_ids": [],
    "abstract": "This paper presents a new task, the grounding of spatio-temporal identifying\ndescriptions in videos. Previous work suggests potential bias in existing\ndatasets and emphasizes the need for a new data creation schema to better model\nlinguistic structure. We introduce a new data collection scheme based on\ngrammatical constraints for surface realization to enable us to investigate the\nproblem of grounding spatio-temporal identifying descriptions in videos. We\nthen propose a two-stream modular attention network that learns and grounds\nspatio-temporal identifying descriptions based on appearance and motion. We\nshow that motion modules help to ground motion-related words and also help to\nlearn in appearance modules because modular neural networks resolve task\ninterference between modules. Finally, we propose a future challenge and a need\nfor a robust system arising from replacing ground truth visual annotations with\nautomatic video object detector and temporal event localization.",
    "published_date": "2019-04-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03885v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03779v1",
    "title": "Cluster Developing 1-Bit Matrix Completion",
    "authors": [
      "Chengkun Zhang. Junbin Gao",
      "Stephen Lu"
    ],
    "author_ids": [],
    "abstract": "Matrix completion has a long-time history of usage as the core technique of\nrecommender systems. In particular, 1-bit matrix completion, which considers\nthe prediction as a ``Recommended'' or ``Not Recommended'' question, has proved\nits significance and validity in the field. However, while customers and\nproducts aggregate into interacted clusters, state-of-the-art model-based 1-bit\nrecommender systems do not take the consideration of grouping bias. To tackle\nthe gap, this paper introduced Group-Specific 1-bit Matrix Completion (GS1MC)\nby first-time consolidating group-specific effects into 1-bit recommender\nsystems under the low-rank latent variable framework. Additionally, to empower\nGS1MC even when grouping information is unobtainable, Cluster Developing Matrix\nCompletion (CDMC) was proposed by integrating the sparse subspace clustering\ntechnique into GS1MC. Namely, CDMC allows clustering users/items and to\nleverage their group effects into matrix completion at the same time.\nExperiments on synthetic and real-world data show that GS1MC outperforms the\ncurrent 1-bit matrix completion methods. Meanwhile, it is compelling that CDMC\ncan successfully capture items' genre features only based on sparse binary\nuser-item interactive data. Notably, GS1MC provides a new insight to\nincorporate and evaluate the efficacy of clustering methods while CDMC can be\nserved as a new tool to explore unrevealed social behavior or market\nphenomenon.",
    "published_date": "2019-04-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03779v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03647v4",
    "title": "Bayesian Estimation of Mixed Multinomial Logit Models: Advances and Simulation-Based Evaluations",
    "authors": [
      "Prateek Bansal",
      "Rico Krueger",
      "Michel Bierlaire",
      "Ricardo A. Daziano",
      "Taha H. Rashidi"
    ],
    "author_ids": [],
    "abstract": "Variational Bayes (VB) methods have emerged as a fast and\ncomputationally-efficient alternative to Markov chain Monte Carlo (MCMC)\nmethods for scalable Bayesian estimation of mixed multinomial logit (MMNL)\nmodels. It has been established that VB is substantially faster than MCMC at\npractically no compromises in predictive accuracy. In this paper, we address\ntwo critical gaps concerning the usage and understanding of VB for MMNL. First,\nextant VB methods are limited to utility specifications involving only\nindividual-specific taste parameters. Second, the finite-sample properties of\nVB estimators and the relative performance of VB, MCMC and maximum simulated\nlikelihood estimation (MSLE) are not known. To address the former, this study\nextends several VB methods for MMNL to admit utility specifications including\nboth fixed and random utility parameters. To address the latter, we conduct an\nextensive simulation-based evaluation to benchmark the extended VB methods\nagainst MCMC and MSLE in terms of estimation times, parameter recovery and\npredictive accuracy. The results suggest that all VB variants with the\nexception of the ones relying on an alternative variational lower bound\nconstructed with the help of the modified Jensen's inequality perform as well\nas MCMC and MSLE at prediction and parameter recovery. In particular, VB with\nnonconjugate variational message passing and the delta-method (VB-NCVMP-Delta)\nis up to 16 times faster than MCMC and MSLE. Thus, VB-NCVMP-Delta can be an\nattractive alternative to MCMC and MSLE for fast, scalable and accurate\nestimation of MMNL models.",
    "published_date": "2019-04-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03647v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03534v1",
    "title": "Automatic Target Recognition Using Discrimination Based on Optimal Transport",
    "authors": [
      "Ali Sadeghian",
      "Deoksu Lim",
      "Johan Karlsson",
      "Jian Li"
    ],
    "author_ids": [],
    "abstract": "The use of distances based on optimal transportation has recently shown\npromise for discrimination of power spectra. In particular, spectral estimation\nmethods based on l1 regularization as well as covariance based methods can be\nshown to be robust with respect to such distances. These transportation\ndistances provide a geometric framework where geodesics corresponds to smooth\ntransition of spectral mass, and have been useful for tracking. In this paper,\nwe investigate the use of these distances for automatic target recognition. We\nstudy the use of the Monge-Kantorovich distance compared to the standard l2\ndistance for classifying civilian vehicles based on SAR images. We use a\nversion of the Monge-Kantorovich distance that applies also for the case where\nthe spectra may have different total mass, and we formulate the optimization\nproblem as a minimum flow problem that can be computed using efficient\nalgorithms.",
    "published_date": "2019-04-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03534v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03352v2",
    "title": "Simple dynamic word embeddings for mapping perceptions in the public sphere",
    "authors": [
      "Nabeel Gillani",
      "Roger Levy"
    ],
    "author_ids": [],
    "abstract": "Word embeddings trained on large-scale historical corpora can illuminate\nhuman biases and stereotypes that perpetuate social inequalities. These\nembeddings are often trained in separate vector space models defined according\nto different attributes of interest. In this paper, we develop a unified\ndynamic embedding model that learns attribute-specific word embeddings. We\napply our model to investigate i) 20th century gender and ethnic occupation\nbiases embedded in the Corpus of Historical American English (COHA), and ii)\nbiases against refugees embedded in a novel corpus of talk radio transcripts\ncontaining 119 million words produced over one month across 83 stations and 64\ncities. Our results shed preliminary light on scenarios when dynamic embedding\nmodels may be more suitable for representing linguistic biases than individual\nvector space models, and vice-versa.",
    "published_date": "2019-04-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03352v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03328v2",
    "title": "Mitigating Gyral Bias in Cortical Tractography via Asymmetric Fiber Orientation Distributions",
    "authors": [
      "Ye Wu",
      "Yoonmi Hong",
      "Yuanjing Feng",
      "Dinggang Shen",
      "Pew-Thian Yap"
    ],
    "author_ids": [],
    "abstract": "Diffusion tractography in brain connectomics often involves tracing axonal\ntrajectories across gray-white matter boundaries in gyral blades of complex\ncortical convolutions. To date, gyral bias is observed in most tractography\nalgorithms with streamlines predominantly terminating at gyral crowns instead\nof sulcal banks. This work demonstrates that asymmetric fiber orientation\ndistribution functions (AFODFs), computed via a multi-tissue global estimation\nframework, can mitigate the effects of gyral bias, enabling fiber streamlines\nat gyral blades to make sharper turns into the cortical gray matter. We use\nex-vivo data of an adult rhesus macaque and in-vivo data from the Human\nConnectome Project (HCP) to show that the fiber streamlines given by AFODFs\nbend more naturally into the cortex than the conventional symmetric FODFs in\ntypical gyral blades. We demonstrate that AFODF tractography improves\ncortico-cortical connectivity and provides highly consistent outcomes between\ntwo different field strengths (3T and 7T).",
    "published_date": "2019-04-06T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.NC",
      "cs.CE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03328v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.03310v1",
    "title": "Gender Bias in Contextualized Word Embeddings",
    "authors": [
      "Jieyu Zhao",
      "Tianlu Wang",
      "Mark Yatskar",
      "Ryan Cotterell",
      "Vicente Ordonez",
      "Kai-Wei Chang"
    ],
    "author_ids": [],
    "abstract": "In this paper, we quantify, analyze and mitigate gender bias exhibited in\nELMo's contextualized word vectors. First, we conduct several intrinsic\nanalyses and find that (1) training data for ELMo contains significantly more\nmale than female entities, (2) the trained ELMo embeddings systematically\nencode gender information and (3) ELMo unequally encodes gender information\nabout male and female entities. Then, we show that a state-of-the-art\ncoreference system that depends on ELMo inherits its bias and demonstrates\nsignificant bias on the WinoBias probing corpus. Finally, we explore two\nmethods to mitigate such gender bias and show that the bias demonstrated on\nWinoBias can be eliminated.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03310v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.05335v1",
    "title": "Adapting Stochastic Block Models to Power-Law Degree Distributions",
    "authors": [
      "Maoying Qiao",
      "Jun Yu",
      "Wei Bian",
      "Qiang Li",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Stochastic block models (SBMs) have been playing an important role in\nmodeling clusters or community structures of network data. But, it is incapable\nof handling several complex features ubiquitously exhibited in real-world\nnetworks, one of which is the power-law degree characteristic. To this end, we\npropose a new variant of SBM, termed power-law degree SBM (PLD-SBM), by\nintroducing degree decay variables to explicitly encode the varying degree\ndistribution over all nodes. With an exponential prior, it is proved that\nPLD-SBM approximately preserves the scale-free feature in real networks. In\naddition, from the inference of variational E-Step, PLD-SBM is indeed to\ncorrect the bias inherited in SBM with the introduced degree decay factors.\nFurthermore, experiments conducted on both synthetic networks and two\nreal-world datasets including Adolescent Health Data and the political blogs\nnetwork verify the effectiveness of the proposed model in terms of cluster\nprediction accuracies.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.05335v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03063v1",
    "title": "Bayesian Heatmaps: Probabilistic Classification with Multiple Unreliable Information Sources",
    "authors": [
      "Edwin Simpson",
      "Steven Reece",
      "Stephen J. Roberts"
    ],
    "author_ids": [],
    "abstract": "Unstructured data from diverse sources, such as social media and aerial\nimagery, can provide valuable up-to-date information for intelligent situation\nassessment. Mining these different information sources could bring major\nbenefits to applications such as situation awareness in disaster zones and\nmapping the spread of diseases. Such applications depend on classifying the\nsituation across a region of interest, which can be depicted as a spatial\n\"heatmap\". Annotating unstructured data using crowdsourcing or automated\nclassifiers produces individual classifications at sparse locations that\ntypically contain many errors. We propose a novel Bayesian approach that models\nthe relevance, error rates and bias of each information source, enabling us to\nlearn a spatial Gaussian Process classifier by aggregating data from multiple\nsources with varying reliability and relevance. Our method does not require\ngold-labelled data and can make predictions at any location in an area of\ninterest given only sparse observations. We show empirically that our approach\ncan handle noisy and biased data sources, and that simultaneously inferring\nreliability and transferring information between neighbouring reports leads to\nmore accurate predictions. We demonstrate our method on two real-world problems\nfrom disaster response, showing how our approach reduces the amount of\ncrowdsourced data required and can be used to generate valuable heatmap\nvisualisations from SMS messages and satellite images.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03063v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03042v2",
    "title": "Event-triggered Learning",
    "authors": [
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ],
    "author_ids": [],
    "abstract": "The efficient exchange of information is an essential aspect of intelligent\ncollective behavior. Event-triggered control and estimation achieve some\nefficiency by replacing continuous data exchange between agents with\nintermittent, or event-triggered communication. Typically, model-based\npredictions are used at times of no data transmission, and updates are sent\nonly when the prediction error grows too large. The effectiveness in reducing\ncommunication thus strongly depends on the quality of the prediction model. In\nthis article, we propose event-triggered learning as a novel concept to reduce\ncommunication even further and to also adapt to changing dynamics. By\nmonitoring the actual communication rate and comparing it to the one that is\ninduced by the model, we detect a mismatch between model and reality and\ntrigger model learning when needed. Specifically, for linear Gaussian dynamics,\nwe derive different classes of learning triggers solely based on a statistical\nanalysis of inter-communication times and formally prove their effectiveness\nwith the aid of concentration inequalities.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03042v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.03041v1",
    "title": "Automatic detection of lesion load change in Multiple Sclerosis using convolutional neural networks with segmentation confidence",
    "authors": [
      "Richard McKinley",
      "Lorenz Grunder",
      "Rik Wepfer",
      "Fabian Aschwanden",
      "Tim Fischer",
      "Christoph Friedli",
      "Raphaela Muri",
      "Christian Rummel",
      "Rajeev Verma",
      "Christian Weisstanner",
      "Mauricio Reyes",
      "Anke Salmen",
      "Andrew Chan",
      "Roland Wiest",
      "Franca Wagner"
    ],
    "author_ids": [],
    "abstract": "The detection of new or enlarged white-matter lesions in multiple sclerosis\nis a vital task in the monitoring of patients undergoing disease-modifying\ntreatment for multiple sclerosis. However, the definition of 'new or enlarged'\nis not fixed, and it is known that lesion-counting is highly subjective, with\nhigh degree of inter- and intra-rater variability. Automated methods for lesion\nquantification hold the potential to make the detection of new and enlarged\nlesions consistent and repeatable. However, the majority of lesion segmentation\nalgorithms are not evaluated for their ability to separate progressive from\nstable patients, despite this being a pressing clinical use-case. In this paper\nwe show that change in volumetric measurements of lesion load alone is not a\ngood method for performing this separation, even for highly performing\nsegmentation methods. Instead, we propose a method for identifying lesion\nchanges of high certainty, and establish on a dataset of longitudinal multiple\nsclerosis cases that this method is able to separate progressive from stable\ntimepoints with a very high level of discrimination (AUC = 0.99), while changes\nin lesion volume are much less able to perform this separation (AUC = 0.71).\nValidation of the method on a second external dataset confirms that the method\nis able to generalize beyond the setting in which it was trained, achieving an\naccuracy of 83% in separating stable and progressive timepoints. Both lesion\nvolume and count have previously been shown to be strong predictors of disease\ncourse across a population. However, we demonstrate that for individual\npatients, changes in these measures are not an adequate means of establishing\nno evidence of disease activity. Meanwhile, directly detecting tissue which\nchanges, with high confidence, from non-lesion to lesion is a feasible\nmethodology for identifying radiologically active patients.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03041v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.03035v1",
    "title": "Identifying and Reducing Gender Bias in Word-Level Language Models",
    "authors": [
      "Shikha Bordia",
      "Samuel R. Bowman"
    ],
    "author_ids": [],
    "abstract": "Many text corpora exhibit socially problematic biases, which can be\npropagated or amplified in the models trained on such data. For example, doctor\ncooccurs more frequently with male pronouns than female pronouns. In this study\nwe (i) propose a metric to measure gender bias; (ii) measure bias in a text\ncorpus and the text generated from a recurrent neural network language model\ntrained on the text corpus; (iii) propose a regularization loss term for the\nlanguage model that minimizes the projection of encoder-trained embeddings onto\nan embedding subspace that encodes gender; (iv) finally, evaluate efficacy of\nour proposed method on reducing gender bias. We find this regularization method\nto be effective in reducing gender bias up to an optimal weight assigned to the\nloss term, beyond which the model becomes unstable as the perplexity increases.\nWe replicate this study on three training corpora---Penn Treebank, WikiText-2,\nand CNN/Daily Mail---resulting in similar conclusions.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03035v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.02963v2",
    "title": "Graph Learning over Partially Observed Diffusion Networks: Role of Degree Concentration",
    "authors": [
      "Vincenzo Matta",
      "Augusto Santos",
      "Ali H. Sayed"
    ],
    "author_ids": [],
    "abstract": "This work examines the problem of graph learning over a diffusion network\nwhen data can be collected from a limited portion of the network (partial\nobservability). The main question is to establish technical guarantees of\nconsistent recovery of the subgraph of probed network nodes, i) despite the\npresence of unobserved nodes; and ii) under different connectivity regimes,\nincluding the dense regime where the probed nodes are influenced by many\nconnections coming from the unobserved ones. We ascertain that suitable\nestimators of the combination matrix (i.e., the matrix that quantifies the\npairwise interaction between nodes) possess an identifiability gap that enables\nthe discrimination between connected and disconnected nodes. Fundamental\nconditions are established under which the subgraph of monitored nodes can be\nrecovered, with high probability as the network size increases, through\nuniversal clustering algorithms. This claim is proved for three matrix\nestimators: i) the Granger estimator that adapts to the partial observability\nsetting the solution that is exact under full observability ; ii) the one-lag\ncorrelation matrix; and iii) the residual estimator based on the difference\nbetween two consecutive time samples. A detailed characterization of the\nasymptotic behavior of these estimators is established in terms of an error\nbias and of the identifiability gap, and a sample complexity analysis is\nperformed to establish how the number of samples scales with the network size\nto achieve consistent learning. Comparison among the estimators is performed\nthrough illustrative examples that show how estimators that are not optimal in\nthe full observability regime can outperform the Granger estimator in the\npartial observability regime. The analysis reveals that the fundamental\nproperty enabling consistent graph learning is the statistical concentration of\nnode degrees.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "math.ST",
      "cs.MA",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02963v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.02832v1",
    "title": "A Regularization Approach for Instance-Based Superset Label Learning",
    "authors": [
      "Chen Gong",
      "Tongliang Liu",
      "Yuanyan Tang",
      "Jian Yang",
      "Jie Yang",
      "Dacheng Tao"
    ],
    "author_ids": [],
    "abstract": "Different from the traditional supervised learning in which each training\nexample has only one explicit label, superset label learning (SLL) refers to\nthe problem that a training example can be associated with a set of candidate\nlabels, and only one of them is correct. Existing SLL methods are either\nregularization-based or instance-based, and the latter of which has achieved\nstate-of-the-art performance. This is because the latest instance-based methods\ncontain an explicit disambiguation operation that accurately picks up the\ngroundtruth label of each training example from its ambiguous candidate labels.\nHowever, such disambiguation operation does not fully consider the mutually\nexclusive relationship among different candidate labels, so the disambiguated\nlabels are usually generated in a nondiscriminative way, which is unfavorable\nfor the instance-based methods to obtain satisfactory performance. To address\nthis defect, we develop a novel regularization approach for instance-based\nsuperset label (RegISL) learning so that our instance-based method also\ninherits the good discriminative ability possessed by the regularization\nscheme. Specifically, we employ a graph to represent the training set, and\nrequire the examples that are adjacent on the graph to obtain similar labels.\nMore importantly, a discrimination term is proposed to enlarge the gap of\nvalues between possible labels and unlikely labels for every training example.\nAs a result, the intrinsic constraints among different candidate labels are\ndeployed, and the disambiguated labels generated by RegISL are more\ndiscriminative and accurate than those output by existing instance-based\nalgorithms. The experimental results on various tasks convincingly demonstrate\nthe superiority of our RegISL to other typical SLL methods in terms of both\ntraining accuracy and test accuracy.",
    "published_date": "2019-04-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02832v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.02672v1",
    "title": "Deep Multi-class Adversarial Specularity Removal",
    "authors": [
      "John Lin",
      "Mohamed El Amine Seddik",
      "Mohamed Tamaazousti",
      "Youssef Tamaazousti",
      "Adrien Bartoli"
    ],
    "author_ids": [],
    "abstract": "We propose a novel learning approach, in the form of a fully-convolutional\nneural network (CNN), which automatically and consistently removes specular\nhighlights from a single image by generating its diffuse component. To train\nthe generative network, we define an adversarial loss on a discriminative\nnetwork as in the GAN framework and combined it with a content loss. In\ncontrast to existing GAN approaches, we implemented the discriminator to be a\nmulti-class classifier instead of a binary one, to find more constraining\nfeatures. This helps the network pinpoint the diffuse manifold by providing two\nmore gradient terms. We also rendered a synthetic dataset designed to help the\nnetwork generalize well. We show that our model performs well across various\nsynthetic and real images and outperforms the state-of-the-art in consistency.",
    "published_date": "2019-04-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02672v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.02477v1",
    "title": "Is it Possible to Disregard Obsolete Requirements? - An Initial Experiment on a Potentially New Bias in Software Effort Estimation",
    "authors": [
      "Lucas Gren",
      "Richard Berntsson Svensson",
      "Michael Unterkalmsteiner"
    ],
    "author_ids": [],
    "abstract": "Effort estimation is a complex area in decision-making, and is influenced by\na diversity of factors that could increase the estimation error. The effects on\neffort estimation accuracy of having obsolete requirements in specifications\nhave not yet been studied. This study aims at filling that gap. A total of 150\nstudents were asked to provide effort estimates for different amounts of\nrequirements, and one group was explicitly told to disregard some of the given\nrequirements. The results show that even the extra text instructing\nparticipants to exclude requirements in the estimation task, had the subjects\ngive higher estimates. The effect of having obsolete requirements in\nrequirements specifications and backlogs in software effort estimation is not\ntaken into account enough today, and this study provides empirical evidence\nthat it possibly should. We also suggest different psychological explanations\nto the found effect.",
    "published_date": "2019-04-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02477v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.02426v2",
    "title": "Efficient GAN-based method for cyber-intrusion detection",
    "authors": [
      "Hongyu Chen",
      "Li Jiang"
    ],
    "author_ids": [],
    "abstract": "Ubiquitous anomalies endanger the security of our system constantly. They may\nbring irreversible damages to the system and cause leakage of privacy. Thus, it\nis of vital importance to promptly detect these anomalies. Traditional\nsupervised methods such as Decision Trees and Support Vector Machine (SVM) are\nused to classify normality and abnormality. However, in some case the abnormal\nstatus are largely rarer than normal status, which leads to decision bias of\nthese methods. Generative adversarial network (GAN) has been proposed to handle\nthe case. With its strong generative ability, it only needs to learn the\ndistribution of normal status, and identify the abnormal status through the gap\nbetween it and the learned distribution. Nevertheless, existing GAN-based\nmodels are not suitable to process data with discrete values, leading to\nimmense degradation of detection performance. To cope with the discrete\nfeatures, in this paper, we propose an efficient GAN-based model with\nspecifically-designed loss function. Experiment results show that our model\noutperforms state-of-the-art models on discrete dataset and remarkably reduce\nthe overhead.",
    "published_date": "2019-04-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02426v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.02385v2",
    "title": "Non-Bayesian Social Learning with Imperfect Private Signal Structure",
    "authors": [
      "Sannyuya Liu",
      "Zhonghua Yan",
      "Xiufeng Cheng",
      "Liang Zhao"
    ],
    "author_ids": [],
    "abstract": "As one of the classic models that describe the belief dynamics over social\nnetworks, a non-Bayesian social learning model assumes that members in the\nnetwork possess accurate signal knowledge through the process of Bayesian\ninference. In order to make the non-Bayesian social learning model more\napplicable to human and animal societies, this paper extended this model by\nassuming the existence of private signal structure bias. Each social member in\neach time step uses an imperfect signal knowledge to form its Bayesian part\nbelief and then incorporates its neighbors' beliefs into this Bayesian part\nbelief to form a new belief report. First, we investigated the intrinsic\nlearning ability of an isolated agent and deduced the conditions that the\nsignal structure needs to satisfy for this isolated agent to make an eventually\ncorrect decision. According to these conditions, agents' signal structures were\nfurther divided into three different types, \"conservative,\" \"radical,\" and\n\"negative.\" Then, we switched the context from isolated agents to a connected\nnetwork; our propositions and simulations show that the conservative agents are\nthe dominant force for the social network to learn the real state, while the\nother two types might prevent the network from successful learning. Although\nfragilities do exist in non-Bayesian social learning mechanism, \"be more\nconservative\" and \"avoid overconfidence\" could be effective strategies for each\nagent in the real social networks to collectively improve social learning\nprocesses and results.",
    "published_date": "2019-04-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02385v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.02314v1",
    "title": "Two remarks on generalized entropy power inequalities",
    "authors": [
      "Mokshay Madiman",
      "Piotr Nayar",
      "Tomasz Tkocz"
    ],
    "author_ids": [],
    "abstract": "This note contributes to the understanding of generalized entropy power\ninequalities. Our main goal is to construct a counter-example regarding\nmonotonicity and entropy comparison of weighted sums of independent identically\ndistributed log-concave random variables. We also present a complex analogue of\na recent dependent entropy power inequality of Hao and Jog, and give a very\nsimple proof.",
    "published_date": "2019-04-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.PR",
      "94A17"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02314v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.04047v3",
    "title": "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
    "authors": [
      "Thomas Manzini",
      "Yao Chong Lim",
      "Yulia Tsvetkov",
      "Alan W Black"
    ],
    "author_ids": [],
    "abstract": "Online texts -- across genres, registers, domains, and styles -- are riddled\nwith human stereotypes, expressed in overt or subtle ways. Word embeddings,\ntrained on these texts, perpetuate and amplify these stereotypes, and propagate\nbiases to machine learning models that use word embeddings as features. In this\nwork, we propose a method to debias word embeddings in multiclass settings such\nas race and religion, extending the work of (Bolukbasi et al., 2016) from the\nbinary setting, such as binary gender. Next, we propose a novel methodology for\nthe evaluation of multiclass debiasing. We demonstrate that our multiclass\ndebiasing is robust and maintains the efficacy in standard NLP tasks.",
    "published_date": "2019-04-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.04047v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.02098v2",
    "title": "The Medical Deconfounder: Assessing Treatment Effects with Electronic Health Records",
    "authors": [
      "Linying Zhang",
      "Yixin Wang",
      "Anna Ostropolets",
      "Jami J. Mulgrave",
      "David M. Blei",
      "George Hripcsak"
    ],
    "author_ids": [],
    "abstract": "The treatment effects of medications play a key role in guiding medical\nprescriptions. They are usually assessed with randomized controlled trials\n(RCTs), which are expensive. Recently, large-scale electronic health records\n(EHRs) have become available, opening up new opportunities for more\ncost-effective assessments. However, assessing a treatment effect from EHRs is\nchallenging: it is biased by unobserved confounders, unmeasured variables that\naffect both patients' medical prescription and their outcome, e.g. the\npatients' social economic status. To adjust for unobserved confounders, we\ndevelop the medical deconfounder, a machine learning algorithm that unbiasedly\nestimates treatment effects from EHRs. The medical deconfounder first\nconstructs a substitute confounder by modeling which medications were\nprescribed to each patient; this substitute confounder is guaranteed to capture\nall multi-medication confounders, observed or unobserved (arXiv:1805.06826). It\nthen uses this substitute confounder to adjust for the confounding bias in the\nanalysis. We validate the medical deconfounder on two simulated and two real\nmedical data sets. Compared to classical approaches, the medical deconfounder\nproduces closer-to-truth treatment effect estimates; it also identifies\neffective medications that are more consistent with the findings in the medical\nliterature.",
    "published_date": "2019-04-03T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02098v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.02095v5",
    "title": "Discrimination through optimization: How Facebook's ad delivery can lead to skewed outcomes",
    "authors": [
      "Muhammad Ali",
      "Piotr Sapiezynski",
      "Miranda Bogen",
      "Aleksandra Korolova",
      "Alan Mislove",
      "Aaron Rieke"
    ],
    "author_ids": [],
    "abstract": "The enormous financial success of online advertising platforms is partially\ndue to the precise targeting features they offer. Although researchers and\njournalists have found many ways that advertisers can target---or\nexclude---particular groups of users seeing their ads, comparatively little\nattention has been paid to the implications of the platform's ad delivery\nprocess, comprised of the platform's choices about which users see which ads.\n  It has been hypothesized that this process can \"skew\" ad delivery in ways\nthat the advertisers do not intend, making some users less likely than others\nto see particular ads based on their demographic characteristics. In this\npaper, we demonstrate that such skewed delivery occurs on Facebook, due to\nmarket and financial optimization effects as well as the platform's own\npredictions about the \"relevance\" of ads to different groups of users. We find\nthat both the advertiser's budget and the content of the ad each significantly\ncontribute to the skew of Facebook's ad delivery. Critically, we observe\nsignificant skew in delivery along gender and racial lines for \"real\" ads for\nemployment and housing opportunities despite neutral targeting parameters.\n  Our results demonstrate previously unknown mechanisms that can lead to\npotentially discriminatory ad delivery, even when advertisers set their\ntargeting parameters to be highly inclusive. This underscores the need for\npolicymakers and platforms to carefully consider the role of the ad delivery\noptimization run by ad platforms themselves---and not just the targeting\nchoices of advertisers---in preventing discrimination in digital advertising.",
    "published_date": "2019-04-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02095v5",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.02057v2",
    "title": "Interpreting Adversarial Examples by Activation Promotion and Suppression",
    "authors": [
      "Kaidi Xu",
      "Sijia Liu",
      "Gaoyuan Zhang",
      "Mengshu Sun",
      "Pu Zhao",
      "Quanfu Fan",
      "Chuang Gan",
      "Xue Lin"
    ],
    "author_ids": [],
    "abstract": "It is widely known that convolutional neural networks (CNNs) are vulnerable\nto adversarial examples: images with imperceptible perturbations crafted to\nfool classifiers. However, interpretability of these perturbations is less\nexplored in the literature. This work aims to better understand the roles of\nadversarial perturbations and provide visual explanations from pixel, image and\nnetwork perspectives. We show that adversaries have a promotion-suppression\neffect (PSE) on neurons' activations and can be primarily categorized into\nthree types: i) suppression-dominated perturbations that mainly reduce the\nclassification score of the true label, ii) promotion-dominated perturbations\nthat focus on boosting the confidence of the target label, and iii) balanced\nperturbations that play a dual role in suppression and promotion. We also\nprovide image-level interpretability of adversarial examples. This links PSE of\npixel-level perturbations to class-specific discriminative image regions\nlocalized by class activation mapping (Zhou et al. 2016). Further, we examine\nthe adversarial effect through network dissection (Bau et al. 2017), which\noffers concept-level interpretability of hidden units. We show that there\nexists a tight connection between the units' sensitivity to adversarial attacks\nand their interpretability on semantic concepts. Lastly, we provide some new\ninsights from our interpretation to improve the adversarial robustness of\nnetworks.",
    "published_date": "2019-04-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.02057v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01793v2",
    "title": "Preference-Informed Fairness",
    "authors": [
      "Michael P. Kim",
      "Aleksandra Korolova",
      "Guy N. Rothblum",
      "Gal Yona"
    ],
    "author_ids": [],
    "abstract": "We study notions of fairness in decision-making systems when individuals have\ndiverse preferences over the possible outcomes of the decisions. Our starting\npoint is the seminal work of Dwork et al. which introduced a notion of\nindividual fairness (IF): given a task-specific similarity metric, every pair\nof individuals who are similarly qualified according to the metric should\nreceive similar outcomes. We show that when individuals have diverse\npreferences over outcomes, requiring IF may unintentionally lead to\nless-preferred outcomes for the very individuals that IF aims to protect. A\nnatural alternative to IF is the classic notion of fair division, envy-freeness\n(EF): no individual should prefer another individual's outcome over their own.\nAlthough EF allows for solutions where all individuals receive a\nhighly-preferred outcome, EF may also be overly-restrictive. For instance, if\nmany individuals agree on the best outcome, then if any individual receives\nthis outcome, they all must receive it, regardless of each individual's\nunderlying qualifications for the outcome.\n  We introduce and study a new notion of preference-informed individual\nfairness (PIIF) that is a relaxation of both individual fairness and\nenvy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style\nconstraints, but allows for deviations provided they are in line with\nindividuals' preferences. We show that PIIF can permit outcomes that are more\nfavorable to individuals than any IF solution, while providing considerably\nmore flexibility to the decision-maker than EF. In addition, we show how to\nefficiently optimize any convex objective over the outcomes subject to PIIF for\na rich class of individual preferences. Finally, we demonstrate the broad\napplicability of the PIIF framework by extending our definitions and algorithms\nto the multiple-task targeted advertising setting introduced by Dwork and\nIlvento.",
    "published_date": "2019-04-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01793v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01748v1",
    "title": "Evaluation of the Spatio-Temporal features and GAN for Micro-expression Recognition System",
    "authors": [
      "Sze-Teng Liong",
      "Y. S. Gan",
      "Danna Zheng",
      "Shu-Meng Lic",
      "Hao-Xuan Xua",
      "Han-Zhe Zhang",
      "Ran-Ke Lyu",
      "Kun-Hong Liu"
    ],
    "author_ids": [],
    "abstract": "Owing to the development and advancement of artificial intelligence, numerous\nworks were established in the human facial expression recognition system.\nMeanwhile, the detection and classification of micro-expressions are attracting\nattentions from various research communities in the recent few years. In this\npaper, we first review the processes of a conventional optical-flow-based\nrecognition system, which comprised of facial landmarks annotations, optical\nflow guided images computation, features extraction and emotion class\ncategorization. Secondly, a few approaches have been proposed to improve the\nfeature extraction part, such as exploiting GAN to generate more image samples.\nParticularly, several variations of optical flow are computed in order to\ngenerate optimal images to lead to high recognition accuracy. Next, GAN, a\ncombination of Generator and Discriminator, is utilized to generate new \"fake\"\nimages to increase the sample size. Thirdly, a modified state-of-the-art\nConvolutional neural networks is proposed. To verify the effectiveness of the\nthe proposed method, the results are evaluated on spontaneous micro-expression\ndatabases, namely SMIC, CASME II and SAMM. Both the F1-score and accuracy\nperformance metrics are reported in this paper.",
    "published_date": "2019-04-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01748v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01656v1",
    "title": "Combinatorial inequalities",
    "authors": [
      "Igor Pak"
    ],
    "author_ids": [],
    "abstract": "This is an expanded version of the Notices of the AMS column with the same\ntitle. The text is unchanged, but we added acknowledgements and a large number\nof endnotes which provide the context and the references.",
    "published_date": "2019-04-02T00:00:00",
    "year": 2019,
    "categories": [
      "math.CO",
      "cs.DM",
      "05A20 (Primary), 05-02, 05A10, 05E10 (Secondary)"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01656v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.01628v1",
    "title": "Identification, Interpretability, and Bayesian Word Embeddings",
    "authors": [
      "Adam M. Lauretig"
    ],
    "author_ids": [],
    "abstract": "Social scientists have recently turned to analyzing text using tools from\nnatural language processing like word embeddings to measure concepts like\nideology, bias, and affinity. However, word embeddings are difficult to use in\nthe regression framework familiar to social scientists: embeddings are are\nneither identified, nor directly interpretable. I offer two advances on\nstandard embedding models to remedy these problems. First, I develop Bayesian\nWord Embeddings with Automatic Relevance Determination priors, relaxing the\nassumption that all embedding dimensions have equal weight. Second, I apply\nwork identifying latent variable models to anchor the dimensions of the\nresulting embeddings, identifying them, and making them interpretable and\nusable in a regression. I then apply this model and anchoring approach to two\ncases, the shift in internationalist rhetoric in the American presidents'\ninaugural addresses, and the relationship between bellicosity in American\nforeign policy decision-makers' deliberations. I find that inaugural addresses\nbecame less internationalist after 1945, which goes against the conventional\nwisdom, and that an increase in bellicosity is associated with an increase in\nhostile actions by the United States, showing that elite deliberations are not\ncheap talk, and helping confirm the validity of the model.",
    "published_date": "2019-04-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01628v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01531v2",
    "title": "Rating Reliability and Bias in News Articles: Does AI Assistance Help Everyone?",
    "authors": [
      "Benjamin D. Horne",
      "Dorit Nevo",
      "John O'Donovan",
      "Jin-Hee Cho",
      "Sibel Adali"
    ],
    "author_ids": [],
    "abstract": "With the spread of false and misleading information in current news, many\nalgorithmic tools have been introduced with the aim of assessing bias and\nreliability in written content. However, there has been little work exploring\nhow effective these tools are at changing human perceptions of content. To this\nend, we conduct a study with 654 participants to understand if algorithmic\nassistance improves the accuracy of reliability and bias perceptions, and\nwhether there is a difference in the effectiveness of the AI assistance for\ndifferent types of news consumers. We find that AI assistance with\nfeature-based explanations improves the accuracy of news perceptions. However,\nsome consumers are helped more than others. Specifically, we find that\nparticipants who read and share news often on social media are worse at\nrecognizing bias and reliability issues in news articles than those who do not,\nwhile frequent news readers and those familiar with politics perform much\nbetter. We discuss these differences and their implication to offer insights\nfor future research.",
    "published_date": "2019-04-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01531v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01341v1",
    "title": "Looking back at Labels: A Class based Domain Adaptation Technique",
    "authors": [
      "Vinod Kumar Kurmi",
      "Vinay P. Namboodiri"
    ],
    "author_ids": [],
    "abstract": "In this paper, we solve the problem of adapting classifiers across domains.\nWe consider the problem of domain adaptation for multi-class classification\nwhere we are provided a labeled set of examples in a source dataset and we are\nprovided a target dataset with no supervision. In this setting, we propose an\nadversarial discriminator based approach. While the approach based on\nadversarial discriminator has been previously proposed; in this paper, we\npresent an informed adversarial discriminator. Our observation relies on the\nanalysis that shows that if the discriminator has access to all the information\navailable including the class structure present in the source dataset, then it\ncan guide the transformation of features of the target set of classes to a more\nstructure adapted space. Using this formulation, we obtain state-of-the-art\nresults for the standard evaluation on benchmark datasets. We further provide\ndetailed analysis which shows that using all the labeled information results in\nan improved domain adaptation.",
    "published_date": "2019-04-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01341v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01269v1",
    "title": "Experiments on Open-Set Speaker Identification with Discriminatively Trained Neural Networks",
    "authors": [
      "Stefano Imoscopi",
      "Volodya Grancharov",
      "Sigurdur Sverrisson",
      "Erlendur Karlsson",
      "Harald Pobloth"
    ],
    "author_ids": [],
    "abstract": "This paper presents a study on discriminative artificial neural network\nclassifiers in the context of open-set speaker identification. Both 2-class and\nmulti-class architectures are tested against the conventional Gaussian mixture\nmodel based classifier on enrolled speaker sets of different sizes. The\nperformance evaluation shows that the multi-class neural network system has\nsuperior performance for large population sizes.",
    "published_date": "2019-04-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01269v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.01219v2",
    "title": "Deep Learning for Face Recognition: Pride or Prejudiced?",
    "authors": [
      "Shruti Nagpal",
      "Maneet Singh",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "author_ids": [],
    "abstract": "Do very high accuracies of deep networks suggest pride of effective AI or are\ndeep networks prejudiced? Do they suffer from in-group biases (own-race-bias\nand own-age-bias), and mimic the human behavior? Is in-group specific\ninformation being encoded sub-consciously by the deep networks?\n  This research attempts to answer these questions and presents an in-depth\nanalysis of `bias' in deep learning based face recognition systems. This is the\nfirst work which decodes if and where bias is encoded for face recognition.\nTaking cues from cognitive studies, we inspect if deep networks are also\naffected by social in- and out-group effect. Networks are analyzed for own-race\nand own-age bias, both of which have been well established in human beings. The\nsub-conscious behavior of face recognition models is examined to understand if\nthey encode race or age specific features for face recognition. Analysis is\nperformed based on 36 experiments conducted on multiple datasets. Four deep\nlearning networks either trained from scratch or pre-trained on over 10M images\nare used. Variations across class activation maps and feature visualizations\nprovide novel insights into the functioning of deep learning systems,\nsuggesting behavior similar to humans. It is our belief that a better\nunderstanding of state-of-the-art deep learning networks would enable\nresearchers to address the given challenge of bias in AI, and develop fairer\nsystems.",
    "published_date": "2019-04-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01219v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.00863v2",
    "title": "DefectNET: multi-class fault detection on highly-imbalanced datasets",
    "authors": [
      "N. Anantrasirichai",
      "David Bull"
    ],
    "author_ids": [],
    "abstract": "As a data-driven method, the performance of deep convolutional neural\nnetworks (CNN) relies heavily on training data. The prediction results of\ntraditional networks give a bias toward larger classes, which tend to be the\nbackground in the semantic segmentation task. This becomes a major problem for\nfault detection, where the targets appear very small on the images and vary in\nboth types and sizes. In this paper we propose a new network architecture,\nDefectNet, that offers multi-class (including but not limited to) defect\ndetection on highly-imbalanced datasets. DefectNet consists of two parallel\npaths, which are a fully convolutional network and a dilated convolutional\nnetwork to detect large and small objects respectively. We propose a hybrid\nloss maximising the usefulness of a dice loss and a cross entropy loss, and we\nalso employ the leaky rectified linear unit (ReLU) to deal with rare occurrence\nof some targets in training batches. The prediction results show that our\nDefectNet outperforms state-of-the-art networks for detecting multi-class\ndefects with the average accuracy improvement of approximately 10% on a wind\nturbine.",
    "published_date": "2019-04-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.00863v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.00647v1",
    "title": "On Minimizing the Maximum Age-of-Information For Wireless Erasure Channels",
    "authors": [
      "Arunabh Srivastava",
      "Abhishek Sinha",
      "Krishna Jagannathan"
    ],
    "author_ids": [],
    "abstract": "Age-of-Information (AoI) is a recently proposed metric for quantifying the\nfreshness of information from the UE's perspective in a communication network.\nRecently, Kadota et al. [1] have proposed an index-type approximately optimal\nscheduling policy for minimizing the average-AoI metric for a downlink\ntransmission problem. For delay-sensitive applications, including real-time\ncontrol of a cyber-physical system, or scheduling URLLC traffic in 5G, it is\nessential to have a more stringent uniform control on AoI across all users. In\nthis paper, we derive an exactly optimal scheduling policy for this problem in\na downlink cellular system with erasure channels. Our proof of optimality\ninvolves an explicit solution to the associated average-cost Bellman Equation,\nwhich might be of independent theoretical interest. We also establish that the\nresulting Age-process is positive recurrent under the optimal policy, and has\nan exponentially light tail, with the optimal large-deviation exponent.\nFinally, motivated by typical applications in small-cell residential networks,\nwe consider the problem of minimizing the peak-AoI with throughput constraints\nto specific UEs, and derive a heuristic policy for this problem. Extensive\nnumerical simulations have been carried out to compare the efficacy of the\nproposed policies with other well-known scheduling policies, such as Randomized\nscheduling and Proportional Fair.",
    "published_date": "2019-04-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.00647v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1904.00542v1",
    "title": "Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness and the Leading Political Ideology of News Media",
    "authors": [
      "Ramy Baly",
      "Georgi Karadzhov",
      "Abdelrhman Saleh",
      "James Glass",
      "Preslav Nakov"
    ],
    "author_ids": [],
    "abstract": "In the context of fake news, bias, and propaganda, we study two important but\nrelatively under-explored problems: (i) trustworthiness estimation (on a\n3-point scale) and (ii) political ideology detection (left/right bias on a\n7-point scale) of entire news outlets, as opposed to evaluating individual\narticles. In particular, we propose a multi-task ordinal regression framework\nthat models the two problems jointly. This is motivated by the observation that\nhyper-partisanship is often linked to low trustworthiness, e.g., appealing to\nemotions rather than sticking to the facts, while center media tend to be\ngenerally more impartial and trustworthy. We further use several auxiliary\ntasks, modeling centrality, hyperpartisanship, as well as left-vs.-right bias\non a coarse-grained scale. The evaluation results show sizable performance\ngains by the joint models over models that target the problems in isolation.",
    "published_date": "2019-04-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.00542v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.00244v1",
    "title": "Person Re-identification with Bias-controlled Adversarial Training",
    "authors": [
      "Sara Iodice",
      "Krystian Mikolajczyk"
    ],
    "author_ids": [],
    "abstract": "Inspired by the effectiveness of adversarial training in the area of\nGenerative Adversarial Networks we present a new approach for learning feature\nrepresentations in person re-identification. We investigate different types of\nbias that typically occur in re-ID scenarios, i.e., pose, body part and camera\nview, and propose a general approach to address them. We introduce an\nadversarial strategy for controlling bias, named Bias-controlled Adversarial\nframework (BCA), with two complementary branches to reduce or to enhance\nbias-related features. The results and comparison to the state of the art on\ndifferent benchmarks show that our framework is an effective strategy for\nperson re-identification. The performance improvements are in both full and\npartial views of persons.",
    "published_date": "2019-03-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.00244v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1904.00058v1",
    "title": "From DB-nets to Coloured Petri Nets with Priorities (Extended Version)",
    "authors": [
      "Marco Montali",
      "Andrey Rivkin"
    ],
    "author_ids": [],
    "abstract": "The recently introduced formalism of DB-nets has brought in a new conceptual\nway of modelling complex dynamic systems that equally account for the process\nand data dimensions, considering local data as well as persistent,\ntransactional data. DB-nets combine a coloured variant of Petri nets with name\ncreation and management (which we call nu-CPN), with a relational database. The\nintegration of these two components is realized by equipping the net with\nspecial ``view'' places that query the database and expose the resulting\nanswers to the net, with actions that allow transitions to update the content\nof the database, and with special arcs capturing compensation in case of\ntransaction failure. In this work, we study whether this sophisticated model\ncan be encoded back into nu-CPNs. In particular, we show that the meaningful\nfragment of DB-nets where database queries are expressed using unions of\nconjunctive queries with inequalities can be faithfully encoded into $\\nu$-CPNs\nwith transition priorities. This allows us to directly exploit state-of-the-art\ntechnologies such as CPN Tools to simulate and analyse this relevant class of\nDB-nets.",
    "published_date": "2019-03-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LO",
      "cs.DB"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.00058v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.12113v1",
    "title": "A Counterexample-guided Approach to Finding Numerical Invariants",
    "authors": [
      "ThanhVu Nguyen",
      "Timos Antopoulos",
      "Andrew Ruef",
      "Michael Hicks"
    ],
    "author_ids": [],
    "abstract": "Numerical invariants, e.g., relationships among numerical variables in a\nprogram, represent a useful class of properties to analyze programs. General\npolynomial invariants represent more complex numerical relations, but they are\noften required in many scientific and engineering applications. We present\nNumInv, a tool that implements a counterexample-guided invariant generation\n(CEGIR) technique to automatically discover numerical invariants, which are\npolynomial equality and inequality relations among numerical variables. This\nCEGIR technique infers candidate invariants from program traces and then checks\nthem against the program source code using the KLEE test-input generation tool.\nIf the invariants are incorrect KLEE returns counterexample traces, which help\nthe dynamic inference obtain better results. Existing CEGIR approaches often\nrequire sound invariants, however NumInv sacrifices soundness and produces\nresults that KLEE cannot refute within certain time bounds. This design and the\nuse of KLEE as a verifier allow NumInv to discover useful and important\nnumerical invariants for many challenging programs.\n  Preliminary results show that NumInv generates required invariants for\nunderstanding and verifying correctness of programs involving complex\narithmetic. We also show that NumInv discovers polynomial invariants that\ncapture precise complexity bounds of programs used to benchmark existing static\ncomplexity analysis techniques. Finally, we show that NumInv performs\ncompetitively comparing to state of the art numerical invariant analysis tools.",
    "published_date": "2019-03-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.12113v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.11719v1",
    "title": "Fairness in Algorithmic Decision Making: An Excursion Through the Lens of Causality",
    "authors": [
      "Aria Khademi",
      "Sanghack Lee",
      "David Foley",
      "Vasant Honavar"
    ],
    "author_ids": [],
    "abstract": "As virtually all aspects of our lives are increasingly impacted by\nalgorithmic decision making systems, it is incumbent upon us as a society to\nensure such systems do not become instruments of unfair discrimination on the\nbasis of gender, race, ethnicity, religion, etc. We consider the problem of\ndetermining whether the decisions made by such systems are discriminatory,\nthrough the lens of causal models. We introduce two definitions of group\nfairness grounded in causality: fair on average causal effect (FACE), and fair\non average causal effect on the treated (FACT). We use the Rubin-Neyman\npotential outcomes framework for the analysis of cause-effect relationships to\nrobustly estimate FACE and FACT. We demonstrate the effectiveness of our\nproposed approach on synthetic data. Our analyses of two real-world data sets,\nthe Adult income data set from the UCI repository (with gender as the protected\nattribute), and the NYC Stop and Frisk data set (with race as the protected\nattribute), show that the evidence of discrimination obtained by FACE and FACT,\nor lack thereof, is often in agreement with the findings from other studies. We\nfurther show that FACT, being somewhat more nuanced compared to FACE, can yield\nfindings of discrimination that differ from those obtained using FACE.",
    "published_date": "2019-03-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.11719v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.11587v2",
    "title": "Characteristic-Dependent Linear Rank Inequalities via Complementary Vector Spaces",
    "authors": [
      "Victor Pena",
      "Humberto Sarria"
    ],
    "author_ids": [],
    "abstract": "A characteristic-dependent linear rank inequality is a linear inequality that\nholds by ranks of subspaces of a vector space over a finite field of determined\ncharacteristic, and does not in general hold over other characteristics. In\nthis paper, we produce new characteristic-dependent linear rank inequalities by\nan alternative technique to the usual Dougherty's inverse function method [9].\nWe take up some ideas of Blasiak [4], applied to certain complementary vector\nspaces, in order to produce them. Also, we present some applications to network\ncoding. In particular, for each finite or co-finite set of primes $P$, we show\nthat there exists a sequence of networks $\\mathcal{N}\\left(k\\right)$ in which\neach member is linearly solvable over a field if and only if the characteristic\nof the field is in $P$, and the linear capacity, over fields whose\ncharacteristic is not in $P$, $\\rightarrow0$ as $k\\rightarrow\\infty$.",
    "published_date": "2019-03-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT",
      "68P30"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.11587v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.11452v3",
    "title": "Lexical convergence and collective identities on Facebook",
    "authors": [
      "Emanuele Brugnoli",
      "Matteo Cinelli",
      "Fabiana Zollo",
      "Walter Quattrociocchi",
      "Antonio Scala"
    ],
    "author_ids": [],
    "abstract": "Recent studies, targeting Facebook, showed the tendency of users to interact\nwith information adhering to their preferred narrative and to ignore dissenting\ninformation. Primarily driven by confirmation bias, users tend to join\npolarized clusters where they cooperate to reinforce a like-minded system of\nbeliefs, thus facilitating fake news and misinformation cascades. To gain a\ndeeper understanding of these phenomena, in this work we analyze the lexicons\nused by the communities of users emerging on Facebook around verified and\nunverified contents. We show how the lexical approach provides important\ninsights about the kind of information processed by the two communities of\nusers and about their overall sentiment. Furthermore, by focusing on comment\nthreads, we observe a strong positive correlation between the lexical\nconvergence of co-commenters and their number of interactions, which in turns\nsuggests that such a trend could be a proxy for the emergence of collective\nidentities and polarization in opinion dynamics.",
    "published_date": "2019-03-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "physics.soc-ph",
      "91F20"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.11452v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.10956v2",
    "title": "On the Influence of Bias-Correction on Distributed Stochastic Optimization",
    "authors": [
      "Kun Yuan",
      "Sulaiman A. Alghunaim",
      "Bicheng Ying",
      "Ali H. Sayed"
    ],
    "author_ids": [],
    "abstract": "Various bias-correction methods such as EXTRA, gradient tracking methods, and\nexact diffusion have been proposed recently to solve distributed {\\em\ndeterministic} optimization problems. These methods employ constant step-sizes\nand converge linearly to the {\\em exact} solution under proper conditions.\nHowever, their performance under stochastic and adaptive settings is less\nexplored. It is still unknown {\\em whether}, {\\em when} and {\\em why} these\nbias-correction methods can outperform their traditional counterparts (such as\nconsensus and diffusion) with noisy gradient and constant step-sizes.\n  This work studies the performance of exact diffusion under the stochastic and\nadaptive setting, and provides conditions under which exact diffusion has\nsuperior steady-state mean-square deviation (MSD) performance than traditional\nalgorithms without bias-correction. In particular, it is proven that this\nsuperiority is more evident over sparsely-connected network topologies such as\nlines, cycles, or grids. Conditions are also provided under which exact\ndiffusion method match or may even degrade the performance of traditional\nmethods. Simulations are provided to validate the theoretical findings.",
    "published_date": "2019-03-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10956v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10765v1",
    "title": "Micro-expression detection in long videos using optical flow and recurrent neural networks",
    "authors": [
      "Michiel Verburg",
      "Vlado Menkovski"
    ],
    "author_ids": [],
    "abstract": "Facial micro-expressions are subtle and involuntary expressions that can\nreveal concealed emotions. Micro-expressions are an invaluable source of\ninformation in application domains such as lie detection, mental health,\nsentiment analysis and more. One of the biggest challenges in this field of\nresearch is the small amount of available spontaneous micro-expression data.\nHowever, spontaneous data collection is burdened by time-consuming and\nexpensive annotation. Hence, methods are needed which can reduce the amount of\ndata that annotators have to review. This paper presents a novel\nmicro-expression spotting method using a recurrent neural network (RNN) on\noptical flow features. We extract Histogram of Oriented Optical Flow (HOOF)\nfeatures to encode the temporal changes in selected face regions. Finally, the\nRNN spots short intervals which are likely to contain occurrences of relevant\nfacial micro-movements. The proposed method is evaluated on the SAMM database.\nAny chance of subject bias is eliminated by training the RNN using\nLeave-One-Subject-Out cross-validation. Comparing the spotted intervals with\nthe labeled data shows that the method produced 1569 false positives while\nobtaining a recall of 0.4654. The initial results show that the proposed method\nwould reduce the video length by a factor of 3.5, while still retaining almost\nhalf of the relevant micro-movements. Lastly, as the model gets more data, it\nbecomes better at detecting intervals, which makes the proposed method suitable\nfor supporting the annotation process.",
    "published_date": "2019-03-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10765v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10620v4",
    "title": "An Optimal Graph-Search Method for Secure State Estimation",
    "authors": [
      "Xusheng Luo",
      "Miroslav Pajic",
      "Michael M. Zavlanos"
    ],
    "author_ids": [],
    "abstract": "The growing complexity of modern Cyber-Physical Systems (CPS) and the\nfrequent communication between their components make them vulnerable to\nmalicious attacks. As a result, secure state estimation is a critical\nrequirement for the control of these systems. Many existing secure state\nestimation methods suffer from combinatorial complexity which grows with the\nnumber of states and sensors in the system. This complexity can be mitigated\nusing optimization-based methods that relax the original state estimation\nproblem, although at the cost of optimality as these methods often identify\nattack-free sensors as attacked. In this paper, we propose a new optimal\ngraph-search algorithm to correctly identify malicious attacks and to securely\nestimate the states even in large-scale CPS modeled as linear time-invariant\nsystems. The graph consists of layers, each one containing two nodes capturing\na truth assignment of any given sensor, and directed edges connecting adjacent\nlayers only. Then, our algorithm searches the layers of this graph\nincrementally, favoring directions at higher layers with more attack-free\nassignments, while actively managing a repository of nodes to be expanded at\nlater iterations. The proposed search bias and the ability to revisit nodes in\nthe repository and self-correct, allow our graph-search algorithm to reach the\noptimal assignment faster and tackle larger problems. We show that our\nalgorithm is complete and optimal provided that process and measurement noises\ndo not dominate the attack signal. Moreover, we provide numerical simulations\nthat demonstrate the ability of our algorithm to correctly identify attacked\nsensors and securely reconstruct the state. Our simulations show that our\nmethod outperforms existing algorithms both in terms of optimality and\nexecution time.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.CR",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10620v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.10598v1",
    "title": "Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making",
    "authors": [
      "Sina Aghaei",
      "Mohammad Javad Azizi",
      "Phebe Vayanos"
    ],
    "author_ids": [],
    "abstract": "In recent years, automated data-driven decision-making systems have enjoyed a\ntremendous success in a variety of fields (e.g., to make product\nrecommendations, or to guide the production of entertainment). More recently,\nthese algorithms are increasingly being used to assist socially sensitive\ndecision-making (e.g., to decide who to admit into a degree program or to\nprioritize individuals for public housing). Yet, these automated tools may\nresult in discriminative decision-making in the sense that they may treat\nindividuals unfairly or unequally based on membership to a category or a\nminority, resulting in disparate treatment or disparate impact and violating\nboth moral and ethical standards. This may happen when the training dataset is\nitself biased (e.g., if individuals belonging to a particular group have\nhistorically been discriminated upon). However, it may also happen when the\ntraining dataset is unbiased, if the errors made by the system affect\nindividuals belonging to a category or minority differently (e.g., if\nmisclassification rates for Blacks are higher than for Whites). In this paper,\nwe unify the definitions of unfairness across classification and regression. We\npropose a versatile mixed-integer optimization framework for learning optimal\nand fair decision trees and variants thereof to prevent disparate treatment\nand/or disparate impact as appropriate. This translates to a flexible schema\nfor designing fair and interpretable policies suitable for socially sensitive\ndecision-making. We conduct extensive computational studies that show that our\nframework improves the state-of-the-art in the field (which typically relies on\nheuristics) to yield non-discriminative decisions at lower cost to overall\naccuracy.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10598v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10561v1",
    "title": "On Measuring Social Biases in Sentence Encoders",
    "authors": [
      "Chandler May",
      "Alex Wang",
      "Shikha Bordia",
      "Samuel R. Bowman",
      "Rachel Rudinger"
    ],
    "author_ids": [],
    "abstract": "The Word Embedding Association Test shows that GloVe and word2vec word\nembeddings exhibit human-like implicit biases based on gender, race, and other\nsocial constructs (Caliskan et al., 2017). Meanwhile, research on learning\nreusable text representations has begun to explore sentence-level texts, with\nsome sentence encoders seeing enthusiastic adoption. Accordingly, we extend the\nWord Embedding Association Test to measure bias in sentence encoders. We then\ntest several sentence encoders, including state-of-the-art methods such as ELMo\nand BERT, for the social biases studied in prior work and two important biases\nthat are difficult or impossible to test at the word level. We observe mixed\nresults including suspicious patterns of sensitivity that suggest the test's\nassumptions may not hold in general. We conclude by proposing directions for\nfuture work on measuring bias in sentence encoders.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10561v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10399v1",
    "title": "Learning-to-Learn Stochastic Gradient Descent with Biased Regularization",
    "authors": [
      "Giulia Denevi",
      "Carlo Ciliberto",
      "Riccardo Grazzi",
      "Massimiliano Pontil"
    ],
    "author_ids": [],
    "abstract": "We study the problem of learning-to-learn: inferring a learning algorithm\nthat works well on tasks sampled from an unknown distribution. As class of\nalgorithms we consider Stochastic Gradient Descent on the true risk regularized\nby the square euclidean distance to a bias vector. We present an average excess\nrisk bound for such a learning algorithm. This result quantifies the potential\nbenefit of using a bias vector with respect to the unbiased case. We then\naddress the problem of estimating the bias from a sequence of tasks. We propose\na meta-algorithm which incrementally updates the bias, as new tasks are\nobserved. The low space and time complexity of this approach makes it appealing\nin practice. We provide guarantees on the learning ability of the\nmeta-algorithm. A key feature of our results is that, when the number of tasks\ngrows and their variance is relatively small, our learning-to-learn approach\nhas a significant advantage over learning each task in isolation by Stochastic\nGradient Descent without a bias term. We report on numerical experiments which\ndemonstrate the effectiveness of our approach.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10399v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10375v2",
    "title": "Evolving Academia/Industry Relations in Computing Research",
    "authors": [
      "Greg Morrisett",
      "Shwetak Patel",
      "Jennifer Rexford",
      "Benjamin Zorn"
    ],
    "author_ids": [],
    "abstract": "In 2015, the CCC co-sponsored an industry round table that produced the\ndocument \"The Future of Computing Research: Industry-Academic Collaborations\".\nSince then, several important trends in computing research have emerged, and\nthis document considers how those trends impact the interaction between\nacademia and industry in computing fields. We reach the following conclusions:\n- In certain computing disciplines, such as currently artificial intelligence,\nwe observe significant increases in the level of interaction between professors\nand companies, which take the form of extended joint appointments. -\nIncreasingly, companies are highly motivated to engage both professors and\ngraduate students working in specific technical areas because companies view\ncomputing research and technical talent as a core aspect of their business\nsuccess. - There is also the further potential for principles and values from\nthe academy (e.g., ethics, human-centered approaches, etc.) informing products\nand R&D roadmaps in new ways through these unique joint arrangements. - This\nincreasing connection between faculty, students, and companies has the\npotential to change (either positively or negatively) numerous things,\nincluding: the academic culture in computing research universities, the\nresearch topics that faculty and students pursue, the ability of universities\nto train undergraduate and graduate students, etc. This report is the first\nstep in engaging the broader computing research community, raising awareness of\nthe opportunities, complexities and challenges of this trend but further work\nis required. We recommend follow-up to measure the degree and impact of this\ntrend and to establish best practices that are shared widely among computing\nresearch institutions.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10375v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10187v6",
    "title": "Designing Normative Theories for Ethical and Legal Reasoning: LogiKEy Framework, Methodology, and Tool Support",
    "authors": [
      "Christoph Benzmüller",
      "Xavier Parent",
      "Leendert van der Torre"
    ],
    "author_ids": [],
    "abstract": "A framework and methodology---termed LogiKEy---for the design and engineering\nof ethical reasoners, normative theories and deontic logics is presented. The\noverall motivation is the development of suitable means for the control and\ngovernance of intelligent autonomous systems. LogiKEy's unifying formal\nframework is based on semantical embeddings of deontic logics, logic\ncombinations and ethico-legal domain theories in expressive classic\nhigher-order logic (HOL). This meta-logical approach enables the provision of\npowerful tool support in LogiKEy: off-the-shelf theorem provers and model\nfinders for HOL are assisting the LogiKEy designer of ethical intelligent\nagents to flexibly experiment with underlying logics and their combinations,\nwith ethico-legal domain theories, and with concrete examples---all at the same\ntime. Continuous improvements of these off-the-shelf provers, without further\nado, leverage the reasoning performance in LogiKEy. Case studies, in which the\nLogiKEy framework and methodology has been applied and tested, give evidence\nthat HOL's undecidability often does not hinder efficient experimentation.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "03B60, 03B15, 68T27, 68T30, 68T15",
      "I.2.3; I.2.4; I.2.0; F.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10187v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10132v1",
    "title": "f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning",
    "authors": [
      "Yongqin Xian",
      "Saurabh Sharma",
      "Bernt Schiele",
      "Zeynep Akata"
    ],
    "author_ids": [],
    "abstract": "When labeled training data is scarce, a promising data augmentation approach\nis to generate visual features of unknown classes using their attributes. To\nlearn the class conditional distribution of CNN features, these models rely on\npairs of image features and class attributes. Hence, they can not make use of\nthe abundance of unlabeled data samples. In this paper, we tackle any-shot\nlearning problems i.e. zero-shot and few-shot, in a unified feature generating\nframework that operates in both inductive and transductive learning settings.\nWe develop a conditional generative model that combines the strength of VAE and\nGANs and in addition, via an unconditional discriminator, learns the marginal\nfeature distribution of unlabeled images. We empirically show that our model\nlearns highly discriminative CNN features for five datasets, i.e. CUB, SUN, AWA\nand ImageNet, and establish a new state-of-the-art in any-shot learning, i.e.\ninductive and transductive (generalized) zero- and few-shot learning settings.\nWe also demonstrate that our learned features are interpretable: we visualize\nthem by inverting them back to the pixel space and we explain them by\ngenerating textual arguments of why they are associated with a certain label.",
    "published_date": "2019-03-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10132v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.10000v3",
    "title": "Approximate Query Processing using Deep Generative Models",
    "authors": [
      "Saravanan Thirumuruganathan",
      "Shohedul Hasan",
      "Nick Koudas",
      "Gautam Das"
    ],
    "author_ids": [],
    "abstract": "Data is generated at an unprecedented rate surpassing our ability to analyze\nthem. The database community has pioneered many novel techniques for\nApproximate Query Processing (AQP) that could give approximate results in a\nfraction of time needed for computing exact results. In this work, we explore\nthe usage of deep learning (DL) for answering aggregate queries specifically\nfor interactive applications such as data exploration and visualization. We use\ndeep generative models, an unsupervised learning based approach, to learn the\ndata distribution faithfully such that aggregate queries could be answered\napproximately by generating samples from the learned model. The model is often\ncompact - few hundred KBs - so that arbitrary AQP queries could be answered on\nthe client side without contacting the database server. Our other contributions\ninclude identifying model bias and minimizing it through a rejection sampling\nbased approach and an algorithm to build model ensembles for AQP for improved\naccuracy. Our extensive experiments show that our proposed approach can provide\nanswers with high accuracy and low latency.",
    "published_date": "2019-03-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10000v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.09980v2",
    "title": "Cluster Alignment with a Teacher for Unsupervised Domain Adaptation",
    "authors": [
      "Zhijie Deng",
      "Yucen Luo",
      "Jun Zhu"
    ],
    "author_ids": [],
    "abstract": "Deep learning methods have shown promise in unsupervised domain adaptation,\nwhich aims to leverage a labeled source domain to learn a classifier for the\nunlabeled target domain with a different distribution. However, such methods\ntypically learn a domain-invariant representation space to match the marginal\ndistributions of the source and target domains, while ignoring their fine-level\nstructures. In this paper, we propose Cluster Alignment with a Teacher (CAT)\nfor unsupervised domain adaptation, which can effectively incorporate the\ndiscriminative clustering structures in both domains for better adaptation.\nTechnically, CAT leverages an implicit ensembling teacher model to reliably\ndiscover the class-conditional structure in the feature space for the unlabeled\ntarget domain. Then CAT forces the features of both the source and the target\ndomains to form discriminative class-conditional clusters and aligns the\ncorresponding clusters across domains. Empirical results demonstrate that CAT\nachieves state-of-the-art results in several unsupervised domain adaptation\nscenarios.",
    "published_date": "2019-03-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.09980v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.09818v2",
    "title": "Harnessing Higher-Order (Meta-)Logic to Represent and Reason with Complex Ethical Theories",
    "authors": [
      "David Fuenmayor",
      "Christoph Benzmüller"
    ],
    "author_ids": [],
    "abstract": "The computer-mechanization of an ambitious explicit ethical theory, Gewirth's\nPrinciple of Generic Consistency, is used to showcase an approach for\nrepresenting and reasoning with ethical theories exhibiting complex logical\nfeatures like alethic and deontic modalities, indexicals, higher-order\nquantification, among others. Harnessing the high expressive power of Church's\ntype theory as a meta-logic to semantically embed a combination of quantified\nnon-classical logics, our work pushes existing boundaries in knowledge\nrepresentation and reasoning. We demonstrate that intuitive encodings of\ncomplex ethical theories and their automation on the computer are no longer\nantipodes.",
    "published_date": "2019-03-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LO",
      "03B60, 03B15, 68T27, 68T30, 68T15",
      "I.2.3; I.2.4; I.2.0; F.4"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.09818v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.09730v3",
    "title": "Generative Adversarial Minority Oversampling",
    "authors": [
      "Sankha Subhra Mullick",
      "Shounak Datta",
      "Swagatam Das"
    ],
    "author_ids": [],
    "abstract": "Class imbalance is a long-standing problem relevant to a number of real-world\napplications of deep learning. Oversampling techniques, which are effective for\nhandling class imbalance in classical learning systems, can not be directly\napplied to end-to-end deep learning systems. We propose a three-player\nadversarial game between a convex generator, a multi-class classifier network,\nand a real/fake discriminator to perform oversampling in deep learning systems.\nThe convex generator generates new samples from the minority classes as convex\ncombinations of existing instances, aiming to fool both the discriminator as\nwell as the classifier into misclassifying the generated samples. Consequently,\nthe artificial samples are generated at critical locations near the peripheries\nof the classes. This, in turn, adjusts the classifier induced boundaries in a\nway which is more likely to reduce misclassification from the minority classes.\nExtensive experiments on multiple class imbalanced image datasets establish the\nefficacy of our proposal.",
    "published_date": "2019-03-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.09730v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.09493v1",
    "title": "The invisible power of fairness. How machine learning shapes democracy",
    "authors": [
      "Elena Beretta",
      "Antonio Santangelo",
      "Bruno Lepri",
      "Antonio Vetrò",
      "Juan Carlos De Martin"
    ],
    "author_ids": [],
    "abstract": "Many machine learning systems make extensive use of large amounts of data\nregarding human behaviors. Several researchers have found various\ndiscriminatory practices related to the use of human-related machine learning\nsystems, for example in the field of criminal justice, credit scoring and\nadvertising. Fair machine learning is therefore emerging as a new field of\nstudy to mitigate biases that are inadvertently incorporated into algorithms.\nData scientists and computer engineers are making various efforts to provide\ndefinitions of fairness. In this paper, we provide an overview of the most\nwidespread definitions of fairness in the field of machine learning, arguing\nthat the ideas highlighting each formalization are closely related to different\nideas of justice and to different interpretations of democracy embedded in our\nculture. This work intends to analyze the definitions of fairness that have\nbeen proposed to date to interpret the underlying criteria and to relate them\nto different ideas of democracy.",
    "published_date": "2019-03-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.09493v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.09239v1",
    "title": "Multi-Domain Adversarial Learning",
    "authors": [
      "Alice Schoenauer-Sebag",
      "Louise Heinrich",
      "Marc Schoenauer",
      "Michele Sebag",
      "Lani F. Wu",
      "Steve J. Altschuler"
    ],
    "author_ids": [],
    "abstract": "Multi-domain learning (MDL) aims at obtaining a model with minimal average\nrisk across multiple domains. Our empirical motivation is automated microscopy\ndata, where cultured cells are imaged after being exposed to known and unknown\nchemical perturbations, and each dataset displays significant experimental\nbias. This paper presents a multi-domain adversarial learning approach, MuLANN,\nto leverage multiple datasets with overlapping but distinct class sets, in a\nsemi-supervised setting. Our contributions include: i) a bound on the average-\nand worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss\nto accommodate semi-supervised multi-domain learning and domain adaptation;\niii) the experimental validation of the approach, improving on the state of the\nart on two standard image benchmarks, and a novel bioimage dataset, Cell.",
    "published_date": "2019-03-21T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.09239v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.09209v1",
    "title": "A Simulation Based Dynamic Evaluation Framework for System-wide Algorithmic Fairness",
    "authors": [
      "Efrén Cruz Cortés",
      "Debashis Ghosh"
    ],
    "author_ids": [],
    "abstract": "We propose the use of Agent Based Models (ABMs) inside a reinforcement\nlearning framework in order to better understand the relationship between\nautomated decision making tools, fairness-inspired statistical constraints, and\nthe social phenomena giving rise to discrimination towards sensitive groups.\nThere have been many instances of discrimination occurring due to the\napplications of algorithmic tools by public and private institutions. Until\nrecently, these practices have mostly gone unchecked. Given the large-scale\ntransformation these new technologies elicit, a joint effort of social sciences\nand machine learning researchers is necessary. Much of the research has been\ndone on determining statistical properties of such algorithms and the data they\nare trained on. We aim to complement that approach by studying the social\ndynamics in which these algorithms are implemented. We show how bias can be\naccumulated and reinforced through automated decision making, and the\npossibility of finding a fairness inducing policy. We focus on the case of\nrecidivism risk assessment by considering simplified models of arrest. We find\nthat if we limit our attention to what is observed and manipulated by these\nalgorithmic tools, we may determine some blatantly unfair practices as fair,\nillustrating the advantage of analyzing the otherwise elusive property with a\nsystem-wide model. We expect the introduction of agent based simulation\ntechniques will strengthen collaboration with social scientists, arriving at a\nbetter understanding of the social systems affected by technology and to\nhopefully lead to concrete policy proposals that can be presented to\npolicymakers for a true systemic transformation.",
    "published_date": "2019-03-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.09209v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.08789v1",
    "title": "Interpreting Neural Networks Using Flip Points",
    "authors": [
      "Roozbeh Yousefzadeh",
      "Dianne P. O'Leary"
    ],
    "author_ids": [],
    "abstract": "Neural networks have been criticized for their lack of easy interpretation,\nwhich undermines confidence in their use for important applications. Here, we\nintroduce a novel technique, interpreting a trained neural network by\ninvestigating its flip points. A flip point is any point that lies on the\nboundary between two output classes: e.g. for a neural network with a binary\nyes/no output, a flip point is any input that generates equal scores for \"yes\"\nand \"no\". The flip point closest to a given input is of particular importance,\nand this point is the solution to a well-posed optimization problem. This paper\ngives an overview of the uses of flip points and how they are computed. Through\nresults on standard datasets, we demonstrate how flip points can be used to\nprovide detailed interpretation of the output produced by a neural network.\nMoreover, for a given input, flip points enable us to measure confidence in the\ncorrectness of outputs much more effectively than softmax score. They also\nidentify influential features of the inputs, identify bias, and find changes in\nthe input that change the output of the model. We show that distance between an\ninput and the closest flip point identifies the most influential points in the\ntraining data. Using principal component analysis (PCA) and rank-revealing QR\nfactorization (RR-QR), the set of directions from each training input to its\nclosest flip point provides explanations of how a trained neural network\nprocesses an entire dataset: what features are most important for\nclassification into a given class, which features are most responsible for\nparticular misclassifications, how an adversary might fool the network, etc.\nAlthough we investigate flip points for neural networks, their usefulness is\nactually model-agnostic.",
    "published_date": "2019-03-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08789v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.08550v1",
    "title": "OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations",
    "authors": [
      "Pramuditha Perera",
      "Ramesh Nallapati",
      "Bing Xiang"
    ],
    "author_ids": [],
    "abstract": "We present a novel model called OCGAN for the classical problem of one-class\nnovelty detection, where, given a set of examples from a particular class, the\ngoal is to determine if a query example is from the same class. Our solution is\nbased on learning latent representations of in-class examples using a denoising\nauto-encoder network. The key contribution of our work is our proposal to\nexplicitly constrain the latent space to exclusively represent the given class.\nIn order to accomplish this goal, firstly, we force the latent space to have\nbounded support by introducing a tanh activation in the encoder's output layer.\nSecondly, using a discriminator in the latent space that is trained\nadversarially, we ensure that encoded representations of in-class examples\nresemble uniform random samples drawn from the same bounded space. Thirdly,\nusing a second adversarial discriminator in the input space, we ensure all\nrandomly drawn latent samples generate examples that look real. Finally, we\nintroduce a gradient-descent based sampling technique that explores points in\nthe latent space that generate potential out-of-class examples, which are fed\nback to the network to further train it to generate in-class examples from\nthose points. The effectiveness of the proposed method is measured across four\npublicly available datasets using two one-class novelty detection protocols\nwhere we achieve state-of-the-art results.",
    "published_date": "2019-03-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08550v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.12616v1",
    "title": "Activity Classification Using Smartphone Gyroscope and Accelerometer Data",
    "authors": [
      "Emily Huang",
      "Jukka-Pekka Onnela"
    ],
    "author_ids": [],
    "abstract": "Activities, such as walking and sitting, are commonly used in biomedical\nsettings either as an outcome or covariate of interest. Researchers have\ntraditionally relied on surveys to quantify activity levels of subjects in both\nresearch and clinical settings, but surveys are not objective in nature and\nhave many known limitations, such as recall bias. Smartphones provide an\nopportunity for unobtrusive objective measurement of various activities in\nnaturalistic settings, but their data tends to be noisy and needs to be\nanalyzed with care. We explored the potential of smartphone accelerometer and\ngyroscope data to distinguish between five different types of activity:\nwalking, sitting, standing, ascending stairs, and descending stairs. We\nconducted a study in which four participants followed a study protocol and\nperformed a sequence of various activities with one phone in their front pocket\nand another phone in their back pocket. The subjects were filmed throughout,\nand the obtained footage was annotated to establish ground truth activity. We\napplied the so-called movelet method to classify their activity. Our results\ndemonstrate the promise of smartphones for activity detection in naturalistic\nsettings, but they also highlight common challenges in this field of research.",
    "published_date": "2019-03-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.12616v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.08314v2",
    "title": "Inequalities related to some types of entropies and divergences",
    "authors": [
      "Shigeru Furuichi",
      "Nicuşor Minculete"
    ],
    "author_ids": [],
    "abstract": "The aim of this paper is to discuss new results concerning some kinds of\nparametric extended entropies and divergences. As a result of our studies for\nmathematical properties on entropy and divergence, we give new bounds for the\nTsallis quasilinear entropy and divergence by applying the Hermite-Hadamard\ninequality. We also give bounds for biparametrical extended entropies and\ndivergences which have been given in \\cite{7}. In addition, we study\n$(r,q)$-quasilinear entropies and divergences as alternative biparametrical\nextended entropy and divergence, and then we give bounds for them. Finally we\nobtain inequalities for an extended Lin's divergence and some characterizations\nof Fermi-Dirac entropy and Bose-Einstein entropy.",
    "published_date": "2019-03-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.CA",
      "math.IT",
      "Primary 46C05, secondary 26D15, 26D10"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08314v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.08983v3",
    "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)",
    "authors": [
      "Marcos Zampieri",
      "Shervin Malmasi",
      "Preslav Nakov",
      "Sara Rosenthal",
      "Noura Farra",
      "Ritesh Kumar"
    ],
    "author_ids": [],
    "abstract": "We present the results and the main findings of SemEval-2019 Task 6 on\nIdentifying and Categorizing Offensive Language in Social Media (OffensEval).\nThe task was based on a new dataset, the Offensive Language Identification\nDataset (OLID), which contains over 14,000 English tweets. It featured three\nsub-tasks. In sub-task A, the goal was to discriminate between offensive and\nnon-offensive posts. In sub-task B, the focus was on the type of offensive\ncontent in the post. Finally, in sub-task C, systems had to detect the target\nof the offensive posts. OffensEval attracted a large number of participants and\nit was one of the most popular tasks in SemEval-2019. In total, about 800 teams\nsigned up to participate in the task, and 115 of them submitted results, which\nwe present and analyze in this report.",
    "published_date": "2019-03-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08983v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.08136v1",
    "title": "Debiasing Community Detection: The Importance of Lowly-Connected Nodes",
    "authors": [
      "Ninareh Mehrabi",
      "Fred Morstatter",
      "Nanyun Peng",
      "Aram Galstyan"
    ],
    "author_ids": [],
    "abstract": "Community detection is an important task in social network analysis, allowing\nus to identify and understand the communities within the social structures.\nHowever, many community detection approaches either fail to assign low degree\n(or lowly-connected) users to communities, or assign them to trivially small\ncommunities that prevent them from being included in analysis. In this work, we\ninvestigate how excluding these users can bias analysis results. We then\nintroduce an approach that is more inclusive for lowly-connected users by\nincorporating them into larger groups. Experiments show that our approach\noutperforms the existing state-of-the-art in terms of F1 and Jaccard similarity\nscores while reducing the bias towards low-degree users.",
    "published_date": "2019-03-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08136v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.08075v1",
    "title": "Multi timescale bandwidth profile and its application for burst-aware fairness",
    "authors": [
      "Szilveszter Nádas",
      "Balázs Varga",
      "Illés Horváth",
      "András Mészáros",
      "Miklós Telek"
    ],
    "author_ids": [],
    "abstract": "We propose a resource sharing scheme that takes into account the traffic\nhistory over several predefined time scales and provides fair resource sharing\nconsidering the traffic history. Our concept builds on a simplified version of\ncore-stateless resource sharing, where we only use a few Drop Precedences\n(DPs). For packet marking we introduce Multi timescale bandwidth profile.\nAdditionally, we provide basic dimensioning concepts for the proposed schema\nand present its simulation based performance analysis.",
    "published_date": "2019-03-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08075v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.07609v1",
    "title": "Multi-Differential Fairness Auditor for Black Box Classifiers",
    "authors": [
      "Xavier Gitiaux",
      "Huzefa Rangwala"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms are increasingly involved in sensitive\ndecision-making process with adversarial implications on individuals. This\npaper presents mdfa, an approach that identifies the characteristics of the\nvictims of a classifier's discrimination. We measure discrimination as a\nviolation of multi-differential fairness. Multi-differential fairness is a\nguarantee that a black box classifier's outcomes do not leak information on the\nsensitive attributes of a small group of individuals. We reduce the problem of\nidentifying worst-case violations to matching distributions and predicting\nwhere sensitive attributes and classifier's outcomes coincide. We apply mdfa to\na recidivism risk assessment classifier and demonstrate that individuals\nidentified as African-American with little criminal history are three-times\nmore likely to be considered at high risk of violent recidivism than similar\nindividuals but not African-American.",
    "published_date": "2019-03-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07609v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.07581v2",
    "title": "MediaRank: Computational Ranking of Online News Sources",
    "authors": [
      "Junting Ye",
      "Steven Skiena"
    ],
    "author_ids": [],
    "abstract": "In the recent political climate, the topic of news quality has drawn\nattention both from the public and the academic communities. The growing\ndistrust of traditional news media makes it harder to find a common base of\naccepted truth. In this work, we design and build MediaRank\n(www.media-rank.com), a fully automated system to rank over 50,000 online news\nsources around the world. MediaRank collects and analyzes one million news\nwebpages and two million related tweets everyday. We base our algorithmic\nanalysis on four properties journalists have established to be associated with\nreporting quality: peer reputation, reporting bias / breadth, bottomline\nfinancial pressure, and popularity.\n  Our major contributions of this paper include: (i) Open, interpretable\nquality rankings for over 50,000 of the world's major news sources. Our\nrankings are validated against 35 published news rankings, including French,\nGerman, Russian, and Spanish language sources. MediaRank scores correlate\npositively with 34 of 35 of these expert rankings. (ii) New computational\nmethods for measuring influence and bottomline pressure. To the best of our\nknowledge, we are the first to study the large-scale news reporting citation\ngraph in-depth. We also propose new ways to measure the aggressiveness of\nadvertisements and identify social bots, establishing a connection between both\ntypes of bad behavior. (iii) Analyzing the effect of media source bias and\nsignificance. We prove that news sources cite others despite different\npolitical views in accord with quality measures. However, in four\nEnglish-speaking countries (US, UK, Canada, and Australia), the highest ranking\nsources all disproportionately favor left-wing parties, even when the majority\nof news sources exhibited conservative slants.",
    "published_date": "2019-03-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07581v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.07273v1",
    "title": "Prototype-based classifiers in the presence of concept drift: A modelling framework",
    "authors": [
      "Michael Biehl",
      "Fthi Abadi",
      "Christina Göpfert",
      "Barbara Hammer"
    ],
    "author_ids": [],
    "abstract": "We present a modelling framework for the investigation of prototype-based\nclassifiers in non-stationary environments. Specifically, we study Learning\nVector Quantization (LVQ) systems trained from a stream of high-dimensional,\nclustered data.We consider standard winner-takes-all updates known as LVQ1.\nStatistical properties of the input data change on the time scale defined by\nthe training process. We apply analytical methods borrowed from statistical\nphysics which have been used earlier for the exact description of learning in\nstationary environments. The suggested framework facilitates the computation of\nlearning curves in the presence of virtual and real concept drift. Here we\nfocus on timedependent class bias in the training data. First results\ndemonstrate that, while basic LVQ algorithms are suitable for the training in\nnon-stationary environments, weight decay as an explicit mechanism of\nforgetting does not improve the performance under the considered drift\nprocesses.",
    "published_date": "2019-03-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07273v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.07224v1",
    "title": "An End-to-End Joint Unsupervised Learning of Deep Model and Pseudo-Classes for Remote Sensing Scene Representation",
    "authors": [
      "Zhiqiang Gong",
      "Ping Zhong",
      "Weidong Hu",
      "Fang Liu",
      "Bingwei Hui"
    ],
    "author_ids": [],
    "abstract": "This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.",
    "published_date": "2019-03-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07224v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.07171v1",
    "title": "Responsible and Representative Multimodal Data Acquisition and Analysis: On Auditability, Benchmarking, Confidence, Data-Reliance & Explainability",
    "authors": [
      "Alice Baird",
      "Simone Hantke",
      "Björn Schuller"
    ],
    "author_ids": [],
    "abstract": "The ethical decisions behind the acquisition and analysis of audio, video or\nphysiological human data, harnessed for (deep) machine learning algorithms, is\nan increasing concern for the Artificial Intelligence (AI) community. In this\nregard, herein we highlight the growing need for responsible, and\nrepresentative data collection and analysis, through a discussion of modality\ndiversification. Factors such as Auditability, Benchmarking, Confidence,\nData-reliance, and Explainability (ABCDE), have been touched upon within the\nmachine learning community, and here we lay out these ABCDE sub-categories in\nrelation to the acquisition and analysis of multimodal data, to weave through\nthe high priority ethical concerns currently under discussion for AI. To this\nend, we propose how these five subcategories can be included in early planning\nof such acquisition paradigms.",
    "published_date": "2019-03-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07171v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.07140v1",
    "title": "Stability of the Shannon-Stam inequality via the Föllmer process",
    "authors": [
      "Ronen Eldan",
      "Dan Mikulincer"
    ],
    "author_ids": [],
    "abstract": "We prove stability estimates for the Shannon-Stam inequality (also known as\nthe entropy-power inequality) for log-concave random vectors in terms of\nentropy and transportation distance. In particular, we give the first stability\nestimate for general log-concave random vectors in the following form: for\nlog-concave random vectors $X,Y \\in \\mathbb{R}^d$, the deficit in the\nShannon-Stam inequality is bounded from below by the expression\n  $$\n  C \\left(\\mathrm{D}\\left(X||G\\right) + \\mathrm{D}\\left(Y||G\\right)\\right),\n  $$\n  where $\\mathrm{D}\\left( \\cdot ~ ||G\\right)$ denotes the relative entropy with\nrespect to the standard Gaussian and the constant $C$ depends only on the\ncovariance structures and the spectral gaps of $X$ and $Y$. In the case of\nuniformly log-concave vectors our analysis gives dimension-free bounds. Our\nproofs are based on a new approach which uses an entropy-minimizing process\nfrom stochastic control theory.",
    "published_date": "2019-03-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.FA",
      "math.IT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07140v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.07021v1",
    "title": "Responses to a Critique of Artificial Moral Agents",
    "authors": [
      "Adam Poulsen",
      "Michael Anderson",
      "Susan L. Anderson",
      "Ben Byford",
      "Fabio Fossa",
      "Erica L. Neely",
      "Alejandro Rosas",
      "Alan Winfield"
    ],
    "author_ids": [],
    "abstract": "The field of machine ethics is concerned with the question of how to embed\nethical behaviors, or a means to determine ethical behaviors, into artificial\nintelligence (AI) systems. The goal is to produce artificial moral agents\n(AMAs) that are either implicitly ethical (designed to avoid unethical\nconsequences) or explicitly ethical (designed to behave ethically). Van\nWynsberghe and Robbins' (2018) paper Critiquing the Reasons for Making\nArtificial Moral Agents critically addresses the reasons offered by machine\nethicists for pursuing AMA research; this paper, co-authored by machine\nethicists and commentators, aims to contribute to the machine ethics\nconversation by responding to that critique. The reasons for developing AMAs\ndiscussed in van Wynsberghe and Robbins (2018) are: it is inevitable that they\nwill be developed; the prevention of harm; the necessity for public trust; the\nprevention of immoral use; such machines are better moral reasoners than\nhumans, and building these machines would lead to a better understanding of\nhuman morality. In this paper, each co-author addresses those reasons in turn.\nIn so doing, this paper demonstrates that the reasons critiqued are not shared\nby all co-authors; each machine ethicist has their own reasons for researching\nAMAs. But while we express a diverse range of views on each of the six reasons\nin van Wynsberghe and Robbins' critique, we nevertheless share the opinion that\nthe scientific study of AMAs has considerable value.",
    "published_date": "2019-03-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07021v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.07008v4",
    "title": "Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks",
    "authors": [
      "Rodrigo Canaan",
      "Christoph Salge",
      "Julian Togelius",
      "Andy Nealen"
    ],
    "author_ids": [],
    "abstract": "From the beginning if the history of AI, there has been interest in games as\na platform of research. As the field developed, human-level competence in\ncomplex games became a target researchers worked to reach. Only relatively\nrecently has this target been finally met for traditional tabletop games such\nas Backgammon, Chess and Go. Current research focus has shifted to electronic\ngames, which provide unique challenges. As is often the case with AI research,\nthese results are liable to be exaggerated or misrepresented by either authors\nor third parties. The extent to which these games benchmark consist of fair\ncompetition between human and AI is also a matter of debate. In this work, we\nreview the statements made by authors and third parties in the general media\nand academic circle about these game benchmark results and discuss factors that\ncan impact the perception of fairness in the contest between humans and\nmachines",
    "published_date": "2019-03-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07008v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.07004v3",
    "title": "Network topology design to influence the effects of manipulative behaviors in a social choice procedure",
    "authors": [
      "Athanasios-Rafail Lagos",
      "George P. Papavassilopoulos"
    ],
    "author_ids": [],
    "abstract": "A social choice procedure is modeled as a repeated Nash game between the\nsocial agents, who are communicating with each other through a social\ncommunication network modeled by an undirected graph. The agents' criteria for\nthis game are describing a trade off between self-consistent and manipulative\nbehaviors. Their best response strategies are resulting in two dynamics rules,\none for the agents' opinions and one for their actions. The stability\nproperties of these dynamics are studied. In the case of instability, the\nstabilization of these dynamics through the design of the network topology is\nformulated as a constrained integer programming problem. The constraints have\nthe form of a Bilinear Matrix Inequality (BMI), which is known to result in a\nnonconvex feasible set in the general case. To deal with this problem a Genetic\nAlgorithm is designed. Finally, simulations are presented for several different\ninitial topologies and conclusions are derived concerning both the\nfunctionality of the algorithm and the advisability of the problem formulation.",
    "published_date": "2019-03-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07004v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.06762v1",
    "title": "The scenario approach meets uncertain variational inequalities and game theory",
    "authors": [
      "Dario Paccagnan",
      "Marco C. Campi"
    ],
    "author_ids": [],
    "abstract": "Variational inequalities are modelling tools used to capture a variety of\ndecision-making problems arising in mathematical optimization, operations\nresearch, game theory. The scenario approach is a set of techniques developed\nto tackle stochastic optimization problems, take decisions based on historical\ndata, and quantify their risk. The overarching goal of this manuscript is to\nbridge these two areas of research, and thus broaden the class of problems\namenable to be studied under the lens of the scenario approach. First and\nforemost, we provide out-of-samples feasibility guarantees for the solution of\nvariational and quasi variational inequality problems. Second, we apply these\nresults to two classes of uncertain games. In the first class, the uncertainty\nenters in the constraint sets, while in the second class the uncertainty enters\nin the cost functions. Finally, we exemplify the quality and relevance of our\nbounds through numerical simulations on a demand-response model.",
    "published_date": "2019-03-15T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06762v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.06726v1",
    "title": "Predicting Academic Performance for College Students: A Campus Behavior Perspective",
    "authors": [
      "Huaxiu Yao",
      "Defu Lian",
      "Yi Cao",
      "Yifan Wu",
      "Tao Zhou"
    ],
    "author_ids": [],
    "abstract": "Detecting abnormal behaviors of students in time and providing personalized\nintervention and guidance at the early stage is important in educational\nmanagement. Academic performance prediction is an important building block to\nenabling this pre-intervention and guidance. Most of the previous studies are\nbased on questionnaire surveys and self-reports, which suffer from small sample\nsize and social desirability bias. In this paper, we collect longitudinal\nbehavioral data from 6,597 students' smart cards and propose three major types\nof discriminative behavioral factors, diligence, orderliness, and sleep\npatterns. Empirical analysis demonstrates these behavioral factors are strongly\ncorrelated with academic performance. Furthermore, motivated by social\ninfluence theory, we analyze the correlation between each student's academic\nperformance with his/her behaviorally similar students'. Statistical tests\nindicate this correlation is significant. Based on these factors, we further\nbuild a multi-task predictive framework based on a learning-to-rank algorithm\nfor academic performance prediction. This framework captures inter-semester\ncorrelation, inter-major correlation and integrates student similarity to\npredict students' academic performance. The experiments on a large-scale\nreal-world dataset show the effectiveness of our methods for predicting\nacademic performance and the effectiveness of proposed behavioral factors.",
    "published_date": "2019-03-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06726v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.06469v3",
    "title": "Using Data Science to Understand the Film Industry's Gender Gap",
    "authors": [
      "Dima Kagan",
      "Thomas Chesney",
      "Michael Fire"
    ],
    "author_ids": [],
    "abstract": "Data science can offer answers to a wide range of social science questions.\nHere we turn attention to the portrayal of women in movies, an industry that\nhas a significant influence on society, impacting such aspects of life as\nself-esteem and career choice. To this end, we fused data from the online movie\ndatabase IMDb with a dataset of movie dialogue subtitles to create the largest\navailable corpus of movie social networks (15,540 networks). Analyzing this\ndata, we investigated gender bias in on-screen female characters over the past\ncentury. We find a trend of improvement in all aspects of women`s roles in\nmovies, including a constant rise in the centrality of female characters. There\nhas also been an increase in the number of movies that pass the well-known\nBechdel test, a popular--albeit flawed--measure of women in fiction. Here we\npropose a new and better alternative to this test for evaluating female roles\nin movies. Our study introduces fresh data, an open-code framework, and novel\ntechniques that present new opportunities in the research and analysis of\nmovies.",
    "published_date": "2019-03-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.data-an"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06469v3",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.06400v2",
    "title": "Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages",
    "authors": [
      "Shauli Ravfogel",
      "Yoav Goldberg",
      "Tal Linzen"
    ],
    "author_ids": [],
    "abstract": "How do typological properties such as word order and morphological case\nmarking affect the ability of neural sequence models to acquire the syntax of a\nlanguage? Cross-linguistic comparisons of RNNs' syntactic performance (e.g., on\nsubject-verb agreement prediction) are complicated by the fact that any two\nlanguages differ in multiple typological properties, as well as by differences\nin training corpus. We propose a paradigm that addresses these issues: we\ncreate synthetic versions of English, which differ from English in one or more\ntypological parameters, and generate corpora for those languages based on a\nparsed English corpus. We report a series of experiments in which RNNs were\ntrained to predict agreement features for verbs in each of those synthetic\nlanguages. Among other findings, (1) performance was higher in\nsubject-verb-object order (as in English) than in subject-object-verb order (as\nin Japanese), suggesting that RNNs have a recency bias; (2) predicting\nagreement with both subject and object (polypersonal agreement) improves over\npredicting each separately, suggesting that underlying syntactic knowledge\ntransfers across the two tasks; and (3) overt morphological case makes\nagreement prediction significantly easier, regardless of word order.",
    "published_date": "2019-03-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06400v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.06045v1",
    "title": "Using Machine Learning and Big Data Analytics to Prioritize Outpatients in HetNets",
    "authors": [
      "Mohammed Hadi",
      "Ahmed Lawey",
      "Taisir El-Gorashi",
      "Jaafar Elmirghani"
    ],
    "author_ids": [],
    "abstract": "In this paper, we introduce machine learning approaches that are used to\nprioritize outpatients (OP) according to their current health state, resulting\nin self-optimizing heterogeneous networks (HetNet) that intelligently adapt\naccording to users' needs. We use a na\\\"ive Bayesian classifier to analyze data\nacquired from OPs' medical records, alongside data from medical Internet of\nThings (IoT) sensors that provide the current state of the OP. We use this\nmachine learning algorithm to calculate the likelihood of a life-threatening\nmedical condition, in this case an imminent stroke. An OP is assigned\nhigh-powered resource blocks (RBs) according to the seriousness of their\ncurrent health state, enabling them to remain connected and send their critical\ndata to the designated medical facility with minimal delay. Using a mixed\ninteger linear programming formulation (MILP), we present two approaches to\noptimizing the uplink side of a HetNet in terms of user-RB assignment: a\nWeighted Sum Rate Maximization (WSRMax) approach and a Proportional Fairness\n(PF) approach. Using these approaches, we illustrate the utility of the\nproposed system in terms of providing reliable connectivity to medical IoT\nsensors, enabling the OPs to maintain the quality and speed of their\nconnection. Moreover, we demonstrate how system response can change according\nto alterations in the OPs' medical conditions.",
    "published_date": "2019-03-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06045v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.06549v1",
    "title": "Constrained Mutual Convex Cone Method for Image Set Based Recognition",
    "authors": [
      "Naoya Sogi",
      "Rui Zhu",
      "Jing-Hao Xue",
      "Kazuhiro Fukui"
    ],
    "author_ids": [],
    "abstract": "In this paper, we propose a method for image-set classification based on\nconvex cone models. Image set classification aims to classify a set of images,\nwhich were usually obtained from video frames or multi-view cameras, into a\ntarget object. To accurately and stably classify a set, it is essential to\nrepresent structural information of the set accurately. There are various\nrepresentative image features, such as histogram based features, HLAC, and\nConvolutional Neural Network (CNN) features. We should note that most of them\nhave non-negativity and thus can be effectively represented by a convex cone.\nThis leads us to introduce the convex cone representation to image-set\nclassification. To establish a convex cone based framework, we mathematically\ndefine multiple angles between two convex cones, and then define the geometric\nsimilarity between the cones using the angles. Moreover, to enhance the\nframework, we introduce a discriminant space that maximizes the between-class\nvariance (gaps) and minimizes the within-class variance of the projected convex\ncones onto the discriminant space, similar to the Fisher discriminant analysis.\nFinally, the classification is performed based on the similarity between\nprojected convex cones. The effectiveness of the proposed method is\ndemonstrated experimentally by using five databases: CMU PIE dataset, ETH-80,\nCMU Motion of Body dataset, Youtube Celebrity dataset, and a private database\nof multi-view hand shapes.",
    "published_date": "2019-03-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06549v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.05356v1",
    "title": "Age-of-Information vs. Value-of-Information Scheduling for Cellular Networked Control Systems",
    "authors": [
      "Onur Ayan",
      "Mikhail Vilgelm",
      "Markus Klügel",
      "Sandra Hirche",
      "Wolfgang Kellerer"
    ],
    "author_ids": [],
    "abstract": "Age-of-Information (AoI) is a recently introduced metric for network\noperation with sensor applications which quantifies the freshness of data. In\nthe context of networked control systems (NCSs), we compare the worth of the\nAoI metric with the value-of-information (VoI) metric, which is related to the\nuncertainty reduction in stochastic processes. First, we show that the\nuncertainty propagates non-linearly over time depending on system dynamics.\nNext, we define the value of a new update of the process of interest as a\nfunction of AoI and system parameters of the NCSs. We use the aggregated update\nvalue as a utility for the centralized scheduling problem in a cellular NCS\ncomposed of multiple heterogeneous control loops. By conducting a simulative\nanalysis, we show that prioritizing transmissions with higher VoI improves\nperformance of the NCSs compared with providing fair data freshness to all\nsub-systems equally.",
    "published_date": "2019-03-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.NI",
      "cs.SY",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.05356v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.12071v1",
    "title": "Big Data Analytics and AI in Mental Healthcare",
    "authors": [
      "Ariel Rosenfeld",
      "David Benrimoh",
      "Caitrin Armstrong",
      "Nykan Mirchi",
      "Timothe Langlois-Therrien",
      "Colleen Rollins",
      "Myriam Tanguay-Sela",
      "Joseph Mehltretter",
      "Robert Fratila",
      "Sonia Israel",
      "Emily Snook",
      "Kelly Perlman",
      "Akiva Kleinerman",
      "Bechara Saab",
      "Mark Thoburn",
      "Cheryl Gabbay",
      "Amit Yaniv-Rosenfeld"
    ],
    "author_ids": [],
    "abstract": "Mental health conditions cause a great deal of distress or impairment;\ndepression alone will affect 11% of the world's population. The application of\nArtificial Intelligence (AI) and big-data technologies to mental health has\ngreat potential for personalizing treatment selection, prognosticating,\nmonitoring for relapse, detecting and helping to prevent mental health\nconditions before they reach clinical-level symptomatology, and even delivering\nsome treatments. However, unlike similar applications in other fields of\nmedicine, there are several unique challenges in mental health applications\nwhich currently pose barriers towards the implementation of these technologies.\nSpecifically, there are very few widely used or validated biomarkers in mental\nhealth, leading to a heavy reliance on patient and clinician derived\nquestionnaire data as well as interpretation of new signals such as digital\nphenotyping. In addition, diagnosis also lacks the same objective 'gold\nstandard' as in other conditions such as oncology, where clinicians and\nresearchers can often rely on pathological analysis for confirmation of\ndiagnosis. In this chapter we discuss the major opportunities, limitations and\ntechniques used for improving mental healthcare through AI and big-data. We\nexplore both the computational, clinical and ethical considerations and best\npractices as well as lay out the major researcher directions for the near\nfuture.",
    "published_date": "2019-03-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.12071v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.05072v1",
    "title": "Characterization of Local Attitudes Toward Immigration Using Social Media",
    "authors": [
      "Yerka Freire",
      "Eduardo Graells-Garrido"
    ],
    "author_ids": [],
    "abstract": "Migration is a worldwide phenomenon that may generate different reactions in\nthe population. Attitudes vary from those that support multiculturalism and\ncommunion between locals and foreigners, to contempt and hatred toward\nimmigrants. Since anti-immigration attitudes are often materialized in acts of\nviolence and discrimination, it is important to identify factors that\ncharacterize these attitudes. However, doing so is expensive and impractical,\nas traditional methods require enormous efforts to collect data. In this paper,\nwe propose to leverage Twitter to characterize local attitudes toward\nimmigration, with a case study on Chile, where immigrant population has\ndrastically increased in recent years. Using semi-supervised topic modeling, we\nsituated 49K users into a spectrum ranging from in-favor to against\nimmigration. We characterized both sides of the spectrum in two aspects: the\nemotions and lexical categories relevant for each attitude, and the discussion\nnetwork structure. We found that the discussion is mostly driven by Haitian\nimmigration; that there are temporal trends in tendency and polarity of\ndiscussion; and that assortative behavior on the network differs with respect\nto attitude. These insights may inform policy makers on how people feel with\nrespect to migration, with potential implications on communication of policy\nand the design of interventions to improve inter-group relations.",
    "published_date": "2019-03-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.05072v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.04991v5",
    "title": "Theory III: Dynamics and Generalization in Deep Networks",
    "authors": [
      "Andrzej Banburski",
      "Qianli Liao",
      "Brando Miranda",
      "Lorenzo Rosasco",
      "Fernanda De La Torre",
      "Jack Hidary",
      "Tomaso Poggio"
    ],
    "author_ids": [],
    "abstract": "The key to generalization is controlling the complexity of the network.\nHowever, there is no obvious control of complexity -- such as an explicit\nregularization term -- in the training of deep networks for classification. We\nwill show that a classical form of norm control -- but kind of hidden -- is\npresent in deep networks trained with gradient descent techniques on\nexponential-type losses. In particular, gradient descent induces a dynamics of\nthe normalized weights which converge for $t \\to \\infty$ to an equilibrium\nwhich corresponds to a minimum norm (or maximum margin) solution. For\nsufficiently large but finite $\\rho$ -- and thus finite $t$ -- the dynamics\nconverges to one of several margin maximizers, with the margin monotonically\nincreasing towards a limit stationary point of the flow. In the usual case of\nstochastic gradient descent, most of the stationary points are likely to be\nconvex minima corresponding to a constrained minimizer -- the network with\nnormalized weights-- which corresponds to vanishing regularization. The\nsolution has zero generalization gap, for fixed architecture, asymptotically\nfor $N \\to \\infty$, where $N$ is the number of training examples. Our approach\nextends some of the original results of Srebro from linear networks to deep\nnetworks and provides a new perspective on the implicit bias of gradient\ndescent. We believe that the elusive complexity control we describe is\nresponsible for the puzzling empirical finding of good predictive performance\nby deep networks, despite overparametrization.",
    "published_date": "2019-03-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04991v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.05457v2",
    "title": "An Exponential Efron-Stein Inequality for Lq Stable Learning Rules",
    "authors": [
      "Karim Abou-Moustafa",
      "Csaba Szepesvari"
    ],
    "author_ids": [],
    "abstract": "There is accumulating evidence in the literature that stability of learning\nalgorithms is a key characteristic that permits a learning algorithm to\ngeneralize. Despite various insightful results in this direction, there seems\nto be an overlooked dichotomy in the type of stability-based generalization\nbounds we have in the literature. On one hand, the literature seems to suggest\nthat exponential generalization bounds for the estimated risk, which are\noptimal, can be only obtained through stringent, distribution independent and\ncomputationally intractable notions of stability such as uniform stability. On\nthe other hand, it seems that weaker notions of stability such as hypothesis\nstability, although it is distribution dependent and more amenable to\ncomputation, can only yield polynomial generalization bounds for the estimated\nrisk, which are suboptimal.\n  In this paper, we address the gap between these two regimes of results. In\nparticular, the main question we address here is \\emph{whether it is possible\nto derive exponential generalization bounds for the estimated risk using a\nnotion of stability that is computationally tractable and distribution\ndependent, but weaker than uniform stability. Using recent advances in\nconcentration inequalities, and using a notion of stability that is weaker than\nuniform stability but distribution dependent and amenable to computation, we\nderive an exponential tail bound for the concentration of the estimated risk of\na hypothesis returned by a general learning rule, where the estimated risk is\nexpressed in terms of either the resubstitution estimate (empirical error), or\nthe deleted (or, leave-one-out) estimate. As an illustration, we derive\nexponential tail bounds for ridge regression with unbounded responses, where we\nshow how stability changes with the tail behavior of the response variables.",
    "published_date": "2019-03-12T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.05457v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.04613v1",
    "title": "Learning Edge Properties in Graphs from Path Aggregations",
    "authors": [
      "Rakshit Agrawal",
      "Luca de Alfaro"
    ],
    "author_ids": [],
    "abstract": "Graph edges, along with their labels, can represent information of\nfundamental importance, such as links between web pages, friendship between\nusers, the rating given by users to other users or items, and much more. We\nintroduce LEAP, a trainable, general framework for predicting the presence and\nproperties of edges on the basis of the local structure, topology, and labels\nof the graph. The LEAP framework is based on the exploration and\nmachine-learning aggregation of the paths connecting nodes in a graph. We\nprovide several methods for performing the aggregation phase by training path\naggregators, and we demonstrate the flexibility and generality of the framework\nby applying it to the prediction of links and user ratings in social networks.\n  We validate the LEAP framework on two problems: link prediction, and user\nrating prediction. On eight large datasets, among which the arXiv collaboration\nnetwork, the Yeast protein-protein interaction, and the US airlines routes\nnetwork, we show that the link prediction performance of LEAP is at least as\ngood as the current state of the art methods, such as SEAL and WLNM. Next, we\nconsider the problem of predicting user ratings on other users: this problem is\nknown as the edge-weight prediction problem in weighted signed networks (WSN).\nOn Bitcoin networks, and Wikipedia RfA, we show that LEAP performs consistently\nbetter than the Fairness & Goodness based regression models, varying the amount\nof training edges between 10 to 90%. These examples demonstrate that LEAP, in\nspite of its generality, can match or best the performance of approaches that\nhave been especially crafted to solve very specific edge prediction problems.",
    "published_date": "2019-03-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04613v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.04561v2",
    "title": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification",
    "authors": [
      "Daniel Borkan",
      "Lucas Dixon",
      "Jeffrey Sorensen",
      "Nithum Thain",
      "Lucy Vasserman"
    ],
    "author_ids": [],
    "abstract": "Unintended bias in Machine Learning can manifest as systemic differences in\nperformance for different demographic groups, potentially compounding existing\nchallenges to fairness in society at large. In this paper, we introduce a suite\nof threshold-agnostic metrics that provide a nuanced view of this unintended\nbias, by considering the various ways that a classifier's score distribution\ncan vary across designated groups. We also introduce a large new test set of\nonline comments with crowd-sourced annotations for identity references. We use\nthis to show how our metrics can be used to find new and potentially subtle\nunintended bias in existing public models.",
    "published_date": "2019-03-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04561v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.04209v6",
    "title": "From interpretability to inference: an estimation framework for universal approximators",
    "authors": [
      "Andreas Joseph"
    ],
    "author_ids": [],
    "abstract": "We present a novel framework for estimation and inference with the broad\nclass of universal approximators. Estimation is based on the decomposition of\nmodel predictions into Shapley values. Inference relies on analyzing the bias\nand variance properties of individual Shapley components. We show that Shapley\nvalue estimation is asymptotically unbiased, and we introduce Shapley\nregressions as a tool to uncover the true data generating process from noisy\ndata alone. The well-known case of the linear regression is the special case in\nour framework if the model is linear in parameters. We present theoretical,\nnumerical, and empirical results for the estimation of heterogeneous treatment\neffects as our guiding example.",
    "published_date": "2019-03-11T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "62G10, 62G20, 62-07, 91-08, 91A12",
      "G.1; G.2; G.3; I.2"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04209v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.04143v3",
    "title": "The Unconstrained Ear Recognition Challenge 2019 - ArXiv Version With Appendix",
    "authors": [
      "Žiga Emeršič",
      "Aruna Kumar S. V.",
      "B. S. Harish",
      "Weronika Gutfeter",
      "Jalil Nourmohammadi Khiarak",
      "Andrzej Pacut",
      "Earnest Hansley",
      "Mauricio Pamplona Segundo",
      "Sudeep Sarkar",
      "Hyeonjung Park",
      "Gi Pyo Nam",
      "Ig-Jae Kim",
      "Sagar G. Sangodkar",
      "Ümit Kaçar",
      "Murvet Kirci",
      "Li Yuan",
      "Jishou Yuan",
      "Haonan Zhao",
      "Fei Lu",
      "Junying Mao",
      "Xiaoshuang Zhang",
      "Dogucan Yaman",
      "Fevziye Irem Eyiokur",
      "Kadir Bulut Özler",
      "Hazım Kemal Ekenel",
      "Debbrota Paul Chowdhury",
      "Sambit Bakshi",
      "Pankaj K. Sa",
      "Banshidhar Majhi",
      "Peter Peer",
      "Vitomir Štruc"
    ],
    "author_ids": [],
    "abstract": "This paper presents a summary of the 2019 Unconstrained Ear Recognition\nChallenge (UERC), the second in a series of group benchmarking efforts centered\naround the problem of person recognition from ear images captured in\nuncontrolled settings. The goal of the challenge is to assess the performance\nof existing ear recognition techniques on a challenging large-scale ear dataset\nand to analyze performance of the technology from various viewpoints, such as\ngeneralization abilities to unseen data characteristics, sensitivity to\nrotations, occlusions and image resolution and performance bias on sub-groups\nof subjects, selected based on demographic criteria, i.e. gender and ethnicity.\nResearch groups from 12 institutions entered the competition and submitted a\ntotal of 13 recognition approaches ranging from descriptor-based methods to\ndeep-learning models. The majority of submissions focused on ensemble based\nmethods combining either representations from multiple deep models or\nhand-crafted with learned image descriptors. Our analysis shows that methods\nincorporating deep learning models clearly outperform techniques relying solely\non hand-crafted descriptors, even though both groups of techniques exhibit\nsimilar behaviour when it comes to robustness to various covariates, such\npresence of occlusions, changes in (head) pose, or variability in image\nresolution. The results of the challenge also show that there has been\nconsiderable progress since the first UERC in 2017, but that there is still\nample room for further research in this area.",
    "published_date": "2019-03-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04143v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.03910v4",
    "title": "Fairness for Robust Log Loss Classification",
    "authors": [
      "Ashkan Rezaei",
      "Rizal Fathony",
      "Omid Memarrast",
      "Brian Ziebart"
    ],
    "author_ids": [],
    "abstract": "Developing classification methods with high accuracy that also avoid unfair\ntreatment of different groups has become increasingly important for data-driven\ndecision making in social applications. Many existing methods enforce fairness\nconstraints on a selected classifier (e.g., logistic regression) by directly\nforming constrained optimizations. We instead re-derive a new classifier from\nthe first principles of distributional robustness that incorporates fairness\ncriteria into a worst-case logarithmic loss minimization. This construction\ntakes the form of a minimax game and produces a parametric exponential family\nconditional distribution that resembles truncated logistic regression. We\npresent the theoretical benefits of our approach in terms of its convexity and\nasymptotic convergence. We then demonstrate the practical advantages of our\napproach on three benchmark fairness datasets.",
    "published_date": "2019-03-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.03910v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.03862v2",
    "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
    "authors": [
      "Hila Gonen",
      "Yoav Goldberg"
    ],
    "author_ids": [],
    "abstract": "Word embeddings are widely used in NLP for a vast range of tasks. It was\nshown that word embeddings derived from text corpora reflect gender biases in\nsociety. This phenomenon is pervasive and consistent across different word\nembedding models, causing serious concern. Several recent works tackle this\nproblem, and propose methods for significantly reducing this gender bias in\nword embeddings, demonstrating convincing results. However, we argue that this\nremoval is superficial. While the bias is indeed substantially reduced\naccording to the provided bias definition, the actual effect is mostly hiding\nthe bias, not removing it. The gender bias information is still reflected in\nthe distances between \"gender-neutralized\" words in the debiased embeddings,\nand can be recovered from them. We present a series of experiments to support\nthis claim, for two debiasing methods. We conclude that existing bias removal\ntechniques are insufficient, and should not be trusted for providing\ngender-neutral modeling.",
    "published_date": "2019-03-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.03862v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.03683v1",
    "title": "Transparency, Fairness, Data Protection, Neutrality: Data Management Challenges in the Face of New Regulation",
    "authors": [
      "Serge Abiteboul",
      "Julia Stoyanovich"
    ],
    "author_ids": [],
    "abstract": "The data revolution continues to transform every sector of science, industry\nand government. Due to the incredible impact of data-driven technology on\nsociety, we are becoming increasingly aware of the imperative to use data and\nalgorithms responsibly -- in accordance with laws and ethical norms. In this\narticle we discuss three recent regulatory frameworks: the European Union's\nGeneral Data Protection Regulation (GDPR), the New York City Automated\nDecisions Systems (ADS) Law, and the Net Neutrality principle, that aim to\nprotect the rights of individuals who are impacted by data collection and\nanalysis. These frameworks are prominent examples of a global trend:\nGovernments are starting to recognize the need to regulate data-driven\nalgorithmic technology.\n  Our goal in this paper is to bring these regulatory frameworks to the\nattention of the data management community, and to underscore the technical\nchallenges they raise and which we, as a community, are well-equipped to\naddress. The main take-away of this article is that legal and ethical norms\ncannot be incorporated into data-driven systems as an afterthought. Rather, we\nmust think in terms of responsibility by design, viewing it as a systems\nrequirement.",
    "published_date": "2019-03-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.03683v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.12582v2",
    "title": "Epistemological and Bibliometric Analysis of Ethics and Shared Responsibility Health Policy and IoT Systems",
    "authors": [
      "Petar Radanliev",
      "David De Roure"
    ],
    "author_ids": [],
    "abstract": "The focus in this paper is placed on shared responsibility and ethics in\nhealth policy, specific to Internet of Things (IoT) devices in healthcare\nsystems. The article assesses how the introduction of IoT brings risks to the\nsecurity of medical systems. The justification for this research emerges from\nthe opportunities emerging from digital technologies for medical services, but\nalso creating a range of new cyber risks in the shared healthcare\ninfrastructure. Such concerns are often not visible to individual departments\nin an integrated healthcare system. In addition, many healthcare organisations\ndo not possess cyber skills and are faced with barriers to the adoption of\nsmart manufacturing technologies, e.g., cost. These barriers trigger ethical\nconcerns related to responsibility of cyber risks in shared healthcare systems.",
    "published_date": "2019-03-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.OH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.12582v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.06669v3",
    "title": "Stay Ahead of Poachers: Illegal Wildlife Poaching Prediction and Patrol Planning Under Uncertainty with Field Test Evaluations",
    "authors": [
      "Lily Xu",
      "Shahrzad Gholami",
      "Sara Mc Carthy",
      "Bistra Dilkina",
      "Andrew Plumptre",
      "Milind Tambe",
      "Rohit Singh",
      "Mustapha Nsubuga",
      "Joshua Mabonga",
      "Margaret Driciru",
      "Fred Wanyama",
      "Aggrey Rwetsiba",
      "Tom Okello",
      "Eric Enyel"
    ],
    "author_ids": [],
    "abstract": "Illegal wildlife poaching threatens ecosystems and drives endangered species\ntoward extinction. However, efforts for wildlife protection are constrained by\nthe limited resources of law enforcement agencies. To help combat poaching, the\nProtection Assistant for Wildlife Security (PAWS) is a machine learning\npipeline that has been developed as a data-driven approach to identify areas at\nhigh risk of poaching throughout protected areas and compute optimal patrol\nroutes. In this paper, we take an end-to-end approach to the data-to-deployment\npipeline for anti-poaching. In doing so, we address challenges including\nextreme class imbalance (up to 1:200), bias, and uncertainty in wildlife\npoaching data to enhance PAWS, and we apply our methodology to three national\nparks with diverse characteristics. (i) We use Gaussian processes to quantify\npredictive uncertainty, which we exploit to improve robustness of our\nprescribed patrols and increase detection of snares by an average of 30%. We\nevaluate our approach on real-world historical poaching data from Murchison\nFalls and Queen Elizabeth National Parks in Uganda and, for the first time,\nSrepok Wildlife Sanctuary in Cambodia. (ii) We present the results of\nlarge-scale field tests conducted in Murchison Falls and Srepok Wildlife\nSanctuary which confirm that the predictive power of PAWS extends promisingly\nto multiple parks. This paper is part of an effort to expand PAWS to 800 parks\naround the world through integration with SMART conservation software.",
    "published_date": "2019-03-08T00:00:00",
    "year": 2019,
    "categories": [
      "stat.AP",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.06669v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.05999v1",
    "title": "Estimating Social Influence Using Latent Space Adjusted Approach in R",
    "authors": [
      "Ran Xu"
    ],
    "author_ids": [],
    "abstract": "Social influence, sometimes referred to as spillover or contagion, have been\nextensively studied in various empirical social network research. However,\nthere are various estimation challenges in identifying social influence\neffects, as they are often entangled with other factors, such as homophily in\nthe selection process, the individual's preference for the same social\nsettings, etc. Methods currently available either do not solve these problems\nor require strong assumptions. Recent works by Xu 2018 and others show that a\nlatent-space adjusted approach based on the latent space model has potential to\ndisentangle the influence from other processes, and the simulation evidence\nshows the approach performs better than other state-of-the-art approaches in\nterms of recovering the true social influence effect when there is an\nunobserved trait co-determining influence and selection. In this paper we\nillustrate how latent space adjusted approach accounts for bias in the\nestimation of the social influence effect, and demonstrate how this approach\ncan be implemented to estimate various social influence models with an\nempirical example in R.",
    "published_date": "2019-03-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "62F99, 62-01"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.05999v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.02843v1",
    "title": "Neighborhood Mutual Remainder: Self-Stabilizing Implementation of Look-Compute-Move Robots (Extended Abstract)",
    "authors": [
      "Shlomi Dolev",
      "Sayaka Kamei",
      "Yoshiaki Katayama",
      "Fukuhito Ooshita",
      "Koichi Wada"
    ],
    "author_ids": [],
    "abstract": "Local mutual exclusion guarantees that no two neighboring processes enter a\ncritical section at the same time while satisfying both mutual exclusion and no\nstarvation properties. On the other hand, processes may want to execute some\noperation simultaneously with the neighbors. Of course, we can use a globally\nsynchronized clock to achieve the task but it is very expensive to realize it\nin a distributed system in general.\n  In this paper, we define a new concept neighborhood mutual remainder. A\ndistributed algorithm that satisfies the neighborhood mutual remainder\nrequirement should satisfy global fairness, l-exclusion and repeated local\nrendezvous requirements. Global fairness is satisfied when each process (that\nrequests to enter the critical section infinitely often) executes the critical\nsection infinitely often, l-exclusion is satisfied when at most l neighboring\nprocesses enter the critical section at the same time, and repeated local\nrendezvous is satisfied when for each process infinitely often no process in\nthe closed neighborhood is in the critical or trying section.\n  We first formalize the concept of neighborhood mutual remainder, and give a\nsimple self-stabilizing algorithm to demonstrate the design paradigm to achieve\nneighborhood mutual remainder. We also present two applications of neighborhood\nmutual remainder to a Look-Compute-Move robot system. One is for implementing a\nmove-atomic property and the other is for implementing FSYNC scheduler, where\nrobots possess an independent clock that is advanced in the same speed. These\nare the first self-stabilizing implementations of the LCM synchronization.",
    "published_date": "2019-03-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.02843v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.04277v2",
    "title": "Distributed Online Convex Optimization with Time-Varying Coupled Inequality Constraints",
    "authors": [
      "Xinlei Yi",
      "Xiuxian Li",
      "Lihua Xie",
      "Karl H. Johansson"
    ],
    "author_ids": [],
    "abstract": "This paper considers distributed online optimization with time-varying\ncoupled inequality constraints. The global objective function is composed of\nlocal convex cost and regularization functions and the coupled constraint\nfunction is the sum of local convex functions. A distributed online primal-dual\ndynamic mirror descent algorithm is proposed to solve this problem, where the\nlocal cost, regularization, and constraint functions are held privately and\nrevealed only after each time slot. Without assuming Slater's condition, we\nfirst derive regret and constraint violation bounds for the algorithm and show\nhow they depend on the stepsize sequences, the accumulated dynamic variation of\nthe comparator sequence, the number of agents, and the network connectivity. As\na result, under some natural decreasing stepsize sequences, we prove that the\nalgorithm achieves sublinear dynamic regret and constraint violation if the\naccumulated dynamic variation of the optimal sequence also grows sublinearly.\nWe also prove that the algorithm achieves sublinear static regret and\nconstraint violation under mild conditions. Assuming Slater's condition, we\nshow that the algorithm achieves smaller bounds on the constraint violation. In\naddition, smaller bounds on the static regret are achieved when the objective\nfunction is strongly convex. Finally, numerical simulations are provided to\nillustrate the effectiveness of the theoretical results.",
    "published_date": "2019-03-06T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.DC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04277v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.04278v1",
    "title": "Coping with Large Traffic Volumes in Schedule-Driven Traffic Signal Control",
    "authors": [
      "Hsu-Chieh Hu",
      "Stephen F. Smith"
    ],
    "author_ids": [],
    "abstract": "Recent work in decentralized, schedule-driven traffic control has\ndemonstrated the ability to significantly improve traffic flow efficiency in\ncomplex urban road networks. However, in situations where vehicle volumes\nincrease to the point that the physical capacity of a road network reaches or\nexceeds saturation, it has been observed that the effectiveness of a\nschedule-driven approach begins to degrade, leading to progressively higher\nnetwork congestion. In essence, the traffic control problem becomes less of a\nscheduling problem and more of a queue management problem in this circumstance.\nIn this paper we propose a composite approach to real-time traffic control that\nuses sensed information on queue lengths to influence scheduling decisions and\ngracefully shift the signal control strategy to queue management in high\nvolume/high congestion settings. Specifically, queue-length information is used\nto establish weights for the sensed vehicle clusters that must be scheduled\nthrough a given intersection at any point, and hence bias the wait time\nminimization calculation. To compute these weights, we develop a model in which\nsuccessive movement phases are viewed as different states of an Ising model,\nand parameters quantify strength of interactions. To ensure scalability, queue\ninformation is only exchanged between direct neighbors and the asynchronous\nnature of local intersection scheduling is preserved. We demonstrate the\npotential of the approach through microscopic traffic simulation of a\nreal-world road network, showing a 60% reduction in average wait times over the\nbaseline schedule-driven approach in heavy traffic scenarios. We also report\ninitial field test results, which show the ability to reduce queues during\nheavy traffic periods.",
    "published_date": "2019-03-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.04278v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.02591v1",
    "title": "Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing",
    "authors": [
      "Wenhan Xiong",
      "Jiawei Wu",
      "Deren Lei",
      "Mo Yu",
      "Shiyu Chang",
      "Xiaoxiao Guo",
      "William Yang Wang"
    ],
    "author_ids": [],
    "abstract": "Existing entity typing systems usually exploit the type hierarchy provided by\nknowledge base (KB) schema to model label correlations and thus improve the\noverall performance. Such techniques, however, are not directly applicable to\nmore open and practical scenarios where the type set is not restricted by KB\nschema and includes a vast number of free-form types. To model the underly-ing\nlabel correlations without access to manually annotated label structures, we\nintroduce a novel label-relational inductive bias, represented by a graph\npropagation layer that effectively encodes both global label co-occurrence\nstatistics and word-level similarities.On a large dataset with over 10,000\nfree-form types, the graph-enhanced model equipped with an attention-based\nmatching module is able to achieve a much higher recall score while maintaining\na high-level precision. Specifically, it achieves a 15.3% relative F1\nimprovement and also less inconsistency in the outputs. We further show that a\nsimple modification of our proposed graph layer can also improve the\nperformance on a conventional and widely-tested dataset that only includes\nKB-schema types.",
    "published_date": "2019-03-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.02591v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.02088v1",
    "title": "Limitations of Pinned AUC for Measuring Unintended Bias",
    "authors": [
      "Daniel Borkan",
      "Lucas Dixon",
      "John Li",
      "Jeffrey Sorensen",
      "Nithum Thain",
      "Lucy Vasserman"
    ],
    "author_ids": [],
    "abstract": "This report examines the Pinned AUC metric introduced and highlights some of\nits limitations. Pinned AUC provides a threshold-agnostic measure of unintended\nbias in a classification model, inspired by the ROC-AUC metric. However, as we\nhighlight in this report, there are ways that the metric can obscure different\nkinds of unintended biases when the underlying class distributions on which\nbias is being measured are not carefully controlled.",
    "published_date": "2019-03-05T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.02088v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.01462v2",
    "title": "Deep learning based pulse shape discrimination for germanium detectors",
    "authors": [
      "P. Holl",
      "L. Hauertmann",
      "B. Majorovits",
      "O. Schulz",
      "M. Schuster",
      "A. J. Zsigmond"
    ],
    "author_ids": [],
    "abstract": "Experiments searching for rare processes like neutrinoless double beta decay\nheavily rely on the identification of background events to reduce their\nbackground level and increase their sensitivity. We present a novel machine\nlearning based method to recognize one of the most abundant classes of\nbackground events in these experiments. By combining a neural network for\nfeature extraction with a smaller classification network, our method can be\ntrained with only a small number of labeled events. To validate our method, we\nuse signals from a broad-energy germanium detector irradiated with a $^{228}$Th\ngamma source. We find that it matches the performance of state-of-the-art\nalgorithms commonly used for this detector type. However, it requires less\ntuning and calibration and shows potential to identify certain types of\nbackground events missed by other methods.",
    "published_date": "2019-03-04T00:00:00",
    "year": 2019,
    "categories": [
      "physics.ins-det",
      "cs.LG",
      "nucl-ex"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.01462v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.01333v1",
    "title": "A Serious Game for Introducing Software Engineering Ethics to University Students",
    "authors": [
      "Michalis Xenos",
      "Vasiliki Velli"
    ],
    "author_ids": [],
    "abstract": "This paper presents a game based on storytelling, in which the players are\nfaced with ethical dilemmas related to software engineering specific issues.\nThe players' choices have consequences on how the story unfolds and could lead\nto various alternative endings. This Ethics Game was used as a tool to mediate\nthe learning activity and it was evaluated by 144 students during a Software\nEngineering Course on the 2017-2018 academic year. This evaluation was based on\na within-subject pre-post design methodology and provided insights on the\nstudents learning gain (academic performance), as well as on the students'\nperceived educational experience. In addition, it provided the results of the\nstudents' usability evaluation of the Ethics Game. The results indicated that\nthe students did improve their knowledge about software engineering ethics by\nplaying this game. Also, they considered this game to be a useful educational\ntool and of high usability. Female students had statistically significant\nhigher knowledge gain and higher evaluation scores than male students, while no\nstatistically significant differences were measured in groups based on the year\nof study.",
    "published_date": "2019-03-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.01333v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.01209v2",
    "title": "On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning",
    "authors": [
      "Hoda Heidari",
      "Vedant Nanda",
      "Krishna P. Gummadi"
    ],
    "author_ids": [],
    "abstract": "Most existing notions of algorithmic fairness are one-shot: they ensure some\nform of allocative equality at the time of decision making, but do not account\nfor the adverse impact of the algorithmic decisions today on the long-term\nwelfare and prosperity of certain segments of the population. We take a broader\nperspective on algorithmic fairness. We propose an effort-based measure of\nfairness and present a data-driven framework for characterizing the long-term\nimpact of algorithmic policies on reshaping the underlying population.\nMotivated by the psychological literature on \\emph{social learning} and the\neconomic literature on equality of opportunity, we propose a micro-scale model\nof how individuals may respond to decision-making algorithms. We employ\nexisting measures of segregation from sociology and economics to quantify the\nresulting macro-scale population-level change. Importantly, we observe that\ndifferent models may shift the group-conditional distribution of qualifications\nin different directions. Our findings raise a number of important questions\nregarding the formalization of fairness for decision-making models.",
    "published_date": "2019-03-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.01209v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.00967v2",
    "title": "Group-Fairness in Influence Maximization",
    "authors": [
      "Alan Tsang",
      "Bryan Wilder",
      "Eric Rice",
      "Milind Tambe",
      "Yair Zick"
    ],
    "author_ids": [],
    "abstract": "Influence maximization is a widely used model for information dissemination\nin social networks. Recent work has employed such interventions across a wide\nrange of social problems, spanning public health, substance abuse, and\ninternational development (to name a few examples). A critical but understudied\nquestion is whether the benefits of such interventions are fairly distributed\nacross different groups in the population; e.g., avoiding discrimination with\nrespect to sensitive attributes such as race or gender. Drawing on legal and\ngame-theoretic concepts, we introduce formal definitions of fairness in\ninfluence maximization. We provide an algorithmic framework to find solutions\nwhich satisfy fairness constraints, and in the process improve the state of the\nart for general multi-objective submodular maximization problems. Experimental\nresults on real data from an HIV prevention intervention for homeless youth\nshow that standard influence maximization techniques oftentimes neglect smaller\ngroups which contribute less to overall utility, resulting in a disparity which\nour proposed algorithms substantially reduce.",
    "published_date": "2019-03-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.00967v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.00780v1",
    "title": "Fairness in Recommendation Ranking through Pairwise Comparisons",
    "authors": [
      "Alex Beutel",
      "Jilin Chen",
      "Tulsee Doshi",
      "Hai Qian",
      "Li Wei",
      "Yi Wu",
      "Lukasz Heldt",
      "Zhe Zhao",
      "Lichan Hong",
      "Ed H. Chi",
      "Cristos Goodrow"
    ],
    "author_ids": [],
    "abstract": "Recommender systems are one of the most pervasive applications of machine\nlearning in industry, with many services using them to match users to products\nor information. As such it is important to ask: what are the possible fairness\nrisks, how can we quantify them, and how should we address them? In this paper\nwe offer a set of novel metrics for evaluating algorithmic fairness concerns in\nrecommender systems. In particular we show how measuring fairness based on\npairwise comparisons from randomized experiments provides a tractable means to\nreason about fairness in rankings from recommender systems. Building on this\nmetric, we offer a new regularizer to encourage improving this metric during\nmodel training and thus improve fairness in the resulting rankings. We apply\nthis pairwise regularization to a large-scale, production recommender system\nand show that we are able to significantly improve the system's pairwise\nfairness.",
    "published_date": "2019-03-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.00780v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.00516v1",
    "title": "Introducing Super Pseudo Panels: Application to Transport Preference Dynamics",
    "authors": [
      "Stanislav S. Borysov",
      "Jeppe Rich"
    ],
    "author_ids": [],
    "abstract": "We propose a new approach for constructing synthetic pseudo-panel data from\ncross-sectional data. The pseudo panel and the preferences it intends to\ndescribe is constructed at the individual level and is not affected by\naggregation bias across cohorts. This is accomplished by creating a\nhigh-dimensional probabilistic model representation of the entire data set,\nwhich allows sampling from the probabilistic model in such a way that all of\nthe intrinsic correlation properties of the original data are preserved. The\nkey to this is the use of deep learning algorithms based on the Conditional\nVariational Autoencoder (CVAE) framework. From a modelling perspective, the\nconcept of a model-based resampling creates a number of opportunities in that\ndata can be organized and constructed to serve very specific needs of which the\nforming of heterogeneous pseudo panels represents one. The advantage, in that\nrespect, is the ability to trade a serious aggregation bias (when aggregating\ninto cohorts) for an unsystematic noise disturbance. Moreover, the approach\nmakes it possible to explore high-dimensional sparse preference distributions\nand their linkage to individual specific characteristics, which is not possible\nif applying traditional pseudo-panel methods. We use the presented approach to\nreveal the dynamics of transport preferences for a fixed pseudo panel of\nindividuals based on a large Danish cross-sectional data set covering the\nperiod from 2006 to 2016. The model is also utilized to classify individuals\ninto 'slow' and 'fast' movers with respect to the speed at which their\npreferences change over time. It is found that the prototypical fast mover is a\nyoung woman who lives as a single in a large city whereas the typical slow\nmover is a middle-aged man with high income from a nuclear family who lives in\na detached house outside a city.",
    "published_date": "2019-03-01T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.00516v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1903.00449v2",
    "title": "TEEvil: Identity Lease via Trusted Execution Environments",
    "authors": [
      "Ivan Puddu",
      "Daniele Lain",
      "Moritz Schneider",
      "Elizaveta Tretiakova",
      "Sinisa Matetic",
      "Srdjan Capkun"
    ],
    "author_ids": [],
    "abstract": "We investigate identity lease, a new type of service in which users lease\ntheir identities to third parties by providing them with full or restricted\naccess to their online accounts or credentials. We discuss how identity lease\ncould be abused to subvert the digital society, facilitating the spread of fake\nnews and subverting electronic voting by enabling the sale of votes. We show\nthat the emergence of Trusted Execution Environments and anonymous\ncryptocurrencies, for the first time, allows the implementation of such a lease\nservice while guaranteeing fairness, plausible deniability and anonymity,\ntherefore shielding the users and account renters from prosecution. To show\nthat such a service can be practically implemented, we build an example service\nthat we call TEEvil leveraging Intel SGX and ZCash. Finally, we discuss defense\nmechanisms and challenges in the mitigation of identity lease services.",
    "published_date": "2019-03-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.00449v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.03443v1",
    "title": "Egocentric Bias and Doubt in Cognitive Agents",
    "authors": [
      "Nanda Kishore Sreenivas",
      "Shrisha Rao"
    ],
    "author_ids": [],
    "abstract": "Modeling social interactions based on individual behavior has always been an\narea of interest, but prior literature generally presumes rational behavior.\nThus, such models may miss out on capturing the effects of biases humans are\nsusceptible to. This work presents a method to model egocentric bias, the\nreal-life tendency to emphasize one's own opinion heavily when presented with\nmultiple opinions. We use a symmetric distribution centered at an agent's own\nopinion, as opposed to the Bounded Confidence (BC) model used in prior work. We\nconsider a game of iterated interactions where an agent cooperates based on its\nopinion about an opponent. Our model also includes the concept of domain-based\nself-doubt, which varies as the interaction succeeds or not. An increase in\ndoubt makes an agent reduce its egocentricity in subsequent interactions, thus\nenabling the agent to learn reactively. The agent system is modeled with\nfactions not having a single leader, to overcome some of the issues associated\nwith leader-follower factions. We find that agents belonging to factions\nperform better than individual agents. We observe that an intermediate level of\negocentricity helps the agent perform at its best, which concurs with\nconventional wisdom that neither overconfidence nor low self-esteem brings\nbenefits.",
    "published_date": "2019-03-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.MA",
      "68T42, 91E10",
      "I.2.11; I.2.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.03443v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.11200v1",
    "title": "Equivalent Stability Notions, Lyapunov Inequality, and Its Application in Discrete-Time Linear Systems with Stochastic Dynamics Determined by an i.i.d. Process",
    "authors": [
      "Yohei Hosoe",
      "Tomomichi Hagiwara"
    ],
    "author_ids": [],
    "abstract": "This paper is concerned with stability analysis and synthesis for\ndiscrete-time linear systems with stochastic dynamics. Equivalence is first\nproved for three stability notions under some key assumptions on the randomness\nbehind the systems. In particular, we use the assumption that the stochastic\nprocess determining the system dynamics is independent and identically\ndistributed (i.i.d.) with respect to the discrete time. Then, a Lyapunov\ninequality condition is derived for stability in a necessary and sufficient\nsense. Although our Lyapunov inequality will involve decision variables\ncontained in the expectation operation, an idea is provided to solve it as a\nstandard linear matrix inequality; the idea also plays an important role in\nstate feedback synthesis based on the Lyapunov inequality. Motivating numerical\nexamples are further discussed as an application of our approach.",
    "published_date": "2019-02-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.11200v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1903.03425v2",
    "title": "The Ethics of AI Ethics -- An Evaluation of Guidelines",
    "authors": [
      "Thilo Hagendorff"
    ],
    "author_ids": [],
    "abstract": "Current advances in research, development and application of artificial\nintelligence (AI) systems have yielded a far-reaching discourse on AI ethics.\nIn consequence, a number of ethics guidelines have been released in recent\nyears. These guidelines comprise normative principles and recommendations aimed\nto harness the \"disruptive\" potentials of new AI technologies. Designed as a\ncomprehensive evaluation, this paper analyzes and compares these guidelines\nhighlighting overlaps but also omissions. As a result, I give a detailed\noverview of the field of AI ethics. Finally, I also examine to what extent the\nrespective ethical principles and values are implemented in the practice of\nresearch, development and application of AI systems - and how the effectiveness\nin the demands of AI ethics can be improved.",
    "published_date": "2019-02-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.03425v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.10974v1",
    "title": "Gaussian Process Modulated Cox Processes under Linear Inequality Constraints",
    "authors": [
      "Andrés F. López-Lopera",
      "ST John",
      "Nicolas Durrande"
    ],
    "author_ids": [],
    "abstract": "Gaussian process (GP) modulated Cox processes are widely used to model point\npatterns. Existing approaches require a mapping (link function) between the\nunconstrained GP and the positive intensity function. This commonly yields\nsolutions that do not have a closed form or that are restricted to specific\ncovariance functions. We introduce a novel finite approximation of GP-modulated\nCox processes where positiveness conditions can be imposed directly on the GP,\nwith no restrictions on the covariance function. Our approach can also ensure\nother types of inequality constraints (e.g. monotonicity, convexity), resulting\nin more versatile models that can be used for other classes of point processes\n(e.g. renewal processes). We demonstrate on both synthetic and real-world data\nthat our framework accurately infers the intensity functions. Where\nmonotonicity is a feature of the process, our ability to include this in the\ninference improves results.",
    "published_date": "2019-02-28T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.10974v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.10184v2",
    "title": "Convergence in uncertain linear systems",
    "authors": [
      "Filippo Fabiani",
      "Giuseppe Belgioioso",
      "Franco Blanchini",
      "Patrizio Colaneri",
      "Sergio Grammatico"
    ],
    "author_ids": [],
    "abstract": "State convergence is essential in several scientific areas, e.g. multi-agent\nconsensus/disagreement, distributed optimization, monotone game theory,\nmulti-agent learning over time-varying networks. This paper is the first on\nstate convergence in both continuous- and discrete-time linear systems affected\nby polytopic uncertainty. First, we characterize state convergence in linear\ntime invariant systems via equivalent necessary and sufficient conditions. In\nthe presence of uncertainty, we complement the canonical definition of (weak)\nconvergence with a stronger notion of convergence, which requires the existence\nof a common kernel among the generator matrices of the difference/differential\ninclusion (strong convergence). We investigate under which conditions the two\ndefinitions are equivalent. Then, we characterize weak and strong convergence\nby means of Lyapunov and LaSalle arguments, (linear) matrix inequalities and\nseparability of the eigenvalues of the generator matrices. We also show that,\nunlike asymptotic stability, state convergence lacks of duality.",
    "published_date": "2019-02-26T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.SY",
      "math.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.10184v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.10041v9",
    "title": "Population protocols with unreliable communication",
    "authors": [
      "Mikhail Raskin"
    ],
    "author_ids": [],
    "abstract": "Population protocols are a model of distributed computation intended for the\nstudy of networks of independent computing agents with dynamic communication\nstructure. Each agent has a finite number of states, and communication\nopportunities occur nondeterministically, allowing the agents involved to\nchange their states based on each other's states.\n  In the present paper we study unreliable models based on population protocols\nand their variations from the point of view of expressive power. We model the\neffects of message loss. We show that for a general definition of unreliable\nprotocols with constant-storage agents such protocols can only compute\npredicates computable by immediate observation population protocols (sometimes\nalso called one-way protocols). Immediate observation population protocols are\ninherently tolerant of unreliable communication and keep their expressive power\nunder a wide range of fairness conditions. We also prove that a large class of\nmessage-based models that are generally more expressive than immediate\nobservation becomes strictly less expressive than immediate observation in the\nunreliable case.",
    "published_date": "2019-02-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.10041v9",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.11162v2",
    "title": "The FAIR Funder pilot programme to make it easy for funders to require and for grantees to produce FAIR Data",
    "authors": [
      "P. Wittenburg",
      "H. Pergl Sustkova",
      "A. Montesanti",
      "S. M. Bloemers",
      "S. H. de Waard",
      "M. A. Musen",
      "J. B. Graybeal",
      "K. M. Hettne",
      "A. Jacobsen",
      "R. Pergl",
      "R. W. W. Hooft",
      "C. Staiger",
      "C. W. G. van Gelder",
      "S. L. Knijnenburg",
      "A. C. van Arkel",
      "B. Meerman",
      "M. D. Wilkinson",
      "S-A Sansone",
      "P. Rocca-Serra",
      "P. McQuilton",
      "A. N. Gonzalez-Beltran",
      "G. J. C. Aben",
      "P. Henning",
      "S. Alencar",
      "C. Ribeiro",
      "C. R. L. Silva",
      "L. Sayao",
      "L. Sales",
      "V. Veiga",
      "J. Lima",
      "S. Dib",
      "P. Xavier",
      "R. Murtinho",
      "J. Tendel",
      "B. F. Schaap",
      "P. M. Brouwer",
      "A. K. Gavai",
      "Y. Bouzembrak",
      "H. J. P. Marvin",
      "A. Mons",
      "T. Kuhn",
      "A. A. Gambardella",
      "R. de Miranda Azevedo",
      "V. Muhonen",
      "M. van der Naald",
      "N. W. Smit",
      "M. J. Buys",
      "T. F. de Bruin",
      "F. Schoots",
      "H. J. E. Goodson",
      "H. S. Rzepa",
      "K. G. Jeffery",
      "H. P. Shanahan",
      "M. Axton",
      "V. Tkachenko",
      "A. D. Maya",
      "N. K. Meyers",
      "M. Conlon",
      "L. L. Haak",
      "E. A. Schultes"
    ],
    "author_ids": [],
    "abstract": "There is a growing acknowledgement in the scientific community of the\nimportance of making experimental data machine findable, accessible,\ninteroperable, and reusable (FAIR). Recognizing that high quality metadata are\nessential to make datasets FAIR, members of the GO FAIR Initiative and the\nResearch Data Alliance (RDA) have initiated a series of workshops to encourage\nthe creation of Metadata for Machines (M4M), enabling any self-identified\nstakeholder to define and promote the reuse of standardized, comprehensive\nmachine-actionable metadata. The funders of scientific research recognize that\nthey have an important role to play in ensuring that experimental results are\nFAIR, and that high quality metadata and careful planning for FAIR data\nstewardship are central to these goals. We describe the outcome of a recent M4M\nworkshop that has led to a pilot programme involving two national science\nfunders, the Health Research Board of Ireland (HRB) and the Netherlands\nOrganisation for Health Research and Development (ZonMW). These funding\norganizations will explore new technologies to define at the time that a\nrequest for proposals is issued the minimal set of machine-actionable metadata\nthat they would like investigators to use to annotate their datasets, to enable\ninvestigators to create such metadata to help make their data FAIR, and to\ndevelop data-stewardship plans that ensure that experimental data will be\nmanaged appropriately abiding by the FAIR principles. The FAIR Funders design\nenvisions a data-management workflow having seven essential stages, where\nsolution providers are openly invited to participate. The initial pilot\nprogramme will launch using existing computer-based tools of those who attended\nthe M4M Workshop.",
    "published_date": "2019-02-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.11162v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.09581v2",
    "title": "Tile-Based Joint Caching and Delivery of $360^o$ Videos in Heterogeneous Networks",
    "authors": [
      "Pantelis Maniotis",
      "Eirina Bourtsoulatze",
      "Nikolaos Thomos"
    ],
    "author_ids": [],
    "abstract": "The recent surge of applications involving the use of $360^o$ video\nchallenges mobile networks infrastructure, as $360^o$ video files are of\nsignificant size, and current delivery and edge caching architectures are\nunable to guarantee their timely delivery. In this paper, we investigate the\nproblem of joint collaborative content-aware caching and delivery of $360^o$\nvideos in a video on demand setting. The proposed scheme takes advantage of\n$360^o$ video encoding in multiple tiles and layers to make fine-grained\ndecisions regarding which tiles to cache in each Small Base Station (SBS), and\nwhere to deliver them from to the end users, as users may reside in the\ncoverage area of multiple SBSs. This permits to cache the most popular tiles in\nthe SBSs, while the remaining tiles may be obtained through the backhaul. In\naddition, we explicitly consider the time delivery constraints to ensure\ncontinuous video playback. To reduce the computational complexity of the\noptimization problem, we simplify it by introducing a fairness constraint. This\nallows us to split the original problem into subproblems corresponding to\nGroups of Pictures (GoP). Each of the subproblems is then solved with the\nmethod of Lagrange partial relaxation. Finally, we evaluate the performance of\nthe proposed method for various system parameters and compare it with schemes\nthat do not consider $360^o$ video encoding into multiple tiles and quality\nlayers, as well as with two variants of the proposed method one that considers\nlayered encoding and SBSs collaboration and another that uses tiles encoding\nbut with no SBSs collaboration. The results showcase the benefits coming from\ncaching and delivery decisions on per tile basis and the importance of\nexploiting SBSs collaboration.",
    "published_date": "2019-02-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI",
      "cs.MM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.09581v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.09357v1",
    "title": "CFM-BD: a distributed rule induction algorithm for building Compact Fuzzy Models in Big Data classification problems",
    "authors": [
      "Mikel Elkano",
      "Jose Sanz",
      "Edurne Barrenechea",
      "Humberto Bustince",
      "Mikel Galar"
    ],
    "author_ids": [],
    "abstract": "Interpretability has always been a major concern for fuzzy rule-based\nclassifiers. The usage of human-readable models allows them to explain the\nreasoning behind their predictions and decisions. However, when it comes to Big\nData classification problems, fuzzy rule-based classifiers have not been able\nto maintain the good trade-off between accuracy and interpretability that has\ncharacterized these techniques in non-Big Data environments. The most accurate\nmethods build too complex models composed of a large number of rules and fuzzy\nsets, while those approaches focusing on interpretability do not provide\nstate-of-the-art discrimination capabilities. In this paper, we propose a new\ndistributed learning algorithm named CFM-BD to construct accurate and compact\nfuzzy rule-based classification systems for Big Data. This method has been\nspecifically designed from scratch for Big Data problems and does not adapt or\nextend any existing algorithm. The proposed learning process consists of three\nstages: 1) pre-processing based on the probability integral transform theorem;\n2) rule induction inspired by CHI-BD and Apriori algorithms; 3) rule selection\nby means of a global evolutionary optimization. We conducted a complete\nempirical study to test the performance of our approach in terms of accuracy,\ncomplexity, and runtime. The results obtained were compared and contrasted with\nfour state-of-the-art fuzzy classifiers for Big Data (FBDT, FMDT, Chi-Spark-RS,\nand CHI-BD). According to this study, CFM-BD is able to provide competitive\ndiscrimination capabilities using significantly simpler models composed of a\nfew rules of less than 3 antecedents, employing 5 linguistic labels for all\nvariables.",
    "published_date": "2019-02-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.09357v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.09355v2",
    "title": "Liability, Ethics, and Culture-Aware Behavior Specification using Rulebooks",
    "authors": [
      "Andrea Censi",
      "Konstantin Slutsky",
      "Tichakorn Wongpiromsarn",
      "Dmitry Yershov",
      "Scott Pendleton",
      "James Fu",
      "Emilio Frazzoli"
    ],
    "author_ids": [],
    "abstract": "The behavior of self-driving cars must be compatible with an enormous set of\nconflicting and ambiguous objectives, from law, from ethics, from the local\nculture, and so on. This paper describes a new way to conveniently define the\ndesired behavior for autonomous agents, which we use on the self-driving cars\ndeveloped at nuTonomy. We define a \"rulebook\" as a pre-ordered set of \"rules\",\neach akin to a violation metric on the possible outcomes (\"realizations\"). The\nrules are partially ordered by priority. The semantics of a rulebook imposes a\npre-order on the set of realizations. We study the compositional properties of\nthe rulebooks, and we derive which operations we can allow on the rulebooks to\npreserve previously-introduced constraints. While we demonstrate the\napplication of these techniques in the self-driving domain, the methods are\ndomain-independent.",
    "published_date": "2019-02-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.09355v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08893v1",
    "title": "Critical Clearing Time Sensitivity for Inequality Constrained Systems",
    "authors": [
      "Chetan Mishra",
      "Anamitra Pal",
      "Virgilio A. Centeno"
    ],
    "author_ids": [],
    "abstract": "From a stability perspective, a renewable generation (RG)-rich power system\nis a constrained system. As the quasistability boundary of a constrained system\nis structurally very different from that of an unconstrained system, finding\nthe sensitivity of critical clearing time (CCT) to change in system parameters\nis very beneficial for a constrained power system, especially for\nplanning/revising constraints arising from system protection settings. In this\npaper, we derive the first order sensitivity of a constrained power system\nusing trajectory sensitivities of fault-on and post-fault trajectories. The\nresults for the test system demonstrate the dependence between ability to meet\nangle and frequency constraints, and change in power system parameters such as\noperating conditions and inertia.",
    "published_date": "2019-02-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY",
      "math.DS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08893v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.08670v3",
    "title": "Discriminative Pattern Mining for Breast Cancer Histopathology Image Classification via Fully Convolutional Autoencoder",
    "authors": [
      "Xingyu Li",
      "Marko Radulovic",
      "Ksenija Kanjer",
      "Konstantinos N. Plataniotis"
    ],
    "author_ids": [],
    "abstract": "Accurate diagnosis of breast cancer in histopathology images is challenging\ndue to the heterogeneity of cancer cell growth as well as of a variety of\nbenign breast tissue proliferative lesions. In this paper, we propose a\npractical and self-interpretable invasive cancer diagnosis solution. With\nminimum annotation information, the proposed method mines contrast patterns\nbetween normal and malignant images in unsupervised manner and generates a\nprobability map of abnormalities to verify its reasoning. Particularly, a fully\nconvolutional autoencoder is used to learn the dominant structural patterns\namong normal image patches. Patches that do not share the characteristics of\nthis normal population are detected and analyzed by one-class support vector\nmachine and 1-layer neural network. We apply the proposed method to a public\nbreast cancer image set. Our results, in consultation with a senior\npathologist, demonstrate that the proposed method outperforms existing methods.\nThe obtained probability map could benefit the pathology practice by providing\nvisualized verification data and potentially leads to a better understanding of\ndata-driven diagnosis solutions.",
    "published_date": "2019-02-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "q-bio.QM"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08670v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08628v1",
    "title": "Trajectories of Blocked Community Members: Redemption, Recidivism and Departure",
    "authors": [
      "Jonathan P. Chang",
      "Cristian Danescu-Niculescu-Mizil"
    ],
    "author_ids": [],
    "abstract": "Community norm violations can impair constructive communication and\ncollaboration online. As a defense mechanism, community moderators often\naddress such transgressions by temporarily blocking the perpetrator. Such\nactions, however, come with the cost of potentially alienating community\nmembers. Given this tradeoff, it is essential to understand to what extent, and\nin which situations, this common moderation practice is effective in\nreinforcing community rules.\n  In this work, we introduce a computational framework for studying the future\nbehavior of blocked users on Wikipedia. After their block expires, they can\ntake several distinct paths: they can reform and adhere to the rules, but they\ncan also recidivate, or straight-out abandon the community. We reveal that\nthese trajectories are tied to factors rooted both in the characteristics of\nthe blocked individual and in whether they perceived the block to be fair and\njustified. Based on these insights, we formulate a series of prediction tasks\naiming to determine which of these paths a user is likely to take after being\nblocked for their first offense, and demonstrate the feasibility of these new\ntasks. Overall, this work builds towards a more nuanced approach to moderation\nby highlighting the tradeoffs that are in play.",
    "published_date": "2019-02-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.SI",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08628v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08472v1",
    "title": "Model-based clustering in very high dimensions via adaptive projections",
    "authors": [
      "Bernd Taschler",
      "Frank Dondelinger",
      "Sach Mukherjee"
    ],
    "author_ids": [],
    "abstract": "Mixture models are a standard approach to dealing with heterogeneous data\nwith non-i.i.d. structure. However, when the dimension $p$ is large relative to\nsample size $n$ and where either or both of means and covariances/graphical\nmodels may differ between the latent groups, mixture models face statistical\nand computational difficulties and currently available methods cannot\nrealistically go beyond $p \\! \\sim \\! 10^4$ or so. We propose an approach\ncalled Model-based Clustering via Adaptive Projections (MCAP). Instead of\nestimating mixtures in the original space, we work with a low-dimensional\nrepresentation obtained by linear projection. The projection dimension itself\nplays an important role and governs a type of bias-variance tradeoff with\nrespect to recovery of the relevant signals. MCAP sets the projection dimension\nautomatically in a data-adaptive manner, using a proxy for the assignment risk.\nCombining a full covariance formulation with the adaptive projection allows\ndetection of both mean and covariance signals in very high dimensional\nproblems. We show real-data examples in which covariance signals are reliably\ndetected in problems with $p \\! \\sim \\! 10^4$ or more, and simulations going up\nto $p = 10^6$. In some examples, MCAP performs well even when the mean signal\nis entirely removed, leaving differential covariance structure in the\nhigh-dimensional space as the only signal. Across a number of regimes, MCAP\nperforms as well or better than a range of existing methods, including a\nrecently-proposed $\\ell_1$-penalized approach; and performance remains broadly\nstable with increasing dimension. MCAP can be run \"out of the box\" and is fast\nenough for interactive use on large-$p$ problems using standard desktop\ncomputing resources.",
    "published_date": "2019-02-22T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08472v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08297v3",
    "title": "Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods",
    "authors": [
      "Maher Nouiehed",
      "Maziar Sanjabi",
      "Tianjian Huang",
      "Jason D. Lee",
      "Meisam Razaviyayn"
    ],
    "author_ids": [],
    "abstract": "Recent applications that arise in machine learning have surged significant\ninterest in solving min-max saddle point games. This problem has been\nextensively studied in the convex-concave regime for which a global equilibrium\nsolution can be computed efficiently. In this paper, we study the problem in\nthe non-convex regime and show that an \\varepsilon--first order stationary\npoint of the game can be computed when one of the player's objective can be\noptimized to global optimality efficiently. In particular, we first consider\nthe case where the objective of one of the players satisfies the\nPolyak-{\\L}ojasiewicz (PL) condition. For such a game, we show that a simple\nmulti-step gradient descent-ascent algorithm finds an \\varepsilon--first order\nstationary point of the problem in \\widetilde{\\mathcal{O}}(\\varepsilon^{-2})\niterations. Then we show that our framework can also be applied to the case\nwhere the objective of the \"max-player\" is concave. In this case, we propose a\nmulti-step gradient descent-ascent algorithm that finds an \\varepsilon--first\norder stationary point of the game in \\widetilde{\\cal O}(\\varepsilon^{-3.5})\niterations, which is the best known rate in the literature. We applied our\nalgorithm to a fair classification problem of Fashion-MNIST dataset and\nobserved that the proposed algorithm results in smoother training and better\ngeneralization.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08297v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08288v1",
    "title": "L2 Observers for a Class of Nonlinear Systems with Unknown Inputs",
    "authors": [
      "Martin Corless",
      "Ankush Chakrabarty"
    ],
    "author_ids": [],
    "abstract": "We consider the problem of estimating the state and unknown input for a large\nclass of nonlinear systems subject to unknown exogenous inputs. The exogenous\ninputs themselves are modeled as being generated by a nonlinear system subject\nto unknown inputs. The nonlinearities considered in this work are characterized\nby multiplier matrices that include many commonly encountered nonlinearities.\nWe obtain a linear matrix inequality (LMI), that, if feasible, provides the\ngains for an observer which results in certified L2 performance of the error\ndynamics associated with the observer. We also present conditions which\nguarantee that the L2 norm of the error can be made arbitrarily small and\ninvestigate conditions for feasibility of the proposed LMIs.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SY",
      "math.DS",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08288v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.08283v5",
    "title": "Capuchin: Causal Database Repair for Algorithmic Fairness",
    "authors": [
      "Babak Salimi",
      "Luke Rodriguez",
      "Bill Howe",
      "Dan Suciu"
    ],
    "author_ids": [],
    "abstract": "Fairness is increasingly recognized as a critical component of machine\nlearning systems. However, it is the underlying data on which these systems are\ntrained that often reflect discrimination, suggesting a database repair\nproblem. Existing treatments of fairness rely on statistical correlations that\ncan be fooled by statistical anomalies, such as Simpson's paradox. Proposals\nfor causality-based definitions of fairness can correctly model some of these\nsituations, but they require specification of the underlying causal models. In\nthis paper, we formalize the situation as a database repair problem, proving\nsufficient conditions for fair classifiers in terms of admissible variables as\nopposed to a complete causal model. We show that these conditions correctly\ncapture subtle fairness violations. We then use these conditions as the basis\nfor database repair algorithms that provide provable fairness guarantees about\nclassifiers trained on their training labels. We evaluate our algorithms on\nreal data, demonstrating improvement over the state of the art on multiple\nfairness metrics proposed in the literature while retaining high utility.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08283v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.11097v1",
    "title": "Predictive Inequity in Object Detection",
    "authors": [
      "Benjamin Wilson",
      "Judy Hoffman",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "In this work, we investigate whether state-of-the-art object detection\nsystems have equitable predictive performance on pedestrians with different\nskin tones. This work is motivated by many recent examples of ML and vision\nsystems displaying higher error rates for certain demographic groups than\nothers. We annotate an existing large scale dataset which contains pedestrians,\nBDD100K, with Fitzpatrick skin tones in ranges [1-3] or [4-6]. We then provide\nan in-depth comparative analysis of performance between these two skin tone\ngroupings, finding that neither time of day nor occlusion explain this\nbehavior, suggesting this disparity is not merely the result of pedestrians in\nthe 4-6 range appearing in more difficult scenes for detection. We investigate\nto what extent time of day, occlusion, and reweighting the supervised loss\nduring training affect this predictive bias.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.11097v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08236v1",
    "title": "Lung Cancer Detection using Co-learning from Chest CT Images and Clinical Demographics",
    "authors": [
      "Jiachen Wang",
      "Riqiang Gao",
      "Yuankai Huo",
      "Shunxing Bao",
      "Yunxi Xiong",
      "Sanja L. Antic",
      "Travis J. Osterman",
      "Pierre P. Massion",
      "Bennett A. Landman"
    ],
    "author_ids": [],
    "abstract": "Early detection of lung cancer is essential in reducing mortality. Recent\nstudies have demonstrated the clinical utility of low-dose computed tomography\n(CT) to detect lung cancer among individuals selected based on very limited\nclinical information. However, this strategy yields high false positive rates,\nwhich can lead to unnecessary and potentially harmful procedures. To address\nsuch challenges, we established a pipeline that co-learns from detailed\nclinical demographics and 3D CT images. Toward this end, we leveraged data from\nthe Consortium for Molecular and Cellular Characterization of Screen-Detected\nLesions (MCL), which focuses on early detection of lung cancer. A 3D\nattention-based deep convolutional neural net (DCNN) is proposed to identify\nlung cancer from the chest CT scan without prior anatomical location of the\nsuspicious nodule. To improve upon the non-invasive discrimination between\nbenign and malignant, we applied a random forest classifier to a dataset\nintegrating clinical information to imaging data. The results show that the AUC\nobtained from clinical demographics alone was 0.635 while the attention network\nalone reached an accuracy of 0.687. In contrast when applying our proposed\npipeline integrating clinical and imaging variables, we reached an AUC of 0.787\non the testing dataset. The proposed network both efficiently captures\nanatomical information for classification and also generates attention maps\nthat explain the features that drive performance.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08236v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08068v1",
    "title": "Towards Reliable, Automated General Movement Assessment for Perinatal Stroke Screening in Infants Using Wearable Accelerometers",
    "authors": [
      "Yan Gao",
      "Yang Long",
      "Yu Guan",
      "Anna Basu",
      "Jessica Baggaley",
      "Thomas Ploetz"
    ],
    "author_ids": [],
    "abstract": "Perinatal stroke (PS) is a serious condition that, if undetected and thus\nuntreated, often leads to life-long disability, in particular Cerebral Palsy\n(CP). In clinical settings, Prechtl's General Movement Assessment (GMA) can be\nused to classify infant movements using a Gestalt approach, identifying infants\nat high risk of developing PS. Training and maintenance of assessment skills\nare essential and expensive for the correct use of GMA, yet many practitioners\nlack these skills, preventing larger-scale screening and leading to significant\nrisks of missing opportunities for early detection and intervention for\naffected infants. We present an automated approach to GMA, based on body-worn\naccelerometers and a novel sensor data analysis method-Discriminative Pattern\nDiscovery (DPD)-that is designed to cope with scenarios where only coarse\nannotations of data are available for model training. We demonstrate the\neffectiveness of our approach in a study with 34 newborns (21 typically\ndeveloping infants and 13 PS infants with abnormal movements). Our method is\nable to correctly recognise the trials with abnormal movements with at least\nthe accuracy that is required by newly trained human annotators (75%), which is\nencouraging towards our ultimate goal of an automated PS screening system that\ncan be used population-wide.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08068v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.08003v1",
    "title": "Minimal Envy and Popular Matchings",
    "authors": [
      "Aleksei Y. Kondratev",
      "Alexander S. Nesterov"
    ],
    "author_ids": [],
    "abstract": "We study ex-post fairness in the object allocation problem where objects are\nvaluable and commonly owned. A matching is fair from individual perspective if\nit has only inevitable envy towards agents who received most preferred objects\n-- minimal envy matching. A matching is fair from social perspective if it is\nsupported by majority against any other matching -- popular matching.\nSurprisingly, the two perspectives give the same outcome: when a popular\nmatching exists it is equivalent to a minimal envy matching. We show the\nequivalence between global and local popularity: a matching is popular if and\nonly if there does not exist a group of size up to 3 agents that decides to\nexchange their objects by majority, keeping the remaining matching fixed. We\nalgorithmically show that an arbitrary matching is path-connected to a popular\nmatching where along the path groups of up to 3 agents exchange their objects\nby majority. A market where random groups exchange objects by majority\nconverges to a popular matching given such matching exists. When popular\nmatching might not exist we define most popular matching as a matching that is\npopular among the largest subset of agents. We show that each minimal envy\nmatching is a most popular matching and propose a polynomial-time algorithm to\nfind them.",
    "published_date": "2019-02-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "91B68"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.08003v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.07653v1",
    "title": "On the effect of age perception biases for real age regression",
    "authors": [
      "Julio C. S. Jacques Junior",
      "Cagri Ozcinar",
      "Marina Marjanovic",
      "Xavier Baró",
      "Gholamreza Anbarjafari",
      "Sergio Escalera"
    ],
    "author_ids": [],
    "abstract": "Automatic age estimation from facial images represents an important task in\ncomputer vision. This paper analyses the effect of gender, age, ethnic, makeup\nand expression attributes of faces as sources of bias to improve deep apparent\nage prediction. Following recent works where it is shown that apparent age\nlabels benefit real age estimation, rather than direct real to real age\nregression, our main contribution is the integration, in an end-to-end\narchitecture, of face attributes for apparent age prediction with an additional\nloss for real age regression. Experimental results on the APPA-REAL dataset\nindicate the proposed network successfully take advantage of the adopted\nattributes to improve both apparent and real age estimation. Our model\noutperformed a state-of-the-art architecture proposed to separately address\napparent and real age regression. Finally, we present preliminary results and\ndiscussion of a proof of concept application using the proposed model to\nregress the apparent age of an individual based on the gender of an external\nobserver.",
    "published_date": "2019-02-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.07653v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.07349v2",
    "title": "Solutions sets to systems of equations in hyperbolic groups are EDT0L in PSPACE",
    "authors": [
      "Laura Ciobanu",
      "Murray Elder"
    ],
    "author_ids": [],
    "abstract": "We show that the full set of solutions to systems of equations and\ninequations in a hyperbolic group, with or without torsion, as shortlex\ngeodesic words, is an EDT0L language whose specification can be computed in\n$\\mathsf{NSPACE}(n^2\\log n)$ for the torsion-free case and\n$\\mathsf{NSPACE}(n^4\\log n)$ in the torsion case. Our work combines deep\ngeometric results by Rips, Sela, Dahmani and Guirardel on decidability of\nexistential theories of hyperbolic groups, work of computer scientists\nincluding Plandowski, Je\\.z, Diekert and others on $\\mathsf{PSPACE}$ algorithms\nto solve equations in free monoids and groups using compression, and an\nintricate language-theoretic analysis. The present work gives an essentially\noptimal formal language description for all solutions in all hyperbolic groups,\nand an explicit and surprising low space complexity to compute them.",
    "published_date": "2019-02-19T00:00:00",
    "year": 2019,
    "categories": [
      "math.GR",
      "cs.CC",
      "cs.FL",
      "03D05, 20F65, 20F70, 68Q25, 68Q45"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.07349v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.07283v1",
    "title": "Graph Spectral Characterization of Brain Cortical Morphology",
    "authors": [
      "Sevil Maghsadhagh",
      "Anders Eklund",
      "Hamid Behjat"
    ],
    "author_ids": [],
    "abstract": "The human brain cortical layer has a convoluted morphology that is unique to\neach individual. Characterization of the cortical morphology is necessary in\nlongitudinal studies of structural brain change, as well as in discriminating\nindividuals in health and disease. A method for encoding the cortical\nmorphology in the form of a graph is presented. The design of graphs that\nencode the global cerebral hemisphere cortices as well as localized cortical\nregions is proposed. Spectral metrics derived from these graphs are then\nstudied and proposed as descriptors of cortical morphology. As proof-of-concept\nof their applicability in characterizing cortical morphology, the metrics are\nstudied in the context of hemispheric asymmetry as well as gender dependent\ndiscrimination of cortical morphology.",
    "published_date": "2019-02-19T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.NC",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.07283v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.07133v1",
    "title": "Estimating Network Effects Using Naturally Occurring Peer Notification Queue Counterfactuals",
    "authors": [
      "Craig Tutterow",
      "Guillaume Saint-Jacques"
    ],
    "author_ids": [],
    "abstract": "Randomized experiments, or A/B tests are used to estimate the causal impact\nof a feature on the behavior of users by creating two parallel universes in\nwhich members are simultaneously assigned to treatment and control. However, in\nsocial network settings, members interact, such that the impact of a feature is\nnot always contained within the treatment group. Researchers have developed a\nnumber of experimental designs to estimate network effects in social settings.\nAlternatively, naturally occurring exogenous variation, or 'natural\nexperiments,' allow researchers to recover causal estimates of peer effects\nfrom observational data in the absence of experimental manipulation. Natural\nexperiments trade off the engineering costs and some of the ethical concerns\nassociated with network randomization with the search costs of finding\nsituations with natural exogenous variation. To mitigate the search costs\nassociated with discovering natural counterfactuals, we identify a common\nengineering requirement used to scale massive online systems, in which natural\nexogenous variation is likely to exist: notification queueing. We identify two\nnatural experiments on the LinkedIn platform based on the order of notification\nqueues to estimate the causal impact of a received message on the engagement of\na recipient. We show that receiving a message from another member significantly\nincreases a member's engagement, but that some popular observational\nspecifications, such as fixed-effects estimators, overestimate this effect by\nas much as 2.7x. We then apply the estimated network effect coefficients to a\nlarge body of past experiments to quantify the extent to which it changes our\ninterpretation of experimental results. The study points to the benefits of\nusing messaging queues to discover naturally occurring counterfactuals for the\nestimation of causal effects without experimenter intervention.",
    "published_date": "2019-02-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CY",
      "econ.EM",
      "stat.AP",
      "91D30, 91G70, 62P20, 62P25",
      "J.4; G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.07133v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.06888v1",
    "title": "Probabilistic Modeling with Matrix Product States",
    "authors": [
      "James Stokes",
      "John Terilla"
    ],
    "author_ids": [],
    "abstract": "Inspired by the possibility that generative models based on quantum circuits\ncan provide a useful inductive bias for sequence modeling tasks, we propose an\nefficient training algorithm for a subset of classically simulable quantum\ncircuit models. The gradient-free algorithm, presented as a sequence of exactly\nsolvable effective models, is a modification of the density matrix\nrenormalization group procedure adapted for learning a probability\ndistribution. The conclusion that circuit-based models offer a useful inductive\nbias for classical datasets is supported by experimental results on the parity\nlearning problem.",
    "published_date": "2019-02-19T00:00:00",
    "year": 2019,
    "categories": [
      "quant-ph",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.06888v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.06671v1",
    "title": "An Exploration of User and Bystander Attitudes About Mobile Live-Streaming Video",
    "authors": [
      "Cori Faklaris",
      "Asa Blevins",
      "Matthew O'Haver",
      "Neha Singhal",
      "Francesco Cafaro"
    ],
    "author_ids": [],
    "abstract": "Thanks to mobile apps such as Periscope and Facebook Live, live-streaming\nvideo is having a moment again. It has not been clear, however, to what extent\nthe current ubiquity of smartphones is impacting this technology's acceptance\nin everyday social situations and how mobile contexts or affordances will\naffect and be affected by shifts in social norms and policy debates regarding\nprivacy, surveillance and intellectual property. This ethnographic-style\nresearch explores familiarity with and attitudes about mobile live-streaming\nvideo and related legal and ethical issues among a sample of \"Middle America\"\nparticipants at two typical outdoor social events: sports tailgating and a\nrooftop party. In situ observations of n=110 bystanders to the use of a\nsmartphone, including interviews with n=20, revealed that many are not fully\naware of when their image or speech is being live-streamed in a casual context\nand want stronger notifications of and ability to consent to such broadcasting.",
    "published_date": "2019-02-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "K.4.0"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.06671v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.06286v1",
    "title": "Semiparametric correction for endogenous truncation bias with Vox Populi based participation decision",
    "authors": [
      "Nir Billfeld",
      "Moshe Kim"
    ],
    "author_ids": [],
    "abstract": "We synthesize the knowledge present in various scientific disciplines for the\ndevelopment of semiparametric endogenous truncation-proof algorithm, correcting\nfor truncation bias due to endogenous self-selection. This synthesis enriches\nthe algorithm's accuracy, efficiency and applicability. Improving upon the\ncovariate shift assumption, data are intrinsically affected and largely\ngenerated by their own behavior (cognition). Refining the concept of Vox Populi\n(Wisdom of Crowd) allows data points to sort themselves out depending on their\nestimated latent reference group opinion space. Monte Carlo simulations, based\non 2,000,000 different distribution functions, practically generating 100\nmillion realizations, attest to a very high accuracy of our model.",
    "published_date": "2019-02-17T00:00:00",
    "year": 2019,
    "categories": [
      "econ.EM",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.06286v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.06243v1",
    "title": "Prophet inequality for bipartite matching: merits of being simple and non adaptive",
    "authors": [
      "Nick Gravin",
      "Hongao Wang"
    ],
    "author_ids": [],
    "abstract": "We consider Bayesian online selection problem of a matching in bipartite\ngraphs, i.e., online weighted matching problem with edge arrivals where online\nalgorithm knows distributions of weights, that corresponds to the intersection\nof two matroids in [Kleinberg and Wienberg STOC 12] model. We consider a simple\nclass of non adaptive vertex-additive policies that assign static prices to all\nvertices in the graph and accept each edge only if its weight exceeds the sum\nof the prices of the edge's endpoints. We show existence of a vertex-additive\npolicy with the expected payoff of at least one third of the prophet's payoff\nand present gradient decent type algorithm that quickly converges to the\ndesired vector of vertex prices. This improves the adaptive online policy of\n[Kleinberg and Wienberg STOC 12] for the intersection of two matroids in two\nways: our policy is non adaptive and has better approximation guarantee of $3$\ninstead of previous guarantee of $5.82$ against the prophet. We give a\ncomplementary lower bound of $2.25$ for any online algorithm in the bipartite\nmatching setting.",
    "published_date": "2019-02-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.DM",
      "cs.DS",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.06243v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.05826v2",
    "title": "The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the xAUC Metric",
    "authors": [
      "Nathan Kallus",
      "Angela Zhou"
    ],
    "author_ids": [],
    "abstract": "Where machine-learned predictive risk scores inform high-stakes decisions,\nsuch as bail and sentencing in criminal justice, fairness has been a serious\nconcern. Recent work has characterized the disparate impact that such risk\nscores can have when used for a binary classification task. This may not\naccount, however, for the more diverse downstream uses of risk scores and their\nnon-binary nature. To better account for this, in this paper, we investigate\nthe fairness of predictive risk scores from the point of view of a bipartite\nranking task, where one seeks to rank positive examples higher than negative\nones. We introduce the xAUC disparity as a metric to assess the disparate\nimpact of risk scores and define it as the difference in the probabilities of\nranking a random positive example from one protected group above a negative one\nfrom another group and vice versa. We provide a decomposition of bipartite\nranking loss into components that involve the discrepancy and components that\ninvolve pure predictive ability within each group. We use xAUC analysis to\naudit predictive risk scores for recidivism prediction, income prediction, and\ncardiac arrest prediction, where it describes disparities that are not evident\nfrom simply comparing within-group predictive performance.",
    "published_date": "2019-02-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.05826v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.05640v1",
    "title": "Achieving Large Sum Rate and Good Fairness in MISO Broadcast Communication",
    "authors": [
      "Ji-You Huang",
      "Hsiao-feng Francis Lu"
    ],
    "author_ids": [],
    "abstract": "A tradeoff between sum rate and fairness for MISO broadcast communication\nemploying dirty paper coding or zero-forcing dirty paper coding at physical\nlayer is investigated in this paper. The tradeoff is based on a new design\nobjective termed \"tri-stage\" approach as well as a new L1-based fairness\nmeasure that is much more robust than the well-known Jain's index for comparing\nfairness levels achieved by various design objectives at a much finer\nresolution in high SNR regime. The newly proposed tri-stage design also\nintroduces a new concept of statistical power allocation that randomly\nallocates powers to users based on an optimal probability distribution derived\nfrom the tradeoff between sum rate and fairness. Simulation results show that\nthe proposed approach can simultaneously achieve a larger sum rate and better\nfairness than the reputable proportional fairness criterion. A performance\nupper bound is also given in the paper to show that the excellent performance\nof the proposed approach at moderate and high SNR regimes as well as some\npotential for further improvement in low SNR regime.",
    "published_date": "2019-02-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.05640v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.05361v1",
    "title": "Crowd Work on a CV? Understanding How AMT Fits into Turkers' Career Goals and Professional Profiles",
    "authors": [
      "Anna Kasunic",
      "Chun-Wei Chiang",
      "Geoff Kaufman",
      "Saiph Savage"
    ],
    "author_ids": [],
    "abstract": "In 2013, scholars laid out a framework for a sustainable, ethical future of\ncrowd work, recommending career ladders so that crowd work can lead to career\nadvancement and more economic mobility. Five years later, we consider this\nvision in the context of Amazon Mechanical Turk (AMT). To understand how\nworkers currently view their experiences on AMT, and how they publicly present\nand share these experiences in their professional lives, we conducted a survey\nstudy with workers on AMT (n=98). The survey we administered included a\ncombination of multiple choice, binary, and open-ended (short paragraph) items\ngauging Turkers' perceptions of their experiences on AMT within the context of\ntheir broader work experience and career goals. This work extends existing\nunderstandings of who crowd workers are and why they crowd work by seeking to\nbetter understand how crowd work factors into Turkers' professional profiles,\nand how we can subsequently better support crowd workers in their career\nadvancement. Our survey results can inform the design of better tools to\nempower crowd workers in their professional development both inside and outside\nof AMT.",
    "published_date": "2019-02-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.05361v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.04783v4",
    "title": "Mathematical Notions vs. Human Perception of Fairness: A Descriptive Approach to Fairness for Machine Learning",
    "authors": [
      "Megha Srivastava",
      "Hoda Heidari",
      "Andreas Krause"
    ],
    "author_ids": [],
    "abstract": "Fairness for Machine Learning has received considerable attention, recently.\nVarious mathematical formulations of fairness have been proposed, and it has\nbeen shown that it is impossible to satisfy all of them simultaneously. The\nliterature so far has dealt with these impossibility results by quantifying the\ntradeoffs between different formulations of fairness. Our work takes a\ndifferent perspective on this issue. Rather than requiring all notions of\nfairness to (partially) hold at the same time, we ask which one of them is the\nmost appropriate given the societal domain in which the decision-making model\nis to be deployed. We take a descriptive approach and set out to identify the\nnotion of fairness that best captures \\emph{lay people's perception of\nfairness}. We run adaptive experiments designed to pinpoint the most compatible\nnotion of fairness with each participant's choices through a small number of\ntests. Perhaps surprisingly, we find that the most simplistic mathematical\ndefinition of fairness---namely, demographic parity---most closely matches\npeople's idea of fairness in two distinct application scenarios. This\nconclusion remains intact even when we explicitly tell the participants about\nthe alternative, more complicated definitions of fairness, and we reduce the\ncognitive burden of evaluating those notions for them. Our findings have\nimportant implications for the Fair ML literature and the discourse on\nformalizing algorithmic fairness.",
    "published_date": "2019-02-13T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04783v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.04707v2",
    "title": "Sampling networks by nodal attributes",
    "authors": [
      "Yohsuke Murase",
      "Hang-Hyun Jo",
      "János Török",
      "János Kertész",
      "Kimmo Kaski"
    ],
    "author_ids": [],
    "abstract": "In a social network individuals or nodes connect to other nodes by choosing\none of the channels of communication at a time to re-establish the existing\nsocial links. Since available data sets are usually restricted to a limited\nnumber of channels or layers, these autonomous decision making processes by the\nnodes constitute the sampling of a multiplex network leading to just one\n(though very important) example of sampling bias caused by the behavior of the\nnodes. We develop a general setting to get insight and understand the class of\nnetwork sampling models, where the probability of sampling a link in the\noriginal network depends on the attributes $h$ of its adjacent nodes. Assuming\nthat the nodal attributes are independently drawn from an arbitrary\ndistribution $\\rho(h)$ and that the sampling probability $r(h_i , h_j)$ for a\nlink $ij$ of nodal attributes $h_i$ and $h_j$ is also arbitrary, we derive\nexact analytic expressions of the sampled network for such network\ncharacteristics as the degree distribution, degree correlation, and clustering\nspectrum. The properties of the sampled network turn out to be sums of\nquantities for the original network topology weighted by the factors stemming\nfrom the sampling. Based on our analysis, we find that the sampled network may\nhave sampling-induced network properties that are absent in the original\nnetwork, which implies the potential risk of a naive generalization of the\nresults of the sample to the entire original network. We also consider the\ncase, when neighboring nodes have correlated attributes to show how to\ngeneralize our formalism for such sampling bias and we get good agreement\nbetween the analytic results and the numerical simulations.",
    "published_date": "2019-02-13T00:00:00",
    "year": 2019,
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04707v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.05064v1",
    "title": "PLIT: An alignment-free computational tool for identification of long non-coding RNAs in plant transcriptomic datasets",
    "authors": [
      "S. Deshpande",
      "J. Shuttleworth",
      "J. Yang",
      "S. Taramonli",
      "M. England"
    ],
    "author_ids": [],
    "abstract": "Long non-coding RNAs (lncRNAs) are a class of non-coding RNAs which play a\nsignificant role in several biological processes. RNA-seq based transcriptome\nsequencing has been extensively used for identification of lncRNAs. However,\naccurate identification of lncRNAs in RNA-seq datasets is crucial for exploring\ntheir characteristic functions in the genome as most coding potential\ncomputation (CPC) tools fail to accurately identify them in transcriptomic\ndata. Well-known CPC tools such as CPC2, lncScore, CPAT are primarily designed\nfor prediction of lncRNAs based on the GENCODE, NONCODE and CANTATAdb\ndatabases. The prediction accuracy of these tools often drops when tested on\ntranscriptomic datasets. This leads to higher false positive results and\ninaccuracy in the function annotation process. In this study, we present a\nnovel tool, PLIT, for the identification of lncRNAs in plants RNA-seq datasets.\nPLIT implements a feature selection method based on L1 regularization and\niterative Random Forests (iRF) classification for selection of optimal\nfeatures. Based on sequence and codon-bias features, it classifies the RNA-seq\nderived FASTA sequences into coding or long non-coding transcripts. Using L1\nregularization, 31 optimal features were obtained based on lncRNA and\nprotein-coding transcripts from 8 plant species. The performance of the tool\nwas evaluated on 7 plant RNA-seq datasets using 10-fold cross-validation. The\nanalysis exhibited superior accuracy when evaluated against currently available\nstate-of-the-art CPC tools.",
    "published_date": "2019-02-12T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.GN",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.05064v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.04607v1",
    "title": "Inequalities and Approximations for Fisher Information in the Presence of Nuisance Parameters",
    "authors": [
      "Eric Clarkson"
    ],
    "author_ids": [],
    "abstract": "Many imaging systems are used to estimate a vector of parameters associated\nwith the object being imaged. In many cases there are other parameters in the\nmodel for the imaging data that are not of interest for the task at hand. We\nrefer to these as nuisance parameters and use them to form the components of\nthe nuisance parameter vector. If we have a prior probability distribution\nfunction (PDF) for the nuisance parameter vector, then we may mariginalize over\nthe nuisance parameters to produce a conditional PDF for the data that only\ndepends on the parameters of interest. We will examine this approach to develop\ninequalities and approximations for the FIM when the data is affected by\nnuisance parameters.",
    "published_date": "2019-02-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04607v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.04376v3",
    "title": "An adaptive stochastic optimization algorithm for resource allocation",
    "authors": [
      "Xavier Fontaine",
      "Shie Mannor",
      "Vianney Perchet"
    ],
    "author_ids": [],
    "abstract": "We consider the classical problem of sequential resource allocation where a\ndecision maker must repeatedly divide a budget between several resources, each\nwith diminishing returns. This can be recast as a specific stochastic\noptimization problem where the objective is to maximize the cumulative reward,\nor equivalently to minimize the regret. We construct an algorithm that is {\\em\nadaptive} to the complexity of the problem, expressed in term of the regularity\nof the returns of the resources, measured by the exponent in the {\\L}ojasiewicz\ninequality (or by their universal concavity parameter). Our\nparameter-independent algorithm recovers the optimal rates for strongly-concave\nfunctions and the classical fast rates of multi-armed bandit (for linear reward\nfunctions). Moreover, the algorithm improves existing results on stochastic\noptimization in this regret minimization setting for intermediate cases.",
    "published_date": "2019-02-12T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04376v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.04235v1",
    "title": "Effects of empathy on the evolution of fairness in group-structured populations",
    "authors": [
      "Yanling Zhang",
      "Jian Liu",
      "Aming Li"
    ],
    "author_ids": [],
    "abstract": "The ultimatum game has been a prominent paradigm in studying the evolution of\nfairness. It predicts that responders should accept any nonzero offer and\nproposers should offer the smallest possible amount according to orthodox game\ntheory. However, the prediction strongly contradicts with experimental findings\nwhere responders usually reject low offers below $20\\%$ and proposers usually\nmake higher offers than expected. To explain the evolution of such fair\nbehaviors, we here introduce empathy in group-structured populations by\nallowing a proportion $\\alpha$ of the population to play empathetic strategies.\nInterestingly, we find that for high mutation probabilities, the mean offer\ndecreases with $\\alpha$ and the mean demand increases, implying empathy\ninhibits the evolution of fairness. For low mutation probabilities, the mean\noffer and demand approach to the fair ones with increasing $\\alpha$, implying\nempathy promotes the evolution of fairness. Furthermore, under both weak and\nstrong intensities of natural selection, we analytically calculate the mean\noffer and demand for different levels of $\\alpha$. Counterintuitively, we\ndemonstrate that although a higher mutation probability leads to a higher level\nof fairness under weak selection, an intermediate mutation probability\ncorresponds to the lowest level of fairness under strong selection. Our study\nprovides systematic insights into the evolutionary origin of fairness in\ngroup-structured populations with empathetic strategies.",
    "published_date": "2019-02-12T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "physics.soc-ph",
      "q-bio.PE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04235v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.04056v2",
    "title": "Policy Learning for Fairness in Ranking",
    "authors": [
      "Ashudeep Singh",
      "Thorsten Joachims"
    ],
    "author_ids": [],
    "abstract": "Conventional Learning-to-Rank (LTR) methods optimize the utility of the\nrankings to the users, but they are oblivious to their impact on the ranked\nitems. However, there has been a growing understanding that the latter is\nimportant to consider for a wide range of ranking applications (e.g. online\nmarketplaces, job placement, admissions). To address this need, we propose a\ngeneral LTR framework that can optimize a wide range of utility metrics (e.g.\nNDCG) while satisfying fairness of exposure constraints with respect to the\nitems. This framework expands the class of learnable ranking functions to\nstochastic ranking policies, which provides a language for rigorously\nexpressing fairness specifications. Furthermore, we provide a new LTR algorithm\ncalled Fair-PG-Rank for directly searching the space of fair ranking policies\nvia a policy-gradient approach. Beyond the theoretical evidence in deriving the\nframework and the algorithm, we provide empirical results on simulated and\nreal-world datasets verifying the effectiveness of the approach in individual\nand group-fairness settings.",
    "published_date": "2019-02-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04056v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03731v1",
    "title": "Discrimination in the Age of Algorithms",
    "authors": [
      "Jon Kleinberg",
      "Jens Ludwig",
      "Sendhil Mullainathan",
      "Cass R. Sunstein"
    ],
    "author_ids": [],
    "abstract": "The law forbids discrimination. But the ambiguity of human decision-making\noften makes it extraordinarily hard for the legal system to know whether anyone\nhas actually discriminated. To understand how algorithms affect discrimination,\nwe must therefore also understand how they affect the problem of detecting\ndiscrimination. By one measure, algorithms are fundamentally opaque, not just\ncognitively but even mathematically. Yet for the task of proving\ndiscrimination, processes involving algorithms can provide crucial forms of\ntransparency that are otherwise unavailable. These benefits do not happen\nautomatically. But with appropriate requirements in place, the use of\nalgorithms will make it possible to more easily examine and interrogate the\nentire decision process, thereby making it far easier to know whether\ndiscrimination has occurred. By forcing a new level of specificity, the use of\nalgorithms also highlights, and makes transparent, central tradeoffs among\ncompeting values. Algorithms are not only a threat to be regulated; with the\nright safeguards in place, they have the potential to be a positive force for\nequity.",
    "published_date": "2019-02-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03731v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03706v1",
    "title": "Attaining Fairness in Communication for Omniscience",
    "authors": [
      "Ni Ding",
      "Parastoo Sadeghi",
      "David Smith",
      "Thierry Rakotoarivelo"
    ],
    "author_ids": [],
    "abstract": "This paper studies how to attain fairness in communication for omniscience,\nwhere a set of users exchange their observations of a discrete multiple random\nsource to attain omniscience---the state that all users recover the entire\nsource. The optimal rate region containing all source coding rate vectors that\nachieve the omniscience with the minimum sum rate is shown to coincide with the\ncore (the solution set) of a coalitional game. Two game-theoretic fairness\nsolutions are studied: the Shapley value and the egalitarian solution. It is\nshown that the Shapley value assigns each user the source coding rate measured\nby his/her remaining information of the multiple source given the common\nrandomness that is shared by all users, while the egalitarian solution simply\ndistributes the rates as evenly as possible in the core. To avoid the\nexponentially growing complexity of obtaining the Shapley value, a\npolynomial-time approximation method is proposed by utilizing the fact that the\nShapley value is the mean value over all extreme points in the core. In\naddition, a steepest descent algorithm is proposed which converges in\npolynomial time to the fractional egalitarian solution in the core that can be\nimplemented by network coding schemes. Finally, it is shown that the game can\nbe decomposed into subgames so that both the Shapley value and the egalitarian\nsolution can be obtained within each subgame in a distributed manner with\nreduced complexity.",
    "published_date": "2019-02-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03706v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.03689v2",
    "title": "Safe Artificial General Intelligence via Distributed Ledger Technology",
    "authors": [
      "Kristen W. Carlson"
    ],
    "author_ids": [],
    "abstract": "Background. Expert observers and artificial intelligence (AI) progression\nmetrics indicate AI will exceed human intelligence within a few decades.\nWhether general AI that exceeds human capabilities (AGI) will be the single\ngreatest boon in history or a disaster is unknown. No proofs exist that AGI\nwill benefit humans or that AGI will not harm or eliminate humans.\n  Objective. I propose a set of logically distinct conceptual components that\nare necessary and sufficient to 1) ensure that most known AGI scenarios will\nnot harm humanity and 2) robustly align AGI values and goals with human values.\n  Methods. By systematically addressing each pathway category to malevolent AI\nwe can induce the methods/axioms required to redress the category.\n  Results and Discussion. Distributed ledger technology (DLT, blockchain) is\nintegral to this proposal, e.g. to reduce the probability of hacking, provide\nan audit trail to detect and correct errors or identify components causing\nvulnerability or failure and replace them or shut them down remotely and/or\nautomatically, and to separate and balance key AGI components via decentralized\napps (dApps). Smart contracts based on DLT are necessary to address evolution\nof AI that will be too fast for human monitoring and intervention.\n  The proposed axioms. 1) Access to technology by market license. 2)\nTransparent ethics embodied in DLT. 3) Morality encrypted via DLT. 4) Behavior\ncontrol structure with values (ethics) at roots. 5) Individual bar-code\nidentification of all critical components. 6) Configuration Item (from business\ncontinuity/disaster recovery planning). 7) Identity verification secured via\nDLT. 8) Smart automated contracts based on DLT. 9) Decentralized applications -\nAI software code modules encrypted via DLT. 10) Audit trail of component usage\nstored via DLT. 11) Social ostracism (denial of societal resources) augmented\nby DLT petitions.",
    "published_date": "2019-02-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03689v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03582v2",
    "title": "Colorectal Cancer Outcome Prediction from H&E Whole Slide Images using Machine Learning and Automatically Inferred Phenotype Profiles",
    "authors": [
      "Xingzhi Yue",
      "Neofytos Dimitriou",
      "Ognjen Arandjelovic"
    ],
    "author_ids": [],
    "abstract": "Digital pathology (DP) is a new research area which falls under the broad\numbrella of health informatics. Owing to its potential for major public health\nimpact, in recent years DP has been attracting much research attention.\nNevertheless, a wide breadth of significant conceptual and technical challenges\nremain, few of them greater than those encountered in the field of oncology.\nThe automatic analysis of digital pathology slides of cancerous tissues is\nparticularly problematic due to the inherent heterogeneity of the disease,\nextremely large images, amongst numerous others. In this paper we introduce a\nnovel machine learning based framework for the prediction of colorectal cancer\noutcome from whole digitized haematoxylin & eosin (H&E) stained histopathology\nslides. Using a real-world data set we demonstrate the effectiveness of the\nmethod and present a detailed analysis of its different elements which\ncorroborate its ability to extract and learn salient, discriminative, and\nclinically meaningful content.",
    "published_date": "2019-02-10T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03582v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03565v1",
    "title": "Cross-spectral Face Completion for NIR-VIS Heterogeneous Face Recognition",
    "authors": [
      "Ran He",
      "Jie Cao",
      "Lingxiao Song",
      "Zhenan Sun",
      "Tieniu Tan"
    ],
    "author_ids": [],
    "abstract": "Near infrared-visible (NIR-VIS) heterogeneous face recognition refers to the\nprocess of matching NIR to VIS face images. Current heterogeneous methods try\nto extend VIS face recognition methods to the NIR spectrum by synthesizing VIS\nimages from NIR images. However, due to self-occlusion and sensing gap, NIR\nface images lose some visible lighting contents so that they are always\nincomplete compared to VIS face images. This paper models high resolution\nheterogeneous face synthesis as a complementary combination of two components,\na texture inpainting component and pose correction component. The inpainting\ncomponent synthesizes and inpaints VIS image textures from NIR image textures.\nThe correction component maps any pose in NIR images to a frontal pose in VIS\nimages, resulting in paired NIR and VIS textures. A warping procedure is\ndeveloped to integrate the two components into an end-to-end deep network. A\nfine-grained discriminator and a wavelet-based discriminator are designed to\nsupervise intra-class variance and visual quality respectively. One UV loss,\ntwo adversarial losses and one pixel loss are imposed to ensure synthesis\nresults. We demonstrate that by attaching the correction component, we can\nsimplify heterogeneous face synthesis from one-to-many unpaired image\ntranslation to one-to-one paired image translation, and minimize spectral and\npose discrepancy during heterogeneous recognition. Extensive experimental\nresults show that our network not only generates high-resolution VIS face\nimages and but also facilitates the accuracy improvement of heterogeneous face\nrecognition.",
    "published_date": "2019-02-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03565v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03439v1",
    "title": "PoliFi: Airtime Policy Enforcement for WiFi",
    "authors": [
      "Toke Høiland-Jørgensen",
      "Per Hurtig",
      "Anna Brunstrom"
    ],
    "author_ids": [],
    "abstract": "As WiFi grows ever more popular, airtime contention becomes an increasing\nproblem. One way to alleviate this is through network policy enforcement.\nUnfortunately, WiFi lacks protocol support for configuring policies for its\nusage, and since network-wide coordination cannot generally be ensured,\nenforcing policy is challenging. However, as we have shown in previous work, an\naccess point can influence the behaviour of connected devices by changing its\nscheduling of transmission opportunities, which can be used to achieve airtime\nfairness. In this work, we show that this mechanism can be extended to\nsuccessfully enforce airtime usage policies in WiFi networks. We implement this\nas an extension our previous airtime fairness work, and present PoliFi, the\nresulting policy enforcement system. Our evaluation shows that PoliFi makes it\npossible to express a range of useful policies. These include prioritisation of\nspecific devices; balancing groups of devices for sharing between different\nlogical networks or network slices; and limiting groups of devices to implement\nguest networks or other low-priority services. We also show how these can be\nused to improve the performance of a real-world DASH video streaming\napplication.",
    "published_date": "2019-02-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03439v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.06598v1",
    "title": "Network connectivity dynamics affect the evolution of culturally transmitted variants",
    "authors": [
      "José Segovia Martín",
      "Bradley Walker",
      "Nicolas Fay",
      "Monica Tamariz"
    ],
    "author_ids": [],
    "abstract": "The distribution of cultural variants in a population is shaped by both\nneutral evolutionary dynamics and by selection pressures, which include several\nindividual cognitive biases, demographic factors and social network structures.\nThe temporal dynamics of social network connectivity, i.e. the order in which\nindividuals in a population interact with each other, has been largely\nunexplored. In this paper we investigate how, in a fully connected social\nnetwork, connectivity dynamics, alone and in interaction with different\ncognitive biases, affect the evolution of cultural variants. Using agent-based\ncomputer simulations, we manipulate population connectivity dynamics (early,\nmiddle and late full-population connectivity); content bias, or a preference\nfor high-quality variants; coordination bias, or whether agents tend to use\nself-produced variants (egocentric bias), or to switch to variants observed in\nothers (allocentric bias); and memory size, or the number of items that agents\ncan store in their memory. We show that connectivity dynamics affect the\ntime-course of variant spread, with lower connectivity slowing down convergence\nof the population onto a single cultural variant. We also show that, compared\nto a neutral evolutionary model, content bias accelerates convergence and\namplifies the effects of connectivity dynamics, whilst larger memory size and\ncoordination bias, especially egocentric bias, slow down convergence.\nFurthermore, connectivity dynamics affect the frequency of high quality\nvariants (adaptiveness), with late connectivity populations showing bursts of\nrapid change in adaptiveness followed by periods of relatively slower change,\nand early connectivity populations following a single-peak evolutionary\ndynamic. In this way, we provide for the first time a direct connection between\nthe order of agents' interactions and punctuational evolution.",
    "published_date": "2019-02-09T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.06598v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03355v1",
    "title": "Forward-backward-forward methods with variance reduction for stochastic variational inequalities",
    "authors": [
      "Radu Ioan Bot",
      "Panayotis Mertikopoulos",
      "Mathias Staudigl",
      "Phan Tu Vuong"
    ],
    "author_ids": [],
    "abstract": "We develop a new stochastic algorithm with variance reduction for solving\npseudo-monotone stochastic variational inequalities. Our method builds on\nTseng's forward-backward-forward (FBF) algorithm, which is known in the\ndeterministic literature to be a valuable alternative to Korpelevich's\nextragradient method when solving variational inequalities over a convex and\nclosed set governed by pseudo-monotone, Lipschitz continuous operators. The\nmain computational advantage of Tseng's algorithm is that it relies only on a\nsingle projection step and two independent queries of a stochastic oracle. Our\nalgorithm incorporates a variance reduction mechanism and leads to almost sure\n(a.s.) convergence to an optimal solution. To the best of our knowledge, this\nis the first stochastic look-ahead algorithm achieving this by using only a\nsingle projection at each iteration..",
    "published_date": "2019-02-09T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.LG",
      "Primary 65K15, 62L20, secondary 90C15, 90C33"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03355v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.03290v1",
    "title": "Motion Scaling Solutions for Improved Performance in High Delay Surgical Teleoperation",
    "authors": [
      "Florian Richter",
      "Ryan K. Orosco",
      "Michael C. Yip"
    ],
    "author_ids": [],
    "abstract": "Robotic teleoperation brings great potential for advances within the field of\nsurgery. The ability of a surgeon to reach patient remotely opens exciting\nopportunities. Early experience with telerobotic surgery has been interesting,\nbut the clinical feasibility remains out of reach, largely due to the\ndeleterious effects of communication delays. Teleoperation tasks are\nsignificantly impacted by unavoidable signal latency, which directly results in\nslower operations, less precision in movements, and increased human errors.\nIntroducing significant changes to the surgical workflow, for example by\nintroducing semi-automation or self-correction, present too significant a\ntechnological and ethical burden for commercial surgical robotic systems to\nadopt. In this paper, we present three simple and intuitive motion scaling\nsolutions to combat teleoperated robotic systems under delay and help improve\noperator accuracy. Motion scaling offers potentially improved user performance\nand reduction in errors with minimal change to the underlying teleoperation\narchitecture. To validate the use of motion scaling as a performance enhancer\nin telesurgery, we conducted a user study with 17 participants, and our results\nshow that the proposed solutions do indeed reduce the error rate when operating\nunder high delay.",
    "published_date": "2019-02-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.RO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.03290v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.02693v1",
    "title": "StampNet: unsupervised multi-class object discovery",
    "authors": [
      "Joost Visser",
      "Alessandro Corbetta",
      "Vlado Menkovski",
      "Federico Toschi"
    ],
    "author_ids": [],
    "abstract": "Unsupervised object discovery in images involves uncovering recurring\npatterns that define objects and discriminates them against the background.\nThis is more challenging than image clustering as the size and the location of\nthe objects are not known: this adds additional degrees of freedom and\nincreases the problem complexity. In this work, we propose StampNet, a novel\nautoencoding neural network that localizes shapes (objects) over a simple\nbackground in images and categorizes them simultaneously. StampNet consists of\na discrete latent space that is used to categorize objects and to determine the\nlocation of the objects. The object categories are formed during the training,\nresulting in the discovery of a fixed set of objects. We present a set of\nexperiments that demonstrate that StampNet is able to localize and cluster\nmultiple overlapping shapes with varying complexity including the digits from\nthe MNIST dataset. We also present an application of StampNet in the\nlocalization of pedestrians in overhead depth-maps.",
    "published_date": "2019-02-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.02693v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.02554v1",
    "title": "Random Matrix Improved Covariance Estimation for a Large Class of Metrics",
    "authors": [
      "Malik Tiomoko",
      "Florent Bouchard",
      "Guillaume Ginholac",
      "Romain Couillet"
    ],
    "author_ids": [],
    "abstract": "Relying on recent advances in statistical estimation of covariance distances\nbased on random matrix theory, this article proposes an improved covariance and\nprecision matrix estimation for a wide family of metrics. The method is shown\nto largely outperform the sample covariance matrix estimate and to compete with\nstate-of-the-art methods, while at the same time being computationally simpler.\nApplications to linear and quadratic discriminant analyses also demonstrate\nsignificant gains, therefore suggesting practical interest to statistical\nmachine learning.",
    "published_date": "2019-02-07T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.02554v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.02412v1",
    "title": "A Bayesian Approach for Accurate Classification-Based Aggregates",
    "authors": [
      "Q. A. Meertens",
      "C. G. H. Diks",
      "H. J. van den Herik",
      "F W Takes"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the accuracy of values aggregated over classes\npredicted by a classification algorithm. The problem is that the resulting\naggregates (e.g., sums of a variable) are known to be biased. The bias can be\nlarge even for highly accurate classification algorithms, in particular when\ndealing with class-imbalanced data. To correct this bias, the algorithm's\nclassification error rates have to be estimated. In this estimation, two issues\narise when applying existing bias correction methods. First, inaccuracies in\nestimating classification error rates have to be taken into account. Second,\nimpermissible estimates, such as a negative estimate for a positive value, have\nto be dismissed. We show that both issues are relevant in applications where\nthe true labels are known only for a small set of data points. We propose a\nnovel bias correction method using Bayesian inference. The novelty of our\nmethod is that it imposes constraints on the model parameters. We show that our\nmethod solves the problem of biased classification-based aggregates as well as\nthe two issues above, in the general setting of multi-class classification. In\nthe empirical evaluation, using a binary classifier on a real-world dataset of\ncompany tax returns, we show that our method outperforms existing methods in\nterms of mean squared error.",
    "published_date": "2019-02-06T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.02412v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.02242v2",
    "title": "Equal Opportunity in Online Classification with Partial Feedback",
    "authors": [
      "Yahav Bechavod",
      "Katrina Ligett",
      "Aaron Roth",
      "Bo Waggoner",
      "Zhiwei Steven Wu"
    ],
    "author_ids": [],
    "abstract": "We study an online classification problem with partial feedback in which\nindividuals arrive one at a time from a fixed but unknown distribution, and\nmust be classified as positive or negative. Our algorithm only observes the\ntrue label of an individual if they are given a positive classification. This\nsetting captures many classification problems for which fairness is a concern:\nfor example, in criminal recidivism prediction, recidivism is only observed if\nthe inmate is released; in lending applications, loan repayment is only\nobserved if the loan is granted. We require that our algorithms satisfy common\nstatistical fairness constraints (such as equalizing false positive or negative\nrates -- introduced as \"equal opportunity\" in Hardt et al. (2016)) at every\nround, with respect to the underlying distribution. We give upper and lower\nbounds characterizing the cost of this constraint in terms of the regret rate\n(and show that it is mild), and give an oracle efficient algorithm that\nachieves the upper bound.",
    "published_date": "2019-02-06T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.02242v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.01889v1",
    "title": "Analyzing and Improving Representations with the Soft Nearest Neighbor Loss",
    "authors": [
      "Nicholas Frosst",
      "Nicolas Papernot",
      "Geoffrey Hinton"
    ],
    "author_ids": [],
    "abstract": "We explore and expand the $\\textit{Soft Nearest Neighbor Loss}$ to measure\nthe $\\textit{entanglement}$ of class manifolds in representation space: i.e.,\nhow close pairs of points from the same class are relative to pairs of points\nfrom different classes. We demonstrate several use cases of the loss. As an\nanalytical tool, it provides insights into the evolution of class similarity\nstructures during learning. Surprisingly, we find that $\\textit{maximizing}$\nthe entanglement of representations of different classes in the hidden layers\nis beneficial for discrimination in the final layer, possibly because it\nencourages representations to identify class-independent similarity structures.\nMaximizing the soft nearest neighbor loss in the hidden layers leads not only\nto improved generalization but also to better-calibrated estimates of\nuncertainty on outlier data. Data that is not from the training distribution\ncan be recognized by observing that in the hidden layers, it has fewer than the\nnormal number of neighbors from the predicted class.",
    "published_date": "2019-02-05T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.01889v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.01785v4",
    "title": "Homogeneous Linear Inequality Constraints for Neural Network Activations",
    "authors": [
      "Thomas Frerix",
      "Matthias Nießner",
      "Daniel Cremers"
    ],
    "author_ids": [],
    "abstract": "We propose a method to impose homogeneous linear inequality constraints of\nthe form $Ax\\leq 0$ on neural network activations. The proposed method allows a\ndata-driven training approach to be combined with modeling prior knowledge\nabout the task. One way to achieve this task is by means of a projection step\nat test time after unconstrained training. However, this is an expensive\noperation. By directly incorporating the constraints into the architecture, we\ncan significantly speed-up inference at test time; for instance, our\nexperiments show a speed-up of up to two orders of magnitude over a projection\nmethod. Our algorithm computes a suitable parameterization of the feasible set\nat initialization and uses standard variants of stochastic gradient descent to\nfind solutions to the constrained network. Thus, the modeling constraints are\nalways satisfied during training. Crucially, our approach avoids to solve an\noptimization problem at each training step or to manually trade-off data and\nconstraint fidelity with additional hyperparameters. We consider constrained\ngenerative modeling as an important application domain and experimentally\ndemonstrate the proposed method by constraining a variational autoencoder.",
    "published_date": "2019-02-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.01785v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.01637v1",
    "title": "A Universal Algorithm for Variational Inequalities Adaptive to Smoothness and Noise",
    "authors": [
      "Francis Bach",
      "Kfir Y. Levy"
    ],
    "author_ids": [],
    "abstract": "We consider variational inequalities coming from monotone operators, a\nsetting that includes convex minimization and convex-concave saddle-point\nproblems. We assume an access to potentially noisy unbiased values of the\nmonotone operators and assess convergence through a compatible gap function\nwhich corresponds to the standard optimality criteria in the aforementioned\nsubcases. We present a universal algorithm for these inequalities based on the\nMirror-Prox algorithm. Concretely, our algorithm simultaneously achieves the\noptimal rates for the smooth/non-smooth, and noisy/noiseless settings. This is\ndone without any prior knowledge of these properties, and in the general set-up\nof arbitrary norms and compatible Bregman divergences. For convex minimization\nand convex-concave saddle-point problems, this leads to new adaptive\nalgorithms. Our method relies on a novel yet simple adaptive choice of the\nstep-size, which can be seen as the appropriate extension of AdaGrad to handle\nconstrained problems.",
    "published_date": "2019-02-05T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.01637v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00768v1",
    "title": "Learning Linear Dynamical Systems with Semi-Parametric Least Squares",
    "authors": [
      "Max Simchowitz",
      "Ross Boczar",
      "Benjamin Recht"
    ],
    "author_ids": [],
    "abstract": "We analyze a simple prefiltered variation of the least squares estimator for\nthe problem of estimation with biased, semi-parametric noise, an error model\nstudied more broadly in causal statistics and active learning. We prove an\noracle inequality which demonstrates that this procedure provably mitigates the\nvariance introduced by long-term dependencies. We then demonstrate that\nprefiltered least squares yields, to our knowledge, the first algorithm that\nprovably estimates the parameters of partially-observed linear systems that\nattains rates which do not not incur a worst-case dependence on the rate at\nwhich these dependencies decay. The algorithm is provably consistent even for\nsystems which satisfy the weaker marginal stability condition obeyed by many\nclassical models based on Newtonian mechanics. In this context, our\nsemi-parametric framework yields guarantees for both stochastic and worst-case\nnoise.",
    "published_date": "2019-02-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "math.OC",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00768v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00746v3",
    "title": "On the bias, risk and consistency of sample means in multi-armed bandits",
    "authors": [
      "Jaehyeok Shin",
      "Aaditya Ramdas",
      "Alessandro Rinaldo"
    ],
    "author_ids": [],
    "abstract": "The sample mean is among the most well studied estimators in statistics,\nhaving many desirable properties such as unbiasedness and consistency. However,\nwhen analyzing data collected using a multi-armed bandit (MAB) experiment, the\nsample mean is biased and much remains to be understood about its properties.\nFor example, when is it consistent, how large is its bias, and can we bound its\nmean squared error? This paper delivers a thorough and systematic treatment of\nthe bias, risk and consistency of MAB sample means. Specifically, we identify\nfour distinct sources of selection bias (sampling, stopping, choosing and\nrewinding) and analyze them both separately and together. We further\ndemonstrate that a new notion of \\emph{effective sample size} can be used to\nbound the risk of the sample mean under suitable loss functions. We present\nseveral carefully designed examples to provide intuition on the different\nsources of selection bias we study. Our treatment is nonparametric and\nalgorithm-agnostic, meaning that it is not tied to a specific algorithm or\ngoal. In a nutshell, our proofs combine variational representations of\ninformation-theoretic divergences with new martingale concentration\ninequalities.",
    "published_date": "2019-02-02T00:00:00",
    "year": 2019,
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00746v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00580v2",
    "title": "On the Bias of Directed Information Estimators",
    "authors": [
      "Gabriel Schamberg",
      "Todd P. Coleman"
    ],
    "author_ids": [],
    "abstract": "When estimating the directed information between two jointly stationary\nMarkov processes, it is typically assumed that the recipient of the directed\ninformation is itself Markov of the same order as the joint process. While this\nassumption is often made explicit in the presentation of such estimators, a\ncharacterization of when we can expect the assumption to hold is lacking. Using\nthe concept of d-separation from Bayesian networks, we present sufficient\nconditions for which this assumption holds. We further show that the set of\nparameters for which the condition is not also necessary has Lebesgue measure\nzero. Given the strictness of these conditions, we introduce a notion of\npartial directed information, which can be used to bound the bias of directed\ninformation estimates when the directed information recipient is not itself\nMarkov. Lastly we estimate this bound on simulations in a variety of settings\nto assess the extent to which the bias should be cause for concern.",
    "published_date": "2019-02-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00580v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.00496v1",
    "title": "Examining the Presence of Gender Bias in Customer Reviews Using Word Embedding",
    "authors": [
      "A. Mishra",
      "H. Mishra",
      "S. Rathee"
    ],
    "author_ids": [],
    "abstract": "Humans have entered the age of algorithms. Each minute, algorithms shape\ncountless preferences from suggesting a product to a potential life partner. In\nthe marketplace algorithms are trained to learn consumer preferences from\ncustomer reviews because user-generated reviews are considered the voice of\ncustomers and a valuable source of information to firms. Insights mined from\nreviews play an indispensable role in several business activities ranging from\nproduct recommendation, targeted advertising, promotions, segmentation etc. In\nthis research, we question whether reviews might hold stereotypic gender bias\nthat algorithms learn and propagate Utilizing data from millions of\nobservations and a word embedding approach, GloVe, we show that algorithms\ndesigned to learn from human language output also learn gender bias. We also\nexamine why such biases occur: whether the bias is caused because of a negative\nbias against females or a positive bias for males. We examine the impact of\ngender bias in reviews on choice and conclude with policy implications for\nfemale consumers, especially when they are unaware of the bias, and the ethical\nimplications for firms.",
    "published_date": "2019-02-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00496v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00375v2",
    "title": "Dynamic fairness - Breaking vicious cycles in automatic decision making",
    "authors": [
      "Benjamin Paaßen",
      "Astrid Bunge",
      "Carolin Hainke",
      "Leon Sindelar",
      "Matthias Vogelsang"
    ],
    "author_ids": [],
    "abstract": "In recent years, machine learning techniques have been increasingly applied\nin sensitive decision making processes, raising fairness concerns. Past\nresearch has shown that machine learning may reproduce and even exacerbate\nhuman bias due to biased training data or flawed model assumptions, and thus\nmay lead to discriminatory actions. To counteract such biased models,\nresearchers have proposed multiple mathematical definitions of fairness\naccording to which classifiers can be optimized. However, it has also been\nshown that the outcomes generated by some fairness notions may be\nunsatisfactory.\n  In this contribution, we add to this research by considering decision making\nprocesses in time. We establish a theoretic model in which even perfectly\naccurate classifiers which adhere to almost all common fairness definitions\nlead to stable long-term inequalities due to vicious cycles. Only demographic\nparity, which enforces equal rates of positive decisions across groups, avoids\nthese effects and establishes a virtuous cycle, which leads to perfectly\naccurate and fair classification in the long term.",
    "published_date": "2019-02-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00375v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00334v3",
    "title": "SensitiveNets: Learning Agnostic Representations with Application to Face Images",
    "authors": [
      "Aythami Morales",
      "Julian Fierrez",
      "Ruben Vera-Rodriguez",
      "Ruben Tolosana"
    ],
    "author_ids": [],
    "abstract": "This work proposes a novel privacy-preserving neural network feature\nrepresentation to suppress the sensitive information of a learned space while\nmaintaining the utility of the data. The new international regulation for\npersonal data protection forces data controllers to guarantee privacy and avoid\ndiscriminative hazards while managing sensitive data of users. In our approach,\nprivacy and discrimination are related to each other. Instead of existing\napproaches aimed directly at fairness improvement, the proposed feature\nrepresentation enforces the privacy of selected attributes. This way fairness\nis not the objective, but the result of a privacy-preserving learning method.\nThis approach guarantees that sensitive information cannot be exploited by any\nagent who process the output of the model, ensuring both privacy and equality\nof opportunity. Our method is based on an adversarial regularizer that\nintroduces a sensitive information removal function in the learning objective.\nThe method is evaluated on three different primary tasks (identity,\nattractiveness, and smiling) and three publicly available benchmarks. In\naddition, we present a new face annotation dataset with balanced distribution\nbetween genders and ethnic origins. The experiments demonstrate that it is\npossible to improve the privacy and equality of opportunity while retaining\ncompetitive performance independently of the task.",
    "published_date": "2019-02-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00334v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00313v2",
    "title": "VrR-VG: Refocusing Visually-Relevant Relationships",
    "authors": [
      "Yuanzhi Liang",
      "Yalong Bai",
      "Wei Zhang",
      "Xueming Qian",
      "Li Zhu",
      "Tao Mei"
    ],
    "author_ids": [],
    "abstract": "Relationships encode the interactions among individual instances, and play a\ncritical role in deep visual scene understanding. Suffering from the high\npredictability with non-visual information, existing methods tend to fit the\nstatistical bias rather than ``learning'' to ``infer'' the relationships from\nimages. To encourage further development in visual relationships, we propose a\nnovel method to automatically mine more valuable relationships by pruning\nvisually-irrelevant ones. We construct a new scene-graph dataset named\nVisually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome.\nCompared with existing datasets, the performance gap between learnable and\nstatistical method is more significant in VrR-VG, and frequency-based analysis\ndoes not work anymore. Moreover, we propose to learn a relationship-aware\nrepresentation by jointly considering instances, attributes and relationships.\nBy applying the representation-aware feature learned on VrR-VG, the\nperformances of image captioning and visual question answering are\nsystematically improved with a large margin, which demonstrates the gain of our\ndataset and the features embedding schema. VrR-VG is available via\nhttp://vrr-vg.com/.",
    "published_date": "2019-02-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00313v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00215v3",
    "title": "Causally Driven Incremental Multi Touch Attribution Using a Recurrent Neural Network",
    "authors": [
      "Ruihuan Du",
      "Yu Zhong",
      "Harikesh Nair",
      "Bo Cui",
      "Ruyang Shou"
    ],
    "author_ids": [],
    "abstract": "This paper describes a practical system for Multi Touch Attribution (MTA) for\nuse by a publisher of digital ads. We developed this system for JD.com, an\neCommerce company, which is also a publisher of digital ads in China. The\napproach has two steps. The first step ('response modeling') fits a user-level\nmodel for purchase of a product as a function of the user's exposure to ads.\nThe second ('credit allocation') uses the fitted model to allocate the\nincremental part of the observed purchase due to advertising, to the ads the\nuser is exposed to over the previous T days. To implement step one, we train a\nRecurrent Neural Network (RNN) on user-level conversion and exposure data. The\nRNN has the advantage of flexibly handling the sequential dependence in the\ndata in a semi-parametric way. The specific RNN formulation we implement\ncaptures the impact of advertising intensity, timing, competition, and\nuser-heterogeneity, which are known to be relevant to ad-response. To implement\nstep two, we compute Shapley Values, which have the advantage of having\naxiomatic foundations and satisfying fairness considerations. The specific\nformulation of the Shapley Value we implement respects incrementality by\nallocating the overall incremental improvement in conversion to the exposed\nads, while handling the sequence-dependence of exposures on the observed\noutcomes. The system is under production at JD.com, and scales to handle the\nhigh dimensionality of the problem on the platform (attribution of the orders\nof about 300M users, for roughly 160K brands, across 200+ ad-types, served\nabout 80B ad-impressions over a typical 15-day period).",
    "published_date": "2019-02-01T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00215v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1902.00119v1",
    "title": "Race, Ethnicity and National Origin-based Discrimination in Social Media and Hate Crimes Across 100 U.S. Cities",
    "authors": [
      "Kunal Relia",
      "Zhengyi Li",
      "Stephanie H. Cook",
      "Rumi Chunara"
    ],
    "author_ids": [],
    "abstract": "We study malicious online content via a specific type of hate speech: race,\nethnicity and national-origin based discrimination in social media, alongside\nhate crimes motivated by those characteristics, in 100 cities across the United\nStates. We develop a spatially-diverse training dataset and classification\npipeline to delineate targeted and self-narration of discrimination on social\nmedia, accounting for language across geographies. Controlling for census\nparameters, we find that the proportion of discrimination that is targeted is\nassociated with the number of hate crimes. Finally, we explore the linguistic\nfeatures of discrimination Tweets in relation to hate crimes by city, features\nused by users who Tweet different amounts of discrimination, and features of\ndiscrimination compared to non-discrimination Tweets. Findings from this\nspatial study can inform future studies of how discrimination in physical and\nvirtual worlds vary by place, or how physical and virtual world discrimination\nmay synergize.",
    "published_date": "2019-01-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00119v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1902.00043v1",
    "title": "Perils and Challenges of Social Media and Election Manipulation Analysis: The 2018 US Midterms",
    "authors": [
      "Ashok Deb",
      "Luca Luceri",
      "Adam Badawy",
      "Emilio Ferrara"
    ],
    "author_ids": [],
    "abstract": "One of the hallmarks of a free and fair society is the ability to conduct a\npeaceful and seamless transfer of power from one leader to another.\nDemocratically, this is measured in a citizen population's trust in the\nelectoral system of choosing a representative government. In view of the well\ndocumented issues of the 2016 US Presidential election, we conducted an\nin-depth analysis of the 2018 US Midterm elections looking specifically for\nvoter fraud or suppression. The Midterm election occurs in the middle of a 4\nyear presidential term. For the 2018 midterms, 35 senators and all the 435\nseats in the House of Representatives were up for re-election, thus, every\ncongressional district and practically every state had a federal election. In\norder to collect election related tweets, we analyzed Twitter during the month\nprior to, and the two weeks following, the November 6, 2018 election day. In a\ntargeted analysis to detect statistical anomalies or election interference, we\nidentified several biases that can lead to wrong conclusions. Specifically, we\nlooked for divergence between actual voting outcomes and instances of the\n#ivoted hashtag on the election day. This analysis highlighted three states of\nconcern: New York, California, and Texas. We repeated our analysis discarding\nmalicious accounts, such as social bots. Upon further inspection and against a\nbackdrop of collected general election-related tweets, we identified some\nconfounding factors, such as population bias, or bot and political ideology\ninference, that can lead to false conclusions. We conclude by providing an\nin-depth discussion of the perils and challenges of using social media data to\nexplore questions about election manipulation.",
    "published_date": "2019-01-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.00043v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.11184v1",
    "title": "Human-Centered Artificial Intelligence and Machine Learning",
    "authors": [
      "Mark O. Riedl"
    ],
    "author_ids": [],
    "abstract": "Humans are increasingly coming into contact with artificial intelligence and\nmachine learning systems. Human-centered artificial intelligence is a\nperspective on AI and ML that algorithms must be designed with awareness that\nthey are part of a larger system consisting of humans. We lay forth an argument\nthat human-centered artificial intelligence can be broken down into two\naspects: (1) AI systems that understand humans from a sociocultural\nperspective, and (2) AI systems that help humans understand them. We further\nargue that issues of social responsibility such as fairness, accountability,\ninterpretability, and transparency.",
    "published_date": "2019-01-31T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.11184v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10893v2",
    "title": "Transportation Proof of an inequality by Anantharam, Jog and Nair",
    "authors": [
      "Thomas A. Courtade"
    ],
    "author_ids": [],
    "abstract": "Anantharam, Jog and Nair recently put forth an entropic inequality which\nsimultaneously generalizes the Shannon-Stam entropy power inequality and the\nBrascamp-Lieb inequality in entropic form. We give a brief proof of their\nresult based on optimal transport.",
    "published_date": "2019-01-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.FA",
      "math.IT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10893v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.10837v4",
    "title": "Noise-tolerant fair classification",
    "authors": [
      "Alexandre Louis Lamy",
      "Ziyuan Zhong",
      "Aditya Krishna Menon",
      "Nakul Verma"
    ],
    "author_ids": [],
    "abstract": "Fairness-aware learning involves designing algorithms that do not\ndiscriminate with respect to some sensitive feature (e.g., race or gender).\nExisting work on the problem operates under the assumption that the sensitive\nfeature available in one's training sample is perfectly reliable. This\nassumption may be violated in many real-world cases: for example, respondents\nto a survey may choose to conceal or obfuscate their group identity out of fear\nof potential discrimination. This poses the question of whether one can still\nlearn fair classifiers given noisy sensitive features. In this paper, we answer\nthe question in the affirmative: we show that if one measures fairness using\nthe mean-difference score, and sensitive features are subject to noise from the\nmutually contaminated learning model, then owing to a simple identity we only\nneed to change the desired fairness-tolerance. The requisite tolerance can be\nestimated by leveraging existing noise-rate estimators from the label noise\nliterature. We finally show that our procedure is empirically effective on two\ncase-studies involving sensitive feature censoring.",
    "published_date": "2019-01-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10837v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10720v1",
    "title": "Effects of Moderation and Opinion Heterogeneity on Attitude towards the Online Deliberation Experience",
    "authors": [
      "Simon T. Perrault",
      "Weiyu Zhang"
    ],
    "author_ids": [],
    "abstract": "Online deliberation offers a way for citizens to collectively discuss an\nissue and provide input for policy makers. The overall experience of online\ndeliberation can be affected by multiple factors. We decided to investigate the\neffects of moderation and opinion heterogeneity on the perceived deliberation\nexperience, by running the first online deliberation experiment in Singapore.\nOur study took place in three months with three phases. In phase 1, our 2,006\nparticipants answered a survey, that we used to create groups of different\nopinion heterogeneity. During the second phase, 510 participants discussed\nabout the population issue on the online platform we developed. We gathered\ndata on their online deliberation experience during phase 3. We found out that\nhigher levels of moderation negatively impact the experience of deliberation on\nperceived procedural fairness, validity claim and policy legitimacy; and that\nhigh opinion heterogeneity is important in order to get a fair assessment of\nthe deliberation experience.",
    "published_date": "2019-01-30T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10720v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.10501v2",
    "title": "Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions",
    "authors": [
      "Hao Wang",
      "Berk Ustun",
      "Flavio P. Calmon"
    ],
    "author_ids": [],
    "abstract": "When the performance of a machine learning model varies over groups defined\nby sensitive attributes (e.g., gender or ethnicity), the performance disparity\ncan be expressed in terms of the probability distributions of the input and\noutput variables over each group. In this paper, we exploit this fact to reduce\nthe disparate impact of a fixed classification model over a population of\ninterest. Given a black-box classifier, we aim to eliminate the performance gap\nby perturbing the distribution of input variables for the disadvantaged group.\nWe refer to the perturbed distribution as a counterfactual distribution, and\ncharacterize its properties for common fairness criteria. We introduce a\ndescent algorithm to learn a counterfactual distribution from data. We then\ndiscuss how the estimated distribution can be used to build a data preprocessor\nthat can reduce disparate impact without training a new model. We validate our\napproach through experiments on real-world datasets, showing that it can repair\ndifferent forms of disparity without a significant drop in accuracy.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10501v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10450v2",
    "title": "Toward Controlling Discrimination in Online Ad Auctions",
    "authors": [
      "L. Elisa Celis",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "author_ids": [],
    "abstract": "Online advertising platforms are thriving due to the customizable audiences\nthey offer advertisers. However, recent studies show that advertisements can be\ndiscriminatory with respect to the gender or race of the audience that sees the\nad, and may inadvertently cross ethical and/or legal boundaries. To prevent\nthis, we propose a constrained ad auction framework that maximizes the\nplatform's revenue conditioned on ensuring that the audience seeing an\nadvertiser's ad is distributed appropriately across sensitive types such as\ngender or race. Building upon Myerson's classic work, we first present an\noptimal auction mechanism for a large class of fairness constraints. Finding\nthe parameters of this optimal auction, however, turns out to be a non-convex\nproblem. We show that this non-convex problem can be reformulated as a more\nstructured non-convex problem with no saddle points or local-maxima; this\nallows us to develop a gradient-descent-based algorithm to solve it. Our\nempirical results on the A1 Yahoo! dataset demonstrate that our algorithm can\nobtain uniform coverage across different user types for each advertiser at a\nminor loss to the revenue of the platform, and a small change to the size of\nthe audience each advertiser reaches.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10450v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10443v1",
    "title": "Improved Adversarial Learning for Fair Classification",
    "authors": [
      "L. Elisa Celis",
      "Vijay Keswani"
    ],
    "author_ids": [],
    "abstract": "Motivated by concerns that machine learning algorithms may introduce\nsignificant bias in classification models, developing fair classifiers has\nbecome an important problem in machine learning research. One important\nparadigm towards this has been providing algorithms for adversarially learning\nfair classifiers (Zhang et al., 2018; Madras et al., 2018). We formulate the\nadversarial learning problem as a multi-objective optimization problem and find\nthe fair model using gradient descent-ascent algorithm with a modified gradient\nupdate step, inspired by the approach of Zhang et al., 2018. We provide\ntheoretical insight and guarantees that formalize the heuristic arguments\npresented previously towards taking such an approach. We test our approach\nempirically on the Adult dataset and synthetic datasets and compare against\nstate of the art algorithms (Celis et al., 2018; Zhang et al., 2018; Zafar et\nal., 2017). The results show that our models and algorithms have comparable or\nbetter accuracy than other algorithms while performing better in terms of\nfairness, as measured using statistical rate or false discovery rate.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10443v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10437v2",
    "title": "Quantifying the Impact of User Attention on Fair Group Representation in Ranked Lists",
    "authors": [
      "Piotr Sapiezynski",
      "Wesley Zeng",
      "Ronald E. Robertson",
      "Alan Mislove",
      "Christo Wilson"
    ],
    "author_ids": [],
    "abstract": "In this work, we introduce a novel metric for auditing group fairness in\nranked lists. Our approach offers two benefits compared to the state of the\nart. First, we offer a blueprint for modeling of user attention. Rather than\nassuming a logarithmic loss in importance as a function of the rank, we can\naccount for varying user behaviors through parametrization. For example, we\nexpect a user to see more items during a viewing of a social media feed than\nwhen they inspect the results list of a single web search query. Second, we\nallow non-binary protected attributes to enable investigating inherently\ncontinuous attributes (\\eg political alignment on the liberal to conservative\nspectrum) as well as to facilitate measurements across aggregated sets of\nsearch results, rather than separately for each result list. By combining these\ntwo elements into our metric, we are able to better address the human factors\ninherent in this problem. We measure the whole sociotechnical system,\nconsisting of a ranking algorithm and individuals using it, instead of\nexclusively focusing on the ranking algorithm. Finally, we use our metric to\nperform three simulated fairness audits. We show that determining fairness of a\nranked output necessitates knowledge (or a model) of the end-users of the\nparticular service. Depending on their attention distribution function, a fixed\nranking of results can appear biased both in favor and against a protected\ngroup.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10437v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.10436v6",
    "title": "Diversity in Faces",
    "authors": [
      "Michele Merler",
      "Nalini Ratha",
      "Rogerio S. Feris",
      "John R. Smith"
    ],
    "author_ids": [],
    "abstract": "Face recognition is a long standing challenge in the field of Artificial\nIntelligence (AI). The goal is to create systems that accurately detect,\nrecognize, verify, and understand human faces. There are significant technical\nhurdles in making these systems accurate, particularly in unconstrained\nsettings due to confounding factors related to pose, resolution, illumination,\nocclusion, and viewpoint. However, with recent advances in neural networks,\nface recognition has achieved unprecedented accuracy, largely built on\ndata-driven deep learning methods. While this is encouraging, a critical aspect\nthat is limiting facial recognition accuracy and fairness is inherent facial\ndiversity. Every face is different. Every face reflects something unique about\nus. Aspects of our heritage - including race, ethnicity, culture, geography -\nand our individual identify - age, gender, and other visible manifestations of\nself-expression, are reflected in our faces. We expect face recognition to work\nequally accurately for every face. Face recognition needs to be fair. As we\nrely on data-driven methods to create face recognition technology, we need to\nensure necessary balance and coverage in training data. However, there are\nstill scientific questions about how to represent and extract pertinent facial\nfeatures and quantitatively measure facial diversity. Towards this goal,\nDiversity in Faces (DiF) provides a data set of one million annotated human\nface images for advancing the study of facial diversity. The annotations are\ngenerated using ten well-established facial coding schemes from the scientific\nliterature. The facial coding schemes provide human-interpretable quantitative\nmeasures of facial features. We believe that by making the extracted coding\nschemes available on a large set of faces, we can accelerate research and\ndevelopment towards creating more fair and accurate facial recognition systems.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10436v6",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10417v1",
    "title": "Sliced generative models",
    "authors": [
      "Szymon Knop",
      "Marcin Mazur",
      "Jacek Tabor",
      "Igor Podolak",
      "Przemysław Spurek"
    ],
    "author_ids": [],
    "abstract": "In this paper we discuss a class of AutoEncoder based generative models based\non one dimensional sliced approach. The idea is based on the reduction of the\ndiscrimination between samples to one-dimensional case. Our experiments show\nthat methods can be divided into two groups. First consists of methods which\nare a modification of standard normality tests, while the second is based on\nclassical distances between samples. It turns out that both groups are correct\ngenerative models, but the second one gives a slightly faster decrease rate of\nFr\\'{e}chet Inception Distance (FID).",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10417v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10338v1",
    "title": "Limitations of Assessing Active Learning Performance at Runtime",
    "authors": [
      "Daniel Kottke",
      "Jim Schellinger",
      "Denis Huseljic",
      "Bernhard Sick"
    ],
    "author_ids": [],
    "abstract": "Classification algorithms aim to predict an unknown label (e.g., a quality\nclass) for a new instance (e.g., a product). Therefore, training samples\n(instances and labels) are used to deduct classification hypotheses. Often, it\nis relatively easy to capture instances but the acquisition of the\ncorresponding labels remain difficult or expensive. Active learning algorithms\nselect the most beneficial instances to be labeled to reduce cost. In research,\nthis labeling procedure is simulated and therefore a ground truth is available.\nBut during deployment, active learning is a one-shot problem and an evaluation\nset is not available. Hence, it is not possible to reliably estimate the\nperformance of the classification system during learning and it is difficult to\ndecide when the system fulfills the quality requirements (stopping criteria).\nIn this article, we formalize the task and review existing strategies to assess\nthe performance of an actively trained classifier during training. Furthermore,\nwe identified three major challenges: 1)~to derive a performance distribution,\n2)~to preserve representativeness of the labeled subset, and 3) to correct\nagainst sampling bias induced by an intelligent selection strategy. In a\nqualitative analysis, we evaluate different existing approaches and show that\nnone of them reliably estimates active learning performance stating a major\nchallenge for future research for such systems. All plots and experiments are\nprovided in a Jupyter notebook that is available for download.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10338v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10334v2",
    "title": "Rank-one Convexification for Sparse Regression",
    "authors": [
      "Alper Atamturk",
      "Andres Gomez"
    ],
    "author_ids": [],
    "abstract": "Sparse regression models are increasingly prevalent due to their ease of\ninterpretability and superior out-of-sample performance. However, the exact\nmodel of sparse regression with an $\\ell_0$ constraint restricting the support\nof the estimators is a challenging (\\NP-hard) non-convex optimization problem.\nIn this paper, we derive new strong convex relaxations for sparse regression.\nThese relaxations are based on the ideal (convex-hull) formulations for\nrank-one quadratic terms with indicator variables. The new relaxations can be\nformulated as semidefinite optimization problems in an extended space and are\nstronger and more general than the state-of-the-art formulations, including the\nperspective reformulation and formulations with the reverse Huber penalty and\nthe minimax concave penalty functions. Furthermore, the proposed rank-one\nstrengthening can be interpreted as a \\textit{non-separable, non-convex,\nunbiased} sparsity-inducing regularizer, which dynamically adjusts its penalty\naccording to the shape of the error function without inducing bias for the\nsparse solutions. In our computational experiments with benchmark datasets, the\nproposed conic formulations are solved within seconds and result in\nnear-optimal solutions (with 0.4\\% optimality gap) for non-convex\n$\\ell_0$-problems. Moreover, the resulting estimators also outperform\nalternative convex approaches from a statistical perspective, achieving high\nprediction accuracy and good interpretability.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10334v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10265v3",
    "title": "Implicit Diversity in Image Summarization",
    "authors": [
      "L. Elisa Celis",
      "Vijay Keswani"
    ],
    "author_ids": [],
    "abstract": "Studies have shown that the people depicted in image search results tend to\nbe of majority groups with respect to socially salient attributes. This skew\ngoes beyond that which already exists in the world - e.g., Kay et al. showed\nthat although 28% of CEOs in US are women, only 10% of the top 100 results for\nCEO in Google Image Search are women. Most existing approaches to correct for\nthis kind of bias assume that the images of people include socially salient\nattribute labels. However, such labels are often unknown. Further, using\nautomated techniques to infer these labels may often not be possible within\nacceptable accuracy ranges, and may not be desirable due to the additional\nbiases this process could incur. We develop a novel approach that takes as\ninput a visibly diverse control set of images and uses this set to select a set\nof images of people in response to a query. The goal is to have a resulting set\nthat is more visibly diverse in a manner that emulates the diversity depicted\nin the control set. Importantly, this approach does not require images to be\nlabelled at any point; effectively, it gives a way to implicitly diversify the\nset of images selected. We provide two variants of our approach: the first is a\nmodification of the MMR algorithm to incorporate the diversity scores, and\nsecond is a more efficient variant that does not consider within-list\nredundancy. We evaluate these approaches empirically on two datasets 1) a new\ndataset containing top Google image results for 96 occupations, for which we\nevaluate gender and skin-tone diversity with respect to occupations and 2) the\nCelebA dataset for which we evaluate gender diversity with respect to facial\nfeatures. Our approaches produce image sets that significantly improve the\nvisible diversity of the results, compared to current Google search and other\ndiverse image summarization algorithms, at a minimal cost to accuracy.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10265v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10080v3",
    "title": "General Fair Empirical Risk Minimization",
    "authors": [
      "Luca Oneto",
      "Michele Donini",
      "Massimiliano Pontil"
    ],
    "author_ids": [],
    "abstract": "We tackle the problem of algorithmic fairness, where the goal is to avoid the\nunfairly influence of sensitive information, in the general context of\nregression with possible continuous sensitive attributes. We extend the\nframework of fair empirical risk minimization to this general scenario,\ncovering in this way the whole standard supervised learning setting. Our\ngeneralized fairness measure reduces to well known notions of fairness\navailable in literature. We derive learning guarantees for our method, that\nimply in particular its statistical consistency, both in terms of the risk and\nthe fairness measure. We then specialize our approach to kernel methods and\npropose a convex fair estimator in that setting. We test the estimator on a\ncommonly used benchmark dataset (Communities and Crime) and on a new dataset\ncollected at the University of Genova, containing the information of the\nacademic career of five thousand students. The latter dataset provides a\nchallenging real case scenario of unfair behaviour of standard regression\nmethods that benefits from our methodology. The experimental results show that\nour estimator is effective at mitigating the trade-off between accuracy and\nfairness requirements.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10080v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10053v1",
    "title": "Towards Fair Deep Clustering With Multi-State Protected Variables",
    "authors": [
      "Bokun Wang",
      "Ian Davidson"
    ],
    "author_ids": [],
    "abstract": "Fair clustering under the disparate impact doctrine requires that population\nof each protected group should be approximately equal in every cluster.\nPrevious work investigated a difficult-to-scale pre-processing step for\n$k$-center and $k$-median style algorithms for the special case of this problem\nwhen the number of protected groups is two. In this work, we consider a more\ngeneral and practical setting where there can be many protected groups. To this\nend, we propose Deep Fair Clustering, which learns a discriminative but fair\ncluster assignment function. The experimental results on three public datasets\nwith different types of protected attribute show that our approach can steadily\nimprove the degree of fairness while only having minor loss in terms of\nclustering quality.",
    "published_date": "2019-01-29T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10053v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09849v2",
    "title": "Activation Adaptation in Neural Networks",
    "authors": [
      "Farnoush Farhadi",
      "Vahid Partovi Nia",
      "Andrea Lodi"
    ],
    "author_ids": [],
    "abstract": "Many neural network architectures rely on the choice of the activation\nfunction for each hidden layer. Given the activation function, the neural\nnetwork is trained over the bias and the weight parameters. The bias catches\nthe center of the activation, and the weights capture the scale. Here we\npropose to train the network over a shape parameter as well. This view allows\neach neuron to tune its own activation function and adapt the neuron curvature\ntowards a better prediction. This modification only adds one further equation\nto the back-propagation for each neuron. Re-formalizing activation functions as\nCDF generalizes the class of activation function extensively. We aimed at\ngeneralizing an extensive class of activation functions to study: i) skewness\nand ii) smoothness of activation functions. Here we introduce adaptive Gumbel\nactivation function as a bridge between Gumbel and sigmoid. A similar approach\nis used to invent a smooth version of ReLU. Our comparison with common\nactivation functions suggests different data representation especially in early\nneural network layers. This adaptation also provides prediction improvement.",
    "published_date": "2019-01-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML",
      "92B20, 68T05"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09849v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09749v3",
    "title": "Fairwashing: the risk of rationalization",
    "authors": [
      "Ulrich Aïvodji",
      "Hiromi Arai",
      "Olivier Fortineau",
      "Sébastien Gambs",
      "Satoshi Hara",
      "Alain Tapp"
    ],
    "author_ids": [],
    "abstract": "Black-box explanation is the problem of explaining how a machine learning\nmodel -- whose internal logic is hidden to the auditor and generally complex --\nproduces its outcomes. Current approaches for solving this problem include\nmodel explanation, outcome explanation as well as model inspection. While these\ntechniques can be beneficial by providing interpretability, they can be used in\na negative manner to perform fairwashing, which we define as promoting the\nfalse perception that a machine learning model respects some ethical values. In\nparticular, we demonstrate that it is possible to systematically rationalize\ndecisions taken by an unfair black-box model using the model explanation as\nwell as the outcome explanation approaches with a given fairness metric. Our\nsolution, LaundryML, is based on a regularized rule list enumeration algorithm\nwhose objective is to search for fair rule lists approximating an unfair\nblack-box model. We empirically evaluate our rationalization technique on\nblack-box models trained on real-world datasets and show that one can obtain\nrule lists with high fidelity to the black-box model while being considerably\nless unfair at the same time.",
    "published_date": "2019-01-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09749v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09672v2",
    "title": "Personalized Dialogue Generation with Diversified Traits",
    "authors": [
      "Yinhe Zheng",
      "Guanyi Chen",
      "Minlie Huang",
      "Song Liu",
      "Xuan Zhu"
    ],
    "author_ids": [],
    "abstract": "Endowing a dialogue system with particular personality traits is essential to\ndeliver more human-like conversations. However, due to the challenge of\nembodying personality via language expression and the lack of large-scale\npersona-labeled dialogue data, this research problem is still far from\nwell-studied. In this paper, we investigate the problem of incorporating\nexplicit personality traits in dialogue generation to deliver personalized\ndialogues.\n  To this end, firstly, we construct PersonalDialog, a large-scale multi-turn\ndialogue dataset containing various traits from a large number of speakers. The\ndataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.\nEach utterance is associated with a speaker who is marked with traits like Age,\nGender, Location, Interest Tags, etc. Several anonymization schemes are\ndesigned to protect the privacy of each speaker. This large-scale dataset will\nfacilitate not only the study of personalized dialogue generation, but also\nother researches on sociolinguistics or social science.\n  Secondly, to study how personality traits can be captured and addressed in\ndialogue generation, we propose persona-aware dialogue generation models within\nthe sequence to sequence learning framework. Explicit personality traits\n(structured by key-value pairs) are embedded using a trait fusion module.\nDuring the decoding process, two techniques, namely persona-aware attention and\npersona-aware bias, are devised to capture and address trait-related\ninformation. Experiments demonstrate that our model is able to address proper\ntraits in different contexts. Case studies also show interesting results for\nthis challenging research problem.",
    "published_date": "2019-01-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09672v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09631v1",
    "title": "Secrecy Throughput Maximization for Full-Duplex Wireless Powered IoT Networks under Fairness Constraints",
    "authors": [
      "Roohollah Rezaei",
      "Sumei Sun",
      "Xin Kang",
      "Yong Liang Guan",
      "Mohammad Reza Pakravan"
    ],
    "author_ids": [],
    "abstract": "In this paper, we study the secrecy throughput of a full-duplex wireless\npowered communication network (WPCN) for internet of things (IoT). The WPCN\nconsists of a full-duplex multi-antenna base station (BS) and a number of\nsensor nodes. The BS transmits energy all the time, and each node harvests\nenergy prior to its transmission time slot. The nodes sequentially transmit\ntheir confidential information to the BS, and the other nodes are considered as\npotential eavesdroppers. We first formulate the sum secrecy throughput\noptimization problem of all the nodes. The optimization variables are the\nduration of the time slots and the BS beamforming vectors in different time\nslots. The problem is shown to be non-convex. To tackle the problem, we propose\na suboptimal two stage approach, referred to as sum secrecy throughput\nmaximization (SSTM). In the first stage, the BS focuses its beamforming to\nblind the potential eavesdroppers (other nodes) during information transmission\ntime slots. Then, the optimal beamforming vector in the initial non-information\ntransmission time slot and the optimal time slots are derived. We then consider\nfairness among the nodes and propose max-min fair (MMF) and proportional fair\n(PLF) algorithms. The MMF algorithm maximizes the minimum secrecy throughput of\nthe nodes, while the PLF tries to achieve a good trade-off between the sum\nsecrecy throughput and fairness among the nodes. Through numerical simulations,\nwe first demonstrate the superior performance of the SSTM to uniform time\nslotting and beamforming in different settings. Then, we show the effectiveness\nof the proposed fair algorithms.",
    "published_date": "2019-01-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09631v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.09565v1",
    "title": "Fairness in representation: quantifying stereotyping as a representational harm",
    "authors": [
      "Mohsen Abbasi",
      "Sorelle A. Friedler",
      "Carlos Scheidegger",
      "Suresh Venkatasubramanian"
    ],
    "author_ids": [],
    "abstract": "While harms of allocation have been increasingly studied as part of the\nsubfield of algorithmic fairness, harms of representation have received\nconsiderably less attention. In this paper, we formalize two notions of\nstereotyping and show how they manifest in later allocative harms within the\nmachine learning pipeline. We also propose mitigation strategies and\ndemonstrate their effectiveness on synthetic datasets.",
    "published_date": "2019-01-28T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09565v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.10566v2",
    "title": "Fair Regression for Health Care Spending",
    "authors": [
      "Anna Zink",
      "Sherri Rose"
    ],
    "author_ids": [],
    "abstract": "The distribution of health care payments to insurance plans has substantial\nconsequences for social policy. Risk adjustment formulas predict spending in\nhealth insurance markets in order to provide fair benefits and health care\ncoverage for all enrollees, regardless of their health status. Unfortunately,\ncurrent risk adjustment formulas are known to underpredict spending for\nspecific groups of enrollees leading to undercompensated payments to health\ninsurers. This incentivizes insurers to design their plans such that\nindividuals in undercompensated groups will be less likely to enroll, impacting\naccess to health care for these groups. To improve risk adjustment formulas for\nundercompensated groups, we expand on concepts from the statistics, computer\nscience, and health economics literature to develop new fair regression methods\nfor continuous outcomes by building fairness considerations directly into the\nobjective function. We additionally propose a novel measure of fairness while\nasserting that a suite of metrics is necessary in order to evaluate risk\nadjustment formulas more fully. Our data application using the IBM MarketScan\nResearch Databases and simulation studies demonstrate that these new fair\nregression methods may lead to massive improvements in group fairness (e.g.,\n98%) with only small reductions in overall fit (e.g., 4%).",
    "published_date": "2019-01-28T00:00:00",
    "year": 2019,
    "categories": [
      "stat.AP",
      "cs.CY",
      "stat.ME",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.10566v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.09465v7",
    "title": "Deconstructing Generative Adversarial Networks",
    "authors": [
      "Banghua Zhu",
      "Jiantao Jiao",
      "David Tse"
    ],
    "author_ids": [],
    "abstract": "We deconstruct the performance of GANs into three components:\n  1. Formulation: we propose a perturbation view of the population target of\nGANs. Building on this interpretation, we show that GANs can be viewed as a\ngeneralization of the robust statistics framework, and propose a novel GAN\narchitecture, termed as Cascade GANs, to provably recover meaningful\nlow-dimensional generator approximations when the real distribution is\nhigh-dimensional and corrupted by outliers.\n  2. Generalization: given a population target of GANs, we design a systematic\nprinciple, projection under admissible distance, to design GANs to meet the\npopulation requirement using finite samples. We implement our principle in\nthree cases to achieve polynomial and sometimes near-optimal sample\ncomplexities: (1) learning an arbitrary generator under an arbitrary\npseudonorm; (2) learning a Gaussian location family under TV distance, where we\nutilize our principle provide a new proof for the optimality of Tukey median\nviewed as GANs; (3) learning a low-dimensional Gaussian approximation of a\nhigh-dimensional arbitrary distribution under Wasserstein distance. We\ndemonstrate a fundamental trade-off in the approximation error and statistical\nerror in GANs, and show how to apply our principle with empirical samples to\npredict how many samples are sufficient for GANs in order not to suffer from\nthe discriminator winning problem.\n  3. Optimization: we demonstrate alternating gradient descent is provably not\nlocally asymptotically stable in optimizing the GAN formulation of PCA. We\ndiagnose the problem as the minimax duality gap being non-zero, and propose a\nnew GAN architecture whose duality gap is zero, where the value of the game is\nequal to the previous minimax value (not the maximin value). We prove the new\nGAN architecture is globally asymptotically stable in optimization under\nalternating gradient descent.",
    "published_date": "2019-01-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09465v7",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09451v1",
    "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting",
    "authors": [
      "Maria De-Arteaga",
      "Alexey Romanov",
      "Hanna Wallach",
      "Jennifer Chayes",
      "Christian Borgs",
      "Alexandra Chouldechova",
      "Sahin Geyik",
      "Krishnaram Kenthapadi",
      "Adam Tauman Kalai"
    ],
    "author_ids": [],
    "abstract": "We present a large-scale study of gender bias in occupation classification, a\ntask where the use of machine learning may lead to negative outcomes on\npeoples' lives. We analyze the potential allocation harms that can result from\nsemantic representation bias. To do so, we study the impact on occupation\nclassification of including explicit gender indicators---such as first names\nand pronouns---in different semantic representations of online biographies.\nAdditionally, we quantify the bias that remains when these indicators are\n\"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit\ngender indicators. As we demonstrate, differences in true positive rates\nbetween genders are correlated with existing gender imbalances in occupations,\nwhich may compound these imbalances.",
    "published_date": "2019-01-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09451v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09351v1",
    "title": "Automated Quality Control in Image Segmentation: Application to the UK Biobank Cardiac MR Imaging Study",
    "authors": [
      "Robert Robinson",
      "Vanya V. Valindria",
      "Wenjia Bai",
      "Ozan Oktay",
      "Bernhard Kainz",
      "Hideaki Suzuki",
      "Mihir M. Sanghvi",
      "Nay Aung",
      "Jos$é$ Miguel Paiva",
      "Filip Zemrak",
      "Kenneth Fung",
      "Elena Lukaschuk",
      "Aaron M. Lee",
      "Valentina Carapella",
      "Young Jin Kim",
      "Stefan K. Piechnik",
      "Stefan Neubauer",
      "Steffen E. Petersen",
      "Chris Page",
      "Paul M. Matthews",
      "Daniel Rueckert",
      "Ben Glocker"
    ],
    "author_ids": [],
    "abstract": "Background: The trend towards large-scale studies including population\nimaging poses new challenges in terms of quality control (QC). This is a\nparticular issue when automatic processing tools, e.g. image segmentation\nmethods, are employed to derive quantitative measures or biomarkers for later\nanalyses. Manual inspection and visual QC of each segmentation isn't feasible\nat large scale. However, it's important to be able to automatically detect when\na segmentation method fails so as to avoid inclusion of wrong measurements into\nsubsequent analyses which could lead to incorrect conclusions. Methods: To\novercome this challenge, we explore an approach for predicting segmentation\nquality based on Reverse Classification Accuracy, which enables us to\ndiscriminate between successful and failed segmentations on a per-cases basis.\nWe validate this approach on a new, large-scale manually-annotated set of 4,800\ncardiac magnetic resonance scans. We then apply our method to a large cohort of\n7,250 cardiac MRI on which we have performed manual QC. Results: We report\nresults used for predicting segmentation quality metrics including Dice\nSimilarity Coefficient (DSC) and surface-distance measures. As initial\nvalidation, we present data for 400 scans demonstrating 99% accuracy for\nclassifying low and high quality segmentations using predicted DSC scores. As\nfurther validation we show high correlation between real and predicted scores\nand 95% classification accuracy on 4,800 scans for which manual segmentations\nwere available. We mimic real-world application of the method on 7,250 cardiac\nMRI where we show good agreement between predicted quality metrics and manual\nvisual QC scores. Conclusions: We show that RCA has the potential for accurate\nand fully automatic segmentation QC on a per-case basis in the context of\nlarge-scale population imaging as in the UK Biobank Imaging Study.",
    "published_date": "2019-01-27T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09351v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09280v2",
    "title": "Points2Pix: 3D Point-Cloud to Image Translation using conditional Generative Adversarial Networks",
    "authors": [
      "Stefan Milz",
      "Martin Simon",
      "Kai Fischer",
      "Maximillian Pöpperl"
    ],
    "author_ids": [],
    "abstract": "We present the first approach for 3D point-cloud to image translation based\non conditional Generative Adversarial Networks (cGAN). The model handles\nmulti-modal information sources from different domains, i.e. raw point-sets and\nimages. The generator is capable of processing three conditions, whereas the\npoint-cloud is encoded as raw point-set and camera projection. An image\nbackground patch is used as constraint to bias environmental texturing. A\nglobal approximation function within the generator is directly applied on the\npoint-cloud (Point-Net). Hence, the representative learning model incorporates\nglobal 3D characteristics directly at the latent feature space. Conditions are\nused to bias the background and the viewpoint of the generated image. This\nopens up new ways in augmenting or texturing 3D data to aim the generation of\nfully individual images. We successfully evaluated our method on the Kitti and\nSunRGBD dataset with an outstanding object detection inception score.",
    "published_date": "2019-01-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09280v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09236v1",
    "title": "Coverage and Rate Analysis of Downlink Cellular Vehicle-to-Everything (C-V2X) Communication",
    "authors": [
      "Vishnu Vardhan Chetlur",
      "Harpreet S. Dhillon"
    ],
    "author_ids": [],
    "abstract": "In this paper, we present the downlink coverage and rate analysis of a\ncellular vehicle-to-everything (C-V2X) communication network where the\nlocations of vehicular nodes and road side units (RSUs) are modeled as Cox\nprocesses driven by a Poisson line process (PLP) and the locations of cellular\nmacro base stations (MBSs) are modeled as a 2D Poisson point process (PPP).\nAssuming a fixed selection bias and maximum average received power based\nassociation, we compute the probability with which a {\\em typical receiver}, an\narbitrarily chosen receiving node, connects to a vehicular node or an RSU and a\ncellular MBS. For this setup, we derive the signal-to-interference ratio\n(SIR)-based coverage probability of the typical receiver. One of the key\nchallenges in the computation of coverage probability stems from the inclusion\nof shadowing effects. As the standard procedure of interpreting the shadowing\neffects as random displacement of the location of nodes is not directly\napplicable to the Cox process, we propose an approximation of the spatial model\ninspired by the asymptotic behavior of the Cox process. Using this asymptotic\ncharacterization, we derive the coverage probability in terms of the Laplace\ntransform of interference power distribution. Further, we compute the downlink\nrate coverage of the typical receiver by characterizing the load on the serving\nvehicular nodes or RSUs and serving MBSs. We also provide several key design\ninsights by studying the trends in the coverage probability and rate coverage\nas a function of network parameters. We observe that the improvement in rate\ncoverage obtained by increasing the density of MBSs can be equivalently\nachieved by tuning the selection bias appropriately without the need to deploy\nadditional MBSs.",
    "published_date": "2019-01-26T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09236v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.11398v3",
    "title": "A study on general visual categorization of objects into animal and plant groups using global shape descriptors with a focus on category-specific deficits",
    "authors": [
      "Zahra Sadeghi"
    ],
    "author_ids": [],
    "abstract": "How do humans distinguish between general categories of objects? In a number\nof semantic category deficits, patients are good at making broad categorization\nbut are unable to remember fine and specific details. It has been well accepted\nthat general information about concepts is more robust to damages related to\nsemantic memory. Results from patients with semantic memory disorders\ndemonstrate the loss of ability in subcategory recognition. In this paper, we\nreview the behavioral evidence for category specific disorder and show that\ngeneral categories of animal and plant are visually distinguishable without\nprocessing textural information. To this aim, we utilize shape descriptors with\nan additional phase of feature learning. The results are evaluated with both\nsupervised and unsupervised learning mechanisms and confirm that the proposed\nmethod can effectively discriminates between animal and plant object categories\nin visual domain.",
    "published_date": "2019-01-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.11398v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09100v2",
    "title": "Communication Complexity of Estimating Correlations",
    "authors": [
      "Uri Hadar",
      "Jingbo Liu",
      "Yury Polyanskiy",
      "Ofer Shayevitz"
    ],
    "author_ids": [],
    "abstract": "We characterize the communication complexity of the following distributed\nestimation problem. Alice and Bob observe infinitely many iid copies of\n$\\rho$-correlated unit-variance (Gaussian or $\\pm1$ binary) random variables,\nwith unknown $\\rho\\in[-1,1]$. By interactively exchanging $k$ bits, Bob wants\nto produce an estimate $\\hat\\rho$ of $\\rho$. We show that the best possible\nperformance (optimized over interaction protocol $\\Pi$ and estimator $\\hat\n\\rho$) satisfies $\\inf_{\\Pi \\hat\\rho}\\sup_\\rho \\mathbb{E} [|\\rho-\\hat\\rho|^2] =\n\\tfrac{1}{k} (\\frac{1}{2 \\ln 2} + o(1))$. Curiously, the number of samples in\nour achievability scheme is exponential in $k$; by contrast, a naive scheme\nexchanging $k$ samples achieves the same $\\Omega(1/k)$ rate but with a\nsuboptimal prefactor. Our protocol achieving optimal performance is one-way\n(non-interactive). We also prove the $\\Omega(1/k)$ bound even when $\\rho$ is\nrestricted to any small open sub-interval of $[-1,1]$ (i.e. a local minimax\nlower bound). Our proof techniques rely on symmetric strong data-processing\ninequalities and various tensorization techniques from information-theoretic\ninteractive common-randomness extraction. Our results also imply an $\\Omega(n)$\nlower bound on the information complexity of the Gap-Hamming problem, for which\nwe show a direct information-theoretic proof.",
    "published_date": "2019-01-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09100v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09060v1",
    "title": "Learning Models from Data with Measurement Error: Tackling Underreporting",
    "authors": [
      "Roy Adams",
      "Yuelong Ji",
      "Xiaobin Wang",
      "Suchi Saria"
    ],
    "author_ids": [],
    "abstract": "Measurement error in observational datasets can lead to systematic bias in\ninferences based on these datasets. As studies based on observational data are\nincreasingly used to inform decisions with real-world impact, it is critical\nthat we develop a robust set of techniques for analyzing and adjusting for\nthese biases. In this paper we present a method for estimating the distribution\nof an outcome given a binary exposure that is subject to underreporting. Our\nmethod is based on a missing data view of the measurement error problem, where\nthe true exposure is treated as a latent variable that is marginalized out of a\njoint model. We prove three different conditions under which the outcome\ndistribution can still be identified from data containing only error-prone\nobservations of the exposure. We demonstrate this method on synthetic data and\nanalyze its sensitivity to near violations of the identifiability conditions.\nFinally, we use this method to estimate the effects of maternal smoking and\nopioid use during pregnancy on childhood obesity, two import problems from\npublic health. Using the proposed method, we estimate these effects using only\nsubject-reported drug use data and substantially refine the range of estimates\ngenerated by a sensitivity analysis-based approach. Further, the estimates\nproduced by our method are consistent with existing literature on both the\neffects of maternal smoking and the rate at which subjects underreport smoking.",
    "published_date": "2019-01-25T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09060v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.09017v2",
    "title": "Finding a Mediocre Player",
    "authors": [
      "Adrian Dumitrescu"
    ],
    "author_ids": [],
    "abstract": "Consider a totally ordered set $S$ of $n$ elements; as an example, a set of\ntennis players and their rankings. Further assume that their ranking is a total\norder and thus satisfies transitivity and anti-symmetry. Following Frances Yao\n(1974), an element (player) is said to be $(i,j)$-\\emph{mediocre} if it is\nneither among the top $i$ nor among the bottom $j$ elements of $S$. Finding a\nmediocre element is closely related to finding the median element. More than\n$40$ years ago, Yao suggested a very simple and elegant algorithm for finding\nan $(i,j)$-mediocre element: Pick $i+j+1$ elements arbitrarily and select the\n$(i+1)$-th largest among them. She also asked: \"Is this the best algorithm?\" No\none seems to have found a better algorithm ever since. We first provide a\ndeterministic algorithm that beats the worst-case comparison bound in Yao's\nalgorithm for a large range of values of $i$ (and corresponding suitable\n$j=j(i)$) even if the current best selection algorithm is used. We then repeat\nthe exercise for randomized algorithms; the average number of comparisons of\nour algorithm beats the average comparison bound in Yao's algorithm for another\nlarge range of values of $i$ (and corresponding suitable $j=j(i)$) even if the\nbest selection algorithm is used; the improvement is most notable in the\nsymmetric case $i=j$. Moreover, the tight bound obtained in the analysis of\nYao's algorithm allows us to give a definite answer for this class of\nalgorithms. In summary, we answer Yao's question as follows: (i)~\"Presently\nnot\" for deterministic algorithms and (ii)~\"Definitely not\" for randomized\nalgorithms. (In fairness, it should be said however that Yao posed the question\nin the context of deterministic algorithms.)",
    "published_date": "2019-01-25T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "math.CO"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.09017v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.08974v4",
    "title": "On the cross-validation bias due to unsupervised pre-processing",
    "authors": [
      "Amit Moscovich",
      "Saharon Rosset"
    ],
    "author_ids": [],
    "abstract": "Cross-validation is the de facto standard for predictive model evaluation and\nselection. In proper use, it provides an unbiased estimate of a model's\npredictive performance. However, data sets often undergo various forms of\ndata-dependent preprocessing, such as mean-centering, rescaling, dimensionality\nreduction, and outlier removal. It is often believed that such preprocessing\nstages, if done in an unsupervised manner (that does not incorporate the class\nlabels or response values) are generally safe to do prior to cross-validation.\n  In this paper, we study three commonly-practiced preprocessing procedures\nprior to a regression analysis: (i) variance-based feature selection; (ii)\ngrouping of rare categorical features; and (iii) feature rescaling. We\ndemonstrate that unsupervised preprocessing can, in fact, introduce a\nsubstantial bias into cross-validation estimates and potentially hurt model\nselection. This bias may be either positive or negative and its exact magnitude\ndepends on all the parameters of the problem in an intricate manner. Further\nresearch is needed to understand the real-world impact of this bias across\ndifferent application domains, particularly when dealing with small sample\nsizes and high-dimensional data.",
    "published_date": "2019-01-25T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML",
      "62-07",
      "G.3"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08974v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.08668v2",
    "title": "Guarantees for Spectral Clustering with Fairness Constraints",
    "authors": [
      "Matthäus Kleindessner",
      "Samira Samadi",
      "Pranjal Awasthi",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "Given the widespread popularity of spectral clustering (SC) for partitioning\ngraph data, we study a version of constrained SC in which we try to incorporate\nthe fairness notion proposed by Chierichetti et al. (2017). According to this\nnotion, a clustering is fair if every demographic group is approximately\nproportionally represented in each cluster. To this end, we develop variants of\nboth normalized and unnormalized constrained SC and show that they help find\nfairer clusterings on both synthetic and real data. We also provide a rigorous\ntheoretical analysis of our algorithms on a natural variant of the stochastic\nblock model, where $h$ groups have strong inter-group connectivity, but also\nexhibit a \"natural\" clustering structure which is fair. We prove that our\nalgorithms can recover this fair clustering with high probability.",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08668v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.08665v1",
    "title": "Fairness risk measures",
    "authors": [
      "Robert C. Williamson",
      "Aditya Krishna Menon"
    ],
    "author_ids": [],
    "abstract": "Ensuring that classifiers are non-discriminatory or fair with respect to a\nsensitive feature (e.g., race or gender) is a topical problem. Progress in this\ntask requires fixing a definition of fairness, and there have been several\nproposals in this regard over the past few years. Several of these, however,\nassume either binary sensitive features (thus precluding categorical or\nreal-valued sensitive groups), or result in non-convex objectives (thus\nadversely affecting the optimisation landscape). In this paper, we propose a\nnew definition of fairness that generalises some existing proposals, while\nallowing for generic sensitive features and resulting in a convex objective.\nThe key idea is to enforce that the expected losses (or risks) across each\nsubgroup induced by the sensitive feature are commensurate. We show how this\nrelates to the rich literature on risk measures from mathematical finance. As a\nspecial case, this leads to a new convex fairness-aware objective based on\nminimising the conditional value at risk (CVaR).",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08665v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.08628v2",
    "title": "Fair k-Center Clustering for Data Summarization",
    "authors": [
      "Matthäus Kleindessner",
      "Pranjal Awasthi",
      "Jamie Morgenstern"
    ],
    "author_ids": [],
    "abstract": "In data summarization we want to choose $k$ prototypes in order to summarize\na data set. We study a setting where the data set comprises several demographic\ngroups and we are restricted to choose $k_i$ prototypes belonging to group $i$.\nA common approach to the problem without the fairness constraint is to optimize\na centroid-based clustering objective such as $k$-center. A natural extension\nthen is to incorporate the fairness constraint into the clustering problem.\nExisting algorithms for doing so run in time super-quadratic in the size of the\ndata set, which is in contrast to the standard $k$-center problem being\napproximable in linear time. In this paper, we resolve this gap by providing a\nsimple approximation algorithm for the $k$-center problem under the fairness\nconstraint with running time linear in the size of the data set and $k$. If the\nnumber of demographic groups is small, the approximation guarantee of our\nalgorithm only incurs a constant-factor overhead.",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08628v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.08568v2",
    "title": "Algorithms for Fairness in Sequential Decision Making",
    "authors": [
      "Min Wen",
      "Osbert Bastani",
      "Ufuk Topcu"
    ],
    "author_ids": [],
    "abstract": "It has recently been shown that if feedback effects of decisions are ignored,\nthen imposing fairness constraints such as demographic parity or equality of\nopportunity can actually exacerbate unfairness. We propose to address this\nchallenge by modeling feedback effects as Markov decision processes (MDPs).\nFirst, we propose analogs of fairness properties for the MDP setting. Second,\nwe propose algorithms for learning fair decision-making policies for MDPs.\nFinally, we demonstrate the need to account for dynamical effects using\nsimulations on a loan applicant MDP.",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08568v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.08463v2",
    "title": "Almost Envy-Freeness in Group Resource Allocation",
    "authors": [
      "Maria Kyropoulou",
      "Warut Suksompong",
      "Alexandros A. Voudouris"
    ],
    "author_ids": [],
    "abstract": "We study the problem of fairly allocating indivisible goods between groups of\nagents using the recently introduced relaxations of envy-freeness. We consider\nthe existence of fair allocations under different assumptions on the valuations\nof the agents. In particular, our results cover cases of arbitrary monotonic,\nresponsive, and additive valuations, while for the case of binary valuations we\nfully characterize the cardinalities of two groups of agents for which a fair\nallocation can be guaranteed with respect to both envy-freeness up to one good\n(EF1) and envy-freeness up to any good (EFX). Moreover, we introduce a new\nmodel where the agents are not partitioned into groups in advance, but instead\nthe partition can be chosen in conjunction with the allocation of the goods. In\nthis model, we show that for agents with arbitrary monotonic valuations, there\nis always a partition of the agents into two groups of any given sizes along\nwith an EF1 allocation of the goods. We also provide an extension of this\nresult to any number of groups.",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08463v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.08291v2",
    "title": "Faking Fairness via Stealthily Biased Sampling",
    "authors": [
      "Kazuto Fukuchi",
      "Satoshi Hara",
      "Takanori Maehara"
    ],
    "author_ids": [],
    "abstract": "Auditing fairness of decision-makers is now in high demand. To respond to\nthis social demand, several fairness auditing tools have been developed. The\nfocus of this study is to raise an awareness of the risk of malicious\ndecision-makers who fake fairness by abusing the auditing tools and thereby\ndeceiving the social communities. The question is whether such a fraud of the\ndecision-maker is detectable so that the society can avoid the risk of fake\nfairness. In this study, we answer this question negatively. We specifically\nput our focus on a situation where the decision-maker publishes a benchmark\ndataset as the evidence of his/her fairness and attempts to deceive a person\nwho uses an auditing tool that computes a fairness metric. To assess the\n(un)detectability of the fraud, we explicitly construct an algorithm, the\nstealthily biased sampling, that can deliberately construct an evil benchmark\ndataset via subsampling. We show that the fraud made by the stealthily based\nsampling is indeed difficult to detect both theoretically and empirically.",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08291v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.08221v1",
    "title": "When is it right and good for an intelligent autonomous vehicle to take over control (and hand it back)?",
    "authors": [
      "Ajit Narayanan"
    ],
    "author_ids": [],
    "abstract": "There is much debate in machine ethics about the most appropriate way to\nintroduce ethical reasoning capabilities into intelligent autonomous machines.\nRecent incidents involving autonomous vehicles in which humans have been killed\nor injured have raised questions about how we ensure that such vehicles have an\nethical dimension to their behaviour and are therefore trustworthy. The main\nproblem is that hardwiring such machines with rules not to cause harm or damage\nis not consistent with the notion of autonomy and intelligence. Also, such\nethical hardwiring does not leave intelligent autonomous machines with any\ncourse of action if they encounter situations or dilemmas for which they are\nnot programmed or where some harm is caused no matter what course of action is\ntaken. Teaching machines so that they learn ethics may also be problematic\ngiven recent findings in machine learning that machines pick up the prejudices\nand biases embedded in their learning algorithms or data. This paper describes\na fuzzy reasoning approach to machine ethics. The paper shows how it is\npossible for an ethics architecture to reason when taking over from a human\ndriver is morally justified. The design behind such an ethical reasoner is also\napplied to an ethical dilemma resolution case. One major advantage of the\napproach is that the ethical reasoner can generate its own data for learning\nmoral rules (hence, autometric) and thereby reduce the possibility of picking\nup human biases and prejudices. The results show that a new type of\nmetric-based ethics appropriate for autonomous intelligent machines is feasible\nand that our current concept of ethical reasoning being largely qualitative in\nnature may need revising if want to construct future autonomous machines that\nhave an ethical dimension to their reasoning so that they become moral\nmachines.",
    "published_date": "2019-01-24T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08221v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07858v3",
    "title": "Evolving the pulmonary nodules diagnosis from classical approaches to deep learning aided decision support: three decades development course and future prospect",
    "authors": [
      "Bo Liu",
      "Wenhao Chi",
      "Xinran Li",
      "Peng Li",
      "Wenhua Liang",
      "Haiping Liu",
      "Wei Wang",
      "Jianxing He"
    ],
    "author_ids": [],
    "abstract": "Lung cancer is the commonest cause of cancer deaths worldwide, and its\nmortality can be reduced significantly by performing early diagnosis and\nscreening. Since the 1960s, driven by the pressing needs to accurately and\neffectively interpret the massive volume of chest images generated daily,\ncomputer-assisted diagnosis of pulmonary nodule has opened up new opportunities\nto relax the limitation from physicians' subjectivity, experiences and fatigue.\nAnd the fair access to the reliable and affordable computer-assisted diagnosis\nwill fight the inequalities in incidence and mortality between populations. It\nhas been witnessed that significant and remarkable advances have been achieved\nsince the 1980s, and consistent endeavors have been exerted to deal with the\ngrand challenges on how to accurately detect the pulmonary nodules with high\nsensitivity at low false-positives rate as well as on how to precisely\ndifferentiate between benign and malignant nodules. There is a lack of\ncomprehensive examination of the techniques' development which is evolving the\npulmonary nodules diagnosis from classical approaches to machine\nlearning-assisted decision support. The main goal of this investigation is to\nprovide a comprehensive state-of-the-art review of the computer-assisted\nnodules detection and benign-malignant classification techniques developed over\n3 decades, which have evolved from the complicated ad hoc analysis pipeline of\nconventional approaches to the simplified seamlessly integrated deep learning\ntechniques. This review also identifies challenges and highlights opportunities\nfor future work in learning models, learning algorithms and enhancement schemes\nfor bridging current state to future prospect and satisfying future demand.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07858v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07841v2",
    "title": "Error estimates of penalty schemes for quasi-variational inequalities arising from impulse control problems",
    "authors": [
      "Christoph Reisinger",
      "Yufei Zhang"
    ],
    "author_ids": [],
    "abstract": "This paper proposes penalty schemes for a class of weakly coupled systems of\nHamilton-Jacobi-Bellman quasi-variational inequalities (HJBQVIs) arising from\nstochastic hybrid control problems of regime-switching models with both\ncontinuous and impulse controls. We show that the solutions of the penalized\nequations converge monotonically to those of the HJBQVIs. We further establish\nthat the schemes are half-order accurate for HJBQVIs with Lipschitz\ncoefficients, and first-order accurate for equations with more regular\ncoefficients. Moreover, we construct the action regions and optimal impulse\ncontrols based on the error estimates and the penalized solutions. The penalty\nschemes and convergence results are then extended to HJBQVIs with possibly\nnegative impulse costs. We also demonstrate the convergence of monotone\ndiscretizations of the penalized equations, and establish that policy iteration\napplied to the discrete equation is monotonically convergent with an arbitrary\ninitial guess in an infinite dimensional setting. Numerical examples for\ninfinite-horizon optimal switching problems are presented to illustrate the\neffectiveness of the penalty schemes over the conventional direct control\nscheme.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07841v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.07719v1",
    "title": "On the Fundamental Limits of Multi-user Scheduling under Short-term Fairness Constraints",
    "authors": [
      "Shahram Shahsavari",
      "Farhad Shirani",
      "Elza Erkip"
    ],
    "author_ids": [],
    "abstract": "In the conventional information theoretic analysis of multiterminal\ncommunication scenarios, it is often assumed that all of the distributed\nterminals use the communication channel simultaneously. However, in practical\nwireless communication systems - due to restricted computation complexity at\nnetwork terminals - a limited number of users can be activated either in uplink\nor downlink simultaneously. This necessitates the design of a scheduler which\ndetermines the set of active users at each time-slot. A well designed scheduler\nmaximizes the average system utility subject to a set of fairness criteria,\nwhich must be met in a limited window-length to avoid long starvation periods.\nIn this work, scheduling under short-term temporal fairness constraints is\nconsidered. The objective is to maximize the average system utility such that\nthe fraction of the time-slots that each user is activated is within desired\nupper and lower bounds in the fairness window-length. The set of feasible\nwindow-lengths is characterized as a function of system parameters. It is shown\nthat the optimal system utility is non-monotonic and super-additive in\nwindow-length. Furthermore, a scheduling strategy is proposed which satisfies\nshort-term fairness constraints for arbitrary window-lengths, and achieves\noptimal average system utility as the window-length is increased\nasymptotically. Numerical simulations are provided to verify the results.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07719v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.07711v1",
    "title": "Max-margin Class Imbalanced Learning with Gaussian Affinity",
    "authors": [
      "Munawar Hayat",
      "Salman Khan",
      "Waqas Zamir",
      "Jianbing Shen",
      "Ling Shao"
    ],
    "author_ids": [],
    "abstract": "Real-world object classes appear in imbalanced ratios. This poses a\nsignificant challenge for classifiers which get biased towards frequent\nclasses. We hypothesize that improving the generalization capability of a\nclassifier should improve learning on imbalanced datasets. Here, we introduce\nthe first hybrid loss function that jointly performs classification and\nclustering in a single formulation. Our approach is based on an `affinity\nmeasure' in Euclidean space that leads to the following benefits: (1) direct\nenforcement of maximum margin constraints on classification boundaries, (2) a\ntractable way to ensure uniformly spaced and equidistant cluster centers, (3)\nflexibility to learn multiple class prototypes to support diversity and\ndiscriminability in feature space. Our extensive experiments demonstrate the\nsignificant performance improvements on visual classification and verification\ntasks on multiple imbalanced datasets. The proposed loss can easily be plugged\nin any deep architecture as a differentiable block and demonstrates robustness\nagainst different levels of data imbalance and corrupted labels.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07711v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07708v4",
    "title": "Cascade Submodular Maximization: Question Selection and Sequencing in Online Personality Quiz",
    "authors": [
      "Shaojie Tang",
      "Jing Yuan"
    ],
    "author_ids": [],
    "abstract": "Personality quiz is a powerful tool that enables costumer segmentation by\nactively asking them questions, and marketers are using it as an effective\nmethod of generating leads and increasing e-commerce sales. In this paper, we\nstudy the problem of how to select and sequence a group of quiz questions so as\nto optimize the quality of customer segmentation. We assume that the customer\nwill sequentially scan the list of questions. After reading a question, the\ncustomer makes two, possibly correlated, random decisions: 1) she first decides\nwhether to answer this question or not, and then 2) decides whether to continue\nreading the next question or not. We further assume that the utility of\nquestions that have been answered can be captured by a monotone and submodular\nfunction. In general, our problem falls into the category of non-adaptive\nactive learning based customer profiling. Note that under the our model, the\nprobability of a question being answered depends on the location of that\nquestion, as well as the set of other questions placed ahead of that question,\nthis makes our problem fundamentally different from existing studies on\nsubmodular optimization. We develop a series of question selection and\nsequencing strategies with provable performance bound. Although we focus on the\napplication of quiz design in this paper, our results apply to a broad range of\napplications, including assortment optimization with position bias effect.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07708v4",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07694v1",
    "title": "Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment",
    "authors": [
      "Jonathan Dodge",
      "Q. Vera Liao",
      "Yunfeng Zhang",
      "Rachel K. E. Bellamy",
      "Casey Dugan"
    ],
    "author_ids": [],
    "abstract": "Ensuring fairness of machine learning systems is a human-in-the-loop process.\nIt relies on developers, users, and the general public to identify fairness\nproblems and make improvements. To facilitate the process we need effective,\nunbiased, and user-friendly explanations that people can confidently rely on.\nTowards that end, we conducted an empirical study with four types of\nprogrammatically generated explanations to understand how they impact people's\nfairness judgments of ML systems. With an experiment involving more than 160\nMechanical Turk workers, we show that: 1) Certain explanations are considered\ninherently less fair, while others can enhance people's confidence in the\nfairness of the algorithm; 2) Different fairness problems--such as model-wide\nfairness issues versus case-specific fairness discrepancies--may be more\neffectively exposed through different styles of explanation; 3) Individual\ndifferences, including prior positions and judgment criteria of algorithmic\nfairness, impact how people react to different styles of explanation. We\nconclude with a discussion on providing personalized and adaptive explanations\nto support fairness judgments of ML systems.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07694v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07683v1",
    "title": "Class Activation Map Generation by Representative Class Selection and Multi-Layer Feature Fusion",
    "authors": [
      "Fanman Meng",
      "Kaixu Huang",
      "Hongliang Li",
      "Qingbo Wu"
    ],
    "author_ids": [],
    "abstract": "Existing method generates class activation map (CAM) by a set of fixed\nclasses (i.e., using all the classes), while the discriminative cues between\nclass pairs are not considered. Note that activation maps by considering\ndifferent class pair are complementary, and therefore can provide more\ndiscriminative cues to overcome the shortcoming of the existing CAM generation\nthat the highlighted regions are usually local part regions rather than global\nobject regions due to the lack of object cues. In this paper, we generate CAM\nby using a few of representative classes, with aim of extracting more\ndiscriminative cues by considering each class pair to obtain CAM more globally.\nThe advantages are twofold. Firstly, the representative classes are able to\nobtain activation regions that are complementary to each other, and therefore\nleads to generating activation map more accurately. Secondly, we only need to\nconsider a small number of representative classes, making the CAM generation\nsuitable for small networks. We propose a clustering based method to select the\nrepresentative classes. Multiple binary classification models rather than a\nmultiple class classification model are used to generate the CAM. Moreover, we\npropose a multi-layer fusion based CAM generation method to simultaneously\ncombine high-level semantic features and low-level detail features. We validate\nthe proposed method on the PASCAL VOC and COCO database in terms of\nsegmentation groundtruth. Various networks such as classical network\n(Resnet-50, Resent-101 and Resnet-152) and small network (VGG-19, Resnet-18 and\nMobilenet) are considered. Experimental results show that the proposed method\nimproves the CAM generation obviously.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07683v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07656v1",
    "title": "Attenuating Bias in Word Vectors",
    "authors": [
      "Sunipa Dev",
      "Jeff Phillips"
    ],
    "author_ids": [],
    "abstract": "Word vector representations are well developed tools for various NLP and\nMachine Learning tasks and are known to retain significant semantic and\nsyntactic structure of languages. But they are prone to carrying and amplifying\nbias which can perpetrate discrimination in various applications. In this work,\nwe explore new simple ways to detect the most stereotypically gendered words in\nan embedding and remove the bias from them. We verify how names are masked\ncarriers of gender bias and then use that as a tool to attenuate bias in\nembeddings. Further, we extend this property of names to show how names can be\nused to detect other types of bias in the embeddings such as bias based on\nrace, ethnicity, and age.",
    "published_date": "2019-01-23T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07656v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07675v2",
    "title": "A New CGAN Technique for Constrained Topology Design Optimization",
    "authors": [
      "M. -H. Herman Shen",
      "Liang Chen"
    ],
    "author_ids": [],
    "abstract": "This paper presents a new conditional GAN (named convex relaxing CGAN or\ncrCGAN) to replicate the conventional constrained topology optimization\nalgorithms in an extremely effective and efficient process. The proposed crCGAN\nconsists of a generator and a discriminator, both of which are deep\nconvolutional neural networks (CNN) and the topology design constraint can be\nconditionally set to both the generator and discriminator. In order to improve\nthe training efficiency and accuracy due to the dependency between the training\nimages and the condition, a variety of crCGAN formulation are introduced to\nrelax the non-convex design space. These new formulations were evaluated and\nvalidated via a series of comprehensive experiments. Moreover, a minibatch\ndiscrimination technique was introduced in the crCGAN training process to\nstabilize the convergence and avoid the mode collapse problems. Additional\nverifications were conducted using the state-of-the-art MNIST digits and\nCIFAR-10 images conditioned by class labels. The experimental evaluations\nclearly reveal that the new objective formulation with the minibatch\ndiscrimination training provides not only the accuracy but also the consistency\nof the designs.",
    "published_date": "2019-01-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07675v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07555v4",
    "title": "Managing Popularity Bias in Recommender Systems with Personalized Re-ranking",
    "authors": [
      "Himan Abdollahpouri",
      "Robin Burke",
      "Bamshad Mobasher"
    ],
    "author_ids": [],
    "abstract": "Many recommender systems suffer from popularity bias: popular items are\nrecommended frequently while less popular, niche products, are recommended\nrarely or not at all. However, recommending the ignored products in the `long\ntail' is critical for businesses as they are less likely to be discovered. In\nthis paper, we introduce a personalized diversification re-ranking approach to\nincrease the representation of less popular items in recommendations while\nmaintaining acceptable recommendation accuracy. Our approach is a\npost-processing step that can be applied to the output of any recommender\nsystem. We show that our approach is capable of managing popularity bias more\neffectively, compared with an existing method based on regularization. We also\nexamine both new and existing metrics to measure the coverage of long-tail\nitems in the recommendation.",
    "published_date": "2019-01-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07555v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.07169v1",
    "title": "Energy Confused Adversarial Metric Learning for Zero-Shot Image Retrieval and Clustering",
    "authors": [
      "Binghui Chen",
      "Weihong Deng"
    ],
    "author_ids": [],
    "abstract": "Deep metric learning has been widely applied in many computer vision tasks,\nand recently, it is more attractive in \\emph{zero-shot image retrieval and\nclustering}(ZSRC) where a good embedding is requested such that the unseen\nclasses can be distinguished well. Most existing works deem this 'good'\nembedding just to be the discriminative one and thus race to devise powerful\nmetric objectives or hard-sample mining strategies for leaning discriminative\nembedding. However, in this paper, we first emphasize that the generalization\nability is a core ingredient of this 'good' embedding as well and largely\naffects the metric performance in zero-shot settings as a matter of fact. Then,\nwe propose the Energy Confused Adversarial Metric Learning(ECAML) framework to\nexplicitly optimize a robust metric. It is mainly achieved by introducing an\ninteresting Energy Confusion regularization term, which daringly breaks away\nfrom the traditional metric learning idea of discriminative objective devising,\nand seeks to 'confuse' the learned model so as to encourage its generalization\nability by reducing overfitting on the seen classes. We train this confusion\nterm together with the conventional metric objective in an adversarial manner.\nAlthough it seems weird to 'confuse' the network, we show that our ECAML indeed\nserves as an efficient regularization technique for metric learning and is\napplicable to various conventional metric methods. This paper empirically and\nexperimentally demonstrates the importance of learning embedding with good\ngeneralization, achieving state-of-the-art performances on the popular CUB,\nCARS, Stanford Online Products and In-Shop datasets for ZSRC tasks.\n\\textcolor[rgb]{1, 0, 0}{Code available at http://www.bhchen.cn/}.",
    "published_date": "2019-01-22T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07169v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07002v2",
    "title": "Error-Correcting Neural Sequence Prediction",
    "authors": [
      "James O' Neill",
      "Danushka Bollegala"
    ],
    "author_ids": [],
    "abstract": "We propose a novel neural sequence prediction method based on\n\\textit{error-correcting output codes} that avoids exact softmax normalization\nand allows for a tradeoff between speed and performance. Instead of minimizing\nmeasures between the predicted probability distribution and true distribution,\nwe use error-correcting codes to represent both predictions and outputs.\nSecondly, we propose multiple ways to improve accuracy and convergence rates by\nmaximizing the separability between codes that correspond to classes\nproportional to word embedding similarities. Lastly, we introduce our main\ncontribution called \\textit{Latent Variable Mixture Sampling}, a technique that\nis used to mitigate exposure bias, which can be integrated into training latent\nvariable-based neural sequence predictors such as ECOC. This involves mixing\nthe latent codes of past predictions and past targets in one of two ways: (1)\naccording to a predefined sampling schedule or (2) a differentiable sampling\nprocedure whereby the mixing probability is learned throughout training by\nreplacing the greedy argmax operation with a smooth approximation. ECOC-NSP\nleads to consistent improvements on language modelling datasets and the\nproposed Latent Variable mixture sampling methods are found to perform well for\ntext generation tasks such as image captioning.",
    "published_date": "2019-01-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07002v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06905v2",
    "title": "Equality in the Matrix Entropy-Power Inequality and Blind Separation of Real and Complex sources",
    "authors": [
      "Olivier Rioul",
      "Ram Zamir"
    ],
    "author_ids": [],
    "abstract": "The matrix version of the entropy-power inequality for real or complex\ncoefficients and variables is proved using a transportation argument that\neasily settles the equality case. An application to blind source extraction is\ngiven.",
    "published_date": "2019-01-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06905v2",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.06852v5",
    "title": "Maximum Likelihood with Bias-Corrected Calibration is Hard-To-Beat at Label Shift Adaptation",
    "authors": [
      "Amr Alexandari",
      "Anshul Kundaje",
      "Avanti Shrikumar"
    ],
    "author_ids": [],
    "abstract": "Label shift refers to the phenomenon where the prior class probability p(y)\nchanges between the training and test distributions, while the conditional\nprobability p(x|y) stays fixed. Label shift arises in settings like medical\ndiagnosis, where a classifier trained to predict disease given symptoms must be\nadapted to scenarios where the baseline prevalence of the disease is different.\nGiven estimates of p(y|x) from a predictive model, Saerens et al. proposed an\nefficient maximum likelihood algorithm to correct for label shift that does not\nrequire model retraining, but a limiting assumption of this algorithm is that\np(y|x) is calibrated, which is not true of modern neural networks. Recently,\nBlack Box Shift Learning (BBSL) and Regularized Learning under Label Shifts\n(RLLS) have emerged as state-of-the-art techniques to cope with label shift\nwhen a classifier does not output calibrated probabilities, but both methods\nrequire model retraining with importance weights and neither has been\nbenchmarked against maximum likelihood. Here we (1) show that combining maximum\nlikelihood with a type of calibration we call bias-corrected calibration\noutperforms both BBSL and RLLS across diverse datasets and distribution shifts,\n(2) prove that the maximum likelihood objective is concave, and (3) introduce a\nprincipled strategy for estimating source-domain priors that improves\nrobustness to poor calibration. This work demonstrates that the maximum\nlikelihood with appropriate calibration is a formidable and efficient baseline\nfor label shift adaptation; notebooks reproducing experiments available at\nhttps://github.com/kundajelab/labelshiftexperiments",
    "published_date": "2019-01-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06852v5",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06845v1",
    "title": "Signed Network Structural Analysis and Applications with a Focus on Balance Theory",
    "authors": [
      "Samin Aref"
    ],
    "author_ids": [],
    "abstract": "We analyse signed networks from the perspective of balance theory which\npredicts structural balance as a global structure for signed social networks\nthat represent groups of friends and enemies. The scarcity of balanced networks\nencouraged us to define the notion of partial balance in order to quantify the\nextent to which a network is balanced. We evaluate several numerical measures\nof partial balance and recommend using the frustration index, a measure that\nsatisfies key axiomatic properties and allows us to analyse graphs based on\ntheir levels of partial balance. The exact algorithms used in the literature to\ncompute the frustration index, also called the line index of balance, are not\nscalable and cannot process graphs with a few hundred edges. We formulate\ncomputing the frustration index as a graph optimisation problem to find the\nminimum number of edges whose removal results in a balanced network given\nbinary decision variables associated with graph nodes and edges. We use our\nfirst optimisation model to analyse graphs with up to 3000 edges. Reformulating\nthe optimisation problem, we develop three more efficient binary linear\nprogramming models. Equipping the models with valid inequalities and\nprioritised branching as speed-up techniques allows us to process graphs with\n15000 edges on inexpensive hardware. Besides making exact computations possible\nfor large graphs, we show that our models outperform heuristics and\napproximation algorithms suggested in the literature by orders of magnitude. We\nextend the concepts of balance and frustration in signed networks to\napplications beyond the classic friend-enemy interpretation of balance theory\nin social context. Using a high-performance computer, we analyse graphs with up\nto 100000 edges to investigate a range of applications from biology and\nchemistry to finance, international relations, and physics.",
    "published_date": "2019-01-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "math.OC",
      "physics.soc-ph",
      "05C22, 05C15, 90B10, 90C09, 90C10, 90C11, 90C20, 90C35, 90C57,\n  90C90, 91D30"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06845v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.06789v1",
    "title": "Dual Loomis-Whitney inequalities via information theory",
    "authors": [
      "Jing Hao",
      "Varun Jog"
    ],
    "author_ids": [],
    "abstract": "We establish lower bounds on the volume and the surface area of a geometric\nbody using the size of its slices along different directions. In the first part\nof the paper, we derive volume bounds for convex bodies using generalized\nsubadditivity properties of entropy combined with entropy bounds for\nlog-concave random variables. In the second part, we investigate a new notion\nof Fisher information which we call the $L_1$-Fisher information, and show that\ncertain superadditivity properties of the $L_1$-Fisher information lead to\nlower bounds for the surface areas of polyconvex sets in terms of its slices.",
    "published_date": "2019-01-21T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06789v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.06631v3",
    "title": "CommunityGAN: Community Detection with Generative Adversarial Nets",
    "authors": [
      "Yuting Jia",
      "Qinqin Zhang",
      "Weinan Zhang",
      "Xinbing Wang"
    ],
    "author_ids": [],
    "abstract": "Community detection refers to the task of discovering groups of vertices\nsharing similar properties or functions so as to understand the network data.\nWith the recent development of deep learning, graph representation learning\ntechniques are also utilized for community detection. However, the communities\ncan only be inferred by applying clustering algorithms based on learned vertex\nembeddings. These general cluster algorithms like K-means and Gaussian Mixture\nModel cannot output much overlapped communities, which have been proved to be\nvery common in many real-world networks. In this paper, we propose\nCommunityGAN, a novel community detection framework that jointly solves\noverlapping community detection and graph representation learning. First,\nunlike the embedding of conventional graph representation learning algorithms\nwhere the vector entry values have no specific meanings, the embedding of\nCommunityGAN indicates the membership strength of vertices to communities.\nSecond, a specifically designed Generative Adversarial Net (GAN) is adopted to\noptimize such embedding. Through the minimax competition between the\nmotif-level generator and discriminator, both of them can alternatively and\niteratively boost their performance and finally output a better community\nstructure. Extensive experiments on synthetic data and real-world tasks\ndemonstrate that CommunityGAN achieves substantial community detection\nperformance gains over the state-of-the-art methods.",
    "published_date": "2019-01-20T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06631v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06543v2",
    "title": "MOROCO: The Moldavian and Romanian Dialectal Corpus",
    "authors": [
      "Andrei M. Butnaru",
      "Radu Tudor Ionescu"
    ],
    "author_ids": [],
    "abstract": "In this work, we introduce the MOldavian and ROmanian Dialectal COrpus\n(MOROCO), which is freely available for download at\nhttps://github.com/butnaruandrei/MOROCO. The corpus contains 33564 samples of\ntext (with over 10 million tokens) collected from the news domain. The samples\nbelong to one of the following six topics: culture, finance, politics, science,\nsports and tech. The data set is divided into 21719 samples for training, 5921\nsamples for validation and another 5924 samples for testing. For each sample,\nwe provide corresponding dialectal and category labels. This allows us to\nperform empirical studies on several classification tasks such as (i) binary\ndiscrimination of Moldavian versus Romanian text samples, (ii) intra-dialect\nmulti-class categorization by topic and (iii) cross-dialect multi-class\ncategorization by topic. We perform experiments using a shallow approach based\non string kernels, as well as a novel deep approach based on character-level\nconvolutional neural networks containing Squeeze-and-Excitation blocks. We also\npresent and analyze the most discriminative features of our best performing\nmodel, before and after named entity removal.",
    "published_date": "2019-01-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06543v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06512v4",
    "title": "A Geometric Method for Passivation and Cooperative Control of Equilibrium-Independent Passivity-Short Systems",
    "authors": [
      "Miel Sharf",
      "Anoop Jain",
      "Daniel Zelazo"
    ],
    "author_ids": [],
    "abstract": "Equilibrium-independent passive-short (EIPS) systems are a class of systems\nthat satisfy a passivity-like dissipation inequality with respect to any forced\nequilibria with non-positive passivity indices. This paper presents a geometric\napproach for finding a passivizing transformation for such systems, relying on\ntheir steady-state input-output relation and the notion of projective quadratic\ninequalities (PQIs). We show that PQIs arise naturally from passivity-shortage\ncharacteristics of an EIPS system, and the set of their solutions can be\nexplicitly expressed. We leverage this connection to build an input-output\nmapping that transforms the steady-state input-output relation to a monotone\nrelation, and show that the same mapping passivizes the EIPS system. We show\nthat the proposed transformation can be implemented through a combination of\nfeedback, feed-through, post- and pre-multiplication gains. Furthermore, we\nconsider an application of the presented passivation scheme for the analysis of\nnetworks comprised of EIPS systems. Numerous examples are provided to\nillustrate the theoretical findings.",
    "published_date": "2019-01-19T00:00:00",
    "year": 2019,
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06512v4",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.06474v1",
    "title": "Deep Representation Learning Characterized by Inter-class Separation for Image Clustering",
    "authors": [
      "Dipanjan Das",
      "Ratul Ghosh",
      "Brojeshwar Bhowmick"
    ],
    "author_ids": [],
    "abstract": "Despite significant advances in clustering methods in recent years, the\noutcome of clustering of a natural image dataset is still unsatisfactory due to\ntwo important drawbacks. Firstly, clustering of images needs a good feature\nrepresentation of an image and secondly, we need a robust method which can\ndiscriminate these features for making them belonging to different clusters\nsuch that intra-class variance is less and inter-class variance is high. Often\nthese two aspects are dealt with independently and thus the features are not\nsufficient enough to partition the data meaningfully. In this paper, we propose\na method where we discover these features required for the separation of the\nimages using deep autoencoder. Our method learns the image representation\nfeatures automatically for the purpose of clustering and also select a coherent\nimage and an incoherent image simultaneously for a given image so that the\nfeature representation learning can learn better discriminative features for\ngrouping the similar images in a cluster and at the same time separating the\ndissimilar images across clusters. Experiment results show that our method\nproduces significantly better result than the state-of-the-art methods and we\nalso show that our method is more generalized across different dataset without\nusing any pre-trained model like other existing methods.",
    "published_date": "2019-01-19T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06474v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06230v2",
    "title": "Computing large market equilibria using abstractions",
    "authors": [
      "Christian Kroer",
      "Alexander Peysakhovich",
      "Eric Sodomka",
      "Nicolas E. Stier-Moses"
    ],
    "author_ids": [],
    "abstract": "Computing market equilibria is an important practical problem for market\ndesign, for example in fair division of items. However, computing equilibria\nrequires large amounts of information (typically the valuation of every buyer\nfor every item) and computing power. We consider ameliorating these issues by\napplying a method used for solving complex games: constructing a coarsened\nabstraction of a given market, solving for the equilibrium in the abstraction,\nand lifting the prices and allocations back to the original market. We show how\nto bound important quantities such as regret, envy, Nash social welfare, Pareto\noptimality, and maximin share/proportionality when the abstracted prices and\nallocations are used in place of the real equilibrium. We then study two\nabstraction methods of interest for practitioners: (1) filling in unknown\nvaluations using techniques from matrix completion, (2) reducing the problem\nsize by aggregating groups of buyers/items into smaller numbers of\nrepresentative buyers/items and solving for equilibrium in this coarsened\nmarket. We find that in real data allocations/prices that are relatively close\nto equilibria can be computed from even very coarse abstractions.",
    "published_date": "2019-01-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06230v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06153v1",
    "title": "Infeasibility and structural bias in Differential Evolution",
    "authors": [
      "Fabio Caraffini",
      "Anna V. Kononova",
      "David Corne"
    ],
    "author_ids": [],
    "abstract": "This paper thoroughly investigates a range of popular DE configurations to\nidentify components responsible for the emergence of structural bias - recently\nidentified tendency of the algorithm to prefer some regions of the search space\nfor reasons directly unrelated to the objective function values. Such tendency\nwas already studied in GA and PSO where a connection was established between\nthe strength of structural bias and population sizes and potential weaknesses\nof these algorithms was highlighted. For DE, this study goes further and\nextends the range of aspects that can contribute to presence of structural bias\nby including algorithmic component which is usually overlooked - constraint\nhandling technique. A wide range of DE configurations were subjected to the\nprotocol for testing for bias. Results suggest that triggering mechanism for\nthe bias in DE differs to the one previously found for GA and PSO - no clear\ndependency on population size exists. Setting of DE parameters is based on a\nseparate study which on its own leads to interesting directions of new\nresearch. Overall, DE turned out to be robust against structural bias - only\nDE/current-to-best/1/bin is clearly biased but this effect is mitigated by the\nuse of penalty constraint handling technique.",
    "published_date": "2019-01-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.NE",
      "68Txx"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06153v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06046v1",
    "title": "Good Similar Patches for Image Denoising",
    "authors": [
      "Si Lu"
    ],
    "author_ids": [],
    "abstract": "Patch-based denoising algorithms like BM3D have achieved outstanding\nperformance. An important idea for the success of these methods is to exploit\nthe recurrence of similar patches in an input image to estimate the underlying\nimage structures. However, in these algorithms, the similar patches used for\ndenoising are obtained via Nearest Neighbour Search (NNS) and are sometimes not\noptimal. First, due to the existence of noise, NNS can select similar patches\nwith similar noise patterns to the reference patch. Second, the unreliable\nnoisy pixels in digital images can bring a bias to the patch searching process\nand result in a loss of color fidelity in the final denoising result. We\nobserve that given a set of good similar patches, their distribution is not\nnecessarily centered at the noisy reference patch and can be approximated by a\nGaussian component. Based on this observation, we present a patch searching\nmethod that clusters similar patch candidates into patch groups using Gaussian\nMixture Model-based clustering, and selects the patch group that contains the\nreference patch as the final patches for denoising. We also use an unreliable\npixel estimation algorithm to pre-process the input noisy images to further\nimprove the patch searching. Our experiments show that our approach can better\ncapture the underlying patch structures and can consistently enable the\nstate-of-the-art patch-based denoising algorithms, such as BM3D, LPCA and PLOW,\nto better denoise images by providing them with patches found by our approach\nwhile without modifying these algorithms.",
    "published_date": "2019-01-18T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06046v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.06010v1",
    "title": "Degrees of Freedom Region of the $(M,N_1,N_2)$ MIMO Broadcast Channel with Partial CSIT: An Application of Sum-set Inequalities Based on Aligned Image Sets",
    "authors": [
      "Arash Gholami Davoodi",
      "Syed A. Jafar"
    ],
    "author_ids": [],
    "abstract": "The degrees of freedom (DoF) region is characterized for the $2$-user\nmultiple input multiple output (MIMO) broadcast channel (BC), where the\ntransmitter is equipped with $M$ antennas, the two receivers are equipped with\n$N_1$ and $N_2$ antennas, and the levels of channel state information at the\ntransmitter (CSIT) for the two users are parameterized by $\\beta_1, \\beta_2$,\nrespectively. The achievability of the DoF region was established by Hao,\nRassouli and Clerckx, but no proof of optimality was heretofore available. The\nproof of optimality is provided in this work with the aid of sum-set\ninequalities based on the aligned image sets (AIS) approach.",
    "published_date": "2019-01-17T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.06010v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.05958v1",
    "title": "A Semi-Supervised Machine Learning Approach to Detecting Recurrent Metastatic Breast Cancer Cases Using Linked Cancer Registry and Electronic Medical Record Data",
    "authors": [
      "Albee Y. Ling",
      "Allison W. Kurian",
      "Jennifer L. Caswell-Jin",
      "George W. Sledge Jr.",
      "Nigam H. Shah",
      "Suzanne R. Tamang"
    ],
    "author_ids": [],
    "abstract": "Objectives: Most cancer data sources lack information on metastatic\nrecurrence. Electronic medical records (EMRs) and population-based cancer\nregistries contain complementary information on cancer treatment and outcomes,\nyet are rarely used synergistically. To enable detection of metastatic breast\ncancer (MBC), we applied a semi-supervised machine learning framework to linked\nEMR-California Cancer Registry (CCR) data. Materials and Methods: We studied\n11,459 female patients treated at Stanford Health Care who received an incident\nbreast cancer diagnosis from 2000-2014. The dataset consisted of structured\ndata and unstructured free-text clinical notes from EMR, linked to CCR, a\ncomponent of the Surveillance, Epidemiology and End Results (SEER) database. We\nextracted information on metastatic disease from patient notes to infer a class\nlabel and then trained a regularized logistic regression model for MBC\nclassification. We evaluated model performance on a gold standard set of set of\n146 patients. Results: There are 495 patients with de novo stage IV MBC, 1,374\npatients initially diagnosed with Stage 0-III disease had recurrent MBC, and\n9,590 had no evidence of metastatis. The median follow-up time is 96.3 months\n(mean 97.8, standard deviation 46.7). The best-performing model incorporated\nboth EMR and CCR features. The area under the receiver-operating characteristic\ncurve=0.925 [95% confidence interval: 0.880-0.969], sensitivity=0.861,\nspecificity=0.878 and overall accuracy=0.870. Discussion and Conclusion: A\nframework for MBC case detection combining EMR and CCR data achieved good\nsensitivity, specificity and discrimination without requiring expert-labeled\nexamples. This approach enables population-based research on how patients die\nfrom cancer and may identify novel predictors of cancer recurrence.",
    "published_date": "2019-01-17T00:00:00",
    "year": 2019,
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.05958v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.05417v2",
    "title": "Training neural nets to learn reactive potential energy surfaces using interactive quantum chemistry in virtual reality",
    "authors": [
      "Silvia Amabilino",
      "Lars A. Bratholm",
      "Simon J. Bennie",
      "Alain C. Vaucher",
      "Markus Reiher",
      "David R. Glowacki"
    ],
    "author_ids": [],
    "abstract": "Whilst the primary bottleneck to a number of computational workflows was not\nso long ago limited by processing power, the rise of machine learning\ntechnologies has resulted in a paradigm shift which places increasing value on\nissues related to data curation - i.e., data size, quality, bias, format, and\ncoverage. Increasingly, data-related issues are equally as important as the\nalgorithmic methods used to process and learn from the data. Here we introduce\nan open source GPU-accelerated neural network (NN) framework for learning\nreactive potential energy surfaces (PESs), and investigate the use of real-time\ninteractive ab initio molecular dynamics in virtual reality (iMD-VR) as a new\nstrategy for rapidly sampling geometries along reaction pathways which can be\nused to train NNs to learn reactive PESs. Focussing on hydrogen abstraction\nreactions of CN radical with isopentane, we compare the performance of NNs\ntrained using iMD-VR data versus NNs trained using a more traditional method,\nnamely molecular dynamics (MD) constrained to sample a predefined grid of\npoints along hydrogen abstraction reaction coordinates. Both the NN trained\nusing iMD-VR data and the NN trained using the constrained MD data reproduce\nimportant qualitative features of the reactive PESs, such as a low and early\nbarrier to abstraction. Quantitatively, learning is sensitive to the training\ndataset. Our results show that user-sampled structures obtained with the\nquantum chemical iMD-VR machinery enable better sampling in the vicinity of the\nminimum energy path (MEP). As a result, the NN trained on the iMD-VR data does\nvery well predicting energies in the vicinity of the MEP, but less well\npredicting energies for 'off-path' structures. The NN trained on the\nconstrained MD data does better in predicting energies for 'off-path'\nstructures, given that it included a number of such structures in its training\nset.",
    "published_date": "2019-01-16T00:00:00",
    "year": 2019,
    "categories": [
      "physics.chem-ph",
      "cs.ET",
      "physics.comp-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.05417v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.05389v1",
    "title": "Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter",
    "authors": [
      "Jacobo Levy Abitbol",
      "Márton Karsai",
      "Eric Fleury"
    ],
    "author_ids": [],
    "abstract": "The socioeconomic status of people depends on a combination of individual\ncharacteristics and environmental variables, thus its inference from online\nbehavioral data is a difficult task. Attributes like user semantics in\ncommunication, habitat, occupation, or social network are all known to be\ndeterminant predictors of this feature. In this paper we propose three\ndifferent data collection and combination methods to first estimate and, in\nturn, infer the socioeconomic status of French Twitter users from their online\nsemantics. Our methods are based on open census data, crawled professional\nprofiles, and remotely sensed, expert annotated information on living\nenvironment. Our inference models reach similar performance of earlier results\nwith the advantage of relying on broadly available datasets and of providing a\ngeneralizable framework to estimate socioeconomic status of large numbers of\nTwitter users. These results may contribute to the scientific discussion on\nsocial stratification and inequalities, and may fuel several applications.",
    "published_date": "2019-01-16T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.CY",
      "physics.data-an",
      "physics.soc-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.05389v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.04966v1",
    "title": "Identifying and Correcting Label Bias in Machine Learning",
    "authors": [
      "Heinrich Jiang",
      "Ofir Nachum"
    ],
    "author_ids": [],
    "abstract": "Datasets often contain biases which unfairly disadvantage certain groups, and\nclassifiers trained on such datasets can inherit these biases. In this paper,\nwe provide a mathematical formulation of how this bias can arise. We do so by\nassuming the existence of underlying, unknown, and unbiased labels which are\noverwritten by an agent who intends to provide accurate labels but may have\nbiases against certain groups. Despite the fact that we only observe the biased\nlabels, we are able to show that the bias may nevertheless be corrected by\nre-weighting the data points without changing the labels. We show, with\ntheoretical guarantees, that training on the re-weighted dataset corresponds to\ntraining on the unobserved but unbiased labels, thus leading to an unbiased\nmachine learning classifier. Our procedure is fast and robust and can be used\nwith virtually any learning algorithm. We evaluate on a number of standard\nmachine learning fairness datasets and a variety of fairness notions, finding\nthat our method outperforms standard approaches in achieving fair\nclassification.",
    "published_date": "2019-01-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.04966v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.04827v2",
    "title": "Approximating Gaussian Process Emulators with Linear Inequality Constraints and Noisy Observations via MC and MCMC",
    "authors": [
      "Andrés F. López-Lopera",
      "François Bachoc",
      "Nicolas Durrande",
      "Jérémy Rohmer",
      "Déborah Idier",
      "Olivier Roustant"
    ],
    "author_ids": [],
    "abstract": "Adding inequality constraints (e.g. boundedness, monotonicity, convexity)\ninto Gaussian processes (GPs) can lead to more realistic stochastic emulators.\nDue to the truncated Gaussianity of the posterior, its distribution has to be\napproximated. In this work, we consider Monte Carlo (MC) and Markov Chain Monte\nCarlo (MCMC) methods. However, strictly interpolating the observations may\nentail expensive computations due to highly restrictive sample spaces.\nFurthermore, having (constrained) GP emulators when data are actually noisy is\nalso of interest for real-world implementations. Hence, we introduce a noise\nterm for the relaxation of the interpolation conditions, and we develop the\ncorresponding approximation of GP emulators under linear inequality\nconstraints. We show with various toy examples that the performance of MC and\nMCMC samplers improves when considering noisy observations. Finally, on 2D and\n5D coastal flooding applications, we show that more flexible and realistic GP\nimplementations can be obtained by considering noise effects and by enforcing\nthe (linear) inequality constraints.",
    "published_date": "2019-01-15T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.04827v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.04730v1",
    "title": "Fair and Unbiased Algorithmic Decision Making: Current State and Future Challenges",
    "authors": [
      "Songül Tolan"
    ],
    "author_ids": [],
    "abstract": "Machine learning algorithms are now frequently used in sensitive contexts\nthat substantially affect the course of human lives, such as credit lending or\ncriminal justice. This is driven by the idea that `objective' machines base\ntheir decisions solely on facts and remain unaffected by human cognitive\nbiases, discriminatory tendencies or emotions. Yet, there is overwhelming\nevidence showing that algorithms can inherit or even perpetuate human biases in\ntheir decision making when they are based on data that contains biased human\ndecisions. This has led to a call for fairness-aware machine learning. However,\nfairness is a complex concept which is also reflected in the attempts to\nformalize fairness for algorithmic decision making. Statistical formalizations\nof fairness lead to a long list of criteria that are each flawed (or harmful\neven) in different contexts. Moreover, inherent tradeoffs in these criteria\nmake it impossible to unify them in one general framework. Thus, fairness\nconstraints in algorithms have to be specific to the domains to which the\nalgorithms are applied. In the future, research in algorithmic decision making\nsystems should be aware of data and developer biases and add a focus on\ntransparency to facilitate regular fairness audits.",
    "published_date": "2019-01-15T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.04730v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.04562v1",
    "title": "Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements",
    "authors": [
      "Alex Beutel",
      "Jilin Chen",
      "Tulsee Doshi",
      "Hai Qian",
      "Allison Woodruff",
      "Christine Luu",
      "Pierre Kreitmann",
      "Jonathan Bischof",
      "Ed H. Chi"
    ],
    "author_ids": [],
    "abstract": "As more researchers have become aware of and passionate about algorithmic\nfairness, there has been an explosion in papers laying out new metrics,\nsuggesting algorithms to address issues, and calling attention to issues in\nexisting applications of machine learning. This research has greatly expanded\nour understanding of the concerns and challenges in deploying machine learning,\nbut there has been much less work in seeing how the rubber meets the road.\n  In this paper we provide a case-study on the application of fairness in\nmachine learning research to a production classification system, and offer new\ninsights in how to measure and address algorithmic fairness issues. We discuss\nopen questions in implementing equality of opportunity and describe our\nfairness metric, conditional equality, that takes into account distributional\ndifferences. Further, we provide a new approach to improve on the fairness\nmetric during model training and demonstrate its efficacy in improving\nperformance for a real-world product",
    "published_date": "2019-01-14T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.04562v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.04824v1",
    "title": "Approaching Ethical Guidelines for Data Scientists",
    "authors": [
      "Ursula Garzcarek",
      "Detlef Steuer"
    ],
    "author_ids": [],
    "abstract": "The goal of this article is to inspire data scientists to participate in the\ndebate on the impact that their professional work has on society, and to become\nactive in public debates on the digital world as data science professionals.\nHow do ethical principles (e.g., fairness, justice, beneficence, and\nnon-maleficence) relate to our professional lives? What lies in our\nresponsibility as professionals by our expertise in the field? More\nspecifically this article makes an appeal to statisticians to join that debate,\nand to be part of the community that establishes data science as a proper\nprofession in the sense of Airaksinen, a philosopher working on professional\nethics. As we will argue, data science has one of its roots in statistics and\nextends beyond it. To shape the future of statistics, and to take\nresponsibility for the statistical contributions to data science, statisticians\nshould actively engage in the discussions. First the term data science is\ndefined, and the technical changes that have led to a strong influence of data\nscience on society are outlined. Next the systematic approach from CNIL is\nintroduced. Prominent examples are given for ethical issues arising from the\nwork of data scientists. Further we provide reasons why data scientists should\nengage in shaping morality around and to formulate codes of conduct and codes\nof practice for data science. Next we present established ethical guidelines\nfor the related fields of statistics and computing machinery. Thereafter\nnecessary steps in the community to develop professional ethics for data\nscience are described. Finally we give our starting statement for the debate:\nData science is in the focal point of current societal development. Without\nbecoming a profession with professional ethics, data science will fail in\nbuilding trust in its interaction with and its much needed contributions to\nsociety!",
    "published_date": "2019-01-14T00:00:00",
    "year": 2019,
    "categories": [
      "stat.OT",
      "cs.AI",
      "cs.CY",
      "stat.ML",
      "62A01"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.04824v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.03684v1",
    "title": "Multi-Level Batch Normalization In Deep Networks For Invasive Ductal Carcinoma Cell Discrimination In Histopathology Images",
    "authors": [
      "Francisco Perdigon Romero",
      "An Tang",
      "Samuel Kadoury"
    ],
    "author_ids": [],
    "abstract": "Breast cancer is the most diagnosed cancer and the most predominant cause of\ndeath in women worldwide. Imaging techniques such as the breast cancer\npathology helps in the diagnosis and monitoring of the disease. However\nidentification of malignant cells can be challenging given the high\nheterogeneity in tissue absorbotion from staining agents. In this work, we\npresent a novel approach for Invasive Ductal Carcinoma (IDC) cells\ndiscrimination in histopathology slides. We propose a model derived from the\nInception architecture, proposing a multi-level batch normalization module\nbetween each convolutional steps. This module was used as a base block for the\nfeature extraction in a CNN architecture. We used the open IDC dataset in which\nwe obtained a balanced accuracy of 0.89 and an F1 score of 0.90, thus\nsurpassing recent state of the art classification algorithms tested on this\npublic dataset.",
    "published_date": "2019-01-11T00:00:00",
    "year": 2019,
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.03684v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.03614v1",
    "title": "Jammer-Assisted Resource Allocation in Secure OFDMA With Untrusted Users",
    "authors": [
      "Ravikant Saini",
      "Abhishek Jindal",
      "Swades De"
    ],
    "author_ids": [],
    "abstract": "In this paper, we consider the problem of resource allocation in the\northogonal frequency division multiple access system with single source and M\nuntrusted users in presence of a friendly jammer. The jammer is used to improve\neither the weighted sum secure rate or the overall system fairness. The\nformulated optimization problem in both the cases is a mixed integer non-linear\nprogramming problem, belonging to the class of NP-hard. In the sum secure rate\nmaximization scenario, we decouple the problem and first obtain the subcarrier\nallocation at source and the decision for jammer power utilization on a\nper-subcarrier basis. Then, we do joint source and jammer power allocation\nusing primal decomposition and alternating optimization framework. Next, we\nconsider fair resource allocation by introducing a novel concept of subcarrier\nsnatching with the help of jammer. We propose two schemes for jammer power\nutilization, called proactively fair allocation (PFA) and on-demand allocation\n(ODA). PFA considers equitable distribution of jammer power among the\nsubcarriers, while ODA distributes jammer power based on the user demand. In\nboth cases of jammer usage, we also present suboptimal solutions that solve the\npower allocation at a highly reduced complexity. Asymptotically optimal\nsolutions are derived to benchmark optimality of the proposed schemes. We\ncompare the performance of our proposed schemes with equal power allocation at\nsource and jammer. Our simulation results demonstrate that the jammer can\nindeed help in improving either the sum secure rate or the overall system\nfairness.",
    "published_date": "2019-01-11T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.03614v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.03146v3",
    "title": "Cosine-similarity penalty to discriminate sound classes in weakly-supervised sound event detection",
    "authors": [
      "Thomas Pellegrini",
      "Léo Cances"
    ],
    "author_ids": [],
    "abstract": "The design of new methods and models when only weakly-labeled data are\navailable is of paramount importance in order to reduce the costs of manual\nannotation and the considerable human effort associated with it. In this work,\nwe address Sound Event Detection in the case where a weakly annotated dataset\nis available for training. The weak annotations provide tags of audio events\nbut do not provide temporal boundaries. The objective is twofold: 1) audio\ntagging, i.e. multi-label classification at recording level, 2) sound event\ndetection, i.e. localization of the event boundaries within the recordings.\nThis work focuses mainly on the second objective. We explore an approach\ninspired by Multiple Instance Learning, in which we train a convolutional\nrecurrent neural network to give predictions at frame-level, using a custom\nloss function based on the weak labels and the statistics of the frame-based\npredictions. Since some sound classes cannot be distinguished with this\napproach, we improve the method by penalizing similarity between the\npredictions of the positive classes during training. On the test set used in\nthe DCASE 2018 challenge, consisting of 288 recordings and 10 sound classes,\nthe addition of a penalty resulted in a localization F-score of 34.75%, and\nbrought 10% relative improvement compared to not using the penalty. Our best\nmodel achieved a 26.20% F-score on the DCASE-2018 official Eval subset close to\nthe 10-system ensemble approach that ranked second in the challenge with a\n29.9% F-score.",
    "published_date": "2019-01-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.03146v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.03134v2",
    "title": "Gaussian processes with linear operator inequality constraints",
    "authors": [
      "Christian Agrell"
    ],
    "author_ids": [],
    "abstract": "This paper presents an approach for constrained Gaussian Process (GP)\nregression where we assume that a set of linear transformations of the process\nare bounded. It is motivated by machine learning applications for\nhigh-consequence engineering systems, where this kind of information is often\nmade available from phenomenological knowledge. We consider a GP $f$ over\nfunctions on $\\mathcal{X} \\subset \\mathbb{R}^{n}$ taking values in\n$\\mathbb{R}$, where the process $\\mathcal{L}f$ is still Gaussian when\n$\\mathcal{L}$ is a linear operator. Our goal is to model $f$ under the\nconstraint that realizations of $\\mathcal{L}f$ are confined to a convex set of\nfunctions. In particular, we require that $a \\leq \\mathcal{L}f \\leq b$, given\ntwo functions $a$ and $b$ where $a < b$ pointwise. This formulation provides a\nconsistent way of encoding multiple linear constraints, such as\nshape-constraints based on e.g. boundedness, monotonicity or convexity. We\nadopt the approach of using a sufficiently dense set of virtual observation\nlocations where the constraint is required to hold, and derive the exact\nposterior for a conjugate likelihood. The results needed for stable numerical\nimplementation are derived, together with an efficient sampling scheme for\nestimating the posterior process.",
    "published_date": "2019-01-10T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.03134v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.03116v2",
    "title": "Equalizing Gender Biases in Neural Machine Translation with Word Embeddings Techniques",
    "authors": [
      "Joel Escudé Font",
      "Marta R. Costa-jussà"
    ],
    "author_ids": [],
    "abstract": "Neural machine translation has significantly pushed forward the quality of\nthe field. However, there are remaining big issues with the output translations\nand one of them is fairness. Neural models are trained on large text corpora\nwhich contain biases and stereotypes. As a consequence, models inherit these\nsocial biases. Recent methods have shown results in reducing gender bias in\nother natural language processing tools such as word embeddings. We take\nadvantage of the fact that word embeddings are used in neural machine\ntranslation to propose a method to equalize gender biases in neural machine\ntranslation using these representations. Specifically, we propose, experiment\nand analyze the integration of two debiasing techniques over GloVe embeddings\nin the Transformer translation architecture. We evaluate our proposed system on\nthe WMT English-Spanish benchmark task, showing gains up to one BLEU point. As\nfor the gender bias evaluation, we generate a test set of occupations and we\nshow that our proposed system learns to equalize existing biases from the\nbaseline system.",
    "published_date": "2019-01-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.03116v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.07528v1",
    "title": "Learning Continuous Face Age Progression: A Pyramid of GANs",
    "authors": [
      "Hongyu Yang",
      "Di Huang",
      "Yunhong Wang",
      "Anil K. Jain"
    ],
    "author_ids": [],
    "abstract": "The two underlying requirements of face age progression, i.e. aging accuracy\nand identity permanence, are not well studied in the literature. This paper\npresents a novel generative adversarial network based approach to address the\nissues in a coupled manner. It separately models the constraints for the\nintrinsic subject-specific characteristics and the age-specific facial changes\nwith respect to the elapsed time, ensuring that the generated faces present\ndesired aging effects while simultaneously keeping personalized properties\nstable. To ensure photo-realistic facial details, high-level age-specific\nfeatures conveyed by the synthesized face are estimated by a pyramidal\nadversarial discriminator at multiple scales, which simulates the aging effects\nwith finer details. Further, an adversarial learning scheme is introduced to\nsimultaneously train a single generator and multiple parallel discriminators,\nresulting in smooth continuous face aging sequences. The proposed method is\napplicable even in the presence of variations in pose, expression, makeup,\netc., achieving remarkably vivid aging effects. Quantitative evaluations by a\nCOTS face recognition system demonstrate that the target age distributions are\naccurately recovered, and 99.88% and 99.98% age progressed faces can be\ncorrectly verified at 0.001% FAR after age transformations of approximately 28\nand 23 years elapsed time on the MORPH and CACD databases, respectively. Both\nvisual and quantitative assessments show that the approach advances the\nstate-of-the-art.",
    "published_date": "2019-01-10T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07528v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.02547v1",
    "title": "Problem Formulation and Fairness",
    "authors": [
      "Samir Passi",
      "Solon Barocas"
    ],
    "author_ids": [],
    "abstract": "Formulating data science problems is an uncertain and difficult process. It\nrequires various forms of discretionary work to translate high-level objectives\nor strategic goals into tractable problems, necessitating, among other things,\nthe identification of appropriate target variables and proxies. While these\nchoices are rarely self-evident, normative assessments of data science projects\noften take them for granted, even though different translations can raise\nprofoundly different ethical concerns. Whether we consider a data science\nproject fair often has as much to do with the formulation of the problem as any\nproperty of the resulting model. Building on six months of ethnographic\nfieldwork with a corporate data science team---and channeling ideas from\nsociology and history of science, critical data studies, and early writing on\nknowledge discovery in databases---we describe the complex set of actors and\nactivities involved in problem formulation. Our research demonstrates that the\nspecification and operationalization of the problem are always negotiated and\nelastic, and rarely worked out with explicit normative considerations in mind.\nIn so doing, we show that careful accounts of everyday data science work can\nhelp us better understand how and why data science problems are posed in\ncertain ways---and why specific formulations prevail in practice, even in the\nface of what might seem like normatively preferable alternatives. We conclude\nby discussing the implications of our findings, arguing that effective\nnormative interventions will require attending to the practical work of problem\nformulation.",
    "published_date": "2019-01-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.02547v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.02393v2",
    "title": "Fair Algorithms for Clustering",
    "authors": [
      "Suman K. Bera",
      "Deeparnab Chakrabarty",
      "Nicolas J. Flores",
      "Maryam Negahbani"
    ],
    "author_ids": [],
    "abstract": "We study the problem of finding low-cost Fair Clusterings in data where each\ndata point may belong to many protected groups. Our work significantly\ngeneralizes the seminal work of Chierichetti et.al. (NIPS 2017) as follows.\n  - We allow the user to specify the parameters that define fair\nrepresentation. More precisely, these parameters define the maximum over- and\nminimum under-representation of any group in any cluster.\n  - Our clustering algorithm works on any $\\ell_p$-norm objective (e.g.\n$k$-means, $k$-median, and $k$-center). Indeed, our algorithm transforms any\nvanilla clustering solution into a fair one incurring only a slight loss in\nquality.\n  - Our algorithm also allows individuals to lie in multiple protected groups.\nIn other words, we do not need the protected groups to partition the data and\nwe can maintain fairness across different groups simultaneously.\n  Our experiments show that on established data sets, our algorithm performs\nmuch better in practice than what our theoretical results suggest.",
    "published_date": "2019-01-08T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.02393v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.01726v1",
    "title": "Evaluating software defect prediction performance: an updated benchmarking study",
    "authors": [
      "Libo Li",
      "Stefan Lessmann",
      "Bart Baesens"
    ],
    "author_ids": [],
    "abstract": "Accurately predicting faulty software units helps practitioners target faulty\nunits and prioritize their efforts to maintain software quality. Prior studies\nuse machine-learning models to detect faulty software code. We revisit past\nstudies and point out potential improvements. Our new study proposes a revised\nbenchmarking configuration. The configuration considers many new dimensions,\nsuch as class distribution sampling, evaluation metrics, and testing\nprocedures. The new study also includes new datasets and models. Our findings\nsuggest that predictive accuracy is generally good. However, predictive power\nis heavily influenced by the evaluation metrics and testing procedure\n(frequentist or Bayesian approach). The classifier results depend on the\nsoftware project. While it is difficult to choose the best classifier,\nresearchers should consider different dimensions to overcome potential bias.",
    "published_date": "2019-01-07T00:00:00",
    "year": 2019,
    "categories": [
      "cs.SE"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.01726v1",
    "is_ai_related": false
  },
  {
    "id": "http://arxiv.org/abs/1901.05560v1",
    "title": "Off-Policy Evaluation of Probabilistic Identity Data in Lookalike Modeling",
    "authors": [
      "Randell Cotta",
      "Mingyang Hu",
      "Dan Jiang",
      "Peizhou Liao"
    ],
    "author_ids": [],
    "abstract": "We evaluate the impact of probabilistically-constructed digital identity data\ncollected from Sep. to Dec. 2017 (approx.), in the context of\nLookalike-targeted campaigns. The backbone of this study is a large set of\nprobabilistically-constructed \"identities\", represented as small bags of\ncookies and mobile ad identifiers with associated metadata, that are likely all\nowned by the same underlying user. The identity data allows to generate\n\"identity-based\", rather than \"identifier-based\", user models, giving a fuller\npicture of the interests of the users underlying the identifiers. We employ\noff-policy techniques to evaluate the potential of identity-powered lookalike\nmodels without incurring the risk of allowing untested models to direct large\namounts of ad spend or the large cost of performing A/B tests. We add to\nhistorical work on off-policy evaluation by noting a significant type of\n\"finite-sample bias\" that occurs for studies combining modestly-sized datasets\nand evaluation metrics involving rare events (e.g., conversions). We illustrate\nthis bias using a simulation study that later informs the handling of inverse\npropensity weights in our analyses on real data. We demonstrate significant\nlift in identity-powered lookalikes versus an identity-ignorant baseline: on\naverage ~70% lift in conversion rate. This rises to factors of ~(4-32)x for\nidentifiers having little data themselves, but that can be inferred to belong\nto users with substantial data to aggregate across identifiers. This implies\nthat identity-powered user modeling is especially important in the context of\nidentifiers having very short lifespans (i.e., frequently churned cookies). Our\nwork motivates and informs the use of probabilistically-constructed identities\nin marketing. It also deepens the canon of examples in which off-policy\nlearning has been employed to evaluate the complex systems of the internet\neconomy.",
    "published_date": "2019-01-04T00:00:00",
    "year": 2019,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.05560v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.00844v3",
    "title": "Machine Learning at the Wireless Edge: Distributed Stochastic Gradient Descent Over-the-Air",
    "authors": [
      "Mohammad Mohammadi Amiri",
      "Deniz Gunduz"
    ],
    "author_ids": [],
    "abstract": "We study federated machine learning (ML) at the wireless edge, where power-\nand bandwidth-limited wireless devices with local datasets carry out\ndistributed stochastic gradient descent (DSGD) with the help of a remote\nparameter server (PS). Standard approaches assume separate computation and\ncommunication, where local gradient estimates are compressed and transmitted to\nthe PS over orthogonal links. Following this digital approach, we introduce\nD-DSGD, in which the wireless devices employ gradient quantization and error\naccumulation, and transmit their gradient estimates to the PS over a multiple\naccess channel (MAC). We then introduce a novel analog scheme, called A-DSGD,\nwhich exploits the additive nature of the wireless MAC for over-the-air\ngradient computation, and provide convergence analysis for this approach. In\nA-DSGD, the devices first sparsify their gradient estimates, and then project\nthem to a lower dimensional space imposed by the available channel bandwidth.\nThese projections are sent directly over the MAC without employing any digital\ncode. Numerical results show that A-DSGD converges faster than D-DSGD thanks to\nits more efficient use of the limited bandwidth and the natural alignment of\nthe gradient estimates over the channel. The improvement is particularly\ncompelling at low power and low bandwidth regimes. We also illustrate for a\nclassification problem that, A-DSGD is more robust to bias in data distribution\nacross devices, while D-DSGD significantly outperforms other digital schemes in\nthe literature. We also observe that both D-DSGD and A-DSGD perform better by\nincreasing the number of devices (while keeping the total dataset size\nconstant), showing their ability in harnessing the computation power of edge\ndevices.",
    "published_date": "2019-01-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.00844v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.00590v1",
    "title": "Towards a Framework Combining Machine Ethics and Machine Explainability",
    "authors": [
      "Kevin Baum",
      "Holger Hermanns",
      "Timo Speith"
    ],
    "author_ids": [],
    "abstract": "We find ourselves surrounded by a rapidly increasing number of autonomous and\nsemi-autonomous systems. Two grand challenges arise from this development:\nMachine Ethics and Machine Explainability. Machine Ethics, on the one hand, is\nconcerned with behavioral constraints for systems, so that morally acceptable,\nrestricted behavior results; Machine Explainability, on the other hand, enables\nsystems to explain their actions and argue for their decisions, so that human\nusers can understand and justifiably trust them.\n  In this paper, we try to motivate and work towards a framework combining\nMachine Ethics and Machine Explainability. Starting from a toy example, we\ndetect various desiderata of such a framework and argue why they should and how\nthey could be incorporated in autonomous systems. Our main idea is to apply a\nframework of formal argumentation theory both, for decision-making under\nethical constraints and for the task of generating useful explanations given\nonly limited knowledge of the world. The result of our deliberations can be\ndescribed as a first version of an ethically motivated, principle-governed\nframework combining Machine Ethics and Machine Explainability",
    "published_date": "2019-01-03T00:00:00",
    "year": 2019,
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.00590v1",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.00555v3",
    "title": "An Introductory Guide to Fano's Inequality with Applications in Statistical Estimation",
    "authors": [
      "Jonathan Scarlett",
      "Volkan Cevher"
    ],
    "author_ids": [],
    "abstract": "Information theory plays an indispensable role in the development of\nalgorithm-independent impossibility results, both for communication problems\nand for seemingly distinct areas such as statistics and machine learning. While\nnumerous information-theoretic tools have been proposed for this purpose, the\noldest one remains arguably the most versatile and widespread: Fano's\ninequality. In this chapter, we provide a survey of Fano's inequality and its\nvariants in the context of statistical estimation, adopting a versatile\nframework that covers a wide range of specific problems. We present a variety\nof key tools and techniques used for establishing impossibility results via\nthis approach, and provide representative examples covering group testing,\ngraphical model selection, sparse linear regression, density estimation, and\nconvex optimization.",
    "published_date": "2019-01-02T00:00:00",
    "year": 2019,
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.00555v3",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.00433v2",
    "title": "Causal Calculus in the Presence of Cycles, Latent Confounders and Selection Bias",
    "authors": [
      "Patrick Forré",
      "Joris M. Mooij"
    ],
    "author_ids": [],
    "abstract": "We prove the main rules of causal calculus (also called do-calculus) for i/o\nstructural causal models (ioSCMs), a generalization of a recently proposed\ngeneral class of non-/linear structural causal models that allow for cycles,\nlatent confounders and arbitrary probability distributions. We also generalize\nadjustment criteria and formulas from the acyclic setting to the general one\n(i.e. ioSCMs). Such criteria then allow to estimate (conditional) causal\neffects from observational data that was (partially) gathered under selection\nbias and cycles. This generalizes the backdoor criterion, the\nselection-backdoor criterion and extensions of these to arbitrary ioSCMs.\nTogether, our results thus enable causal reasoning in the presence of cycles,\nlatent confounders and selection bias. Finally, we extend the ID algorithm for\nthe identification of causal effects to ioSCMs.",
    "published_date": "2019-01-02T00:00:00",
    "year": 2019,
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.00433v2",
    "is_ai_related": true
  },
  {
    "id": "http://arxiv.org/abs/1901.00304v4",
    "title": "Normal Approximation and Confidence Region of Singular Subspaces",
    "authors": [
      "Dong Xia"
    ],
    "author_ids": [],
    "abstract": "This paper is on the normal approximation of singular subspaces when the\nnoise matrix has i.i.d. entries. Our contributions are three-fold. First, we\nderive an explicit representation formula of the empirical spectral projectors.\nThe formula is neat and holds for deterministic matrix perturbations. Second,\nwe calculate the expected projection distance between the empirical singular\nsubspaces and true singular subspaces. Our method allows obtaining arbitrary\n$k$-th order approximation of the expected projection distance. Third, we prove\nthe non-asymptotical normal approximation of the projection distance with\ndifferent levels of bias corrections. By the $\\lceil \\log(d_1+d_2)\\rceil$-th\norder bias corrections, the asymptotical normality holds under optimal\nsignal-to-noise ration (SNR) condition where $d_1$ and $d_2$ denote the matrix\nsizes. In addition, it shows that higher order approximations are unnecessary\nwhen $|d_1-d_2|=O((d_1+d_2)^{1/2})$. Finally, we provide comprehensive\nsimulation results to merit our theoretic discoveries.\n  Unlike the existing results, our approach is non-asymptotical and the\nconvergence rates are established. Our method allows the rank $r$ to diverge as\nfast as $o((d_1+d_2)^{1/3})$. Moreover, our method requires no eigen-gap\ncondition (except the SNR) and no constraints between $d_1$ and $d_2$.",
    "published_date": "2019-01-02T00:00:00",
    "year": 2019,
    "categories": [
      "math.ST",
      "cs.IT",
      "math.IT",
      "stat.ML",
      "stat.TH"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.00304v4",
    "is_ai_related": false
  }
]