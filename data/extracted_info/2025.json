[
  {
    "id": "http://arxiv.org/abs/2504.15941v1",
    "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity",
    "year": 2025,
    "authors": [
      "Fanny Jourdan",
      "Yannick Chevalier",
      "Cécile Favre"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender biases in machine translation, addressing gender fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language translation",
      "affected_populations": [
        "gender minorities",
        "non-binary individuals"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Evaluates LLMs on gender bias dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.14882v1",
    "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness",
    "year": 2025,
    "authors": [
      "Mojtaba Kolahdouzi",
      "Hatice Gunes",
      "Ali Etemad"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates how optimization algorithms affect group fairness in AI, addressing social bias and discrimination related to gender and other groups. It emphasizes fairness outcomes across different social categories, linking AI optimization to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Focuses on fairness in AI systems affecting social groups",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Optimization dynamics and empirical validation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.14663v1",
    "title": "The Developer Experience of LGBTQIA+ People in Agile Teams: a Multivocal Literature Review",
    "year": 2025,
    "authors": [
      "Edvaldo Wassouf Jr",
      "Débora Paiva",
      "Kiev Gama",
      "Awdren Fontão"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines challenges faced by LGBTQIA+ professionals, highlighting discrimination and invisibility, which are social inequalities related to gender and sexual orientation.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Focuses on workplace social discrimination in tech",
      "affected_populations": [
        "LGBTQIA+ professionals"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Multivocal Literature Review of traditional and grey literature",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.14388v1",
    "title": "Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach",
    "year": 2025,
    "authors": [
      "Xiaoyang Wang",
      "Christopher C. Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in healthcare AI, addressing disparities across demographic groups, which relates to social inequalities such as health disparities and bias.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "addresses healthcare disparities and algorithmic fairness",
      "affected_populations": [
        "patients",
        "demographic subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Optimization",
        "Quantitative Analysis"
      ],
      "methodology_detail": "gradient reconciliation for fairness-performance balance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.14120v1",
    "title": "Inclusive Education with AI: Supporting Special Needs and Tackling Language Barriers",
    "year": 2025,
    "authors": [
      "Ricardo Fitas"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses inclusion in education, focusing on supporting students with disabilities and language barriers, which relate to social inequalities. It discusses equitable access and ethical implementation of AI, impacting marginalized groups. These aspects directly pertain to social inequality issues.",
      "inequality_type": [
        "disability",
        "linguistic",
        "educational"
      ],
      "other_detail": "Focus on inclusive education and access",
      "affected_populations": [
        "students with disabilities",
        "multilingual students"
      ],
      "methodology": [
        "Literature Review",
        "System Design"
      ],
      "methodology_detail": "Review of AI tools and implementation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.13674v1",
    "title": "Beyond Stereotypes: Exploring How Minority College Students Experience Stigma on Reddit",
    "year": 2025,
    "authors": [
      "Chaeeun Han",
      "Sangpil Youm",
      "Hojeong Yoo",
      "Sou Hyun Jang"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines stigma experiences among minority college students, focusing on racial and intersectional identities, highlighting social discrimination and inequality in online spaces.",
      "inequality_type": [
        "racial",
        "educational",
        "intersectional"
      ],
      "other_detail": "Focus on online stigma experiences",
      "affected_populations": [
        "minority students",
        "racial minorities",
        "intersectional groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Semantic Analysis",
        "Stance Detection"
      ],
      "methodology_detail": "Using Stereotype-BERT and semantic distance measures",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.13085v1",
    "title": "Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia",
    "year": 2025,
    "authors": [
      "Georgina Curto",
      "Svetlana Kiritchenko",
      "Muhammad Hammad Fahim Siddiqui",
      "Isar Nejadgholi",
      "Kathleen C. Fraser"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on societal bias against the poor, a socioeconomic inequality.",
      "inequality_type": [
        "socioeconomic",
        "class"
      ],
      "other_detail": "Bias against impoverished populations",
      "affected_populations": [
        "poor people"
      ],
      "methodology": [
        "Data Collection",
        "Manual Annotation",
        "Classification"
      ],
      "methodology_detail": "Annotating social media data for bias detection",
      "geographic_focus": [
        "multiple regions"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.12767v1",
    "title": "Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts",
    "year": 2025,
    "authors": [
      "Fatma Elsafoury",
      "David Hartmann"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in language models against marginalized social groups, including race, gender, and ethnicity, highlighting social discrimination issues. It analyzes how AI systems perpetuate or amplify social inequalities. The focus on marginalized groups and bias measurement directly relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "linguistic"
      ],
      "other_detail": "Bias against marginalized social groups in AI",
      "affected_populations": [
        "Minority groups",
        "LGBTQIA+",
        "Black women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement across multiple languages and groups",
      "geographic_focus": [
        "Egypt",
        "Arab countries",
        "Germany",
        "UK",
        "US"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.12587v1",
    "title": "Software Engineering Principles for Fairer Systems: Experiments with GroupCART",
    "year": 2025,
    "authors": [
      "Kewen Peng",
      "Hao Zhuo",
      "Yicheng Yang",
      "Tim Menzies"
    ],
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on discrimination against protected groups such as gender and ethnicity.",
      "inequality_type": [
        "gender",
        "ethnic",
        "discrimination"
      ],
      "other_detail": "Fairness in decision-making algorithms",
      "affected_populations": [
        "social groups",
        "protected groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness-aware decision tree optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.13959v1",
    "title": "AI Safety Should Prioritize the Future of Work",
    "year": 2025,
    "authors": [
      "Sanchaita Hazra",
      "Bodhisattwa Prasad Majumder",
      "Tuhin Chakrabarty"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI's impact on labor markets, income inequality, and fair compensation mechanisms, addressing socioeconomic and economic disparities.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focuses on labor market and income disparities",
      "affected_populations": [
        "workers",
        "labor force"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Economic theories and policy proposals",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.12458v1",
    "title": "M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness",
    "year": 2025,
    "authors": [
      "Jansen S. B. Pereira",
      "Giovani Valdrighi",
      "Marcos Medeiros Raimundo"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing discrimination against marginalized groups based on protected attributes like gender and race.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Fairness in AI systems for marginalized groups",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Optimization"
      ],
      "methodology_detail": "Min-max fairness optimization in gradient boosting",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.11913v1",
    "title": "Broadening Participation through Physical Computing: Replicating Sensor-Based Programming Workshops for Rural Students in Sri Lanka",
    "year": 2025,
    "authors": [
      "Poornima Meegammana",
      "Hussel Suriyaarachchi",
      "Paul Denny",
      "Suranga Nanayakkara"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study focuses on increasing participation in computing among rural, under-resourced students, addressing educational and geographic inequalities. It aims to promote equitable access and confidence in computing careers for under-represented groups. The intervention targets marginalized communities, highlighting social disparities in technology access.",
      "inequality_type": [
        "educational",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on rural, under-resourced communities in Sri Lanka",
      "affected_populations": [
        "rural students",
        "under-represented groups"
      ],
      "methodology": [
        "Experiment",
        "Survey"
      ],
      "methodology_detail": "Comparative study of sensor vs. non-sensor workshops",
      "geographic_focus": [
        "Sri Lanka"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.11431v1",
    "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models",
    "year": 2025,
    "authors": [
      "Maria Teleki",
      "Xiangjue Dong",
      "Haoran Liu",
      "James Caverlee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias and masculine defaults in discourse and AI models, highlighting disparities in representation and system performance related to gender, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language and AI",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "discourse analysis and embedding association tests",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.11183v1",
    "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting",
    "year": 2025,
    "authors": [
      "Ej Zhou",
      "Weiming Lu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social bias related to gender, religion, nationality, and race-color in language models, which are social discrimination issues.",
      "inequality_type": [
        "gender",
        "religion",
        "nationality",
        "race-color"
      ],
      "other_detail": "Focus on social bias in multilingual NLP models",
      "affected_populations": [
        "gender groups",
        "religious groups",
        "nationalities",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Evaluating bias and debiasing in multilingual models",
      "geographic_focus": [
        "English-speaking countries",
        "Chinese",
        "Russian",
        "Indonesian",
        "Thai"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.11169v1",
    "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos",
    "year": 2025,
    "authors": [
      "Laura De Grazia",
      "Pol Pastells",
      "Mauro Vázquez Chas",
      "Desmond Elliott",
      "Danae Sánchez Villegas",
      "Mireia Farrús",
      "Mariona Taulé"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses sexism, a form of gender-based social discrimination, and analyzes how AI models detect sexist content online.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on online sexism detection in social media videos",
      "affected_populations": [
        "women",
        "social media users"
      ],
      "methodology": [
        "Dataset Creation",
        "Machine Learning",
        "Multimodal Analysis"
      ],
      "methodology_detail": "Develops a new dataset and evaluates AI models",
      "geographic_focus": [
        "Spain"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.11059v1",
    "title": "Quantifying Group Fairness in Community Detection",
    "year": 2025,
    "authors": [
      "Elze de Vink",
      "Frank W. Takes",
      "Akrati Saxena"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in community detection, addressing structural inequalities related to social attributes such as ethnicity, gender, and wealth, which are key aspects of social inequality.",
      "inequality_type": [
        "ethnic",
        "gender",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Addresses structural social inequalities in network communities",
      "affected_populations": [
        "minority groups",
        "underrepresented communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Evaluation"
      ],
      "methodology_detail": "Fairness metrics and comparative evaluation of algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.11504v2",
    "title": "Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets",
    "year": 2025,
    "authors": [
      "Woojin Kim",
      "Hyeoncheol Kim"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in educational AI, focusing on bias and individual fairness, which relate to social inequalities in education.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in educational datasets",
      "affected_populations": [
        "students",
        "educational groups"
      ],
      "methodology": [
        "Machine Learning",
        "Counterfactual Analysis"
      ],
      "methodology_detail": "Analyzes causal fairness in educational models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.10797v1",
    "title": "Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies",
    "year": 2025,
    "authors": [
      "Annabella Sakunkoo",
      "Jonathan Sakunkoo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "H.5; J.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs related to race, gender, and ethnicity, highlighting social disparities and discrimination embedded in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Biases in AI rankings based on names and social signals",
      "affected_populations": [
        "students",
        "ethnic groups",
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes name variations and AI rankings across ethnicities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.10389v2",
    "title": "Diversity-Fair Online Selection",
    "year": 2025,
    "authors": [
      "Ming Hu",
      "Yanzhi Li",
      "Tongwen Wu"
    ],
    "categories": [
      "econ.TH",
      "cs.DS",
      "math.OC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fostering workforce diversity across multiple dimensions, addressing fairness and representation issues relevant to social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Multiple social dimensions of diversity considered",
      "affected_populations": [
        "underrepresented groups",
        "diverse demographics"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designing policies for fair selection processes",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.10276v1",
    "title": "Who Speaks for Ethics? How Demographics Shape Ethical Advocacy in Software Development",
    "year": 2025,
    "authors": [
      "Lauren Olson",
      "Ricarda Anna-Lena Fischer",
      "Florian Kunneman",
      "Emitzá Guzmán"
    ],
    "categories": [
      "cs.SE",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities among software practitioners, highlighting gender, BIPOC, and disabled groups' ethical concerns and empowerment, indicating a focus on social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focus on marginalized groups in software development",
      "affected_populations": [
        "women",
        "BIPOC",
        "disabled individuals"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Practitioner survey across roles, industries, countries",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.09346v1",
    "title": "\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services",
    "year": 2025,
    "authors": [
      "Shira Michel",
      "Sufi Kaur",
      "Sarah Elizabeth Gillespie",
      "Jeffrey Gleason",
      "Christo Wilson",
      "Avijit Ghosh"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines accent bias and digital exclusion, highlighting social discrimination and inequality in linguistic and technological access.",
      "inequality_type": [
        "linguistic",
        "digital",
        "social bias"
      ],
      "other_detail": "Focus on accent-based discrimination and digital exclusion",
      "affected_populations": [
        "accented speakers",
        "linguistic minorities"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Assessing perceptions and performance across accents",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.12323v2",
    "title": "The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation",
    "year": 2025,
    "authors": [
      "Zheng Zhang",
      "Ning Li",
      "Qi Liu",
      "Rui Li",
      "Weibo Gao",
      "Qingyang Mao",
      "Zhenya Huang",
      "Baosheng Yu",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness issues in AI systems, focusing on bias mitigation and societal impacts, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "informational"
      ],
      "other_detail": "Focuses on fairness in AI systems affecting social groups",
      "affected_populations": [
        "social groups",
        "users of AI systems"
      ],
      "methodology": [
        "Experiment",
        "Fairness Analysis"
      ],
      "methodology_detail": "Evaluates fairness interventions in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.08260v2",
    "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
    "year": 2025,
    "authors": [
      "Yonchanok Khaokaew",
      "Flora D. Salim",
      "Andreas Züfle",
      "Hao Xue",
      "Taylor Anderson",
      "C. Raina MacIntyre",
      "Matthew Scotch",
      "David J Heslop"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and income in LLMs' decision-making, highlighting social disparities. It discusses how AI models may reflect or distort real-world social inequalities. The focus on demographic variations and biases indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "racial",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Biases in AI reflecting social disparities",
      "affected_populations": [
        "racial groups",
        "income groups"
      ],
      "methodology": [
        "Survey",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparing real survey data with AI responses",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.07801v1",
    "title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality Awareness",
    "year": 2025,
    "authors": [
      "Chandan Kumar Sah",
      "Xiaoli Lian",
      "Tony Xu",
      "Li Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness across demographic and psychological groups, addressing social bias issues in AI recommendations.",
      "inequality_type": [
        "gender",
        "race",
        "age",
        "psychological"
      ],
      "other_detail": "Includes personality traits in fairness assessment",
      "affected_populations": [
        "demographic groups",
        "psychological users"
      ],
      "methodology": [
        "Experiment",
        "Fairness Metric Development"
      ],
      "methodology_detail": "Evaluates models using a new fairness metric",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.07787v1",
    "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
    "year": 2025,
    "authors": [
      "Yisong Xiao",
      "Aishan Liu",
      "Siyuan Liang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases and stereotypes in language models, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on stereotype associations in AI models",
      "affected_populations": [
        "social groups",
        "minority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Adversarial Debiasing"
      ],
      "methodology_detail": "Bias mitigation in language models during inference",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.07395v1",
    "title": "FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair",
    "year": 2025,
    "authors": [
      "Arya Fayyazi",
      "Mehdi Kamal",
      "Massoud Pedram"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems, addressing social bias and disparities across groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Fairness in computer vision systems",
      "affected_populations": [
        "social groups",
        "underrepresented groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness assessment and correction in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.07312v2",
    "title": "The Gendered Algorithm: Navigating Financial Inclusion & Equity in AI-facilitated Access to Credit",
    "year": 2025,
    "authors": [
      "Genevieve Smith"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI-driven credit access, highlighting gender inequality and biases embedded in algorithms, affecting women's financial inclusion.",
      "inequality_type": [
        "gender",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender biases in AI and financial inclusion",
      "affected_populations": [
        "women",
        "financially excluded groups"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Interviews with fintech leaders, investors, and data scientists",
      "geographic_focus": [
        "low- and middle-income countries"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.06557v1",
    "title": "Market, power, gift, and concession economies: Comparison using four-mode primitive network models",
    "year": 2025,
    "authors": [
      "Takeshi Kato",
      "Junichi Miyakoshi",
      "Misa Owa",
      "Ryuji Mine"
    ],
    "categories": [
      "econ.TH",
      "cs.SI",
      "physics.soc-ph",
      "91B72, 05C90, 91D30",
      "J.4; K.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses economic modes related to wealth distribution and inequality, referencing concepts like baseline communism and concession economies, which impact socioeconomic disparities.",
      "inequality_type": [
        "wealth",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on economic models and societal structures",
      "affected_populations": [
        "general society"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Network model comparison of economic modes",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.07997v1",
    "title": "BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models",
    "year": 2025,
    "authors": [
      "Tian Xie",
      "Tongxin Yin",
      "Vaishakh Keshava",
      "Xueru Zhang",
      "Siddhartha Reddy Jonnalagadda"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in LLMs related to sensitive groups, revealing biased causal reasoning and potential impacts on social groups, thus addressing social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focuses on social bias and fairness in AI reasoning",
      "affected_populations": [
        "social groups",
        "sensitive groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Manual validation"
      ],
      "methodology_detail": "Question synthesis and causal graph analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.06160v3",
    "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
    "year": 2025,
    "authors": [
      "Rijul Magu",
      "Arka Dutta",
      "Sean Kim",
      "Ashiqur R. KhudaBukhsh",
      "Munmun De Choudhury"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "J.4; K.4.1; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs related to mental health groups, highlighting stigmatization and harmful discourse, which are social issues affecting vulnerable populations.",
      "inequality_type": [
        "health",
        "disability"
      ],
      "other_detail": "Focus on mental health-related biases and stigmatization",
      "affected_populations": [
        "mental health groups"
      ],
      "methodology": [
        "Network Analysis",
        "Bias Evaluation",
        "Sociological Framework"
      ],
      "methodology_detail": "Analyzes attack networks and stigmatization patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.05610v1",
    "title": "Fairness in Machine Learning-based Hand Load Estimation: A Case Study on Load Carriage Tasks",
    "year": 2025,
    "authors": [
      "Arafat Rahman",
      "Sol Lim",
      "Seokhyun Chung"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI predictions, aiming for fairness across sexes, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in predictive modeling",
      "affected_populations": [
        "male workers",
        "female workers"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Algorithm Development"
      ],
      "methodology_detail": "Using VAE for feature disentanglement to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.06303v1",
    "title": "On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions",
    "year": 2025,
    "authors": [
      "Dang Nguyen",
      "Chenhao Tan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in AI models and interventions to reduce them, addressing social discrimination and fairness issues related to race.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "",
      "affected_populations": [
        "Black",
        "White",
        "Asian"
      ],
      "methodology": [
        "Experiment",
        "Representation Analysis"
      ],
      "methodology_detail": "Debiasing via representation intervention in model activations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.04199v1",
    "title": "Investigating and Mitigating Stereotype-aware Unfairness in LLM-based Recommendations",
    "year": 2025,
    "authors": [
      "Zihuai Zhao",
      "Wenqi Fan",
      "Yao Wu",
      "Qing Li"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates stereotypes and fairness issues related to social groups in AI recommendations, addressing social bias and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on stereotypes in language models and fairness",
      "affected_populations": [
        "users",
        "items"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness evaluation and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.07982v1",
    "title": "Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT",
    "year": 2025,
    "authors": [
      "Harishwar Reddy",
      "Madhusudan Srinivasan",
      "Upulee Kanewala"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias in LLMs, addressing social discrimination issues related to demographic attributes.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "health",
        "educational"
      ],
      "other_detail": "Bias detection in AI models",
      "affected_populations": [
        "women",
        "minorities",
        "vulnerable groups"
      ],
      "methodology": [
        "Experiment",
        "Fairness Testing",
        "Natural Language Processing"
      ],
      "methodology_detail": "Metamorphic testing for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.02917v1",
    "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review",
    "year": 2025,
    "authors": [
      "Thanathip Suenghataiphorn",
      "Narisara Tribuddharat",
      "Pojsakorn Danpanichkul",
      "Narathorn Kulthamrongsri"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs affecting attributes like race, gender, and disability, which are related to social inequalities. It discusses how bias manifests and impacts marginalized groups in healthcare. The focus on systemic bias and health disparities indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "health"
      ],
      "other_detail": "Focus on healthcare disparities and marginalized populations",
      "affected_populations": [
        "racial minorities",
        "women",
        "disabled individuals"
      ],
      "methodology": [
        "Systematic Review",
        "Literature Review",
        "Bias Evaluation"
      ],
      "methodology_detail": "Analyzes existing studies on bias in clinical LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.01951v1",
    "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
    "year": 2025,
    "authors": [
      "Massimiliano Luca",
      "Ciro Beneduce",
      "Bruno Lepri",
      "Jacopo Staiano"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI models using online shopping data, highlighting stereotypes and biases related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in AI predictions",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Data Analysis"
      ],
      "methodology_detail": "Analyzing model predictions and product-gender associations",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.01420v1",
    "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations",
    "year": 2025,
    "authors": [
      "Athena Wen",
      "Tanush Patil",
      "Ansh Saxena",
      "Yicheng Fu",
      "Sean O'Brien",
      "Kevin Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender bias in AI hiring systems, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI-based resume evaluation",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Benchmark Creation"
      ],
      "methodology_detail": "Testing bias via resume alterations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.00860v1",
    "title": "Investigating the Capabilities and Limitations of Machine Learning for Identifying Bias in English Language Data with Information and Heritage Professionals",
    "year": 2025,
    "authors": [
      "Lucy Havens",
      "Benjamin Bach",
      "Melissa Terras",
      "Beatrice Alex"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "I.2.7; J.0; K.4.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in ML systems affecting marginalized groups and social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "informational"
      ],
      "other_detail": "Focuses on bias detection in language data",
      "affected_populations": [
        "marginalized groups",
        "information professionals"
      ],
      "methodology": [
        "Machine Learning",
        "Workshop",
        "Mixed-Methods"
      ],
      "methodology_detail": "Bias identification models and evaluation workshop",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.00388v1",
    "title": "Using complex prompts to identify fine-grained biases in image generation through ChatGPT-4o",
    "year": 2025,
    "authors": [
      "Marinus Ferreira"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases in AI related to societal disparities such as race, age, and social markers, and how AI outputs can reflect or exacerbate social inequalities.",
      "inequality_type": [
        "racial",
        "age",
        "social"
      ],
      "other_detail": "Bias in societal representation and social markers",
      "affected_populations": [
        "racial minorities",
        "elderly",
        "social groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing prompts and AI outputs for bias patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.00310v1",
    "title": "Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training",
    "year": 2025,
    "authors": [
      "Rajeev Kumar",
      "Harishankar Kumar",
      "Kumari Shalini"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting and mitigating biases related to social groups, such as gender, which directly pertains to social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Bias mitigation in language models for fairness",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Bias assessment"
      ],
      "methodology_detail": "Using knowledge graphs and bias metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.24310v1",
    "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models",
    "year": 2025,
    "authors": [
      "Alok Abhishek",
      "Lisa Erickson",
      "Tushar Bandopadhyay"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T01 (Primary), 68T50 (Secondary)",
      "I.2.0; I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in AI models that can reinforce societal prejudices related to race, gender, and social fairness, directly addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focus on societal biases in AI outputs",
      "affected_populations": [
        "minority groups",
        "women",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis",
        "Bias Benchmarking"
      ],
      "methodology_detail": "Assessing bias metrics in LLM outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.23398v1",
    "title": "A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models",
    "year": 2025,
    "authors": [
      "Leander Girrbach",
      "Stephan Alaniz",
      "Genevieve Smith",
      "Zeynep Akata"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender biases in AI-generated images, highlighting reinforcement of gender stereotypes and underrepresentation of women in certain roles, directly addressing social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in AI-generated imagery",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Large-scale image generation and bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.23056v2",
    "title": "Achieving Socio-Economic Parity through the Lens of EU AI Act",
    "year": 2025,
    "authors": [
      "Arjun Roy",
      "Stavroula Rizou",
      "Symeon Papadopoulos",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on SES-driven disparities and social bias in AI systems, aiming to address economic inequality and fairness issues related to socio-economic status.",
      "inequality_type": [
        "socioeconomic",
        "economic"
      ],
      "other_detail": "Focuses on SES and economic disparities in AI fairness",
      "affected_populations": [
        "underprivileged groups",
        "economically disadvantaged"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Proposes a new fairness measure and model optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.22934v1",
    "title": "FairSAM: Fair Classification on Corrupted Data Through Sharpness-Aware Minimization",
    "year": 2025,
    "authors": [
      "Yucong Dai",
      "Jie Ji",
      "Xiaolong Ma",
      "Yongkai Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and performance disparities across demographic groups under data corruption, highlighting algorithmic bias concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in AI systems under data corruption",
      "affected_populations": [
        "demographic groups",
        "subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Integrates fairness strategies into robustness framework",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.22569v1",
    "title": "Comparing Methods for Bias Mitigation in Graph Neural Networks",
    "year": 2025,
    "authors": [
      "Barbara Hoffmann",
      "Ruben Mayer"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias mitigation in AI, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "demographic"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness metrics evaluation using datasets",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.22454v1",
    "title": "A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination",
    "year": 2025,
    "authors": [
      "Ayan Majumdar",
      "Deborah D. Kanubala",
      "Kavya Gupta",
      "Isabel Valera"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and discrimination in algorithmic decision-making affecting social groups, focusing on treatment disparities that impact outcomes related to social inequality.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in algorithmic treatment decisions",
      "affected_populations": [
        "social groups",
        "decision subjects"
      ],
      "methodology": [
        "Causal Framework",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Extends fairness analysis with causal modeling",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.22066v2",
    "title": "Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community",
    "year": 2025,
    "authors": [
      "Rohit Dandamudi",
      "Ifeoma Adaji",
      "Gema Rodríguez-Pérez"
    ],
    "categories": [
      "cs.SE",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in gender and geographic representation among contributors, highlighting participation inequality and diversity issues within the Rust community.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Focus on contributor demographics and participation disparities",
      "affected_populations": [
        "contributors",
        "global Rust users"
      ],
      "methodology": [
        "Social Network Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed GitHub pull request data and network graphs",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.21092v1",
    "title": "FAIR-QR: Enhancing Fairness-aware Information Retrieval through Query Refinement",
    "year": 2025,
    "authors": [
      "Fumian Chen",
      "Hui Fang"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in information retrieval, addressing social bias and group fairness, which relate to social inequalities such as discrimination and unequal access.",
      "inequality_type": [
        "informational",
        "digital",
        "social bias"
      ],
      "other_detail": "Fairness in AI systems impacting social groups",
      "affected_populations": [
        "underrepresented groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Query refinement and re-ranking algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.20483v1",
    "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability",
    "year": 2025,
    "authors": [
      "Yingdong Shi",
      "Changming Li",
      "Yifan Wang",
      "Yongxiang Zhao",
      "Anqi Pang",
      "Sibei Yang",
      "Jingyi Yu",
      "Kan Ren"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases related to gender and race in diffusion models, which can reinforce societal inequalities. It focuses on bias mitigation and interpretability within AI systems affecting social groups. The research aims to understand and control biases that impact marginalized populations.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Mechanistic Interpretability"
      ],
      "methodology_detail": "Analyzing internal model decision mechanisms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.20414v1",
    "title": "Active Data Sampling and Generation for Bias Remediation",
    "year": 2025,
    "authors": [
      "Antonio Maratea",
      "Rita Perna"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models that can lead to discriminatory outcomes, specifically focusing on fairness issues related to gender bias. It discusses how unmitigated biases in training data affect model fairness and societal impacts.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias correction in AI models",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation via data sampling and generation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.19182v1",
    "title": "Evaluating Bias in LLMs for Job-Resume Matching: Gender, Race, and Education",
    "year": 2025,
    "authors": [
      "Hayate Iso",
      "Pouya Pezeshkpour",
      "Nikita Bhutani",
      "Estevam Hruschka"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, race, and education in AI hiring tools, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Biases in AI affecting employment fairness",
      "affected_populations": [
        "job applicants",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Assessing bias impacts in LLM-based matching",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.18826v2",
    "title": "Interpretable and Fair Mechanisms for Abstaining Classifiers",
    "year": 2025,
    "authors": [
      "Daphne Lenders",
      "Andrea Pugnana",
      "Roberto Pellungrini",
      "Toon Calders",
      "Dino Pedreschi",
      "Fosca Giannotti"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and discrimination in AI decision-making, focusing on demographic group disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI decision processes",
      "affected_populations": [
        "demographic groups",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Checks",
        "Rule-based Fairness"
      ],
      "methodology_detail": "Interpretable fairness mechanisms and rejection rules",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.18389v1",
    "title": "Agent-based Modeling meets the Capability Approach for Human Development: Simulating Homelessness Policy-making",
    "year": 2025,
    "authors": [
      "Alba Aguilera",
      "Nardine Osman",
      "Georgina Curto"
    ],
    "categories": [
      "cs.MA"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on homelessness, health inequity, and expanding capabilities, which relate to social inequalities. It discusses policy-making aimed at addressing disparities in opportunities and well-being. The framework aims to assess and improve real opportunities for marginalized groups.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on homelessness and health inequity",
      "affected_populations": [
        "homeless people",
        "marginalized groups"
      ],
      "methodology": [
        "Agent-based Modeling",
        "Reinforcement Learning",
        "Case Study",
        "Simulation"
      ],
      "methodology_detail": "Combines CA with computational modeling techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.08742v1",
    "title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents",
    "year": 2025,
    "authors": [
      "Nicholas Sukiennik",
      "Haoyu Wang",
      "Zailin Zeng",
      "Chen Gao",
      "Yong Li"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in recommendation systems affecting vulnerable groups, aiming to promote inclusivity and equity.",
      "inequality_type": [
        "gender",
        "income",
        "digital",
        "informational"
      ],
      "other_detail": "Focus on algorithmic bias and equitable content distribution",
      "affected_populations": [
        "women",
        "low-income groups"
      ],
      "methodology": [
        "Simulation",
        "Large-scale Data Analysis",
        "Bias Mitigation Strategies"
      ],
      "methodology_detail": "Using LLM-based simulation with real-world data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.17231v1",
    "title": "LoGoFair: Post-Processing for Local and Global Fairness in Federated Learning",
    "year": 2025,
    "authors": [
      "Li Zhang",
      "Chaochao Chen",
      "Zhongxuan Han",
      "Qiyong Zhong",
      "Xiaolin Zheng"
    ],
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in federated learning, focusing on social groups like gender. It aims to mitigate algorithmic bias affecting different populations, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on fairness across social groups in AI",
      "affected_populations": [
        "female",
        "male"
      ],
      "methodology": [
        "Post-processing",
        "Fairness Optimization",
        "Federated Learning"
      ],
      "methodology_detail": "Achieves fairness via model-agnostic post-processing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.17089v1",
    "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
    "year": 2025,
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Antón",
      "Bram Ruijsink",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in AI-based medical imaging, addressing racial disparities in healthcare outcomes.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial bias in medical AI models",
      "affected_populations": [
        "Black subjects",
        "White subjects"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.16826v1",
    "title": "When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts",
    "year": 2025,
    "authors": [
      "Jun Seong Kim",
      "Kyaw Ye Thu",
      "Javad Ismayilzada",
      "Junyeong Park",
      "Eunsu Kim",
      "Huzama Ahmad",
      "Na Min An",
      "James Thorne",
      "Alice Oh"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines cultural bias in AI models, highlighting disparities across ethnic and resource groups, which relates to social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI recognition across cultural and resource contexts",
      "affected_populations": [
        "ethnic groups",
        "low-resource cultures"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Develops a benchmark and tests model robustness",
      "geographic_focus": [
        "Korea",
        "Africa",
        "multiple countries"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.15904v1",
    "title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling",
    "year": 2025,
    "authors": [
      "Evan Chen",
      "Run-Jun Zhan",
      "Yan-Bai Lin",
      "Hung-Hsuan Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in LLMs' occupational narratives, highlighting gender stereotypes and disparities, which relate directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI-generated narratives",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes model outputs and stereotypes in storytelling",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.15815v1",
    "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
    "year": 2025,
    "authors": [
      "Vishnu Asutosh Dasu",
      "Md Rafi ur Rashid",
      "Vipul Gupta",
      "Saeid Tizpaz-Niari",
      "Gang Tan"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating biases in language models, specifically addressing fairness and gender bias reduction, which relate to social inequalities such as gender discrimination.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Surrogate models and simulated annealing for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.15682v1",
    "title": "Transfeminist AI Governance",
    "year": 2025,
    "authors": [
      "Blair Attard-Frost"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses power imbalances, marginalization, and harm to vulnerable groups, especially trans people, within AI governance, addressing social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "social",
        "marginalization"
      ],
      "other_detail": "Focus on trans and marginalized communities in AI governance",
      "affected_populations": [
        "trans people",
        "marginalized groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Critical Self-Reflexivity"
      ],
      "methodology_detail": "Reinterpretation of empirical studies",
      "geographic_focus": [
        "Canada",
        "Global"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.15454v3",
    "title": "Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems",
    "year": 2025,
    "authors": [
      "Yuelyu Ji",
      "Hang Zhang",
      "Yanshan Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates demographic biases related to race, gender, and socioeconomic factors in medical AI systems, addressing social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Bias mitigation in medical AI systems",
      "affected_populations": [
        "patients",
        "medical practitioners"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Bias Evaluation"
      ],
      "methodology_detail": "Analyzes disparities across multiple benchmarks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.16534v1",
    "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental",
    "year": 2025,
    "authors": [
      "Roberto Balestri"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, highlighting disparities in content moderation and acceptance rates for different gender prompts, which relates directly to gender inequality and bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias and content moderation practices",
      "affected_populations": [
        "males",
        "females"
      ],
      "methodology": [
        "Experiment",
        "Content Analysis"
      ],
      "methodology_detail": "Comparative analysis of model responses to gender prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.14138v1",
    "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions",
    "year": 2025,
    "authors": [
      "Siddharth D Jaiswal",
      "Sagnik Basu",
      "Sandipan Sikdar",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in face recognition accuracy, highlighting bias and fairness issues related to gender and dataset diversity.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in facial recognition systems across datasets",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes model performance and bias across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2504.05325v1",
    "title": "Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models",
    "year": 2025,
    "authors": [
      "Shiran Dudy",
      "Thulasi Tholeti",
      "Resmi Ramachandranpillai",
      "Muhammad Ali",
      "Toby Jia-Jun Li",
      "Ricardo Baeza-Yates"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLM recommendations related to geographic locations, highlighting demographic biases that can reinforce economic disparities, thus addressing social inequality.",
      "inequality_type": [
        "economic",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Biases in AI affecting economic disparities",
      "affected_populations": [
        "underserved areas",
        "disadvantaged communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing response consistency and bias patterns",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.12613v1",
    "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies",
    "year": 2025,
    "authors": [
      "Rashid Mushkani",
      "Hugo Berard",
      "Shin Koseki"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses structural inequalities and marginalized groups in urban settings, emphasizing disparities related to accessibility, safety, and social identities. It addresses how AI frameworks can preserve minority perspectives, highlighting social fairness issues.",
      "inequality_type": [
        "disability",
        "gender",
        "racial",
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Focus on urban social disparities and marginalized populations",
      "affected_populations": [
        "wheelchair users",
        "seniors",
        "LGBTQIA2+"
      ],
      "methodology": [
        "Qualitative Study",
        "Survey",
        "Case Study",
        "Experimental"
      ],
      "methodology_detail": "Community-centered rating and ranking tasks in Montreal",
      "geographic_focus": [
        "Montreal"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.12536v1",
    "title": "Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model",
    "year": 2025,
    "authors": [
      "Lin-Chun Huang",
      "Ching Chieh Tsao",
      "Fang-Yi Su",
      "Jung-Hsien Chiang"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models that lead to societal inequities, focusing on fairness and representation. It discusses mitigating biases related to sensitive attributes, which are linked to social groups. The emphasis on fairness in AI models inherently relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Bias mitigation in AI models affecting social groups",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Latent representation learning for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.11985v1",
    "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models",
    "year": 2025,
    "authors": [
      "Charaka Vinayak Kumar",
      "Ashok Urlana",
      "Gopichand Kanumolu",
      "Bala Mallikarjunarao Garlapati",
      "Pruthwik Mishra"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in LLMs related to physical, social, and economic categories, which are linked to social inequalities. It discusses biases affecting different social groups and societal categories, indicating a focus on social discrimination issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender",
        "health",
        "educational"
      ],
      "other_detail": "Bias evaluation across social and physical categories",
      "affected_populations": [
        "social groups",
        "minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Benchmark Evaluation",
        "Prompting Approaches",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and evaluation in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.11962v1",
    "title": "HInter: Exposing Hidden Intersectional Bias in Large Language Models",
    "year": 2025,
    "authors": [
      "Badr Souani",
      "Ezekiel Soremekun",
      "Mike Papadakis",
      "Setsuko Yokoyama",
      "Sudipta Chattopadhyay",
      "Yves Le Traon"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68T05"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting intersectional bias in LLMs, which relates to social discrimination based on attributes like race and gender.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on bias detection in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.09866v1",
    "title": "EquiPy: Sequential Fairness using Optimal Transport in Python",
    "year": 2025,
    "authors": [
      "Agathe Fernandes Machado",
      "Suzie Grondin",
      "Philipp Ratz",
      "Arthur Charpentier",
      "François Hu"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic fairness, bias mitigation, and impacts on social groups, indicating a focus on social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Multiple sensitive variables in fairness assessment",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Fairness mitigation using optimal transport techniques",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.09805v1",
    "title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models",
    "year": 2025,
    "authors": [
      "Jordan Taylor",
      "Joel Mire",
      "Franchesca Spektor",
      "Alicia DeVrio",
      "Maarten Sap",
      "Haiyi Zhu",
      "Sarah Fox"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how queer artists engage with AI models, highlighting normative biases that impact marginalized gender identities, thus addressing social discrimination and inequality related to gender and sexuality.",
      "inequality_type": [
        "gender",
        "sexuality"
      ],
      "other_detail": "Focus on queer gender and sexual identity biases",
      "affected_populations": [
        "queer artists",
        "LGBTQ+ community"
      ],
      "methodology": [
        "Workshop Study",
        "Qualitative Analysis"
      ],
      "methodology_detail": "Group sensemaking activities with artists",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.09763v1",
    "title": "BiasConnect: Investigating Bias Interactions in Text-to-Image Models",
    "year": 2025,
    "authors": [
      "Pushkar Shukla",
      "Aditya Chinchure",
      "Emily Diana",
      "Alexander Tolbert",
      "Kartik Hosanagar",
      "Vineeth N. Balasubramanian",
      "Leonid Sigal",
      "Matthew A. Turk"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in AI models related to social dimensions like ethnicity, gender, and intersectionality, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "intersectional"
      ],
      "other_detail": "Focuses on bias interactions and fairness in AI models",
      "affected_populations": [
        "ethnic groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias interaction measurement and causal graph analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.09659v2",
    "title": "Edge AI for Real-time Fetal Assessment in Rural Guatemala",
    "year": 2025,
    "authors": [
      "Nasim Katebi",
      "Mohammad Ahmad",
      "Mohsen Motie-Shirazi",
      "Daniel Phan",
      "Ellen Kolesnikova",
      "Sepideh Nikookar",
      "Alireza Rafiei",
      "Murali K. Korikana",
      "Rachel Hall-Clifford",
      "Esteban Castro",
      "Rosibely Sut",
      "Enma Coyote",
      "Anahi Venzor Strader",
      "Edlyn Ramos",
      "Peter Rohloff",
      "Reza Sameni",
      "Gari D. Clifford"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in healthcare access and outcomes in underserved, rural communities, highlighting socioeconomic and geographic inequalities affecting maternal and neonatal health.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "health"
      ],
      "other_detail": "Focus on rural healthcare disparities in Guatemala",
      "affected_populations": [
        "pregnant women",
        "new mothers"
      ],
      "methodology": [
        "Machine Learning",
        "System Design"
      ],
      "methodology_detail": "Real-time AI decision support tool development",
      "geographic_focus": [
        "Guatemala"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.16499v1",
    "title": "Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities",
    "year": 2025,
    "authors": [
      "Alva Markelius",
      "Julie Bailey",
      "Jenny L. Gibson",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.RO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social robots supporting students with disabilities, addressing structural inequalities and accessibility issues in higher education.",
      "inequality_type": [
        "disability",
        "educational",
        "accessibility"
      ],
      "other_detail": "Focus on structural and social barriers in education",
      "affected_populations": [
        "students with disabilities",
        "disability practitioners"
      ],
      "methodology": [
        "Qualitative Study",
        "Participatory Research"
      ],
      "methodology_detail": "Interviews and focus groups with stakeholders",
      "geographic_focus": [
        "United Kingdom"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.16494v1",
    "title": "The impact of artificial intelligence: from cognitive costs to global inequality",
    "year": 2025,
    "authors": [
      "Guy Paić",
      "Leonid Serkin"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses AI's impact on global equity and inequalities, emphasizing disparities across social groups and the need for fair governance.",
      "inequality_type": [
        "economic",
        "educational",
        "health",
        "informational",
        "geographic"
      ],
      "other_detail": "Focus on global disparities and social fairness issues",
      "affected_populations": [
        "global populations",
        "social groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing societal impacts and ethical considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.08731v1",
    "title": "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face Obfuscation Methods",
    "year": 2025,
    "authors": [
      "Seyyed Mohammad Sadegh Moosavi Khorzooghi",
      "Poojitha Thota",
      "Mohit Singhal",
      "Abolfazl Asudeh",
      "Gautam Das",
      "Shirin Nilizadeh"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness and biases in face obfuscation methods across demographic groups, addressing social discrimination and inequality issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness in privacy protection methods",
      "affected_populations": [
        "vulnerable populations",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Fairness Metrics",
        "Visualization"
      ],
      "methodology_detail": "Evaluates robustness and bias across datasets and attacks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.07817v1",
    "title": "Group Fairness in Multi-Task Reinforcement Learning",
    "year": 2025,
    "authors": [
      "Kefan Song",
      "Runnan Jiang",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints across demographic groups in RL, addressing social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Fairness across demographic groups in multi-task settings",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Experiment",
        "Constrained Optimization"
      ],
      "methodology_detail": "Enforcing fairness constraints in RL algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.07806v1",
    "title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models",
    "year": 2025,
    "authors": [
      "Kefan Song",
      "Jin Yao",
      "Runnan Jiang",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines group fairness and bias in AI models, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focuses on fairness across demographic groups in AI outputs",
      "affected_populations": [
        "demographic groups",
        "users of LLMs"
      ],
      "methodology": [
        "Benchmarking",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates fairness of reward models without uniform prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.07575v1",
    "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models",
    "year": 2025,
    "authors": [
      "Jen-tse Huang",
      "Jiantong Qin",
      "Jianping Zhang",
      "Youliang Yuan",
      "Wenxuan Wang",
      "Jieyu Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to gender and race in VLMs, which are forms of social inequality. It analyzes explicit and implicit biases that can perpetuate discrimination. The focus on social bias measurement aligns with addressing social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social biases in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing",
        "Computer Vision"
      ],
      "methodology_detail": "Bias measurement and analysis in AI responses",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.07510v1",
    "title": "Sometimes the Model doth Preach: Quantifying Religious Bias in Open LLMs through Demographic Analysis in Asian Nations",
    "year": 2025,
    "authors": [
      "Hari Shankar",
      "Vedanta S P",
      "Tejas Cavale",
      "Ponnurangam Kumaraguru",
      "Abhijnan Chakraborty"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in LLM outputs related to religion and identity across different social groups in Asian countries, highlighting issues of social bias and representation.",
      "inequality_type": [
        "religion",
        "ethnic",
        "cultural"
      ],
      "other_detail": "Focus on religious bias in AI outputs",
      "affected_populations": [
        "minorities",
        "religious groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Using Hamming Distance to infer demographics",
      "geographic_focus": [
        "India",
        "Asian nations"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.06792v1",
    "title": "On the Mutual Influence of Gender and Occupation in LLM Representations",
    "year": 2025,
    "authors": [
      "Haozhe An",
      "Connor Baumler",
      "Abhilasha Sancheti",
      "Rachel Rudinger"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender representations and biases in language models, highlighting how these influence perceptions of gendered occupations, which relates to social gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender stereotypes in AI representations",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes model representations and correlations with real-world data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.06734v1",
    "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
    "year": 2025,
    "authors": [
      "Mahdi Zakizadeh",
      "Mohammad Taher Pilehvar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, addressing social discrimination and fairness issues related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on internal gender bias representation in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Information-Theoretic Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes internal model representations and bias mitigation effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.06431v1",
    "title": "Fairness-aware organ exchange and kidney paired donation",
    "year": 2025,
    "authors": [
      "Mingrui Zhang",
      "Xiaowu Dai",
      "Lexin Li"
    ],
    "categories": [
      "stat.ME",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in organ transplantation, focusing on equitable access related to protected characteristics like race and gender, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Fairness in medical resource allocation",
      "affected_populations": [
        "patients",
        "donors"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Theoretical Analysis",
        "Simulation",
        "Empirical Study"
      ],
      "methodology_detail": "Fairness constraints and graph models analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.05684v1",
    "title": "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "year": 2025,
    "authors": [
      "Parameswaran Kamalaruban",
      "Mark Anderson",
      "Stuart Burrell",
      "Maeve Madigan",
      "Piotr Skalski",
      "David Sutton"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on bias reduction across social groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI model adaptation under privacy constraints",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness-aware Fine-tuning",
        "Experiment"
      ],
      "methodology_detail": "Distributed fairness-aware LoRA fine-tuning methods",
      "geographic_focus": [
        "CelebA",
        "UTK-Face datasets"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.05665v1",
    "title": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning Biased Models with Contextual Synthetic Data",
    "year": 2025,
    "authors": [
      "Zengqun Zhao",
      "Ziquan Liu",
      "Yu Cao",
      "Shaogang Gong",
      "Ioannis Patras"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on improving fairness in AI models, addressing bias and discrimination issues related to social groups. It aims to reduce algorithmic bias that can perpetuate social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fine-tuning biased models with synthetic data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.05093v1",
    "title": "Visual Cues of Gender and Race are Associated with Stereotyping in Vision-Language Models",
    "year": 2025,
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in VLMs, highlighting stereotyping and homogeneity bias, which are forms of social inequality. It discusses how these biases affect different social groups and the limitations of bias mitigation strategies.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social bias in AI models",
      "affected_populations": [
        "women",
        "Black Americans",
        "White Americans"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Testing VLMs with facial images and story generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.04576v1",
    "title": "Belonging Beyond Code: Queer Software Engineering and Humanities Student Experiences",
    "year": 2025,
    "authors": [
      "Emily Vorderwülbeke",
      "Isabella Graßl"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in belonging, openness, and challenges faced by queer students in different academic fields, highlighting social discrimination and inclusivity issues related to gender identity.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focuses on LGBTQ+ inclusion in academia",
      "affected_populations": [
        "queer students",
        "academic communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Comparative survey of student experiences",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.04542v1",
    "title": "Inducing Efficient and Equitable Professional Networks through Link Recommendations",
    "year": 2025,
    "authors": [
      "Cynthia Dwork",
      "Chris Hays",
      "Lunjia Hu",
      "Nicole Immorlica",
      "Juan Perdomo"
    ],
    "categories": [
      "cs.GT",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how professional networks influence labor market inequality across demographic groups, focusing on social disparities such as privilege and unprivilege. It discusses mechanisms that exacerbate or reduce inequality, indicating a direct engagement with social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "class",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on labor market and network formation",
      "affected_populations": [
        "privileged individuals",
        "unprivileged individuals"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Formal model of network formation and policy simulation",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.04372v1",
    "title": "Assumed Identities: Quantifying Gender Bias in Machine Translation of Ambiguous Occupational Terms",
    "year": 2025,
    "authors": [
      "Orfeas Menis Mastromichalakis",
      "Giorgos Filandrianos",
      "Maria Symeonaki",
      "Giorgos Stamou"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, reflecting societal gender stereotypes and inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and measurement in translation outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.03446v1",
    "title": "Biased Heritage: How Datasets Shape Models in Facial Expression Recognition",
    "year": 2025,
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar",
      "MaryBeth Defrance",
      "Maarten Buyl",
      "Tijl De Bie"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "I.2.10"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in facial expression recognition related to demographic groups, addressing social fairness and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Bias propagation in AI systems affecting social groups",
      "affected_populations": [
        "demographic groups",
        "protected groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias induction and correlation analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.02707v1",
    "title": "Multilingualism, Transnationality, and K-pop in the Online #StopAsianHate Movement",
    "year": 2025,
    "authors": [
      "Tessa Masis",
      "Zhangqi Duan",
      "Weiai Wayne Xu",
      "Ethan Zuckerman",
      "Jane Yeahin Pyo",
      "Brendan O'Connor"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines online activism related to anti-Asian hate, a racial discrimination issue, and analyzes transnational participation, highlighting racial and ethnic inequalities in online social movements.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial discrimination and transnational racial solidarity",
      "affected_populations": [
        "Asian Americans",
        "Asian communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing",
        "User Modeling",
        "Hand Annotation"
      ],
      "methodology_detail": "Analyzes large-scale multilingual social media data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.01947v2",
    "title": "Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts",
    "year": 2025,
    "authors": [
      "Akito Nakanishi",
      "Yukie Sano",
      "Geng Liu",
      "Francesco Pierri"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and stereotypes in Japanese LLMs, highlighting social bias issues related to gender, age, and social groups, which are key aspects of social inequality. It discusses safety and fairness concerns in AI systems affecting different social groups. The focus on stereotypes and bias mitigation aligns with social inequality discourse.",
      "inequality_type": [
        "gender",
        "age",
        "social bias"
      ],
      "other_detail": "Bias and safety in Japanese language AI models",
      "affected_populations": [
        "social groups",
        "Japanese users"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Analysis"
      ],
      "methodology_detail": "Constructed prompts and analyzed model responses",
      "geographic_focus": [
        "Japan"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.01532v2",
    "title": "Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios",
    "year": 2025,
    "authors": [
      "Bryan Chen Zhengyu Tan",
      "Roy Ka-Wei Lee"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases and power disparities in LLM responses, highlighting societal biases related to race, age, gender, and other social categories, thus addressing social inequality issues.",
      "inequality_type": [
        "racial",
        "age",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI responses across social demographic groups",
      "affected_populations": [
        "racial minorities",
        "elderly",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Semantic Shift Measurement"
      ],
      "methodology_detail": "Using cosine distance and preference win rate metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.01030v1",
    "title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups",
    "year": 2025,
    "authors": [
      "Yu Hou",
      "Hal Daumé III",
      "Rachel Rudinger"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines intergroup biases related to race, ethnicity, nationality, and religion, which are social categories linked to inequality and discrimination. It analyzes how AI models replicate human-like biases in these social groupings, reflecting social disparities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "nationality",
        "religion"
      ],
      "other_detail": "Bias in AI models reflecting social group disparities",
      "affected_populations": [
        "racial groups",
        "ethnic groups",
        "nationalities",
        "religious groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Emotion prediction task with manipulated social identities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.00907v1",
    "title": "Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems",
    "year": 2025,
    "authors": [
      "Ajinkya Kulkarni",
      "Atharva Kulkarni",
      "Miguel Couceiro",
      "Isabel Trancoso"
    ],
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender, accent, and age, which are social identity factors, and examines their impact on fairness and equity in ASR systems.",
      "inequality_type": [
        "gender",
        "age",
        "accent"
      ],
      "other_detail": "Focuses on social biases in AI systems",
      "affected_populations": [
        "gender groups",
        "elderly",
        "non-native speakers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias and environmental impact assessments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.00825v1",
    "title": "Who Reaps All the Superchats? A Large-Scale Analysis of Income Inequality in Virtual YouTuber Livestreaming",
    "year": 2025,
    "authors": [
      "Ruijing Zhao",
      "Brian Diep",
      "Jiaxin Pei",
      "Dongwook Yoon",
      "David Jurgens",
      "Jian Zhu"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes income disparity among VTuber streamers, highlighting economic inequality and monopolization, which relate to socioeconomic disparities.",
      "inequality_type": [
        "income",
        "economic",
        "wealth"
      ],
      "other_detail": "Focuses on digital economy and income disparity",
      "affected_populations": [
        "VTuber streamers",
        "digital content creators"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of streaming data and income distribution",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.00333v1",
    "title": "More of the Same: Persistent Representational Harms Under Increased Representation",
    "year": 2025,
    "authors": [
      "Jennifer Mickel",
      "Maria De-Arteaga",
      "Leqi Liu",
      "Kevin Tian"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender representation biases in AI outputs, highlighting persistent stereotypes and systemic harms, which directly relate to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI-generated representations",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes word differences across genders in AI outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.20898v1",
    "title": "A database to support the evaluation of gender biases in GPT-4o output",
    "year": 2025,
    "authors": [
      "Luise Mehner",
      "Lena Alicija Philine Fiedler",
      "Sabine Ammon",
      "Dorothea Kolossa"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender biases in LLM outputs, addressing social discrimination and fairness issues related to gender in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "disadvantaged gender groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Evaluation",
        "Analysis"
      ],
      "methodology_detail": "Constructing a database for bias evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.20354v1",
    "title": "Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study",
    "year": 2025,
    "authors": [
      "Nazarii Drushchak",
      "Vladyslava Tyshchenko",
      "Nataliya Polyakovska"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems affecting students, highlighting inequalities in access to educational resources.",
      "inequality_type": [
        "educational",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Bias detection and reduction in educational AI systems",
      "affected_populations": [
        "K-12 students",
        "protected student groups"
      ],
      "methodology": [
        "System Design",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and fairness framework implementation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.19846v1",
    "title": "Fair and Actionable Causal Prescription Ruleset",
    "year": 2025,
    "authors": [
      "Benton Li",
      "Nativ Levy",
      "Brit Youngmann",
      "Sainyam Galhotra",
      "Sudeepa Roy"
    ],
    "categories": [
      "cs.DB"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper emphasizes fairness, equity, and preventing inequalities in decision-making, addressing social fairness issues.",
      "inequality_type": [
        "social",
        "racial",
        "gender",
        "health",
        "educational"
      ],
      "other_detail": "Focus on equitable outcomes across protected groups",
      "affected_populations": [
        "protected groups",
        "non-protected groups"
      ],
      "methodology": [
        "Causal Reasoning",
        "Optimization",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness-aware causal prescription rules generation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.19749v1",
    "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
    "year": 2025,
    "authors": [
      "Jinhao Pan",
      "Chahat Raj",
      "Ziyu Yao",
      "Ziwei Zhu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases embedded in AI models, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on hidden biases in language models",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Develops a benchmark to detect hidden biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.19721v1",
    "title": "Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs",
    "year": 2025,
    "authors": [
      "Hannah Cyberey",
      "Yangfeng Ji",
      "David Evans"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender stereotypes and bias in LLMs, addressing social bias and fairness issues related to gender representation.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Representation Engineering",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Extracting and manipulating gender concept representations in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.19625v2",
    "title": "Revealing Treatment Non-Adherence Bias in Clinical Machine Learning Using Large Language Models",
    "year": 2025,
    "authors": [
      "Zhongyuan Liang",
      "Arvind Suresh",
      "Irene Y. Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in treatment adherence affecting vulnerable populations, highlighting biases that exacerbate health inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on health disparities and bias in clinical AI",
      "affected_populations": [
        "vulnerable groups",
        "patients with non-adherence"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Using LLMs to extract adherence info from clinical notes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.19582v1",
    "title": "Where Are We? Evaluating LLM Performance on African Languages",
    "year": 2025,
    "authors": [
      "Ife Adebara",
      "Hawau Olamide Toyin",
      "Nahom Tesfu Ghebremichael",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses linguistic inequality and data disparities affecting African languages, impacting access and representation in AI, which relate to social inequality issues.",
      "inequality_type": [
        "linguistic",
        "informational",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on language representation and data inequities",
      "affected_populations": [
        "African language speakers",
        "Indigenous communities"
      ],
      "methodology": [
        "Empirical Evaluation",
        "Benchmark Creation",
        "Data Analysis"
      ],
      "methodology_detail": "Assessing LLM performance on linguistic diversity",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.19104v2",
    "title": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine Translation",
    "year": 2025,
    "authors": [
      "Michelle Kappl"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in German machine translation, addressing gender stereotypes and underrepresentation, which are social inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on occupational stereotyping in language translation",
      "affected_populations": [
        "German speakers",
        "occupational groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "evaluates bias using a new dataset and large-scale testing",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.18774v1",
    "title": "Measuring risks inherent to our digital economies using Amazon purchase histories from US consumers",
    "year": 2025,
    "authors": [
      "Alex Berke",
      "Kent Larson",
      "Sandy Pentland",
      "Dana Calacci"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how consumer purchase data can reveal personal attributes like race, gender, health, and age, highlighting risks of online discrimination and privacy violations, which are related to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "age",
        "privacy"
      ],
      "other_detail": "Focus on privacy and discrimination risks from digital data",
      "affected_populations": [
        "US consumers",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predictive modeling of personal attributes from purchase data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.01872v1",
    "title": "FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance",
    "year": 2025,
    "authors": [
      "Mintong Kang",
      "Vinayshekhar Bannihatti Kumar",
      "Shamik Roy",
      "Abhishek Kumar",
      "Sopan Khosla",
      "Balakrishnan Murali Narayanaswamy",
      "Rashmi Gangadharaiah"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models related to gender, a social attribute, aiming to reduce unfair representation and discrimination. It focuses on mitigating demographic biases in generated images, which relate to social inequality issues. The work directly engages with fairness and bias concerns affecting social groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI-generated imagery",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Diffusion Model Development",
        "Bias Evaluation"
      ],
      "methodology_detail": "Adaptive latent guidance and bias benchmarking",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.18434v1",
    "title": "Exploring Gender Disparities in Automatic Speech Recognition Technology",
    "year": 2025,
    "authors": [
      "Hend ElGhazaly",
      "Bahman Mirheidari",
      "Nafise Sadat Moosavi",
      "Heidi Christensen"
    ],
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in ASR performance, addressing gender bias and fairness issues in AI systems, which are social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation through training data curation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Data Analysis"
      ],
      "methodology_detail": "Analyzing performance variations across gender representations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.05769v1",
    "title": "Effect of Gender Fair Job Description on Generative AI Images",
    "year": 2025,
    "authors": [
      "Finn Böckling",
      "Jan Marquenie",
      "Ingo Siegert"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines gender biases in AI-generated images related to STEM, highlighting societal gender disparities and biases in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI-generated representations",
      "affected_populations": [
        "women in STEM",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed image outputs from prompts in different languages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.17921v1",
    "title": "Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness",
    "year": 2025,
    "authors": [
      "Tahsin Alamgir Kheya",
      "Mohamed Reda Bouadjenek",
      "Sunil Aryal"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in recommendation systems, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI recommendations",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Metrics development and evaluation on datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.17730v1",
    "title": "Gender Bias in Perception of Human Managers Extends to AI Managers",
    "year": 2025,
    "authors": [
      "Hao Cui",
      "Taha Yasseri"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in perceptions of managers, including AI, highlighting gender-based discrimination and stereotypes in organizational contexts.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI and human management",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Randomized Controlled Trials"
      ],
      "methodology_detail": "Participants evaluated managers in controlled settings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.17611v1",
    "title": "Evaluating the Effect of Retrieval Augmentation on Social Biases",
    "year": 2025,
    "authors": [
      "Tianhui Zhang",
      "Yi Zhou",
      "Danushka Bollegala"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases related to gender, race, age, and religion in AI-generated text, which are key aspects of social inequality. It examines how biases in data and AI systems can amplify social disparities. The focus on social biases and fairness issues indicates a direct relevance to social inequality concerns.",
      "inequality_type": [
        "gender",
        "race",
        "age",
        "religion"
      ],
      "other_detail": "Focus on social biases in AI-generated language",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly",
        "religious groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias evaluation across multilingual datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.17390v1",
    "title": "Mitigating Bias in RAG: Controlling the Embedder",
    "year": 2025,
    "authors": [
      "Taeyoun Kim",
      "Jacob Springer",
      "Aditi Raghunathan",
      "Maarten Sap"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and political perspectives, which are social discrimination issues, and discusses fairness in AI systems.",
      "inequality_type": [
        "gender",
        "political"
      ],
      "other_detail": "Bias control in AI systems",
      "affected_populations": [
        "gender groups",
        "political groups"
      ],
      "methodology": [
        "Experiment",
        "Linear analysis",
        "Fine-tuning"
      ],
      "methodology_detail": "Bias analysis and control experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.05765v1",
    "title": "Encoding Inequity: Examining Demographic Bias in LLM-Driven Robot Caregiving",
    "year": 2025,
    "authors": [
      "Raj Korpan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race, gender, disability, and age in AI-driven caregiving, highlighting disparities and stereotypes affecting diverse social groups.",
      "inequality_type": [
        "race",
        "gender",
        "disability",
        "age"
      ],
      "other_detail": "Bias propagation in AI systems affecting social groups",
      "affected_populations": [
        "diverse care recipients"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzing LLM responses to demographic prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.16841v1",
    "title": "Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives",
    "year": 2025,
    "authors": [
      "Dilermando Queiroz",
      "Anderson Carlos",
      "André Anjos",
      "Lilian Berton"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and bias mitigation in AI healthcare systems, addressing social disparities across demographic groups.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on equitable AI in underserved populations",
      "affected_populations": [
        "underserved groups",
        "regional populations"
      ],
      "methodology": [
        "Literature Review",
        "System Design",
        "Bias Mitigation"
      ],
      "methodology_detail": "Systematic bias intervention strategies across development stages",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.16477v1",
    "title": "Unmasking Societal Biases in Respiratory Support for ICU Patients through Social Determinants of Health",
    "year": 2025,
    "authors": [
      "Mira Moukheiber",
      "Lama Moukheiber",
      "Dana Moukheiber",
      "Hyung-Chul Lee"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in respiratory support outcomes across demographic and social determinants, highlighting health inequities. It conducts fairness audits and considers social factors beyond basic attributes. The focus on social determinants indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "health",
        "racial",
        "social determinants of health"
      ],
      "other_detail": "Focus on ICU respiratory interventions and social factors",
      "affected_populations": [
        "diverse ICU patients"
      ],
      "methodology": [
        "Fairness Auditing",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating model fairness across social groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.16043v2",
    "title": "African Data Ethics: A Discursive Framework for Black Decolonial Data Science",
    "year": 2025,
    "authors": [
      "Teanna Barrett",
      "Chinasa T. Okolo",
      "B. Biira",
      "Eman Sherif",
      "Amy X. Zhang",
      "Leilani Battle"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper centers African perspectives on data ethics, decolonization, and anti-blackness, addressing racial and social inequalities in AI and data science practices.",
      "inequality_type": [
        "racial",
        "social",
        "educational"
      ],
      "other_detail": "Focus on anti-blackness and colonialism in data science",
      "affected_populations": [
        "African communities",
        "Black populations"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review",
        "Case Study"
      ],
      "methodology_detail": "Thematic analysis of documents and case studies",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.15939v1",
    "title": "\"Kya family planning after marriage hoti hai?\": Integrating Cultural Sensitivity in an LLM Chatbot for Reproductive Health",
    "year": 2025,
    "authors": [
      "Roshini Deva",
      "Dhruv Ramani",
      "Tanvi Divate",
      "Suhani Jalota",
      "Azra Ismail"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses cultural sensitivity in reproductive health, impacting underserved women, highlighting gender and health inequalities.",
      "inequality_type": [
        "gender",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on underserved urban women in India",
      "affected_populations": [
        "women",
        "underserved communities"
      ],
      "methodology": [
        "User interactions",
        "Focus groups",
        "Interviews"
      ],
      "methodology_detail": "Qualitative insights into chatbot responses and context",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.14996v1",
    "title": "A Rapid Test for Accuracy and Bias of Face Recognition Technology",
    "year": 2025,
    "authors": [
      "Manuel Knott",
      "Ignacio Serna",
      "Ethan Mann",
      "Pietro Perona"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases in face recognition, highlighting lower accuracy for Asian women, which relates to racial and gender disparities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in facial recognition accuracy across social groups",
      "affected_populations": [
        "Asian women",
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Benchmarking face recognition systems and bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.14653v1",
    "title": "Gender Influence on Student Teams' Online Communication in Software Engineering Education",
    "year": 2025,
    "authors": [
      "Rita Garcia",
      "Christoph Treude"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines gender differences in communication and behaviors in SE teams, highlighting gender bias and its impact on learning, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "female students",
        "male students"
      ],
      "methodology": [
        "Mixed-Methods"
      ],
      "methodology_detail": "Analysis of Slack communications and team behaviors",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.13319v1",
    "title": "Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare",
    "year": 2025,
    "authors": [
      "Hiba Ahsan",
      "Arnab Sen Sharma",
      "Silvio Amir",
      "David Bau",
      "Byron C. Wallace"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in healthcare AI, addressing social discrimination and bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Focuses on sociodemographic biases in language models",
      "affected_populations": [
        "patients",
        "healthcare providers"
      ],
      "methodology": [
        "Mechanistic interpretability",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Analyzes internal model representations and interventions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.13221v1",
    "title": "Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations",
    "year": 2025,
    "authors": [
      "Lee Cohen",
      "Jack Hsieh",
      "Connie Hong",
      "Judy Hanwen Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in hiring, highlighting unequal impacts of AI on different social groups.",
      "inequality_type": [
        "educational",
        "informational",
        "digital"
      ],
      "other_detail": "Focuses on AI fairness in employment screening",
      "affected_populations": [
        "job candidates",
        "employers"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Theoretical guarantees and empirical validation on resumes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.13120v1",
    "title": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context",
    "year": 2025,
    "authors": [
      "Marion Bartl",
      "Thomas Brendan Murphy",
      "Susan Leavy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, addressing gender-related social bias and discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI processing",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "adapting psycholinguistic methods across languages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.12858v1",
    "title": "Rejected Dialects: Biases Against African American Language in Reward Models",
    "year": 2025,
    "authors": [
      "Joel Mire",
      "Zubin Trivadi Aysola",
      "Daniel Chechelnitsky",
      "Nicholas Deas",
      "Chrysoula Zerva",
      "Maarten Sap"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "I.2.7; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases against African American Language in AI models, highlighting racial bias and representational harms, which relate directly to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Biases against African American Language in AI",
      "affected_populations": [
        "African American community"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Comparative analysis of reward model preferences and behaviors",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.12838v1",
    "title": "Towards Equitable AI: Detecting Bias in Using Large Language Models for Marketing",
    "year": 2025,
    "authors": [
      "Berk Yilmaz",
      "Huthaifa I. Ashqar"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI-generated marketing targeting demographic groups, highlighting social disparities related to gender, age, income, and education. It discusses how these biases lead to unequal messaging, reflecting social inequalities. The focus on demographic-based bias detection aligns with social inequality issues.",
      "inequality_type": [
        "gender",
        "age",
        "income",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI-generated marketing messages",
      "affected_populations": [
        "women",
        "younger individuals",
        "low-income earners",
        "lower education groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and thematic analysis of slogans",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.05721v1",
    "title": "What Are They Filtering Out? A Survey of Filtering Strategies for Harm Reduction in Pretraining Datasets",
    "year": 2025,
    "authors": [
      "Marco Antonio Stranisci",
      "Christian Hardmeier"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how data filtering impacts vulnerable groups, highlighting issues of discrimination and underrepresentation, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focuses on vulnerable groups in datasets",
      "affected_populations": [
        "disadvantaged groups",
        "minorities",
        "vulnerable populations"
      ],
      "methodology": [
        "Survey",
        "Experiment"
      ],
      "methodology_detail": "Analyzes filtering strategies and tests their impact",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.11611v1",
    "title": "Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks",
    "year": 2025,
    "authors": [
      "Fatemeh Mohammadi",
      "Marta Annamaria Tamborini",
      "Paolo Ceravolo",
      "Costanza Nardocci",
      "Samira Maghool"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social inequality issue related to gender discrimination and inclusion.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in language translation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Cosine similarity and bias measurement techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.04773v2",
    "title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content",
    "year": 2025,
    "authors": [
      "Bingbing Fan",
      "Lin Chen",
      "Songwei Li",
      "Jian Yuan",
      "Fengli Xu",
      "Pan Hui",
      "Yong Li"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on urban segregation, social perceptions, and inclusiveness, addressing societal inequalities related to social barriers and marginalization.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "urban-rural"
      ],
      "other_detail": "Focus on social segregation and inclusiveness in cities",
      "affected_populations": [
        "urban residents",
        "marginalized groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using LLMs and predictive frameworks for social insights",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.11603v1",
    "title": "DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning",
    "year": 2025,
    "authors": [
      "Hongye Qiu",
      "Yue Xu",
      "Meikang Qiu",
      "Wenjie Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias mitigation in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.11559v1",
    "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
    "year": 2025,
    "authors": [
      "Yue Xu",
      "Chengyan Fu",
      "Li Xiong",
      "Sibei Yang",
      "Wenjie Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in large language models, a form of social bias and inequality. It focuses on mitigating gender bias, which relates directly to social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation and model evaluation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.11349v1",
    "title": "Biases in Edge Language Models: Detection, Analysis, and Mitigation",
    "year": 2025,
    "authors": [
      "Vinamra Sharma",
      "Danilo Pietro Pau",
      "José Cano"
    ],
    "categories": [
      "cs.LG",
      "cs.PF",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in language models, focusing on fairness and bias reduction, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "health"
      ],
      "other_detail": "Focus on fairness and bias in AI outputs",
      "affected_populations": [
        "minority groups",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Bias Analysis",
        "Model Evaluation"
      ],
      "methodology_detail": "Comparative bias analysis across deployment environments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.11195v1",
    "title": "From Deception to Perception: The Surprising Benefits of Deepfakes for Detecting, Measuring, and Mitigating Bias",
    "year": 2025,
    "authors": [
      "Yizhi Liu",
      "Balaji Padmanabhan",
      "Siva Viswanathan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.0; I.2.10; I.4.0; J.4; H.4; K.4.1; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in societal domains and tools for fairness.",
      "inequality_type": [
        "racial",
        "health",
        "educational"
      ],
      "other_detail": "Bias measurement and mitigation in societal contexts",
      "affected_populations": [
        "racial groups",
        "patients",
        "students"
      ],
      "methodology": [
        "Experiment",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using deepfake-generated images for bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.10641v1",
    "title": "Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate Public Perceptions of Health Resource Accessibility",
    "year": 2025,
    "authors": [
      "Zhaoqian Xue",
      "Guanhong Liu",
      "Kai Wei",
      "Chong Zhang",
      "Qingcheng Zeng",
      "Songhua Hu",
      "Wenyue Hua",
      "Lizhou Fan",
      "Yongfeng Zhang",
      "Lingyao Li"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates disparities in health resource access across demographic and geographic groups, highlighting socioeconomic, racial, and educational inequalities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational",
        "geographic",
        "health"
      ],
      "other_detail": "Focus on health resource accessibility disparities during COVID-19",
      "affected_populations": [
        "racial minorities",
        "low-income groups",
        "rural residents"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using ModernBERT and PLS regression techniques",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.10577v1",
    "title": "Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias",
    "year": 2025,
    "authors": [
      "Enzo Doyen",
      "Amalia Todirascu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, focusing on masculine generics, which relates directly to gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on linguistic gender bias in AI responses",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes LLM responses to gendered instructions",
      "geographic_focus": [
        "French"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.08893v1",
    "title": "Uncovering Disparities in Rideshare Drivers Earning and Work Patterns: A Case Study of Chicago",
    "year": 2025,
    "authors": [
      "Hy Dang",
      "Yuwen Lu",
      "Jason Spicer",
      "Tamara Kay",
      "Di Yang",
      "Yang Yang",
      "Jay Brockman",
      "Meng Jiang",
      "Toby Jia-Jun Li"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines disparities in earnings and work patterns among drivers based on location and time, highlighting economic and geographic inequalities. It discusses fairness issues related to platform design and pricing models, impacting driver equity.",
      "inequality_type": [
        "economic",
        "geographic"
      ],
      "other_detail": "Focuses on income and spatial disparities among drivers",
      "affected_populations": [
        "rideshare drivers",
        "urban low-demand regions"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Development",
        "Case Study"
      ],
      "methodology_detail": "Reconstructed driver work patterns and analyzed earnings data",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2503.03888v2",
    "title": "AI for Scaling Legal Reform: Mapping and Redacting Racial Covenants in Santa Clara County",
    "year": 2025,
    "authors": [
      "Faiz Surani",
      "Mirac Suzgun",
      "Vyoma Raman",
      "Christopher D. Manning",
      "Peter Henderson",
      "Daniel E. Ho"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial covenants that historically and currently contribute to racial housing discrimination, a form of social inequality. It analyzes the persistence and geographic distribution of racially restrictive property clauses, highlighting racial disparities in housing access.",
      "inequality_type": [
        "racial",
        "housing",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial housing discrimination and legal reform",
      "affected_populations": [
        "racial minorities",
        "homeowners"
      ],
      "methodology": [
        "Natural Language Processing",
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Finetuned language model and geographic data analysis",
      "geographic_focus": [
        "Santa Clara County",
        "California"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.08779v2",
    "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
    "year": 2025,
    "authors": [
      "Vishal Narnaware",
      "Ashmal Vayani",
      "Rohit Gupta",
      "Sirnam Swetha",
      "Mubarak Shah"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses stereotype biases in AI, which relate to social discrimination and fairness issues affecting marginalized groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focus on stereotypes and fairness in AI systems",
      "affected_populations": [
        "minority groups",
        "socially marginalized"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Benchmark development and bias evaluation in LMMs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.07771v1",
    "title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies",
    "year": 2025,
    "authors": [
      "Sibo Ma",
      "Alejandro Salinas",
      "Peter Henderson",
      "Julian Nyarko"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in language models, addressing social discrimination and bias mitigation in AI.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on racial biases in AI models",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Pruning strategies to analyze bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.07637v1",
    "title": "BiaSWE: An Expert Annotated Dataset for Misogyny Detection in Swedish",
    "year": 2025,
    "authors": [
      "Kätriin Kukk",
      "Danila Petrelli",
      "Judit Casademont",
      "Eric J. W. Orlowski",
      "Michał Dzieliński",
      "Maria Jacobson"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on misogyny detection, addressing gender bias and discrimination in Swedish language data, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "Swedish women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Expert Annotation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Expert-annotated dataset capturing cultural nuances",
      "geographic_focus": [
        "Sweden"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.07111v1",
    "title": "Likelihood-Free Estimation for Spatiotemporal Hawkes processes with missing data and application to predictive policing",
    "year": 2025,
    "authors": [
      "Pramit Das",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in crime data affecting vulnerable communities, impacting equitable policing.",
      "inequality_type": [
        "geographic",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias in predictive policing due to missing crime reports",
      "affected_populations": [
        "vulnerable communities",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Likelihood-free estimation with WGANs for missing data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.06439v1",
    "title": "Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain",
    "year": 2025,
    "authors": [
      "Marco Rondina",
      "Antonio Vetrò",
      "Riccardo Coppola",
      "Oumaima Regragrui",
      "Alessandro Fabris",
      "Gianmaria Silvello",
      "Gian Antonio Susto",
      "Juan Carlos De Martin"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination in car insurance pricing based on demographic attributes, highlighting issues of social bias and fairness affecting different social groups.",
      "inequality_type": [
        "racial",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on discrimination based on birthplace and driver profiles",
      "affected_populations": [
        "non-Italian born individuals",
        "car insurance users"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Black box testing of pricing algorithms",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.06341v1",
    "title": "Facial Analysis Systems and Down Syndrome",
    "year": 2025,
    "authors": [
      "Marco Rondina",
      "Fabiana Vinci",
      "Antonio Vetrò",
      "Juan Carlos De Martin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in facial analysis systems affecting people with Down syndrome, highlighting issues of discrimination and stereotyping. It discusses how technology impacts marginalized groups and propagates social biases. The focus on vulnerable populations and bias in AI systems aligns with social inequality concerns.",
      "inequality_type": [
        "disability",
        "health",
        "educational",
        "gender"
      ],
      "other_detail": "Biases in facial recognition technology",
      "affected_populations": [
        "people with Down syndrome",
        "males",
        "adults",
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Testing commercial tools on curated face datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.06317v1",
    "title": "The digital labour of artificial intelligence in Latin America: a comparison of Argentina, Brazil, and Venezuela",
    "year": 2025,
    "authors": [
      "Paola Tubaro",
      "Antonio A. Casilli",
      "Mariana Fernández Massi",
      "Julieta Longo",
      "Juana Torres-Cierpe",
      "Matheus Viana Braz"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines data workers' precarious conditions linked to economic hardship, inequalities, and informality, highlighting socioeconomic disparities and their relation to AI labor in Latin America.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "informational",
        "educational"
      ],
      "other_detail": "Focus on labor precarity and inequality in data work",
      "affected_populations": [
        "data workers",
        "Latin American workers"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Questionnaires and interviews with workers",
      "geographic_focus": [
        "Argentina",
        "Brazil",
        "Venezuela"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.06094v1",
    "title": "Fair-MoE: Fairness-Oriented Mixture of Experts in Vision-Language Models",
    "year": 2025,
    "authors": [
      "Peiran Wang",
      "Linjie Tong",
      "Jiaxiang Liu",
      "Zuozhu Liu"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing biases affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "educational"
      ],
      "other_detail": "Fairness in medical vision-language models",
      "affected_populations": [
        "medical patients",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Fairness-oriented loss and ensemble approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.05649v1",
    "title": "Gender Bias in Instruction-Guided Speech Synthesis Models",
    "year": 2025,
    "authors": [
      "Chun-Yi Kuan",
      "Hung-yi Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI models, highlighting social gender stereotypes and their amplification, which directly relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender stereotypes in AI models",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Testing model responses to occupation prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.05622v2",
    "title": "Social inequality and cultural factors impact the awareness and reaction during the cryptic transmission period of pandemic",
    "year": 2025,
    "authors": [
      "Zhuoren Jiang",
      "Xiaozhong Liu",
      "Yangyang Kang",
      "Changlong Sun",
      "Yong-Yeol Ahn",
      "Johan Bollen"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social inequities in awareness and response during a pandemic, focusing on demographic disparities and cultural factors.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "health",
        "informational",
        "geographic"
      ],
      "other_detail": "Focus on demographic and cultural disparities in pandemic awareness",
      "affected_populations": [
        "vulnerable groups",
        "demographic cohorts"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes e-commerce data and social network modeling",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.05333v1",
    "title": "A Tutorial On Intersectionality in Fair Rankings",
    "year": 2025,
    "authors": [
      "Chiara Criscuolo",
      "Davide Martinenghi",
      "Giuseppe Piccirillo"
    ],
    "categories": [
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biased algorithms and unfair rankings affecting marginalized groups, addressing social discrimination and inequality. It emphasizes intersectionality to ensure fair treatment across social identities.",
      "inequality_type": [
        "race",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on social identities and algorithmic fairness",
      "affected_populations": [
        "marginalized groups",
        "underrepresented groups"
      ],
      "methodology": [
        "Literature Review",
        "Comparative Analysis"
      ],
      "methodology_detail": "Overview of existing fair ranking methodologies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.05099v1",
    "title": "Navigating Automated Hiring: Perceptions, Strategy Use, and Outcomes Among Young Job Seekers",
    "year": 2025,
    "authors": [
      "Lena Armstrong",
      "Danaé Metaxa"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines socioeconomic privilege, perceptions of fairness, and access disparities in AI-driven hiring, highlighting social inequalities.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on privilege and access in automated hiring",
      "affected_populations": [
        "young job seekers",
        "families"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Survey of computer science students' perceptions and strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.04860v1",
    "title": "Where does AI come from? A global case study across Europe, Africa, and Latin America",
    "year": 2025,
    "authors": [
      "Paola Tubaro",
      "Antonio A Casilli",
      "Maxime Cornet",
      "Clément Le Ludec",
      "Juana Torres Cierpe"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how AI supply chains reproduce exclusion and inequality, especially affecting workers in the Global South, impacting remuneration, job security, and working conditions.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "geographic",
        "labor"
      ],
      "other_detail": "Focus on global labor and economic disparities in AI supply chains",
      "affected_populations": [
        "workers in Global South"
      ],
      "methodology": [
        "Case Study",
        "Qualitative Study"
      ],
      "methodology_detail": "Comparative analysis across France, Madagascar, and Venezuela",
      "geographic_focus": [
        "Europe",
        "Africa",
        "Latin America"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.04218v1",
    "title": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data",
    "year": 2025,
    "authors": [
      "Laura Biester"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models using Olympic data, highlighting gender stereotypes and bias against women, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement metrics and data analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.03826v1",
    "title": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing",
    "year": 2025,
    "authors": [
      "Jinya Sakurai",
      "Issei Sato"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in AI-generated images, focusing on mitigating stereotypes related to social groups such as occupations, which relate to social inequality issues like gender and racial bias.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI-generated content",
      "affected_populations": [
        "women",
        "minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias detection and attribute rebalancing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.04386v1",
    "title": "Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings",
    "year": 2025,
    "authors": [
      "Guangyao Zheng",
      "Michael A. Jacobs",
      "Vladimir Braverman",
      "Vishwa S. Parekh"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in medical AI, focusing on demographic information encoding, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "health",
        "age",
        "gender"
      ],
      "other_detail": "Bias mitigation in clinical AI embeddings",
      "affected_populations": [
        "racial groups",
        "age groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Adversarial debiasing of embeddings",
      "geographic_focus": [
        "unspecified/clinical datasets"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.03429v1",
    "title": "On Fairness of Unified Multimodal Large Language Model for Image Generation",
    "year": 2025,
    "authors": [
      "Ming Liu",
      "Hao Chen",
      "Jindong Wang",
      "Liwen Wang",
      "Bhiksha Raj Ramakrishnan",
      "Wensheng Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases related to gender and race in AI outputs, addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Bias mitigation in multimodal AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Benchmarking",
        "Bias auditing",
        "Proposed mitigation strategy"
      ],
      "methodology_detail": "Analyzes bias sources and tests bias reduction methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.02309v1",
    "title": "Review of Demographic Bias in Face Recognition",
    "year": 2025,
    "authors": [
      "Ketan Kotwal",
      "Sebastien Marcel"
    ],
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic bias in face recognition, focusing on fairness and disparities across social groups such as race, ethnicity, and gender, which are key aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Focuses on social group disparities in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Systematic review of existing research and metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.02072v1",
    "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping",
    "year": 2025,
    "authors": [
      "Rajiv Bahl",
      "Venkatesan N",
      "Parimal Aglawe",
      "Aastha Sarasapalli",
      "Bhavya Kancharla",
      "Chaitanya kolukuluri",
      "Harish Mohite",
      "Japneet Hora",
      "Kiran Kakollu",
      "Rahul Diman",
      "Shubham Kapale",
      "Sri Bhagya Kathula",
      "Vamsikrishna Motru",
      "Yogeshwar Reddy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting biases related to gender, caste, age, disability, and socioeconomic status, which are social inequalities. It discusses bias and stereotyping in AI models within specific cultural contexts. The framework aims to address social discrimination issues embedded in language models.",
      "inequality_type": [
        "gender",
        "caste",
        "age",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Cultural context-specific bias detection framework",
      "affected_populations": [
        "women",
        "castes",
        "elderly",
        "disabled",
        "socioeconomic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "System Design"
      ],
      "methodology_detail": "Custom datasets based on Indian Census 2011",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.01926v1",
    "title": "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs",
    "year": 2025,
    "authors": [
      "Angelina Wang",
      "Michelle Phan",
      "Daniel E. Ho",
      "Sanmi Koyejo"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses group differences and fairness in AI, addressing social bias and discrimination issues related to gender and other social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on fairness and discrimination measurement in AI models",
      "affected_populations": [
        "women",
        "Muslims",
        "social groups"
      ],
      "methodology": [
        "Benchmark Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops a benchmark suite and evaluates models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.01713v1",
    "title": "Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool",
    "year": 2025,
    "authors": [
      "Floris Holstege",
      "Mackenzie Jorgensen",
      "Kirtan Padh",
      "Jurriaan Parie",
      "Joel Persson",
      "Krsto Prorokovic",
      "Lukas Snoek"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates bias related to demographic groups, specifically migration background, in a high-stakes decision-making process, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Bias detection in algorithmic decision-making",
      "affected_populations": [
        "students with non-European background",
        "Dutch students"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Unsupervised Clustering",
        "Simulation Studies"
      ],
      "methodology_detail": "Using clustering to detect bias without demographic data",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.01406v1",
    "title": "GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models",
    "year": 2025,
    "authors": [
      "Jonathan Drechsel",
      "Steffen Herbold"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI systems, a form of social inequality, and proposes a debiasing method to mitigate harmful gender biases in transformer models.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Gradient-based feature learning for debiasing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.01267v2",
    "title": "Counterfactual Situation Testing: From Single to Multidimensional Discrimination",
    "year": 2025,
    "authors": [
      "Jose M. Alvarez",
      "Salvatore Ruggieri"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting individual discrimination related to protected attributes like gender and race, addressing social fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "discrimination"
      ],
      "other_detail": "Focuses on intersectional discrimination detection",
      "affected_populations": [
        "gender minorities",
        "racial minorities"
      ],
      "methodology": [
        "Causal Data Mining",
        "Counterfactual Reasoning",
        "Experiment"
      ],
      "methodology_detail": "Uses counterfactual and similarity-based testing approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.01211v1",
    "title": "Privilege Scores",
    "year": 2025,
    "authors": [
      "Ludwig Bothmann",
      "Philip A. Boustani",
      "Jose M. Alvarez",
      "Giuseppe Casalicchio",
      "Bernd Bischl",
      "Susanne Dandl"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper introduces privilege scores to measure and interpret social privilege related to protected attributes like gender and race, addressing bias and fairness in AI systems affecting social groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on privilege and bias in social contexts",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Develops measurement and interpretation methods for privilege scores",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.01188v1",
    "title": "FairUDT: Fairness-aware Uplift Decision Trees",
    "year": 2025,
    "authors": [
      "Anam Zahid",
      "Abdur Rehman Ali",
      "Shaina Raza",
      "Rai Shahnawaz",
      "Faisal Kamiran",
      "Asim Karim"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in machine learning related to protected attributes like gender and race, aiming to detect and mitigate discrimination, which are core aspects of social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "discrimination"
      ],
      "other_detail": "Focus on fairness in AI decision-making processes",
      "affected_populations": [
        "minority groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness-aware uplift decision trees with relabeling",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.19337v1",
    "title": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models",
    "year": 2025,
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models related to marginalized groups, specifically focusing on racial and gender homogeneity bias, which are social inequalities. It analyzes how AI systems may reflect or reinforce social disparities. The focus on marginalized groups indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI models related to marginalized social groups",
      "affected_populations": [
        "Black Americans",
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes token sampling uncertainty measures",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.19231v1",
    "title": "The geography of inequalities in access to healthcare across England: the role of bus travel time variability",
    "year": 2025,
    "authors": [
      "Zihao Chen",
      "Federico Botta"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines geographical inequalities in healthcare access, highlighting spatial and socioeconomic disparities, especially related to deprivation and urban-rural divides.",
      "inequality_type": [
        "geographic",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on healthcare accessibility disparities across regions",
      "affected_populations": [
        "residents in deprived areas",
        "urban and rural populations"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Travel time variability and spatial pattern analysis",
      "geographic_focus": [
        "England"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.19086v1",
    "title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification",
    "year": 2025,
    "authors": [
      "Xiangyu Sun",
      "Xiaoguang Zou",
      "Yuanquan Wu",
      "Guotai Wang",
      "Shaoting Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes fairness and demographic disparities in AI models applied to medical imaging, addressing social bias and fairness issues related to patient demographics.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in medical AI models",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Fairness Analysis",
        "Experiment",
        "Model Evaluation"
      ],
      "methodology_detail": "Assessing fairness across demographics and model tuning methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.18129v1",
    "title": "Revisiting gender bias research in bibliometrics: Standardizing methodological variability using Scholarly Data Analysis (SoDA) Cards",
    "year": 2025,
    "authors": [
      "HaeJin Lee",
      "Shubhanshu Mishra",
      "Apratim Mishra",
      "Zhiwen You",
      "Jinseok Kim",
      "Jana Diesner"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.SI",
      "K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in scholarly metrics, a form of social inequality, and discusses methodological issues affecting gender-related research in academia.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in bibliometrics",
      "affected_populations": [
        "female scholars",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Review of 70 publications on gender bias methods",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.17420v1",
    "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
    "year": 2025,
    "authors": [
      "Yuxuan Li",
      "Hirokazu Shirado",
      "Sauvik Das"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates implicit biases in language models related to sociodemographic groups, highlighting disparities that mirror social inequalities. It discusses biases across categories like race, gender, and sociodemographic status, and their amplification in AI systems. This aligns with social inequality issues in discrimination and fairness in technology.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Implicit biases in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "sociodemographic groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Assessing decision disparities via agent personas",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.18642v1",
    "title": "DebiasPI: Inference-time Debiasing by Prompt Iteration of a Text-to-Image Generative Model",
    "year": 2025,
    "authors": [
      "Sarah Bonna",
      "Yu-Cheng Huang",
      "Ekaterina Novozhilova",
      "Sejin Paik",
      "Zhengyang Shan",
      "Michelle Yilin Feng",
      "Ge Gao",
      "Yonish Tayal",
      "Rushil Kulkarni",
      "Jialin Yu",
      "Nupur Divekar",
      "Deepti Ghadiyaram",
      "Derry Wijaya",
      "Margrit Betke"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating demographic biases related to race and gender in AI-generated images, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation in AI-generated visual representations",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Prompt Engineering"
      ],
      "methodology_detail": "Inference-time prompt iteration for bias control",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.15985v1",
    "title": "Demographic Benchmarking: Bridging Socio-Technical Gaps in Bias Detection",
    "year": 2025,
    "authors": [
      "Gemma Galdon Clavell",
      "Rubén González-Sendino",
      "Paola Vazquez"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on demographic benchmarking, measuring impacts on populations, and reducing bias, which relates to social disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on demographic groups impacted by AI models",
      "affected_populations": [
        "demographic groups",
        "AI users",
        "target populations"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Benchmarking and measuring demographic impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.15920v1",
    "title": "Vienna Mosaic: Navigating Social Borders in a Melting Pot",
    "year": 2025,
    "authors": [
      "Marc Sadurní",
      "Samuel Martin-Gutierrez",
      "Ola Ali",
      "Ana María Jaramillo",
      "Rafael Prieto-Curiel",
      "Fariba Karimi"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI",
      "physics.data-an"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes urban segregation, which relates to socioeconomic and racial inequalities, impacting migrant communities. It examines disparities in co-residence preferences influenced by wealth and nationality, highlighting social inequality mechanisms.",
      "inequality_type": [
        "economic",
        "wealth",
        "socioeconomic",
        "racial",
        "ethnic",
        "nationality",
        "geographic"
      ],
      "other_detail": "Focus on urban segregation and migrant integration",
      "affected_populations": [
        "migrants",
        "local residents"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using administrative data analysis",
      "geographic_focus": [
        "Vienna"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.16399v1",
    "title": "Detecting clinician implicit biases in diagnoses using proximal causal inference",
    "year": 2025,
    "authors": [
      "Kara Liu",
      "Russ Altman",
      "Vasilis Syrgkanis"
    ],
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates implicit biases in healthcare affecting marginalized groups, addressing social discrimination and inequality. It focuses on racial, health, and sociodemographic disparities in medical outcomes.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Implicit biases impacting marginalized patient groups",
      "affected_populations": [
        "disadvantaged groups",
        "racial minorities",
        "healthcare patients"
      ],
      "methodology": [
        "Causal Inference",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using proximal causal inference on medical datasets",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.15775v1",
    "title": "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image Models?",
    "year": 2025,
    "authors": [
      "Yunbo Lyu",
      "Zhou Yang",
      "Yuqing Niu",
      "Jing Jiang",
      "David Lo"
    ],
    "categories": [
      "cs.CV",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI-generated images, highlighting social discrimination related to gender. It assesses how AI models perpetuate or amplify gender stereotypes, which relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Manual Labeling",
        "Comparison Analysis"
      ],
      "methodology_detail": "Validation of bias detectors against human-labeled data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.15634v1",
    "title": "Be Intentional About Fairness!: Fairness, Size, and Multiplicity in the Rashomon Set",
    "year": 2025,
    "authors": [
      "Gordon Dai",
      "Pavan Ravishankar",
      "Rachel Yuan",
      "Daniel B. Neill",
      "Emily Black"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI models, which relates to social bias and discrimination issues affecting groups. It emphasizes the importance of fairness metrics and the impact on individuals and groups, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on fairness and bias in AI decision-making",
      "affected_populations": [
        "individuals",
        "social groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Sampling Methods",
        "Fairness Metrics Evaluation"
      ],
      "methodology_detail": "Analyzes fairness metrics and model sampling techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.15430v1",
    "title": "Evaluating Simple Debiasing Techniques in RoBERTa-based Hate Speech Detection Models",
    "year": 2025,
    "authors": [
      "Diana Iftimie",
      "Erik Zinn"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in hate speech detection, highlighting disparities affecting African American English speakers, which relates to racial inequality and social bias.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in AI systems",
      "affected_populations": [
        "African American English speakers"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "Evaluates debiasing techniques in NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.15351v1",
    "title": "Fairness in LLM-Generated Surveys",
    "year": 2025,
    "authors": [
      "Andrés Abeliuk",
      "Vanessa Gaete",
      "Naim Bro"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs across socio-demographic groups, highlighting disparities related to race, gender, education, and geography, which are core aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "geographic"
      ],
      "other_detail": "Focuses on socio-demographic biases in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "educational groups",
        "geographic populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey",
        "Model Development"
      ],
      "methodology_detail": "Analyzing survey data and bias measurement frameworks",
      "geographic_focus": [
        "Chile",
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.14551v1",
    "title": "Fairness of Deep Ensembles: On the interplay between per-group task difficulty and under-representation",
    "year": 2025,
    "authors": [
      "Estanislao Claucich",
      "Sara Hooker",
      "Diego H. Milone",
      "Enzo Ferrante",
      "Rodrigo Echeveste"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI models across demographic groups, addressing biases related to protected attributes such as gender and ethnicity, and examines disparities among subgroups, which are core concerns of social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "educational"
      ],
      "other_detail": "Focuses on algorithmic fairness and subgroup disparities",
      "affected_populations": [
        "demographic groups",
        "under-represented subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes ensemble methods and subgroup performance",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.14457v1",
    "title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing",
    "year": 2025,
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in large language models, a social inequality issue related to gender discrimination and fairness in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Bias evaluation and neuron editing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.14110v2",
    "title": "Developing a Fair Online Recruitment Framework Based on Job-seekers' Fairness Concerns",
    "year": 2025,
    "authors": [
      "Changyang He",
      "Yue Deng",
      "Alessandro Fabris",
      "Bo Li",
      "Asia Biega"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness concerns in online recruitment, highlighting discrimination and power imbalances affecting job-seekers, which relate to social inequalities such as discrimination based on attributes like gender or race.",
      "inequality_type": [
        "gender",
        "racial",
        "disability",
        "educational"
      ],
      "other_detail": "Focuses on fairness concerns in digital hiring systems",
      "affected_populations": [
        "job-seekers",
        "disadvantaged groups"
      ],
      "methodology": [
        "Qualitative Study",
        "System Design"
      ],
      "methodology_detail": "Analyzes fairness concerns and derives design implications",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13868v1",
    "title": "Lost in Siting: The Hidden Carbon Cost of Inequitable Residential Solar Installations",
    "year": 2025,
    "authors": [
      "Cooper Sigrist",
      "Adam Lechowicz",
      "Jovan Champ",
      "Noman Bashir",
      "Mohammad Hajiesmaili"
    ],
    "categories": [
      "cs.CE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in solar installation distribution across racial and income groups, highlighting inequities. It analyzes how these disparities impact environmental benefits and climate outcomes for different social groups.",
      "inequality_type": [
        "racial",
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on racial and income disparities in solar siting",
      "affected_populations": [
        "Black communities",
        "low-income neighborhoods"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Data analysis of solar distribution and carbon offset potential",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13836v1",
    "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages",
    "year": 2025,
    "authors": [
      "Farhana Shahid",
      "Mona Elswah",
      "Aditya Vashistha"
    ],
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines systemic biases in AI moderation affecting marginalized language groups, highlighting inequalities rooted in data access and linguistic complexity, which reinforce historical power imbalances.",
      "inequality_type": [
        "linguistic",
        "informational",
        "digital",
        "geographic"
      ],
      "other_detail": "Systemic biases in AI moderation pipelines",
      "affected_populations": [
        "Low-resource language speakers",
        "Global South communities"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews",
        "System Analysis"
      ],
      "methodology_detail": "Interviews with AI researchers and practitioners",
      "geographic_focus": [
        "South Asia",
        "East Africa",
        "North Africa",
        "South America"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.19407v2",
    "title": "Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces Intergenerational Inequality",
    "year": 2025,
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Pattie Maes"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates surname biases influencing AI decisions, reinforcing intergenerational inequality, which relates to socioeconomic and class disparities.",
      "inequality_type": [
        "wealth",
        "socioeconomic",
        "class"
      ],
      "other_detail": "Biases linked to social status markers in surnames",
      "affected_populations": [
        "low-status surname holders"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluation of AI decision biases across surname categories",
      "geographic_focus": [
        "United States",
        "Thailand"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13302v1",
    "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers",
    "year": 2025,
    "authors": [
      "Akshit Achara",
      "Anshuman Chhabra"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and potential disparities in AI safety classifiers, focusing on their impact on different social groups, which relates to social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focus on fairness and robustness in content moderation",
      "affected_populations": [
        "minority users",
        "general social media users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness metrics and robustness testing of classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13257v1",
    "title": "Investigating the Developer eXperience of LGBTQIAPN+ People in Agile Teams",
    "year": 2025,
    "authors": [
      "Edvaldo Wassouf Jr",
      "Pedro Fukuda",
      "Awdren Fontão"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination, visibility, and harassment faced by LGBTQIAPN+ professionals, highlighting social biases and inclusivity issues in the workplace.",
      "inequality_type": [
        "gender",
        "sexual orientation",
        "disability"
      ],
      "other_detail": "Focus on workplace social inclusion and bias reduction",
      "affected_populations": [
        "LGBTQIAPN+ professionals"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Survey of LGBTQIAPN+ developers in agile teams",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13223v2",
    "title": "Scaling for Fairness? Analyzing Model Size, Data Composition, and Multilinguality in Vision-Language Bias",
    "year": 2025,
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to gender and race in AI models, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social bias in vision-language AI systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement across datasets and model variations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13219v1",
    "title": "Enhancing Multi-Attribute Fairness in Healthcare Predictive Modeling",
    "year": 2025,
    "authors": [
      "Xiaoyang Wang",
      "Christopher C. Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare AI, focusing on disparities across demographic attributes such as race, gender, and health, which are key social inequality factors.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focuses on multi-attribute fairness in healthcare AI",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Optimization"
      ],
      "methodology_detail": "Multi-attribute fairness optimization approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13061v1",
    "title": "Systematic comparison of gender inequality in scientific rankings across disciplines",
    "year": 2025,
    "authors": [
      "Ana Maria Jaramillo",
      "Mariana Macedo",
      "Marcos Oliveira",
      "Fariba Karimi",
      "Ronaldo Menezes"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in academic participation, rankings, and career progression, addressing gender inequality in science.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focuses on gender representation and career advancement",
      "affected_populations": [
        "women in academia"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of publication and citation data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.12897v1",
    "title": "Discrimination and AI in insurance: what do people find fair? Results from a survey",
    "year": 2025,
    "authors": [
      "Frederik Zuiderveen Borgesius",
      "Marvin van Bekkum",
      "Iris van Ooijen",
      "Gabi Schaap",
      "Maaike Harbers",
      "Tjerk Timan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines perceptions of fairness related to insurance practices affecting different social groups, including concerns about higher prices for poorer people, group-specific products, and fairness perceptions tied to socioeconomic status.",
      "inequality_type": [
        "income",
        "wealth",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on fairness perceptions and social impact",
      "affected_populations": [
        "poorer people",
        "specific social groups"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Public opinion survey on fairness perceptions",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.12538v3",
    "title": "Academic case reports lack diversity: Assessing the presence and diversity of sociodemographic and behavioral factors related to Post COVID-19 Condition",
    "year": 2025,
    "authors": [
      "Juan Andres Medina Florez",
      "Shaina Raza",
      "Rashida Lynn",
      "Zahra Shakeri",
      "Brendan T. Smith",
      "Elham Dolatabadi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study analyzes disparities in sociodemographic factors within PCC case reports, highlighting underrepresentation of sensitive groups like race and housing status, which relates to social inequalities.",
      "inequality_type": [
        "racial",
        "housing",
        "health",
        "age"
      ],
      "other_detail": "Focus on sociodemographic disparities in health data",
      "affected_populations": [
        "racial minorities",
        "low-income groups",
        "vulnerable populations"
      ],
      "methodology": [
        "Natural Language Processing",
        "Data Analysis",
        "Corpus Construction"
      ],
      "methodology_detail": "Using NLP to analyze sociodemographic disparities in reports",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.12020v1",
    "title": "On the \"Illusion\" of Gender Bias in Face Recognition: Explaining the Fairness Issue Through Non-demographic Attributes",
    "year": 2025,
    "authors": [
      "Paul Jonas Kurz",
      "Haiyu Wu",
      "Kevin W. Bowyer",
      "Philipp Terhörst"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in face recognition, linking it to social definitions of appearance rather than biology, thus addressing gender-related social inequality and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on social gender bias in AI",
      "affected_populations": [
        "male",
        "female"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "decorrelation and fairness metrics for large-scale data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.11752v2",
    "title": "Are generative models fair? A study of racial bias in dermatological image generation",
    "year": 2025,
    "authors": [
      "Miguel López-Pérez",
      "Søren Hauberg",
      "Aasa Feragen"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in dermatological AI models, highlighting disparities across skin tones, which relates directly to racial inequality and fairness issues.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focuses on racial bias in medical AI models",
      "affected_populations": [
        "darker skin tones",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Model Development",
        "Dataset Creation"
      ],
      "methodology_detail": "Training and evaluating generative models on skin images",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.13120v1",
    "title": "Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness",
    "year": 2025,
    "authors": [
      "Ambreesh Parthasarathy",
      "Chandrasekar Subramanian",
      "Ganesh Senrayan",
      "Shreyash Adappanavar",
      "Aparna Taneja",
      "Balaraman Ravindran",
      "Milind Tambe"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias issues related to language resource disparities affecting marginalized groups, especially in developing countries. It analyzes how language and prompt complexity impact fairness in AI reward functions, highlighting social disparities.",
      "inequality_type": [
        "linguistic",
        "digital",
        "geographic",
        "educational"
      ],
      "other_detail": "Language resource disparities and fairness implications",
      "affected_populations": [
        "low-resource language speakers",
        "developing country communities"
      ],
      "methodology": [
        "Experiment",
        "Synthetic Environment Analysis"
      ],
      "methodology_detail": "Testing LLM reward functions across languages and prompts",
      "geographic_focus": [
        "India",
        "developing countries"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.11597v1",
    "title": "Fairness Testing through Extreme Value Theory",
    "year": 2025,
    "authors": [
      "Verya Monjezi",
      "Ashutosh Trivedi",
      "Vladik Kreinovich",
      "Saeid Tizpaz-Niari"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on societal impacts and discrimination, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on societal attitudes and discrimination shifts",
      "affected_populations": [
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness testing using EVT and generative AI",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.11214v1",
    "title": "Mitigating Spatial Disparity in Urban Prediction Using Residual-Aware Spatiotemporal Graph Neural Networks: A Chicago Case Study",
    "year": 2025,
    "authors": [
      "Dingyi Zhuang",
      "Hanyong Xu",
      "Xiaotong Guo",
      "Yunhan Zheng",
      "Shenhao Wang",
      "Jinhua Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses spatial disparities and resource allocation in urban areas, focusing on fairness and equitable prediction outcomes, which relate to social inequalities such as socioeconomic and geographic disparities.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on spatial and demographic disparities in urban prediction",
      "affected_populations": [
        "urban communities",
        "central regions"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Includes residual-aware attention and fairness metrics",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.10150v2",
    "title": "Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair Language Modeling and Translation",
    "year": 2025,
    "authors": [
      "Tomasz Limisiewicz",
      "David Mareček",
      "Tomáš Musil"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI language systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias reduction techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.10484v1",
    "title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude",
    "year": 2025,
    "authors": [
      "Yile Yan",
      "Yuqi Zhu",
      "Wentao Xu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to protected attributes such as race, gender, age, and disability, which are directly linked to social inequalities and discrimination. It analyzes how AI models reflect and potentially reinforce these biases, impacting social groups differently.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "disability"
      ],
      "other_detail": "Focus on biases in ethical decision-making",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly",
        "disabled individuals"
      ],
      "methodology": [
        "Experiment",
        "Systematic Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed model responses across multiple protected attributes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.09687v1",
    "title": "U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection",
    "year": 2025,
    "authors": [
      "Jiaee Cheong",
      "Aditya Bangar",
      "Sinan Kalkan",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in depression detection, focusing on gender-based disparities, which relates to social gender inequality.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender fairness in mental health AI",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Gender-based task reweighting using uncertainty",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.09534v1",
    "title": "AI in Support of Diversity and Inclusion",
    "year": 2025,
    "authors": [
      "Çiçek Güven",
      "Afra Alishahi",
      "Henry Brighton",
      "Gonzalo Nápoles",
      "Juan Sebastian Olier",
      "Marie Šafář",
      "Eric Postma",
      "Dimitar Shterionov",
      "Mirella De Sisto",
      "Eva Vanmassenhove"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases, stereotypes, and social group portrayals in AI, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias",
        "discrimination"
      ],
      "other_detail": "Focus on social biases and representation in AI systems",
      "affected_populations": [
        "LGBTQ+ community",
        "deaf people",
        "social groups"
      ],
      "methodology": [
        "Research Projects",
        "Case Study",
        "System Design"
      ],
      "methodology_detail": "Includes project demonstrations and system implementations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.09482v1",
    "title": "Building Bridges across Papua New Guinea's Digital Divide in Growing the ICT Industry",
    "year": 2025,
    "authors": [
      "Marc Cheong",
      "Sankwi Abuzo",
      "Hideaki Hata",
      "Priscilla Kevin",
      "Winifred Kula",
      "Benson Mirou",
      "Christoph Treude",
      "Dong Wang",
      "Raula Gaikovina Kula"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses digital divide, access to ICT, and OSS adoption affecting PNG users, highlighting inequalities related to geographic and social boundaries.",
      "inequality_type": [
        "digital",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on digital access and technological equity",
      "affected_populations": [
        "PNG users",
        "emerging economy populations"
      ],
      "methodology": [
        "Literature Review",
        "Case Study"
      ],
      "methodology_detail": "Analysis of workshop outcomes and existing research",
      "geographic_focus": [
        "Papua New Guinea"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.09409v2",
    "title": "mGeNTE: A Multilingual Resource for Gender-Neutral Language and Translation",
    "year": 2025,
    "authors": [
      "Beatrice Savoldi",
      "Eleonora Cupin",
      "Manjinder Thind",
      "Anne Lauscher",
      "Andrea Piergentili",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language and translation, reflecting social inequalities related to gender representation and inclusivity in AI and linguistic technologies.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender-neutral language and translation",
      "affected_populations": [
        "gender minorities",
        "non-binary individuals"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Develops multilingual dataset for gender-neutral translation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.09014v1",
    "title": "How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias",
    "year": 2025,
    "authors": [
      "Tosin Fadahunsi",
      "Giordano d'Aloisio",
      "Antinisca Di Marco",
      "Federica Sarro"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and ethnicity in AI-generated images, highlighting societal disparities. It analyzes how these biases reinforce existing social inequalities within the software engineering domain. The focus on racial and gender disparities indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias amplification in AI-generated content",
      "affected_populations": [
        "women",
        "Black",
        "Arab",
        "Asian",
        "White"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement in generated images",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.08910v1",
    "title": "Lights, Camera, Matching: The Role of Image Illumination in Fair Face Recognition",
    "year": 2025,
    "authors": [
      "Gabriella Pangelinan",
      "Grace Bezold",
      "Haiyu Wu",
      "Michael C. King",
      "Kevin W. Bowyer"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in face recognition accuracy, a social bias issue.",
      "inequality_type": [
        "racial",
        "fairness"
      ],
      "other_detail": "Focuses on demographic fairness in AI systems",
      "affected_populations": [
        "Caucasian females",
        "African American females"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Brightness balancing to reduce accuracy gap",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.08497v1",
    "title": "Addressing Intersectionality, Explainability, and Ethics in AI-Driven Diagnostics: A Rebuttal and Call for Transdiciplinary Action",
    "year": 2025,
    "authors": [
      "Myles Joshua Toledo Tan",
      "Panayiotis V. Benos"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses fairness, intersectionality, and disparities in AI diagnostics, addressing social biases and inequalities affecting diverse populations.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on health disparities and social biases in AI",
      "affected_populations": [
        "diverse patient groups",
        "marginalized communities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Integrates social sciences, ethics, and public health perspectives",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.08441v1",
    "title": "Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection, and Debiasing Strategies",
    "year": 2025,
    "authors": [
      "Ajwad Abrar",
      "Nafisa Tabassum Oeshy",
      "Mohsinul Kabir",
      "Sophia Ananiadou"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates religious bias in AI models, highlighting social discrimination and fairness issues affecting religious groups, intersecting with demographic factors.",
      "inequality_type": [
        "religion",
        "social bias",
        "fairness"
      ],
      "other_detail": "Focus on religious bias in AI systems",
      "affected_populations": [
        "religious groups",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Analysis"
      ],
      "methodology_detail": "Probing prompts, bias detection, debiasing evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.08429v1",
    "title": "Modeling Discrimination with Causal Abstraction",
    "year": 2025,
    "authors": [
      "Milan Mossé",
      "Kara Schechtman",
      "Frederick Eberhardt",
      "Thomas Icard"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial discrimination, a key aspect of social inequality, by proposing a causal framework for modeling discrimination related to race. It discusses social construction and causal effects, directly engaging with issues of racial bias. The focus on discrimination and race indicates a clear concern with social inequality.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial discrimination modeling",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Framework for causal abstraction of race",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.08401v2",
    "title": "Navigating Gender Disparities in Communication Research Leadership: Academic Recognition, Career Development, and Compensation",
    "year": 2025,
    "authors": [
      "Diego F. M. Oliveira",
      "Qian Huang"
    ],
    "categories": [
      "cs.DL",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in recognition, career development, and compensation, addressing gender inequality in academia.",
      "inequality_type": [
        "gender",
        "educational",
        "economic"
      ],
      "other_detail": "Focuses on gender disparities in communication research leadership",
      "affected_populations": [
        "female academics",
        "female researchers"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Citation metrics, authorship, salary data analysis",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.10453v1",
    "title": "Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation",
    "year": 2025,
    "authors": [
      "Shuzhou Sun",
      "Li Liu",
      "Yongxiang Liu",
      "Zhen Liu",
      "Shuanghui Zhang",
      "Janne Heikkilä",
      "Xiang Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to social attributes like gender, race, and age, which are central to social inequalities. It discusses societal discrimination, stereotypes, and fairness issues in AI systems, impacting marginalized groups.",
      "inequality_type": [
        "gender",
        "racial",
        "age",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focus on societal biases in AI models",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly",
        "students"
      ],
      "methodology": [
        "Experiment",
        "System Design",
        "Bias Testing"
      ],
      "methodology_detail": "Bias detection and mitigation techniques in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.08155v1",
    "title": "FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification",
    "year": 2025,
    "authors": [
      "Nurit Cohen-Inger",
      "Lior Rokach",
      "Bracha Shapira",
      "Seffi Cohen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in algorithmic decision-making, addressing bias against unprivileged groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI decision systems",
      "affected_populations": [
        "unprivileged groups",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Evaluation",
        "Post-processing Adjustment"
      ],
      "methodology_detail": "Bias mitigation applied post-training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.07885v1",
    "title": "Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal Modeling",
    "year": 2025,
    "authors": [
      "Min Sik Byun",
      "Wendy Wan Yee Hui",
      "Wai Kwong Lau"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI classification, highlighting social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in facial emotion recognition",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Causal modeling for bias adjustment",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.07690v1",
    "title": "An Investigation of Experiences Engaging the Margins in Data-Centric Innovation",
    "year": 2025,
    "authors": [
      "Gabriella Thompson",
      "Ebtesam Al Haque",
      "Paulette Blanc",
      "Meme Styles",
      "Denae Ford",
      "Angela D. R. Smith",
      "Brittany Johnson"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses systemic inequities affecting dataset representation, highlighting age and identity as factors. It addresses social disparities influencing data and technology outcomes.",
      "inequality_type": [
        "age",
        "identity",
        "socioeconomic"
      ],
      "other_detail": "Focus on representation barriers in data-centric innovation",
      "affected_populations": [
        "demographics",
        "underrepresented groups"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Survey of technologists and researchers",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.07324v1",
    "title": "Foundation Models at Work: Fine-Tuning for Fairness in Algorithmic Hiring",
    "year": 2025,
    "authors": [
      "Buse Sibel Korkmaz",
      "Rahul Nair",
      "Elizabeth M. Daly",
      "Evangelos Anagnostopoulos",
      "Christos Varytimidis",
      "Antonio del Rio Chanona"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in hiring algorithms, focusing on fairness and diversity, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social bias"
      ],
      "other_detail": "Bias mitigation in algorithmic hiring",
      "affected_populations": [
        "job candidates",
        "underrepresented groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Reinforcement Learning"
      ],
      "methodology_detail": "Fine-tuning models for fairness using feedback",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.07193v1",
    "title": "Combined effect of incentives and coupling in multigames in two-layer networks",
    "year": 2025,
    "authors": [
      "Luo-Luo Jiang",
      "Yi-Ming Li",
      "Wen-Jing Li",
      "Attila Szolnoki"
    ],
    "categories": [
      "physics.soc-ph",
      "cond-mat.stat-mech",
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates cooperation and wealth distribution, focusing on inequality reduction.",
      "inequality_type": [
        "income",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focuses on wealth disparity and cooperation dynamics",
      "affected_populations": [
        "society members",
        "large populations"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Game theory models and network analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.06913v1",
    "title": "Towards Fair and Privacy-Aware Transfer Learning for Educational Predictive Modeling: A Case Study on Retention Prediction in Community Colleges",
    "year": 2025,
    "authors": [
      "Chengyuan Yao",
      "Carmen Cortez",
      "Renzhe Yu"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and performance disparities in predictive models across social groups in education.",
      "inequality_type": [
        "educational",
        "fairness"
      ],
      "other_detail": "Focus on equitable access to predictive analytics in education",
      "affected_populations": [
        "community college students",
        "demographic groups"
      ],
      "methodology": [
        "Transfer Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Model transfer and fairness evaluation techniques",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.06795v1",
    "title": "Bridging the Fairness Gap: Enhancing Pre-trained Models with LLM-Generated Sentences",
    "year": 2025,
    "authors": [
      "Liu Yu",
      "Ludie Guo",
      "Ping Kuang",
      "Fan Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social inequality, and discusses fairness in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Causal Analysis",
        "Experiment"
      ],
      "methodology_detail": "Filtering and aligning sentences for debiasing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.06753v1",
    "title": "Procedural Fairness and Its Relationship with Distributive Fairness in Machine Learning",
    "year": 2025,
    "authors": [
      "Ziming Wang",
      "Changwu Huang",
      "Ke Tang",
      "Xin Yao"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in machine learning, focusing on biases affecting social groups and decision-making equity, which relates to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "disadvantaged groups",
        "minority populations"
      ],
      "methodology": [
        "Experiment",
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness metrics validation across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.06366v2",
    "title": "Counterfactually Fair Reinforcement Learning via Sequential Data Preprocessing",
    "year": 2025,
    "authors": [
      "Jitao Wang",
      "Chengchun Shi",
      "John D. Piette",
      "Joshua R. Loftus",
      "Donglin Zeng",
      "Zhenke Wu"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare AI, focusing on disparities affecting socioeconomically disadvantaged groups, which relates to social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "health"
      ],
      "other_detail": "Fairness in healthcare AI decision making",
      "affected_populations": [
        "disadvantaged patients",
        "subpopulations"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Causal Inference",
        "Statistical Analysis"
      ],
      "methodology_detail": "Sequential data preprocessing for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.06365v1",
    "title": "Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts",
    "year": 2025,
    "authors": [
      "Elizabeth Schaefer",
      "Kirk Roberts"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in medical language models, impacting gender fairness. It focuses on reducing gendered occupational bias, a form of social discrimination. This relates to gender inequality in AI applications.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "medical professionals",
        "researchers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Model Development",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias mitigation pipeline and model training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.05989v1",
    "title": "Addressing speaker gender bias in large scale speech translation systems",
    "year": 2025,
    "authors": [
      "Shubham Bansal",
      "Vikas Joshi",
      "Harveen Chadha",
      "Rupeshkumar Mehta",
      "Jinyu Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in speech translation, impacting gender fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI systems",
      "affected_populations": [
        "female speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Using LLMs and fine-tuning for gender-specific translation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.05926v1",
    "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
    "year": 2025,
    "authors": [
      "Ruby Ostrow",
      "Adam Lopez"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to gender and sexual minorities, addressing social discrimination and bias in AI systems. It highlights how these biases can perpetuate stereotypes and harm marginalized groups.",
      "inequality_type": [
        "gender",
        "sexuality"
      ],
      "other_detail": "Focuses on social stereotypes and representational harms",
      "affected_populations": [
        "sexual minorities",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Survey"
      ],
      "methodology_detail": "Analyzes model responses and generated text stereotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.05197v1",
    "title": "An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes",
    "year": 2025,
    "authors": [
      "Drago Plecko",
      "Paul Secombe",
      "Andrea Clarke",
      "Amelia Fiske",
      "Samarra Toby",
      "Donisha Duff",
      "David Pilcher",
      "Leo Anthony Celi",
      "Rinaldo Bellomo",
      "Elias Bareinboim"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial disparities in ICU outcomes, highlighting health inequities. It analyzes how access and outcomes differ across racial groups, addressing social disparities. The development of a monitoring system further emphasizes social inequality concerns.",
      "inequality_type": [
        "racial",
        "health",
        "geographic"
      ],
      "other_detail": "Focus on racial health disparities in ICU outcomes",
      "affected_populations": [
        "Indigenous Australians",
        "Non-Indigenous Australians",
        "African-Americans",
        "White Americans"
      ],
      "methodology": [
        "Causal Inference",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using causal inference to analyze disparities",
      "geographic_focus": [
        "Australia",
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.04683v2",
    "title": "Toward Sufficient Statistical Power in Algorithmic Bias Assessment: A Test for ABROCA",
    "year": 2025,
    "authors": [
      "Conrad Borchers"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on algorithmic fairness in educational data mining, addressing bias measurement and significance testing, which relates to social inequalities in education.",
      "inequality_type": [
        "educational",
        "social",
        "inequity"
      ],
      "other_detail": "Focuses on fairness assessment in educational AI models",
      "affected_populations": [
        "students",
        "demographic groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Simulation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Nonparametric tests and power analysis methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.04363v1",
    "title": "Neighborhood Disparities in Smart City Service Adoption",
    "year": 2025,
    "authors": [
      "Shahaf Donio",
      "Eran Toch"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in smart city service adoption across neighborhoods, highlighting how place influences digital proficiency and privacy perceptions, which relate to social inequalities. It addresses how urban location impacts access and usage, reflecting broader social disparities.",
      "inequality_type": [
        "geographic",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Focus on neighborhood-level disparities in urban areas",
      "affected_populations": [
        "urban residents",
        "Tel Aviv neighborhoods"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Analysis of survey and digital service data",
      "geographic_focus": [
        "Tel Aviv"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.04316v1",
    "title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts",
    "year": 2025,
    "authors": [
      "Preethi Seshadri",
      "Seraphina Goldfarb-Tarrant"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI hiring systems, highlighting potential discriminatory outcomes affecting social groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Biases in AI decision-making processes",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Synthetic dataset and bias analysis in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2502.00014v1",
    "title": "Algorithmic Bias and the New Chicago School",
    "year": 2025,
    "authors": [
      "Jyh-An Lee"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases in AI affecting decisions impacting individuals and society, implying social disparities. It addresses algorithmic bias related to demographic features, which can perpetuate social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on algorithmic bias and societal impact",
      "affected_populations": [
        "minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Framework development based on Lessig's theory",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.05482v1",
    "title": "HP-BERT: A framework for longitudinal study of Hinduphobia on social media via LLMs",
    "year": 2025,
    "authors": [
      "Ashutosh Singh",
      "Rohitash Chandra"
    ],
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines Hinduphobia, a form of social discrimination based on ethnicity and religion, and analyzes its prevalence and impact during COVID-19, addressing social bias and community polarization.",
      "inequality_type": [
        "racial",
        "religion"
      ],
      "other_detail": "Focus on religious discrimination and social bias",
      "affected_populations": [
        "Hindu community",
        "social media users"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Using LLMs for abuse detection and sentiment analysis",
      "geographic_focus": [
        "India",
        "Australia",
        "Brazil",
        "Indonesia",
        "Japan",
        "United Kingdom"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.04142v1",
    "title": "BiasGuard: Guardrailing Fairness in Machine Learning Production Systems",
    "year": 2025,
    "authors": [
      "Nurit Cohen-Inger",
      "Seffi Cohen",
      "Neomi Rabaev",
      "Lior Rokach",
      "Bracha Shapira"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems affecting social groups, addressing bias and equitable outcomes, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI deployment across social groups",
      "affected_populations": [
        "privileged groups",
        "unprivileged groups"
      ],
      "methodology": [
        "Experiment",
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness metrics and data synthesis techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.03946v2",
    "title": "Proxy Discrimination After Students for Fair Admissions",
    "year": 2025,
    "authors": [
      "Frank Fagan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses proxy discrimination in algorithms, focusing on racial and protected class variables, which directly relates to social inequality and fairness issues.",
      "inequality_type": [
        "racial",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "focus on proxy discrimination in decision-making algorithms",
      "affected_populations": [
        "racial minorities",
        "students",
        "loan applicants"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Legal Analysis",
        "Comparative Analysis"
      ],
      "methodology_detail": "Develops legal and algorithmic proxy discrimination tests",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.03594v1",
    "title": "InclusiViz: Visual Analytics of Human Mobility Data for Understanding and Mitigating Urban Segregation",
    "year": 2025,
    "authors": [
      "Yue Yu",
      "Yifang Wang",
      "Yongjun Zhang",
      "Huamin Qu",
      "Dongyu Liu"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses urban segregation, which relates to racial, socioeconomic, and spatial inequalities, and discusses interventions to promote inclusivity.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Focus on urban segregation and social inclusion",
      "affected_populations": [
        "racial groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Deep Learning",
        "Visual Analytics",
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Predictive modeling and visualization of mobility patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.03569v1",
    "title": "What Does a Software Engineer Look Like? Exploring Societal Stereotypes in LLMs",
    "year": 2025,
    "authors": [
      "Muneera Bano",
      "Hashini Gunatilake",
      "Rashina Hoda"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and race in LLM outputs, highlighting societal stereotypes that influence professional opportunities and representation, thus addressing social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias reinforcement in AI systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Text and image analysis"
      ],
      "methodology_detail": "Generating and evaluating profiles and images for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.02442v1",
    "title": "Unsupervised Search for Ethnic Minorities' Medical Segmentation Training Set",
    "year": 2025,
    "authors": [
      "Yixiao Chen",
      "Yue Yao",
      "Ruining Yang",
      "Md Zakir Hossain",
      "Ashu Gupta",
      "Tom Gedeon"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in medical imaging datasets and their impact on minority populations, highlighting issues of bias and inequity in healthcare outcomes.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in medical AI datasets",
      "affected_populations": [
        "minority racial groups",
        "Black individuals"
      ],
      "methodology": [
        "Algorithm Search",
        "Data Selection",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Greedy algorithm for dataset balancing",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.02211v1",
    "title": "Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4",
    "year": 2025,
    "authors": [
      "Messi H. J. Lee"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI models, which are social inequalities. It analyzes how these biases persist and vary with hyperparameter adjustments, indicating a focus on social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias robustness across social group representations",
      "affected_populations": [
        "Black Americans",
        "White Americans",
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using vector representations to compare group similarities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.01889v1",
    "title": "Exploring Equality: An Investigation into Custom Loss Functions for Fairness Definitions",
    "year": 2025,
    "authors": [
      "Gordon Lee",
      "Simeon Sayer"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI systems like COMPAS, addressing racial and social biases affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on fairness metrics and algorithmic bias mitigation",
      "affected_populations": [
        "criminal defendants",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Custom loss functions and fairness evaluation procedures",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.01785v1",
    "title": "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms",
    "year": 2025,
    "authors": [
      "Qinyi Liu",
      "Oscar Deho",
      "Farhad Vadiee",
      "Mohammad Khalil",
      "Srecko Joksimovic",
      "George Siemens"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and privacy in learning analytics, which relate to social discrimination and bias issues affecting different social groups.",
      "inequality_type": [
        "educational",
        "digital",
        "informational"
      ],
      "other_detail": "Focuses on fairness and privacy in AI models",
      "affected_populations": [
        "students",
        "educational users"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Algorithms",
        "Synthetic Data Generation"
      ],
      "methodology_detail": "Evaluates fairness and privacy trade-offs in synthetic data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.03259v1",
    "title": "Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens",
    "year": 2025,
    "authors": [
      "Abdullah Mushtaq",
      "Muhammad Rafay Naeem",
      "Muhammad Imran Taj",
      "Ibrahim Ghaznavi",
      "Junaid Qadir"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.MA"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines cultural biases in LLMs, highlighting issues of bias and fairness across different cultural groups, which relate to social inequality concerns.",
      "inequality_type": [
        "cultural",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on cultural bias and inclusivity in AI systems",
      "affected_populations": [
        "global learners",
        "cultural groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "System Design",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Assessing biases and proposing inclusive frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.01168v1",
    "title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets",
    "year": 2025,
    "authors": [
      "Mahdi Zakizadeh",
      "Mohammad Taher Pilehvar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender stereotypes in language models, a social bias issue. It discusses bias measurement and mitigation, which relate to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender stereotypes in AI datasets",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzing data distributions and stereotype components",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.00995v1",
    "title": "Is It Still Fair? Investigating Gender Fairness in Cross-Corpus Speech Emotion Recognition",
    "year": 2025,
    "authors": [
      "Shreya G. Upadhyay",
      "Woan-Shiuan Chien",
      "Chi-Chun Lee"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender fairness in speech emotion recognition, addressing gender bias and fairness issues in AI systems, which are social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness in AI systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Adaptation"
      ],
      "methodology_detail": "Proposes fairness adaptation mechanisms for gender fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  }
]