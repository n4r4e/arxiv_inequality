[
  {
    "id": "http://arxiv.org/abs/2501.00129v1",
    "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection",
    "year": 2024,
    "authors": [
      "Julia Ive",
      "Paulina Bondaronek",
      "Vishal Yadav",
      "Daniel Santel",
      "Tracy Glauser",
      "Tina Cheng",
      "Jeffrey R. Strawn",
      "Greeshma Agasthya",
      "Jordan Tschida",
      "Sanghyun Choo",
      "Mayanka Chandrashekar",
      "Anuj J. Kapadia",
      "John Pestian"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in pediatric mental health AI, highlighting disparities in diagnosis accuracy between male and female patients, which relates to social gender inequality.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender disparities in mental health diagnosis",
      "affected_populations": [
        "female adolescents",
        "male adolescents"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "De-biasing framework"
      ],
      "methodology_detail": "Text analysis and bias mitigation in clinical notes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.01987v2",
    "title": "Gender Bias in Text-to-Video Generation Models: A case study of Sora",
    "year": 2024,
    "authors": [
      "Mohammad Nadeem",
      "Shahab Saquib Sohail",
      "Erik Cambria",
      "Bj√∂rn W. Schuller",
      "Amir Hussain"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI, reflecting societal gender stereotypes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "bias in AI models reflecting societal prejudices",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Analyzing generated videos from prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.01447v2",
    "title": "Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines",
    "year": 2024,
    "authors": [
      "Sharika J. Hegde",
      "Max T. M. Ng",
      "Marcos Rios",
      "Hani S. Mahmassani",
      "Ying Chen",
      "Karen Smilowitz"
    ],
    "categories": [
      "econ.GN",
      "cs.LG",
      "econ.EM",
      "q-fin.EC",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in vaccination rates linked to infrastructure, socioeconomic status, and age, highlighting structural inequities affecting different social groups.",
      "inequality_type": [
        "economic",
        "health",
        "age",
        "geographic"
      ],
      "other_detail": "Focus on infrastructure and access disparities",
      "affected_populations": [
        "elderly",
        "low-income countries",
        "rural populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Regression",
        "Machine Learning"
      ],
      "methodology_detail": "Analyzes vaccination capacity determinants using statistical and ML methods",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.20377v1",
    "title": "Impact of Data Distribution on Fairness Guarantees in Equitable Deep Learning",
    "year": 2024,
    "authors": [
      "Yan Luo",
      "Congcong Wen",
      "Min Shi",
      "Hao Huang",
      "Yi Fang",
      "Mengyu Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes fairness in AI models across demographic groups, highlighting disparities related to race and data heterogeneity, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focuses on fairness in medical and facial recognition AI",
      "affected_populations": [
        "racial groups",
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bounds on fairness and empirical validation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.20374v2",
    "title": "FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair Bayesian Perturbation",
    "year": 2024,
    "authors": [
      "Yan Luo",
      "Muhammad Osama Khan",
      "Congcong Wen",
      "Muhammad Muneeb Afzal",
      "Titus Fidelis Wuermeling",
      "Min Shi",
      "Yu Tian",
      "Yi Fang",
      "Mengyu Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness and biases in medical AI models across demographic groups, addressing social discrimination and inequality in healthcare.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "ethnic"
      ],
      "other_detail": "Focus on fairness in medical AI image generation",
      "affected_populations": [
        "patients",
        "medical professionals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Evaluates fairness across demographic subgroups and datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.20298v1",
    "title": "An experimental study on fairness-aware machine learning for credit scoring problem",
    "year": 2024,
    "authors": [
      "Huyen Giang Thi Thu",
      "Thang Viet Doan",
      "Tai Le Quy"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in credit scoring, focusing on biases related to protected attributes like race and gender, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in algorithmic decision-making processes",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Fairness measures evaluation"
      ],
      "methodology_detail": "Comparative analysis of fairness-aware models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.20158v1",
    "title": "Stronger together? The homophily trap in networks",
    "year": 2024,
    "authors": [
      "Marcos Oliveira",
      "Leonie Neuhauser",
      "Fariba Karimi"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.MA",
      "physics.soc-ph",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how homophily affects minority groups' structural opportunities, highlighting inequalities in network visibility and access, which relate to social disparities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on minority group structural opportunities in networks",
      "affected_populations": [
        "minority groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analytical modeling of network homophily effects",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.01973v3",
    "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
    "year": 2024,
    "authors": [
      "Di Jin",
      "Xing Liu",
      "Yu Liu",
      "Jia Qing Yap",
      "Andrea Wong",
      "Adriana Crespo",
      "Qi Lin",
      "Zhiyuan Yin",
      "Qiang Yan",
      "Ryan Ye"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates societal biases and fairness in AI-generated content, addressing social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on societal biases in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "societal minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and fairness evaluation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.19495v2",
    "title": "Disparate Model Performance and Stability in Machine Learning Clinical Support for Diabetes and Heart Diseases",
    "year": 2024,
    "authors": [
      "Ioannis Bilionis",
      "Ricardo C. Berrios",
      "Luis Fernandez-Luque",
      "Carlos Castillo"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in ML model performance across demographic groups, highlighting gender and age inequities in healthcare datasets, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focuses on demographic disparities in clinical AI models",
      "affected_populations": [
        "older patients",
        "younger patients",
        "males",
        "females"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes model performance across demographic groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.19168v2",
    "title": "GFG -- Gender-Fair Generation: A CALAMITA Challenge",
    "year": 2024,
    "authors": [
      "Simona Frenda",
      "Andrea Piergentili",
      "Beatrice Savoldi",
      "Marco Madeddu",
      "Martina Rosola",
      "Silvia Casola",
      "Chiara Ferrando",
      "Viviana Patti",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on promoting gender-fair language, addressing gender inequality and stereotypes in language use, which relates directly to social gender discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender-fair language in AI systems",
      "affected_populations": [
        "women",
        "non-binary individuals",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Developing datasets and evaluating NLP tasks for gender fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.18810v1",
    "title": "DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions",
    "year": 2024,
    "authors": [
      "Yilei Jiang",
      "Weihong Li",
      "Yiyuan Zhang",
      "Minghong Cai",
      "Xiangyu Yue"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on debiasing biases related to gender and race in AI-generated images, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in AI-generated visual content",
      "affected_populations": [
        "minority groups",
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Self-discovering latent attribute directions in diffusion models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.18048v1",
    "title": "Fair Knowledge Tracing in Second Language Acquisition",
    "year": 2024,
    "authors": [
      "Weitao Tang",
      "Guanliang Chen",
      "Shuaishuai Zu",
      "Jiangyi Luo"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in predictive models across social groups, addressing biases related to geography and platform, which relate to social inequalities.",
      "inequality_type": [
        "geographic",
        "digital",
        "educational"
      ],
      "other_detail": "Fairness across regions and technology platforms",
      "affected_populations": [
        "learners in developing countries",
        "mobile users",
        "regional groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates fairness and accuracy of models across groups",
      "geographic_focus": [
        "developed countries",
        "developing countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.17803v2",
    "title": "Examining Imbalance Effects on Performance and Demographic Fairness of Clinical Language Models",
    "year": 2024,
    "authors": [
      "Precious Jones",
      "Weisi Liu",
      "I-Chan Huang",
      "Xiaolei Huang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and performance disparities across demographic groups, addressing social bias issues in AI systems applied to healthcare.",
      "inequality_type": [
        "gender",
        "ethnic",
        "health",
        "demographic"
      ],
      "other_detail": "Focus on fairness in biomedical language models",
      "affected_populations": [
        "gender groups",
        "ethnic groups",
        "patients"
      ],
      "methodology": [
        "Statistical Analysis",
        "Natural Language Processing",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analyzes data imbalance and fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.17486v1",
    "title": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of Large Language Models such as ChatGPT in Educational Settings",
    "year": 2024,
    "authors": [
      "J√©r√©mie Sublime",
      "Ilaria Renna"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gender disparities, AI literacy gaps, and educational inequalities related to AI use among students.",
      "inequality_type": [
        "gender",
        "educational",
        "digital"
      ],
      "other_detail": "Focus on gender and educational disparities in AI usage",
      "affected_populations": [
        "female students",
        "younger students",
        "students in France and Italy"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Survey of student AI usage patterns",
      "geographic_focus": [
        "France",
        "Italy"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.17123v2",
    "title": "Fairness in Reinforcement Learning with Bisimulation Metrics",
    "year": 2024,
    "authors": [
      "Sahand Rezaei-Shoshtari",
      "Hanna Yurchyk",
      "Scott Fujimoto",
      "Doina Precup",
      "David Meger"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on group disparities in decision-making, which relates to social inequality issues such as discrimination and bias.",
      "inequality_type": [
        "racial",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Fairness in sequential decision-making environments",
      "affected_populations": [
        "group1",
        "group2"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Using bisimulation metrics to promote fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.16699v1",
    "title": "FAP-CD: Fairness-Driven Age-Friendly Community Planning via Conditional Diffusion Generation",
    "year": 2024,
    "authors": [
      "Jinlin Li",
      "Xintong Li",
      "Xiao Zhou"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on equitable urban planning for elderly populations, addressing regional disparities and service distribution, which relate to social inequality issues.",
      "inequality_type": [
        "age",
        "geographic",
        "urban-rural"
      ],
      "other_detail": "Focuses on age-related and spatial equity in urban environments",
      "affected_populations": [
        "elderly",
        "urban residents"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Diffusion models and graph-based optimization techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.16523v1",
    "title": "Physics-Guided Fair Graph Sampling for Water Temperature Prediction in River Networks",
    "year": 2024,
    "authors": [
      "Erhu He",
      "Declan Kutscher",
      "Yiqun Xie",
      "Jacob Zwart",
      "Zhe Jiang",
      "Huaxiu Yao",
      "Xiaowei Jia"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "physics.soc-ph",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses spatial bias and fairness in GNNs, focusing on equitable treatment across sensitive groups, which relates to social inequality issues.",
      "inequality_type": [
        "income",
        "educational",
        "geographic"
      ],
      "other_detail": "aims to reduce bias across underprivileged groups",
      "affected_populations": [
        "underprivileged groups",
        "rural communities"
      ],
      "methodology": [
        "Machine Learning",
        "Graph Neural Networks",
        "Physics-based modeling"
      ],
      "methodology_detail": "integrates physical knowledge to improve fairness",
      "geographic_focus": [
        "Delaware River Basin"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.16428v2",
    "title": "Data-Driven Fairness Generalization for Deepfake Detection",
    "year": 2024,
    "authors": [
      "Uzoamaka Ezeakunne",
      "Chrisantus Eze",
      "Xiuwen Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems affecting demographic groups, highlighting disparities across race and gender. It focuses on fairness generalization, which relates to social discrimination issues. The emphasis on demographic fairness indicates a focus on social inequality in AI applications.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation in AI fairness",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Synthetic data generation and optimization strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.16406v1",
    "title": "Learning Disease Progression Models That Capture Health Disparities",
    "year": 2024,
    "authors": [
      "Erica Chiang",
      "Divya Shanmugam",
      "Ashley N. Beecy",
      "Gabriel Sayer",
      "Nir Uriel",
      "Deborah Estrin",
      "Nikhil Garg",
      "Emma Pierson"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly models health disparities affecting different patient groups, addressing social inequalities in healthcare access and outcomes.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focuses on disparities in disease care and progression",
      "affected_populations": [
        "disadvantaged patients",
        "healthcare minorities"
      ],
      "methodology": [
        "Bayesian Modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Interpretable Bayesian disease progression model",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.16373v1",
    "title": "FairREAD: Re-fusing Demographic Attributes after Disentanglement for Fair Medical Image Classification",
    "year": 2024,
    "authors": [
      "Yicheng Gao",
      "Jinkui Hao",
      "Bo Zhou"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in medical imaging, focusing on demographic disparities.",
      "inequality_type": [
        "health",
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in medical AI systems",
      "affected_populations": [
        "demographic groups",
        "patients"
      ],
      "methodology": [
        "Deep Learning",
        "Adversarial Training",
        "Orthogonality Constraints"
      ],
      "methodology_detail": "Disentanglement and re-fusion of demographic info",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.16335v1",
    "title": "Improving Equity in Health Modeling with GPT4-Turbo Generated Synthetic Data: A Comparative Study",
    "year": 2024,
    "authors": [
      "Daniel Smolyak",
      "Arshana Welivita",
      "Margr√©t V. Bjarnad√≥ttir",
      "Ritu Agarwal"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic disparities in medical data and fairness in AI models, aiming to improve health equity.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic representation in medical datasets",
      "affected_populations": [
        "minority groups",
        "underserved populations"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Synthetic data generation for demographic groups",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.15504v2",
    "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework",
    "year": 2024,
    "authors": [
      "Zhenjie Xu",
      "Wenqing Chen",
      "Yi Tang",
      "Xuanying Li",
      "Cheng Hu",
      "Zhixuan Chu",
      "Kui Ren",
      "Zibin Zheng",
      "Zhichao Lu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating social bias in language models, addressing fairness issues related to social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Addresses social bias in AI outputs",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Multi-Objective Optimization",
        "Multi-Agent Framework",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation without performance loss",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.15162v1",
    "title": "Equal Merit Does Not Imply Equality: Discrimination at Equilibrium in a Hiring Market with Symmetric Agents",
    "year": 2024,
    "authors": [
      "Serafina Kamp",
      "Benjamin Fish"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination and inequality in hiring markets, highlighting social biases and relational inequality mechanisms.",
      "inequality_type": [
        "economic",
        "educational",
        "social"
      ],
      "other_detail": "Focuses on non-distributional sources of inequality",
      "affected_populations": [
        "job candidates",
        "workers"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Game Theory",
        "Model Development",
        "Convergence Analysis"
      ],
      "methodology_detail": "Analyzes equilibria and learning dynamics in hiring models",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.14329v1",
    "title": "Embedding Cultural Diversity in Prototype-based Recommender Systems",
    "year": 2024,
    "authors": [
      "Armin Moradi",
      "Nicola Neophytou",
      "Florian Carichon",
      "Golnoosh Farnadi"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses cultural bias and underrepresentation in recommender systems, impacting marginalized cultural groups, which relates to social inequality issues.",
      "inequality_type": [
        "cultural",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on cultural diversity and representation",
      "affected_populations": [
        "underrepresented cultural groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Embedding space refinement and bias reduction techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.14307v1",
    "title": "Race Discrimination in Internet Advertising: Evidence From a Field Experiment",
    "year": 2024,
    "authors": [
      "Neil K. R. Sehgal",
      "Dan Svirsky"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in internet advertising, highlighting discrimination against darker skin tones and its economic impacts, thus addressing racial inequality and social bias in AI systems.",
      "inequality_type": [
        "racial",
        "economic"
      ],
      "other_detail": "focus on racial bias in digital advertising",
      "affected_populations": [
        "darker skin individuals",
        "advertisers"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Field experiment measuring ad engagement and spending",
      "geographic_focus": [
        "Brazil",
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.14304v1",
    "title": "Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs",
    "year": 2024,
    "authors": [
      "David Restrepo",
      "Chenwei Wu",
      "Zhengxu Tang",
      "Zitao Shuai",
      "Thao Nguyen Minh Phan",
      "Jun-En Ding",
      "Cong-Tinh Dao",
      "Jack Gallifant",
      "Robyn Gayle Dychiao",
      "Jose Carlo Artiaga",
      "Andr√© Hiroshi Bando",
      "Carolina Pelegrini Barbosa Gracitelli",
      "Vincenz Ferrer",
      "Leo Anthony Celi",
      "Danielle Bitterman",
      "Michael G Morley",
      "Luis Filipe Nakayama"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in healthcare access and AI performance across languages in LMICs, highlighting potential exacerbation of health inequalities.",
      "inequality_type": [
        "health",
        "linguistic",
        "geographic"
      ],
      "other_detail": "Focus on healthcare disparities in LMICs",
      "affected_populations": [
        "patients in LMICs",
        "non-English speakers"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Multilingual benchmark and bias mitigation techniques",
      "geographic_focus": [
        "Low and Middle-Income Countries"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.12610v2",
    "title": "Gender Bias and Property Taxes",
    "year": 2024,
    "authors": [
      "Gordon Burtch",
      "Alejandro Zentner"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in property tax hearings, highlighting gender-based disparities and perceptions, which are social inequalities.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on gender bias in administrative hearings",
      "affected_populations": [
        "women",
        "female appellants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes hearing records and audio with AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.11878v1",
    "title": "Using Instruction-Tuned Large Language Models to Identify Indicators of Vulnerability in Police Incident Narratives",
    "year": 2024,
    "authors": [
      "Sam Relins",
      "Daniel Birks",
      "Charlie Lloyd"
    ],
    "categories": [
      "cs.CL",
      "I.2.7; H.3.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI classification of police narratives, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias assessment in AI classification of social groups",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Counterfactual Analysis"
      ],
      "methodology_detail": "Bias and fairness evaluation in AI models",
      "geographic_focus": [
        "Boston, USA"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.10669v2",
    "title": "FairGP: A Scalable and Fair Graph Transformer Using Graph Partitioning",
    "year": 2024,
    "authors": [
      "Renqiang Luo",
      "Huafei Huang",
      "Ivan Lee",
      "Chengpei Xu",
      "Jianzhong Qi",
      "Feng Xia"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI models, focusing on bias related to sensitive features, which directly relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in graph-based AI models",
      "affected_populations": [
        "social groups",
        "subgroups by sensitive features"
      ],
      "methodology": [
        "Experiment",
        "Theoretical Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Graph partitioning and attention mechanism optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.10575v2",
    "title": "Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration",
    "year": 2024,
    "authors": [
      "Karina Halevy",
      "Karly Hou",
      "Charumathi Badrinath"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI models, focusing on minority groups and multicalibration, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on minority groups and fairness metrics",
      "affected_populations": [
        "minority groups",
        "marginalized groups"
      ],
      "methodology": [
        "Experiment",
        "Multicalibration",
        "Data Augmentation"
      ],
      "methodology_detail": "Testing fairness across multiple minority groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.10513v1",
    "title": "Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender Bias Study Case on BERT-based Language Models",
    "year": 2024,
    "authors": [
      "Ana Ozaki",
      "Roberto Confalonieri",
      "Ricardo Guimar√£es",
      "Anders Imenes"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, highlighting social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias detection in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "PAC framework for model fidelity guarantees",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2501.10371v1",
    "title": "What we learned while automating bias detection in AI hiring systems for compliance with NYC Local Law 144",
    "year": 2024,
    "authors": [
      "Gemma Galdon Clavell",
      "Rub√©n Gonz√°lez-Sendino"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias detection and fairness in AI hiring systems, which directly relate to social discrimination and inequality issues such as race, gender, and socioeconomic disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias and fairness in employment AI systems",
      "affected_populations": [
        "job applicants",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias audits and compliance tools analysis",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.09900v2",
    "title": "Analyzing Fairness of Computer Vision and Natural Language Processing Models",
    "year": 2024,
    "authors": [
      "Ahmed Rashed",
      "Abdelkrim Kallich",
      "Mohamed Eltayeb"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in AI models, addressing social bias and systemic inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "health"
      ],
      "other_detail": "Focus on bias mitigation in ML models",
      "affected_populations": [
        "social groups",
        "minorities",
        "disadvantaged"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Using fairness libraries to evaluate and mitigate bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.09896v2",
    "title": "Analyzing Fairness of Classification Machine Learning Model with Structured Dataset",
    "year": 2024,
    "authors": [
      "Ahmed Rashed",
      "Abdelkrim Kallich",
      "Mohamed Eltayeb"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness and bias in ML models, highlighting potential for systemic inequalities, which relates to social discrimination issues.",
      "inequality_type": [
        "racial",
        "educational",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Libraries",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates bias mitigation tools in classification tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.09668v2",
    "title": "Vision-Language Models Generate More Homogeneous Stories for Phenotypically Black Individuals",
    "year": 2024,
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in AI-generated representations of Black individuals, highlighting stereotyping and phenotypicality effects, which relate directly to racial inequality and bias in social perception.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on racial phenotypicality and intersectionality",
      "affected_populations": [
        "Black Americans",
        "Black women",
        "Black men"
      ],
      "methodology": [
        "Experiment",
        "Computer Vision",
        "Natural Language Processing"
      ],
      "methodology_detail": "Using generated images and story analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.09160v1",
    "title": "Pinpoint Counterfactuals: Reducing social bias in foundation models via localized counterfactual generation",
    "year": 2024,
    "authors": [
      "Kirill Sirotkin",
      "Marcos Escudero-Vi√±olo",
      "Pablo Carballeira",
      "Mayug Maniparambil",
      "Catarina Barata",
      "Noel E. O'Connor"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing societal biases in AI models, specifically gender bias, which relates to social inequality issues such as gender discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Computer Vision",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Localized counterfactual generation for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.12165v1",
    "title": "Multimodal Approaches to Fair Image Classification: An Ethical Perspective",
    "year": 2024,
    "authors": [
      "Javon Hickmon"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI systems affecting marginalized social groups, focusing on fairness and ethical implications.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI systems for social equity",
      "affected_populations": [
        "underrepresented groups",
        "minorities",
        "disadvantaged populations"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Ethics Analysis"
      ],
      "methodology_detail": "Multimodal bias mitigation techniques and ethical evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.08167v1",
    "title": "Diversity Drives Fairness: Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software",
    "year": 2024,
    "authors": [
      "Zhenpeng Chen",
      "Xinyue Li",
      "Jie M. Zhang",
      "Federica Sarro",
      "Yang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on intersectional fairness in ML, addressing social discrimination across multiple protected attributes, which relates directly to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on fairness across social identity groups",
      "affected_populations": [
        "social minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Ensemble mutation approach for fairness enhancement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.07303v2",
    "title": "Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia",
    "year": 2024,
    "authors": [
      "Lance Calvin Lim Gamboa",
      "Mark Lee"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and queer identities in language models, which are forms of social discrimination. It focuses on societal stereotypes and prejudices embedded in AI systems, impacting marginalized groups.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Bias in multilingual AI models for Filipino language",
      "affected_populations": [
        "women",
        "queer communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Cultural adaptation of bias evaluation datasets",
      "geographic_focus": [
        "Philippines"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.06134v2",
    "title": "Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings",
    "year": 2024,
    "authors": [
      "Zhao Liu",
      "Tian Xie",
      "Xueru Zhang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases related to protected attributes like age and socio-economic status, addressing social discrimination issues in AI systems.",
      "inequality_type": [
        "socioeconomic",
        "age"
      ],
      "other_detail": "Focus on bias mitigation in language models",
      "affected_populations": [
        "older adults",
        "socioeconomic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Extending datasets and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.05832v1",
    "title": "Fairness in Computational Innovations: Identifying Bias in Substance Use Treatment Length of Stay Prediction Models with Policy Implications",
    "year": 2024,
    "authors": [
      "Ugur Kursuncu",
      "Aaron Baird",
      "Yusen Xia"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race, region, diagnosis, and payment source, highlighting social disparities in health outcomes and fairness in AI models. It discusses societal biases and their impact on vulnerable groups, emphasizing social justice and health equity.",
      "inequality_type": [
        "racial",
        "geographic",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on health disparities and social justice implications",
      "affected_populations": [
        "disadvantaged patients",
        "racial minorities",
        "vulnerable groups"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Assessment",
        "Policy Analysis"
      ],
      "methodology_detail": "Evaluates bias and fairness in predictive models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.04100v2",
    "title": "Missing Melodies: AI Music Generation and its \"Nearly\" Complete Omission of the Global South",
    "year": 2024,
    "authors": [
      "Atharva Mehta",
      "Shivam Chauhan",
      "Monojit Choudhury"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper highlights underrepresentation of Global South music in AI datasets and research, reflecting social and cultural inequality. It discusses disparities in cultural inclusion and diversity, which relate to broader social inequality issues.",
      "inequality_type": [
        "cultural",
        "geographic"
      ],
      "other_detail": "Underrepresentation of Global South in AI music datasets",
      "affected_populations": [
        "musicians from Global South",
        "cultural communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analysis of datasets and research papers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.04025v1",
    "title": "Exploring the Influence of Label Aggregation on Minority Voices: Implications for Dataset Bias and Model Training",
    "year": 2024,
    "authors": [
      "Mugdha Pandya",
      "Nafise Sadat Moosavi",
      "Diana Maynard"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines representation of minority opinions in datasets, highlighting potential biases affecting marginalized groups, specifically in gender-related content. It discusses how aggregation methods may silence minority voices, impacting fairness in AI models. This relates to social bias and inequality in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Minority voice representation in datasets",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Data Analysis"
      ],
      "methodology_detail": "Analyzing label aggregation effects on dataset bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.03963v1",
    "title": "Augmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI's Impact on Creative Tasks",
    "year": 2024,
    "authors": [
      "Meiling Huang",
      "Ming Jin",
      "Ning Li"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how AI impacts cognitive inequalities and skill valuation, affecting disparities in creative work and workplace hierarchies.",
      "inequality_type": [
        "educational",
        "socioeconomic",
        "digital"
      ],
      "other_detail": "Focuses on skill valuation and cognitive disparities",
      "affected_populations": [
        "creative workers",
        "workforce"
      ],
      "methodology": [
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Controlled experiments and framework development",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.03706v2",
    "title": "Fairness without Demographics through Learning Graph of Gradients",
    "year": 2024,
    "authors": [
      "Yingtao Luo",
      "Zhixun Li",
      "Qiang Liu",
      "Jun Zhu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI related to demographic groups, implying social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness without demographic data",
      "affected_populations": [
        "minority groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Graph-based Learning",
        "Adversarial Architecture"
      ],
      "methodology_detail": "Using gradient graphs for fairness without demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.03537v1",
    "title": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models",
    "year": 2024,
    "authors": [
      "Natalie Mackraz",
      "Nivedha Sivakumar",
      "Samira Khorshidi",
      "Krishna Patel",
      "Barry-John Theobald",
      "Luca Zappella",
      "Nicholas Apostoloff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias transfer in language models, addressing social bias and fairness issues related to gender, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias correlation analysis across prompting methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.03349v1",
    "title": "Fairer Analysis and Demographically Balanced Face Generation for Fairer Face Verification",
    "year": 2024,
    "authors": [
      "Alexandre Fournier-Montgieux",
      "Michael Soumm",
      "Adrian Popescu",
      "Bertrand Luvison",
      "Herv√© Le Borgne"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in face recognition, addressing biases related to demographic groups, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "demographic"
      ],
      "other_detail": "Bias mitigation in AI systems for social fairness",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Fairness metrics and statistical analysis methods used",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.02528v1",
    "title": "Bias Analysis of AI Models for Undergraduate Student Admissions",
    "year": 2024,
    "authors": [
      "Kelly Van Busum",
      "Shiaofen Fang"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in AI models affecting admissions based on gender, race, and first-generation status, which are social identity factors related to inequality. It discusses fairness metrics and bias persistence, addressing social discrimination issues. The focus on demographic impacts indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Bias in AI affecting marginalized student groups",
      "affected_populations": [
        "racial minorities",
        "women",
        "first-generation students"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Fairness Metrics"
      ],
      "methodology_detail": "Analyzing bias and fairness in predictive models",
      "geographic_focus": [
        "urban research university in the US"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.02057v2",
    "title": "Comparative Analysis of Multi-Agent Reinforcement Learning Policies for Crop Planning Decision Support",
    "year": 2024,
    "authors": [
      "Anubha Mahajan",
      "Shreya Hegde",
      "Ethan Shay",
      "Daniel Wu",
      "Aviva Prins"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on crop planning for small farmers, addressing income inequality and fairness among farmers, which relates to socioeconomic disparities.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on income distribution among farmers",
      "affected_populations": [
        "small farmers",
        "marginal farmers"
      ],
      "methodology": [
        "Machine Learning",
        "Reinforcement Learning",
        "Simulation"
      ],
      "methodology_detail": "Evaluates MARL approaches for crop planning optimization",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.01711v1",
    "title": "Towards Resource Efficient and Interpretable Bias Mitigation in Large Language Models",
    "year": 2024,
    "authors": [
      "Schrasing Tong",
      "Eliott Zemour",
      "Rawisara Lohanimit",
      "Lalana Kagal"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender, race, and religion in AI, which are social inequalities. It focuses on mitigating social biases in language models, impacting marginalized groups.",
      "inequality_type": [
        "gender",
        "racial",
        "religion"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias reduction techniques evaluated via metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.01549v1",
    "title": "Silenced Voices: Exploring Social Media Polarization and Women's Participation in Peacebuilding in Ethiopia",
    "year": 2024,
    "authors": [
      "Adem Chanie Ali",
      "Seid Muhie Yimam",
      "Martin Semmann",
      "Abinew Ali Ayele",
      "Chris Biemann"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-based disparities, women's marginalization, and social media's impact on women in conflict, addressing gender inequality and social discrimination.",
      "inequality_type": [
        "gender",
        "digital",
        "social"
      ],
      "other_detail": "Focus on women's participation and safety in conflict",
      "affected_populations": [
        "women",
        "conflict-affected groups"
      ],
      "methodology": [
        "Case Study",
        "Qualitative Study"
      ],
      "methodology_detail": "Analysis of social media and conflict impacts in Ethiopia",
      "geographic_focus": [
        "Ethiopia"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.01065v1",
    "title": "Lookahead Counterfactual Fairness",
    "year": 2024,
    "authors": [
      "Zhiqun Zuo",
      "Tian Xie",
      "Xuwei Tan",
      "Xueru Zhang",
      "Mohammad Mahdi Khalili"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI predictions affecting individuals, considering downstream effects on social groups, which relates to social inequality issues.",
      "inequality_type": [
        "social",
        "discrimination"
      ],
      "other_detail": "focus on fairness and downstream impacts",
      "affected_populations": [
        "individuals in social groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Algorithm Development",
        "Experiment"
      ],
      "methodology_detail": "includes theoretical conditions and validation experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.00606v1",
    "title": "Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions",
    "year": 2024,
    "authors": [
      "Resmi Ramachandranpillai",
      "Kishore Sampath",
      "Ayaazuddin Mohammad",
      "Malihe Alikhani"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in clinical AI affecting diverse social groups, highlighting disparities related to race, gender, and intersectional identities in healthcare outcomes.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "intersectional"
      ],
      "other_detail": "Focuses on intersectional biases in healthcare AI",
      "affected_populations": [
        "minority patients",
        "intersectional groups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias mitigation in multimodal clinical predictions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.00554v2",
    "title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective",
    "year": 2024,
    "authors": [
      "Yue Zhou",
      "Barbara Di Eugenio",
      "Lu Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and demographic disparities in healthcare AI, addressing social bias and inequality issues across demographic groups.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in healthcare AI systems",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing LLM performance across groups and tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.00245v1",
    "title": "Integrating Social Determinants of Health into Knowledge Graphs: Evaluating Prediction Bias and Fairness in Healthcare",
    "year": 2024,
    "authors": [
      "Tianqi Shang",
      "Weiqing He",
      "Tianlong Chen",
      "Ying Ding",
      "Huanmei Wu",
      "Kaixiong Zhou",
      "Li Shen"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias related to social determinants of health, which are linked to social inequalities such as socioeconomic and health disparities.",
      "inequality_type": [
        "socioeconomic",
        "health"
      ],
      "other_detail": "Bias mitigation in healthcare AI systems",
      "affected_populations": [
        "patients with SDoH factors"
      ],
      "methodology": [
        "Machine Learning",
        "Graph Embedding",
        "Fairness Formulation",
        "Bias Mitigation"
      ],
      "methodology_detail": "Graph-based fairness and reweighting techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.19678v1",
    "title": "Privacy-Preserving Orthogonal Aggregation for Guaranteeing Gender Fairness in Federated Recommendation",
    "year": 2024,
    "authors": [
      "Siqing Zhang",
      "Yuchen Ding",
      "Wei Tang",
      "Wei Sun",
      "Yong Liao",
      "Peng Yuan Zhou"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender fairness in federated recommendation systems, highlighting disparities and privacy issues affecting gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness in AI systems",
      "affected_populations": [
        "females",
        "males"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Development"
      ],
      "methodology_detail": "Privacy-preserving aggregation techniques and fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.19623v1",
    "title": "FairDD: Fair Dataset Distillation via Synchronized Matching",
    "year": 2024,
    "authors": [
      "Qihang Zhou",
      "Shenhao Fang",
      "Shibo He",
      "Wenchao Meng",
      "Jiming Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI datasets related to protected attributes like gender and race, aiming to reduce discrimination against minority groups.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on dataset fairness in image recognition",
      "affected_populations": [
        "minority groups",
        "protected attribute groups"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fair dataset distillation with synchronized matching",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.19240v1",
    "title": "How far can bias go? -- Tracing bias from pretraining data to alignment",
    "year": 2024,
    "authors": [
      "Marion Thaler",
      "Abdullatif K√∂ksal",
      "Alina Leidinger",
      "Anna Korhonen",
      "Hinrich Sch√ºtze"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, which relates to social gender inequality and discrimination. It analyzes how biases in training data influence model outputs, highlighting societal implications.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias amplification in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and influence analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.19140v1",
    "title": "Examining Multimodal Gender and Content Bias in ChatGPT-4o",
    "year": 2024,
    "authors": [
      "Roberto Balestri"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "stat.OT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and content moderation in AI, highlighting disparities that reflect social inequalities. It discusses gender bias and societal influences on AI behavior, which are core social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in AI content moderation practices",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Content Analysis"
      ],
      "methodology_detail": "Analyzes AI responses to different content prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.18994v1",
    "title": "Descriptions of women are longer than that of men: An analysis of gender portrayal prompts in Stable Diffusion",
    "year": 2024,
    "authors": [
      "Yan Asadchy",
      "Maximilian Schich"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender stereotypes and biases in AI-generated images, highlighting societal representations of women and men, which relate directly to gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender stereotypes in AI-generated imagery",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes prompt length and content statistically",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.18177v1",
    "title": "Machine Unlearning reveals that the Gender-based Violence Victim Condition can be detected from Speech in a Speaker-Agnostic Setting",
    "year": 2024,
    "authors": [
      "Emma Reyner-Fuentes",
      "Esther Rituerto-Gonzalez",
      "Carmen Pelaez-Moreno"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender-based violence, a social inequality issue affecting women's mental health, and discusses AI's role in detecting and addressing this inequality.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender-based violence victim detection",
      "affected_populations": [
        "women",
        "GBV victims"
      ],
      "methodology": [
        "Deep Learning",
        "Domain-Adversarial Training",
        "Experiment"
      ],
      "methodology_detail": "Robust speaker-agnostic GBV detection models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.17554v1",
    "title": "Navigating Spatial Inequities in Freight Truck Crash Severity via Counterfactual Inference in Los Angeles",
    "year": 2024,
    "authors": [
      "Yichen Wang",
      "Hao Yin",
      "Yifan Yang",
      "Chenyang Zhao",
      "Siqin Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines spatial disparities in crash severity linked to socioeconomic and demographic factors, highlighting issues of spatial justice and inequity.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "geographic"
      ],
      "other_detail": "Focus on spatial justice and community disparities",
      "affected_populations": [
        "low-income communities",
        "minority populations"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Counterfactual inference models and spatial data analysis",
      "geographic_focus": [
        "Los Angeles"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.17374v1",
    "title": "Fairness And Performance In Harmony: Data Debiasing Is All You Need",
    "year": 2024,
    "authors": [
      "Junhua Liu",
      "Wendy Wan Yee Hui",
      "Roy Ka-Wei Lee",
      "Kwan Hui Lim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making processes, focusing on gender bias in university admissions, which relates to social gender inequality.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender bias in admissions",
      "affected_populations": [
        "female applicants",
        "male applicants"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Using ML models and debiasing pipelines on real data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.15971v1",
    "title": "Advancing Transformative Education: Generative AI as a Catalyst for Equity and Innovation",
    "year": 2024,
    "authors": [
      "Chiranjeevi Bura",
      "Praveen Kumar Myakala"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses equity gaps, ethical considerations, and access issues in education, which relate to social inequalities such as educational and digital divides.",
      "inequality_type": [
        "educational",
        "digital",
        "inequity"
      ],
      "other_detail": "Focus on equity gaps in educational access and fairness",
      "affected_populations": [
        "students",
        "educational marginalized groups"
      ],
      "methodology": [
        "Framework Development",
        "Ethics Analysis"
      ],
      "methodology_detail": "Proposes actionable frameworks for equity",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.00051v2",
    "title": "TransFair: Transferring Fairness from Ocular Disease Classification to Progression Prediction",
    "year": 2024,
    "authors": [
      "Leila Gheisi",
      "Henry Chu",
      "Raju Gottumukkala",
      "Yan Luo",
      "Xingquan Zhu",
      "Mengyu Wang",
      "Min Shi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on demographic equity in healthcare, which relates to social inequalities such as health disparities and demographic biases.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in disease progression prediction",
      "affected_populations": [
        "underprivileged patients",
        "diverse demographics"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Knowledge Distillation"
      ],
      "methodology_detail": "Fairness-aware model transfer and adaptation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.13738v1",
    "title": "Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics",
    "year": 2024,
    "authors": [
      "Tetiana Bas"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in LLMs, comparing outputs to human perceptions and official gender statistics, directly addressing gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Developed new occupational dataset for bias testing",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.13184v1",
    "title": "Quantitative Fairness -- A Framework For The Design Of Equitable Cybernetic Societies",
    "year": 2024,
    "authors": [
      "Kevin Riehl",
      "Michail Makridis",
      "Anastasios Kouvelas"
    ],
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in algorithmic decision-making, social cohesion, and social mobility, indicating a focus on social inequalities and discrimination issues.",
      "inequality_type": [
        "socioeconomic",
        "class",
        "educational",
        "social"
      ],
      "other_detail": "Focus on societal fairness and equitable decision systems",
      "affected_populations": [
        "social groups",
        "communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Develops a fairness framework for algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.13022v2",
    "title": "Fast MRI for All: Bridging Equity Gaps via Training without Raw Data Access",
    "year": 2024,
    "authors": [
      "Ya≈üar Utku Al√ßalar",
      "Merve G√ºlle",
      "Mehmet Ak√ßakaya"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in MRI access and quality between underserved populations and specialized centers, aiming to improve equitable healthcare through AI methods.",
      "inequality_type": [
        "health",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Access to advanced MRI technology in underserved areas",
      "affected_populations": [
        "rural populations",
        "underserved communities"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Training without raw data using clinical images",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.12846v1",
    "title": "Towards Fairness in AI for Melanoma Detection: Systemic Review and Recommendations",
    "year": 2024,
    "authors": [
      "Laura N Montoya",
      "Jennafer Shae Roberts",
      "Belen Sanchez Hidalgo"
    ],
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.HC",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in AI melanoma detection and proposes methods to improve fairness across skin tones.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on skin tone representation and bias mitigation",
      "affected_populations": [
        "patients with darker skin",
        "diverse skin tones"
      ],
      "methodology": [
        "Systematic Review",
        "Deep Learning",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analyzes datasets and AI fairness methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.12785v1",
    "title": "Joint Vision-Language Social Bias Removal for CLIP",
    "year": 2024,
    "authors": [
      "Haoyu Zhang",
      "Yangyang Guo",
      "Mohan Kankanhalli"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias and fairness issues in vision-language AI models, which relate to social discrimination and inequality. It focuses on mitigating biases that affect specific social groups, indicating a concern with social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on social bias mitigation in AI models",
      "affected_populations": [
        "specific social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias removal and alignment techniques in multimodal models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.12074v1",
    "title": "Mitigating Gender Bias in Contextual Word Embeddings",
    "year": 2024,
    "authors": [
      "Navya Yarrabelly",
      "Vinay Damodaran",
      "Feng-Guang Su"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in word embeddings, addressing social gender stereotypes and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Addressing gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation in embeddings and evaluation metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.11939v1",
    "title": "Fair Distillation: Teaching Fairness from Biased Teachers in Medical Imaging",
    "year": 2024,
    "authors": [
      "Milad Masroor",
      "Tahir Hassan",
      "Yu Tian",
      "Kevin Wells",
      "David Rosewarne",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on biases affecting demographic groups such as race, gender, and health, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in medical AI systems",
      "affected_populations": [
        "demographic groups",
        "patients"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware model training in medical imaging",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.03576v1",
    "title": "Ethical Challenges and Evolving Strategies in the Integration of Artificial Intelligence into Clinical Practice",
    "year": 2024,
    "authors": [
      "Ellison B. Weiner",
      "Irene Dankwa-Mullan",
      "William A. Nelson",
      "Saeed Hassanpour"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses ethical concerns like fairness, bias, and equitable care in AI healthcare applications, directly relating to social inequalities such as race, socioeconomic status, and patient populations.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focus on fairness and bias in healthcare AI systems",
      "affected_populations": [
        "patients",
        "vulnerable groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Review of frameworks and ethical considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.10746v1",
    "title": "LTCXNet: Advancing Chest X-Ray Analysis with Solutions for Long-Tailed Multi-Label Classification and Fairness Challenges",
    "year": 2024,
    "authors": [
      "Chin-Wei Huang",
      "Mu-Yi Shen",
      "Kuan-Chang Shih",
      "Shih-Chih Lin",
      "Chi-Yu Chen",
      "Po-Chih Kuo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness and demographic impacts in medical AI, addressing social bias and inequality issues.",
      "inequality_type": [
        "health",
        "fairness"
      ],
      "other_detail": "Focus on fairness across demographic groups",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Assessing fairness impacts in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.10636v1",
    "title": "Gender Bias Mitigation for Bangla Classification Tasks",
    "year": 2024,
    "authors": [
      "Sajib Kumar Saha Joy",
      "Arman Hassan Mahy",
      "Meherin Sultana",
      "Azizah Mamun Abha",
      "MD Piyal Ahmmed",
      "Yue Dong",
      "G M Shahariar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social discrimination. It focuses on mitigating gender bias in AI systems for Bangla, impacting gender equality. The study directly engages with social bias issues in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and mitigation in language models",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.12590v2",
    "title": "Debias your Large Multi-Modal Model at Test-Time via Non-Contrastive Visual Attribute Steering",
    "year": 2024,
    "authors": [
      "Neale Ratzlaff",
      "Matthew Lyle Olson",
      "Musashi Hinck",
      "Estelle Aflalo",
      "Shao-Yen Tseng",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses societal biases in AI models related to protected attributes such as demographics, aiming to reduce discriminatory responses. It focuses on bias mitigation in large multi-modal models, which impacts social fairness. The work directly engages with issues of social bias and fairness in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI responses",
      "affected_populations": [
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias reduction techniques during text generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.10544v2",
    "title": "Debias-CLR: A Contrastive Learning Based Debiasing Method for Algorithmic Fairness in Healthcare Applications",
    "year": 2024,
    "authors": [
      "Ankita Agarwal",
      "Tanvi Banerjee",
      "William Romine",
      "Mia Cajita"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic biases in healthcare AI models related to gender and ethnicity, aiming to reduce health disparities. It discusses fairness and bias mitigation, which are central to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Focuses on healthcare disparities and demographic bias",
      "affected_populations": [
        "patients by gender",
        "patients by ethnicity"
      ],
      "methodology": [
        "Deep Learning",
        "Contrastive Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias mitigation in clinical embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.10351v4",
    "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
    "year": 2024,
    "authors": [
      "Lin Ling",
      "Fazle Rabbi",
      "Song Wang",
      "Jinqiu Yang"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in AI-generated code, addressing fairness and bias issues related to social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on social bias in AI code generation",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Assessing bias and mitigation strategies in code generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.09826v1",
    "title": "Evaluating Gender Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Michael D√∂ll",
      "Markus D√∂hring",
      "Andreas M√ºller"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, highlighting social discrimination related to gender roles and representation. It analyzes how AI systems reflect and potentially reinforce gender stereotypes, impacting social perceptions and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI language models",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Assessing pronoun and name gender distributions in generated text",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.09431v1",
    "title": "Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data",
    "year": 2024,
    "authors": [
      "Rik Raes",
      "Saskia Lensink",
      "Mykola Pechenizkiy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-based performance disparities in ASR systems, highlighting biases affecting gender groups. It discusses fairness and quality of service harms, indicating a focus on social bias. The analysis of biases in technology impacts social groups differently, addressing social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in AI affecting gender groups",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzed error rates and semantic similarity across genders",
      "geographic_focus": [
        "Dutch"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2412.00012v1",
    "title": "The Femininomenon of Inequality: A Data-Driven Analysis and Cluster Profiling in Indonesia",
    "year": 2024,
    "authors": [
      "J. S. Muthmaina"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes regional gender disparities and empowerment, addressing social gender inequality.",
      "inequality_type": [
        "gender",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender disparities across Indonesian regions",
      "affected_populations": [
        "women",
        "regional communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Clustering",
        "Correlation Analysis"
      ],
      "methodology_detail": "Uses indices and clustering to profile regions",
      "geographic_focus": [
        "Indonesia"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.09056v1",
    "title": "Optimisation Strategies for Ensuring Fairness in Machine Learning: With and Without Demographics",
    "year": 2024,
    "authors": [
      "Quan Zhou"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on biases related to sensitive attributes like gender and race, and aims to mitigate social discrimination in algorithms.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Addresses bias mitigation without sensitive attribute reliance",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Frameworks for bias mitigation and fairness optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.08135v1",
    "title": "On the Role of Speech Data in Reducing Toxicity Detection Bias",
    "year": 2024,
    "authors": [
      "Samuel J. Bell",
      "Mariano Coria Meglioli",
      "Megan Richards",
      "Eduardo S√°nchez",
      "Christophe Ropers",
      "Skyler Wang",
      "Adina Williams",
      "Levent Sagun",
      "Marta R. Costa-juss√†"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in toxicity detection related to demographic groups, highlighting social bias and fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in toxicity detection systems",
      "affected_populations": [
        "demographic groups",
        " marginalized communities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Comparison of speech- and text-based classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.17704v1",
    "title": "Racism in the Machine: Visualization Ethics in Digital Humanities Projects",
    "year": 2024,
    "authors": [
      "K. J. Hepworth",
      "Christopher Church"
    ],
    "categories": [
      "cs.MM",
      "J.5; I.3.6; I.m"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial bias in data visualizations related to digital humanities projects, highlighting implicit bias and ethical considerations, which relate to racial inequality and social discrimination.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "Focus on racial bias in digital mapping projects",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Case Study",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzes two digital mapping projects for ethical considerations",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.07521v5",
    "title": "Fair Summarization: Bridging Quality and Diversity in Extractive Summaries",
    "year": 2024,
    "authors": [
      "Sina Bagheri Nezhad",
      "Sayan Bandyapadhyay",
      "Ameeta Agrawal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in summarization across social groups, highlighting bias and representation issues related to race and ethnicity.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social"
      ],
      "other_detail": "Focus on social group fairness in NLP outputs",
      "affected_populations": [
        "White-aligned",
        "Hispanic",
        "African-American"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops fair summarization methods and evaluates on social group data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.06970v1",
    "title": "Enhancing Accessibility in Special Libraries: A Study on AI-Powered Assistive Technologies for Patrons with Disabilities",
    "year": 2024,
    "authors": [
      "Snehasish Paul Shivali Chauhan"
    ],
    "categories": [
      "cs.DL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on improving access for persons with disabilities, addressing social inequality related to physical ability and inclusion in library services.",
      "inequality_type": [
        "disability",
        "informational",
        "digital"
      ],
      "other_detail": "Enhances access for disabled populations",
      "affected_populations": [
        "persons with disabilities"
      ],
      "methodology": [
        "Literature Review",
        "Survey",
        "Interviews",
        "Case Study"
      ],
      "methodology_detail": "Mixed-methods approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.05782v1",
    "title": "Gender Inequalities in Content Collaborations: Asymmetric Creator Synergy and Symmetric Audience Biases",
    "year": 2024,
    "authors": [
      "Mingyue Zha",
      "Ho-Chun Herbert Chang"
    ],
    "categories": [
      "cs.CY",
      "97P70"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in content collaborations, audience biases, and in-group dominance, addressing gender inequality in digital environments.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gaming communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Shapley value analysis"
      ],
      "methodology_detail": "Using cooperative game theory to measure collaboration synergy",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.04453v1",
    "title": "Comparing Fairness of Generative Mobility Models",
    "year": 2024,
    "authors": [
      "Daniel Wang",
      "Jack McFarland",
      "Afra Mashhadi",
      "Ekin Ugurel"
    ],
    "categories": [
      "cs.LG",
      "I.6.4; I.2.6; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and biases in mobility models, addressing geographic inequities and social disparities in model performance.",
      "inequality_type": [
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on spatial fairness in mobility data",
      "affected_populations": [
        "urban communities",
        "geographic regions"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Fairness metrics and model comparison framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.04371v2",
    "title": "ComFairGNN: Community Fair Graph Neural Network",
    "year": 2024,
    "authors": [
      "Yonas Sium",
      "Qi Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in GNNs, which relate to social discrimination and inequality, especially in demographic subgroups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias in AI affecting social groups",
      "affected_populations": [
        "demographic subgroups",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Evaluation",
        "Algorithm Debiasing"
      ],
      "methodology_detail": "Community-level bias measurement and debiasing framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.04298v1",
    "title": "A Capabilities Approach to Studying Bias and Harm in Language Technologies",
    "year": 2024,
    "authors": [
      "Hellina Hailu Nigatu",
      "Zeerak Talat"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases, harms, and inclusion issues in language technologies, emphasizing community needs and fairness, which relate to social inequalities.",
      "inequality_type": [
        "linguistic",
        "social",
        "cultural"
      ],
      "other_detail": "Focus on marginalized language communities and social contexts",
      "affected_populations": [
        "language communities",
        "minority groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Community Collaboration"
      ],
      "methodology_detail": "Applying the Capabilities Approach framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.04228v2",
    "title": "dsld: A Socially Relevant Tool for Teaching Statistics",
    "year": 2024,
    "authors": [
      "Taha Abdullah",
      "Arjun Ashok",
      "Brandon Zarate",
      "Shubhada Martha",
      "Billy Ouattara",
      "Norman Matloff",
      "Aditya Mittal"
    ],
    "categories": [
      "stat.ME",
      "cs.IR",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on assessing and mitigating discrimination related to protected social groups, addressing social bias and fairness issues in algorithms.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Focus on discrimination analysis in prediction algorithms",
      "affected_populations": [
        "race groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Tools for bias detection and mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.03700v1",
    "title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models",
    "year": 2024,
    "authors": [
      "Anaelia Ovalle",
      "Krunoslav Lehman Pavasovic",
      "Louis Martin",
      "Luke Zettlemoyer",
      "Eric Michael Smith",
      "Adina Williams",
      "Levent Sagun"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender biases in language models, focusing on gender-diverse identities and harms, addressing social discrimination related to gender.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender-diverse bias in AI models",
      "affected_populations": [
        "transgender",
        "nonbinary",
        "gender-diverse"
      ],
      "methodology": [
        "Survey",
        "Systematic Evaluation",
        "Framework Development"
      ],
      "methodology_detail": "Bias evaluation and framework for implicit signals",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.02671v1",
    "title": "Fair In-Context Learning via Latent Concept Variables",
    "year": 2024,
    "authors": [
      "Karuna Bhaila",
      "Minh-Hao Van",
      "Kennedy Edemacu",
      "Chen Zhao",
      "Feng Chen",
      "Xintao Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social bias and fairness issues in AI, focusing on reducing discrimination related to sensitive variables during in-context learning.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focus on fairness in AI predictions",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Strategies",
        "Data Augmentation"
      ],
      "methodology_detail": "Using latent variables and demonstration selection for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.02600v1",
    "title": "Building New Clubhouses: Bridging Refugee and Migrant Women into Technology Design and Production by Leveraging Assets",
    "year": 2024,
    "authors": [
      "Sonali Hedditch",
      "Dhaval Vyas"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on refugee and migrant women facing intersecting barriers, addressing gender, socioeconomic, and racial inequalities in STEM participation and access.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on marginalized refugee and migrant women",
      "affected_populations": [
        "refugee women",
        "migrant women"
      ],
      "methodology": [
        "Qualitative Study",
        "Community-led Participatory Design"
      ],
      "methodology_detail": "Workshops exploring barriers and assets",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.02307v1",
    "title": "Can Personalized Medicine Coexist with Health Equity? Examining the Cost Barrier and Ethical Implications",
    "year": 2024,
    "authors": [
      "Kishi Kobe Yee Francisco",
      "Andrane Estelle Carnicer Apuhin",
      "Myles Joshua Toledo Tan",
      "Mickael Cavanaugh Byers",
      "Nicholle Mae Amor Tan Maravilla",
      "Hezerul Abdul Karim",
      "Nouar AlDahoul"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in access to personalized medicine between high-income and low-income countries, highlighting economic and health inequalities. It addresses ethical and infrastructural barriers affecting different populations, indicating a focus on social disparities.",
      "inequality_type": [
        "economic",
        "health",
        "geographic"
      ],
      "other_detail": "Focus on global healthcare access disparities",
      "affected_populations": [
        "LMIC populations",
        "HIC populations"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing ethical and infrastructural challenges",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.01685v1",
    "title": "Mitigating Matching Biases Through Score Calibration",
    "year": 2024,
    "authors": [
      "Mohammad Hossein Moslemi",
      "Mostafa Milani"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues and demographic disparities in record matching, which relate to social discrimination and inequality in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on demographic fairness in data integration",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness calibration using optimal transport theory",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.08048v1",
    "title": "Equitable Length of Stay Prediction for Patients with Learning Disabilities and Multiple Long-term Conditions Using Machine Learning",
    "year": 2024,
    "authors": [
      "Emeka Abakasanga",
      "Rania Kousovista",
      "Georgina Cosma",
      "Ashley Akbari",
      "Francesco Zaccardi",
      "Navjot Kaur",
      "Danielle Fitt",
      "Gyuchan Thomas Jun",
      "Reza Kiani",
      "Satheesh Gangadharan"
    ],
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in hospital stay predictions for patients with learning disabilities, focusing on bias mitigation across ethnic groups, highlighting social fairness issues.",
      "inequality_type": [
        "health",
        "disability",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in healthcare AI models",
      "affected_populations": [
        "patients with learning disabilities",
        "ethnic minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Mitigation Algorithms"
      ],
      "methodology_detail": "Applying bias correction techniques to healthcare data",
      "geographic_focus": [
        "Wales"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.00997v1",
    "title": "Identifying Implicit Social Biases in Vision-Language Models",
    "year": 2024,
    "authors": [
      "Kimia Hamidieh",
      "Haoran Zhang",
      "Walter Gerych",
      "Thomas Hartvigsen",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases related to demographic groups, highlighting societal harm and stereotypes, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender"
      ],
      "other_detail": "Focuses on social biases in AI models",
      "affected_populations": [
        "demographic groups",
        "minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Bias taxonomy and image retrieval analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.00585v1",
    "title": "Benchmarking Bias in Large Language Models during Role-Playing",
    "year": 2024,
    "authors": [
      "Xinyue Li",
      "Zhenpeng Chen",
      "Jie M. Zhang",
      "Yiling Lou",
      "Tianlin Li",
      "Weisong Sun",
      "Yang Liu",
      "Xuanzhe Liu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in LLMs related to social roles and demographic attributes, highlighting social bias and fairness issues. It addresses how AI systems may perpetuate or reveal social inequalities during role-playing scenarios.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on social biases in AI outputs",
      "affected_populations": [
        "minorities",
        "women",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias benchmarking and bias detection framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.00190v2",
    "title": "Monitoring fairness in machine learning models that predict patient mortality in the ICU",
    "year": 2024,
    "authors": [
      "Tempest A. van Schaik",
      "Xinggang Liu",
      "Louis Atallah",
      "Omar Badawi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in ML models predicting ICU patient mortality across different social groups, addressing social bias and inequality in healthcare.",
      "inequality_type": [
        "health",
        "racial",
        "gender"
      ],
      "other_detail": "Focus on healthcare disparities and fairness monitoring",
      "affected_populations": [
        "patients by race",
        "patients by sex",
        "patients with diagnoses"
      ],
      "methodology": [
        "Fairness Analysis",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness monitoring and performance comparison",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.23394v1",
    "title": "Auditing for Bias in Ad Delivery Using Inferred Demographic Attributes",
    "year": 2024,
    "authors": [
      "Basileal Imana",
      "Aleksandra Korolova",
      "John Heidemann"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias in ad delivery related to demographic groups such as race, age, and gender, which are key social inequality factors. It addresses fairness and bias in algorithms affecting social groups. The work aims to improve auditing methods to detect social bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Bias detection in algorithmic ad delivery",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Adjusting for demographic inference errors",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.22963v2",
    "title": "Even the \"Devil\" has Rights!",
    "year": 2024,
    "authors": [
      "Mennatullah Siam"
    ],
    "categories": [
      "cs.OH"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination, human rights violations, and social exclusion based on gender, ethnicity, and community labelling, highlighting social biases and inequalities.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social discrimination"
      ],
      "other_detail": "Focus on human rights violations and social exclusion",
      "affected_populations": [
        "women",
        "ethnic minorities",
        "discriminated individuals"
      ],
      "methodology": [
        "Case Study",
        "Qualitative Study"
      ],
      "methodology_detail": "Documented incidents from personal perspective",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.22551v2",
    "title": "FairSkin: Fair Diffusion for Skin Disease Image Generation",
    "year": 2024,
    "authors": [
      "Ruichen Zhang",
      "Yuguang Yao",
      "Zhen Tan",
      "Zhiming Li",
      "Pan Wang",
      "Huan Liu",
      "Jingtong Hu",
      "Sijia Liu",
      "Tianlong Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI-generated skin images related to race, aiming to improve fairness and representation across skin tones, which impacts health disparities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias mitigation in medical image generation",
      "affected_populations": [
        "skin tone groups",
        "patients with skin diseases"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation via resampling mechanism",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.22282v2",
    "title": "Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced by Large Language Models",
    "year": 2024,
    "authors": [
      "Renzhe Yu",
      "Zhen Xu",
      "Sky CH-Wang",
      "Richard Arum"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines educational disparities related to socioeconomic status and digital divides, analyzing impacts on different social groups in educational equity.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "digital"
      ],
      "other_detail": "Focuses on digital divides in education",
      "affected_populations": [
        "disadvantaged students",
        "advantaged students"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analysis of academic submissions over time",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.00831v1",
    "title": "Saliency-Based diversity and fairness Metric and FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and Diversity",
    "year": 2024,
    "authors": [
      "Teerath Kumar",
      "Alessandra Mileo",
      "Malika Bendechache"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating gender bias and promoting fairness in computer vision models, addressing social discrimination issues related to gender and diversity.",
      "inequality_type": [
        "gender",
        "diversity"
      ],
      "other_detail": "Focuses on fairness and bias reduction in AI models",
      "affected_populations": [
        "women",
        "diverse gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation and fairness evaluation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2411.02587v1",
    "title": "A Big Data-empowered System for Real-time Detection of Regional Discriminatory Comments on Vietnamese Social Media",
    "year": 2024,
    "authors": [
      "An Nghiep Huynh",
      "Thanh Dat Do",
      "Trong Hop Do"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses regional discrimination, a form of social inequality related to geographic location, using AI for detection.",
      "inequality_type": [
        "geographic",
        "discrimination"
      ],
      "other_detail": "Focuses on regional discrimination in Vietnam",
      "affected_populations": [
        "regional communities",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Transfer Learning",
        "System Design"
      ],
      "methodology_detail": "Real-time detection system using big data and streaming",
      "geographic_focus": [
        "Vietnam"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.21898v2",
    "title": "A Longitudinal Analysis of Racial and Gender Bias in New York Times and Fox News Images and Articles",
    "year": 2024,
    "authors": [
      "Hazem Ibrahim",
      "Nouar AlDahoul",
      "Syed Mustafa Ali Abbasi",
      "Fareed Zaffar",
      "Talal Rahwan",
      "Yasir Zaki"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in news media, highlighting under-representation and portrayal disparities, which are key aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on media representation and bias analysis",
      "affected_populations": [
        "racial minorities",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops classifiers and analyzes large news datasets",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.20965v1",
    "title": "Simultaneous Unlearning of Multiple Protected User Attributes From Variational Autoencoder Recommenders Using Adversarial Training",
    "year": 2024,
    "authors": [
      "Gustavo Escobedo",
      "Christian Ganh√∂r",
      "Stefan Brandl",
      "Mirjam Augstein",
      "Markus Schedl"
    ],
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on removing demographic biases related to gender and age, addressing fairness and privacy issues in AI systems, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Demographic fairness and privacy in AI systems",
      "affected_populations": [
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Adversarial Training"
      ],
      "methodology_detail": "Using adversarial training with variational autoencoders",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.20739v3",
    "title": "Gender Bias in LLM-generated Interview Responses",
    "year": 2024,
    "authors": [
      "Haein Kong",
      "Yongsu Ahn",
      "Sangyub Lee",
      "Yunho Maeng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI-generated interview responses, addressing gender inequality and social bias in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Model Evaluation"
      ],
      "methodology_detail": "Assessing bias across different LLMs and question types",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.20229v1",
    "title": "Modelling of Economic Implications of Bias in AI-Powered Health Emergency Response Systems",
    "year": 2024,
    "authors": [
      "Katsiaryna Bahamazava"
    ],
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes how bias in AI affects resource allocation and social welfare, highlighting impacts on demographic groups, which relates to social inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focuses on health emergency response disparities",
      "affected_populations": [
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Economic Modeling"
      ],
      "methodology_detail": "Integrates health economics and AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.19444v1",
    "title": "Balancing the Scales: Enhancing Fairness in Facial Expression Recognition with Latent Alignment",
    "year": 2024,
    "authors": [
      "Syed Sameen Ahmad Rizvi",
      "Aryan Seth",
      "Pratik Narang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in facial expression datasets related to demographic representation, impacting fairness across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Bias mitigation in facial expression recognition datasets",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Representation Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Using latent space representation learning for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.19314v2",
    "title": "Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)",
    "year": 2024,
    "authors": [
      "Leander Girrbach",
      "Stephan Alaniz",
      "Yiran Huang",
      "Trevor Darrell",
      "Zeynep Akata"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI systems, highlighting social discrimination aspects.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in vision-language models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Debiasing techniques"
      ],
      "methodology_detail": "evaluates bias and fine-tuning debiasing methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.19067v1",
    "title": "Less Discriminatory Alternative and Interpretable XGBoost Framework for Binary Classification",
    "year": 2024,
    "authors": [
      "Andrew Pangia",
      "Agus Sudjianto",
      "Aijun Zhang",
      "Taufiquar Khan"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and discrimination in financial lending, related to social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on fair lending and discrimination mitigation",
      "affected_populations": [
        "borrowers",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Biobjective Optimization",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Balancing accuracy and fairness in classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.18749v1",
    "title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?",
    "year": 2024,
    "authors": [
      "Md. Khairul Islam",
      "Andrew Wang",
      "Tianhao Wang",
      "Yangfeng Ji",
      "Judy Fox",
      "Jieyu Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how differential privacy affects bias against protected groups in NLP models, addressing fairness and social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias in AI models related to protected social groups",
      "affected_populations": [
        "underrepresented groups",
        "protected groups"
      ],
      "methodology": [
        "Empirical Analysis",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes bias metrics under differential privacy conditions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.17555v1",
    "title": "FairDgcl: Fairness-aware Recommendation with Dynamic Graph Contrastive Learning",
    "year": 2024,
    "authors": [
      "Wei Chen",
      "Meng Yuan",
      "Zhao Zhang",
      "Ruobing Xie",
      "Fuzhen Zhuang",
      "Deqing Wang",
      "Rui Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI recommendation systems, focusing on reducing bias across user groups based on sensitive attributes like age and gender.",
      "inequality_type": [
        "gender",
        "age",
        "informational"
      ],
      "other_detail": "Focuses on algorithmic fairness in AI systems",
      "affected_populations": [
        "users by age",
        "users by gender"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Learning",
        "Adversarial Training"
      ],
      "methodology_detail": "Dynamic graph contrastive learning framework",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.17519v2",
    "title": "Large Language Models Still Exhibit Bias in Long Text",
    "year": 2024,
    "authors": [
      "Wonje Jeung",
      "Dongjae Jeon",
      "Ashkan Yousefpour",
      "Jonghyun Choi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to demographic groups such as gender and race, highlighting social discrimination issues in AI responses. It evaluates how language models may favor or neglect certain social groups, indicating concern with social fairness. The focus on biases affecting marginalized groups aligns with social inequality topics.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Biases in long-text AI generation",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Bias evaluation framework and bias mitigation approach",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.17492v1",
    "title": "BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers",
    "year": 2024,
    "authors": [
      "Jiaqi Xue",
      "Qian Lou",
      "Mengxin Zheng"
    ],
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing bias and discrimination affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Targets fairness and bias in AI systems",
      "affected_populations": [
        "target groups",
        "non-target groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Backdoored attack evaluation on fairness mechanisms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.17333v1",
    "title": "Are Large Language Models Ready for Travel Planning?",
    "year": 2024,
    "authors": [
      "Ruiping Ren",
      "Xing Yao",
      "Shu Cole",
      "Haining Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in LLMs, highlighting social discrimination issues. It analyzes how AI systems may perpetuate stereotypes affecting marginalized groups. The focus on biases and stereotypes directly relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Biases in AI systems affecting social groups",
      "affected_populations": [
        "racial minorities",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and mitigation strategies in LLM outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.16927v1",
    "title": "Revealing Hidden Bias in AI: Lessons from Large Language Models",
    "year": 2024,
    "authors": [
      "Django Beatty",
      "Kritsada Masanthia",
      "Teepakorn Kaphol",
      "Niphan Sethi"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "I.2.7; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, race, and age in AI systems, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias reduction in AI recruitment tools",
      "affected_populations": [
        "women",
        "racial minorities",
        "older candidates"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and anonymization effectiveness assessment",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.16574v1",
    "title": "How Can We Diagnose and Treat Bias in Large Language Models for Clinical Decision-Making?",
    "year": 2024,
    "authors": [
      "Kenza Benkirane",
      "Jackie Kay",
      "Maria Perez-Ortiz"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in LLMs related to gender and ethnicity, which are social categories linked to inequality. It discusses social bias evaluation and mitigation in AI systems used in healthcare, impacting social groups differently.",
      "inequality_type": [
        "gender",
        "ethnic",
        "health"
      ],
      "other_detail": "Focus on social biases in clinical AI applications",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Bias Evaluation"
      ],
      "methodology_detail": "Develops a bias evaluation framework using clinical datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.16432v2",
    "title": "Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium",
    "year": 2024,
    "authors": [
      "Mehdi Yazdani-Jahromi",
      "Ali Khodabandeh Yalabadi",
      "AmirArsalan Rajabi",
      "Aida Tayebi",
      "Ivan Garibay",
      "Ozlem Ozmen Garibay"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing bias and equitable treatment across groups, which relates directly to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Addresses bias mitigation in machine learning models",
      "affected_populations": [
        "diverse social groups"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Bilevel optimization for fairness-accuracy balance",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.19003v2",
    "title": "Whither Bias Goes, I Will Go: An Integrative, Systematic Review of Algorithmic Bias Mitigation",
    "year": 2024,
    "authors": [
      "Louis Hickman",
      "Christopher Huynh",
      "Jessica Gass",
      "Brandon Booth",
      "Jason Kuruzovich",
      "Louis Tay"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic bias in personnel assessments, which impacts social groups and fairness.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social"
      ],
      "other_detail": "Focuses on fairness in AI-driven personnel selection",
      "affected_populations": [
        "job applicants",
        "employees"
      ],
      "methodology": [
        "Literature Review",
        "Systematic Review",
        "Framework Development"
      ],
      "methodology_detail": "Integrates multiple disciplinary perspectives on bias mitigation",
      "geographic_focus": [
        "United States",
        "Europe"
      ],
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.15464v2",
    "title": "A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia",
    "year": 2024,
    "authors": [
      "Lance Calvin Lim Gamboa",
      "Mark Lee"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in language models related to gender and sexuality, which are social discrimination issues. It analyzes how biases are attributed and explained in AI systems, focusing on societal stereotypes. The study's focus on bias attribution in AI aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Bias attribution and explainability in AI models",
      "affected_populations": [
        "women",
        "LGBTQ+ individuals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Information Theory",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Token-level bias attribution measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.15233v1",
    "title": "A Semidefinite Relaxation Approach for Fair Graph Clustering",
    "year": 2024,
    "authors": [
      "Sina Baharlouei",
      "Sadra Sabouri"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in graph clustering, addressing disparities among social groups, and aims to prevent biased outcomes, indicating a direct concern with social inequality issues.",
      "inequality_type": [
        "social",
        "demographic",
        "fairness"
      ],
      "other_detail": "addresses social fairness in algorithmic clustering",
      "affected_populations": [
        "diverse communities",
        "social groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "optimization and approximation algorithms for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.14230v1",
    "title": "Global Inequalities in the Production of Artificial Intelligence: A Four-Country Study on Data Work",
    "year": 2024,
    "authors": [
      "Antonio A. Casilli",
      "Paola Tubaro",
      "Maxime Cornet",
      "Cl√©ment Le Ludec",
      "Juana Torres-Cierpe",
      "Matheus Viana Braz"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines global inequalities in AI data work, highlighting economic dependencies, labor conditions, and historical power dynamics across countries, which relate to social and economic inequalities.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on labor and economic dependencies in AI production",
      "affected_populations": [
        "data workers",
        "lower-income countries"
      ],
      "methodology": [
        "Mixed-Method Design"
      ],
      "methodology_detail": "Original data collection and cross-country comparison",
      "geographic_focus": [
        "Venezuela",
        "Brazil",
        "Madagascar",
        "France"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.14201v1",
    "title": "Text-to-Image Representativity Fairness Evaluation Framework",
    "year": 2024,
    "authors": [
      "Asma Yamani",
      "Malak Baslyman"
    ],
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in AI systems related to social groups, focusing on representativity, inclusion, and fairness, which are directly linked to social inequalities such as race and ethnicity.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Bias mitigation in AI systems affecting marginalized groups",
      "affected_populations": [
        "minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Experiment",
        "Model Development",
        "Human-based approaches"
      ],
      "methodology_detail": "Evaluation of bias and fairness in AI-generated images",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.14070v1",
    "title": "FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases via Saliency-Based Data Augmentation",
    "year": 2024,
    "authors": [
      "Teerath Kumar",
      "Alessandra Mileo",
      "Malika Bendechache"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias and biases related to geography and stereotypes in AI models, focusing on fairness and inclusivity. It evaluates and mitigates biases affecting social groups, indicating a focus on social inequality issues.",
      "inequality_type": [
        "gender",
        "geographic",
        "stereotypical"
      ],
      "other_detail": "Bias mitigation in computer vision models",
      "affected_populations": [
        "women",
        "geographic groups",
        "stereotyped groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Saliency-based data augmentation and bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.14029v1",
    "title": "Auditing and Enforcing Conditional Fairness via Optimal Transport",
    "year": 2024,
    "authors": [
      "Mohsen Ghassemi",
      "Alan Mishler",
      "Niccolo Dalmasso",
      "Luhao Zhang",
      "Vamsi K. Potluru",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in predictive models, addressing social bias and discrimination issues related to demographic parity, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "minority groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Regularization Techniques"
      ],
      "methodology_detail": "Using optimal transport for fairness measures",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.14012v2",
    "title": "LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education",
    "year": 2024,
    "authors": [
      "Iain Weissburg",
      "Sathvika Anand",
      "Sharon Levy",
      "Haewon Jeong"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in LLMs related to race, ethnicity, gender, income, and disability, highlighting their impact on educational equity.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender",
        "income",
        "disability",
        "educational"
      ],
      "other_detail": "Focus on bias in personalized educational content",
      "affected_populations": [
        "students from marginalized groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Bias Metrics"
      ],
      "methodology_detail": "Analyzes bias scores across multiple models and demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.13831v1",
    "title": "The Disparate Benefits of Deep Ensembles",
    "year": 2024,
    "authors": [
      "Kajetan Schweighofer",
      "Adrian Arnaiz-Rodriguez",
      "Sepp Hochreiter",
      "Nuria Oliver"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how Deep Ensembles affect fairness across social groups, addressing algorithmic fairness issues related to protected attributes like race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Fairness in AI models across social groups",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "patients"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Empirical analysis on datasets with protected attributes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.13286v2",
    "title": "A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex Fairness Objective Landscapes",
    "year": 2024,
    "authors": [
      "Jake Robertson",
      "Thorsten Schmidt",
      "Frank Hutter",
      "Noor Awad"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on social fairness metrics and social consequences, indicating engagement with social inequality issues.",
      "inequality_type": [
        "racial",
        "educational",
        "social fairness"
      ],
      "other_detail": "Focus on fairness metrics and social impact",
      "affected_populations": [
        "students",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Empirical Evaluation",
        "Case Study"
      ],
      "methodology_detail": "Fairness-aware model selection and empirical analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.13250v1",
    "title": "Perceptions of Discriminatory Decisions of Artificial Intelligence: Unpacking the Role of Individual Characteristics",
    "year": 2024,
    "authors": [
      "Soojong Kim"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines perceptions of AI bias related to gender, race, age, and income, highlighting disparities and societal divisions.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focuses on understanding disparities in AI perceptions and trust",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly",
        "low-income groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Large-scale experiment dataset analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.13146v2",
    "title": "debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias",
    "year": 2024,
    "authors": [
      "Kuleen Sasse",
      "Shan Chen",
      "Jackson Pond",
      "Danielle Bitterman",
      "John Osborne"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases and fairness issues in vision-language AI models, addressing social bias related to gender and potentially other social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on bias detection and mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Model Development",
        "Experiment"
      ],
      "methodology_detail": "Bias analysis and debiasing in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.13114v1",
    "title": "Sound Check: Auditing Audio Datasets",
    "year": 2024,
    "authors": [
      "William Agnew",
      "Julia Barnett",
      "Annie Chu",
      "Rachel Hong",
      "Michael Feffer",
      "Robin Netzorg",
      "Harry H. Jiang",
      "Ezra Awumey",
      "Sauvik Das"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CY",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases, toxicity, and copyright issues in audio datasets, highlighting social discrimination and bias against marginalized groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias and toxicity in audio datasets",
      "affected_populations": [
        "women",
        "marginalized communities"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Auditing prominent datasets for bias and toxicity",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.19803v2",
    "title": "First-Person Fairness in Chatbots",
    "year": 2024,
    "authors": [
      "Tyna Eloundou",
      "Alex Beutel",
      "David G. Robinson",
      "Keren Gu-Lemberg",
      "Anna-Luisa Brakman",
      "Pamela Mishkin",
      "Meghan Shah",
      "Johannes Heidecke",
      "Lilian Weng",
      "Adam Tauman Kalai"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in chatbots related to demographic groups, addressing social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias assessment in AI chatbot responses",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Using a Language Model as a Research Assistant",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.12511v1",
    "title": "Advancing Fairness in Natural Language Processing: From Traditional Methods to Explainability",
    "year": 2024,
    "authors": [
      "Fanny Jourdan"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses fairness, bias mitigation, and impacts on diverse populations, addressing social discrimination issues in NLP systems.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focus on bias and fairness in AI systems",
      "affected_populations": [
        "diverse human populations"
      ],
      "methodology": [
        "Algorithm Development",
        "Explainability Methods",
        "Bias Evaluation",
        "Data Analysis"
      ],
      "methodology_detail": "Develops bias mitigation and explainability techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.12499v2",
    "title": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?",
    "year": 2024,
    "authors": [
      "Samee Arif",
      "Zohaib Khan",
      "Maaidah Kaleem",
      "Suhaib Rashid",
      "Agha Ali Raza",
      "Awais Athar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in LLMs across social dimensions like gender, race, and religion, highlighting social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "religious"
      ],
      "other_detail": "Bias across social groups in AI-generated text",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Human Evaluation"
      ],
      "methodology_detail": "Bias measurement and evaluation in LLM outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.11084v1",
    "title": "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts",
    "year": 2024,
    "authors": [
      "Sharon Levy",
      "William D. Adler",
      "Tahilin Sanchez Karver",
      "Mark Dredze",
      "Michelle R. Kaufman"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI decision-making related to relationships, highlighting gender stereotypes and biases, which are social inequalities. It analyzes how models reflect and potentially reinforce gender-based disparities and stereotypes. The focus on gender bias in AI aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes and biases in AI",
      "affected_populations": [
        "women",
        "men",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Using a new dataset and scenario analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.11059v1",
    "title": "Assessing Bias in Metric Models for LLM Open-Ended Generation Bias Benchmarks",
    "year": 2024,
    "authors": [
      "Nathaniel Demchak",
      "Xin Guan",
      "Zekun Wu",
      "Ziyi Xu",
      "Adriano Koshiyama",
      "Emre Kazim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to demographic descriptors, addressing social bias and fairness issues. It analyzes how AI systems treat different social groups, which relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on social bias in AI outputs",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Counterfactual Analysis",
        "Explainability Tools"
      ],
      "methodology_detail": "Uses counterfactuals and SHAP for bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.11005v2",
    "title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks",
    "year": 2024,
    "authors": [
      "Fangru Lin",
      "Shaoguang Mao",
      "Emanuele La Malfa",
      "Valentin Hofmann",
      "Adrian de Wynter",
      "Xun Wang",
      "Si-Qing Chen",
      "Michael Wooldridge",
      "Janet B. Pierrehumbert",
      "Furu Wei"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias of LLMs towards dialect speakers, highlighting racial and linguistic disparities.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Focus on dialect fairness for African American Vernacular English",
      "affected_populations": [
        "AAVE speakers"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops dialect-specific benchmark and evaluates models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.10995v3",
    "title": "Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation",
    "year": 2024,
    "authors": [
      "Emmanouil Zaranis",
      "Giuseppe Attanasio",
      "Sweta Agrawal",
      "Andr√© F. T. Martins"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in machine translation quality estimation, highlighting social disparities affecting gender groups. It discusses biases that could reinforce gender stereotypes and inequalities in AI systems. The focus on gender bias aligns with social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "feminine referents"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias across datasets and languages",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.10665v1",
    "title": "Double Jeopardy and Climate Impact in the Use of Large Language Models: Socio-economic Disparities and Reduced Utility for Non-English Speakers",
    "year": 2024,
    "authors": [
      "Aivin V. Solatorio",
      "Gabriel Stefanini Vicente",
      "Holly Krambeck",
      "Olivier Dupriez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes disparities in AI access, performance, and costs across linguistic and socioeconomic groups, highlighting inequalities faced by low-resource language speakers in developing countries.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic",
        "linguistic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on language-based socioeconomic disparities",
      "affected_populations": [
        "non-English speakers",
        "low-resource language users",
        "developing countries"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analysis of language datasets and cost data",
      "geographic_focus": [
        "low-income countries",
        "lower-middle-income countries"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.19775v1",
    "title": "Gender Bias of LLM in Economics: An Existentialism Perspective",
    "year": 2024,
    "authors": [
      "Hui Zhong",
      "Songsheng Chen",
      "Mian Liang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in LLMs, highlighting societal gender stereotypes and their impact on economic decision-making.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on gender bias in AI and economic contexts",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Mathematical Proofs",
        "Empirical Experiments"
      ],
      "methodology_detail": "Using WEAT and theoretical analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.09992v1",
    "title": "Evaluating Gender Bias of LLMs in Making Morality Judgements",
    "year": 2024,
    "authors": [
      "Divij Bajaj",
      "Yuanyuan Lei",
      "Jonathan Tong",
      "Ruihong Huang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in LLMs, addressing social discrimination related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI moral judgments",
      "affected_populations": [
        "female characters",
        "male characters"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Curated dataset and bias evaluation experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.12864v1",
    "title": "Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs",
    "year": 2024,
    "authors": [
      "Divyanshu Kumar",
      "Umang Jain",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines implicit biases in LLMs, which relate to social discrimination and fairness issues affecting groups based on race, gender, and other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on AI bias and fairness evaluation",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement across multiple models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.08841v1",
    "title": "Public Transport Network Design for Equality of Accessibility via Message Passing Neural Networks and Reinforcement Learning",
    "year": 2024,
    "authors": [
      "Duo Wang",
      "Maximilien Chau",
      "Andrea Araldo"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing inequality in public transport accessibility, addressing geographic disparities affecting urban populations.",
      "inequality_type": [
        "geographic",
        "urban-rural"
      ],
      "other_detail": "Accessibility inequality in urban regions",
      "affected_populations": [
        "suburban residents",
        "urban poor"
      ],
      "methodology": [
        "Machine Learning",
        "Reinforcement Learning"
      ],
      "methodology_detail": "Combines neural networks and reinforcement learning techniques",
      "geographic_focus": [
        "Montreal"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.21281v1",
    "title": "The Social Impact of Generative LLM-Based AI",
    "year": 2024,
    "authors": [
      "Yu Xie",
      "Sofia Avila"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "00",
      "A.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses AI's impact on social inequality within and between countries, focusing on societal factors and social identity, which relate to social disparities.",
      "inequality_type": [
        "economic",
        "social",
        "educational"
      ],
      "other_detail": "Focuses on societal and social identity impacts",
      "affected_populations": [
        "social groups",
        "countries"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Speculative societal impact discussion",
      "geographic_focus": [
        "US",
        "China"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.08407v2",
    "title": "What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias",
    "year": 2024,
    "authors": [
      "Aida Mohammadshahi",
      "Yani Ioannou"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in AI models, focusing on class bias and individual fairness, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "fairness"
      ],
      "other_detail": "Focus on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "social groups",
        "individuals in sensitive domains"
      ],
      "methodology": [
        "Fairness Metrics Analysis",
        "Deep Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes fairness metrics across datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.08090v1",
    "title": "Crossing Margins: Intersectional Users' Ethical Concerns about Software",
    "year": 2024,
    "authors": [
      "Lauren Olson",
      "Tom P. Humbert",
      "Ricarda Anna-Lena Fischer",
      "Bob Westerveld",
      "Florian Kunneman",
      "Emitz√° Guzm√°n"
    ],
    "categories": [
      "cs.SE",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses ethical concerns faced by marginalized, intersectional communities, highlighting issues like discrimination and cyberbullying, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "social"
      ],
      "other_detail": "Focus on intersectional marginalized identities",
      "affected_populations": [
        "intersectional users",
        "marginalized groups"
      ],
      "methodology": [
        "Deep Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed social media posts for ethical concerns",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.07884v1",
    "title": "Generated Bias: Auditing Internal Bias Dynamics of Text-To-Image Generative Models",
    "year": 2024,
    "authors": [
      "Abhishek Mandal",
      "Susan Leavy",
      "Suzanne Little"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, highlighting social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias amplification in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops metrics to measure bias within models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.07820v1",
    "title": "Mitigating Gender Bias in Code Large Language Models via Model Editing",
    "year": 2024,
    "authors": [
      "Zhanyue Qin",
      "Haochuan Wang",
      "Zecheng Wang",
      "Deyuan Liu",
      "Cunhang Fan",
      "Zhao Lv",
      "Zhiying Tu",
      "Dianhui Chu",
      "Dianbo Sui"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, analyzing social bias and fairness issues related to gender, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in code generation models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Evaluates bias and mitigation techniques in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.07593v2",
    "title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks",
    "year": 2024,
    "authors": [
      "Hoin Jung",
      "Taeuk Jang",
      "Xiaoqian Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models that reinforce societal stereotypes, particularly gender biases, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias reduction in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation without retraining",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.06967v1",
    "title": "$\\texttt{ModSCAN}$: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities",
    "year": 2024,
    "authors": [
      "Yukun Jiang",
      "Zheng Li",
      "Xinyue Shen",
      "Yugeng Liu",
      "Michael Backes",
      "Yang Zhang"
    ],
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates stereotypical biases related to gender and race in AI models, which are social inequalities. It examines biases affecting social groups and discusses bias mitigation, aligning with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias measurement in vision-language AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Bias measurement framework and bias reduction techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.07274v1",
    "title": "Mitigation of gender bias in automatic facial non-verbal behaviors generation",
    "year": 2024,
    "authors": [
      "Alice Delbosc",
      "Magalie Ochs",
      "Nicolas Sabouret",
      "Brian Ravenet",
      "Stephane Ayache"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.NE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in facial non-verbal behaviors, a social discrimination issue, and proposes mitigation methods to reduce gender sensitivity in AI-generated behaviors.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in AI behavior generation",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Includes classifier and behavior generation model",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.06423v1",
    "title": "FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications",
    "year": 2024,
    "authors": [
      "Nga Pham",
      "Minh Kha Do",
      "Tran Vu Dai",
      "Pham Ngoc Hung",
      "Anh Nguyen-Duc"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI/ML models affecting diverse social groups, addressing social bias and discrimination issues related to multiple sensitive features such as gender, race, and age.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on intersectionality in fairness assessments",
      "affected_populations": [
        "students",
        "educational minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Improving fairness across multiple sensitive features",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.06385v2",
    "title": "Skin Cancer Machine Learning Model Tone Bias",
    "year": 2024,
    "authors": [
      "James Pope",
      "Md Hassanuzzaman",
      "William Chapman",
      "Huw Day",
      "Mingmar Sherpa",
      "Omar Emara",
      "Nirmala Adhikari",
      "Ayush Joshi"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias related to skin tone, a social attribute linked to race and health disparities, and discusses fairness concerns in AI models affecting different skin tones.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias in medical AI systems due to dataset imbalance",
      "affected_populations": [
        "darker skin tone individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparative analysis of imbalanced and balanced datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.06214v2",
    "title": "Fair-OBNC: Correcting Label Noise for Fairer Datasets",
    "year": 2024,
    "authors": [
      "In√™s Oliveira e Silva",
      "S√©rgio Jesus",
      "Hugo Ferreira",
      "Pedro Saleiro",
      "In√™s Sousa",
      "Pedro Bizarro",
      "Carlos Soares"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to race and fairness in datasets used for decision-making, aiming to improve demographic parity and reduce discriminatory outcomes.",
      "inequality_type": [
        "racial",
        "fairness"
      ],
      "other_detail": "Focus on bias correction in training data",
      "affected_populations": [
        "African-American offenders",
        "White offenders"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Label noise correction with fairness considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.05810v1",
    "title": "Uncertainty-Aware Fairness-Adaptive Classification Trees",
    "year": 2024,
    "authors": [
      "Anna Gottard",
      "Vanessa Verrina",
      "Sabrina Giordano"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing discrimination across protected groups, which relates to social inequalities such as race, gender, and other social biases.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "fairness"
      ],
      "other_detail": "Fairness in AI decision-making processes",
      "affected_populations": [
        "protected groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness-aware impurity measure with uncertainty modeling",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.05487v1",
    "title": "Group Fairness Metrics for Community Detection Methods in Social Networks",
    "year": 2024,
    "authors": [
      "Elze de Vink",
      "Akrati Saxena"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses structural biases and inequalities in social networks, focusing on fairness in community detection related to social groups such as ethnicity, gender, and wealth.",
      "inequality_type": [
        "ethnic",
        "gender",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focus on structural biases in social network communities",
      "affected_populations": [
        "minority groups",
        "social minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluating fairness metrics and community detection methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.05401v1",
    "title": "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation",
    "year": 2024,
    "authors": [
      "Tunazzina Islam",
      "Dan Goldwasser"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic targeting and fairness in social media ads, highlighting biases related to age and gender, which are social inequality issues.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Biases in demographic classification and targeting strategies",
      "affected_populations": [
        "women",
        "senior citizens",
        "young adults"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using LLMs for classification and fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.05206v1",
    "title": "Studying and Mitigating Biases in Sign Language Understanding Models",
    "year": 2024,
    "authors": [
      "Katherine Atwell",
      "Danielle Bragg",
      "Malihe Alikhani"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in sign language models, focusing on fairness and equitable access, which relate to social inequalities such as disability and linguistic diversity.",
      "inequality_type": [
        "disability",
        "linguistic",
        "informational"
      ],
      "other_detail": "Focus on sign language community and linguistic diversity",
      "affected_populations": [
        "sign language users",
        "linguistic minorities"
      ],
      "methodology": [
        "Bias Analysis",
        "Model Development",
        "Bias Mitigation"
      ],
      "methodology_detail": "Using demographic data and bias reduction techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.05180v2",
    "title": "Mitigating the Risk of Health Inequity Exacerbated by Large Language Models",
    "year": 2024,
    "authors": [
      "Yuelyu Ji",
      "Wenhe Ma",
      "Sonish Sivarajkumar",
      "Hang Zhang",
      "Eugene Mathew Sadhu",
      "Zhuochun Li",
      "Xizhi Wu",
      "Shyam Visweswaran",
      "Yanshan Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses health disparities linked to sociodemographic factors and AI bias, addressing social inequality issues.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on health disparities and AI fairness",
      "affected_populations": [
        "healthcare patients",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "System Design",
        "Evaluation"
      ],
      "methodology_detail": "Detecting and mitigating AI bias in healthcare",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.17269v1",
    "title": "FairFML: Fair Federated Machine Learning with a Case Study on Reducing Gender Disparities in Cardiac Arrest Outcome Prediction",
    "year": 2024,
    "authors": [
      "Siqi Li",
      "Qiming Wu",
      "Xin Li",
      "Di Miao",
      "Chuan Hong",
      "Wenjun Gu",
      "Yuqing Shang",
      "Yohei Okada",
      "Michael Hao Chen",
      "Mengying Yan",
      "Yilin Ning",
      "Marcus Eng Hock Ong",
      "Nan Liu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender disparities in healthcare outcomes, focusing on fairness in AI models, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on reducing gender disparities in medical predictions",
      "affected_populations": [
        "women",
        "cardiac arrest patients"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Framework"
      ],
      "methodology_detail": "Federated learning for privacy-preserving fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.04614v1",
    "title": "Building Solidarity Amid Hostility: Experiences of Fat People in Online Communities",
    "year": 2024,
    "authors": [
      "Blakeley H. Payne",
      "Jordan Taylor",
      "Katta Spiel",
      "Casey Fiesler"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines anti-fatness as a form of social marginalization and discrimination, highlighting societal and technological harms faced by fat individuals. It discusses how online communities serve as spaces for support and resistance, addressing social bias and inequality. The focus on marginalized status and societal oppression confirms its relevance to social inequality.",
      "inequality_type": [
        "health",
        "disability",
        "social discrimination"
      ],
      "other_detail": "Focus on fat marginalization and societal oppression",
      "affected_populations": [
        "fat people"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Semi-structured interviews with participants",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.03855v1",
    "title": "A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research",
    "year": 2024,
    "authors": [
      "Teresa Salazar",
      "Helder Ara√∫jo",
      "Alberto Cano",
      "Pedro Henriques Abreu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "68T01",
      "I.2.6; I.5.1; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems across sensitive social groups, addressing biases related to race and gender, which are core social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness in social contexts",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Literature Review",
        "Survey"
      ],
      "methodology_detail": "Comprehensive review of fairness approaches in federated learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.03126v1",
    "title": "Understanding Decision Subjects' Engagement with and Perceived Fairness of AI Models When Opportunities of Qualification Improvement Exist",
    "year": 2024,
    "authors": [
      "Meric Altug Gemalmaz",
      "Ming Yin"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness perceptions and strategic responses to AI decisions affecting groups, highlighting biases against protected attributes and barriers to improvement, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focus on fairness perceptions and group biases",
      "affected_populations": [
        "decision subjects",
        "protected groups"
      ],
      "methodology": [
        "Experiment",
        "Human-Subject Study"
      ],
      "methodology_detail": "Three human-subject experiments on engagement and fairness perceptions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.03102v3",
    "title": "Examining Racial Stereotypes in YouTube Autocomplete Suggestions",
    "year": 2024,
    "authors": [
      "Eunbin Ha",
      "Haein Kong",
      "Shagun Jhaver"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial stereotypes and biases in YouTube autocomplete suggestions, addressing racial discrimination and representation issues. It analyzes how algorithms reflect and perpetuate social biases related to race. The focus on racial stereotypes and societal contexts indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on racial stereotypes and biases",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Critical Discourse Analysis"
      ],
      "methodology_detail": "Analyzes autocomplete outputs and sociocultural contexts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.02584v1",
    "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
    "year": 2024,
    "authors": [
      "Angana Borah",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates implicit gender biases in LLM interactions, addressing gender inequality and social bias issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on implicit gender bias in AI interactions",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "bias detection and mitigation strategies in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.02482v2",
    "title": "It is Giving Major Satisfaction: Why Fairness Matters for Developers",
    "year": 2024,
    "authors": [
      "Emeralda Sesari",
      "Federica Sarro",
      "Ayushi Rastogi"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness perceptions related to gender, ethnicity, and demographics in software workplaces, addressing social discrimination and bias.",
      "inequality_type": [
        "gender",
        "ethnic",
        "disability",
        "educational"
      ],
      "other_detail": "Focus on workplace fairness and social group disparities",
      "affected_populations": [
        "female practitioners",
        "ethnically underrepresented",
        "less experienced",
        "those with work limitations"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Ordinal logistic regression and moderation analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.02042v1",
    "title": "EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning",
    "year": 2024,
    "authors": [
      "Syed Irfan Ali Meerza",
      "Jian Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on exacerbating algorithmic bias against demographic groups, addressing fairness issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "fairness"
      ],
      "other_detail": "Bias amplification in federated learning",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluates attack effectiveness on fairness algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.01888v2",
    "title": "Conformal Prediction Sets Can Cause Disparate Impact",
    "year": 2024,
    "authors": [
      "Jesse C. Cresswell",
      "Bhargava Kumar",
      "Yi Sui",
      "Mouloud Belbahri"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparate impact in AI decision-making across social groups, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "disparity"
      ],
      "other_detail": "Focuses on fairness in AI decision processes",
      "affected_populations": [
        "protected groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Human participant experiments on fairness outcomes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.01227v1",
    "title": "See Me and Believe Me: Causality and Intersectionality in Testimonial Injustice in Healthcare",
    "year": 2024,
    "authors": [
      "Kenya S. Andrews",
      "Mesrob I. Ohannessian",
      "Elena Zheleva"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines testimonial injustice related to demographic factors like race, gender, and age, highlighting social biases in healthcare. It analyzes how prejudices contribute to unequal treatment and marginalization of vulnerable groups. The focus on social discrimination and bias in medical settings confirms its relevance to social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focus on healthcare-related testimonial injustice",
      "affected_populations": [
        "patients of marginalized groups"
      ],
      "methodology": [
        "Causal Discovery",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using causal discovery to analyze medical notes",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.01089v1",
    "title": "FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks",
    "year": 2024,
    "authors": [
      "Peiran Wu",
      "Che Liu",
      "Canyu Chen",
      "Jun Li",
      "Cosmin I. Bercea",
      "Rossella Arcucci"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness across demographic groups in medical AI models, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender",
        "health"
      ],
      "other_detail": "Focus on fairness in healthcare AI models",
      "affected_populations": [
        "racial groups",
        "ethnic groups",
        "gender groups",
        "patients"
      ],
      "methodology": [
        "Benchmarking",
        "Fairness Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Includes demographic fairness metrics and model assessment",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.00836v1",
    "title": "Towards Fairness and Privacy: A Novel Data Pre-processing Optimization Framework for Non-binary Protected Attributes",
    "year": 2024,
    "authors": [
      "Manh Khoi Duong",
      "Stefan Conrad"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI datasets, which relate to social discrimination issues such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on dataset bias and fairness in AI systems",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Optimization",
        "Genetic Algorithms",
        "Data Debiasing"
      ],
      "methodology_detail": "Debiasing via combinatorial optimization and synthetic data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.00545v2",
    "title": "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study",
    "year": 2024,
    "authors": [
      "Beatrice Savoldi",
      "Sara Papi",
      "Matteo Negri",
      "Ana Guerberof",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation and its societal impacts, highlighting disparities and costs faced by women, thus addressing gender-based social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Human-centered Study",
        "Behavioral Data Collection",
        "Post-editing Analysis"
      ],
      "methodology_detail": "Participants post-edited MT outputs for gender accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.20204v1",
    "title": "Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach",
    "year": 2024,
    "authors": [
      "Aditi Dutta",
      "Susan Banducci",
      "Chico Q. Camargo"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on online sexism and misogyny, which are forms of gender-based social discrimination. It examines measurement approaches for gender-related hate speech, addressing social bias in digital spaces. The research relates to gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "online gender-based discrimination",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Semi-automated search",
        "Topic analysis"
      ],
      "methodology_detail": "Systematic review of interdisciplinary research from 2012-2022",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.19992v2",
    "title": "A large-scale operational study of fingerprint quality and demographics",
    "year": 2024,
    "authors": [
      "Javier Galbally",
      "Aleksandrs Cepilovs",
      "Ramon Blanco-Gonzalo",
      "Gillian Ormiston",
      "Oscar Miguel-Hurtado",
      "Istvan Sz. Racz"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic influences on fingerprint quality and recognition accuracy, highlighting performance variability across population segments, which relates to social fairness issues.",
      "inequality_type": [
        "gender",
        "age",
        "demographic"
      ],
      "other_detail": "Focus on biometric fairness and algorithmic bias",
      "affected_populations": [
        "different demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Large-scale analysis of operational fingerprint data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.19959v2",
    "title": "Gender Biases in LLMs: Higher intelligence in LLM does not necessarily solve gender bias and stereotyping",
    "year": 2024,
    "authors": [
      "Rajesh Ranjan",
      "Shailja Gupta",
      "Surya Naranyan Singh"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender biases in LLMs, highlighting societal stereotypes and disparities related to gender, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender stereotyping and bias in AI models",
      "affected_populations": [
        "females",
        "males",
        "non-binary"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzed personas generated by LLMs for bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 1.0
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.19940v1",
    "title": "Positive-Sum Fairness: Leveraging Demographic Attributes to Achieve Fair AI Outcomes Without Sacrificing Group Gains",
    "year": 2024,
    "authors": [
      "Samia Belhadj",
      "Sanguk Park",
      "Ambika Seth",
      "Hesham Dar",
      "Thijs Kooi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI related to demographic attributes like race, addressing disparities and social bias in healthcare AI outcomes.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on subgroup disparities in medical AI",
      "affected_populations": [
        "racial groups",
        "patients"
      ],
      "methodology": [
        "Experiment",
        "Comparison"
      ],
      "methodology_detail": "Analyzes CNN models with demographic attributes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.19804v2",
    "title": "Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems",
    "year": 2024,
    "authors": [
      "Xuyang Wu",
      "Shuowei Li",
      "Hsin-Tai Wu",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness issues related to demographic attributes in AI systems, addressing social bias concerns.",
      "inequality_type": [
        "gender",
        "geographic",
        "demographic"
      ],
      "other_detail": "Focuses on fairness in AI retrieval and generation components",
      "affected_populations": [
        "gender groups",
        "geographic populations",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Fairness Evaluation Framework"
      ],
      "methodology_detail": "Scenario-based questions and disparity analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.19474v2",
    "title": "FairPIVARA: Reducing and Assessing Biases in CLIP-Based Multimodal Models",
    "year": 2024,
    "authors": [
      "Diego A. B. Moreira",
      "Alef Iury Ferreira",
      "Jhessica Silva",
      "Gabriel Oliveira dos Santos",
      "Luiz Pereira",
      "Jo√£o Medrado Gondim",
      "Gustavo Bonil",
      "Helena Maia",
      "N√°dia da Silva",
      "Simone Tiemi Hashiguti",
      "Jefersson A. dos Santos",
      "Helio Pedrini",
      "Sandra Avila"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models that impact social groups, focusing on fairness and discrimination. It evaluates discriminatory practices and proposes methods to reduce biases, which relate to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "disability",
        "educational"
      ],
      "other_detail": "Bias mitigation in multimodal AI models",
      "affected_populations": [
        "minority groups",
        "language speakers",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Model Development",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Bias reduction techniques in feature embeddings",
      "geographic_focus": [
        "Portugal"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.18470v1",
    "title": "Fairness without Sensitive Attributes via Knowledge Sharing",
    "year": 2024,
    "authors": [
      "Hongliang Ni",
      "Lei Han",
      "Tong Chen",
      "Shazia Sadiq",
      "Gianluca Demartini"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on demographic bias and equitable treatment, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness without sensitive attribute access",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Algorithm Development"
      ],
      "methodology_detail": "Fairness and bias mitigation techniques in AI models",
      "geographic_focus": [
        "COMPAS dataset",
        "New Adult dataset"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.16371v1",
    "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
    "year": 2024,
    "authors": [
      "Amartya Roy",
      "Danush Khanna",
      "Devanshu Mahapatra",
      "Vasanthakumar",
      "Avirup Das",
      "Kripabandhu Ghosh"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social bias mitigation in language models, addressing fairness issues related to social discrimination. It introduces a dataset covering multiple social bias categories, indicating a concern with societal inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focuses on social bias in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "disabled individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias mitigation techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.15949v2",
    "title": "Tuning Into Bias: A Computational Study of Gender Bias in Song Lyrics",
    "year": 2024,
    "authors": [
      "Danqing Chen",
      "Adithi Satish",
      "Rasul Khanbayov",
      "Carolin M. Schuster",
      "Georg Groh"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in song lyrics, highlighting stereotypes and content disparities, which relate to social gender inequality. It examines how thematic content reflects societal biases and stereotypes. The focus on gender bias aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and topic modeling techniques",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.15828v1",
    "title": "Mitigating Digital Discrimination in Dating Apps -- The Dutch Breeze case",
    "year": 2024,
    "authors": [
      "Tim de Jonge",
      "Frederik Zuiderveen Borgesius"
    ],
    "categories": [
      "cs.CY",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial discrimination in a dating app algorithm, highlighting social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "focus on algorithmic racial discrimination",
      "affected_populations": [
        "non-white users",
        "ethnic minorities"
      ],
      "methodology": [
        "Legal Analysis",
        "Computer Science Analysis"
      ],
      "methodology_detail": "Combines legal and technical perspectives on discrimination",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.15567v3",
    "title": "Asking an AI for salary negotiation advice is a matter of concern: Controlled experimental perturbation of ChatGPT for protected and non-protected group discrimination on a contextual task with no clear ground truth answers",
    "year": 2024,
    "authors": [
      "R. Stuart Geiger",
      "Flynn O'Sullivan",
      "Elsie Wang",
      "Jonathan Lo"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, university, and major in AI outputs, highlighting social discrimination concerns.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Bias in AI salary negotiation advice",
      "affected_populations": [
        "women",
        "students",
        "job seekers"
      ],
      "methodology": [
        "Experiment",
        "Bias Audit",
        "Statistical Analysis"
      ],
      "methodology_detail": "Systematic prompts varying social attributes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.14583v3",
    "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
    "year": 2024,
    "authors": [
      "Vishal Mirza",
      "Rahul Kulkarni",
      "Aakanksha Jadhav"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, race, and age in AI models, which directly relate to social discrimination and inequality issues. It analyzes how AI systems reflect and potentially exacerbate social biases affecting different groups. The focus on bias mitigation and social group disparities confirms its relevance to social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias in AI models affecting social groups",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement across multiple LLMs",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.13884v1",
    "title": "A Multi-LLM Debiasing Framework",
    "year": 2024,
    "authors": [
      "Deonna M. Owens",
      "Ryan A. Rossi",
      "Sungchul Kim",
      "Tong Yu",
      "Franck Dernoncourt",
      "Xiang Chen",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Hanieh Deilamsalehy",
      "Nedim Lipka"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in LLMs that perpetuate societal inequalities and social discrimination, focusing on fairness across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focus on bias reduction in AI systems",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Multi-LLM framework for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.13869v1",
    "title": "Generative AI Carries Non-Democratic Biases and Stereotypes: Representation of Women, Black Individuals, Age Groups, and People with Disability in AI-Generated Images across Occupations",
    "year": 2024,
    "authors": [
      "Ayoob Sadeghiani"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in generative AI related to gender, race, age, and disability, highlighting issues of social discrimination and inequality in AI outputs.",
      "inequality_type": [
        "gender",
        "racial",
        "disability",
        "age"
      ],
      "other_detail": "Biases in AI-generated representations across social groups",
      "affected_populations": [
        "Women",
        "Black individuals",
        "Older adults",
        "People with disabilities"
      ],
      "methodology": [
        "Literature Review",
        "Experiment"
      ],
      "methodology_detail": "Analyzes AI outputs for representation biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.13484v1",
    "title": "'Since Lawyers are Males..': Examining Implicit Gender Bias in Hindi Language Generation by LLMs",
    "year": 2024,
    "authors": [
      "Ishika Joshi",
      "Ishita Gupta",
      "Adrita Dey",
      "Tapan Parikh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in language generation, highlighting social discrimination and inequality related to gender. It analyzes how AI systems perpetuate stereotypes, affecting social perceptions and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzing bias patterns in language models",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.12677v1",
    "title": "(Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers",
    "year": 2024,
    "authors": [
      "Manh Khoi Duong",
      "Stefan Conrad"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making, which relates to social discrimination and bias, particularly in social groups such as gender and potentially other social categories.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on fairness metrics and uncertainty in decision-makers",
      "affected_populations": [
        "social groups",
        "decision-makers"
      ],
      "methodology": [
        "Bayesian Statistics",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Quantifies uncertainty in fairness disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.12544v2",
    "title": "Nigerian Software Engineer or American Data Scientist? GitHub Profile Recruitment Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Takashi Nakano",
      "Kazumasa Shimari",
      "Raula Gaikovina Kula",
      "Christoph Treude",
      "Marc Cheong",
      "Kenichi Matsumoto"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI recruitment tools related to geographic and possibly social group disparities, highlighting societal biases in LLM outputs.",
      "inequality_type": [
        "geographic",
        "racial",
        "nationality"
      ],
      "other_detail": "Bias in AI recruitment based on location",
      "affected_populations": [
        "software developers",
        "regional groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing GitHub profiles and LLM responses",
      "geographic_focus": [
        "Nigeria",
        "United States",
        "other regions"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.02801v3",
    "title": "Biases in gendered citation practices: an exploratory study and some reflections on the Matthew and Matilda effects",
    "year": 2024,
    "authors": [
      "Karolina Tchilinguirova",
      "Alvine Boaye Belle",
      "Gouled Mahamud"
    ],
    "categories": [
      "cs.DL",
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines citation biases related to gender, impacting fairness and disparities in academic recognition.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "female researchers"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing citation patterns in software engineering literature",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.12194v3",
    "title": "Gender Representation and Bias in Indian Civil Service Mock Interviews",
    "year": 2024,
    "authors": [
      "Somonnoy Banerjee",
      "Sujan Dutta",
      "Soumyajit Datta",
      "Ashiqur R. KhudaBukhsh"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in interview questions and AI explanations, addressing gender discrimination and bias in social and technological contexts.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female candidates",
        "male candidates"
      ],
      "methodology": [
        "Data Collection",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analysis of interview questions and LLM responses",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.02799v1",
    "title": "A Data Envelopment Analysis Approach for Assessing Fairness in Resource Allocation: Application to Kidney Exchange Programs",
    "year": 2024,
    "authors": [
      "Ali Kaazempur-Mofrad",
      "Xiaowu Dai"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in resource allocation, focusing on ethnic disparities in kidney exchange programs, addressing racial and health inequalities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on ethnic disparities in organ allocation",
      "affected_populations": [
        "ethnic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Data Envelopment Analysis"
      ],
      "methodology_detail": "Evaluates fairness criteria using DEA and conformal prediction",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2410.02791v1",
    "title": "DifFaiRec: Generative Fair Recommender with Conditional Diffusion Model",
    "year": 2024,
    "authors": [
      "Zhenhao Jiang",
      "Jicong Fan"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in recommendation systems related to protected social attributes, highlighting group unfairness and bias reduction.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "social groups",
        "users"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Fairness Module Design"
      ],
      "methodology_detail": "Uses diffusion models and counterfactual fairness techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.11638v1",
    "title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla",
    "year": 2024,
    "authors": [
      "Mahammed Kamruzzaman",
      "Abdullah Al Monsur",
      "Shrabon Das",
      "Enamul Hassan",
      "Gene Louis Kim"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases related to race, gender, region, caste, and religion, which are social inequalities. It analyzes biases in language models affecting social groups within the Bangla-speaking community. The dataset aims to measure and address these biases, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "region",
        "caste",
        "religion"
      ],
      "other_detail": "Bias measurement in multilingual language models",
      "affected_populations": [
        "Bangla speakers",
        "social groups in Bangladesh"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Localized bias dataset for Bangla language",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.11269v1",
    "title": "Testing for racial bias using inconsistent perceptions of race",
    "year": 2024,
    "authors": [
      "Nora Gera",
      "Emma Pierson"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in police treatment, directly addressing racial inequality.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "",
      "affected_populations": [
        "Hispanic",
        "white"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Testing for bias via perceived race differences",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.10989v2",
    "title": "GOSt-MT: A Knowledge Graph for Occupation-related Gender Biases in Machine Translation",
    "year": 2024,
    "authors": [
      "Orfeas Menis Mastromichalakis",
      "Giorgos Filandrianos",
      "Eva Tsouparopoulou",
      "Dimitris Parsanoglou",
      "Maria Symeonaki",
      "Giorgos Stamou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, focusing on societal stereotypes and disparities in occupational representation, which are social inequalities.",
      "inequality_type": [
        "gender",
        "occupational",
        "social bias"
      ],
      "other_detail": "Focus on gender stereotypes in labor and AI systems",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Knowledge Graph Creation",
        "Quantitative Analysis",
        "Textual Data Analysis"
      ],
      "methodology_detail": "Integrates real-world labor data and textual corpora",
      "geographic_focus": [
        "English",
        "French",
        "Greek"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.10825v3",
    "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A Path to Fairness",
    "year": 2024,
    "authors": [
      "Anindya Bijoy Das",
      "Shahnewaz Karim Sakib"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in recommendation systems affecting diverse social groups, highlighting impacts related to socioeconomic status and cultural identities, which are core aspects of social inequality.",
      "inequality_type": [
        "socioeconomic",
        "cultural",
        "educational"
      ],
      "other_detail": "Bias mitigation in AI recommendation systems",
      "affected_populations": [
        "diverse demographic groups",
        "cultural communities"
      ],
      "methodology": [
        "Experiment",
        "Numerical Analysis"
      ],
      "methodology_detail": "Bias analysis and mitigation strategies validation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.10677v1",
    "title": "Mitigating Sex Bias in Audio Data-driven COPD and COVID-19 Breathing Pattern Detection Models",
    "year": 2024,
    "authors": [
      "Rachel Pfeifer",
      "Sudip Vhaduri",
      "James Eric Dietz"
    ],
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses sex bias in healthcare AI models, highlighting fairness issues affecting gender groups.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on sex bias in respiratory disease detection",
      "affected_populations": [
        "male patients",
        "female patients"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Mitigation"
      ],
      "methodology_detail": "Bias reduction techniques in decision tree models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.10676v1",
    "title": "Toward Mitigating Sex Bias in Pilot Trainees' Stress and Fatigue Modeling",
    "year": 2024,
    "authors": [
      "Rachel Pfeifer",
      "Sudip Vhaduri",
      "Mark Wilson",
      "Julius Keller"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models affecting pilot trainees, highlighting fairness and bias mitigation, which are key social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI models for fairness",
      "affected_populations": [
        "female pilot trainees"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Mitigation"
      ],
      "methodology_detail": "Decision trees with bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.09894v1",
    "title": "Estimating Wage Disparities Using Foundation Models",
    "year": 2024,
    "authors": [
      "Keyon Vafa",
      "Susan Athey",
      "David M. Blei"
    ],
    "categories": [
      "cs.LG",
      "econ.EM",
      "stat.ME",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender wage disparities, a key aspect of social inequality, using labor history data. It employs AI models to better understand and decompose gender-based economic disparities.",
      "inequality_type": [
        "gender",
        "economic",
        "income"
      ],
      "other_detail": "Focuses on gender wage gap decomposition using AI",
      "affected_populations": [
        "women",
        "workers"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses foundation models for wage prediction from labor histories",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.09652v1",
    "title": "Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example",
    "year": 2024,
    "authors": [
      "Yuanning Huang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI-generated evaluations, reflecting societal gender stereotypes and inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in language models",
      "affected_populations": [
        "female instructors",
        "male instructors"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Includes OR, WEAT, sentiment, and contextual analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.08135v1",
    "title": "Reducing Population-level Inequality Can Improve Demographic Group Fairness: a Twitter Case Study",
    "year": 2024,
    "authors": [
      "Avijit Ghosh",
      "Tomo Lazovich",
      "Kristian Lum",
      "Christo Wilson"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities and fairness in social media engagement, addressing social groups such as gender, race, and age. It explores how economic inequality metrics relate to demographic biases, linking social inequality with algorithmic fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on social media engagement disparities",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analysis of Twitter user engagement data",
      "geographic_focus": [
        "unspecified",
        "Twitter users globally"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.07424v1",
    "title": "Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation",
    "year": 2024,
    "authors": [
      "Gavin Butts",
      "Pegah Emdad",
      "Jethro Lee",
      "Shannon Song",
      "Chiman Salavati",
      "Willmar Sosa Diaz",
      "Shiri Dori-Hacohen",
      "Fabricio Murai"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "I.2.7; J.3; K.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in medical data affecting health outcomes, which relates to health disparities and social inequality.",
      "inequality_type": [
        "health",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias detection in medical curricula",
      "affected_populations": [
        "patients",
        "vulnerable groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias detection using NLP models and dataset refinement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.05247v1",
    "title": "Socially Responsible Data for Large Multilingual Language Models",
    "year": 2024,
    "authors": [
      "Andrew Smart",
      "Ben Hutchinson",
      "Lameck Mbangula Amugongo",
      "Suzanne Dikker",
      "Alex Zito",
      "Amber Ebinama",
      "Zara Wudiri",
      "Ding Wang",
      "Erin van Liemt",
      "Jo√£o Sedoc",
      "Seyi Olojo",
      "Stanley Uwakwe",
      "Edem Wornyo",
      "Sonja Schmer-Galunder",
      "Jamila Smith-Loud"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses ethical, sociopolitical, and cultural issues related to data collection from marginalized language communities, highlighting power dynamics, consent, and cultural safety, which are linked to social inequalities.",
      "inequality_type": [
        "linguistic",
        "ethnic",
        "cultural",
        "socioeconomic"
      ],
      "other_detail": "Focus on marginalized and underrepresented language communities",
      "affected_populations": [
        "indigenous peoples",
        "colonized communities",
        "non-Western language speakers"
      ],
      "methodology": [
        "Qualitative Study",
        "Community Partnerships",
        "Participatory Design"
      ],
      "methodology_detail": "Engages communities for ethical data collection practices",
      "geographic_focus": [
        "Global South",
        "Global North"
      ],
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.04975v1",
    "title": "PatchAlign:Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels",
    "year": 2024,
    "authors": [
      "Aayushman",
      "Hemanth Gaddey",
      "Vidhi Mittal",
      "Manisha Chawla",
      "Gagan Raj Gupta"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in skin disease diagnosis accuracy and fairness, focusing on reducing ethnic bias in AI models.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on skin tone fairness in medical AI",
      "affected_populations": [
        "different skin tones",
        "ethnic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Graph Optimal Transport",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Aligns clinical labels with image representations for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.04652v2",
    "title": "Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias Measurement in the U.S",
    "year": 2024,
    "authors": [
      "Saikrishna Badrinarayanan",
      "Osonde Osoba",
      "Miao Cheng",
      "Ryan Rogers",
      "Sakshi Jain",
      "Rahul Tandra",
      "Natesh S. Pillai"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring race/ethnicity fairness in AI, addressing social discrimination and bias. It aims to enable equitable assessments across demographic groups while preserving privacy.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on fairness measurement in AI systems",
      "affected_populations": [
        "U.S. LinkedIn members"
      ],
      "methodology": [
        "Bayesian Modeling",
        "Secure Computation",
        "Differential Privacy",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Combines probabilistic modeling with privacy technologies",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.04340v1",
    "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
    "year": 2024,
    "authors": [
      "Shuirong Cao",
      "Ruoxi Cheng",
      "Zhiqiang Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on age bias in LLMs, addressing social fairness issues related to age discrimination.",
      "inequality_type": [
        "age"
      ],
      "other_detail": "none",
      "affected_populations": [
        "different age groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Constructs bias datasets and evaluates fairness improvements",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.13706v1",
    "title": "Decolonising Data Systems: Using Jyutping or Pinyin as tonal representations of Chinese names for data linkage",
    "year": 2024,
    "authors": [
      "Joseph Lam",
      "Mario Cortina-Borja",
      "Robert Aldridge",
      "Ruth Blackburn",
      "Katie Harron"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in data linkage rates affecting Chinese immigrants, impacting health research and policy, which relates to health and racial inequalities.",
      "inequality_type": [
        "health",
        "racial",
        "linguistic"
      ],
      "other_detail": "Improving data accuracy for marginalized Chinese populations",
      "affected_populations": [
        "Chinese immigrants",
        "health research participants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Comparative analysis of romanisation systems",
      "geographic_focus": [
        "Hong Kong",
        "China"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.03843v1",
    "title": "Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption",
    "year": 2024,
    "authors": [
      "Wenchao Dong",
      "Assem Zhunis",
      "Dongyoung Jeong",
      "Hyojin Chin",
      "Jiyoung Han",
      "Meeyoung Cha"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to social groups, addressing social discrimination and bias issues. It discusses outgroup bias and ingroup favoritism, which are core to social inequality. The focus on social identity and bias mitigation aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI models related to social groups",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias detection and mitigation experiments in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.13705v2",
    "title": "Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble",
    "year": 2024,
    "authors": [
      "Olivia Sturman",
      "Aparna Joshi",
      "Bhaktipriya Radharapu",
      "Piyush Kumar",
      "Renee Shelby"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI safety classifiers, focusing on fairness across identity groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "other"
      ],
      "other_detail": "Bias mitigation in AI safety systems",
      "affected_populations": [
        "identity groups",
        "users of AI systems"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Ensemble debiasing, fairness metrics, dataset balancing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.03220v2",
    "title": "FairQuant: Certifying and Quantifying Fairness of Deep Neural Networks",
    "year": 2024,
    "authors": [
      "Brian Hyeongseok Kim",
      "Jingbo Wang",
      "Chao Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, specifically individual fairness related to protected attributes like gender and race, addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Fairness certification in AI systems",
      "affected_populations": [
        "individuals with protected attributes"
      ],
      "methodology": [
        "Formal Certification",
        "Interval Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Certifying and quantifying fairness in DNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.12197v3",
    "title": "Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries",
    "year": 2024,
    "authors": [
      "Mercy Nyamewaa Asiedu",
      "Iskandar Haykel",
      "Awa Dieng",
      "Kerrie Kauer",
      "Tousif Ahmed",
      "Florence Ofori",
      "Charisma Chan",
      "Stephen Pfohl",
      "Negar Rostamzadeh",
      "Katherine Heller"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines perceptions of AI deployment in African health contexts, focusing on trust, fairness, and systemic barriers, which relate to social inequalities such as health disparities and systemic bias.",
      "inequality_type": [
        "health",
        "systemic",
        "social bias"
      ],
      "other_detail": "Focus on systemic barriers and trust in AI deployment",
      "affected_populations": [
        "general population",
        "health experts"
      ],
      "methodology": [
        "Qualitative Study",
        "Survey",
        "Thematic Analysis"
      ],
      "methodology_detail": "Mixed methods combining interviews and surveys",
      "geographic_focus": [
        "African countries"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.03794v1",
    "title": "Evaluating Machine Learning-based Skin Cancer Diagnosis",
    "year": 2024,
    "authors": [
      "Tanish Jain"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness disparities in skin cancer diagnosis models across skin tones, addressing racial and health inequalities. It discusses algorithmic bias and efforts to mitigate disparities among different demographic groups.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on skin tone disparities in medical AI",
      "affected_populations": [
        "light skin",
        "dark skin"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Assessing fairness metrics and applying postprocessing strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.00737v1",
    "title": "Data Collectives as a means to Improve Accountability, Combat Surveillance and Reduce Inequalities",
    "year": 2024,
    "authors": [
      "Jane Hsieh",
      "Angie Zhang",
      "Seyun Kim",
      "Varun Nagaraj Rao",
      "Samantha Dalal",
      "Alexandra Mateescu",
      "Rafael Do Nascimento Grohmann",
      "Motahhare Eslami",
      "Min Kyung Lee",
      "Haiyi Zhu"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "Addresses platform labor issues related to worker surveillance, discrimination, and accountability, which impact social groups and inequalities.",
      "inequality_type": [
        "economic",
        "gender",
        "racial",
        "informational"
      ],
      "other_detail": "Focus on online labor platform worker conditions",
      "affected_populations": [
        "platform workers",
        "digital laborers"
      ],
      "methodology": [
        "Design",
        "Qualitative Study",
        "Policy Analysis"
      ],
      "methodology_detail": "Designing data collectives and governance frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.00551v1",
    "title": "Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness",
    "year": 2024,
    "authors": [
      "Wenxuan Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates social bias and cultural bias in LLMs, addressing fairness issues related to social groups.",
      "inequality_type": [
        "racial",
        "cultural",
        "social bias"
      ],
      "other_detail": "Focuses on bias measurement frameworks in AI models",
      "affected_populations": [
        "social groups",
        "cultural communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias evaluation frameworks and testing methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.17399v1",
    "title": "How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face Recognition",
    "year": 2024,
    "authors": [
      "Pedro C. Neto",
      "Ivona Colakovic",
      "Sa≈°o Karakatiƒç",
      "Ana F. Sequeira"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in face recognition across ethnicities, directly related to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "fairness"
      ],
      "other_detail": "Focuses on bias mitigation in AI systems",
      "affected_populations": [
        "ethnic minorities",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Knowledge Distillation applied to face recognition models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.17285v1",
    "title": "Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution",
    "year": 2024,
    "authors": [
      "Yixin Wu",
      "Yun Shen",
      "Michael Backes",
      "Yang Zhang"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI-generated images, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias amplification and stereotype persistence in AI models",
      "affected_populations": [
        "Non-White groups",
        "Women"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating bias and safety improvements over model updates",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.16881v1",
    "title": "FineFACE: Fair Facial Attribute Classification Leveraging Fine-grained Features",
    "year": 2024,
    "authors": [
      "Ayesha Manzoor",
      "Ajita Rattani"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "Addresses demographic bias and fairness in facial attribute classification, impacting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation without demographic annotations",
      "affected_populations": [
        "women",
        "darker skin tones"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fine-grained analysis and cross-layer attention learning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.16563v1",
    "title": "MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition",
    "year": 2024,
    "authors": [
      "Eduarda Caldeira",
      "Jaime S. Cardoso",
      "Ana F. Sequeira",
      "Pedro C. Neto"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition, highlighting ethnicity-specific features and bias reduction, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "ethnic",
        "educational"
      ],
      "other_detail": "Focus on ethnicity bias in AI systems",
      "affected_populations": [
        "ethnic minorities",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Knowledge distillation with specialized teachers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.16154v2",
    "title": "Does Data-Efficient Generalization Exacerbate Bias in Foundation Models?",
    "year": 2024,
    "authors": [
      "Dilermando Queiroz",
      "Anderson Carlos",
      "Ma√≠ra Fatoretto",
      "Luis Filipe Nakayama",
      "Andr√© Anjos",
      "Lilian Berton"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and fairness issues related to gender and age groups in AI models, highlighting potential fairness concerns in deploying foundation models with limited data.",
      "inequality_type": [
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias in medical AI models across social groups",
      "affected_populations": [
        "gender groups",
        "age groups",
        "medical patients"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluation of model bias across demographic groups",
      "geographic_focus": [
        "Brazil"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.16130v1",
    "title": "Using Backbone Foundation Model for Evaluating Fairness in Chest Radiography Without Demographic Data",
    "year": 2024,
    "authors": [
      "Dilermando Queiroz",
      "Andr√© Anjos",
      "Lilian Berton"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in medical AI, focusing on gender and age disparities, which are social attributes related to inequality.",
      "inequality_type": [
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focuses on fairness in medical diagnostics",
      "affected_populations": [
        "patients",
        "medical groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using foundation model embeddings for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.15696v4",
    "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI technologies: an investigation of Baidu, Ernie and Qwen",
    "year": 2024,
    "authors": [
      "Geng Liu",
      "Carlo Alberto Bono",
      "Francesco Pierri"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases, stereotypes, and negativity in Chinese AI tools, highlighting issues of fairness and inclusivity affecting social perceptions.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on social biases in Chinese-language AI systems",
      "affected_populations": [
        "Chinese social groups",
        "minority groups"
      ],
      "methodology": [
        "Data Collection",
        "Prompting AI models",
        "Bias Analysis"
      ],
      "methodology_detail": "Analyzes social biases via prompts and content analysis",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.15618v2",
    "title": "Super-intelligent society for the silver segment: Ethics in design",
    "year": 2024,
    "authors": [
      "Jaana Leikas",
      "Rebekah Rousi",
      "Hannu Vilpponen",
      "Pertti Saariluoma"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses digital inclusion of older adults, highlighting disparities in technology access and usability, which relate to age-related social inequality.",
      "inequality_type": [
        "age",
        "digital"
      ],
      "other_detail": "Focus on digital divide among older adults",
      "affected_populations": [
        "older adults"
      ],
      "methodology": [
        "Design Thinking",
        "Ethics Analysis"
      ],
      "methodology_detail": "Emphasizes ethical design and impact assessment",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.15550v2",
    "title": "Trustworthy and Responsible AI for Human-Centric Autonomous Decision-Making Systems",
    "year": 2024,
    "authors": [
      "Farzaneh Dehghani",
      "Mahsa Dibaji",
      "Fahim Anzum",
      "Lily Dey",
      "Alican Basdemir",
      "Sayeh Bayat",
      "Jean-Christophe Boucher",
      "Steve Drew",
      "Sarah Elaine Eaton",
      "Richard Frayne",
      "Gouri Ginde",
      "Ashley Harris",
      "Yani Ioannou",
      "Catherine Lebel",
      "John Lysack",
      "Leslie Salgado Arzuaga",
      "Emma Stanley",
      "Roberto Souza",
      "Ronnie de Souza Santos",
      "Lana Wells",
      "Tyler Williamson",
      "Matthias Wilms",
      "Zaman Wahid",
      "Mark Ungrin",
      "Marina Gavrilova",
      "Mariana Bento"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases in AI that perpetuate inequalities and unequal access, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focus on bias mitigation and fairness in AI systems",
      "affected_populations": [
        "disadvantaged groups",
        "minority communities",
        "vulnerable populations"
      ],
      "methodology": [
        "Literature Review",
        "Bias Detection Methods",
        "Metrics Evaluation"
      ],
      "methodology_detail": "Review of bias detection and mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.15398v1",
    "title": "Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome Dataset",
    "year": 2024,
    "authors": [
      "Diego Dimer Rodrigues"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates bias related to protected attributes across regions, focusing on fairness and potential harm, which are core concerns of social inequality.",
      "inequality_type": [
        "health",
        "geographic",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in health-related datasets across regions in Brazil",
      "affected_populations": [
        "patients",
        "regional communities"
      ],
      "methodology": [
        "Visualization",
        "Machine Learning",
        "Bias Metrics Analysis"
      ],
      "methodology_detail": "Pre-training bias metrics and regional model comparison",
      "geographic_focus": [
        "Brazil"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.16088v1",
    "title": "Ensuring Equitable Financial Decisions: Leveraging Counterfactual Fairness and Deep Learning for Bias",
    "year": 2024,
    "authors": [
      "Saish Shinde"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in financial decision-making, a social inequality issue.",
      "inequality_type": [
        "gender",
        "economic"
      ],
      "other_detail": "focus on gender bias in finance",
      "affected_populations": [
        "women",
        "financial applicants"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Bias Mitigation Techniques",
        "Data Augmentation"
      ],
      "methodology_detail": "integrated bias mitigation approaches in ML models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.14842v1",
    "title": "From Bias to Balance: Detecting Facial Expression Recognition Biases in Large Multimodal Foundation Models",
    "year": 2024,
    "authors": [
      "Kaylee Chhua",
      "Zhoujinyi Wen",
      "Vedant Hathalia",
      "Kevin Zhu",
      "Sean O'Brien"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial biases in facial expression recognition, highlighting social discrimination issues. It assesses how AI systems perform differently across racial groups, addressing fairness and bias concerns. This directly relates to social inequality related to race.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial bias in AI systems",
      "affected_populations": [
        "Black Females",
        "White Females",
        "individuals with darker skin tones"
      ],
      "methodology": [
        "Benchmarking",
        "Performance Evaluation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Assessing model accuracy across demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.14435v1",
    "title": "Social perception of faces in a vision-language model",
    "year": 2024,
    "authors": [
      "Carina I. Hausladen",
      "Manuel Knott",
      "Colin F. Camerer",
      "Pietro Perona"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in a vision-language model related to protected attributes such as race, gender, and age, highlighting social perception biases and potential discrimination. It discusses how these biases impact social judgments, indicating relevance to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Biases in AI social perception",
      "affected_populations": [
        "Black women",
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Manipulation of face attributes and embedding similarity analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.14262v1",
    "title": "Self-supervised Speech Representations Still Struggle with African American Vernacular English",
    "year": 2024,
    "authors": [
      "Kalvin Chang",
      "Yi-Hui Chou",
      "Jiatong Shi",
      "Hsuan-Ming Chen",
      "Nicole Holliday",
      "Odette Scharenborg",
      "David R. Mortensen"
    ],
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in speech recognition performance across racial language varieties, highlighting bias against African American Vernacular English, which relates to racial and linguistic inequality.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias in AI systems affecting marginalized linguistic groups",
      "affected_populations": [
        "AAVE speakers",
        "marginalized communities"
      ],
      "methodology": [
        "Experiment",
        "Evaluation"
      ],
      "methodology_detail": "Assessing ASR performance across language varieties",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.14159v1",
    "title": "\"Hi. I'm Molly, Your Virtual Interviewer!\" -- Exploring the Impact of Race and Gender in AI-powered Virtual Interview Experiences",
    "year": 2024,
    "authors": [
      "Shreyan Biswas",
      "Ji-Youn Jung",
      "Abhishek Unnam",
      "Kuldeep Yadav",
      "Shreyansh Gupta",
      "Ujwal Gadiraju"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how demographic attributes of virtual interviewers influence perceptions of fairness, privacy, and impression management, highlighting social biases related to race and gender in AI-mediated processes.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on perceptions influenced by demographic features",
      "affected_populations": [
        "job candidates",
        "applicants"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Manipulation of interviewer demographics and perception measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.06708v1",
    "title": "Ensuring Fairness with Transparent Auditing of Quantitative Bias in AI Systems",
    "year": 2024,
    "authors": [
      "Chih-Cheng Rex Yuan",
      "Bow-Yaw Wang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI systems that impact fairness across social groups, specifically mentioning racial bias in the justice system.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on fairness and bias in AI decision-making",
      "affected_populations": [
        "racial minorities",
        "justice system users"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Transparency and bias measurement tools",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.13295v2",
    "title": "Exploring Bias and Prediction Metrics to Characterise the Fairness of Machine Learning for Equity-Centered Public Health Decision-Making: A Narrative Review",
    "year": 2024,
    "authors": [
      "Shaina Raza",
      "Arash Shaban-Nejad",
      "Elham Dolatabadi",
      "Hiroshi Mamiya"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias and fairness in machine learning applied to public health, focusing on equity and systematic errors affecting populations. It aims to evaluate biases from an equity perspective, which relates to social inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on fairness in public health ML applications",
      "affected_populations": [
        "public health populations",
        "underserved groups"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Review of bias types and metrics in ML",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.12875v2",
    "title": "Disentangling, Amplifying, and Debiasing: Learning Disentangled Representations for Fair Graph Neural Networks",
    "year": 2024,
    "authors": [
      "Yeon-Chang Lee",
      "Hojung Shin",
      "Sang-Wook Kim"
    ],
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI systems, focusing on biases related to attributes and structure, which are linked to social inequalities such as discrimination and unequal treatment.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Bias mitigation in graph neural networks",
      "affected_populations": [
        "social groups",
        "health patients",
        "educational users"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in GNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.12734v1",
    "title": "Towards measuring fairness in speech recognition: Fair-Speech dataset",
    "year": 2024,
    "authors": [
      "Irina-Elena Veliche",
      "Zhuangqun Huang",
      "Vineeth Ayyat Kochaniyan",
      "Fuchun Peng",
      "Ozlem Kalinli",
      "Michael L. Seltzer"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SD",
      "eess.AS",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in speech recognition across demographic groups, addressing social bias and inequality in AI systems.",
      "inequality_type": [
        "race",
        "gender",
        "age",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic fairness in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups",
        "geographic populations"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates ASR performance across demographics",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.12494v2",
    "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Kunsheng Tang",
      "Wenbo Zhou",
      "Jie Zhang",
      "Aishan Liu",
      "Gelei Deng",
      "Shuai Li",
      "Peigui Qi",
      "Weiming Zhang",
      "Tianwei Zhang",
      "Nenghai Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in large language models, addressing social discrimination and fairness issues related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "transgender",
        "non-binary",
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias assessment and debiasing techniques in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.03781v1",
    "title": "It's Not You, It's Me: The Impact of Choice Models and Ranking Strategies on Gender Imbalance in Music Recommendation",
    "year": 2024,
    "authors": [
      "Andres Ferraro",
      "Michael D. Ekstrand",
      "Christine Bauer"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender imbalance in music recommendation, addressing gender fairness and biases, which are social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness in AI systems",
      "affected_populations": [
        "female artists",
        "music consumers"
      ],
      "methodology": [
        "Simulation",
        "Re-ranking strategies analysis",
        "User interaction modeling"
      ],
      "methodology_detail": "Simulates feedback and retraining effects on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.12055v1",
    "title": "Aligning (Medical) LLMs for (Counterfactual) Fairness",
    "year": 2024,
    "authors": [
      "Raphael Poulain",
      "Hamed Fayyaz",
      "Rahmatollah Beheshti"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in medical LLMs that can lead to unfair treatment across social groups, impacting health disparities and trust in AI systems.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in medical AI systems",
      "affected_populations": [
        "patients",
        "medical subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Algorithm Auditing",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Bias analysis and mitigation techniques in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.11667v1",
    "title": "The Problems with Proxies: Making Data Work Visible through Requester Practices",
    "year": 2024,
    "authors": [
      "Annabel Rothschild",
      "Ding Wang",
      "Niveditha Jayakumar Vilvanathan",
      "Lauren Wilcox",
      "Carl DiSalvo",
      "Betsy DiSalvo"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses the exclusion and undervaluation of data workers, highlighting ethical and fairness issues in AI development, which relate to social inequality and bias.",
      "inequality_type": [
        "informational",
        "digital",
        "ethics"
      ],
      "other_detail": "Focuses on data workers' recognition and treatment",
      "affected_populations": [
        "data annotators",
        "AI data workers"
      ],
      "methodology": [
        "Qualitative Study",
        "Case Study"
      ],
      "methodology_detail": "Analysis of platform hiring and engagement practices",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.11448v1",
    "title": "Lookism: The overlooked bias in computer vision",
    "year": 2024,
    "authors": [
      "Aditya Gulati",
      "Bruno Lepri",
      "Nuria Oliver"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "I.2.0; I.4.0; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases related to physical appearance, which impacts social fairness and perpetuates stereotypes, addressing social discrimination and inequality.",
      "inequality_type": [
        "appearance",
        "social bias"
      ],
      "other_detail": "focus on lookism and societal stereotypes",
      "affected_populations": [
        "individuals with diverse appearances"
      ],
      "methodology": [
        "Literature Review",
        "User Study"
      ],
      "methodology_detail": "review and examples illustrating lookism bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.11441v1",
    "title": "Epistemic Injustice in Generative AI",
    "year": 2024,
    "authors": [
      "Jackie Kay",
      "Atoosa Kasirzadeh",
      "Shakir Mohamed"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses epistemic injustices affecting marginalized groups and social equity in knowledge dissemination, highlighting inequalities in information access and representation.",
      "inequality_type": [
        "informational",
        "linguistic",
        "digital",
        "educational"
      ],
      "other_detail": "Focuses on epistemic and representational inequalities",
      "affected_populations": [
        "multilingual users",
        "minority groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Case Study"
      ],
      "methodology_detail": "Philosophical and real-world examples",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.11358v1",
    "title": "Gender Bias Evaluation in Text-to-image Generation: A Survey",
    "year": 2024,
    "authors": [
      "Yankun Wu",
      "Yuta Nakashima",
      "Noa Garcia"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in AI-generated images, addressing gender discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Bias Evaluation"
      ],
      "methodology_detail": "Review of recent bias evaluation studies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.11247v2",
    "title": "Unboxing Occupational Bias: Grounded Debiasing of LLMs with U.S. Labor Data",
    "year": 2024,
    "authors": [
      "Atmika Gorti",
      "Manas Gaur",
      "Aman Chadha"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in LLMs related to societal categories like gender and occupation, which can reinforce social inequalities. It analyzes how AI models perpetuate stereotypes and biases affecting marginalized groups. The focus on bias mitigation aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "occupational",
        "social bias"
      ],
      "other_detail": "Bias in AI models related to societal stereotypes",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Empirical Analysis",
        "Debiasing Technique"
      ],
      "methodology_detail": "Comparison with U.S. labor data, bias mitigation",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.10391v2",
    "title": "Tax Credits and Household Behavior: The Roles of Myopic Decision-Making and Liquidity in a Simulated Economy",
    "year": 2024,
    "authors": [
      "Kshama Dwarakanath",
      "Jialin Dong",
      "Svitlana Vyetrenko"
    ],
    "categories": [
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on tax credit strategies to reduce household inequality and improve social welfare, addressing socioeconomic disparities.",
      "inequality_type": [
        "income",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Targeting household economic disparities through policy simulation",
      "affected_populations": [
        "households",
        "low-income groups"
      ],
      "methodology": [
        "Experiment",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Multi-agent reinforcement learning simulation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.10175v1",
    "title": "Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition",
    "year": 2024,
    "authors": [
      "Rafael M. Mamede",
      "Pedro C. Neto",
      "Ana F. Sequeira"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases in facial recognition, highlighting racial disparities and fairness issues affecting different social groups.",
      "inequality_type": [
        "racial",
        "demographic"
      ],
      "other_detail": "Focuses on racial bias in AI systems",
      "affected_populations": [
        "African individuals"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis",
        "Model Evaluation"
      ],
      "methodology_detail": "Assessing bias via occlusion experiments and fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.09489v1",
    "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
    "year": 2024,
    "authors": [
      "Rameez Qureshi",
      "Na√Øm Es-Sebbani",
      "Luis Gal√°rraga",
      "Yvette Graham",
      "Miguel Couceiro",
      "Zied Bouraoui"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender, ethnicity, religion, and nationality in language models, which are social groupings associated with inequality and discrimination.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "nationality"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "ethnic minorities",
        "religious groups",
        "nationalities"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias reduction without fine-tuning or annotations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2409.12246v1",
    "title": "An Outline for a Jupyter-Materials-Based Repository Website Focused on the Computational Sciences",
    "year": 2024,
    "authors": [
      "Peter Berg",
      "Zachary Kelly"
    ],
    "categories": [
      "cs.CY",
      "cs.DL",
      "97D50",
      "K.3.1"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in access to educational resources due to internet and device limitations, impacting marginalized and remote communities.",
      "inequality_type": [
        "educational",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on access disparities in remote and underdeveloped areas",
      "affected_populations": [
        "remote communities",
        "students in underdeveloped regions"
      ],
      "methodology": [
        "System Design",
        "Feasibility Study"
      ],
      "methodology_detail": "Developing and evaluating a low-cost repository platform",
      "geographic_focus": [
        "Canada"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.08477v1",
    "title": "Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges",
    "year": 2024,
    "authors": [
      "Ishana Shastri",
      "Shomik Jain",
      "Barbara Engelhardt",
      "Ashia Wilson"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses transparency in judicial processes related to bias and fairness, which are connected to social inequalities such as racial bias and discrimination. It addresses how AI can impact social justice issues. The focus on bias detection and fairness in legal contexts indicates a concern with social inequality.",
      "inequality_type": [
        "racial",
        "social",
        "educational"
      ],
      "other_detail": "focus on bias and fairness in legal systems",
      "affected_populations": [
        "racial minorities",
        "defendants",
        "jurors"
      ],
      "methodology": [
        "Natural Language Processing",
        "System Design"
      ],
      "methodology_detail": "Automating analysis of legal documents and transparency mechanisms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.08471v2",
    "title": "Fairness Issues and Mitigations in (Differentially Private) Socio-Demographic Data Processes",
    "year": 2024,
    "authors": [
      "Joonhyuk Ko",
      "Juba Ziani",
      "Saswat Das",
      "Matt Williams",
      "Ferdinando Fioretto"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness issues in socio-demographic data collection, which impacts social groups. It examines how sampling errors and privacy methods influence group-level estimates, affecting fairness. These aspects relate directly to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focus on fairness in socio-demographic data processes",
      "affected_populations": [
        "social groups",
        "minority populations"
      ],
      "methodology": [
        "Statistical Analysis",
        "Optimization",
        "Empirical Validation"
      ],
      "methodology_detail": "Analyzes survey design and privacy impacts on fairness",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.06569v1",
    "title": "Social Debiasing for Fair Multi-modal LLMs",
    "year": 2024,
    "authors": [
      "Harry Cheng",
      "Yangyang Guo",
      "Qingpei Guo",
      "Ming Yang",
      "Tian Gan",
      "Liqiang Nie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases related to race and gender in AI models, which are key aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on social bias mitigation in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Debiasing Strategy",
        "Experiment"
      ],
      "methodology_detail": "Develops datasets and debiasing techniques for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.05895v1",
    "title": "Gender of Recruiter Makes a Difference: A study into Cybersecurity Graduate Recruitment",
    "year": 2024,
    "authors": [
      "Joanne L. Hall",
      "Asha Rao"
    ],
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in cybersecurity recruitment, highlighting gender bias and underrepresentation, which are social inequalities. It discusses how gender influences hiring preferences and workforce diversity, directly relating to gender-based social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "female cybersecurity professionals",
        "male recruiters"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed survey data on recruitment preferences",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.05497v2",
    "title": "MABR: A Multilayer Adversarial Bias Removal Approach Without Prior Bias Knowledge",
    "year": 2024,
    "authors": [
      "Maxwell J. Yin",
      "Boyu Wang",
      "Charles Ling"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases related to race and gender in AI models, which are key aspects of social inequality. It focuses on mitigating biases that affect social groups without prior demographic knowledge, directly engaging with social fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation without demographic annotations",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training",
        "Experiment"
      ],
      "methodology_detail": "Bias removal in AI models without prior bias knowledge",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.04026v1",
    "title": "Multimodal Gender Fairness in Depression Prediction: Insights on Data from the USA & China",
    "year": 2024,
    "authors": [
      "Joseph Cameron",
      "Jiaee Cheong",
      "Micol Spitale",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in ML algorithms related to gender and cultural differences in depression detection, addressing social discrimination and inequality in health and gender domains.",
      "inequality_type": [
        "gender",
        "health",
        "cultural"
      ],
      "other_detail": "Focus on cross-cultural gender fairness in mental health AI",
      "affected_populations": [
        "women",
        "men",
        "US residents",
        "Chinese residents"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Multiple algorithms and datasets from USA and China",
      "geographic_focus": [
        "USA",
        "China"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.04671v1",
    "title": "Prompt and Prejudice",
    "year": 2024,
    "authors": [
      "Lorenzo Berlincioni",
      "Luca Cultrera",
      "Federico Becattini",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases related to gender and ethnicity in AI models, highlighting social biases and fairness issues affecting different social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Biases in AI decision-making scenarios",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and benchmarking in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.02558v4",
    "title": "Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing",
    "year": 2024,
    "authors": [
      "Shiqi Fang",
      "Zexun Chen",
      "Jake Ansell"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "q-fin.CP",
      "q-fin.ST",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic fairness impacting social groups, highlighting discrimination issues.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on fairness in financial decision-making",
      "affected_populations": [
        "micro-firms",
        "discriminated groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness framework and empirical evaluation",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.02462v1",
    "title": "An investigation into the causes of race bias in AI-based cine CMR segmentation",
    "year": 2024,
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Anton",
      "Bram Ruijsink",
      "Sebastien Roujol",
      "Theodore Barfoot",
      "Shaheim Ogbomo-Harmitt",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates race bias in AI systems, highlighting social discrimination issues. It analyzes how AI performance varies across racial groups, indicating social inequality concerns.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on race bias in medical imaging AI",
      "affected_populations": [
        "Black",
        "White"
      ],
      "methodology": [
        "Experiment",
        "AI interpretability methods",
        "Classification",
        "Segmentation"
      ],
      "methodology_detail": "Analyzes bias sources and interpretability in AI models",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.01959v2",
    "title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI",
    "year": 2024,
    "authors": [
      "Robert Wolfe",
      "Aayushi Dangol",
      "Alexis Hiniker",
      "Bill Howe"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models that reflect societal stereotypes related to race, trustworthiness, and sexuality, indicating a focus on social discrimination and bias. It discusses how dataset size influences the reproduction of social biases, linking AI bias to cultural and societal factors. The analysis of racial biases and societal impression formation demonstrates relevance to social inequality issues.",
      "inequality_type": [
        "racial",
        "social",
        "cultural"
      ],
      "other_detail": "Bias reflection in AI models and datasets",
      "affected_populations": [
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Hierarchical Clustering",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing bias patterns and dataset effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.01590v1",
    "title": "Interpretations, Representations, and Stereotypes of Caste within Text-to-Image Generators",
    "year": 2024,
    "authors": [
      "Sourojit Ghosh"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines representation and stereotypes of caste, a social inequality dimension, in AI-generated images, highlighting systemic marginalization and bias.",
      "inequality_type": [
        "class",
        "ethnic",
        "social"
      ],
      "other_detail": "Focus on caste-based social discrimination in AI outputs",
      "affected_populations": [
        "Dalits",
        "caste-oppressed groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "CLIP similarity comparisons of generated images",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.01285v1",
    "title": "The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language Models",
    "year": 2024,
    "authors": [
      "Hannah Chen",
      "Yangfeng Ji",
      "David Evans"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in AI models that impact allocation decisions, which can influence social disparities related to resource distribution. It discusses how biases in LLM predictions can lead to unequal outcomes. The focus on allocation harms suggests relevance to social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on allocation harms and social disparities",
      "affected_populations": [
        "resource-deprived groups",
        "marginalized communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Develops bias index and compares metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.00992v3",
    "title": "Fairness in Large Language Models in Three Hours",
    "year": 2024,
    "authors": [
      "Thang Doan Viet",
      "Zichong Wang",
      "Minh Nhat Nguyen",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and bias in Large Language Models, addressing social discrimination and bias issues affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "marginalized groups",
        "social minorities"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "System Design"
      ],
      "methodology_detail": "Overview of recent fairness strategies and tools",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.00905v1",
    "title": "High-Impact Innovations and Hidden Gender Disparities in Inventor-Evaluator Networks",
    "year": 2024,
    "authors": [
      "Tara Sowrirajan",
      "Ryan Whalen",
      "Brian Uzzi"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in innovation and evaluation processes, highlighting institutional biases affecting women. It discusses gender discrimination and structural barriers impacting women's scientific contributions, directly addressing social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women inventors",
        "female examiners"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes patent data and institutional practices",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.21467v2",
    "title": "Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data",
    "year": 2024,
    "authors": [
      "Mengtian Kang",
      "Yansong Hu",
      "Shuo Gao",
      "Yuanyuan Liu",
      "Hongbei Meng",
      "Xuemeng Li",
      "Xuhang Chen",
      "Hubin Zhao",
      "Jing Fu",
      "Guohua Hu",
      "Wei Wang",
      "Yanning Dai",
      "Arokia Nathan",
      "Peter Smielewski",
      "Ningli Wang",
      "Shiming Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper emphasizes reducing healthcare costs and disparities through accessible AI-based prediction, aiming to mitigate medical inequities caused by economic disparities.",
      "inequality_type": [
        "economic",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on healthcare cost reduction and access",
      "affected_populations": [
        "children",
        "low-income families"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Longitudinal study with AI prediction models",
      "geographic_focus": [
        "Henan, China"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.00025v3",
    "title": "Need of AI in Modern Education: in the Eyes of Explainable AI (xAI)",
    "year": 2024,
    "authors": [
      "Supriya Manna",
      "Niladri Sett"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses how AI decisions are influenced by parental income and highlights biases affecting access and fairness in education, directly addressing socioeconomic inequality.",
      "inequality_type": [
        "income",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on income-related disparities in educational AI applications",
      "affected_populations": [
        "students",
        "families"
      ],
      "methodology": [
        "Explainable AI",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing biases and decision explanations in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.21209v3",
    "title": "Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool",
    "year": 2024,
    "authors": [
      "Lingwei Cheng",
      "Cameron Drayton",
      "Alexandra Chouldechova",
      "Rhema Vaithianathan"
    ],
    "categories": [
      "cs.HC",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in housing allocation influenced by algorithmic tools, addressing racial inequality and bias in decision-making systems.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial disparities in housing services",
      "affected_populations": [
        "Black clients",
        "White clients"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Descriptive Analysis"
      ],
      "methodology_detail": "Analyzes deployment data and score distributions",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.21001v3",
    "title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models",
    "year": 2024,
    "authors": [
      "Ali Abdollahi",
      "Mahdi Ghaznavi",
      "Mohammad Reza Karimi Nejad",
      "Arash Mari Oriyad",
      "Reza Abbasi",
      "Ali Salesi",
      "Melika Behjati",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in vision-language models, highlighting societal stereotypes and their impact on AI predictions, which relates directly to gender inequality and bias in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in AI reflecting societal gender stereotypes",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Testing models on gender-activity bias dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.02676v2",
    "title": "On Biases in a UK Biobank-based Retinal Image Classification Model",
    "year": 2024,
    "authors": [
      "Anissa Alloula",
      "Rima Mustafa",
      "Daniel R McGowan",
      "Bart≈Çomiej W. Papie≈º"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in AI model performance across population groups and assessment centers, highlighting fairness issues related to social biases in healthcare data.",
      "inequality_type": [
        "health",
        "geographic",
        "social bias"
      ],
      "other_detail": "Focus on fairness disparities in medical AI models",
      "affected_populations": [
        "healthcare patients",
        "population groups"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Analysis",
        "Bias Mitigation"
      ],
      "methodology_detail": "Evaluates bias mitigation methods on model disparities",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.20522v1",
    "title": "Evaluating Fairness in Black-box Algorithmic Markets: A Case Study of Ride Sharing in Chicago",
    "year": 2024,
    "authors": [
      "Yuhan Liu",
      "Yuhan Zheng",
      "Siyuan Zhang",
      "Lydia T. Liu"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in wages and fares based on race, ethnicity, health, and working hours, highlighting social inequalities. It discusses fairness issues in algorithmic pricing and wage determination, which are social bias concerns.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focuses on social disparities in algorithmic markets",
      "affected_populations": [
        "drivers",
        "riders"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Replicates proprietary algorithms and statistical hypothesis testing",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.20371v2",
    "title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
    "year": 2024,
    "authors": [
      "Kyra Wilson",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race and gender in AI hiring tools, highlighting social discrimination and inequality. It analyzes how AI systems may perpetuate or amplify social biases affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI employment screening",
      "affected_populations": [
        "racial minorities",
        "women",
        "Black males"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias audit using resume datasets and language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.19670v1",
    "title": "Overview of PerpectiveArg2024: The First Shared Task on Perspective Argument Retrieval",
    "year": 2024,
    "authors": [
      "Neele Falk",
      "Andreas Waldis",
      "Iryna Gurevych"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses perspectives related to demographic and socio-cultural variables, highlighting biases and challenges in argument retrieval systems that affect minority and majority groups, indicating a focus on social bias and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "minority groups",
        "majority groups",
        "female gender"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Multilingual dataset and retrieval system evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.19655v1",
    "title": "AI-Driven Healthcare: A Survey on Ensuring Fairness and Mitigating Bias",
    "year": 2024,
    "authors": [
      "Sribala Vidyadhari Chinta",
      "Zichong Wang",
      "Xingyu Zhang",
      "Thang Doan Viet",
      "Ayesha Kashif",
      "Monique Antoinette Smith",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI affecting healthcare disparities across demographic groups, addressing social fairness issues.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on healthcare disparities and algorithmic bias",
      "affected_populations": [
        "demographic groups",
        "patients"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Review of bias mitigation strategies in AI healthcare",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.19524v3",
    "title": "VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary",
    "year": 2024,
    "authors": [
      "Hanjun Luo",
      "Ziye Deng",
      "Haoyu Huang",
      "Xuecheng Liu",
      "Ruizhe Chen",
      "Zuozhu Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI-generated images related to gender, race, and age, which are social groups affected by inequality. It focuses on fairness and ethical concerns in AI systems that impact social perceptions and discrimination. The emphasis on debiasing for social groups indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias mitigation in AI-generated imagery",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Zero-shot debiasing via prompt engineering",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.19114v2",
    "title": "To which reference class do you belong? Measuring racial fairness of reference classes with normative modeling",
    "year": 2024,
    "authors": [
      "Saige Rutherford",
      "Thomas Wolfers",
      "Charlotte Fraza",
      "Nathaniel G. Harnett",
      "Christian F. Beckmann",
      "Henricus G. Ruhe",
      "Andre F. Marquand"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias and fairness in clinical reference models, highlighting racial disparities and demographic mismatches, which relate directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focuses on racial fairness in medical normative modeling",
      "affected_populations": [
        "racial groups",
        "patients"
      ],
      "methodology": [
        "Normative Modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates bias using deviation scores and normative models",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.18786v1",
    "title": "The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs",
    "year": 2024,
    "authors": [
      "Aleix Sant",
      "Carlos Escolano",
      "Audrey Mash",
      "Francesca De Luca Fornaciari",
      "Maite Melero"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI translation systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Prompt Engineering",
        "Benchmarking"
      ],
      "methodology_detail": "Testing bias reduction techniques in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.18783v2",
    "title": "Science for whom? The influence of the regional academic circuit on gender inequalities in Latin America",
    "year": 2024,
    "authors": [
      "Carolina Pradier",
      "Diego Kozlowski",
      "Natsumi S. Shokida",
      "Vincent Larivi√®re"
    ],
    "categories": [
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in scientific recognition and participation, highlighting gender inequality in Latin American academia.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender disparities in scientific communication",
      "affected_populations": [
        "women scientists"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of scientific publications and circuits",
      "geographic_focus": [
        "Latin America"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.18745v1",
    "title": "FairAIED: Navigating Fairness, Bias, and Ethics in Educational AI Applications",
    "year": 2024,
    "authors": [
      "Sribala Vidyadhari Chinta",
      "Zichong Wang",
      "Zhipeng Yin",
      "Nhat Hoang",
      "Matthew Gonzalez",
      "Tai Le Quy",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases and fairness issues in educational AI, which relate to social discrimination and inequality across demographics.",
      "inequality_type": [
        "educational",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "students",
        "educational minorities"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "Ethics Analysis"
      ],
      "methodology_detail": "Evaluates fairness techniques and ethical frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.18524v1",
    "title": "She Works, He Works: A Curious Exploration of Gender Bias in AI-Generated Imagery",
    "year": 2024,
    "authors": [
      "Amalia Foka"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "I.2.0; J.5"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in AI-generated imagery, highlighting societal gender stereotypes and disparities. It discusses how AI models reflect and perpetuate these biases, impacting cultural perceptions of gender.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "female figures",
        "male figures"
      ],
      "methodology": [
        "Qualitative Study",
        "Literature Review"
      ],
      "methodology_detail": "Theoretical analysis based on visual culture theories",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.18376v1",
    "title": "Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives",
    "year": 2024,
    "authors": [
      "Azmine Toushik Wasi",
      "Raima Islam",
      "Mst Rafia Islam",
      "Taki Hasan Rafi",
      "Dong-Kyu Chae"
    ],
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.CY",
      "cs.MM",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to religious dialects, impacting social perceptions and fairness, which are social inequality issues.",
      "inequality_type": [
        "religion",
        "social bias",
        "fairness"
      ],
      "other_detail": "Focus on religious dialect biases in language models",
      "affected_populations": [
        "Bengali speakers",
        "Muslim community",
        "Hindu community"
      ],
      "methodology": [
        "Experiment",
        "AI bias analysis",
        "Comparative analysis"
      ],
      "methodology_detail": "Bias detection across language models and dialects",
      "geographic_focus": [
        "Bangladesh",
        "West Bengal"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.17677v1",
    "title": "Women's Participation in Computing: Evolving Research Methods",
    "year": 2024,
    "authors": [
      "Thomas J. Misa"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines women's participation in computing, highlighting gender disparities and growth over time, which relates directly to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in computing history",
      "affected_populations": [
        "women in computing"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes authorship data and research methods",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.17459v1",
    "title": "Hidden or Inferred: Fair Learning-To-Rank with Unknown Demographics",
    "year": 2024,
    "authors": [
      "Oluseun Olulana",
      "Kathleen Cachel",
      "Fabricio Murai",
      "Elke Rundensteiner"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI models related to sensitive demographic features such as race and sex, addressing social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in ranking algorithms without demographic data",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Empirical Investigation"
      ],
      "methodology_detail": "Modeling inference errors and case studies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.17441v2",
    "title": "Gender disparities in the dissemination and acquisition of scientific knowledge",
    "year": 2024,
    "authors": [
      "Chiara Zappal√†",
      "Luca Gallo",
      "Jan Bachmann",
      "Federico Battiston",
      "Fariba Karimi"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in scientific knowledge dissemination, highlighting structural disadvantages faced by women in academia.",
      "inequality_type": [
        "gender",
        "educational",
        "informational"
      ],
      "other_detail": "Focuses on gender disparities in scientific collaboration networks",
      "affected_populations": [
        "women scientists",
        "male scientists"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Network Analysis"
      ],
      "methodology_detail": "Analyzes collaboration networks and knowledge diffusion over time",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.17543v2",
    "title": "Dataset Distribution Impacts Model Fairness: Single vs. Multi-Task Learning",
    "year": 2024,
    "authors": [
      "Ralf Raumanns",
      "Gerard Schouten",
      "Josien P. W. Pluim",
      "Veronika Cheplygina"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to patient sex in medical datasets and their impact on model fairness, addressing gender bias in AI systems.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focuses on gender bias in medical AI models",
      "affected_populations": [
        "female patients",
        "male patients"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Uses ResNet CNNs and dataset generation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.16953v1",
    "title": "Open Challenges on Fairness of Artificial Intelligence in Medical Imaging Applications",
    "year": 2024,
    "authors": [
      "Enzo Ferrante",
      "Rodrigo Echeveste"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness issues in AI systems affecting medical imaging, which relate to social bias and disparities in healthcare. It addresses biases impacting different populations and fairness metrics, indicating a focus on social inequalities.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in medical AI systems",
      "affected_populations": [
        "patients",
        "underserved groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing bias and fairness metrics in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.15833v1",
    "title": "Inequalities in Computational Thinking Among Incoming Students in an STEM Chilean University",
    "year": 2024,
    "authors": [
      "Felipe Gonz√°lez-Pizarro",
      "Claudia L√≥pez",
      "Andrea V√°squez",
      "Carlos Castro"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "97P20",
      "K.3.0; K.4.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in computational thinking skills based on gender, school type, and prior programming knowledge, linking these to socio-economic and educational inequalities in Chile.",
      "inequality_type": [
        "educational",
        "socioeconomic",
        "gender"
      ],
      "other_detail": "Focuses on educational disparities in STEM entry skills",
      "affected_populations": [
        "students",
        "incoming university students"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Analyzed responses from over 500 students",
      "geographic_focus": [
        "Chile"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.15810v2",
    "title": "Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems",
    "year": 2024,
    "authors": [
      "Siddharth D Jaiswal",
      "Animesh Ganai",
      "Abhisek Dash",
      "Saptarshi Ghosh",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in facial recognition systems across different social groups, highlighting disparities related to gender, geographic location, and demographic diversity, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in facial recognition systems",
      "affected_populations": [
        "Global South individuals",
        "Global North individuals",
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Benchmarking",
        "Experiment",
        "Bias Mitigation"
      ],
      "methodology_detail": "Develops diverse dataset and evaluates bias reduction techniques",
      "geographic_focus": [
        "Global South",
        "Global North"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.14367v3",
    "title": "Thinking Racial Bias in Fair Forgery Detection: Models, Datasets and Evaluations",
    "year": 2024,
    "authors": [
      "Decheng Liu",
      "Zongqi Wang",
      "Chunlei Peng",
      "Nannan Wang",
      "Ruimin Hu",
      "Xinbo Gao"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in AI models and datasets, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in forgery detection models",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Fairness Metrics"
      ],
      "methodology_detail": "Develops a balanced racial dataset and fairness evaluation metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.14180v1",
    "title": "Automatic Classification of News Subjects in Broadcast News: Application to a Gender Bias Representation Analysis",
    "year": 2024,
    "authors": [
      "Valentin Pelloin",
      "Lena Dodson",
      "√âmile Chapuis",
      "Nicolas Herv√©",
      "David Doukhan"
    ],
    "categories": [
      "cs.CL",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender representation biases in broadcast news topics, highlighting underrepresentation and overrepresentation of women in specific subjects, which relates to gender inequality and social bias.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in media representation",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Dataset Creation"
      ],
      "methodology_detail": "Using LLMs and classification models for bias analysis",
      "geographic_focus": [
        "France"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2408.01438v1",
    "title": "AI for All: Identifying AI incidents Related to Diversity and Inclusion",
    "year": 2024,
    "authors": [
      "Rifat Ara Shams",
      "Didar Zowghi",
      "Muneera Bano"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases and discrimination related to race, gender, and age in AI incidents, addressing social inequalities and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Biases in AI systems affecting social groups",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Manual analysis",
        "Decision tree development",
        "Focus group discussions"
      ],
      "methodology_detail": "Analyzing incident databases and validating decision tree",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.13982v1",
    "title": "Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance",
    "year": 2024,
    "authors": [
      "Changye Li",
      "Trevor Cohen",
      "Serguei Pakhomov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in ASR performance, highlighting fairness issues affecting African American English speakers. It discusses how recording practices and linguistic diversity impact technology fairness, which relates to racial inequality. The focus on disparities in AI performance across social groups indicates addressing social inequality.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Focus on racial and linguistic disparities in AI",
      "affected_populations": [
        "African American English speakers"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing ASR performance across datasets and conditions",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.13928v1",
    "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
    "year": 2024,
    "authors": [
      "Ahmed Allam"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating biases related to gender, racial, and religious groups in language models, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "religious"
      ],
      "other_detail": "Bias mitigation in AI-generated language",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Developing bias mitigation framework and dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.13896v1",
    "title": "Data-Algorithm-Architecture Co-Optimization for Fair Neural Networks on Skin Lesion Dataset",
    "year": 2024,
    "authors": [
      "Yi Sheng",
      "Junhuan Yang",
      "Jinyang Li",
      "James Alaina",
      "Xiaowei Xu",
      "Yiyu Shi",
      "Jingtong Hu",
      "Weiwen Jiang",
      "Lei Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in medical AI, focusing on biases affecting marginalized groups, which relates to social inequalities.",
      "inequality_type": [
        "health",
        "social bias"
      ],
      "other_detail": "Fairness in medical AI datasets and models",
      "affected_populations": [
        "marginalized communities",
        "underrepresented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Neural Architecture Search",
        "Experiment"
      ],
      "methodology_detail": "Optimizing neural network fairness and accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.13439v1",
    "title": "Reducing Barriers to the Use of Marginalised Music Genres in AI",
    "year": 2024,
    "authors": [
      "Nick Bryan-Kinns",
      "Zijin Li"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses barriers to marginalized music genres, emphasizing cultural representation and bias reduction, which relate to social inequality issues such as cultural bias and underrepresentation.",
      "inequality_type": [
        "cultural",
        "ethnic",
        "educational"
      ],
      "other_detail": "Focus on cultural and representation biases in AI",
      "affected_populations": [
        "marginalized cultural groups"
      ],
      "methodology": [
        "Research Project",
        "XAI analysis"
      ],
      "methodology_detail": "Exploration of AI transparency and bias reduction strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.12500v2",
    "title": "Automate or Assist? The Role of Computational Models in Identifying Gendered Discourse in US Capital Trial Transcripts",
    "year": 2024,
    "authors": [
      "Andrea W Wen-Yi",
      "Kathryn Adamson",
      "Nathalie Greenfield",
      "Rachel Goldberg",
      "Sandra Babcock",
      "David Mimno",
      "Allison Koenecke"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-biased language in US capital trials, addressing gender bias in legal discourse, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in legal settings",
      "affected_populations": [
        "women defendants"
      ],
      "methodology": [
        "Natural Language Processing",
        "Case Study",
        "Expert Annotation"
      ],
      "methodology_detail": "combines manual annotation with computational models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.11933v1",
    "title": "Fairly Accurate: Optimizing Accuracy Parity in Fair Target-Group Detection",
    "year": 2024,
    "authors": [
      "Soumyajit Gupta",
      "Venelin Kovatchev",
      "Maria De-Arteaga",
      "Matthew Lease"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI detection to mitigate bias across groups, addressing social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focuses on fairness in target detection algorithms",
      "affected_populations": [
        "demographic groups",
        "targeted communities"
      ],
      "methodology": [
        "Machine Learning",
        "Optimization",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Develops differentiable loss functions for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.11909v1",
    "title": "Cumulative Advantage of Brokerage in Academia",
    "year": 2024,
    "authors": [
      "Jan Bachmann",
      "Lisette Esp√≠n-Noboa",
      "Gerardo I√±iguez",
      "Fariba Karimi"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI",
      "physics.data-an",
      "62P25, 91D30",
      "J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how brokerage roles influence career success and inequality in academia, highlighting gender disparities and potential for reducing success gaps.",
      "inequality_type": [
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on career inequality in scientific collaboration",
      "affected_populations": [
        "women",
        "men",
        "early-career scientists"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes collaboration networks and career trajectories",
      "geographic_focus": [
        "worldwide"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.11624v1",
    "title": "Rethinking Fair Graph Neural Networks from Re-balancing",
    "year": 2024,
    "authors": [
      "Zhixun Li",
      "Yushun Dong",
      "Qiang Liu",
      "Jeffrey Xu Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems, focusing on demographic group disparities, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI decision-making systems",
      "affected_populations": [
        "demographic groups",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Fairness Analysis"
      ],
      "methodology_detail": "Re-balancing techniques and causal analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.20240v2",
    "title": "Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada",
    "year": 2024,
    "authors": [
      "Isar Nejadgholi",
      "Maryam Molamohammadi",
      "Samir Bakhtawar"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.1, I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses risks of AI tools affecting vulnerable immigrant groups, highlighting social and ethical concerns related to integration and potential inequalities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on immigrant integration and AI risks",
      "affected_populations": [
        "newcomers",
        "refugees"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing ethical and social implications of AI use",
      "geographic_focus": [
        "Canada"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.10241v2",
    "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs",
    "year": 2024,
    "authors": [
      "Zhiting Fan",
      "Ruizhe Chen",
      "Ruiling Xu",
      "Zuozhu Liu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting social bias in LLMs, which relates to social discrimination and fairness issues affecting groups based on race, gender, and other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on social bias detection in AI systems",
      "affected_populations": [
        "social groups",
        " marginalized communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection in open-text generation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.10104v1",
    "title": "A Self-Supervised Learning Pipeline for Demographically Fair Facial Attribute Classification",
    "year": 2024,
    "authors": [
      "Sreeraj Ramachandran",
      "Ajita Rattani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic bias and fairness in facial attribute classification, which relates to social discrimination based on race, gender, and other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "demographic"
      ],
      "other_detail": "Bias mitigation in facial attribute AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Contrastive Learning",
        "Data Curation"
      ],
      "methodology_detail": "Self-supervised learning with fairness-focused data curation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.08441v2",
    "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
    "year": 2024,
    "authors": [
      "Riccardo Cantini",
      "Giada Cosenza",
      "Alessio Orsino",
      "Domenico Talia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLM responses related to gender, ethnicity, and other social stereotypes, impacting fairness and inclusivity.",
      "inequality_type": [
        "gender",
        "ethnic",
        "disability",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on social biases in AI responses",
      "affected_populations": [
        "gender groups",
        "ethnic groups",
        "disabled individuals",
        "age groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias elicitation via jailbreak prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.08189v1",
    "title": "fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware Perturbations",
    "year": 2024,
    "authors": [
      "Jinfeng Li",
      "Yuefeng Chen",
      "Xiangyu Liu",
      "Longtao Huang",
      "Rong Zhang",
      "Hui Xue"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating biases related to gender and racial stereotypes in language models, addressing social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "fairness"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Generative Adversarial Network"
      ],
      "methodology_detail": "Bias removal via semantic and fairness-aware perturbations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.08102v1",
    "title": "Dynamics of Gender Bias within Computer Science",
    "year": 2024,
    "authors": [
      "Thomas J. Misa"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in research authorship within computer science, highlighting underrepresentation and growth patterns of women, which relates to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women researchers"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes authorship data over time",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.07350v1",
    "title": "Long-Term Fairness in Sequential Multi-Agent Selection with Positive Reinforcement",
    "year": 2024,
    "authors": [
      "Bhagyashree Puranik",
      "Ozgur Guldogan",
      "Upamanyu Madhow",
      "Ramtin Pedarsani"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in selection processes affecting under-represented groups, addressing social bias and inequality.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in applicant selection processes",
      "affected_populations": [
        "under-represented applicants"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Analysis"
      ],
      "methodology_detail": "Analyzes policies and simulates long-term effects",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.06957v1",
    "title": "Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech Integrated Large Language Models",
    "year": 2024,
    "authors": [
      "Yi-Cheng Lin",
      "Tzu-Quan Lin",
      "Chih-Kai Yang",
      "Ke-Han Lu",
      "Wei-Chih Chen",
      "Chun-Yi Kuan",
      "Hung-yi Lee"
    ],
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in speech-based AI models, highlighting fairness issues affecting marginalized gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on semantic gender bias in speech models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates bias levels across multiple speech tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.06917v3",
    "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models",
    "year": 2024,
    "authors": [
      "Zara Siddique",
      "Liam D. Turner",
      "Luis Espinosa-Anke"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates stereotypes in language models related to gender and ethnicity, which are social categories linked to inequality and discrimination. It examines how biases manifest in AI outputs, affecting marginalized groups. The focus on stereotypes and model behavior relates directly to social bias issues.",
      "inequality_type": [
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias in AI representations of social groups",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Probing models with bias datasets and analyzing outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.06432v1",
    "title": "An Empirical Study of Gendered Stereotypes in Emotional Attributes for Bangla in Multilingual Large Language Models",
    "year": 2024,
    "authors": [
      "Jayanta Sadhu",
      "Maneesha Rani Saha",
      "Rifat Shahriyar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in emotional attributes within Bangla LLMs, highlighting societal stereotypes and gendered emotional attribution, which relate directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on societal gender stereotypes in language models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias in language models' emotion attribution",
      "geographic_focus": [
        "Bangla-speaking regions"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.06098v1",
    "title": "Epistemological Bias As a Means for the Automated Detection of Injustices in Text",
    "year": 2024,
    "authors": [
      "Kenya Andrews",
      "Lamogha Chiazor"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting injustices related to implicit biases and stereotypes in text, particularly in media, which are linked to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on societal biases and prejudices in media narratives",
      "affected_populations": [
        "minority groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Qualitative Study"
      ],
      "methodology_detail": "Uses bias detection models and qualitative analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.06003v1",
    "title": "Surprising gender biases in GPT",
    "year": 2024,
    "authors": [
      "Raluca Alexandra Fulgu",
      "Valerio Capraro"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in GPT, highlighting implicit gender stereotypes and discriminatory tendencies, which relate directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Implicit gender bias in AI systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Experiments assessing gender bias in language generation and moral judgments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.05732v1",
    "title": "FairPFN: Transformers Can do Counterfactual Fairness",
    "year": 2024,
    "authors": [
      "Jake Robertson",
      "Noah Hollmann",
      "Noor Awad",
      "Frank Hutter"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems, addressing biases related to protected attributes, which are linked to social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "racial minorities",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Transformers trained to eliminate causal bias effects",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.05336v1",
    "title": "Artificial intelligence, rationalization, and the limits of control in the public sector: the case of tax policy optimization",
    "year": 2024,
    "authors": [
      "Jakob Mokander",
      "Ralph Schroeder"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses AI's role in optimizing tax policy to reduce economic inequality, highlighting impacts on social and economic equality.",
      "inequality_type": [
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on economic inequality through tax policy optimization",
      "affected_populations": [
        "economic classes"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Thought experiment on AI-driven tax policy",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.05335v1",
    "title": "Understanding and Addressing Gender Bias in Expert Finding Task",
    "year": 2024,
    "authors": [
      "Maddalena Amendola",
      "Carlos Castillo",
      "Andrea Passarella",
      "Raffaele Perego"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in expert finding models, addressing gender inequality and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI models",
      "affected_populations": [
        "female experts",
        "male users"
      ],
      "methodology": [
        "Experiment",
        "Data Analysis"
      ],
      "methodology_detail": "Analyzes bias in EF models using StackOverflow data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.04434v1",
    "title": "From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs",
    "year": 2024,
    "authors": [
      "Marion Bartl",
      "Susan Leavy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social discrimination. It focuses on reducing gender stereotypes and promoting inclusivity, directly engaging with gender inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Model Fine-tuning"
      ],
      "methodology_detail": "Develops inclusive datasets and fine-tunes models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.04268v3",
    "title": "NeuFair: Neural Network Fairness Repair with Dropout",
    "year": 2024,
    "authors": [
      "Vishnu Asutosh Dasu",
      "Ashish Kumar",
      "Saeid Tizpaz-Niari",
      "Gang Tan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing bias mitigation related to social groups, which directly pertains to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in neural networks for fairness",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Post-processing bias mitigation techniques in neural networks",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.03536v3",
    "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias",
    "year": 2024,
    "authors": [
      "Jayanta Sadhu",
      "Maneesha Rani Saha",
      "Rifat Shahriyar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to gender and religion in Bangla LLMs, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "religion"
      ],
      "other_detail": "Focus on social biases in language models",
      "affected_populations": [
        "women",
        "religious groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Probing Techniques",
        "Empirical Study"
      ],
      "methodology_detail": "Bias measurement and detection in Bangla LLM outputs",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.03473v1",
    "title": "Exploring LGBTQ+ Bias in Generative AI Answers across Different Country and Religious Contexts",
    "year": 2024,
    "authors": [
      "Lilla Vicsek",
      "Anna Vancs√≥",
      "Mike Zajko",
      "Judit Takacs"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI responses related to LGBTQ+ issues across cultural and religious contexts, highlighting social discrimination and bias in AI systems.",
      "inequality_type": [
        "gender",
        "religion",
        "cultural"
      ],
      "other_detail": "Focus on social bias and cultural sensitivity in AI",
      "affected_populations": [
        "LGBTQ+ individuals",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing AI responses to contextual prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.03059v2",
    "title": "FairJob: A Real-World Dataset for Fairness in Online Systems",
    "year": 2024,
    "authors": [
      "Mariia Vladimirova",
      "Federico Pavone",
      "Eustache Diemert"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in job recommendation systems, focusing on algorithmic bias and equitable access to employment opportunities, which are social inequality issues.",
      "inequality_type": [
        "economic",
        "employment",
        "gender",
        "racial"
      ],
      "other_detail": "Fairness in employment-related AI systems",
      "affected_populations": [
        "job seekers",
        "disadvantaged groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Bias mitigation and fairness metrics in online systems",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.02814v2",
    "title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective",
    "year": 2024,
    "authors": [
      "Zhaotian Weng",
      "Zijun Gao",
      "Jerone Andrews",
      "Jieyu Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender in vision-language models, which relate to social gender inequality and bias. It discusses how AI systems can perpetuate or mitigate social biases, impacting different social groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Causal Mediation Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias pathways within AI components",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.02810v2",
    "title": "Understanding the Prevalence of Caste: A Critical Discourse Analysis of Community Profiles on X",
    "year": 2024,
    "authors": [
      "Nayana Kirasur",
      "Shagun Jhaver"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines caste-based discrimination and marginalization in India, addressing social inequality related to caste, a social stratification category.",
      "inequality_type": [
        "social",
        "class",
        "ethnic"
      ],
      "other_detail": "Focuses on caste discrimination and social marginalization",
      "affected_populations": [
        "lower castes",
        "upper castes"
      ],
      "methodology": [
        "Critical Discourse Analysis"
      ],
      "methodology_detail": "Analyzes social media profiles' rhetorical strategies",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.02805v1",
    "title": "Efficient DNN-Powered Software with Fair Sparse Models",
    "year": 2024,
    "authors": [
      "Xuanqi Gao",
      "Weipeng Jiang",
      "Juan Zhai",
      "Shiqing Ma",
      "Xiaoyu Zhang",
      "Chao Shen"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on equitable treatment across groups.",
      "inequality_type": [
        "social",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "users of AI systems",
        "general society"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Evaluation",
        "Model Development"
      ],
      "methodology_detail": "Improving fairness in model pruning processes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.02702v3",
    "title": "Practical Guide for Causal Pathways and Sub-group Disparity Analysis",
    "year": 2024,
    "authors": [
      "Farnaz Kohankhaki",
      "Shaina Raza",
      "Oluwanifemi Bamgbose",
      "Deval Pandya",
      "Elham Dolatabadi"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes disparities related to sensitive attributes like race, focusing on causal pathways and sub-group differences, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial disparities in observational data",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Causal Decomposition Analysis",
        "Heterogeneity Assessment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using causal analysis to identify disparities and affected sub-groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.02030v1",
    "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
    "year": 2024,
    "authors": [
      "Chahat Raj",
      "Anjishnu Mukherjee",
      "Aylin Caliskan",
      "Antonios Anastasopoulos",
      "Ziwei Zhu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in LLMs related to societal stereotypes, reflecting social inequalities such as race, gender, and societal prejudices.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on social biases in AI systems",
      "affected_populations": [
        "minority groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Instruction Tuning"
      ],
      "methodology_detail": "Simulating social contact and debiasing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.01697v1",
    "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
    "year": 2024,
    "authors": [
      "Salvatore Greco",
      "Ke Zhou",
      "Licia Capra",
      "Tania Cerquitelli",
      "Daniele Quercia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and reliance on protected attributes like gender and race in NLP classifiers, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias reduction in NLP classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.17477v2",
    "title": "Toward Automated Detection of Biased Social Signals from the Content of Clinical Conversations",
    "year": 2024,
    "authors": [
      "Feng Chen",
      "Manas Satish Bedmutha",
      "Ray-Yuan Chung",
      "Janice Sabin",
      "Wanda Pratt",
      "Brian R. Wood",
      "Nadir Weibel",
      "Andrea L. Hartzler",
      "Trevor Cohen"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial differences in provider communication, highlighting bias and inequity in healthcare interactions.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in clinical communication",
      "affected_populations": [
        "white patients",
        "non-white patients"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Automated detection of social signals in conversations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.00983v3",
    "title": "FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models",
    "year": 2024,
    "authors": [
      "Ruinan Jin",
      "Zikang Xu",
      "Yuan Zhong",
      "Qiongsong Yao",
      "Qi Dou",
      "S. Kevin Zhou",
      "Xiaoxiao Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in medical imaging AI, addressing biases affecting diverse populations, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness across diverse patient groups",
      "affected_populations": [
        "underrepresented patients",
        "diverse populations"
      ],
      "methodology": [
        "Benchmarking",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates fairness metrics across datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.12031v1",
    "title": "Evaluation of Bias Towards Medical Professionals in Large Language Models",
    "year": 2024,
    "authors": [
      "Xi Chen",
      "Yang Xu",
      "MingKe You",
      "Li Wang",
      "WeiZhi Liu",
      "Jian Li"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI systems, which are social inequalities. It analyzes how LLMs may perpetuate or amplify these biases, impacting diversity in healthcare. This directly relates to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI affecting healthcare workforce diversity",
      "affected_populations": [
        "medical professionals",
        "underrepresented groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias testing with fictitious resumes and demographic variations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.00600v1",
    "title": "GenderBias-\\emph{VL}: Benchmarking Gender Bias in Vision Language Models via Counterfactual Probing",
    "year": 2024,
    "authors": [
      "Yisong Xiao",
      "Aishan Liu",
      "QianJia Cheng",
      "Zhenfei Yin",
      "Siyuan Liang",
      "Jiapeng Li",
      "Jing Shao",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in vision-language models, addressing gender inequality and social bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Counterfactual Probing"
      ],
      "methodology_detail": "Constructs visual question counterfactuals to expose biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.00486v1",
    "title": "Towards Massive Multilingual Holistic Bias",
    "year": 2024,
    "authors": [
      "Xiaoqing Ellen Tan",
      "Prangthip Hansanti",
      "Carleigh Wood",
      "Bokai Yu",
      "Christophe Ropers",
      "Marta R. Costa-juss√†"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic biases, gender bias, and toxicity in language models, which relate to social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on gender bias in multilingual AI models",
      "affected_populations": [
        "gender groups",
        "multilingual speakers"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Scaling bias datasets across multiple languages",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.00400v2",
    "title": "Formalising Anti-Discrimination Law in Automated Decision Systems",
    "year": 2024,
    "authors": [
      "Holli Sargeant",
      "M√•ns Magnusson"
    ],
    "categories": [
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic discrimination in legally protected contexts, focusing on fairness and anti-discrimination law, which are directly related to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Legal framework alignment with social discrimination issues",
      "affected_populations": [
        "racial minorities",
        "women",
        "low-income groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Legal-grounded formalism and real-world case study",
      "geographic_focus": [
        "United Kingdom"
      ],
      "ai_relationship": "AI as regulation subject",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.00170v1",
    "title": "Dataset Representativeness and Downstream Task Fairness",
    "year": 2024,
    "authors": [
      "Victor Borza",
      "Andrew Estornell",
      "Chien-Ju Ho",
      "Bradley Malin",
      "Yevgeniy Vorobeychik"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses dataset bias and fairness in AI, affecting social groups.",
      "inequality_type": [
        "racial",
        "health",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in data-driven decision-making",
      "affected_populations": [
        "minority groups",
        "patients",
        "students"
      ],
      "methodology": [
        "Empirical Analysis",
        "Theoretical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes bias and fairness trade-offs empirically and theoretically",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.01617v1",
    "title": "Subjective fairness in algorithmic decision-support",
    "year": 2024,
    "authors": [
      "Sarra Tajouri",
      "Alexis Tsouki√†s"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in decision-making, highlighting social perceptions and societal realities, which relate to social discrimination and bias issues.",
      "inequality_type": [
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on fairness perceptions and stakeholder perspectives",
      "affected_populations": [
        "individuals in decision contexts"
      ],
      "methodology": [
        "Explainable Clustering",
        "Qualitative Study"
      ],
      "methodology_detail": "Using explanations to align perceptions of fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.00138v1",
    "title": "Analyzing Quality, Bias, and Performance in Text-to-Image Generative Models",
    "year": 2024,
    "authors": [
      "Nila Masrourisaadat",
      "Nazanin Sedaghatkish",
      "Fatemeh Sarshartehrani",
      "Edward A. Fox"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "I.2.6; I.2.10; I.2.7; I.4.10"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases in AI models, specifically gender and social biases, which relate to social inequality issues.",
      "inequality_type": [
        "gender",
        "social"
      ],
      "other_detail": "Focuses on social bias in AI-generated images",
      "affected_populations": [
        "women",
        "social groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Bias Analysis"
      ],
      "methodology_detail": "Assessing social biases in image generation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.19668v1",
    "title": "PopAlign: Population-Level Alignment for Fair Text-to-Image Generation",
    "year": 2024,
    "authors": [
      "Shufan Li",
      "Harkanwar Singh",
      "Aditya Grover"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models related to gender and ethnicity, which are social inequalities. It focuses on mitigating these biases in text-to-image generation, a social fairness concern.",
      "inequality_type": [
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI-generated imagery",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Population-level bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.17474v1",
    "title": "\"My Kind of Woman\": Analysing Gender Stereotypes in AI through The Averageness Theory and EU Law",
    "year": 2024,
    "authors": [
      "Miriam Doh",
      "Anastasia Karagianni"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender stereotypes and biases in AI, highlighting social discrimination and stereotypes related to gender. It discusses how AI systems can propagate social prejudices, affecting gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes and bias propagation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Using AI models, psychological and legal theories",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.09551v1",
    "title": "Diminishing Stereotype Bias in Image Generation Model using Reinforcemenlent Learning Feedback",
    "year": 2024,
    "authors": [
      "Xin Chen",
      "Virgile Foussereau"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI-generated images, a form of social discrimination. It focuses on mitigating gender stereotypes, directly related to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Reinforcement Learning",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation in AI image generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.10329v1",
    "title": "Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It",
    "year": 2024,
    "authors": [
      "Philipp Hacker"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in generative AI related to race, gender, and representation, which impact social groups and contribute to discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "informational"
      ],
      "other_detail": "Biases in AI reflect social inequalities and stereotypes",
      "affected_populations": [
        "minority groups",
        "women",
        "underrepresented communities"
      ],
      "methodology": [
        "Literature Review",
        "Legal Analysis",
        "Policy Recommendations"
      ],
      "methodology_detail": "Analyzes legal frameworks and bias mitigation strategies",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.17974v2",
    "title": "Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts",
    "year": 2024,
    "authors": [
      "Xuyang Wu",
      "Yuan Wang",
      "Hsin-Tai Wu",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases related to race, gender, age, and skin tone in AI models, addressing fairness issues affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias mitigation in AI fairness evaluation",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups",
        "skin tone groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness benchmarking and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.17906v1",
    "title": "Unbiasing on the Fly: Explanation-Guided Human Oversight of Machine Learning System Decisions",
    "year": 2024,
    "authors": [
      "Hussaini Mamman",
      "Shuib Basri",
      "Abdullateef Balogun",
      "Abubakar Abdullahi Imam",
      "Ganesh Kumar",
      "Luiz Fernando Capretz"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination in ML systems, which relates to social biases and fairness issues affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "minority groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "System Design"
      ],
      "methodology_detail": "Real-time monitoring and human oversight framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.17737v1",
    "title": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users",
    "year": 2024,
    "authors": [
      "Elinor Poole-Dayan",
      "Deb Roy",
      "Jad Kabbara"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in LLM performance based on user traits like language proficiency, education, and origin, highlighting unequal impacts on vulnerable groups.",
      "inequality_type": [
        "educational",
        "linguistic",
        "geographic"
      ],
      "other_detail": "Focuses on digital and informational disparities",
      "affected_populations": [
        "non-native English speakers",
        "lower education users",
        "users outside US"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Testing LLM responses across user traits and datasets",
      "geographic_focus": [
        "US",
        "outside US"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.11002v2",
    "title": "MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias",
    "year": 2024,
    "authors": [
      "Guorun Wang",
      "Lucia Specia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, a social discrimination issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias mitigation in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in text-to-image models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.17405v1",
    "title": "Less can be more: representational vs. stereotypical gender bias in facial expression recognition",
    "year": 2024,
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Aranzazu Jurio",
      "Mikel Galar"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in facial expression recognition, highlighting social discrimination aspects related to gender. It analyzes how biases propagate into AI models, affecting different gender groups. This aligns with addressing social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Analyzing bias propagation through dataset subsets and model evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.17385v2",
    "title": "Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance",
    "year": 2024,
    "authors": [
      "Manon Reusens",
      "Philipp Borchert",
      "Jochen De Weerdt",
      "Bart Baesens"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in language model responses based on user nativeness, highlighting potential biases affecting non-native English speakers, which relates to social bias and inequality.",
      "inequality_type": [
        "linguistic",
        "educational",
        "informational"
      ],
      "other_detail": "Bias based on language nativeness",
      "affected_populations": [
        "non-native English speakers",
        "native English speakers"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing response quality across user groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.17375v1",
    "title": "An Empirical Study on the Characteristics of Bias upon Context Length Variation for Bangla",
    "year": 2024,
    "authors": [
      "Jayanta Sadhu",
      "Ayan Antik Khan",
      "Abhik Bhattacharjee",
      "Rifat Shahriyar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases, specifically gender bias, in language models, which relates to social discrimination. It focuses on bias measurement and its variation, addressing social fairness issues in AI systems. The study's emphasis on bias in a low-resource language highlights social inequality concerns.",
      "inequality_type": [
        "gender",
        "linguistic"
      ],
      "other_detail": "Bias measurement in low-resource language context",
      "affected_populations": [
        "Bangla speakers",
        "gender groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and context length analysis",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.01595v1",
    "title": "Fairpriori: Improving Biased Subgroup Discovery for Deep Neural Network Fairness",
    "year": 2024,
    "authors": [
      "Kacy Zhou",
      "Jiawen Wen",
      "Nan Yang",
      "Dong Yuan",
      "Qinghua Lu",
      "Huaming Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses intersectional bias in AI, related to social groups such as race and gender, highlighting fairness issues affecting marginalized populations.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focus on intersectional bias detection in AI systems",
      "affected_populations": [
        "women",
        "darker-skinned individuals"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness metric computation and subgroup discovery",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.16756v1",
    "title": "Addressing Polarization and Unfairness in Performative Prediction",
    "year": 2024,
    "authors": [
      "Kun Jin",
      "Tian Xie",
      "Yang Liu",
      "Xueru Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and polarization effects in AI systems, addressing social bias and group disparities. It discusses societal implications of performative prediction, which impacts different social groups. The focus on fairness mechanisms relates directly to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on fairness and polarization in AI systems",
      "affected_populations": [
        "social groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes fairness interventions and stability in models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.16592v3",
    "title": "Toward Fairer Face Recognition Datasets",
    "year": 2024,
    "authors": [
      "Alexandre Fournier-Montgieux",
      "Michael Soumm",
      "Adrian Popescu",
      "Bertrand Luvison",
      "Herv√© Le Borgne"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and demographic bias in face recognition datasets, which relate to social discrimination and bias issues affecting different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on demographic fairness in AI datasets",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Balancing demographic attributes in datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.16207v1",
    "title": "Thinking beyond Bias: Analyzing Multifaceted Impacts and Implications of AI on Gendered Labour",
    "year": 2024,
    "authors": [
      "Satyam Mohla",
      "Bishnupriya Bagh",
      "Anupam Guha"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gendered labor disparities and AI's role in exacerbating gender inequality, focusing on economic and gender impacts.",
      "inequality_type": [
        "economic",
        "gender"
      ],
      "other_detail": "Focus on gendered labor and economic transformation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Qualitative Study",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing economic and gender impacts of AI",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.16152v1",
    "title": "Towards Region-aware Bias Evaluation Metrics",
    "year": 2024,
    "authors": [
      "Angana Borah",
      "Aparna Garimella",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender biases in AI models, highlighting societal biases across regions.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Focus on regional gender bias differences",
      "affected_populations": [
        "women",
        "regional communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias evaluation using region-aware topic pairs and WEAT",
      "geographic_focus": [
        "multiple regions"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.14993v1",
    "title": "Disability Representations: Finding Biases in Automatic Image Generation",
    "year": 2024,
    "authors": [
      "Yannis Tevissen"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in AI models related to disability representation, a social group, highlighting societal biases and inequalities.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Representation bias in AI models",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed depiction biases across multiple models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.14847v2",
    "title": "Fair Text to Medical Image Diffusion Model with Subgroup Distribution Aligned Tuning",
    "year": 2024,
    "authors": [
      "Xu Han",
      "Fangfang Fan",
      "Jingzhao Rong",
      "Zhen Li",
      "Georges El Fakhri",
      "Qingyu Chen",
      "Xiaofeng Liu"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in medical image generation and proposes methods to reduce subgroup disparities, directly engaging with social fairness issues.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in medical AI systems",
      "affected_populations": [
        "female patients",
        "male patients"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias analysis and subgroup distribution alignment",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.14194v2",
    "title": "VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model",
    "year": 2024,
    "authors": [
      "Sibo Wang",
      "Xiangkui Cao",
      "Jie Zhang",
      "Zheng Yuan",
      "Shiguang Shan",
      "Xilin Chen",
      "Wen Gao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in vision-language models across categories like race, gender, and socioeconomic status, directly addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability",
        "age",
        "religion"
      ],
      "other_detail": "Focuses on social biases in AI outputs",
      "affected_populations": [
        "racial groups",
        "women",
        "disabled",
        "elderly",
        "minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Large-scale bias evaluation in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.14154v1",
    "title": "Watching the Watchers: A Comparative Fairness Audit of Cloud-based Content Moderation Services",
    "year": 2024,
    "authors": [
      "David Hartmann",
      "Amin Oueslati",
      "Dimitri Staufer"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in content moderation algorithms affecting marginalized social groups, highlighting issues of fairness and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "disability"
      ],
      "other_detail": "Biases toward specific social identity groups",
      "affected_populations": [
        "Women",
        "LGBTQ+",
        "PoC"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Black-box audit with benchmark datasets and fairness measures",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.14023v2",
    "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective",
    "year": 2024,
    "authors": [
      "Yuchen Wen",
      "Keping Bi",
      "Wei Chen",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates implicit bias in LLMs related to demographic groups, addressing social bias and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "educational",
        "disability"
      ],
      "other_detail": "Focuses on social bias detection and assessment in AI models",
      "affected_populations": [
        "minority groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Psychometric Approach"
      ],
      "methodology_detail": "Using psychometric-inspired attack methods and benchmarks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.13993v2",
    "title": "Exploring Changes in Nation Perception with Nationality-Assigned Personas in LLMs",
    "year": 2024,
    "authors": [
      "Mahammed Kamruzzaman",
      "Gene Louis Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to nationality perceptions in LLMs, highlighting stereotypes and fairness issues. It discusses how AI systems reflect and potentially reinforce social biases across different nations. This relates to social discrimination and bias in AI outputs.",
      "inequality_type": [
        "nationality",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on national stereotypes and biases in AI",
      "affected_populations": [
        "nation groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating LLM perceptions and correlating with human responses",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.13925v3",
    "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Tao Zhang",
      "Ziqian Zeng",
      "Yuxiang Xiao",
      "Huiping Zhuang",
      "Cen Chen",
      "James Foulds",
      "Shimei Pan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI, a form of social inequality, by developing a dataset to mitigate gender bias in language models.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Developing and testing bias mitigation dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.13677v2",
    "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora",
    "year": 2024,
    "authors": [
      "Erik Derner",
      "Sara Sansalvador de la Fuente",
      "Yoan Guti√©rrez",
      "Paloma Moreda",
      "Nuria Oliver"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language corpora, which relates to social gender inequality and discrimination. It focuses on measuring and analyzing gender representation bias, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using LLMs to quantify gender bias in text",
      "geographic_focus": [
        "Spanish-speaking countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.13556v1",
    "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
    "year": 2024,
    "authors": [
      "Yi Zhou",
      "Danushka Bollegala",
      "Jose Camacho-Collados"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to gender and race in language models, analyzing their stability over time and their impact on different demographic groups.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social biases in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of temporal corpora and bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.13544v2",
    "title": "One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes",
    "year": 2024,
    "authors": [
      "Yuchang Zhu",
      "Jintang Li",
      "Yatao Bian",
      "Zibin Zheng",
      "Liang Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI systems related to protected groups such as race and age, highlighting social discrimination concerns.",
      "inequality_type": [
        "racial",
        "age",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness across sensitive attributes",
      "affected_populations": [
        "racial groups",
        "elderly",
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Invariant Learning",
        "Causal Modeling"
      ],
      "methodology_detail": "Fairness framework using invariant learning approach",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.13018v1",
    "title": "Reclaiming Power over AI: Equipping Queer Teens as AI Designers for HIV Prevention",
    "year": 2024,
    "authors": [
      "William Liem",
      "Andrew Berry",
      "Kathryn Macapagal"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social discrimination and bias in AI affecting LGBTQ+ youth, focusing on health and social equity issues.",
      "inequality_type": [
        "gender",
        "health",
        "informational"
      ],
      "other_detail": "Bias and discrimination in AI systems",
      "affected_populations": [
        "LGBTQ+ adolescents",
        "queer youth"
      ],
      "methodology": [
        "Participatory Design",
        "Community-Engaged Approach"
      ],
      "methodology_detail": "Youth-led AI tool development",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.12646v1",
    "title": "An Empirical Study on the Fairness of Foundation Models for Multi-Organ Image Segmentation",
    "year": 2024,
    "authors": [
      "Qin Li",
      "Yizhe Zhang",
      "Yan Li",
      "Jun Lyu",
      "Meng Liu",
      "Longyu Sun",
      "Mengting Sun",
      "Qirong Li",
      "Wenyue Mao",
      "Xinran Wu",
      "Yajing Zhang",
      "Yinghua Chu",
      "Shuo Wang",
      "Chengyan Wang"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities in medical image segmentation across demographic groups, addressing social bias and inequality in AI systems.",
      "inequality_type": [
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focus on fairness in medical AI models",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates segmentation performance across demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.12399v1",
    "title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities",
    "year": 2024,
    "authors": [
      "Mae Sosto",
      "Alberto Barr√≥n-Cede√±o"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination and bias against LGBTQIA+ individuals in language models, addressing social bias and inequality related to gender and sexual orientation.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Focus on discrimination in AI towards queer identities",
      "affected_populations": [
        "LGBTQIA+ individuals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessment framework using template-based MLM tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.12364v1",
    "title": "Does Context Help Mitigate Gender Bias in Neural Machine Translation?",
    "year": 2024,
    "authors": [
      "Harritxu Gete",
      "Thierry Etchegoyhen"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in neural machine translation, a social inequality issue related to gender discrimination. It analyzes how AI systems perpetuate or mitigate gender stereotypes, impacting gender fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzing translation outputs with context-aware models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.12232v2",
    "title": "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
    "year": 2024,
    "authors": [
      "Huy Nghiem",
      "John Prindle",
      "Jieyu Zhao",
      "Hal Daum√© III"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race and gender in AI hiring decisions, highlighting social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI systems affecting employment outcomes",
      "affected_populations": [
        "racial minorities",
        "women",
        "job candidates"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulating hiring decisions and salary recommendations",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.12138v1",
    "title": "Bias in Text Embedding Models",
    "year": 2024,
    "authors": [
      "Vasyl Rakivnenko",
      "Nestor Maslej",
      "Jessica Cervi",
      "Volodymyr Zhukov"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in text embedding models, highlighting social discrimination aspects.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes gender associations in text embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.12033v2",
    "title": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
    "year": 2024,
    "authors": [
      "Yuqing Wang",
      "Yun Zhao",
      "Sara Alessandra Keller",
      "Anne de Hond",
      "Marieke M. van Buchem",
      "Malvika Pillai",
      "Tina Hernandez-Boussard"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases related to social factors like gender, age, and religion in mental health AI analysis, addressing fairness issues affecting vulnerable populations.",
      "inequality_type": [
        "gender",
        "age",
        "religion",
        "health"
      ],
      "other_detail": "Focuses on fairness in mental health AI systems",
      "affected_populations": [
        "vulnerable groups",
        "mental health patients"
      ],
      "methodology": [
        "Systematic Evaluation",
        "Prompt Engineering",
        "Fairness Analysis"
      ],
      "methodology_detail": "Bias assessment and mitigation in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.11665v2",
    "title": "See It from My Perspective: How Language Affects Cultural Bias in Image Understanding",
    "year": 2024,
    "authors": [
      "Amith Ananthram",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Kathleen McKeown"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines cultural biases in AI models, highlighting disparities linked to cultural and linguistic representation, which relate to social bias and inequality issues.",
      "inequality_type": [
        "linguistic",
        "cultural",
        "social bias"
      ],
      "other_detail": "Focuses on cultural bias in AI systems",
      "affected_populations": [
        "East Asian users",
        "Western users"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates model performance across diverse cultural datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.16892v1",
    "title": "Exploring Fusion Techniques in Multimodal AI-Based Recruitment: Insights from FairCVdb",
    "year": 2024,
    "authors": [
      "Swati Swati",
      "Arjun Roy",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness and bias in AI recruitment systems, addressing social bias issues related to demographics.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic fairness in AI systems",
      "affected_populations": [
        "job applicants",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analyzes fusion techniques and bias implications",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.15484v2",
    "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Ze Wang",
      "Zekun Wu",
      "Xin Guan",
      "Michael Thaler",
      "Adriano Koshiyama",
      "Skylar Lu",
      "Sachin Beepath",
      "Ediz Ertekin Jr.",
      "Maria Perez-Ortiz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender hiring bias in LLMs, highlighting social discrimination. It analyzes biases affecting gender groups in employment contexts, addressing social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female",
        "male"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias benchmarking and statistical analysis of LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.11109v5",
    "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
    "year": 2024,
    "authors": [
      "Amit Das",
      "Zheng Zhang",
      "Najib Hasan",
      "Souvika Sarkar",
      "Fatemeh Jamshidi",
      "Tathagata Bhattacharya",
      "Mostafa Rahgouy",
      "Nilanjana Raychawdhary",
      "Dongji Feng",
      "Vinija Jain",
      "Aman Chadha",
      "Mary Sandage",
      "Lauramarie Pope",
      "Gerry Dozier",
      "Cheryl Seals"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender, race, religion, and disability in AI annotation, directly addressing social discrimination and bias issues affecting vulnerable groups.",
      "inequality_type": [
        "gender",
        "race",
        "religion",
        "disability"
      ],
      "other_detail": "Focus on social biases in AI annotation processes",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups",
        "disabled individuals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Analysis"
      ],
      "methodology_detail": "Analyzing biases in LLM annotations and datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.10653v1",
    "title": "Justice in Healthcare Artificial Intelligence in Africa",
    "year": 2024,
    "authors": [
      "Aloysius Ochasi",
      "Abdoul Jalil Djiberou Mahamadou",
      "Russ B. Altman"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses justice, fairness, and social inequities in AI healthcare, emphasizing impacts on marginalized groups in Africa.",
      "inequality_type": [
        "health",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on healthcare access disparities in Africa",
      "affected_populations": [
        "resource-limited communities",
        "marginalized groups"
      ],
      "methodology": [
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing ethical and justice implications",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.10486v1",
    "title": "Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?",
    "year": 2024,
    "authors": [
      "Haozhe An",
      "Christabel Acquaye",
      "Colin Wang",
      "Zongxia Li",
      "Rachel Rudinger"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial and gender biases in AI hiring decisions, addressing social discrimination and bias issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on discrimination in AI-based hiring processes",
      "affected_populations": [
        "White applicants",
        "Hispanic applicants"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Using templatic prompts to measure bias effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.10130v1",
    "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
    "year": 2024,
    "authors": [
      "Yan Liu",
      "Yu Liu",
      "Xiaokang Chen",
      "Pin-Yu Chen",
      "Daoguang Zan",
      "Min-Yen Kan",
      "Tsung-Yi Ho"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases in language models, which relate to social discrimination and inequality issues such as gender and demographic biases.",
      "inequality_type": [
        "gender",
        "racial",
        "demographic"
      ],
      "other_detail": "Focuses on social bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "demographic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Neuron attribution and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.09977v1",
    "title": "Disentangling Dialect from Social Bias via Multitask Learning to Improve Fairness",
    "year": 2024,
    "authors": [
      "Maximilian Splieth√∂ver",
      "Sai Nikhil Menon",
      "Henning Wachsmuth"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in NLP related to dialects, which are tied to social groups, indicating a focus on social bias and inequality.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Focus on dialect-related social bias in language processing",
      "affected_populations": [
        "dialect speakers",
        "social groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Multitask Learning"
      ],
      "methodology_detail": "Dialect modeling to improve fairness",
      "geographic_focus": [
        "African-American English dialect"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.09307v4",
    "title": "What is Fair? Defining Fairness in Machine Learning for Health",
    "year": 2024,
    "authors": [
      "Jianhui Gao",
      "Benson Chou",
      "Zachary R. McCaw",
      "Hilary Thurston",
      "Paul Varghese",
      "Chuan Hong",
      "Jessica Gronsbell"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in ML for health, addressing disparities across patient groups, which relates to social inequalities such as health disparities and potential biases affecting marginalized populations.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on health disparities and fairness metrics",
      "affected_populations": [
        "patients",
        "healthcare groups"
      ],
      "methodology": [
        "Literature Review",
        "Case Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes fairness metrics and real-world EHR data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.09070v3",
    "title": "FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of Thought Reasoning with Multimodal Large Language Models",
    "year": 2024,
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI-generated content, focusing on fairness and representation, which relate to social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Bias mitigation in AI-generated images",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Bias mitigation techniques in multimodal models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.09029v1",
    "title": "Fair by design: A sociotechnical approach to justifying the fairness of AI-enabled systems across the lifecycle",
    "year": 2024,
    "authors": [
      "Marten H. L. Kaas",
      "Christopher Burr",
      "Zoe Porter",
      "Berk Ozturk",
      "Philippa Ryan",
      "Michael Katell",
      "Nuala Polo",
      "Kalle Westerling",
      "Ibrahim Habli"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness considerations in AI systems, particularly in healthcare, addressing discrimination and equity issues affecting different social groups.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in healthcare AI systems",
      "affected_populations": [
        "patients with diabetes",
        "subgroups at risk"
      ],
      "methodology": [
        "System Design",
        "Ethics Analysis",
        "Case Study"
      ],
      "methodology_detail": "Applying assurance framework to clinical AI system",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.08824v1",
    "title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions",
    "year": 2024,
    "authors": [
      "Rumaisa Azeem",
      "Andrew Hundt",
      "Masoumeh Mansouri",
      "Martim Brand√£o"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and discriminatory outcomes in LLMs affecting diverse social groups, highlighting issues related to race, gender, disability, and nationality.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "nationality"
      ],
      "other_detail": "Biases in AI outputs affecting social groups",
      "affected_populations": [
        "racial minorities",
        "women",
        "disabled individuals",
        "nationalities"
      ],
      "methodology": [
        "Experiment",
        "System Design",
        "Ethics Analysis"
      ],
      "methodology_detail": "Evaluation of LLMs in social and safety contexts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.08819v2",
    "title": "AIM: Attributing, Interpreting, Mitigating Data Unfairness",
    "year": 2024,
    "authors": [
      "Zhining Liu",
      "Ruizhong Qiu",
      "Zhichen Zeng",
      "Yada Zhu",
      "Hendrik Hamann",
      "Hanghang Tong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on identifying and mitigating biases in data related to discrimination, which directly pertains to social fairness issues such as racial and group biases.",
      "inequality_type": [
        "racial",
        "group",
        "social"
      ],
      "other_detail": "Bias attribution and mitigation in data",
      "affected_populations": [
        "disadvantaged groups",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Sample bias measurement and data editing algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.08818v3",
    "title": "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination",
    "year": 2024,
    "authors": [
      "Eve Fleisig",
      "Genevieve Smith",
      "Madeline Bossi",
      "Ishita Rustagi",
      "Xavier Yin",
      "Dan Klein"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines linguistic bias and discrimination in AI models, highlighting issues of stereotyping and demeaning content toward non-standard dialect speakers, which relate to social discrimination and inequality.",
      "inequality_type": [
        "linguistic",
        "social bias"
      ],
      "other_detail": "Focuses on language-based discrimination in AI systems",
      "affected_populations": [
        "non-standard dialect speakers"
      ],
      "methodology": [
        "Experiment",
        "Linguistic analysis",
        "Native speaker evaluation"
      ],
      "methodology_detail": "Analysis of model responses and linguistic features",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.08726v1",
    "title": "Standard Language Ideology in AI-Generated Language",
    "year": 2024,
    "authors": [
      "Genevieve Smith",
      "Eve Fleisig",
      "Madeline Bossi",
      "Ishita Rustagi",
      "Xavier Yin"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how standard language ideology in AI reinforces global power structures, impacting minoritized language communities, which relates to social inequality and bias.",
      "inequality_type": [
        "linguistic",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on language bias and societal power dynamics",
      "affected_populations": [
        "minoritized language speakers"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes ideological implications and systemic biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.08222v2",
    "title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion",
    "year": 2024,
    "authors": [
      "Sha Luo",
      "Sang Jung Kim",
      "Zening Duan",
      "Kaiping Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases in computer vision models related to gender and emotion, highlighting discriminatory biases and fairness issues, which are central to social inequality discussions.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Biases against transgender and non-binary individuals",
      "affected_populations": [
        "transgender",
        "non-binary",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Manual validation",
        "Empirical comparison"
      ],
      "methodology_detail": "Benchmarking models with human-validated subsets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.08183v2",
    "title": "Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction",
    "year": 2024,
    "authors": [
      "Micol Spitale",
      "Jiaee Cheong",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models used for depression detection, addressing gender fairness and bias, which are social inequality issues.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender fairness in mental health AI applications",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Evaluates bias using performance metrics and thematic analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.07504v1",
    "title": "Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling Queer Voices",
    "year": 2024,
    "authors": [
      "Atli Sigurgeirsson",
      "Eddie L. Ungless"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses the ethics of modeling queer voices, highlighting safety, fairness, and potential harms, which relate to social discrimination and bias issues affecting LGBTQ+ communities.",
      "inequality_type": [
        "gender",
        "disability",
        "social bias"
      ],
      "other_detail": "Ethics of AI modeling queer identities",
      "affected_populations": [
        "LGBTQ+ individuals"
      ],
      "methodology": [
        "Experiment",
        "Ethics Analysis"
      ],
      "methodology_detail": "Testing voice models and ethical discussion",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.06840v2",
    "title": "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles",
    "year": 2024,
    "authors": [
      "Julia Kruk",
      "Michela Marchini",
      "Rijul Magu",
      "Caleb Ziems",
      "David Muchlinski",
      "Diyi Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "J.4; K.4.1; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on coded communication (dog whistles) used for racial and socioeconomic discrimination, directly addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Uses NLP to detect coded discriminatory language",
      "affected_populations": [
        "racial minorities",
        "socioeconomic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "Disambiguating coded language in social media",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.06131v1",
    "title": "Building Bridges: A Dataset for Evaluating Gender-Fair Machine Translation into German",
    "year": 2024,
    "authors": [
      "Manuel Lardelli",
      "Giuseppe Attanasio",
      "Anne Lauscher"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, highlighting gender visibility issues, which relate to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender-fair language in translation",
      "affected_populations": [
        "women",
        "non-binary people"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Benchmarking translation systems with gender-fair resources",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.05902v1",
    "title": "Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback",
    "year": 2024,
    "authors": [
      "Emilia Agis Lerner",
      "Florian E. Dorner",
      "Elliott Ash",
      "Naman Goel"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness preferences across social groups and their impact on AI moderation, addressing racial, age, political, educational, and LGBTQ+ disparities.",
      "inequality_type": [
        "racial",
        "age",
        "educational",
        "LGBTQ+"
      ],
      "other_detail": "Focuses on social bias in AI fairness judgments",
      "affected_populations": [
        "racial groups",
        "age groups",
        "LGBTQ+ individuals",
        "educational levels"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Collects human feedback data from diverse demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.05602v1",
    "title": "Can Prompt Modifiers Control Bias? A Comparative Analysis of Text-to-Image Generative Models",
    "year": 2024,
    "authors": [
      "Philip Wootaek Shin",
      "Jihyun Janice Ahn",
      "Wenpeng Yin",
      "Jack Sampson",
      "Vijaykrishnan Narayanan"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal biases related to gender, race, geography, and culture in AI models, which are linked to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "geographic",
        "cultural"
      ],
      "other_detail": "Focuses on bias control in AI-generated images",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "cultural communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes bias presence and manipulation via prompt engineering",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.05542v1",
    "title": "The Development of the Reproductive Healthcare Equity Algorithm (RHEA)",
    "year": 2024,
    "authors": [
      "Shriya Karam",
      "Lauren Shanos",
      "Jessica Ford",
      "Lorenzo Castaneda",
      "Megan S. Ryerson",
      "Rakesh Vohra"
    ],
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in reproductive healthcare access for low-income women post-Roe v. Wade repeal, highlighting socioeconomic and geographic inequalities.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "health"
      ],
      "other_detail": "Focus on reproductive healthcare access disparities",
      "affected_populations": [
        "low-income women",
        "underserved women"
      ],
      "methodology": [
        "Optimization Model",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Resource allocation and logistics optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.04997v1",
    "title": "On the social bias of speech self-supervised models",
    "year": 2024,
    "authors": [
      "Yi-Cheng Lin",
      "Tzu-Quan Lin",
      "Hsi-Che Lin",
      "Andy T. Liu",
      "Hung-yi Lee"
    ],
    "categories": [
      "eess.AS",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social bias in AI models affecting marginalized groups, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on bias amplification in AI models",
      "affected_populations": [
        "marginalized groups"
      ],
      "methodology": [
        "Experiment",
        "Model Development",
        "Debiasing Techniques"
      ],
      "methodology_detail": "Analyzes bias propagation and mitigation methods in SSL models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.04382v2",
    "title": "Improving the Fairness of Deep-Learning, Short-term Crime Prediction with Under-reporting-aware Models",
    "year": 2024,
    "authors": [
      "Jiahui Wu",
      "Vanessa Frias-Martinez"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in crime prediction models affecting minority racial and ethnic groups, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on fairness in AI-based crime prediction",
      "affected_populations": [
        "minority racial groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Regularization",
        "Model Development"
      ],
      "methodology_detail": "Combines pre-processing and in-processing bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.03575v1",
    "title": "Reconciling Heterogeneous Effects in Causal Inference",
    "year": 2024,
    "authors": [
      "Audrey Chang",
      "Emily Diana",
      "Alexander Williams Tolbert"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and disparities in high-stakes decisions affecting marginalized groups, addressing social inequality issues.",
      "inequality_type": [
        "health",
        "educational",
        "disability",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in predictive modeling for marginalized communities",
      "affected_populations": [
        "healthcare patients",
        "low-income groups",
        "minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Applying algorithms to reconcile heterogeneous effects",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.03292v1",
    "title": "Evaluating AI fairness in credit scoring with the BRIO tool",
    "year": 2024,
    "authors": [
      "Greta Coraglia",
      "Francesco A. Genco",
      "Pellegrino Piantadosi",
      "Enrico Bagli",
      "Pietro Giuffrida",
      "Davide Posillipo",
      "Giuseppe Primiero"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness issues in AI systems applied to credit scoring, analyzing social biases across demographic groups, which directly relates to social inequalities such as economic and social discrimination.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focus on bias detection in credit scoring models",
      "affected_populations": [
        "credit applicants",
        "demographic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Fairness Metrics",
        "Case Study"
      ],
      "methodology_detail": "Applying fairness metrics to credit dataset",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.02966v3",
    "title": "Generative AI and Digital Neocolonialism in Global Education: Towards an Equitable Framework",
    "year": 2024,
    "authors": [
      "Matthew Nyaaba",
      "Alyson Wright",
      "Gyu Lim Choi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how generative AI perpetuates cultural, linguistic, and access disparities, impacting marginalized groups and exacerbating educational inequalities.",
      "inequality_type": [
        "educational",
        "linguistic",
        "digital",
        "racial",
        "geographic"
      ],
      "other_detail": "Focus on cultural imperialism and access disparities",
      "affected_populations": [
        "non-Western students",
        "indigenous language speakers",
        "marginalized communities"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Critical discussion and conceptual framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.02361v1",
    "title": "Using Self-supervised Learning Can Improve Model Fairness",
    "year": 2024,
    "authors": [
      "Sofia Yfantidou",
      "Dimitris Spathis",
      "Marios Constantinides",
      "Athena Vakali",
      "Daniele Quercia",
      "Fahim Kawsar"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper assesses fairness in AI models across demographic groups, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "educational"
      ],
      "other_detail": "Focuses on fairness across social demographics",
      "affected_populations": [
        "demographic groups",
        "protected attributes"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Assessment",
        "Representation Analysis"
      ],
      "methodology_detail": "Evaluates fairness improvements via SSL models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.01757v1",
    "title": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities",
    "year": 2024,
    "authors": [
      "Golnoosh Farnadi",
      "Mohammad Havaei",
      "Negar Rostamzadeh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities affecting marginalized communities, focusing on social bias, representation, and impacts of foundation models, which are directly related to social inequalities.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "disability",
        "educational"
      ],
      "other_detail": "Interconnected disparities in AI systems and marginalized groups",
      "affected_populations": [
        "marginalized communities",
        "underserved groups"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing disparities and cascading impacts in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.11203v1",
    "title": "The Life Cycle of Large Language Models: A Review of Biases in Education",
    "year": 2024,
    "authors": [
      "Jinsook Lee",
      "Yann Hicke",
      "Renzhe Yu",
      "Christopher Brooks",
      "Ren√© F. Kizilcec"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI systems within educational contexts, which relate to social inequalities such as educational disparities and fairness issues.",
      "inequality_type": [
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on bias and fairness in AI applications in education",
      "affected_populations": [
        "students",
        "teachers"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Mapping AI life cycle and bias sources",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.01399v1",
    "title": "Null Compliance: NYC Local Law 144 and the Challenges of Algorithm Accountability",
    "year": 2024,
    "authors": [
      "Lucas Wright",
      "Roxana Mike Muenster",
      "Briana Vecchione",
      "Tianyao Qu",
      "Pika",
      "Cai",
      "COMM/INFO 2450 Student Investigators",
      "Jacob Metcalf",
      "J. Nathan Matias"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender bias in algorithmic hiring systems and their impact on social groups, highlighting issues of discrimination and fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focus on algorithmic bias and social discrimination",
      "affected_populations": [
        "job seekers",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Audits and compliance recording of employer practices",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.01323v1",
    "title": "Structural Interventions and the Dynamics of Inequality",
    "year": 2024,
    "authors": [
      "Aurora Zhang",
      "Annette Hosoi"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses structural inequality, disparities in financial stability, and social change, indicating a focus on social inequality issues.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focus on structural and financial disparities",
      "affected_populations": [
        "disadvantaged groups",
        "under-resourced groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling decision thresholds and intervention effects",
      "geographic_focus": null,
      "ai_relationship": "AI as mechanism perpetuating inequality",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.00987v2",
    "title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement",
    "year": 2024,
    "authors": [
      "Wenjing Chang",
      "Kay Liu",
      "Philip S. Yu",
      "Jianjun Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems, focusing on societal discrimination related to sensitive attributes such as gender and ethnicity.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Fairness in AI decision-making",
      "affected_populations": [
        "demographic groups",
        "societal minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Disentanglement in GNNs for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.00591v2",
    "title": "Auditing for Racial Discrimination in the Delivery of Education Ads",
    "year": 2024,
    "authors": [
      "Basileal Imana",
      "Aleksandra Korolova",
      "John Heidemann"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in ad delivery algorithms, highlighting discrimination in education access. It addresses social discrimination and bias in AI systems affecting marginalized groups. The focus on racial discrimination in social media ads directly relates to social inequality.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "",
      "affected_populations": [
        "racial minorities",
        "students"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "New auditing method for ad delivery bias measurement",
      "geographic_focus": [
        "Meta platform (global)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.00393v1",
    "title": "Gender Bias Detection in Court Decisions: A Brazilian Case Study",
    "year": 2024,
    "authors": [
      "Raysa Benatti",
      "Fabiana Severi",
      "Sandra Avila",
      "Esther Luna Colombini"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias detection in court decisions, addressing gender-based social discrimination and inequality. It discusses biases affecting gender minorities and their access to rights, aligning with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Automated bias detection in legal texts",
      "geographic_focus": [
        "Brazil"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.20790v3",
    "title": "Intersectional Unfairness Discovery",
    "year": 2024,
    "authors": [
      "Gezheng Xu",
      "Qi Chen",
      "Charles Ling",
      "Boyu Wang",
      "Changjian Shui"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses unfairness in AI systems related to intersectional sensitive attributes, which are linked to social groups such as race, gender, and other social identities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on intersectional fairness in AI bias detection",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Generative AI",
        "Experiment"
      ],
      "methodology_detail": "Bias-guided generative network for intersectional bias discovery",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.20152v1",
    "title": "Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals",
    "year": 2024,
    "authors": [
      "Phillip Howard",
      "Kathleen C. Fraser",
      "Anahita Bhiwandiwalla",
      "Svetlana Kiritchenko"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates social biases related to race, gender, and physical attributes in AI-generated content, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focuses on social bias in multimodal AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "individuals with physical traits"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Large-scale counterfactual analysis of model outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.19701v2",
    "title": "Significance of Chain of Thought in Gender Bias Mitigation for English-Dravidian Machine Translation",
    "year": 2024,
    "authors": [
      "Lavanya Prahallad",
      "Radhika Mamidi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social fairness issue affecting gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias mitigation techniques in translation systems",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.19699v2",
    "title": "Fairness in AI-Driven Recruitment: Challenges, Metrics, Methods, and Future Directions",
    "year": 2024,
    "authors": [
      "Dena F. Mujtaba",
      "Nihar R. Mahapatra"
    ],
    "categories": [
      "cs.CY",
      "K.4.3; I.2.0; J.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI recruitment affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "job candidates",
        "minority groups"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "Fairness Metrics"
      ],
      "methodology_detail": "Review of bias and fairness assessment methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.18563v1",
    "title": "Interdisciplinary Expertise to Advance Equitable Explainable AI",
    "year": 2024,
    "authors": [
      "Chloe R. Bennett",
      "Heather Cole-Lewis",
      "Stephanie Farquhar",
      "Naama Haamel",
      "Boris Babenko",
      "Oran Lang",
      "Mat Fleck",
      "Ilana Traynis",
      "Charles Lau",
      "Ivor Horn",
      "Courtney Lyles"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias, equity, and interpretability in AI affecting marginalized populations, emphasizing social context and disparities.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on health equity and bias reduction",
      "affected_populations": [
        "health marginalized groups"
      ],
      "methodology": [
        "Interdisciplinary review",
        "Expert panel discussion"
      ],
      "methodology_detail": "Critical assessment of AI explanations from multiple perspectives",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.19300v3",
    "title": "Measuring and Mitigating Bias for Tabular Datasets with Multiple Protected Attributes",
    "year": 2024,
    "authors": [
      "Manh Khoi Duong",
      "Stefan Conrad"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and discrimination in datasets related to protected social attributes such as nationality, age, and sex, which are directly linked to social inequalities. It discusses fairness measures and mitigation strategies relevant to social groups affected by discrimination. The focus on intersectional bias and fairness in AI systems relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "nationality",
        "age",
        "disability"
      ],
      "other_detail": "Focus on intersectional discrimination in datasets",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly",
        "national groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias measurement and dataset de-biasing strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.18662v1",
    "title": "Understanding Intrinsic Socioeconomic Biases in Large Language Models",
    "year": 2024,
    "authors": [
      "Mina Arzaghi",
      "Florian Carichon",
      "Golnoosh Farnadi"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines socioeconomic biases in LLMs, highlighting disparities across demographic groups and the amplification of biases through intersectionality, directly addressing social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "class",
        "demographic"
      ],
      "other_detail": "Focuses on socioeconomic biases in AI models",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Quantifying biases using a large sentence dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.18657v3",
    "title": "The Efficacy of the Connect America Fund in Addressing US Internet Access Inequities",
    "year": 2024,
    "authors": [
      "Haarika Manda",
      "Varshika Srinivasavaradhan",
      "Laasya Koduru",
      "Kevin Zhang",
      "Xuanhe Zhou",
      "Udit Paul",
      "Elizabeth Belding",
      "Arpit Gupta",
      "Tejas N. Narechania"
    ],
    "categories": [
      "cs.NI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in broadband access across rural and underserved areas, highlighting inequities in digital connectivity, which are social inequalities related to geographic location and socioeconomic status.",
      "inequality_type": [
        "digital",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on rural and underserved communities",
      "affected_populations": [
        "rural residents",
        "underserved communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Using novel broadband plan data and ISP reports",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.18461v2",
    "title": "Why Algorithms Remain Unjust: Power Structures Surrounding Algorithmic Activity",
    "year": 2024,
    "authors": [
      "Andrew Balch"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.1; K.4.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social injustices related to algorithmic power structures, emphasizing economic domination and the need for empowering end users, which relates to social inequality issues.",
      "inequality_type": [
        "economic",
        "power",
        "social"
      ],
      "other_detail": "Focuses on power dynamics affecting social justice",
      "affected_populations": [
        "end users",
        "society"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Framework Application"
      ],
      "methodology_detail": "Uses Erik Olin Wright's power configuration framework",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.00046v2",
    "title": "Hate Speech Detection with Generalizable Target-aware Fairness",
    "year": 2024,
    "authors": [
      "Tong Chen",
      "Danny Wang",
      "Xurong Liang",
      "Marten Risius",
      "Gianluca Demartini",
      "Hongzhi Yin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in hate speech detection affecting targeted social groups, highlighting fairness issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "fairness"
      ],
      "other_detail": "Focus on algorithmic bias and fairness in social media moderation",
      "affected_populations": [
        "female",
        "black people"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training",
        "Natural Language Processing"
      ],
      "methodology_detail": "Hypernetwork-based fairness filtering for unseen targets",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.17921v1",
    "title": "Towards Clinical AI Fairness: Filling Gaps in the Puzzle",
    "year": 2024,
    "authors": [
      "Mingxuan Liu",
      "Yilin Ning",
      "Salinelat Teixayavong",
      "Xiaoxuan Liu",
      "Mayli Mertens",
      "Yuqing Shang",
      "Xin Li",
      "Di Miao",
      "Jie Xu",
      "Daniel Shu Wei Ting",
      "Lionel Tim-Ee Cheng",
      "Jasmine Chiat Ling Ong",
      "Zhen Ling Teo",
      "Ting Fang Tan",
      "Narrendar RaviChandran",
      "Fei Wang",
      "Leo Anthony Celi",
      "Marcus Eng Hock Ong",
      "Nan Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in healthcare AI, addressing social bias and disparities among demographic groups, highlighting issues of equity and fairness in social contexts.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in clinical AI applications",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing gaps in AI fairness research",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.17607v1",
    "title": "Advancing Cultural Inclusivity: Optimizing Embedding Spaces for Balanced Music Recommendations",
    "year": 2024,
    "authors": [
      "Armin Moradi",
      "Nicola Neophytou",
      "Golnoosh Farnadi"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in music recommendations related to demographic and cultural groups, highlighting fairness issues. It aims to reduce popularity bias that propagates cultural and demographic disparities. This relates to social inequality in cultural representation and fairness in AI systems.",
      "inequality_type": [
        "racial",
        "ethnic",
        "cultural",
        "demographic"
      ],
      "other_detail": "Bias mitigation in cultural representation",
      "affected_populations": [
        "underrepresented cultural groups",
        "minority artists"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Embedding space analysis and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.16860v1",
    "title": "Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks",
    "year": 2024,
    "authors": [
      "Yunqi Zhang",
      "Songda Li",
      "Chunyuan Deng",
      "Luyi Wang",
      "Hui Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in vision-language models, which relates to social gender inequality and discrimination. It discusses mitigating harmful stereotypes reinforced by AI systems, directly engaging with social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "addressing gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "System Design",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation framework and evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.16762v1",
    "title": "Addressing Discretization-Induced Bias in Demographic Prediction",
    "year": 2024,
    "authors": [
      "Evan Dong",
      "Aaron Schein",
      "Yixin Wang",
      "Nikhil Garg"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in demographic prediction, highlighting disparities in voter race imputation. It discusses how discretization bias affects minority counts, impacting social equity. The focus on racial bias in AI applications indicates a direct link to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias in demographic data due to AI discretization",
      "affected_populations": [
        "African-American voters"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Bias analysis and bias mitigation techniques",
      "geographic_focus": [
        "North Carolina"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.15760v1",
    "title": "GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction",
    "year": 2024,
    "authors": [
      "Virginia K. Felkner",
      "Jennifer A. Thompson",
      "Jonathan May"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "I.2.7; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in language models related to social groups, specifically focusing on social biases and fairness issues in AI systems affecting marginalized communities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Bias benchmark development involving social groups",
      "affected_populations": [
        "Jewish community",
        "social minorities"
      ],
      "methodology": [
        "Survey",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Using community surveys and AI annotation attempts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.15443v2",
    "title": "Fairness-Accuracy Trade-Offs: A Causal Perspective",
    "year": 2024,
    "authors": [
      "Drago Plecko",
      "Elias Bareinboim"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems, focusing on discrimination related to sensitive attributes like gender and race, and analyzes social bias issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "causal fairness constraints in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Causal Analysis",
        "Machine Learning",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Introducing causal fairness constraints and neural approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.15437v1",
    "title": "Learning about Data, Algorithms, and Algorithmic Justice on TikTok in Personally Meaningful Ways",
    "year": 2024,
    "authors": [
      "Luis Morales-Navarro",
      "Yasmin B. Kafai",
      "Ha Nguyen",
      "Kayla DesPortes",
      "Ralph Vacca",
      "Camillia Matuk",
      "Megan Silander",
      "Anna Amato",
      "Peter Woods",
      "Francisco Castro",
      "Mia Shaw",
      "Selin Akgun",
      "Christine Greenhow",
      "Antero Garcia"
    ],
    "categories": [
      "cs.CY",
      "K.3; K.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how youth engage with AI/ML systems on TikTok, considering ethics and algorithmic justice, and how these influence social interactions and advocacy, indicating a focus on social bias and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Focus on youth interactions and algorithmic justice",
      "affected_populations": [
        "young women",
        "people of color"
      ],
      "methodology": [
        "Qualitative Study",
        "Ethics Analysis"
      ],
      "methodology_detail": "Exploring youth perspectives and ethical considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.03133v3",
    "title": "Quantifying the Cross-sectoral Intersecting Discrepancies within Multiple Groups Using Latent Class Analysis Towards Fairness",
    "year": 2024,
    "authors": [
      "Yingfang Yuan",
      "Kefan Chen",
      "Mehdi Rizvi",
      "Lynne Baillie",
      "Wei Pang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities among ethnic groups across sectors, highlighting inequality and fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focus on cross-sectoral ethnic disparities",
      "affected_populations": [
        "minority ethnic groups",
        "non-minority groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Latent Class Analysis"
      ],
      "methodology_detail": "Quantifies discrepancies among social groups",
      "geographic_focus": [
        "England & Wales"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.14848v3",
    "title": "Local Causal Discovery for Structural Evidence of Direct Discrimination",
    "year": 2024,
    "authors": [
      "Jacqueline Maasch",
      "Kyra Gan",
      "Violet Chen",
      "Agni Orfanoudaki",
      "Nil-Jana Akpinar",
      "Fei Wang"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on causal pathways of unfairness and direct discrimination, addressing social bias issues in decision systems.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Analyzes fairness in AI decision-making systems",
      "affected_populations": [
        "racial minorities",
        "patients",
        "criminal defendants"
      ],
      "methodology": [
        "Causal Discovery",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Local causal discovery method for fairness analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.14725v1",
    "title": "A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results",
    "year": 2024,
    "authors": [
      "Karima Makhlouf",
      "Tamara Stefanovic",
      "Heber H. Arcolezi",
      "Catuscia Palamidessi"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how privacy-preserving AI impacts fairness, which relates to social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in machine learning decisions",
      "affected_populations": [
        "subgroups of individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes fairness metrics under privacy constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of fairness issues",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.14555v4",
    "title": "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Abhishek Kumar",
      "Sarfaroz Yunusov",
      "Ali Emami"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs related to social identity groups, reflecting social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on social biases in AI outputs",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "identity groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Develops metrics and tasks to measure biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social biases",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.14521v1",
    "title": "Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical Group Structure",
    "year": 2024,
    "authors": [
      "Gaurav Maheshwari",
      "Aur√©lien Bellet",
      "Pascal Denis",
      "Mikaela Keller"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on intersectional fairness in classification, addressing social bias and discrimination across social groups. It aims to improve fairness metrics, which relate directly to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness across social group intersections",
      "affected_populations": [
        "minority groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Data Augmentation",
        "Experiment"
      ],
      "methodology_detail": "Enhances intersectional fairness via hierarchical data augmentation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.06194v2",
    "title": "More Distinctively Black and Feminine Faces Lead to Increased Stereotyping in Vision-Language Models",
    "year": 2024,
    "authors": [
      "Messi H. J. Lee",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in VLMs, highlighting social stereotypes and their visual cues, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias amplification in AI perception of social groups",
      "affected_populations": [
        "racial minorities",
        "feminine individuals"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Computer Vision"
      ],
      "methodology_detail": "Analyzing stereotypes in image-based story generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.13166v2",
    "title": "FairLENS: Assessing Fairness in Law Enforcement Speech Recognition",
    "year": 2024,
    "authors": [
      "Yicheng Wang",
      "Mark Cusick",
      "Mohamed Laila",
      "Kate Puech",
      "Zhengping Ji",
      "Xia Hu",
      "Michael Wilson",
      "Noah Spitzer-Williams",
      "Bryan Wheeler",
      "Yasser Ibrahim"
    ],
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness and biases in AI speech recognition, addressing social fairness issues related to demographic groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness disparities across demographic groups",
      "affected_populations": [
        "demographic groups",
        "speech users"
      ],
      "methodology": [
        "Dataset Creation",
        "Fairness Evaluation",
        "Experiment"
      ],
      "methodology_detail": "Developed a fairness dataset and evaluation framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.12680v1",
    "title": "Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes",
    "year": 2024,
    "authors": [
      "Chiman Salavati",
      "Shannon Song",
      "Willmar Sosa Diaz",
      "Scott A. Hale",
      "Roberto E. Montenegro",
      "Fabricio Murai",
      "Shiri Dori-Hacohen"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to race, ethnicity, gender, and age in medical education, which are social inequality issues. It focuses on mitigating biases that can perpetuate disparities in health outcomes. The emphasis on fairness and bias detection in curricula relates directly to social discrimination concerns.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias mitigation in medical education",
      "affected_populations": [
        "minority patients",
        "medical students",
        "healthcare providers"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Expert Annotation"
      ],
      "methodology_detail": "Annotated medical texts for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.12372v2",
    "title": "DispaRisk: Auditing Fairness Through Usable Information",
    "year": 2024,
    "authors": [
      "Jonathan Vasquez",
      "Carlotta Domeniconi",
      "Huzefa Rangwala"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness, bias detection, and disparities in AI systems, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "health"
      ],
      "other_detail": "Addresses bias in machine learning datasets and models",
      "affected_populations": [
        "minority groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias risk assessment during ML pipeline stages",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.12312v3",
    "title": "A Principled Approach for a New Bias Measure",
    "year": 2024,
    "authors": [
      "Bruno Scarone",
      "Alfredo Viola",
      "Ren√©e J. Miller",
      "Ricardo Baeza-Yates"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias measurement in AI, addressing harmful societal impacts and discrimination, particularly in employment. It aims to quantify and mitigate bias, which relates to social inequality issues.",
      "inequality_type": [
        "employment",
        "social discrimination"
      ],
      "other_detail": "focus on bias in decision-making algorithms",
      "affected_populations": [
        "disadvantaged groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Development",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation framework development",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.12021v2",
    "title": "Can AI Relate: Testing Large Language Model Response for Mental Health Support",
    "year": 2024,
    "authors": [
      "Saadia Gabriel",
      "Isha Puri",
      "Xuhai Xu",
      "Matteo Malgaroli",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in LLM responses based on patient demographics, notably race, highlighting lower empathy for Black patients, which relates to racial bias and social inequality.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in AI responses",
      "affected_populations": [
        "Black patients"
      ],
      "methodology": [
        "Human evaluation",
        "Automatic quality metrics"
      ],
      "methodology_detail": "Clinician assessments and psychology-based metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.11320v1",
    "title": "Sampling Strategies for Mitigating Bias in Face Synthesis Methods",
    "year": 2024,
    "authors": [
      "Emmanouil Maragkoudakis",
      "Symeon Papadopoulos",
      "Iraklis Varlamis",
      "Christos Diou"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "68T99",
      "I.2; I.5"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender and age in face synthesis, highlighting social disparities. It examines how AI-generated images may reinforce underrepresented groups' marginalization. The focus on protected attributes aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Bias mitigation in AI-generated face images",
      "affected_populations": [
        "women",
        "young people",
        "elderly"
      ],
      "methodology": [
        "Experiment",
        "Sampling Strategy",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and sampling strategy evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.11290v3",
    "title": "MBIAS: Mitigating Bias in Large Language Models While Retaining Context",
    "year": 2024,
    "authors": [
      "Shaina Raza",
      "Ananya Raval",
      "Veronica Chatrath"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing biases and toxicity in LLM outputs across demographics, addressing social bias and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "minorities",
        "underrepresented groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Fine-tuning and evaluation of LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.11240v1",
    "title": "Testing the Performance of Face Recognition for People with Down Syndrome",
    "year": 2024,
    "authors": [
      "Christian Rathgeb",
      "Mathias Ibsen",
      "Denise Hartmann",
      "Simon Hradetzky",
      "Berglind √ìlafsd√≥ttir"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines facial recognition performance on individuals with Down syndrome, highlighting disparities in recognition accuracy, which relates to health and disability-related inequalities and fairness in AI systems.",
      "inequality_type": [
        "health",
        "disability"
      ],
      "other_detail": "Focuses on algorithm fairness for a disability group",
      "affected_populations": [
        "individuals with Down syndrome"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Collected facial images and evaluated recognition algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.11121v1",
    "title": "COVID-19's Unequal Toll: An assessment of small business impact disparities with respect to ethnorace in metropolitan areas in the US using mobility data",
    "year": 2024,
    "authors": [
      "Saad Mohammad Abrar",
      "Kazi Tasnim Zinat",
      "Naman Awasthi",
      "Vanessa Frias-Martinez"
    ],
    "categories": [
      "cs.CY",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines disparities in COVID-19 impacts across racial/ethnic groups, highlighting inequities in small business recovery. It analyzes how mobility restrictions affected different communities unequally, indicating social inequality focus.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial/ethnic disparities in pandemic impact",
      "affected_populations": [
        "Asian communities",
        "Black communities",
        "Hispanic communities",
        "White communities",
        "American Indian communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analysis of geolocated mobility data and visitation patterns",
      "geographic_focus": [
        "US metropolitan areas"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.14891v2",
    "title": "Auditing the Fairness of the US COVID-19 Forecast Hub's Case Prediction Models",
    "year": 2024,
    "authors": [
      "Saad Mohammad Abrar",
      "Naman Awasthi",
      "Daniel Smolyak",
      "Vanessa Frias-Martinez"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates model performance across social determinants like race and urbanization, highlighting disparities affecting minority and less urbanized groups, thus addressing social inequality and fairness issues.",
      "inequality_type": [
        "racial",
        "urban-rural",
        "health"
      ],
      "other_detail": "Focus on COVID-19 prediction disparities",
      "affected_populations": [
        "minority racial groups",
        "less urbanized areas"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes prediction errors across social groups",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.10256v1",
    "title": "Biasing & Debiasing based Approach Towards Fair Knowledge Transfer for Equitable Skin Analysis",
    "year": 2024,
    "authors": [
      "Anshul Pundhir",
      "Balasubramanian Raman",
      "Pravendra Singh"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models related to demographic traits such as skin tone and gender, which are social categories linked to inequality. It aims to mitigate biases that affect different social groups, promoting equitable treatment in skin disease diagnosis.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Bias and fairness in AI healthcare applications",
      "affected_populations": [
        "diverse skin tones",
        "gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation and fairness transfer techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.10355v1",
    "title": "Assessing the Impact of Case Correction Methods on the Fairness of COVID-19 Predictive Models",
    "year": 2024,
    "authors": [
      "Daniel Smolyak",
      "Saad Abrar",
      "Naman Awasthi",
      "Vanessa Frias-Martinez"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how case correction methods impact fairness across racial groups in COVID-19 prediction, highlighting social disparities.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial and socioeconomic disparities in health data",
      "affected_populations": [
        "majority-minority groups",
        "White counties"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes fairness and performance differences between groups",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.09821v2",
    "title": "Evaluating Algorithmic Bias in Models for Predicting Academic Performance of Filipino Students",
    "year": 2024,
    "authors": [
      "Valdemar ≈†v√°bensk√Ω",
      "M√©lina Verger",
      "Maria Mercedes T. Rodrigo",
      "Clarence James G. Monterozo",
      "Ryan S. Baker",
      "Miguel Zenon Nicanor Lerias Saavedra",
      "S√©bastien Lall√©",
      "Atsushi Shimada"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "K.3"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic bias based on regional background, addressing social fairness issues in AI systems affecting educational equity.",
      "inequality_type": [
        "geographic",
        "educational"
      ],
      "other_detail": "Bias based on regional background in education",
      "affected_populations": [
        "Filipino students"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predicting grades from LMS activity data",
      "geographic_focus": [
        "Philippines"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.13025v2",
    "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
    "year": 2024,
    "authors": [
      "Qingyang Ren",
      "Zilin Jiang",
      "Jinghan Cao",
      "Sijia Li",
      "Chiqu Li",
      "Yiyang Liu",
      "Shuning Huo",
      "Tiange He",
      "Yuan Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness, bias, and discrimination issues in AI systems affecting diverse social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "disability"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "minority groups",
        "disadvantaged consumers"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzes fairness challenges and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.09483v2",
    "title": "DemOpts: Fairness corrections in COVID-19 case prediction models",
    "year": 2024,
    "authors": [
      "Naman Awasthi",
      "Saad Abrar",
      "Daniel Smolyak",
      "Vanessa Frias-Martinez"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and ethnic biases in COVID-19 prediction models, addressing fairness and disparities across social groups.",
      "inequality_type": [
        "racial",
        "ethnic",
        "health"
      ],
      "other_detail": "Focus on bias correction in health-related AI models",
      "affected_populations": [
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection and de-biasing techniques in models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.08681v1",
    "title": "Achieving Fairness Through Channel Pruning for Dermatological Disease Diagnosis",
    "year": 2024,
    "authors": [
      "Qingpeng Kong",
      "Ching-Hao Chiu",
      "Dewen Zeng",
      "Yu-Jen Chen",
      "Tsung-Yi Ho",
      "Jingtong hu",
      "Yiyu Shi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models related to demographic attributes such as race, gender, and age, highlighting bias mitigation in medical diagnosis.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Fairness in medical AI diagnosis",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Channel pruning for fairness optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.19342v1",
    "title": "Sonos Voice Control Bias Assessment Dataset: A Methodology for Demographic Bias Assessment in Voice Assistants",
    "year": 2024,
    "authors": [
      "Chlo√© Sekkat",
      "Fanny Leroy",
      "Salima Mdhaffar",
      "Blake Perry Smith",
      "Yannick Est√®ve",
      "Joseph Dureau",
      "Alice Coucke"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper assesses demographic biases in voice assistants, highlighting performance disparities across social groups such as age, ethnicity, and dialectal region, which relate to social inequality issues.",
      "inequality_type": [
        "age",
        "ethnic",
        "geographic",
        "gender"
      ],
      "other_detail": "Focus on demographic fairness in speech technology",
      "affected_populations": [
        "elderly",
        "ethnic minorities",
        "regional dialect speakers"
      ],
      "methodology": [
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias assessment using demographic and performance metrics",
      "geographic_focus": [
        "North America"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.08477v1",
    "title": "Enhancing Gender-Inclusive Machine Translation with Neomorphemes and Large Language Models",
    "year": 2024,
    "authors": [
      "Andrea Piergentili",
      "Beatrice Savoldi",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, focusing on gender inclusivity and fairness, which are social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language translation",
      "affected_populations": [
        "non-binary individuals",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "evaluating translation models with new gender-inclusive resources",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.07011v1",
    "title": "Fair Graph Representation Learning via Sensitive Attribute Disentanglement",
    "year": 2024,
    "authors": [
      "Yuchang Zhu",
      "Jintang Li",
      "Zibin Zheng",
      "Liang Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems related to sensitive attributes like race and gender, which are central to social inequalities. It focuses on algorithmic fairness to prevent discrimination against social groups. The work aims to mitigate bias impacts on marginalized populations.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Algorithm Development"
      ],
      "methodology_detail": "Disentanglement of sensitive attributes in GNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06996v1",
    "title": "Quite Good, but Not Enough: Nationality Bias in Large Language Models -- A Case Study of ChatGPT",
    "year": 2024,
    "authors": [
      "Shucheng Zhu",
      "Weikang Wang",
      "Ying Liu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines nationality bias in language models, highlighting social biases related to nationality, which is a social demographic element. It discusses how biases in AI reflect real-world inequalities and cultural perspectives, impacting social groups. The focus on bias and fairness aligns with social inequality issues.",
      "inequality_type": [
        "nationality"
      ],
      "other_detail": "Bias in language models reflecting societal stereotypes",
      "affected_populations": [
        "nationalities",
        "cultural groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Expert Evaluation"
      ],
      "methodology_detail": "Analysis of generated discourses and bias evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06841v2",
    "title": "Bridging the Gap: Protocol Towards Fair and Consistent Affect Analysis",
    "year": 2024,
    "authors": [
      "Guanyu Hu",
      "Eleni Papadopoulou",
      "Dimitrios Kollias",
      "Paraskevi Tzouveli",
      "Jie Wei",
      "Xinyu Yang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness, bias, and demographic attributes in affect analysis, addressing social bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age",
        "health"
      ],
      "other_detail": "Bias mitigation in affective AI systems",
      "affected_populations": [
        "diverse demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes databases, annotates demographics, proposes protocols",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06433v6",
    "title": "Fair Mixed Effects Support Vector Machine",
    "year": 2024,
    "authors": [
      "Jan Pablo Burgard",
      "Jo√£o Vitor Pamplona"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "math.OC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in machine learning, focusing on social discrimination and clustered data biases, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias mitigation in social data",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness-aware modeling with mixed effects support vector machine",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06404v2",
    "title": "Inclusive content reduces racial and gender biases, yet non-inclusive content dominates popular culture",
    "year": 2024,
    "authors": [
      "Nouar AlDahoul",
      "Hazem Ibrahim",
      "Minsu Park",
      "Talal Rahwan",
      "Yasir Zaki"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in visual media, highlighting their role in shaping societal perceptions and stereotypes, which directly relate to social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on media representation and perception biases",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Survey",
        "Experiment"
      ],
      "methodology_detail": "Analyzes media content and perception biases over time",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06346v1",
    "title": "Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology",
    "year": 2024,
    "authors": [
      "Rishav Hada",
      "Safiya Husain",
      "Varun Gumma",
      "Harshita Diddee",
      "Aditya Yadavalli",
      "Agrima Seth",
      "Nidhi Kulkarni",
      "Ujwal Gadiraju",
      "Aditya Vashistha",
      "Vivek Seshadri",
      "Kalika Bali"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in Hindi language technology, highlighting social discrimination and bias issues affecting women, especially in marginalized communities.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender bias in language technology",
      "affected_populations": [
        "women",
        "rural communities"
      ],
      "methodology": [
        "Field Studies",
        "Computational Models",
        "Mining Techniques",
        "Qualitative Study"
      ],
      "methodology_detail": "Includes field studies with rural and low-income women",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.10367v1",
    "title": "Perceptions of Entrepreneurship Among Graduate Students: Challenges, Opportunities, and Cultural Biases",
    "year": 2024,
    "authors": [
      "Manuela Andreea Petrescu",
      "Dan Mircea Suciu"
    ],
    "categories": [
      "cs.CY",
      "K.3.2; J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gender discrimination, biases, and unequal opportunities for women, addressing social inequality aspects.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender biases in entrepreneurship perceptions",
      "affected_populations": [
        "women",
        "female entrepreneurs"
      ],
      "methodology": [
        "Survey"
      ],
      "methodology_detail": "Online exploratory surveys",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06221v1",
    "title": "For the Misgendered Chinese in Gender Bias Research: Multi-Task Learning with Knowledge Distillation for Pinyin Name-Gender Prediction",
    "year": 2024,
    "authors": [
      "Xiaocong Du",
      "Haipeng Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI tools affecting gender inference accuracy, which impacts gender equality and fairness in social research.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in Chinese name inference",
      "affected_populations": [
        "Chinese individuals",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Knowledge Distillation"
      ],
      "methodology_detail": "Multi-Task Learning Network with semantic embeddings",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06150v1",
    "title": "Lost in Transcription: Identifying and Quantifying the Accuracy Biases of Automatic Speech Recognition Systems Against Disfluent Speech",
    "year": 2024,
    "authors": [
      "Dena Mujtaba",
      "Nihar R. Mahapatra",
      "Megan Arney",
      "J. Scott Yaruss",
      "Hope Gerlach-Houck",
      "Caryn Herring",
      "Jia Bin"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in speech recognition affecting people who stutter, a health and disability group, highlighting issues of inclusivity and equitable participation in digital technology.",
      "inequality_type": [
        "health",
        "disability",
        "digital"
      ],
      "other_detail": "Bias in AI impacting marginalized speech communities",
      "affected_populations": [
        "people who stutter"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Performance evaluation on real and synthetic datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.05506v2",
    "title": "Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias",
    "year": 2024,
    "authors": [
      "Shan Chen",
      "Jack Gallifant",
      "Mingye Gao",
      "Pedro Moreira",
      "Nikolaj Munch",
      "Ajay Muthukkumar",
      "Arvind Rajan",
      "Jaya Kolluri",
      "Amelia Fiske",
      "Janna Hastings",
      "Hugo Aerts",
      "Brian Anthony",
      "Leo Anthony Celi",
      "William G. La Cava",
      "Danielle S. Bitterman"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to disease prevalence across demographic groups, highlighting disparities that reflect social inequalities in health and representation.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on health disparities in demographic groups",
      "affected_populations": [
        "demographic groups",
        "medical patients"
      ],
      "methodology": [
        "Benchmark Framework",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing bias and real-world data comparison",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.05049v1",
    "title": "Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources",
    "year": 2024,
    "authors": [
      "Lasse Hyldig Hansen",
      "Nikolaj Andersen",
      "Jack Gallifant",
      "Liam G. McCoy",
      "James K Stone",
      "Nura Izath",
      "Marcela Aguirre-Jerez",
      "Danielle S Bitterman",
      "Judy Gichoya",
      "Leo Anthony Celi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in online texts related to diseases, highlighting disparities in associations and representation, which directly relate to social inequalities. It discusses biases in AI training data that can perpetuate health and social disparities. The focus on race and gender biases in health-related information aligns with social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Biases in online health-related textual data",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Large-scale textual analysis",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed web sources for disease and demographic associations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.04816v4",
    "title": "Testing the Fairness-Accuracy Improvability of Algorithms",
    "year": 2024,
    "authors": [
      "Eric Auerbach",
      "Annie Liang",
      "Kyohei Okumura",
      "Max Tabord-Meehan"
    ],
    "categories": [
      "econ.EM",
      "cs.DS",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic disparate impact, fairness, and reducing social bias, which relate directly to social inequalities such as racial and health disparities.",
      "inequality_type": [
        "racial",
        "health",
        "social bias"
      ],
      "other_detail": "Focus on algorithmic fairness and impact reduction",
      "affected_populations": [
        "health patients",
        "social groups"
      ],
      "methodology": [
        "Econometric Framework",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Testing fairness-improvability under constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.04756v1",
    "title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Chu Fei Luo",
      "Ahmad Ghawanmeh",
      "Xiaodan Zhu",
      "Faiza Khan Khattak"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in language models, which relate to social discrimination and fairness issues affecting groups based on race, gender, and other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on bias amplification in AI systems",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Experiment",
        "Adversarial Attacking"
      ],
      "methodology_detail": "Using knowledge graphs to induce bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.04652v1",
    "title": "AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large Language Models",
    "year": 2024,
    "authors": [
      "Yinru Long",
      "Zilin Ma",
      "Yiyang Mei",
      "Zhaoyuan Su"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social discrimination and bias against LGBTQ+ individuals in AI systems, focusing on mental health support and affirmativeness, which relates to social inequality and bias issues.",
      "inequality_type": [
        "gender",
        "health",
        "disability"
      ],
      "other_detail": "Focuses on LGBTQ+ mental health and social bias",
      "affected_populations": [
        "LGBTQ+ community"
      ],
      "methodology": [
        "Qualitative Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Framework development and benchmarking of LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.04623v1",
    "title": "The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models",
    "year": 2024,
    "authors": [
      "Abeba Birhane",
      "Sepehr Dehdashtian",
      "Vinay Uday Prabhu",
      "Vishnu Boddeti"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias and misclassification in AI models, highlighting racial disparities and potential harms affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias amplification due to dataset scaling",
      "affected_populations": [
        "Black men",
        "Latino men"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement using facial dataset and model evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06692v1",
    "title": "Analyzing Language Bias Between French and English in Conventional Multilingual Sentiment Analysis Models",
    "year": 2024,
    "authors": [
      "Ethan Parker Wong",
      "Faten M'hiri"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in multilingual NLP systems, which relate to social fairness issues, particularly language bias, affecting equitable treatment across linguistic groups.",
      "inequality_type": [
        "linguistic",
        "social bias"
      ],
      "other_detail": "Focus on language fairness in AI systems",
      "affected_populations": [
        "French speakers",
        "English speakers"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Fairness Assessment"
      ],
      "methodology_detail": "Bias analysis using SVM, Naive Bayes, and Fairlearn",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2407.08740v1",
    "title": "Reimagining AI: Exploring Speculative Design Workshops for Supporting BIPOC Youth Critical AI Literacies",
    "year": 2024,
    "authors": [
      "Sadhbh Kenny",
      "Alissa N. Antle"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on marginalized BIPOC youth, social justice, and AI biases, addressing racial and educational inequalities.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focus on marginalized communities and AI literacy",
      "affected_populations": [
        "BIPOC youth",
        "marginalized communities"
      ],
      "methodology": [
        "Qualitative Study",
        "Workshop Analysis",
        "Surveys",
        "Focus Groups"
      ],
      "methodology_detail": "Analyzes workshop data and youth perceptions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.04412v3",
    "title": "The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring",
    "year": 2024,
    "authors": [
      "Lena Armstrong",
      "Abbey Liu",
      "Stephen MacNeil",
      "Dana√´ Metaxa"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in AI models affecting hiring, highlighting social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Biases in AI reflect societal inequalities and stereotypes",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias assessment through resume scoring and generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.06687v3",
    "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation Attributes",
    "year": 2024,
    "authors": [
      "Damin Zhang",
      "Yi Zhang",
      "Geetanjali Bihani",
      "Julia Rayz"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender stereotypes in LLMs, highlighting biases related to gender, a key aspect of social inequality. It discusses fairness and bias in AI systems, which impact social groups differently.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender stereotypes in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Question answering framework and dataset construction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.02971v1",
    "title": "Achieving Narrative Change Through AR: Displacing the Single Story to Create Spatial Justice",
    "year": 2024,
    "authors": [
      "Janice Tisha Samuels"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses narrative change to address bias and single stories, which relate to social discrimination and inequality, particularly in the context of youth gun violence prevention.",
      "inequality_type": [
        "racial",
        "social",
        "educational"
      ],
      "other_detail": "Focus on narrative and bias reduction",
      "affected_populations": [
        "youth",
        "marginalized groups"
      ],
      "methodology": [
        "Augmented Reality",
        "Qualitative Study"
      ],
      "methodology_detail": "Using AR artifacts for narrative change",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.02490v3",
    "title": "Software Fairness Debt",
    "year": 2024,
    "authors": [
      "Ronnie de Souza Santos",
      "Felipe Fronchetti",
      "Savio Freire",
      "Rodrigo Spinola"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in software systems that lead to discrimination and inequalities affecting individuals and communities, highlighting societal impacts.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Bias and fairness in software systems",
      "affected_populations": [
        "individuals",
        "communities"
      ],
      "methodology": [
        "Scoping Study"
      ],
      "methodology_detail": "Literature review of bias causes and impacts",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.02010v1",
    "title": "The Trade-off between Performance, Efficiency, and Fairness in Adapter Modules for Text Classification",
    "year": 2024,
    "authors": [
      "Minh Duc Bui",
      "Katharina von der Wense"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness and bias in NLP models, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "sensitive groups",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Fairness Analysis"
      ],
      "methodology_detail": "Evaluates bias and fairness across model configurations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.15788v1",
    "title": "Towards Fairness in Provably Communication-Efficient Federated Recommender Systems",
    "year": 2024,
    "authors": [
      "Kirandeep Kaur",
      "Sujit Gujar",
      "Shweta Jain"
    ],
    "categories": [
      "cs.IR",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in federated recommender systems, focusing on demographic bias reduction and equitable treatment of different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "informational"
      ],
      "other_detail": "Fairness in AI systems without revealing protected attributes",
      "affected_populations": [
        "social groups",
        "users of recommender systems"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias reduction and fairness techniques in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.01790v1",
    "title": "Understanding Position Bias Effects on Fairness in Social Multi-Document Summarization",
    "year": 2024,
    "authors": [
      "Olubusayo Olabisi",
      "Ameeta Agrawal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias related to racial/linguistic groups in social media summarization, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "linguistic",
        "social"
      ],
      "other_detail": "Focus on fairness across diverse social groups",
      "affected_populations": [
        "African-American English speakers",
        "Hispanic speakers",
        "White speakers"
      ],
      "methodology": [
        "Empirical Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzing input document order effects on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.01286v1",
    "title": "Data Feminism for AI",
    "year": 2024,
    "authors": [
      "Lauren Klein",
      "Catherine D'Ignazio"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; K.4.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses equitable AI research, addressing power imbalances, harms, and exclusion, which relate to social inequalities such as discrimination and bias.",
      "inequality_type": [
        "gender",
        "racial",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on power, bias, and social justice in AI",
      "affected_populations": [
        "marginalized groups",
        "social minorities"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing principles and ethical frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.01273v2",
    "title": "Towards Inclusive Face Recognition Through Synthetic Ethnicity Alteration",
    "year": 2024,
    "authors": [
      "Praveen Kumar Chandaliya",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Zahid Akhtar",
      "Christoph Busch"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition systems, aiming to mitigate ethnicity-related disparities.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on reducing bias in AI face recognition systems",
      "affected_populations": [
        "ethnic minorities",
        "underrepresented groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Synthetic data generation and performance evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.00910v1",
    "title": "De-Biasing Models of Biased Decisions: A Comparison of Methods Using Mortgage Application Data",
    "year": 2024,
    "authors": [
      "Nicholas Tenev"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "econ.EM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in decision-making related to ethnicity in mortgage applications, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial bias in financial decision processes",
      "affected_populations": [
        "ethnic minorities",
        "loan applicants"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Simulated bias addition and bias mitigation comparison",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.00588v1",
    "title": "Are Models Biased on Text without Gender-related Language?",
    "year": 2024,
    "authors": [
      "Catarina G Bel√©m",
      "Preethi Seshadri",
      "Yasaman Razeghi",
      "Sameer Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, a form of social inequality, by examining biases beyond stereotypical language, highlighting issues of gender fairness in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops a new evaluation framework and benchmarks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.00335v1",
    "title": "Finding the white male: The prevalence and consequences of algorithmic gender and race bias in political Google searches",
    "year": 2024,
    "authors": [
      "Tobias Rohrbach",
      "Mykola Makhortykh",
      "Maryna Sydorova"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how AI-driven search engines reflect and reinforce racial and gender biases, impacting perceptions and reinforcing structural inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Focuses on representation and perception in political information",
      "affected_populations": [
        "women",
        "non-white politicians"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Audits and online experiments on search results and perceptions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.18276v1",
    "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
    "year": 2024,
    "authors": [
      "Malur Narayan",
      "John Pasmore",
      "Elton Sampaio",
      "Vijay Raghavan",
      "Gabriella Waters"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "D.1; I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on racial bias in large language models, addressing social discrimination and fairness issues affecting racial groups.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "Focus on racial bias measurement and mitigation",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement and mitigation strategies in AI",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2405.09543v1",
    "title": "Algorithmic Fairness: A Tolerance Perspective",
    "year": 2024,
    "authors": [
      "Renqiang Luo",
      "Tao Tang",
      "Feng Xia",
      "Jiaying Liu",
      "Chengpei Xu",
      "Leo Yu Zhang",
      "Wei Xiang",
      "Chengqi Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "68T01, 68W40",
      "I.2.6; K.4.2; H.1.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness and social consequences, addressing social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on fairness implications across social groups",
      "affected_populations": [
        "discriminated groups",
        "social minorities"
      ],
      "methodology": [
        "Literature Review",
        "Systematic Review"
      ],
      "methodology_detail": "Analyzes existing fairness literature and industry case studies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.17218v3",
    "title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes",
    "year": 2024,
    "authors": [
      "Mahammed Kamruzzaman",
      "Gene Louis Kim"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in language models, which relate to social discrimination and inequality, particularly gender bias. It discusses reducing biases that can perpetuate stereotypes across social groups. The focus on social bias reduction aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on social bias in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Comparative prompting strategies on bias datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.16663v4",
    "title": "Conditional Fairness for Generative AIs",
    "year": 2024,
    "authors": [
      "Chih-Hong Cheng",
      "Harald Ruess",
      "Changshun Wu",
      "Xingyu Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.LO",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness concerns in generative AI, focusing on social bias and discrimination in outputs, which relate to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Fairness in AI outputs across social groups",
      "affected_populations": [
        "minority groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bounding worst-case unfairness in generated outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.16123v1",
    "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
    "year": 2024,
    "authors": [
      "Eric Slyman",
      "Stefan Lee",
      "Scott Cohen",
      "Kushal Kafle"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "I.4.10; I.2.7; E.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in datasets and models related to social fairness, addressing disparities in AI's impact on social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focuses on bias mitigation in vision-language datasets",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Evaluation",
        "Dataset Analysis"
      ],
      "methodology_detail": "Evaluates bias metrics and fairness improvements",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.15418v1",
    "title": "Machine Learning Techniques with Fairness for Prediction of Completion of Drug and Alcohol Rehabilitation",
    "year": 2024,
    "authors": [
      "Karen Roberts-Licklider",
      "Theodore Trafalis"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study focuses on fairness and bias mitigation in predicting rehabilitation outcomes using demographic data, addressing social bias issues related to race, gender, and socioeconomic factors.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Bias mitigation in social health data",
      "affected_populations": [
        "drug users",
        "rehabilitation patients"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Measures",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness metrics and bias mitigation techniques applied",
      "geographic_focus": [
        "Oklahoma"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.15149v1",
    "title": "Bias patterns in the application of LLMs for clinical decision support: A comprehensive study",
    "year": 2024,
    "authors": [
      "Raphael Poulain",
      "Hamed Fayyaz",
      "Rahmatollah Beheshti"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to protected attributes like race and discusses their impact on clinical decision support, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "health",
        "educational"
      ],
      "other_detail": "Focuses on bias in healthcare AI applications",
      "affected_populations": [
        "patients",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Red-teaming",
        "Bias Evaluation"
      ],
      "methodology_detail": "Evaluates LLMs across clinical datasets with bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.14379v1",
    "title": "Penn & Slavery Project's Augmented Reality Tour: Augmenting a Campus to Reveal a Hidden History",
    "year": 2024,
    "authors": [
      "VanJessica Gladney",
      "Breanna Moore",
      "Kathleen Brown"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The project investigates historical and modern inequalities related to slavery, wealth gaps, and social justice, directly addressing racial and socioeconomic disparities.",
      "inequality_type": [
        "racial",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focus on historical and modern social injustices",
      "affected_populations": [
        "Black community",
        "Penn students",
        "Philadelphia residents"
      ],
      "methodology": [
        "Historical Research",
        "Community Outreach",
        "Digital App Development"
      ],
      "methodology_detail": "Combines historical investigation with digital storytelling",
      "geographic_focus": [
        "Philadelphia",
        "University of Pennsylvania"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.14050v1",
    "title": "Unlawful Proxy Discrimination: A Framework for Challenging Inherently Discriminatory Algorithms",
    "year": 2024,
    "authors": [
      "Hilde Weerts",
      "Aislinn Kelly-Lyth",
      "Reuben Binns",
      "Jeremias Adams-Prassl"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic discrimination linked to protected characteristics, implying social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "disability"
      ],
      "other_detail": "Focus on direct discrimination in algorithms",
      "affected_populations": [
        "minority groups",
        "protected classes"
      ],
      "methodology": [
        "Legal Analysis",
        "Computer Science Literature Review"
      ],
      "methodology_detail": "Analyzes legal and technical frameworks for discrimination detection",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.13634v3",
    "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
    "year": 2024,
    "authors": [
      "Resmi Ramachandranpillai",
      "Md Fahim Sikder",
      "David Bergstr√∂m",
      "Fredrik Heintz"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in healthcare AI, impacting social groups.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in synthetic healthcare data generation",
      "affected_populations": [
        "patients",
        "underrepresented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "GAN-based synthetic data with fairness constraints",
      "geographic_focus": [
        "MIMIC-III database (US)"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.12812v3",
    "title": "Algorithmic Changes Are Not Enough: Evaluating the Removal of Race Adjustment from the eGFR Equation",
    "year": 2024,
    "authors": [
      "Marika M. Cusick",
      "Glenn M. Chertow",
      "Douglas K. Owens",
      "Michelle Y. Williams",
      "Sherri Rose"
    ],
    "categories": [
      "cs.CY",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in CKD care and the impact of removing race adjustment from clinical algorithms, addressing racial inequality in healthcare.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in medical decision-making",
      "affected_populations": [
        "Black or African American"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of healthcare data pre- and post-implementation",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.12143v1",
    "title": "The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action",
    "year": 2024,
    "authors": [
      "Hilde Weerts",
      "Rapha√´le Xenidis",
      "Fabien Tarissan",
      "Henrik Palmer Olsen",
      "Mykola Pechenizkiy"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness and discrimination, which relate to social inequalities such as race, gender, and social bias. It examines legal and ethical issues surrounding fairness interventions in AI systems, impacting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on fairness interventions and legal implications",
      "affected_populations": [
        "discriminated groups",
        "social minorities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Ethics Analysis"
      ],
      "methodology_detail": "Legal and conceptual framework analysis",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.12076v1",
    "title": "Evolutionary Multi-Objective Optimisation for Fairness-Aware Self Adjusting Memory Classifiers in Data Streams",
    "year": 2024,
    "authors": [
      "Pivithuru Thejan Amarasinghe",
      "Diem Pham",
      "Binh Tran",
      "Su Nguyen",
      "Yuan Sun",
      "Damminda Alahakoon"
    ],
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in algorithmic decision-making related to sensitive attributes like race and gender, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "discrimination"
      ],
      "other_detail": "Focus on fairness in data stream classification",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Evolutionary Multi-Objective Optimisation",
        "Experiment"
      ],
      "methodology_detail": "Combines self-adjusting memory classifiers with evolutionary optimisation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.11782v1",
    "title": "REQUAL-LM: Reliability and Equity through Aggregation in Large Language Models",
    "year": 2024,
    "authors": [
      "Sana Ebrahimi",
      "Nima Shahbazi",
      "Abolfazl Asudeh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating bias and promoting equity in LLM outputs, specifically addressing social biases affecting minority groups.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "social"
      ],
      "other_detail": "Addresses bias and fairness in AI outputs",
      "affected_populations": [
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Aggregation",
        "Monte Carlo sampling"
      ],
      "methodology_detail": "Bias mitigation via aggregation and sampling techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.11726v1",
    "title": "Investigating Gender Bias in Turkish Language Models",
    "year": 2024,
    "authors": [
      "Orhun Caglidil",
      "Malte Ostendorff",
      "Georg Rehm"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias and ethnic bias in language models, addressing social discrimination and fairness issues related to gender and ethnicity.",
      "inequality_type": [
        "gender",
        "ethnic"
      ],
      "other_detail": "Focuses on linguistic and ethnic biases in AI models",
      "affected_populations": [
        "women",
        "Kurdish people"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Extends bias evaluation frameworks to Turkish language",
      "geographic_focus": [
        "Turkey"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.11596v3",
    "title": "Urban highways are barriers to social ties",
    "year": 2024,
    "authors": [
      "Luca Maria Aiello",
      "Anastassia Vybornova",
      "S√°ndor Juh√°sz",
      "Michael Szell",
      "Eszter Bok√°nyi"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how urban highways impact social connectivity, highlighting racial and spatial segregation issues, especially in Black neighborhoods, thus addressing racial and geographic inequalities.",
      "inequality_type": [
        "racial",
        "geographic"
      ],
      "other_detail": "Focus on racial segregation and spatial inequality",
      "affected_populations": [
        "Black communities",
        "urban residents"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using geolocated social network data and Barrier Score",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.11305v1",
    "title": "AR for Sexual Violence: Maintaining Ethical Balance While Enhancing Empathy",
    "year": 2024,
    "authors": [
      "Chunwei Lin"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender justice and awareness of sexual violence, which relate to gender inequality and social discrimination.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender-based violence and social awareness",
      "affected_populations": [
        "women",
        "sexual violence victims"
      ],
      "methodology": [
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "AR experience development and user engagement study",
      "geographic_focus": [
        "Taiwan"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.10989v1",
    "title": "FairSSD: Understanding Bias in Synthetic Speech Detectors",
    "year": 2024,
    "authors": [
      "Amit Kumar Singh Yadav",
      "Kratika Bhagtani",
      "Davide Salvi",
      "Paolo Bestagini",
      "Edward J. Delp"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in synthetic speech detectors related to gender, age, and accent, which are social categories, and assesses fairness across different social groups.",
      "inequality_type": [
        "gender",
        "age",
        "accent"
      ],
      "other_detail": "Bias in AI systems affecting social groups",
      "affected_populations": [
        "women",
        "older adults",
        "non-native speakers"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Evaluates bias using extensive speech datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.10942v2",
    "title": "What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning",
    "year": 2024,
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates sources of inequality related to sensitive attributes like race and gender in reinforcement learning, addressing social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and inequality in AI decision-making",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Causal Analysis",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Decomposition of causal effects and counterfactual evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.10508v4",
    "title": "White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs",
    "year": 2024,
    "authors": [
      "Yixin Wan",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases related to gender and race in language models, reflecting social inequalities. It analyzes how AI-generated content perpetuates or amplifies these biases, aligning with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Intersectional bias analysis",
      "affected_populations": [
        "Black females",
        "minority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias evaluation benchmark and classifier development",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.09356v1",
    "title": "LLeMpower: Understanding Disparities in the Control and Access of Large Language Models",
    "year": 2024,
    "authors": [
      "Vishwas Sathish",
      "Hannah Lin",
      "Aditya K Kamath",
      "Anish Nyayachavadi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "K.4.0; K.7.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in access and control of LLMs across nations and organizations, highlighting issues of concentration of power and potential inequities. It discusses ethical implications and future directions for equitable access, indicating a focus on social inequality.",
      "inequality_type": [
        "economic",
        "digital",
        "informational",
        "geographic"
      ],
      "other_detail": "Focus on access and control disparities in AI technology",
      "affected_populations": [
        "nations",
        "organizations",
        "individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes requirements and access disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.09164v1",
    "title": "A computational model for gender asset gap management with a focus on gender disparity in land acquisition and land tenure security",
    "year": 2024,
    "authors": [
      "Oluwatosin Ogundare",
      "Lewis Njualem"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender disparities in land ownership and security, a social inequality issue.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on land acquisition and tenure security",
      "affected_populations": [
        "women",
        "cultural groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Develops a measurement framework incorporating cultural and policy factors",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.11867v1",
    "title": "Not as Simple as It Looked: Are We Concluding for Biased Arrest Practices?",
    "year": 2024,
    "authors": [
      "Murat Ozer",
      "Halil Akbas",
      "Ismail Onat",
      "Mehmet Bastug",
      "Arif Akgul",
      "Nelly ElSayed",
      "Zag ElSayed",
      "Multu Koseli",
      "Niyazi Ekici"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes racial disparities in arrests, focusing on neighborhood socioeconomic factors, thus addressing racial inequality.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on neighborhood socioeconomic influences on racial disparities",
      "affected_populations": [
        "racial minorities",
        "urban residents"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analyzes police data and neighborhood characteristics",
      "geographic_focus": [
        "Cincinnati"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.08760v4",
    "title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models",
    "year": 2024,
    "authors": [
      "Siyang Liu",
      "Trish Maturi",
      "Bowen Yi",
      "Siqi Shen",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines age bias in LLMs, highlighting social bias related to age, a social group. It discusses disparities in AI responses across age groups, addressing social discrimination and bias issues.",
      "inequality_type": [
        "age"
      ],
      "other_detail": "Focuses on age bias in AI systems",
      "affected_populations": [
        "younger",
        "older"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes survey data and prompt responses",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.08592v3",
    "title": "Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized",
    "year": 2024,
    "authors": [
      "Shomik Jain",
      "Kathleen Creel",
      "Ashia Wilson"
    ],
    "categories": [
      "cs.CY",
      "K.4.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses resource allocation fairness, which relates to social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "fairness"
      ],
      "other_detail": "Focuses on fairness in resource distribution",
      "affected_populations": [
        "individuals with scarce resources"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Proposes stochastic procedures for fair allocation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.08254v3",
    "title": "Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models",
    "year": 2024,
    "authors": [
      "Zeyu Yang",
      "Han Yu",
      "Peikun Guo",
      "Khadija Zanna",
      "Xiaoxue Yang",
      "Akane Sano"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating bias and ensuring fairness in synthetic data generation, addressing social discrimination related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI-generated tabular data",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Uses diffusion models with fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.07475v2",
    "title": "Laissez-Faire Harms: Algorithmic Biases in Generative Language Models",
    "year": 2024,
    "authors": [
      "Evan Shieh",
      "Faye-Marie Vassel",
      "Cassidy Sugimoto",
      "Thema Monroe-White"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models affecting minoritized social groups, highlighting discrimination and stereotyping that lead to psychological harms, directly addressing social inequalities related to race, gender, and sexual orientation.",
      "inequality_type": [
        "racial",
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Biases impacting intersectional minoritized identities",
      "affected_populations": [
        "minoritized individuals",
        "racial groups",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzing bias prevalence in generated texts",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2406.02480v1",
    "title": "Fairness Evolution in Continual Learning for Medical Imaging",
    "year": 2024,
    "authors": [
      "Marina Ceccon",
      "Davide Dalle Pezze",
      "Alessandro Fabris",
      "Gian Antonio Susto"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in medical AI models across social groups, addressing fairness and disparities related to sensitive attributes such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on fairness in medical imaging AI models",
      "affected_populations": [
        "patients by race",
        "patients by gender",
        "patients by socioeconomic status"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluation of fairness metrics over multiple tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.06717v1",
    "title": "Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent",
    "year": 2024,
    "authors": [
      "Jennifer Mickel"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial categories and racialization processes, which relate to racial inequality and social discrimination. It discusses potential harms and biases in AI systems affecting racial groups, highlighting social fairness issues.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racialization and representation in datasets",
      "affected_populations": [
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Framework Development",
        "Analysis"
      ],
      "methodology_detail": "Develops CIRCSheets framework for transparency",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.06621v1",
    "title": "What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models",
    "year": 2024,
    "authors": [
      "Jeongrok Yu",
      "Seong Ug Kim",
      "Jacob Choi",
      "Jinho D. Choi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in multilingual language models, addressing gender discrimination in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Multilingual bias evaluation methods and scoring metrics",
      "geographic_focus": [
        "Chinese",
        "English",
        "German",
        "Portuguese",
        "Spanish"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.06619v1",
    "title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations",
    "year": 2024,
    "authors": [
      "Jane Dwivedi-Yu",
      "Raaz Dwivedi",
      "Timo Schick"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases in language models related to gender, highlighting subtle and commonplace biases such as discussing appearances with women, which relates to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in language models",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Counterfactual paired evaluation with variability measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.05678v3",
    "title": "Flexible Fairness-Aware Learning via Inverse Conditional Permutation",
    "year": 2024,
    "authors": [
      "Yuheng Lai",
      "Leying Guan"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in algorithms related to sensitive attributes like race and gender, addressing social bias issues. It aims to promote equitable treatment across multiple social groups. The methodology targets reducing unfair impacts of AI systems on marginalized populations.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Multidimensional fairness in algorithmic decision-making",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Learning",
        "Fairness-aware Learning"
      ],
      "methodology_detail": "Integrates adversarial training with inverse permutation scheme",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.04838v2",
    "title": "Data Bias According to Bipol: Men are Naturally Right and It is the Role of Women to Follow Their Lead",
    "year": 2024,
    "authors": [
      "Irene Pagliai",
      "Goya van Boven",
      "Tosin Adewumi",
      "Lama Alkhaled",
      "Namrata Gurung",
      "Isabella S√∂dergren",
      "Elisa Barney"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social bias and prejudice related to gender in datasets, highlighting gender bias in AI, which relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI datasets",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias measurement and dataset benchmarking",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.04814v4",
    "title": "Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing Biased Rules",
    "year": 2024,
    "authors": [
      "Yi Zhang",
      "Dongyuan Lu",
      "Jitao Sang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on biases related to social attributes like gender and race, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on fairness and bias removal in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Bayesian Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias removal during inference without retraining",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.04534v2",
    "title": "Impact of Fairness Regulations on Institutions' Policies and Population Qualifications",
    "year": 2024,
    "authors": [
      "Hamidreza Montaseri",
      "Amin Gohari"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness regulations in algorithmic systems affecting social groups, addressing discrimination and disparities. It examines how policies impact population qualifications and equity, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focus on algorithmic fairness and social disparities",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes conditions and effects of discrimination penalties",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.03833v1",
    "title": "An ExplainableFair Framework for Prediction of Substance Use Disorder Treatment Completion",
    "year": 2024,
    "authors": [
      "Mary M. Lucas",
      "Xiaoyang Wang",
      "Chia-Hsuan Chang",
      "Christopher C. Yang",
      "Jacqueline E. Braughton",
      "Quyen M. Ngo"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias mitigation in healthcare AI, focusing on race and sex, which are social categories linked to inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Addresses bias in healthcare AI models",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Mitigation",
        "Explainability"
      ],
      "methodology_detail": "Bias mitigation and feature importance visualization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.03800v1",
    "title": "Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement",
    "year": 2024,
    "authors": [
      "Mukund Telukunta",
      "Sukruth Rao",
      "Gabriella Stickney",
      "Venkata Sriram Siddardh Nadendla",
      "Casey Canfield"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social fairness and biases in kidney placement, involving social opinions and group fairness notions, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "health",
        "social",
        "fairness"
      ],
      "other_detail": "Focus on social fairness in medical decision-making",
      "affected_populations": [
        "patients",
        "donors",
        "medical communities"
      ],
      "methodology": [
        "Survey",
        "Algorithm Development",
        "Simulation Experiments"
      ],
      "methodology_detail": "Learning fairness preferences from social feedback data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.03192v2",
    "title": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers",
    "year": 2024,
    "authors": [
      "Yuan Wang",
      "Xuyang Wu",
      "Hsin-Tai Wu",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in LLM rankings concerning protected attributes like gender and geographic location, addressing social bias and inequality issues.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Focus on representation bias in search outcomes",
      "affected_populations": [
        "gender groups",
        "geographic communities"
      ],
      "methodology": [
        "Empirical Analysis",
        "Dataset Evaluation"
      ],
      "methodology_detail": "Using TREC Fair Ranking dataset to assess bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.03086v1",
    "title": "Auditing the Use of Language Models to Guide Hiring Decisions",
    "year": 2024,
    "authors": [
      "Johann D. Gaebler",
      "Sharad Goel",
      "Aziz Huq",
      "Prasanna Tambe"
    ],
    "categories": [
      "stat.AP",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial and gender disparities in AI-driven hiring assessments, addressing social bias and inequality in employment decisions.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Focus on bias in algorithmic candidate evaluation",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using correspondence experiments to detect bias",
      "geographic_focus": [
        "large public school district"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.01857v1",
    "title": "Detecting Gender Bias in Course Evaluations",
    "year": 2024,
    "authors": [
      "Sarah Lindau",
      "Linnea Nilsson"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in course evaluations, addressing gender inequality and social bias in AI analysis of textual data.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "students",
        "examiners"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes textual differences based on examiner gender",
      "geographic_focus": [
        "English",
        "Swedish"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.01768v2",
    "title": "Stereotype Detection in LLMs: A Multiclass, Explainable, and Benchmark-Driven Approach",
    "year": 2024,
    "authors": [
      "Zekun Wu",
      "Sahan Bulathwela",
      "Maria Perez-Ortiz",
      "Adriano Soares Koshiyama"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting stereotypes related to race, gender, and other social categories, addressing bias and fairness in AI systems, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on stereotypes in language models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Explainable AI",
        "Dataset Creation"
      ],
      "methodology_detail": "Uses classifiers, explainability tools, and benchmark datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.01030v3",
    "title": "Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation",
    "year": 2024,
    "authors": [
      "Yixin Wan",
      "Arjun Subramonian",
      "Anaelia Ovalle",
      "Zongyu Lin",
      "Ashima Suvarna",
      "Christina Chance",
      "Hritik Bansal",
      "Rebecca Pattichis",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI models related to gender, skintone, and geo-cultural aspects, which are social categories, and addresses social bias and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "geographic"
      ],
      "other_detail": "Focuses on social bias in AI-generated images",
      "affected_populations": [
        "minority groups",
        "women",
        "cultural groups"
      ],
      "methodology": [
        "Literature Review",
        "Evaluation",
        "Mitigation"
      ],
      "methodology_detail": "Survey of existing bias studies in T2I models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.01349v2",
    "title": "Fairness in Large Language Models: A Taxonomic Survey",
    "year": 2024,
    "authors": [
      "Zhibo Chu",
      "Zichong Wang",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and bias in LLMs, focusing on social discrimination and marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on bias and fairness in AI systems",
      "affected_populations": [
        "marginalized communities",
        "discriminated groups"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Survey of recent fair LLM techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.00463v1",
    "title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
    "year": 2024,
    "authors": [
      "Hannah Chen",
      "Yangfeng Ji",
      "David Evans"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in NLP models, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender fairness in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates debiasing techniques in NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.00166v2",
    "title": "Uncovering Bias in Large Vision-Language Models with Counterfactuals",
    "year": 2024,
    "authors": [
      "Phillip Howard",
      "Anahita Bhiwandiwalla",
      "Kathleen C. Fraser",
      "Svetlana Kiritchenko"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates social biases related to race, gender, and physical characteristics in AI-generated text, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focuses on bias in multimodal AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "individuals with physical traits"
      ],
      "methodology": [
        "Experiment",
        "Counterfactual Analysis",
        "Large-scale Evaluation"
      ],
      "methodology_detail": "Using counterfactual image sets to assess bias effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.19738v1",
    "title": "MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention Editing in Text-to-Image Diffusion Models",
    "year": 2024,
    "authors": [
      "Hidir Yesiltepe",
      "Kiymet Akdemir",
      "Pinar Yanardag"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses intersectional bias in AI models, focusing on social identities such as race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "intersectional"
      ],
      "other_detail": "Bias mitigation in generative AI models",
      "affected_populations": [
        "marginalized groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Disentangled cross-attention editing in diffusion models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.19165v2",
    "title": "Evaluating Fair Feature Selection in Machine Learning for Healthcare",
    "year": 2024,
    "authors": [
      "Md Rahat Shahriar Zawad",
      "Peter Washington"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare AI, focusing on demographic subgroup disparities, which relates to social inequalities such as health disparities and social bias.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness across demographic groups",
      "affected_populations": [
        "healthcare patients",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metric Evaluation",
        "Algorithm Development"
      ],
      "methodology_detail": "Fair feature selection balancing bias and accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.19057v1",
    "title": "Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions",
    "year": 2024,
    "authors": [
      "Zainab Al-Zanbouri",
      "Gauri Sharma",
      "Shaina Raza"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities in ML predictions across demographic groups, addressing social bias and inequality in healthcare outcomes.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "age"
      ],
      "other_detail": "Focus on healthcare disparities across social groups",
      "affected_populations": [
        "diabetic patients",
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparison of models and fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.18803v1",
    "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
    "year": 2024,
    "authors": [
      "Hillary Dawkins",
      "Isar Nejadgholi",
      "Daniel Gillis",
      "Judi McCuaig"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social bias related to gender inequality, and discusses mitigation methods.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates bias reduction effectiveness in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.18216v1",
    "title": "Minimax Optimal Fair Classification with Bounded Demographic Disparity",
    "year": 2024,
    "authors": [
      "Xianli Zeng",
      "Guang Cheng",
      "Edgar Dobriban"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on controlling demographic disparity, a social fairness issue related to protected groups, within AI classification systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness constraints in classification algorithms",
      "affected_populations": [
        "protected groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Development",
        "Experiment"
      ],
      "methodology_detail": "Minimax bounds and thresholding methods for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.18196v1",
    "title": "Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities",
    "year": 2024,
    "authors": [
      "Dana Moukheiber",
      "Saurabh Mahindre",
      "Lama Moukheiber",
      "Mira Moukheiber",
      "Mingchen Gao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and biases in AI models related to social determinants, focusing on racial health inequities and intersectional fairness, which are directly linked to social inequalities.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on social determinants of health disparities",
      "affected_populations": [
        "racial groups",
        "health disparities"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Evaluation",
        "Dataset Analysis"
      ],
      "methodology_detail": "Retraining models with balanced social group data",
      "geographic_focus": [
        "MIMIC-CXR dataset (US)"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.17553v1",
    "title": "RuBia: A Russian Language Bias Detection Dataset",
    "year": 2024,
    "authors": [
      "Veronika Grigoreva",
      "Anastasiia Ivanova",
      "Ilseyar Alimova",
      "Ekaterina Artemova"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases related to gender, nationality, and socio-economic status in language models, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "nationality",
        "socioeconomic"
      ],
      "other_detail": "Bias detection in language models for Russian",
      "affected_populations": [
        "women",
        "Russian speakers",
        "socioeconomic groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias detection dataset and LLM evaluation",
      "geographic_focus": [
        "Russia"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.17158v1",
    "title": "Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels",
    "year": 2024,
    "authors": [
      "Kexin Luo",
      "Yue Mao",
      "Bei Zhang",
      "Sophie Hao"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias and female objectification in literature, addressing gender inequality. It examines societal representations and biases related to gender roles and perceptions. The focus on female objectification aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female characters",
        "literary readers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias scoring and word embedding analysis",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.14409v1",
    "title": "Locating and Mitigating Gender Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Yuchen Cai",
      "Ding Cao",
      "Rongxi Guo",
      "Yaqin Wen",
      "Guiquan Liu",
      "Enhong Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in large language models, addressing social discrimination and fairness issues related to gender. It analyzes how AI systems can perpetuate societal stereotypes, which impacts social equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Causal Mediation Analysis",
        "Experiment",
        "Knowledge Editing"
      ],
      "methodology_detail": "Tracing bias sources and mitigation techniques in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.14040v1",
    "title": "Spatial Fairness: The Case for its Importance, Limitations of Existing Work, and Guidelines for Future Research",
    "year": 2024,
    "authors": [
      "Nripsuta Ani Saxena",
      "Wenbin Zhang",
      "Cyrus Shahabi"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses unfairness related to location, which correlates with protected characteristics like race and national origin, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on spatial biases and fairness in decision-making systems",
      "affected_populations": [
        "racial minorities",
        "low-income groups"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Interdisciplinary review of fairness and spatial data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.13923v10",
    "title": "Credit vs. Discount-Based Congestion Pricing: A Comparison Study",
    "year": 2024,
    "authors": [
      "Chih-Yuan Chiu",
      "Devansh Jalota",
      "Marco Pavone"
    ],
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines congestion pricing policies aimed at reducing societal inequities, especially for low-income users, by providing travel credits or toll discounts. It analyzes how these policies impact access and equity in transportation, a key social inequality issue.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on low-income users' access to tolled roads",
      "affected_populations": [
        "low-income users"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Game theory and empirical pilot study",
      "geographic_focus": [
        "California"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.12774v1",
    "title": "Is open source software culture enough to make AI a common ?",
    "year": 2024,
    "authors": [
      "Robin Quillivic",
      "Salma Mesmoudi"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses inequalities in access to language models, biases, and resource disparities, highlighting social and technological inequities.",
      "inequality_type": [
        "informational",
        "digital",
        "economic"
      ],
      "other_detail": "Focus on access and resource disparities in AI development",
      "affected_populations": [
        "AI developers",
        "users without resources"
      ],
      "methodology": [
        "Case Study",
        "Literature Review"
      ],
      "methodology_detail": "Analysis of open-source platform and resource sharing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.12599v1",
    "title": "Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance",
    "year": 2024,
    "authors": [
      "Catalina Vajiac",
      "Arun Frey",
      "Joachim Baumann",
      "Abigail Smith",
      "Kasun Amarasinghe",
      "Alice Lai",
      "Kit Rodolfa",
      "Rayid Ghani"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial and socioeconomic disparities in eviction and homelessness prevention, using AI to improve fairness and equity in resource allocation.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial and economic disparities in housing support",
      "affected_populations": [
        "evicted individuals",
        "homeless populations"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using administrative data for risk prediction",
      "geographic_focus": [
        "Allegheny County, PA"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.12263v1",
    "title": "Fostering Inclusion: A Regional Initiative Uniting Communities to Co-Design Assistive Technologies",
    "year": 2024,
    "authors": [
      "Katharina Schmermbeck",
      "Oliver Ott",
      "Lennart Ralfs",
      "Robert Weidner"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disability inclusion, awareness, and societal acceptance, which relate to social inequality and discrimination.",
      "inequality_type": [
        "disability",
        "social",
        "educational"
      ],
      "other_detail": "Focus on societal inclusion and ableism mitigation",
      "affected_populations": [
        "people with disabilities",
        "students",
        "communities"
      ],
      "methodology": [
        "Qualitative Study",
        "Interview",
        "Case Study"
      ],
      "methodology_detail": "Analysis of interviews and initiative reflection",
      "geographic_focus": [
        "regional area"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.14713v3",
    "title": "Auditing Fairness under Unobserved Confounding",
    "year": 2024,
    "authors": [
      "Yewon Byun",
      "Dylan Sam",
      "Michael Oberst",
      "Zachary C. Lipton",
      "Bryan Wilder"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ME",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial inequity in treatment allocation and fairness auditing, directly addressing social disparities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial fairness in healthcare decision-making",
      "affected_populations": [
        "racial groups",
        "patients"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bounding treatment rates under unobserved confounding",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.11896v2",
    "title": "Investigating Markers and Drivers of Gender Bias in Machine Translations",
    "year": 2024,
    "authors": [
      "Peter J Barclay",
      "Ashkan Sami"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, which relates to social gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in machine translation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Back-translation and bias measurement across languages",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.11752v2",
    "title": "Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems",
    "year": 2024,
    "authors": [
      "Aditya Narayan Sankaran",
      "Vigneshwaran Shankaran",
      "Sampath Lonka",
      "Rajesh Sharma"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender stereotypes in cultural works, impacting societal perceptions and gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "girls",
        "society"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Model Development",
        "Survey"
      ],
      "methodology_detail": "Using LLMs to identify and rectify stereotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.10774v1",
    "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
    "year": 2024,
    "authors": [
      "J. K. Lee",
      "T. M. Chung"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes societal biases related to ethnicity, gender, and race in language models, which are forms of social inequality. It discusses bias mitigation methods, indicating a focus on fairness and discrimination issues. The abstract explicitly addresses societal bias in AI systems affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias in Korean language models",
      "affected_populations": [
        "ethnic groups",
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and mitigation in language models",
      "geographic_focus": [
        "Korea"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.10737v1",
    "title": "Leveraging Synthetic Data for Generalizable and Fair Facial Action Unit Detection",
    "year": 2024,
    "authors": [
      "Liupei Lu",
      "Yufeng Yin",
      "Yuming Gu",
      "Yizhen Wu",
      "Pratusha Prasad",
      "Yajie Zhao",
      "Mohammad Soleymani"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender fairness in facial AU detection, highlighting social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness in AI systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Synthetic data generation and domain adaptation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.10699v1",
    "title": "A Multilingual Perspective on Probing Gender Bias",
    "year": 2024,
    "authors": [
      "Karolina Sta≈Ñczak"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language and technology, addressing gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Multilingual and multicultural perspective on gender bias",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Interdisciplinary Analysis"
      ],
      "methodology_detail": "Probing language models for gender bias across languages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.10652v1",
    "title": "Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization",
    "year": 2024,
    "authors": [
      "Cecilia Ying",
      "Stephen Thomas"
    ],
    "categories": [
      "cs.LG",
      "q-fin.RM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in credit lending, focusing on gender discrimination, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "economic"
      ],
      "other_detail": "targets discrimination in financial decision-making",
      "affected_populations": [
        "women",
        "credit applicants"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "threshold optimization for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.09986v2",
    "title": "Designing Sousveillance Tools for Gig Workers",
    "year": 2024,
    "authors": [
      "Maya De Los Santos",
      "Kimberly Do",
      "Michael Muller",
      "Saiph Savage"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities faced by gig workers, a socio-economic group, regarding surveillance impacts, highlighting issues of privacy, digital autonomy, and power imbalance.",
      "inequality_type": [
        "socioeconomic",
        "digital",
        "privacy"
      ],
      "other_detail": "Focus on gig workers' surveillance and empowerment",
      "affected_populations": [
        "gig workers",
        "platform users"
      ],
      "methodology": [
        "Qualitative Study",
        "Co-design activities"
      ],
      "methodology_detail": "Interviews and participatory design with workers",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.09516v3",
    "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
    "year": 2024,
    "authors": [
      "Shadi Iskander",
      "Kira Radinsky",
      "Yonatan Belinkov"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias in language models, which relates to social discrimination and inequality issues such as race and gender. It focuses on fairness in AI systems, a key aspect of social inequality. No explicit mention of socioeconomic or health disparities, but social bias mitigation is relevant.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on bias mitigation without demographic labels",
      "affected_populations": [
        "social groups",
        "minority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Regularization"
      ],
      "methodology_detail": "Bias mitigation in language model fine-tuning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.09148v1",
    "title": "Evaluating LLMs for Gender Disparities in Notable Persons",
    "year": 2024,
    "authors": [
      "Lauren Rhue",
      "Sofie Goethals",
      "Arun Sundararajan"
    ],
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI responses, highlighting social disparities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in AI responses related to gender disparities",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating fairness and disparities in LLM responses",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.08564v3",
    "title": "Generalizing Fairness to Generative Language Models via Reformulation of Non-discrimination Criteria",
    "year": 2024,
    "authors": [
      "Sara Sterlie",
      "Nina Weng",
      "Aasa Feragen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI, a social discrimination issue, by analyzing and quantifying gender stereotypes in language models, which directly relates to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focused on occupational gender stereotypes",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designing prompts to measure bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.07857v1",
    "title": "Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias",
    "year": 2024,
    "authors": [
      "Sierra Wyllie",
      "Ilia Shumailov",
      "Nicolas Papernot"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness, biases, and unfairness feedback loops in AI systems, which relate to social discrimination and inequality. It examines how model-induced biases can impact marginalized groups and proposes interventions to address these issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability",
        "educational"
      ],
      "other_detail": "Focuses on fairness and bias in AI systems",
      "affected_populations": [
        "minoritized groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Simulation",
        "Framework Development",
        "Algorithmic Intervention"
      ],
      "methodology_detail": "Simulates bias mitigation strategies in data ecosystems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.06332v2",
    "title": "Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups",
    "year": 2024,
    "authors": [
      "Nelson Col√≥n Vargas"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how AI perpetuates racial, economic, and social inequalities, especially affecting marginalized groups, through biased algorithms, labor exploitation, and systemic barriers.",
      "inequality_type": [
        "racial",
        "economic",
        "socioeconomic",
        "class",
        "discrimination"
      ],
      "other_detail": "Focus on racial and economic injustices in AI practices",
      "affected_populations": [
        "minoritized communities",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes historical patterns and current AI practices",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.06104v4",
    "title": "Debiased Noise Editing on Foundation Models for Fair Medical Image Classification",
    "year": 2024,
    "authors": [
      "Ruinan Jin",
      "Wenlong Deng",
      "Minghui Chen",
      "Xiaoxiao Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias mitigation in medical imaging, aiming for fairness across patient groups, addressing health disparities and social bias in AI systems.",
      "inequality_type": [
        "health",
        "racial",
        "gender"
      ],
      "other_detail": "Bias in medical AI systems",
      "affected_populations": [
        "patients",
        "medical groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.06031v1",
    "title": "FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition",
    "year": 2024,
    "authors": [
      "Dalia Gala",
      "Milo Phillips-Brown",
      "Naman Goel",
      "Carinal Prunkl",
      "Laura Alvarez Jubete",
      "medb corcoran",
      "Ray Eitel-Porter"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI, focusing on biases related to target variable definitions, which can encode social discrimination. It addresses how algorithmic decisions impact different social groups, particularly in hiring contexts.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness implications in algorithmic decision-making",
      "affected_populations": [
        "job applicants",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Case Study",
        "System Design"
      ],
      "methodology_detail": "Real-world hiring data and interactive simulation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.05975v1",
    "title": "Measuring Bias in a Ranked List using Term-based Representations",
    "year": 2024,
    "authors": [
      "Amin Abolghasemi",
      "Leif Azzopardi",
      "Arian Askari",
      "Maarten de Rijke",
      "Suzan Verberne"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in document ranking, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on bias measurement in AI systems",
      "affected_populations": [
        "women",
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Develops and evaluates a new fairness metric",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.05696v1",
    "title": "SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes",
    "year": 2024,
    "authors": [
      "Mukul Bhutani",
      "Kevin Robinson",
      "Vinodkumar Prabhakaran",
      "Shachi Dave",
      "Sunipa Dev"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social stereotypes across cultures and languages, addressing biases related to socio-cultural phenomena, which are linked to social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "linguistic",
        "geographic"
      ],
      "other_detail": "Focus on culturally situated stereotypes and biases",
      "affected_populations": [
        "language communities",
        "regional groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Human Annotation"
      ],
      "methodology_detail": "Combines LLM generation with cultural validation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.05235v1",
    "title": "Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare",
    "year": 2024,
    "authors": [
      "Mingxuan Liu",
      "Yilin Ning",
      "Yuhe Ke",
      "Yuqing Shang",
      "Bibhas Chakraborty",
      "Marcus Eng Hock Ong",
      "Roger Vaughan",
      "Nan Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in healthcare AI, focusing on racial and gender biases, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in healthcare AI models",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metrics",
        "Model Development"
      ],
      "methodology_detail": "Bias reduction and interpretability techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social biases",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.05125v2",
    "title": "Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis",
    "year": 2024,
    "authors": [
      "Muxi Chen",
      "Yi Liu",
      "Jian Yi",
      "Changran Xu",
      "Qiuxia Lai",
      "Hongliang Wang",
      "Tsung-Yi Ho",
      "Qiang Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in AI outputs related to gender, race, and age, highlighting fairness issues and social biases in generative models.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Focus on fairness and bias in AI-generated human images",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Fairness Analysis"
      ],
      "methodology_detail": "Evaluates biases and fairness in image synthesis models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.03357v2",
    "title": "The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa",
    "year": 2024,
    "authors": [
      "Mercy Asiedu",
      "Awa Dieng",
      "Iskandar Haykel",
      "Negar Rostamzadeh",
      "Stephen Pfohl",
      "Chirag Nagpal",
      "Maria Nagawa",
      "Abigail Oppong",
      "Sanmi Koyejo",
      "Katherine Heller"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities related to colonialism, income, and biases in AI affecting Africa, addressing social inequalities in health and systemic power imbalances.",
      "inequality_type": [
        "colonialism",
        "income",
        "health"
      ],
      "other_detail": "Focus on colonial history and socioeconomic disparities",
      "affected_populations": [
        "African populations",
        "global health communities"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study",
        "Survey"
      ],
      "methodology_detail": "Review, interviews, and survey analysis",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.09700v2",
    "title": "Shapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI",
    "year": 2024,
    "authors": [
      "Alex Glinsky",
      "Alexey Sokolsky"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "91A12, 68T05, 91B32",
      "I.2.6; I.3.3; I.2.0; J.5; J.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fair compensation for creators affected by AI automation, addressing socioeconomic impacts and fairness issues.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Fair reward distribution among content creators",
      "affected_populations": [
        "artists",
        "content creators"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using Shapley Values for contribution assessment",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.03297v1",
    "title": "\"It's the only thing I can trust\": Envisioning Large Language Model Use by Autistic Workers for Communication Assistance",
    "year": 2024,
    "authors": [
      "JiWoong Jang",
      "Sanika Moharana",
      "Patrick Carrington",
      "Andrew Begel"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social discrimination faced by autistic adults and examines AI's role in social communication, highlighting inequalities related to disability and social inclusion.",
      "inequality_type": [
        "disability",
        "social",
        "educational"
      ],
      "other_detail": "Focus on autistic adults' workplace communication challenges",
      "affected_populations": [
        "autistic adults"
      ],
      "methodology": [
        "Experiment",
        "Qualitative Study"
      ],
      "methodology_detail": "Participants' interactions with LLMs and confederate",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.02726v1",
    "title": "Bias in Generative AI",
    "year": 2024,
    "authors": [
      "Mi Zhou",
      "Vibhanshu Abhishek",
      "Timothy Derdenger",
      "Jaymo Kim",
      "Kannan Srinivasan"
    ],
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and race in AI-generated images, highlighting social discrimination issues. It discusses how these biases reflect and potentially reinforce societal inequalities. The focus on racial and gender bias in AI systems directly relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI representations of social groups",
      "affected_populations": [
        "women",
        "African Americans"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed generated images for bias patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.02563v1",
    "title": "Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas",
    "year": 2024,
    "authors": [
      "Aashaka Desai",
      "Maartje De Meulder",
      "Julie A. Hochgesang",
      "Annemarie Kocab",
      "Alex X. Lu"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critiques systemic biases and lack of Deaf stakeholder inclusion, highlighting social and linguistic inequalities in sign language AI research.",
      "inequality_type": [
        "disability",
        "linguistic",
        "social"
      ],
      "other_detail": "Focus on Deaf community representation and systemic biases",
      "affected_populations": [
        "Deaf community",
        "sign language users"
      ],
      "methodology": [
        "Systematic Review",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes recent sign language AI research papers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.01755v1",
    "title": "AI Language Models Could Both Help and Harm Equity in Marine Policymaking: The Case Study of the BBNJ Question-Answering Bot",
    "year": 2024,
    "authors": [
      "Matt Ziegler",
      "Sarah Lothian",
      "Brian O'Neill",
      "Richard Anderson",
      "Yoshitaka Ota"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI systems that favor Western perspectives over developing countries, highlighting issues of fairness and equity in marine policymaking.",
      "inequality_type": [
        "geographic",
        "informational"
      ],
      "other_detail": "Biases favor Western economic centers in AI outputs",
      "affected_populations": [
        "developing countries",
        "marine stakeholders"
      ],
      "methodology": [
        "Case Study",
        "Literature Review",
        "Critique"
      ],
      "methodology_detail": "Analysis of AI bias and policy implications",
      "geographic_focus": [
        "global",
        "marine regions"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.01697v1",
    "title": "Dismantling Gender Blindness in Online Discussion of a Crime/Gender Dichotomy",
    "year": 2024,
    "authors": [
      "Yigang Qin",
      "Weilun Duan",
      "Qunfang Wu",
      "Zhicong Lu"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-related discourses and sexism on social media, highlighting gender inequality and biases in online discussions.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias and feminist activism",
      "affected_populations": [
        "women",
        "feminists"
      ],
      "methodology": [
        "Qualitative Study",
        "Mixed Methods"
      ],
      "methodology_detail": "Analyzes social media posts on Weibo",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.01600v1",
    "title": "Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making",
    "year": 2024,
    "authors": [
      "Alba Aguilera",
      "Nieves Montes",
      "Georgina Curto",
      "Carles Sierra",
      "Nardine Osman"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination against the poor (aporophobia) and its impact on wealth inequality, addressing social bias and socioeconomic disparities.",
      "inequality_type": [
        "wealth",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on poverty-related discrimination and societal impacts",
      "affected_populations": [
        "poor people"
      ],
      "methodology": [
        "Agent-based Modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulation using real-world demographic and policy data",
      "geographic_focus": [
        "Barcelona"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.00742v1",
    "title": "Dialect prejudice predicts AI decisions about people's character, employability, and criminality",
    "year": 2024,
    "authors": [
      "Valentin Hofmann",
      "Pratyusha Ria Kalluri",
      "Dan Jurafsky",
      "Sharese King"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases and stereotypes in language models, highlighting their impact on social discrimination and inequality. It discusses how dialect prejudice influences decisions about employment, criminality, and sentencing, affecting marginalized racial groups.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias in AI affecting racial and linguistic groups",
      "affected_populations": [
        "African Americans",
        "language speakers"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing stereotypes and decision-making in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.00527v1",
    "title": "\"There is a Job Prepared for Me Here\": Understanding How Short Video and Live-streaming Platforms Empower Ageing Job Seekers in China",
    "year": 2024,
    "authors": [
      "PiaoHong Wang",
      "Siying Hu",
      "Bo Wen",
      "Zhicong Lu"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.SI",
      "H.5.m; K.4.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines age discrimination in employment and how platforms empower ageing job seekers, addressing age-related social inequality.",
      "inequality_type": [
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on employment and age discrimination in China",
      "affected_populations": [
        "ageing workers"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Interviews with ageing job seekers",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.00277v1",
    "title": "Gender Bias in Large Language Models across Multiple Languages",
    "year": 2024,
    "authors": [
      "Jinman Zhao",
      "Yitian Ding",
      "Chen Jia",
      "Yining Wang",
      "Zifan Qian"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, addressing gender inequality and bias in AI outputs.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement across multiple languages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.00180v3",
    "title": "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
    "year": 2024,
    "authors": [
      "Karina Halevy",
      "Anna Sotnikova",
      "Badr AlKhamissi",
      "Syrielle Montariol",
      "Antoine Bosselut"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race, gender, and geography in language models, highlighting social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "geographic",
        "informational"
      ],
      "other_detail": "Bias amplification in model editing processes",
      "affected_populations": [
        "Asian",
        "African",
        "South American",
        "women",
        "xenophobic groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Develops bias benchmark and analyzes model behavior",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.19226v3",
    "title": "Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain",
    "year": 2024,
    "authors": [
      "Pratik Gajane",
      "Sean Newman",
      "Mykola Pechenizkiy",
      "John D. Piette"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender fairness in AI-driven healthcare, addressing social bias and disparities related to gender. It focuses on fairness in personalized care recommendations, highlighting potential exacerbation of gender-based disparities.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Fairness in AI healthcare recommendations",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Experiment",
        "Feature Selection"
      ],
      "methodology_detail": "Optimizing fairness and utility in personalized care",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.18206v4",
    "title": "Balancing Act: Distribution-Guided Debiasing in Diffusion Models",
    "year": 2024,
    "authors": [
      "Rishubh Parihar",
      "Abhijnya Bhat",
      "Abhipsa Basu",
      "Saswat Mallick",
      "Jogendra Nath Kundu",
      "R. Venkatesh Babu"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models related to demographic attributes such as gender, aiming to promote fairer image generation. It discusses mitigating social biases embedded in training data, particularly demographic disparities. The focus on fairness and bias reduction relates directly to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in AI-generated images",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using attribute distribution prediction and debiasing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.17389v1",
    "title": "FairBelief -- Assessing Harmful Beliefs in Language Models",
    "year": 2024,
    "authors": [
      "Mattia Setzu",
      "Marta Marchiori Manerba",
      "Pasquale Minervini",
      "Debora Nozza"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to gender, a social group, and assesses their potential harm, addressing social fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender-related harmful beliefs in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Fairness Dataset Creation"
      ],
      "methodology_detail": "Assessing beliefs and biases in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.17229v1",
    "title": "Preserving Fairness Generalization in Deepfake Detection",
    "year": 2024,
    "authors": [
      "Li Lin",
      "Xinan He",
      "Yan Ju",
      "Xin Wang",
      "Feng Ding",
      "Shu Hu"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness disparities among demographic groups in deepfake detection, highlighting social bias issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness across social groups",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Disentanglement Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness preservation in cross-domain deepfake detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.05575v1",
    "title": "Enhancing Health Care Accessibility and Equity Through a Geoprocessing Toolbox for Spatial Accessibility Analysis: Development and Case Study",
    "year": 2024,
    "authors": [
      "Soheil Hashtarkhani",
      "David L Schwartz",
      "Arash Shaban-Nejad"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "68U05"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in healthcare access across geographic and urban-rural divides, highlighting inequities in health service distribution.",
      "inequality_type": [
        "health",
        "geographic",
        "urban-rural"
      ],
      "other_detail": "Focus on spatial and access disparities in health services",
      "affected_populations": [
        "rural residents",
        "urban residents",
        "patients needing hemodialysis"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Spatial accessibility measurement and disparity analysis",
      "geographic_focus": [
        "Tennessee"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.00814v1",
    "title": "Gender Biased Legal Case Retrieval System on Users' Decision Process",
    "year": 2024,
    "authors": [
      "Ruizhe Zhang",
      "Qingyao Ai",
      "Yiqun Liu",
      "Yueyue Wu",
      "Beining Wang"
    ],
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in legal AI systems and its impact on perceptions, addressing gender inequality and bias in AI.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in legal AI",
      "affected_populations": [
        "defendants",
        "judges"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Simulated judge decision process with gender-biased cases",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.15603v2",
    "title": "Differentially Private Fair Binary Classifications",
    "year": 2024,
    "authors": [
      "Hrad Ghoukasian",
      "Shahab Asoodeh"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and privacy in binary classification, addressing social bias and discrimination issues in AI systems. It aims to improve fairness guarantees across demographic groups, which relates directly to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI classification algorithms",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Fairness Evaluation",
        "Differential Privacy"
      ],
      "methodology_detail": "Combines fairness algorithms with privacy-preserving techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.15561v1",
    "title": "Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency",
    "year": 2024,
    "authors": [
      "Parian Haghighat",
      "Denisa G'andara",
      "Lulu Kang",
      "Hadis Anahideh"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and equity in predictive models, addressing bias and transparency issues that impact social groups.",
      "inequality_type": [
        "social",
        "educational",
        "inequity"
      ],
      "other_detail": "Focuses on fairness in predictive analytics",
      "affected_populations": [
        "social groups",
        "users of models"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Integration"
      ],
      "methodology_detail": "Incorporates fairness into MARS model",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.15481v4",
    "title": "Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models",
    "year": 2024,
    "authors": [
      "Y Liu",
      "K Yang",
      "Z Qi",
      "X Liu",
      "Y Yu",
      "C Zhai"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal discrimination and biases in LLMs, highlighting stereotypes related to gender and socio-economic factors, which are forms of social inequality.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on societal stereotypes and bias in AI outputs",
      "affected_populations": [
        "women",
        "low-income groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Data Mining"
      ],
      "methodology_detail": "Quantifies discrimination risk via behavioral metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2404.09998v1",
    "title": "Electric Vehicles Limit Equitable Access to Essential Services During Blackouts",
    "year": 2024,
    "authors": [
      "Yamil Essus",
      "Benjamin Rachunok"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses access to essential services during blackouts, highlighting disparities influenced by EV adoption, which can exacerbate existing inequalities in mobility and community resilience.",
      "inequality_type": [
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on access disparities related to infrastructure and location",
      "affected_populations": [
        "urban residents",
        "low-income households"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Modeling impact of EVs on service access",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.14959v2",
    "title": "A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems",
    "year": 2024,
    "authors": [
      "Jessy Xinyi Han",
      "Andrew Miller",
      "S. Craig Watkins",
      "Christopher Winship",
      "Fotini Christia",
      "Devavrat Shah"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in law enforcement, a key aspect of social inequality, using a causal framework. It addresses racial disparities and bias sources affecting different social groups. The focus on racial bias in policing directly relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "social discrimination"
      ],
      "other_detail": "Focus on racial bias in law enforcement systems",
      "affected_populations": [
        "minority race",
        "majority race"
      ],
      "methodology": [
        "Causal Framework",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops a multi-stage causal evaluation method",
      "geographic_focus": [
        "New Orleans"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.14815v1",
    "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
    "year": 2024,
    "authors": [
      "Yuzhe Yang",
      "Yujia Liu",
      "Xin Liu",
      "Avanti Gulhane",
      "Domenico Mastrodicasa",
      "Wei Wu",
      "Edward J Wang",
      "Dushyant W Sahani",
      "Shwetak Patel"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases in AI models affecting marginalized groups, highlighting disparities in medical diagnosis. It discusses fairness issues and potential to worsen healthcare inequalities, directly addressing social discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias in medical AI affecting marginalized populations",
      "affected_populations": [
        "Black patients",
        "Female patients"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes model fairness across datasets and demographic groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.14577v1",
    "title": "Debiasing Text-to-Image Diffusion Models",
    "year": 2024,
    "authors": [
      "Ruifei He",
      "Chuhui Xue",
      "Haoru Tan",
      "Wenqing Zhang",
      "Yingchen Yu",
      "Song Bai",
      "Xiaojuan Qi"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias in AI systems, which relates to social discrimination and inequality issues such as race and gender. It focuses on mitigating biases in AI-generated visual content, highlighting social fairness concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on social bias in AI-generated images",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Reinforcement Learning",
        "Distribution Alignment"
      ],
      "methodology_detail": "Bias mitigation techniques in diffusion models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.14277v1",
    "title": "GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages",
    "year": 2024,
    "authors": [
      "Spencer Rarrick",
      "Ranjita Naik",
      "Sundar Poudel",
      "Vishal Chowdhary"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social fairness issue affecting gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language translation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "creating benchmark dataset and evaluating mitigation strategies",
      "geographic_focus": [
        "Turkish",
        "Hungarian",
        "Finnish",
        "Persian"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.14875v3",
    "title": "What's in a Name? Auditing Large Language Models for Race and Gender Bias",
    "year": 2024,
    "authors": [
      "Alejandro Salinas",
      "Amit Haim",
      "Julian Nyarko"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in language models related to race and gender, highlighting systemic disadvantages for marginalized groups. It addresses social discrimination embedded in AI systems, affecting social equity.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI systems affecting marginalized groups",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Prompt-based bias testing across models and templates",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.13954v3",
    "title": "Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality",
    "year": 2024,
    "authors": [
      "Rahul Zalkikar",
      "Kanchan Chandra"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates social biases in language models, focusing on biases towards disadvantaged and advantaged groups, which relate to social inequalities such as race, gender, and class.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on social biases in AI models",
      "affected_populations": [
        "disadvantaged groups",
        "advantaged groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement via proxy functions and iterative masking",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.13787v3",
    "title": "Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks",
    "year": 2024,
    "authors": [
      "Ana-Andreea Stoica",
      "Nelly Litvak",
      "Augustin Chaintreau"
    ],
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and fairness issues in link analysis algorithms, which relate to social discrimination and inequality in algorithmic outcomes affecting social groups.",
      "inequality_type": [
        "racial",
        "social",
        "informational"
      ],
      "other_detail": "Bias amplification in network algorithms",
      "affected_populations": [
        "minority groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Analysis"
      ],
      "methodology_detail": "Analyzes bias in network algorithms using synthetic and real data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.13636v2",
    "title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
    "year": 2024,
    "authors": [
      "Ashutosh Sathe",
      "Prachi Jain",
      "Sunayana Sitaram"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates societal biases related to gender, race, and age in vision-language models, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias in AI models regarding social attributes",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Synthetic dataset generation and bias evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.13393v2",
    "title": "Fairness Risks for Group-conditionally Missing Demographics",
    "year": 2024,
    "authors": [
      "Kaiqi Jiang",
      "Wenzhe Fan",
      "Mao Li",
      "Xinhua Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on demographic groups and social bias. It discusses the impact of missing sensitive features, which relate to social discrimination issues.",
      "inequality_type": [
        "age",
        "social bias"
      ],
      "other_detail": "focus on demographic fairness and privacy issues",
      "affected_populations": [
        "age groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Variational Auto-Encoder",
        "Fairness Analysis"
      ],
      "methodology_detail": "Probabilistic imputation of sensitive features",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.05564v1",
    "title": "Promoting Fair Vaccination Strategies Through Influence Maximization: A Case Study on COVID-19 Spread",
    "year": 2024,
    "authors": [
      "Nicola Neophytou",
      "Afaf Ta√Øk",
      "Golnoosh Farnadi"
    ],
    "categories": [
      "cs.CY",
      "cs.SI",
      "I.2.1; I.6.3; J.3"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in COVID-19 outcomes across demographic groups, focusing on racial, socioeconomic, and age-related inequalities in vaccine distribution.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "age",
        "health"
      ],
      "other_detail": "Fairness in vaccine allocation across social groups",
      "affected_populations": [
        "racial minorities",
        "low-income communities",
        "elderly"
      ],
      "methodology": [
        "Influence Maximization",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using network influence models for fairness optimization",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.12937v1",
    "title": "GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks",
    "year": 2024,
    "authors": [
      "Anuj Kumar Sirohi",
      "Anjali Gupta",
      "Sayan Ranu",
      "Sandeep Kumar",
      "Amitabha Bagchi"
    ],
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing biases affecting underprivileged groups, which relates to social inequality issues.",
      "inequality_type": [
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in decision-making algorithms",
      "affected_populations": [
        "underprivileged groups",
        "individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Constraint Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses Gini coefficient for fairness measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.12541v1",
    "title": "Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation",
    "year": 2024,
    "authors": [
      "Yuying Zhao",
      "Yu Wang",
      "Yi Zhang",
      "Pamela Wisniewski",
      "Charu Aggarwal",
      "Tyler Derr"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness issues related to gender and sexual orientation in online dating recommender systems, addressing social bias and discrimination concerns.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Focus on fairness in AI-driven social interactions",
      "affected_populations": [
        "users by gender",
        "users by sexual orientation"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes real dataset and proposes fairness strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.12319v1",
    "title": "Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness",
    "year": 2024,
    "authors": [
      "Chen Zhao",
      "Feng Mi",
      "Xintao Wu",
      "Kai Jiang",
      "Latifur Khan",
      "Feng Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in online learning, addressing social bias related to protected groups such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "fairness"
      ],
      "other_detail": "Fairness constraints in AI systems",
      "affected_populations": [
        "race",
        "gender"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Fairness-aware online meta-learning algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.12161v2",
    "title": "Endowing Pre-trained Graph Models with Provable Fairness",
    "year": 2024,
    "authors": [
      "Zhongjian Zhang",
      "Mengmei Zhang",
      "Yue Yu",
      "Cheng Yang",
      "Jiawei Liu",
      "Chuan Shi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models, which relate to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on fairness in AI predictions",
      "affected_populations": [
        "social groups",
        "users of AI systems"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness guarantees and adapter-tuning framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.12062v4",
    "title": "Causal Equal Protection as Algorithmic Fairness",
    "year": 2024,
    "authors": [
      "Marcello Di Bello",
      "Nicol√≤ Cangiotti",
      "Michele Loi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness and classification errors related to protected characteristics, implying concern with social discrimination and bias.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on legal fairness and causal analysis in justice system",
      "affected_populations": [
        "defendants",
        "social groups"
      ],
      "methodology": [
        "Causal Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Uses do-calculus and causal frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.11190v1",
    "title": "Disclosure and Mitigation of Gender Bias in LLMs",
    "year": 2024,
    "authors": [
      "Xiangjue Dong",
      "Yibo Wang",
      "Philip S. Yu",
      "James Caverlee"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in LLMs, addressing social gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias disclosure and mitigation in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Probing framework and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.14633v4",
    "title": "Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models",
    "year": 2024,
    "authors": [
      "Smriti Singh",
      "Shuvam Keshari",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates socioeconomic bias in AI models, focusing on disparities related to economic and social backgrounds, which are key aspects of social inequality.",
      "inequality_type": [
        "socioeconomic",
        "class"
      ],
      "other_detail": "Focuses on economic and social background biases",
      "affected_populations": [
        "underprivileged people"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes bias using a novel dataset and model evaluations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.11089v3",
    "title": "The Male CEO and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects",
    "year": 2024,
    "authors": [
      "Yixin Wan",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI-generated images, highlighting social gender stereotypes and power dynamics, which relate directly to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes and organizational power biases",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluation framework and bias detection methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.10567v4",
    "title": "InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?",
    "year": 2024,
    "authors": [
      "Yogesh Tripathi",
      "Raghav Donakanti",
      "Sahil Girhepuje",
      "Ishan Kavathekar",
      "Bhaskara Hanuma Vedula",
      "Gokul S Krishnan",
      "Shreya Goyal",
      "Anmol Goel",
      "Balaraman Ravindran",
      "Ponnurangam Kumaraguru"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and societal biases in legal AI models within the Indian social context, addressing social discrimination and bias issues.",
      "inequality_type": [
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on societal biases in AI for Indian legal domain",
      "affected_populations": [
        "social groups in India"
      ],
      "methodology": [
        "Natural Language Processing",
        "Fairness Evaluation",
        "Model Fine-tuning"
      ],
      "methodology_detail": "Assessing fairness and accuracy in legal tasks",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier of societal biases",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.09786v4",
    "title": "Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model",
    "year": 2024,
    "authors": [
      "Alvin Grissom II",
      "Ryan F. Lei",
      "Matt Gusdorff",
      "Jeova Farias Sales Rocha Neto",
      "Bailey Lin",
      "Ryan Trotter"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in a GAN discriminator that disproportionately affect gender, race, and other social categories, indicating a focus on social bias and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI discriminator affecting social categories",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Analyzes biases in GAN discriminator models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2403.12074v2",
    "title": "Beyond Quantities: Machine Learning-based Characterization of Inequality in Infrastructure Quality Provision in Cities",
    "year": 2024,
    "authors": [
      "Bo Li",
      "Ali Mostafavi"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in infrastructure quality linked to low-income populations, highlighting urban disparities and environmental justice concerns.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic",
        "environmental"
      ],
      "other_detail": "Focus on infrastructure disparities affecting low-income urban populations",
      "affected_populations": [
        "low-income residents"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Identifies infrastructure features influencing hazard exposure",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.09286v1",
    "title": "Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research",
    "year": 2024,
    "authors": [
      "Jessica Zhu",
      "Michel Cukier",
      "Joseph Richardson Jr"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses trust and bias in AI models used in firearm injury research involving vulnerable populations, specifically Black and Brown Americans, highlighting social disparities and biases.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on vulnerable racial and health disparities",
      "affected_populations": [
        "Black Americans",
        "Brown Americans"
      ],
      "methodology": [
        "Model Development",
        "Framework Design"
      ],
      "methodology_detail": "Creating transparent AI trust frameworks for vulnerable groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.09260v1",
    "title": "Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support",
    "year": 2024,
    "authors": [
      "Zilin Ma",
      "Yiyang Mei",
      "Yinru Long",
      "Zhaoyuan Su",
      "Krzysztof Z. Gajos"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses societal discrimination against LGBTQ+ individuals and societal biases in AI systems, addressing social inequality and bias issues.",
      "inequality_type": [
        "gender",
        "disability",
        "health"
      ],
      "other_detail": "Focus on societal discrimination and bias in AI systems",
      "affected_populations": [
        "LGBTQ+ individuals"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews"
      ],
      "methodology_detail": "Participant interviews on chatbot experiences",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.08242v1",
    "title": "Towards Equitable Agile Research and Development of AI and Robotics",
    "year": 2024,
    "authors": [
      "Andrew Hundt",
      "Julia Schuller",
      "Severin Kacianka"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.RO",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases and harms in AI systems affecting marginalized groups, highlighting issues of fairness, equity, and rights. It addresses social discrimination and unequal impacts across social groups. The focus on bias in facial recognition and societal harms indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "social bias"
      ],
      "other_detail": "Bias and fairness in AI systems",
      "affected_populations": [
        "Black Women",
        "Black Men",
        " marginalized groups"
      ],
      "methodology": [
        "System Design",
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Framework for equitable AI R&D processes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.07778v1",
    "title": "Algorithmic Fairness and Color-blind Racism: Navigating the Intersection",
    "year": 2024,
    "authors": [
      "Jamelle Watson-Daniels"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias, racism, and ideological frameworks related to race, addressing social discrimination and inequality. It discusses algorithmic fairness in relation to racial disparities and systemic racism, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial bias and ideology in algorithms",
      "affected_populations": [
        "racial minorities",
        " marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes racial discourse and ideological shifts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.07519v1",
    "title": "MAFIA: Multi-Adapter Fused Inclusive LanguAge Models",
    "year": 2024,
    "authors": [
      "Prachi Jain",
      "Ashutosh Sathe",
      "Varun Gumma",
      "Kabir Ahuja",
      "Sunayana Sitaram"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in language models related to societal biases such as gender, race, and other social dimensions, aiming to mitigate their impact in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focuses on societal biases in NLP models",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Debiasing Techniques"
      ],
      "methodology_detail": "Multi-bias debiasing using structured knowledge and generative models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.07513v1",
    "title": "The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese",
    "year": 2024,
    "authors": [
      "Ajinkya Kulkarni",
      "Anna Tokareva",
      "Rameez Qureshi",
      "Miguel Couceiro"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in ASR systems related to gender, age, skin tone, and geographic location, which are social categories linked to inequality. It analyzes how these biases impact different social groups, addressing social bias and fairness issues in AI. The focus on quantifying and mitigating biases aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "age",
        "racial",
        "geographic"
      ],
      "other_detail": "Bias mitigation in multilingual speech recognition",
      "affected_populations": [
        "women",
        "elderly",
        "racial minorities",
        "Portuguese speakers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias assessment and mitigation techniques in ASR systems",
      "geographic_focus": [
        "Portugal"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.07339v1",
    "title": "Beyond the Headlines: Understanding Sentiments and Morals Impacting Female Employment in Spain",
    "year": 2024,
    "authors": [
      "Oscar Araque",
      "Luca Barbaglia",
      "Francesco Berlingieri",
      "Marco Colagrossi",
      "Sergio Consoli",
      "Lorenzo Gatti",
      "Caterina Mauri",
      "Kyriaki Kalimeri"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender disparities in female employment and related sentiments, addressing gender inequality. It uses NLP to study societal attitudes and regional differences, directly engaging with social discrimination issues.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on employment and societal attitudes in Spain",
      "affected_populations": [
        "women",
        "female workers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes news corpus for sentiment and moral values",
      "geographic_focus": [
        "Spain"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.07329v2",
    "title": "The Bias of Harmful Label Associations in Vision-Language Models",
    "year": 2024,
    "authors": [
      "Caner Hazirbas",
      "Alicia Sun",
      "Yonathan Efroni",
      "Mark Ibrahim"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates harmful label associations related to race, skin tone, and gender in AI models, highlighting disparities affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on bias in vision-language models",
      "affected_populations": [
        "individuals with darker skin",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias in large video datasets and model predictions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.06811v1",
    "title": "Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation",
    "year": 2024,
    "authors": [
      "Andrew Smart",
      "Ding Wang",
      "Ellis Monk",
      "Mark D√≠az",
      "Atoosa Kasirzadeh",
      "Erin Van Liemt",
      "Sonja Schmer-Galunder"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how data annotation practices impose WEIRD social categories on non-WEIRD workers, affecting global AI fairness and social categorization.",
      "inequality_type": [
        "racial",
        "ethnic",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Impacts global social categorization and fairness",
      "affected_populations": [
        "non-WEIRD annotators",
        "Global South populations"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Critical genealogy and synthesis of recent research",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.05779v1",
    "title": "Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images",
    "year": 2024,
    "authors": [
      "Kathleen C. Fraser",
      "Svetlana Kiritchenko"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender and racial biases in AI systems, highlighting social discrimination issues. It analyzes how AI responses differ based on perceived social group characteristics, directly addressing social bias and fairness concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on bias in vision-language AI models",
      "affected_populations": [
        "women",
        "Black individuals"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Using a novel dataset to test AI responses",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.05713v3",
    "title": "Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations",
    "year": 2024,
    "authors": [
      "Pranav Kulkarni",
      "Andrew Chan",
      "Nithya Navarathna",
      "Skylar Chan",
      "Paul H. Yi",
      "Vishwa S. Parekh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI models affecting vulnerable demographic groups, highlighting social disparities in healthcare outcomes.",
      "inequality_type": [
        "health",
        "gender",
        "age"
      ],
      "other_detail": "Bias in medical AI affecting vulnerable populations",
      "affected_populations": [
        "vulnerable patients",
        "demographic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias attack demonstration and performance evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.04489v1",
    "title": "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning",
    "year": 2024,
    "authors": [
      "Sanjari Srivastava",
      "Piotr Mardziel",
      "Zhikhun Zhang",
      "Archana Ahlawat",
      "Anupam Datta",
      "John C Mitchell"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias amplification in language models related to gender, race, and religion, addressing social fairness issues. It discusses how AI impacts social groups and aims to mitigate bias, which relates directly to social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "religious"
      ],
      "other_detail": "Bias amplification in language models",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Experiment",
        "Counterfactual Data Augmentation"
      ],
      "methodology_detail": "Bias mitigation techniques in model fine-tuning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.04420v1",
    "title": "Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways",
    "year": 2024,
    "authors": [
      "Angelina Wang",
      "Xuechunzi Bai",
      "Solon Barocas",
      "Su Lin Blodgett"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how machine learning errors related to stereotypes impact different social groups, especially gender, highlighting experiential harm and social biases.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes and experiential harm",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Survey",
        "Experiment"
      ],
      "methodology_detail": "Studying reactions to stereotype-related errors",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.04391v1",
    "title": "The Howard-Harvard effect: Institutional reproduction of intersectional inequalities",
    "year": 2024,
    "authors": [
      "Diego Kozlowski",
      "Thema Monroe-White",
      "Vincent Larivi√®re",
      "Cassidy R. Sugimoto"
    ],
    "categories": [
      "cs.DL",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in academic topics, impact, and prestige related to minoritized scholars, highlighting systemic inequalities in US higher education.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Focuses on institutional and intersectional disparities in science",
      "affected_populations": [
        "minoritized scholars",
        "women",
        "minority groups"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes topical alignment, citations, and impact metrics",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 1.0
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.04105v2",
    "title": "Measuring Implicit Bias in Explicitly Unbiased Large Language Models",
    "year": 2024,
    "authors": [
      "Xuechunzi Bai",
      "Angelina Wang",
      "Ilia Sucholutsky",
      "Thomas L. Griffiths"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates implicit biases related to race, gender, religion, and health, which are social inequalities, in large language models. It addresses social bias measurement and potential societal impacts of AI systems. The focus on stereotypes and discrimination links directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "religious",
        "health"
      ],
      "other_detail": "Implicit biases in AI models reflecting societal stereotypes",
      "affected_populations": [
        "racial groups",
        "women",
        "religious minorities",
        "health groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Psychological Research",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Prompt-based bias measurement inspired by psychology",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.03629v3",
    "title": "Disparate Impact on Group Accuracy of Linearization for Private Inference",
    "year": 2024,
    "authors": [
      "Saswat Das",
      "Marco Romanelli",
      "Ferdinando Fioretto"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness impacts of linearization in private inference, highlighting disproportionate accuracy decreases for minority groups, indicating a focus on algorithmic fairness and social bias.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fairness disparities in AI inference",
      "affected_populations": [
        "minority groups",
        "majority groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes accuracy impacts across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.03450v2",
    "title": "Recommendation Fairness in Social Networks Over Time",
    "year": 2024,
    "authors": [
      "Meng Cao",
      "Hussain Hussain",
      "Sandipan Sikdar",
      "Denis Helic",
      "Markus Strohmaier",
      "Roman Kern"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in social recommendation systems related to demographic groups such as gender and race, addressing social bias and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Focuses on fairness in social networks over time",
      "affected_populations": [
        "demographic groups",
        "minority groups"
      ],
      "methodology": [
        "Empirical Analysis",
        "Evaluation of algorithms",
        "Counterfactual Scenarios"
      ],
      "methodology_detail": "Analyzes recommendation fairness and network properties over time",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.02817v2",
    "title": "Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing",
    "year": 2024,
    "authors": [
      "Xianli Zeng",
      "Guang Cheng",
      "Edgar Dobriban"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in classification, addressing social bias and discrimination issues related to protected groups.",
      "inequality_type": [
        "racial",
        "gender",
        "other"
      ],
      "other_detail": "focus on fairness constraints in AI classification",
      "affected_populations": [
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Bayes-optimal classifiers under fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.02680v2",
    "title": "Large Language Models are Geographically Biased",
    "year": 2024,
    "authors": [
      "Rohin Manvi",
      "Samar Khanna",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs related to socioeconomic conditions and geographic disparities, highlighting systemic errors affecting lower socioeconomic regions and subjective perceptions tied to geography.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Biases against regions with lower socioeconomic status",
      "affected_populations": [
        "low-income regions",
        "Africa"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and geospatial prediction analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.02639v2",
    "title": "\"It's how you do things that matters\": Attending to Process to Better Serve Indigenous Communities with Language Technologies",
    "year": 2024,
    "authors": [
      "Ned Cooper",
      "Courtney Heldreth",
      "Ben Hutchinson"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in language technology access and engagement with Indigenous communities, highlighting social and cultural considerations.",
      "inequality_type": [
        "linguistic",
        "cultural",
        "geographic"
      ],
      "other_detail": "Focus on Indigenous language and community engagement",
      "affected_populations": [
        "Indigenous communities",
        "Aboriginal peoples",
        "Torres Strait Islanders"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews"
      ],
      "methodology_detail": "Interviews with researchers and community members",
      "geographic_focus": [
        "Australia"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.01472v1",
    "title": "Synthetic Data for the Mitigation of Demographic Biases in Face Recognition",
    "year": 2024,
    "authors": [
      "Pietro Melzi",
      "Christian Rathgeb",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Aythami Morales",
      "Dominik Lawatsch",
      "Florian Domin",
      "Maxim Schaubert"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic biases in face recognition, which relate to social groups such as race and gender, and discusses fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "demographic"
      ],
      "other_detail": "Bias mitigation in AI face recognition systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Synthetic Data Generation",
        "Experiment"
      ],
      "methodology_detail": "Using synthetic data to balance demographic representation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.01811v1",
    "title": "A Distributionally Robust Optimisation Approach to Fair Credit Scoring",
    "year": 2024,
    "authors": [
      "Pablo Casas",
      "Christophe Mues",
      "Huan Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in credit scoring, a key social issue related to economic and social disparities, and discusses bias against groups. It focuses on algorithmic fairness, which impacts social inequality. The abstract emphasizes fairness metrics and bias correction, indicating concern with social discrimination.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "class"
      ],
      "other_detail": "Focus on fairness in financial decision-making systems",
      "affected_populations": [
        "credit applicants",
        "disadvantaged groups"
      ],
      "methodology": [
        "Distributionally Robust Optimisation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Applying DRO to fairness in credit scoring",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.01071v1",
    "title": "Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities",
    "year": 2024,
    "authors": [
      "Mahdi Erfanian",
      "H. V. Jagadish",
      "Abolfazl Asudeh"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses under-representation of minorities, fairness, and bias in AI, which are related to social inequalities such as race and ethnicity.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on minority coverage in data augmentation",
      "affected_populations": [
        "minorities",
        "under-represented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Data Augmentation"
      ],
      "methodology_detail": "Using generative models for data enhancement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.00955v2",
    "title": "FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with Contrastive Learning in Multimodal Electronic Health Records",
    "year": 2024,
    "authors": [
      "Yuqing Wang",
      "Malvika Pillai",
      "Yun Zhao",
      "Catherine Curtin",
      "Tina Hernandez-Boussard"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in healthcare AI, addressing biases related to demographic factors, which are social inequalities.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses social biases in medical predictive models",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware representation learning in EHRs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.00463v1",
    "title": "Understanding gender differences in experiences and concerns surrounding online harms: A short report on a nationally representative survey of UK adults",
    "year": 2024,
    "authors": [
      "Florence E. Enock",
      "Francesca Stevens",
      "Jonathan Bright",
      "Miranda Cross",
      "Pica Johansson",
      "Judy Wajcman",
      "Helen Z. Margetts"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in online harms, fears, and participation, highlighting gender inequality in online experiences and voice.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Large, nationally representative UK survey",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.00402v1",
    "title": "Investigating Bias Representations in Llama 2 Chat via Activation Steering",
    "year": 2024,
    "authors": [
      "Dawn Lu",
      "Nina Rimsky"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates societal biases related to gender, race, and religion in AI models, which are key aspects of social inequality. It addresses bias representation and fairness issues in AI systems impacting social groups. The focus on bias mitigation aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "race",
        "religion"
      ],
      "other_detail": "Bias mitigation in language models",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Activation Steering"
      ],
      "methodology_detail": "Probing and manipulating model activations for bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.17839v1",
    "title": "Global-Liar: Factuality of LLMs over Time and Geographic Regions",
    "year": 2024,
    "authors": [
      "Shujaat Mirza",
      "Bruno Coelho",
      "Yuyuan Cui",
      "Christina P√∂pper",
      "Damon McCoy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs related to geographic regions, highlighting disparities between Global North and South, which reflect social and informational inequalities.",
      "inequality_type": [
        "geographic",
        "informational"
      ],
      "other_detail": "Biases in AI affecting global information equity",
      "affected_populations": [
        "Global South",
        "Africa",
        "Middle East"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Balanced dataset for geographic and temporal evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.16784v1",
    "title": "Graph Fairness Learning under Distribution Shifts",
    "year": 2024,
    "authors": [
      "Yibo Li",
      "Xiao Wang",
      "Yujie Xing",
      "Shaohua Fan",
      "Ruijia Wang",
      "Yaoqi Liu",
      "Chuan Shi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in graph neural networks related to sensitive attributes like gender and race, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on fairness under distribution shifts",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias factors and tests fairness frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.16197v1",
    "title": "Geospatial Disparities: A Case Study on Real Estate Prices in Paris",
    "year": 2024,
    "authors": [
      "Agathe Fernandes Machado",
      "Fran√ßois Hu",
      "Philipp Ratz",
      "Ewen Gallic",
      "Arthur Charpentier"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases and disparities in geospatial data affecting societal groups, highlighting fairness and ethical concerns related to social inequality.",
      "inequality_type": [
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on spatial disparities and fairness in real estate data",
      "affected_populations": [
        "urban residents",
        "disadvantaged neighborhoods"
      ],
      "methodology": [
        "Case Study",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection and mitigation in geospatial predictive models",
      "geographic_focus": [
        "Paris"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.16092v3",
    "title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You",
    "year": 2024,
    "authors": [
      "Felix Friedrich",
      "Katharina H√§mmerl",
      "Patrick Schramowski",
      "Manuel Brack",
      "Jindrich Libovicky",
      "Kristian Kersting",
      "Alexander Fraser"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender biases in multilingual AI models, highlighting social discrimination and bias issues related to gender. It discusses how these biases vary across languages and impact different social groups, addressing fairness concerns in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in multilingual AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Benchmark Creation",
        "Analysis"
      ],
      "methodology_detail": "Develops MAGBIG benchmark and analyzes bias across languages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.16088v1",
    "title": "Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity",
    "year": 2024,
    "authors": [
      "Andrew Bell",
      "Joao Fonseca",
      "Carlo Abrate",
      "Francesco Bonchi",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in algorithmic recourse related to initial circumstances, effort, and time, which are linked to social inequalities. It addresses fairness issues affecting marginalized populations and considers normative views aligned with substantive equality of opportunity.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "disability"
      ],
      "other_detail": "Focus on initial circumstances and effort disparities",
      "affected_populations": [
        "marginalized groups",
        "disadvantaged individuals"
      ],
      "methodology": [
        "Agent-based simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulating effort and fairness interventions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.16457v2",
    "title": "Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters",
    "year": 2024,
    "authors": [
      "Shahed Masoudian",
      "Cornelia Volaucnik",
      "Markus Schedl",
      "Navid Rekabsaz"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias mitigation and fairness in AI models, addressing social bias issues related to protected attributes. It discusses controlling bias to improve fairness, which relates to social discrimination concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "disability",
        "educational"
      ],
      "other_detail": "Focus on fairness and bias control in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "disabled individuals",
        "educational minorities"
      ],
      "methodology": [
        "Experiment",
        "Fairness Regularization",
        "Adversarial Debiasing"
      ],
      "methodology_detail": "Bias mitigation and fairness evaluation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.01732v2",
    "title": "Identifying and Improving Disability Bias in GPT-Based Resume Screening",
    "year": 2024,
    "authors": [
      "Kate Glazko",
      "Yusuf Mohammed",
      "Ben Kosa",
      "Venkatesh Potluri",
      "Jennifer Mankoff"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias related to disability in AI systems used in hiring, addressing social discrimination and inequality faced by disabled populations, and analyzes AI's impact on marginalized groups.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "focused on disability bias in AI hiring tools",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Experiment",
        "Qualitative Study",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Resume audit and bias analysis in GPT-4",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.15585v1",
    "title": "Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting",
    "year": 2024,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala",
      "Naoaki Okazaki",
      "Timothy Baldwin"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in large language models, addressing social discrimination related to gender. It examines how AI systems reproduce or mitigate societal biases, which directly relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Benchmark Creation"
      ],
      "methodology_detail": "Constructs a gender bias counting benchmark with CoT prompting",
      "geographic_focus": [
        "English-speaking regions"
      ],
      "ai_relationship": "AI as amplifier and mitigator",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.14616v1",
    "title": "Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse",
    "year": 2024,
    "authors": [
      "Seungyoon Lee",
      "Dahyun Jung",
      "Chanjun Park",
      "Seolhwa Lee",
      "Heuiseok Lim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses hate speech related to social issues like racial and gender discrimination, which are forms of social inequality, and discusses combating these biases in AI-mediated discourse.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on hate speech and social bias mitigation",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Developing datasets and NLP techniques for discourse analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.14581v2",
    "title": "AVELA -- A Vision for Engineering Literacy & Access: Understanding Why Technology Alone Is Not Enough",
    "year": 2024,
    "authors": [
      "Kyle Johnson",
      "Vicente Arroyos",
      "Celeste Garcia",
      "Liban Hussein",
      "Aisha Cora",
      "Tsewone Melaku",
      "Jay L. Cunningham",
      "R. Benjamin Shapiro",
      "Vikram Iyer"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in technology access among Black and Latine communities, highlighting social justice and educational inequalities. It discusses socio-technical access barriers and aims to improve engagement for marginalized groups.",
      "inequality_type": [
        "racial",
        "educational",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Focus on urban Black and Latine communities",
      "affected_populations": [
        "Black communities",
        "Latine communities"
      ],
      "methodology": [
        "Qualitative Study",
        "Case Study",
        "Survey"
      ],
      "methodology_detail": "Interviews and program evaluation over four years",
      "geographic_focus": [
        "urban areas in the US"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.14438v2",
    "title": "From the Fair Distribution of Predictions to the Fair Distribution of Social Goods: Evaluating the Impact of Fair Machine Learning on Long-Term Unemployment",
    "year": 2024,
    "authors": [
      "Sebastian Zezulka",
      "Konstantin Genin"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how algorithmic fairness impacts gender inequalities in unemployment, addressing social disparities.",
      "inequality_type": [
        "gender",
        "economic",
        "social"
      ],
      "other_detail": "Focus on long-term unemployment and gender gap",
      "affected_populations": [
        "women",
        "long-term unemployed"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Simulation"
      ],
      "methodology_detail": "Using administrative data and policy simulation",
      "geographic_focus": [
        "Switzerland"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.13935v1",
    "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
    "year": 2024,
    "authors": [
      "Lucius E. J. Bynum",
      "Joshua R. Loftus",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses counterfactual reasoning related to demographic characteristics and social concerns, addressing issues of fairness and social bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "demographic"
      ],
      "other_detail": "Focuses on legal protections and demographic data",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Counterfactual Reasoning"
      ],
      "methodology_detail": "Proposes a new framework for causal reasoning",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.13867v1",
    "title": "Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation",
    "year": 2024,
    "authors": [
      "Yifan Yang",
      "Xiaoyu Liu",
      "Qiao Jin",
      "Furong Huang",
      "Zhiyong Lu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in language models affecting healthcare, highlighting disparities in treatment and outcomes across racial groups.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial bias in medical AI applications",
      "affected_populations": [
        "White patients",
        "other racial groups"
      ],
      "methodology": [
        "Qualitative Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes biases in generated medical reports",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.13555v3",
    "title": "Benchmarking the Fairness of Image Upsampling Methods",
    "year": 2024,
    "authors": [
      "Mike Laszkiewicz",
      "Imant Daunhawer",
      "Julia E. Vogt",
      "Asja Fischer",
      "Johannes Lederer"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness and diversity in AI-generated images, highlighting biases related to dataset imbalance and racial distribution, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial fairness in face image datasets",
      "affected_populations": [
        "racial groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Benchmarking fairness metrics on image upsampling methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.13097v2",
    "title": "Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems",
    "year": 2024,
    "authors": [
      "Michelle R. Greene",
      "Mariam Josyula",
      "Wentao Si",
      "Jennifer A. Hart"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "68-02",
      "I.2.m"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates socioeconomic biases in AI systems affecting social groups, highlighting disparities in performance linked to socioeconomic status and related social biases.",
      "inequality_type": [
        "socioeconomic",
        "income",
        "class"
      ],
      "other_detail": "Biases in AI affecting social and economic groups",
      "affected_populations": [
        "low SES homes",
        "diverse US communities",
        "global communities"
      ],
      "methodology": [
        "Deep Learning",
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzed performance across socioeconomic indicators",
      "geographic_focus": [
        "United States",
        "Global"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.12824v2",
    "title": "MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage",
    "year": 2024,
    "authors": [
      "Ying Song",
      "Balaji Palanisamy"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems, which relate to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on fairness in graph neural networks",
      "affected_populations": [
        "social groups",
        "users of AI systems"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Constraints",
        "Adversarial Debiasing"
      ],
      "methodology_detail": "Debiasing framework for node classification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.12722v2",
    "title": "Falcon: Fair Active Learning using Multi-armed Bandits",
    "year": 2024,
    "authors": [
      "Ki Hyun Tae",
      "Hantian Zhang",
      "Jaeyoung Park",
      "Kexin Rong",
      "Steven Euijong Whang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing bias related to groups such as gender, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "fairness"
      ],
      "other_detail": "Fairness in AI systems and dataset curation",
      "affected_populations": [
        "females",
        "target groups"
      ],
      "methodology": [
        "Active Learning",
        "Multi-armed Bandits",
        "Algorithm Design"
      ],
      "methodology_detail": "Fairness-aware sample selection using bandit algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.12720v1",
    "title": "A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions",
    "year": 2024,
    "authors": [
      "Guilherme H. Resende",
      "Luiz F. Nery",
      "Fabr√≠cio Benevenuto",
      "Savvas Zannettou",
      "Flavio Figueiredo"
    ],
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI systems toward African American English, highlighting racial bias in toxicity and sentiment analysis, which impacts social perceptions and treatment of AAE speakers.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias in AI systems towards linguistic and racial groups",
      "affected_populations": [
        "African Americans",
        "AAE speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias across datasets and linguistic features",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.11892v5",
    "title": "AI, insurance, discrimination and unfair differentiation. An overview and research agenda",
    "year": 2024,
    "authors": [
      "Marvin S. L. van Bekkum",
      "Frederik Zuiderveen Borgesius",
      "Tom Heskes"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discriminatory effects of AI in insurance, focusing on social bias and fairness issues impacting society.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focus on discrimination in AI-driven insurance practices",
      "affected_populations": [
        "racial minorities",
        "women",
        "low-income groups",
        "health vulnerable"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing societal and ethical implications of AI in insurance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.11487v1",
    "title": "Towards Better Inclusivity: A Diverse Tweet Corpus of English Varieties",
    "year": 2024,
    "authors": [
      "Nhi Pham",
      "Lachlan Pham",
      "Adam L. Meyers"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses linguistic diversity and biases affecting underrepresented social groups, highlighting disparities in NLP tools that impact marginalized language speakers.",
      "inequality_type": [
        "linguistic",
        "digital",
        "educational"
      ],
      "other_detail": "Bias in language technology affecting marginalized groups",
      "affected_populations": [
        "non-western English speakers",
        "regional language users"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Annotation Framework"
      ],
      "methodology_detail": "Curated and annotated diverse tweet corpus",
      "geographic_focus": [
        "India",
        "Singapore",
        "Africa",
        "7 countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.10632v2",
    "title": "Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach",
    "year": 2024,
    "authors": [
      "Aoqi Zuo",
      "Yiqing Li",
      "Susan Wei",
      "Mingming Gong"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on preventing discrimination based on sensitive attributes like gender and race, which are central to social inequalities.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Fairness in AI systems addressing social discrimination",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Causal Inference",
        "Constrained Optimization"
      ],
      "methodology_detail": "Using causal graphs to measure and enforce fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.10629v3",
    "title": "A Critical Reflection on the Use of Toxicity Detection Algorithms in Proactive Content Moderation Systems",
    "year": 2024,
    "authors": [
      "Mark Warner",
      "Angelika Strohmayer",
      "Matthew Higgs",
      "Lynne Coventry"
    ],
    "categories": [
      "cs.HC",
      "H.5.2"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how toxicity detection algorithms impact content moderation, highlighting concerns about inequalities and biases affecting different social groups, especially in contexts where these algorithms may exacerbate disparities or be misused.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on content moderation and social biases",
      "affected_populations": [
        "content creators",
        "minority groups",
        "users"
      ],
      "methodology": [
        "Qualitative Study",
        "Design Workshops"
      ],
      "methodology_detail": "Stakeholder engagement and critical analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.10545v3",
    "title": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
    "year": 2024,
    "authors": [
      "Yashar Deldjoo"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and fairness issues in AI recommendation systems, focusing on provider fairness and social biases related to demographic attributes, which are central to social inequality concerns.",
      "inequality_type": [
        "gender",
        "age",
        "racial",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focuses on fairness and bias in AI systems",
      "affected_populations": [
        "demographic groups",
        "users by age",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates prompt strategies and bias impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.10535v1",
    "title": "The \"Colonial Impulse\" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases",
    "year": 2024,
    "authors": [
      "Dipto Das",
      "Shion Guha",
      "Jed Brubaker",
      "Bryan Semaan"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in sentiment analysis tools affecting Bengali communities, focusing on identity categories impacted by colonialism, thus addressing social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "religion",
        "nationality"
      ],
      "other_detail": "Coloniality and identity-based bias in AI tools",
      "affected_populations": [
        "Bengali communities",
        "identity groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Natural Language Processing"
      ],
      "methodology_detail": "Auditing Bengali sentiment analysis tools for bias",
      "geographic_focus": [
        "Bangladesh",
        "West Bengal"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.10016v1",
    "title": "Gender Bias in Machine Translation and The Era of Large Language Models",
    "year": 2024,
    "authors": [
      "Eva Vanmassenhove"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, highlighting issues of fairness and social discrimination related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language technologies",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Literature Review"
      ],
      "methodology_detail": "Assessing ChatGPT's capacity to address gender bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.08788v2",
    "title": "The Impact of Differential Feature Under-reporting on Algorithmic Fairness",
    "year": 2024,
    "authors": [
      "Nil-Jana Akpinar",
      "Zachary C. Lipton",
      "Alexandra Chouldechova"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines data bias affecting algorithmic fairness, impacting marginalized groups.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on data bias in public sector decision-making",
      "affected_populations": [
        "Medicaid recipients",
        "Medicare beneficiaries"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Analytical modeling and bias mitigation methods",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.08495v2",
    "title": "Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans",
    "year": 2024,
    "authors": [
      "Messi H. J. Lee",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in LLMs related to race and gender, highlighting how these models portray social groups as more homogeneous, which can perpetuate stereotypes and discrimination.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI reflecting social psychological phenomena",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Analyzed generated texts for group homogeneity",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.12985v1",
    "title": "The Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias",
    "year": 2024,
    "authors": [
      "Kausik Lakkaraju",
      "Aniket Gupta",
      "Biplav Srivastava",
      "Marco Valtorta",
      "Dezhi Wu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in sentiment analysis systems related to protected attributes like gender, race, and age, which are key social inequality factors. It assesses how AI systems reflect or mitigate social biases, impacting different social groups. The focus on bias measurement and fairness aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias assessment in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Using synthetic and human-generated datasets for bias testing",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.07440v2",
    "title": "The Fairness of Redistricting Ghost",
    "year": 2024,
    "authors": [
      "Jia-Wei Liang",
      "Nina Amenta"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes fairness in redistricting, a process impacting racial and socioeconomic representation, without explicitly addressing social inequalities or discrimination. It focuses on electoral districting fairness, which relates to social group impacts. The abstract discusses minority representation, indicating relevance to racial and social equity issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on electoral districting fairness and minority representation",
      "affected_populations": [
        "minority voters"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Game theory and bounds analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.06495v1",
    "title": "An investigation of structures responsible for gender bias in BERT and DistilBERT",
    "year": 2024,
    "authors": [
      "Thibaud Leteno",
      "Antoine Gourru",
      "Charlotte Laclau",
      "Christophe Gravier"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, addressing social discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Empirical Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes neural mechanisms of gender bias in models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.05655v1",
    "title": "Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability",
    "year": 2024,
    "authors": [
      "Kaixun Yang",
      "Mladen Rakoviƒá",
      "Yuyang Li",
      "Quanlong Guan",
      "Dragan Ga≈°eviƒá",
      "Guanliang Chen"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases and fairness in AES models affecting marginalized groups, highlighting social inequality issues.",
      "inequality_type": [
        "economic",
        "educational",
        "disability"
      ],
      "other_detail": "Bias in AI affecting marginalized student groups",
      "affected_populations": [
        "economically disadvantaged students",
        "students with disabilities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Performance and bias evaluation across demographic groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.04972v2",
    "title": "Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation",
    "year": 2024,
    "authors": [
      "Ian Stewart",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL",
      "I.2.7; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in machine translation related to social relationships, specifically same-gender partnerships, highlighting social discrimination and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias against same-gender relationships in translation",
      "affected_populations": [
        "LGBTQ+ individuals"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Evaluation of translation errors across different contexts",
      "geographic_focus": [
        "Spanish-speaking countries"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2402.00037v1",
    "title": "Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity",
    "year": 2024,
    "authors": [
      "Nia Nixon",
      "Yiwen Lin",
      "Lauren Snow"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses inequality in STEM, focusing on underrepresented groups and promoting inclusion through AI, which relates to social discrimination and bias.",
      "inequality_type": [
        "educational",
        "gender",
        "racial"
      ],
      "other_detail": "Focus on diversity and inclusion in STEM teams",
      "affected_populations": [
        "underrepresented students",
        "minority groups"
      ],
      "methodology": [
        "Computational Modeling",
        "Generative AI",
        "Policy Analysis"
      ],
      "methodology_detail": "Applying AI for assessment and support systems",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.03562v2",
    "title": "GLOCALFAIR: Jointly Improving Global and Local Group Fairness in Federated Learning",
    "year": 2024,
    "authors": [
      "Syed Irfan Ali Meerza",
      "Luyang Liu",
      "Jiaxin Zhang",
      "Jian Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues related to demographic groups in AI models, focusing on racial and gender biases, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in federated learning models",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness-aware clustering",
        "Constrained Optimization",
        "Experiment"
      ],
      "methodology_detail": "Joint fairness optimization without sensitive data sharing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.03307v2",
    "title": "Modeling Processes of Neighborhood Change",
    "year": 2024,
    "authors": [
      "J. Carlos Mart√≠nez Mori",
      "Zhanzhan Zhao"
    ],
    "categories": [
      "cs.MA",
      "91D10, 91A80, 90B06"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models neighborhood change processes affecting access and benefits, which relate to socioeconomic and geographic inequalities. It examines how urban planning impacts different communities over time, addressing fairness and inequality concerns.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on neighborhood and access disparities",
      "affected_populations": [
        "underserved communities",
        "local residents"
      ],
      "methodology": [
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling neighborhood dynamics and spatial economics",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.03097v1",
    "title": "Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification",
    "year": 2024,
    "authors": [
      "Xiaobin Song",
      "Zeyuan Liu",
      "Benben Jiang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SY",
      "eess.SY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI classification, addressing discrimination and bias issues related to social groups. It aims to improve fairness indicators, which are directly linked to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI classification models",
      "affected_populations": [
        "racial minorities",
        "gender groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness-aware Reweighting",
        "Theoretical Analysis",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness-aware reweighting and theoretical bounds",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.03030v1",
    "title": "Exploring Gender Biases in Language Patterns of Human-Conversational Agent Conversations",
    "year": 2024,
    "authors": [
      "Weizi Liu"
    ],
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI interactions, addressing gender inequality and stereotypes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in conversational AI",
      "affected_populations": [
        "users",
        "female personas"
      ],
      "methodology": [
        "Linguistic Analysis",
        "Behavioral Study"
      ],
      "methodology_detail": "Analyzing linguistic styles and perceptions in interactions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.02012v1",
    "title": "Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning",
    "year": 2024,
    "authors": [
      "Allen Minch",
      "Hung Anh Vu",
      "Anne Marie Warren"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.NA",
      "math.NA",
      "65F10, 65F22, 65K05, 90C47"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias related to sensitive attributes like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Adversarial Training",
        "Robust Optimization",
        "Machine Learning",
        "Deep Learning"
      ],
      "methodology_detail": "Second-order optimization for fairness enhancement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.01640v1",
    "title": "Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data",
    "year": 2024,
    "authors": [
      "Sofia Yfantidou",
      "Dimitris Spathis",
      "Marios Constantinides",
      "Athena Vakali",
      "Daniele Quercia",
      "Fahim Kawsar"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI models, focusing on bias reduction across demographic groups, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "health",
        "educational"
      ],
      "other_detail": "Fairness in AI models for human-centric applications",
      "affected_populations": [
        "demographic groups",
        "health patients"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparative analysis of SSL and supervised models on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.01262v2",
    "title": "Fairness Certification for Natural Language Processing and Large Language Models",
    "year": 2024,
    "authors": [
      "Vincent Freiberger",
      "Erik Buchmann"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in NLP, addressing biases and discrimination, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focus on algorithmic fairness in social contexts",
      "affected_populations": [
        "minorities",
        "women",
        "students",
        "disadvantaged groups"
      ],
      "methodology": [
        "Literature Review",
        "Expert Interviews",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Qualitative review and expert insights on fairness criteria",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.00763v3",
    "title": "New Job, New Gender? Measuring the Social Bias in Image Generation Models",
    "year": 2024,
    "authors": [
      "Wenxuan Wang",
      "Haonan Bai",
      "Jen-tse Huang",
      "Yuxuan Wan",
      "Youliang Yuan",
      "Haoyi Qiu",
      "Nanyun Peng",
      "Michael R. Lyu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases related to gender, race, and age in AI-generated images, highlighting social stereotypes and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias evaluation in AI image generation models",
      "affected_populations": [
        "women",
        "racial minorities",
        "older adults"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Automatic bias detection framework and comparative analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  }
]