[
  {
    "id": "http://arxiv.org/abs/2012.15211v1",
    "title": "The Challenges of Crowd Workers in Rural and Urban America",
    "year": 2020,
    "authors": [
      "Claudia Flores-Saviaga",
      "Yuwen Li",
      "Benjamin V. Hanrahan",
      "Jeffrey Bigham",
      "Saiph Savage"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities faced by rural versus urban crowd workers, highlighting inequalities related to geographic location, economic opportunities, and cultural traits, which are key aspects of social inequality.",
      "inequality_type": [
        "economic",
        "geographic",
        "educational"
      ],
      "other_detail": "Focus on rural-urban disparities in gig work",
      "affected_populations": [
        "rural workers",
        "urban workers"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Survey of crowd workers across regions",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.14961v1",
    "title": "Towards Fair Deep Anomaly Detection",
    "year": 2020,
    "authors": [
      "Hongjing Zhang",
      "Ian Davidson"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on social bias and discrimination, which are related to social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Fairness in anomaly detection models",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Adversarial Network",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness measures and model analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.13176v1",
    "title": "Gender Bias in Multilingual Neural Machine Translation: The Architecture Matters",
    "year": 2020,
    "authors": [
      "Marta R. Costa-jussà",
      "Carlos Escolano",
      "Christine Basta",
      "Javier Ferrando",
      "Roser Batlle",
      "Ksenia Kharitonova"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in multilingual neural machine translation, addressing gender fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias mitigation in AI models",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Interpretability Analysis"
      ],
      "methodology_detail": "Analyzes bias across architectures with experiments and interpretability tools",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.11399v2",
    "title": "The COVID-19 pandemic: socioeconomic and health disparities",
    "year": 2020,
    "authors": [
      "Behzad Javaheri"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in COVID-19 outcomes linked to socioeconomic and health factors, highlighting pre-existing inequalities affecting vulnerable groups.",
      "inequality_type": [
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on health and socioeconomic disparities in pandemic outcomes",
      "affected_populations": [
        "disadvantaged groups",
        "vulnerable populations"
      ],
      "methodology": [
        "Correlation Analysis",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predictive modeling of mortality using socioeconomic variables",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.14285v1",
    "title": "Affirmative Algorithms: The Legal Grounds for Fairness as Awareness",
    "year": 2020,
    "authors": [
      "Daniel E. Ho",
      "Alice Xiang"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses legal frameworks related to racial and historical discrimination, addressing social fairness issues in algorithmic decision-making.",
      "inequality_type": [
        "racial",
        "historical discrimination"
      ],
      "other_detail": "Focuses on legal grounds for fairness in algorithms",
      "affected_populations": [
        "racial minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Legal Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes legal cases and doctrinal approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as regulation subject",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.10348v1",
    "title": "Small Business Classification By Name: Addressing Gender and Geographic Origin Biases",
    "year": 2020,
    "authors": [
      "Daniel Shapiro"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in small business classification, a social fairness issue.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Bias mitigation in AI models for social fairness",
      "affected_populations": [
        "women",
        "business owners from specific regions"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Mitigation"
      ],
      "methodology_detail": "Bias reduction techniques in classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.09951v1",
    "title": "Fairkit, Fairkit, on the Wall, Who's the Fairest of Them All? Supporting Data Scientists in Training Fair Models",
    "year": 2020,
    "authors": [
      "Brittany Johnson",
      "Jesse Bartola",
      "Rico Angell",
      "Katherine Keith",
      "Sam Witty",
      "Stephen J. Giguere",
      "Yuriy Brun"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in AI, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in machine learning models",
      "affected_populations": [
        "women",
        "people of color"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates fairness tools via user study",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.06850v1",
    "title": "Trading the System Efficiency for the Income Equality of Drivers in Rideshare",
    "year": 2020,
    "authors": [
      "Yifan Xu",
      "Pan Xu"
    ],
    "categories": [
      "cs.AI",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines income inequality among rideshare drivers, focusing on demographic factors and fairness, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on demographic-based income disparities in rideshare drivers",
      "affected_populations": [
        "drivers",
        "demographic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Modeling and algorithm development for fairness-profit tradeoff",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.06157v3",
    "title": "Fairness in Rating Prediction by Awareness of Verbal and Gesture Quality of Public Speeches",
    "year": 2020,
    "authors": [
      "Ankani Chattoraj",
      "Rupam Acharyya",
      "Shouman Das",
      "Md. Iftekhar Tanveer",
      "Ehsan Hoque"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial and gender biases in speech ratings, highlighting fairness issues related to social attributes.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI-based rating systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Neural Network",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Incorporates fairness-aware loss functions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.06057v1",
    "title": "Interdisciplinary Approaches to Understanding Artificial Intelligence's Impact on Society",
    "year": 2020,
    "authors": [
      "Suresh Venkatasubramanian",
      "Nadya Bliss",
      "Helen Nissenbaum",
      "Melanie Moses"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases, discrimination, and inequalities in AI systems affecting societal groups, highlighting social harms and the need for interdisciplinary approaches.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias",
        "inequality"
      ],
      "other_detail": "Focus on societal biases and inequalities in AI",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "societal groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing societal impacts and interdisciplinary approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.04955v1",
    "title": "Breeding Gender-aware Direct Speech Translation Systems",
    "year": 2020,
    "authors": [
      "Marco Gaido",
      "Beatrice Savoldi",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in speech translation systems, a social inequality issue related to gender discrimination and fairness in AI. It discusses gender bias mitigation, impacting social groups based on gender identity.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "gender minorities",
        "speech users"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Annotating datasets with gender information for testing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.04842v2",
    "title": "Improving the Fairness of Deep Generative Models without Retraining",
    "year": 2020,
    "authors": [
      "Shuhan Tan",
      "Yujun Shen",
      "Bolei Zhou"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in face synthesis related to race and gender, highlighting fairness issues in AI systems that impact social groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation without retraining",
      "affected_populations": [
        "minority groups",
        "racial minorities",
        "gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias analysis and fairness adjustment in GANs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.03812v1",
    "title": "Improving Fairness and Privacy in Selection Problems",
    "year": 2020,
    "authors": [
      "Mohammad Mahdi Khalili",
      "Xueru Zhang",
      "Mahed Abroshan",
      "Somayeh Sojoudi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in decision-making models affecting protected groups, such as race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Focus on fairness in selection processes",
      "affected_populations": [
        "racial minorities",
        "women",
        "applicants"
      ],
      "methodology": [
        "Experiment",
        "Fairness Analysis",
        "Differential Privacy"
      ],
      "methodology_detail": "Using exponential mechanisms for fairness and privacy",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.03063v2",
    "title": "FairOD: Fairness-aware Outlier Detection",
    "year": 2020,
    "authors": [
      "Shubhranshu Shekhar",
      "Neil Shah",
      "Leman Akoglu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in outlier detection related to protected variables like race, ethnicity, sex, and age, which are key social inequality factors. It discusses algorithmic bias and unjust outcomes affecting marginalized groups. The focus on fairness criteria and protected attributes indicates a direct engagement with social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "ethnic",
        "discrimination"
      ],
      "other_detail": "Fairness in AI detection of minority groups",
      "affected_populations": [
        "minority groups",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Criteria Formalization",
        "Experiment"
      ],
      "methodology_detail": "Designing fairness-aware outlier detection algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.02972v3",
    "title": "Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy",
    "year": 2020,
    "authors": [
      "Kit T. Rodolfa",
      "Hemank Lamba",
      "Rayid Ghani"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness in machine learning applied to resource allocation in social policy contexts, focusing on disparities affecting marginalized groups such as racial minorities. It empirically examines fairness-accuracy trade-offs, directly addressing social fairness issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational",
        "health"
      ],
      "other_detail": "Fairness in policy-related machine learning applications",
      "affected_populations": [
        "racial minorities",
        "low-income groups",
        "students",
        "mental health patients",
        "housing residents"
      ],
      "methodology": [
        "Machine Learning",
        "Empirical Study",
        "Post-hoc Disparity Mitigation"
      ],
      "methodology_detail": "Focus on real-world policy settings and fairness methods",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.02845v4",
    "title": "Experimental Evaluation of Algorithm-Assisted Human Decision-Making: Application to Pretrial Public Safety Assessment",
    "year": 2020,
    "authors": [
      "Kosuke Imai",
      "Zhichao Jiang",
      "James Greiner",
      "Ryan Halen",
      "Sooahn Shin"
    ],
    "categories": [
      "cs.CY",
      "stat.AP",
      "stat.ME"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how algorithmic recommendations influence human decisions related to justice, gender bias, and fairness, highlighting social disparities. It analyzes impacts on different social groups, particularly gender and race, in a criminal justice context.",
      "inequality_type": [
        "gender",
        "racial",
        "fairness"
      ],
      "other_detail": "Focus on bias and fairness in criminal justice decisions",
      "affected_populations": [
        "female arrestees",
        "male arrestees",
        "judges"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Randomized controlled trial and causal impact evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.02394v1",
    "title": "Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics",
    "year": 2020,
    "authors": [
      "Bo Cowgill",
      "Fabrizio Dell'Acqua",
      "Samuel Deng",
      "Daniel Hsu",
      "Nakul Verma",
      "Augustin Chaintreau"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and demographic groups in AI predictions, addressing social discrimination and fairness issues. It analyzes how algorithmic bias impacts different social groups and explores interventions to reduce such biases.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on bias in AI predictions related to social groups",
      "affected_populations": [
        "women",
        "minorities",
        "students"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Large-scale field experiment with randomized interventions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.03659v2",
    "title": "Non-portability of Algorithmic Fairness in India",
    "year": 2020,
    "authors": [
      "Nithya Sambasivan",
      "Erin Arnesen",
      "Ben Hutchinson",
      "Vinodkumar Prabhakaran"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness in India, focusing on social groups and oppressed communities, highlighting challenges related to social discrimination and inequality. It emphasizes the impact of AI systems on marginalized populations, addressing social bias and fairness issues.",
      "inequality_type": [
        "social",
        "educational",
        "geographic"
      ],
      "other_detail": "Focus on Indian socio-cultural context",
      "affected_populations": [
        "oppressed communities",
        "marginalized groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Analysis"
      ],
      "methodology_detail": "Expert interviews and deployment analysis",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.15255v1",
    "title": "Gender issues in fundamental physics: Strumia's bibliometric analysis fails to account for key confounders and confuses correlation with causation",
    "year": 2020,
    "authors": [
      "Philip Ball",
      "T. Benjamin Britton",
      "Erin Hengel",
      "Philip Moriarty",
      "Rachel A. Oliver",
      "Gina Rippon",
      "Angela Saini",
      "Jessica Wade"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.DL",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gender differences in physics publications and citations, addressing gender inequality and bias. It critiques interpretations related to gender disparities, implying a focus on social gender issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender disparities in scientific publishing",
      "affected_populations": [
        "women in physics"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Re-analysis adjusting for confounders",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.01696v2",
    "title": "FairBatch: Batch Selection for Model Fairness",
    "year": 2020,
    "authors": [
      "Yuji Roh",
      "Kangwook Lee",
      "Steven Euijong Whang",
      "Changho Suh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing social bias and discrimination issues related to demographic groups. It aims to improve fairness measures like equal opportunity and demographic parity, which are directly linked to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Addresses algorithmic fairness in social contexts",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware batch selection algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.00423v2",
    "title": "Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring",
    "year": 2020,
    "authors": [
      "Tom Sühr",
      "Sophie Hilgard",
      "Himabindu Lakkaraju"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in online hiring affecting underrepresented groups, focusing on gender disparities and their outcomes.",
      "inequality_type": [
        "gender",
        "social discrimination"
      ],
      "other_detail": "Focus on gender biases in online hiring platforms",
      "affected_populations": [
        "women",
        "minorities"
      ],
      "methodology": [
        "Experiment",
        "Large-scale user study",
        "Data analysis"
      ],
      "methodology_detail": "Simulating online hiring scenarios with real platform data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.00282v2",
    "title": "FairFaceGAN: Fairness-aware Facial Image-to-Image Translation",
    "year": 2020,
    "authors": [
      "Sunhee Hwang",
      "Sungho Park",
      "Dohyung Kim",
      "Mirae Do",
      "Hyeran Byun"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in facial image translation, addressing bias related to protected attributes like gender, age, and race, which are social categories linked to inequality. It aims to prevent unwanted translation of these attributes, directly engaging with social fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Fairness in facial attribute translation",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Fairness Metric Development"
      ],
      "methodology_detail": "Develops a fairness-aware generative model and new metric",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.01930v1",
    "title": "Learning Explainable Interventions to Mitigate HIV Transmission in Sex Workers Across Five States in India",
    "year": 2020,
    "authors": [
      "Raghav Awasthi",
      "Prachi Patel",
      "Vineet Joshi",
      "Shama Karkal",
      "Tavpritesh Sethi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses health disparities among female sex workers, a marginalized social group, and explores interventions to improve their access to care and safe-sex practices.",
      "inequality_type": [
        "health",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on vulnerable, stigmatized populations in India",
      "affected_populations": [
        "female sex workers"
      ],
      "methodology": [
        "Machine Learning",
        "Bayesian Network",
        "Discriminative Modeling",
        "Field Trial"
      ],
      "methodology_detail": "Combines structure learning and predictive modeling approaches",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.14075v1",
    "title": "Feedback Effects in Repeat-Use Criminal Risk Assessments",
    "year": 2020,
    "authors": [
      "Benjamin Laufer"
    ],
    "categories": [
      "cs.CY",
      "cs.DS",
      "cs.LG",
      "cs.SI",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines feedback effects in risk assessments used in criminal justice, highlighting how biases can amplify over sequential decisions, which relates to social disparities in legal treatment.",
      "inequality_type": [
        "racial",
        "social",
        "justice"
      ],
      "other_detail": "Focuses on bias propagation in criminal risk assessments",
      "affected_populations": [
        "racial minorities",
        "criminal defendants"
      ],
      "methodology": [
        "Simulation",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Uses Polya Urn model and simulations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2012.01193v1",
    "title": "Black Loans Matter: Distributionally Robust Fairness for Fighting Subgroup Discrimination",
    "year": 2020,
    "authors": [
      "Mark Weber",
      "Mikhail Yurochkin",
      "Sherif Botros",
      "Vanio Markov"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses subgroup discrimination in lending, highlighting racial biases and fairness issues, which relate directly to social inequalities.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial and subgroup discrimination in finance",
      "affected_populations": [
        "minority borrowers",
        "protected groups"
      ],
      "methodology": [
        "Fairness Metrics",
        "Algorithmic Fairness",
        "Statistical Analysis"
      ],
      "methodology_detail": "Using fairness and metric learning algorithms",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.13988v2",
    "title": "Reducing Discrimination in Learning Algorithms for Social Good in Sociotechnical Systems",
    "year": 2020,
    "authors": [
      "Katelyn Morrison"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how machine learning algorithms in sociotechnical systems can unintentionally discriminate against marginalized groups, highlighting issues of social bias and fairness.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "geographic"
      ],
      "other_detail": "Focuses on algorithmic fairness in urban mobility systems",
      "affected_populations": [
        "marginalized communities",
        "urban residents"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Bayesian Optimization"
      ],
      "methodology_detail": "Using Bayesian Optimization to address discrimination",
      "geographic_focus": [
        "Pittsburgh, PA"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.12465v1",
    "title": "The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability",
    "year": 2020,
    "authors": [
      "Sunipa Dev"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CG",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting, quantifying, and mitigating social biases in language representations, which relate to societal inequalities such as racial and gender biases.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Bias mitigation in language AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and mitigation techniques in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.12096v1",
    "title": "Gender bias in magazines oriented to men and women: a computational approach",
    "year": 2020,
    "authors": [
      "Diego Kozlowski",
      "Gabriela Lozano",
      "Carla M. Felcher",
      "Fernando Gonzalez",
      "Edgar Altszyler"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines gender stereotypes and biases in magazine content over time, addressing gender-related social biases and stereotypes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in media content",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Topic modeling and word-frequency analysis",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.12086v1",
    "title": "Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis",
    "year": 2020,
    "authors": [
      "Michael A. Lepori"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates intersectional biases related to race and gender in word embeddings, reflecting social discrimination. It aligns with issues of social bias and inequality in AI systems. The focus on biases against Black women indicates a concern with social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Intersectional bias analysis in AI embeddings",
      "affected_populations": [
        "Black women",
        "White women",
        "Black men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Representational similarity analysis of embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.12014v1",
    "title": "Argument from Old Man's View: Assessing Social Bias in Argumentation",
    "year": 2020,
    "authors": [
      "Maximilian Spliethöver",
      "Henning Wachsmuth"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social bias related to gender and ethnicity in argumentation, highlighting inequalities in data representation and bias amplification in NLP models.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in argumentative data sources",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and co-occurrence analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.11611v1",
    "title": "FERN: Fair Team Formation for Mutually Beneficial Collaborative Learning",
    "year": 2020,
    "authors": [
      "Maria Kalantzi",
      "Agoritsa Polyzou",
      "George Karypis"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in team formation, addressing discrimination related to protected attributes like race and gender, and promotes equitable collaborative learning.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Fairness in collaborative educational settings",
      "affected_populations": [
        "students",
        "educational participants"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Heuristic Algorithm",
        "Experimental Analysis"
      ],
      "methodology_detail": "Heuristic hill-climbing for fair team formation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.11311v2",
    "title": "Uncovering the Bias in Facial Expressions",
    "year": 2020,
    "authors": [
      "Jessica Deuschel",
      "Bettina Finzel",
      "Ines Rieger"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates bias related to gender and skin color in facial expression recognition, addressing social fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in facial expression analysis",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Heatmap Analysis"
      ],
      "methodology_detail": "Neural network training and performance analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.11483v4",
    "title": "Rethinking recidivism through a causal lens",
    "year": 2020,
    "authors": [
      "Vik Shirvaikar",
      "Choudur Lakshminarayan"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines incarceration's impact on recidivism, a social justice issue related to criminal justice inequality, which often intersects with racial and socioeconomic disparities.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on criminal justice and incarceration effects",
      "affected_populations": [
        "formerly incarcerated individuals"
      ],
      "methodology": [
        "Causal Inference",
        "Statistical Analysis",
        "Case Study"
      ],
      "methodology_detail": "Uses DAG adjustment and double machine learning",
      "geographic_focus": [
        "North Carolina"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.09625v2",
    "title": "Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP",
    "year": 2020,
    "authors": [
      "John Chen",
      "Ian Berlot-Attwell",
      "Safwan Hossain",
      "Xindi Wang",
      "Frank Rudzicz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness algorithms in clinical NLP, addressing bias and fairness issues related to protected groups, which are central to social inequality concerns.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in medical AI systems",
      "affected_populations": [
        "patients",
        "clinical groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fairness algorithm comparison in multimodal clinical data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.10472v2",
    "title": "GenderRobustness: Robustness of Gender Detection in Facial Recognition Systems with variation in Image Properties",
    "year": 2020,
    "authors": [
      "Sharadha Srinivasan",
      "Madan Musuvathi"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in facial recognition systems related to gender and other attributes, highlighting social discrimination concerns. It addresses fairness issues in AI systems affecting different social groups. The focus on bias and fairness indicates a connection to social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in facial recognition systems",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Testing robustness of gender detection algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.08946v3",
    "title": "A Unified Seeding Framework",
    "year": 2020,
    "authors": [
      "Ya-Wen Teng",
      "Hsi-Wen Chen",
      "De-Nian Yang",
      "Yvonne-Anne Pignolet",
      "Ting-Wei Li",
      "Lydia Chen"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in social media interactions and aims to promote under-represented groups, addressing gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender disparities in social media influence",
      "affected_populations": [
        "females",
        "under-represented groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation",
        "Algorithm Development"
      ],
      "methodology_detail": "Analyzes interaction data and proposes a seeding framework",
      "geographic_focus": [
        "Instagram",
        "Facebook"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.08898v1",
    "title": "Probing Fairness of Mobile Ocular Biometrics Methods Across Gender on VISOB 2.0 Dataset",
    "year": 2020,
    "authors": [
      "Anoop Krishnan",
      "Ali Almadan",
      "Ajita Rattani"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness across gender in biometric systems, addressing gender bias and discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on biometric fairness across social gender groups",
      "affected_populations": [
        "males",
        "females"
      ],
      "methodology": [
        "Experiment",
        "Deep Learning",
        "Dataset Analysis"
      ],
      "methodology_detail": "Fairness analysis using biometric models and datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.08278v1",
    "title": "A bibliometric methodology to unveil territorial inequities in the scientific wealth to combat COVID-19",
    "year": 2020,
    "authors": [
      "Giovanni Abramo",
      "Ciriaco Andrea D'Angelo"
    ],
    "categories": [
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in scientific research performance across territories, highlighting regional inequalities in medical knowledge during COVID-19, which relates to geographic and health inequalities.",
      "inequality_type": [
        "geographic",
        "health"
      ],
      "other_detail": "Focuses on regional disparities in scientific and health resources",
      "affected_populations": [
        "southern Italy",
        "regional populations"
      ],
      "methodology": [
        "Bibliometric Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing scientific output and disparities across territories",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.07495v1",
    "title": "FAIR: Fair Adversarial Instance Re-weighting",
    "year": 2020,
    "authors": [
      "Andrija Petrović",
      "Mladen Nikolić",
      "Sandro Radovanović",
      "Boris Delibašić",
      "Miloš Jovanović"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on biases related to sensitive attributes like race and gender, which are key social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training",
        "Reweighting"
      ],
      "methodology_detail": "Combines adversarial training with instance re-weighting",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.07194v2",
    "title": "Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy",
    "year": 2020,
    "authors": [
      "Amanda Coston",
      "Neel Guha",
      "Derek Ouyang",
      "Lisa Lu",
      "Alexandra Chouldechova",
      "Daniel E. Ho"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases in mobility data affecting vulnerable groups, highlighting disparities in older and non-white populations during COVID-19 response efforts.",
      "inequality_type": [
        "racial",
        "age",
        "health"
      ],
      "other_detail": "Focus on demographic bias in data collection",
      "affected_populations": [
        "elderly",
        "minority groups"
      ],
      "methodology": [
        "Data Linking",
        "Bias Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using administrative data to assess bias in mobility data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.06738v1",
    "title": "Metric-Free Individual Fairness with Cooperative Contextual Bandits",
    "year": 2020,
    "authors": [
      "Qian Hu",
      "Huzefa Rangwala"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias mitigation affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias reduction",
      "affected_populations": [
        "social groups",
        "individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness optimization in AI algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.06445v2",
    "title": "How to Measure Gender Bias in Machine Translation: Optimal Translators, Multiple Reference Points",
    "year": 2020,
    "authors": [
      "Anna Farkas",
      "Renáta Németh"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, highlighting disparities affecting women, a key aspect of social gender inequality. It analyzes how AI systems reflect and potentially reinforce societal biases. The focus on gender bias in language translation directly relates to social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Case Study",
        "Quantitative Analysis",
        "Survey",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and comparison to non-biased translator",
      "geographic_focus": [
        "Hungary",
        "English-speaking countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.06422v1",
    "title": "Pursuing Open-Source Development of Predictive Algorithms: The Case of Criminal Sentencing Algorithms",
    "year": 2020,
    "authors": [
      "Philip D. Waggoner",
      "Alec Macmillen"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias and societal impacts of sentencing algorithms, highlighting issues of fairness and discrimination affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "social",
        "fairness"
      ],
      "other_detail": "Focus on racial bias in criminal justice algorithms",
      "affected_populations": [
        "racial minorities",
        "criminal defendants"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Replicating algorithms and fitting penalized regressions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.06100v2",
    "title": "Exploring Gender Disparities in Time to Diagnosis",
    "year": 2020,
    "authors": [
      "Tony Y. Sun",
      "Oliver J. Bear Don't Walk IV",
      "Jennifer L. Chen",
      "Harry Reyes Nieva",
      "Noémie Elhadad"
    ],
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in healthcare, focusing on diagnosis timing and diagnostic fairness, addressing gender-based social inequalities.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "None",
      "affected_populations": [
        "men",
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes diagnosis times and classifier fairness across genders",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.05911v1",
    "title": "Situated Data, Situated Systems: A Methodology to Engage with Power Relations in Natural Language Processing Research",
    "year": 2020,
    "authors": [
      "Lucy Havens",
      "Melissa Terras",
      "Benjamin Bach",
      "Beatrice Alex"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias in NLP systems, which relates to social bias and power dynamics affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on bias and power relations in NLP research",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Literature Review",
        "Case Study",
        "System Design"
      ],
      "methodology_detail": "Interdisciplinary approach integrating critical reflection and technical methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.09865v3",
    "title": "An experiment on the mechanisms of racial bias in ML-based credit scoring in Brazil",
    "year": 2020,
    "authors": [
      "Ramon Vilarino",
      "Renato Vicente"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in credit scoring models in Brazil, highlighting racial inequality and algorithmic fairness issues affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on racial bias without protected attribute access",
      "affected_populations": [
        "Black Brazilians",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Game-theoretic explainability",
        "Counterfactual analysis",
        "Data analysis"
      ],
      "methodology_detail": "Analyzes model mechanisms and regional data influences",
      "geographic_focus": [
        "Brazil"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.04219v2",
    "title": "Mitigating Bias in Set Selection with Noisy Protected Attributes",
    "year": 2020,
    "authors": [
      "Anay Mehrotra",
      "L. Elisa Celis"
    ],
    "categories": [
      "cs.CY",
      "cs.DS",
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI subset selection, focusing on protected attributes like gender and race, which are directly related to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Fairness under noisy protected attribute data",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Approximation algorithms for fairness under noise",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.04049v1",
    "title": "FairLens: Auditing Black-box Clinical Decision Support Systems",
    "year": 2020,
    "authors": [
      "Cecilia Panigutti",
      "Alan Perotti",
      "Andrè Panisson",
      "Paolo Bajardi",
      "Dino Pedreschi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "68U99, 68T99",
      "J.3; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting biases in AI systems used in healthcare, addressing fairness and group-specific biases related to attributes like ethnicity, gender, and age, which are social inequality factors.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias detection in healthcare AI systems",
      "affected_populations": [
        "patients by ethnicity",
        "patients by gender",
        "elderly patients"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Explainable AI",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and explanation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.03654v4",
    "title": "Fair Machine Learning Under Partial Compliance",
    "year": 2020,
    "authors": [
      "Jessica Dai",
      "Sina Fazelpour",
      "Zachary C. Lipton"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in decision-making, which relates to social bias and discrimination issues. It examines how partial compliance affects fairness outcomes, implying social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in employment markets",
      "affected_populations": [
        "workers",
        "employers"
      ],
      "methodology": [
        "Simulation",
        "Theoretical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Uses simulation to explore fairness dynamics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.02395v2",
    "title": "Fairness in Biometrics: a figure of merit to assess biometric verification systems",
    "year": 2020,
    "authors": [
      "Tiago de Freitas Pereira",
      "Sébastien Marcel"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in biometric systems, focusing on gender and race demographics, highlighting social bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Fairness assessment in biometric verification systems",
      "affected_populations": [
        "minorities",
        "women",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates biometric systems using fairness metrics and datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.01897v2",
    "title": "A Multi-aspect Analysis of Gender Bias on Online Student Evaluations",
    "year": 2020,
    "authors": [
      "Sofia Maria Nikolakaki",
      "Joseph Lai",
      "Evimaria Terzi"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in online evaluations of professors, addressing gender-related social discrimination and perceptions in higher education.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender bias in academic evaluations",
      "affected_populations": [
        "female professors",
        "male professors"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of two decades of evaluation data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.00603v1",
    "title": "Making ML models fairer through explanations: the case of LimeOut",
    "year": 2020,
    "authors": [
      "Guilherme Alves",
      "Vaishnavi Bhargava",
      "Miguel Couceiro",
      "Amedeo Napoli"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in ML models related to sensitive features, impacting social groups. It discusses bias mitigation, which relates to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on fairness and bias in algorithmic decisions",
      "affected_populations": [
        "social groups",
        "individuals impacted by bias"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using feature dropout and ensemble methods for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.00244v2",
    "title": "Evaluating Bias In Dutch Word Embeddings",
    "year": 2020,
    "authors": [
      "Rodrigo Alejandro Chávez Mulsa",
      "Gerasimos Spanakis"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in Dutch word embeddings, addressing social bias and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation techniques applied to embeddings",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.02282v2",
    "title": "\"What We Can't Measure, We Can't Understand\": Challenges to Demographic Data Procurement in the Pursuit of Fairness",
    "year": 2020,
    "authors": [
      "McKane Andrus",
      "Elena Spitzer",
      "Jeffrey Brown",
      "Alice Xiang"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses demographic data collection related to bias, fairness, and social categories, addressing issues of social discrimination and inequality in algorithmic systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "disability"
      ],
      "other_detail": "Focuses on fairness and bias in AI systems",
      "affected_populations": [
        "minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews"
      ],
      "methodology_detail": "Semi-structured interviews with practitioners",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2011.00092v1",
    "title": "Analyzing Gender Bias within Narrative Tropes",
    "year": 2020,
    "authors": [
      "Dhruvil Gala",
      "Mohammad Omar Khursheed",
      "Hannah Lerner",
      "Brendan O'Connor",
      "Mohit Iyyer"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in media tropes, addressing gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Scoring and analyzing trope genderedness",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.16409v1",
    "title": "Inherent Trade-offs in the Fair Allocation of Treatments",
    "year": 2020,
    "authors": [
      "Yuzi He",
      "Keith Burghardt",
      "Siyi Guo",
      "Kristina Lerman"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in treatment allocation affecting protected groups, specifically minorities and students, indicating a focus on social disparities.",
      "inequality_type": [
        "educational",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic treatment policies",
      "affected_populations": [
        "minority groups",
        "students"
      ],
      "methodology": [
        "Causal Framework",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Learning optimal policies under fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.15300v1",
    "title": "Uncovering Latent Biases in Text: Method and Application to Peer Review",
    "year": 2020,
    "authors": [
      "Emaad Manzoor",
      "Nihar B. Shah"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in text related to subgroup visibility, specifically in peer review, which relates to social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Bias detection in social and institutional contexts",
      "affected_populations": [
        "reviewers",
        "candidates"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias quantification and causal inference framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.15052v3",
    "title": "Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases",
    "year": 2020,
    "authors": [
      "Ryan Steed",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race, gender, weight, disabilities, and ethnicity in AI models, which are social categories linked to inequality and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "ethnic"
      ],
      "other_detail": "Biases in AI reflecting societal stereotypes",
      "affected_populations": [
        "racial groups",
        "women",
        "disabled individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Quantifying social biases in image representations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.15120v3",
    "title": "Gender Bias in Depression Detection Using Audio Features",
    "year": 2020,
    "authors": [
      "Andrew Bailey",
      "Mark D. Plumbley"
    ],
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in depression detection, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender bias in mental health detection",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Fair Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in audio-based depression detection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.14534v1",
    "title": "Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias",
    "year": 2020,
    "authors": [
      "Marion Bartl",
      "Malvina Nissim",
      "Albert Gatt"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, addressing social gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender stereotypes in NLP models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Measuring and mitigating gender bias in BERT",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.14465v4",
    "title": "Evaluating Gender Bias in Speech Translation",
    "year": 2020,
    "authors": [
      "Marta R. Costa-jussà",
      "Christine Basta",
      "Gerard I. Gállego"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in speech translation, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Evaluating bias using challenge sets and speech translation systems",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.13949v2",
    "title": "KFC: A Scalable Approximation Algorithm for $k$-center Fair Clustering",
    "year": 2020,
    "authors": [
      "Elfarouk Harb",
      "Ho Shan Lam"
    ],
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints in clustering related to protected social groups, aiming to prevent over- or under-representation, which directly relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in clustering for social groups",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Approximation algorithms for fair clustering",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.13816v1",
    "title": "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction",
    "year": 2020,
    "authors": [
      "Xinyao Ma",
      "Maarten Sap",
      "Hannah Rashkin",
      "Yejin Choi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in character portrayal, a form of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias correction in media and text",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Unsupervised Learning"
      ],
      "methodology_detail": "Using connotation frames and unsupervised approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.13494v2",
    "title": "One-vs.-One Mitigation of Intersectional Bias: A General Method to Extend Fairness-Aware Binary Classification",
    "year": 2020,
    "authors": [
      "Kenji Kobayashi",
      "Yuri Nakao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "I.6.5; I.2.6"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses intersectional bias in fairness-aware AI, related to social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on multiple sensitive attributes in classification",
      "affected_populations": [
        "subgroups of protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias mitigation in binary classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.13168v1",
    "title": "Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings",
    "year": 2020,
    "authors": [
      "Vaibhav Kumar",
      "Tenzin Singhay Bhotia",
      "Vaibhav Kumar"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in word embeddings, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing bias metrics and mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.12089v1",
    "title": "The Pursuit of Algorithmic Fairness: On \"Correcting\" Algorithmic Unfairness in a Child Welfare Reunification Success Classifier",
    "year": 2020,
    "authors": [
      "Jordan Purdy",
      "Brian Glass"
    ],
    "categories": [
      "cs.LG",
      "stat.CO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic fairness in child welfare, addressing social bias and inequality impacts on vulnerable groups.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on fairness in child welfare decision-making",
      "affected_populations": [
        "children",
        "families",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses novel penalized optimizer and subsampling techniques",
      "geographic_focus": [
        "Oregon"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.15581v1",
    "title": "The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research",
    "year": 2020,
    "authors": [
      "Nur Ahmed",
      "Muntasir Wahed"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in access to computing power among firms and universities, highlighting how this affects participation and knowledge production in AI, which relates to socioeconomic inequality and fairness issues.",
      "inequality_type": [
        "economic",
        "educational",
        "informational"
      ],
      "other_detail": "Access to computing resources and research opportunities",
      "affected_populations": [
        "non-elite universities",
        "mid-tier universities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation",
        "Generalized Synthetic Control Method",
        "Machine Learning"
      ],
      "methodology_detail": "Analyzes publication data and compute access disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.11666v1",
    "title": "Reducing Unintended Identity Bias in Russian Hate Speech Detection",
    "year": 2020,
    "authors": [
      "Nadezhda Zueva",
      "Madina Kabirova",
      "Pavel Kalaidin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in hate speech detection, related to social identities, impacting social discrimination and fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias reduction in hate speech detection models",
      "affected_populations": [
        "minority groups",
        "targeted communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Using language models and word dropout techniques",
      "geographic_focus": [
        "Russia"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.11300v1",
    "title": "How Do Fair Decisions Fare in Long-term Qualification?",
    "year": 2020,
    "authors": [
      "Xueru Zhang",
      "Ruibo Tu",
      "Yang Liu",
      "Mingyan Liu",
      "Hedvig Kjellström",
      "Kun Zhang",
      "Cheng Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines the long-term impacts of fairness constraints on population well-being, addressing social disparities related to algorithmic decision-making. It analyzes how fairness interventions affect group equality, implying relevance to social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "racial"
      ],
      "other_detail": "Focuses on group disparities and fairness in decision systems",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes equilibrium dynamics and tests on real datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.10992v1",
    "title": "The Effect of the Rooney Rule on Implicit Bias in the Long Term",
    "year": 2020,
    "authors": [
      "L. Elisa Celis",
      "Chris Hays",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.CO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines implicit bias related to underrepresented groups, addressing racial and social inequality in hiring practices.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial bias reduction in employment decisions",
      "affected_populations": [
        "minority candidates"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Modeling bias dynamics and empirical MTurk study",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.09768v1",
    "title": "Not Judging a User by Their Cover: Understanding Harm in Multi-Modal Processing within Social Media Research",
    "year": 2020,
    "authors": [
      "Jiachen Jiang",
      "Soroush Vosoughi"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and harms related to demographic prediction and toxicity detection across social groups, indicating a focus on social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "informational"
      ],
      "other_detail": "Focus on demographic biases in social media AI tools",
      "affected_populations": [
        "Twitter users",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes performance disparities and biases in AI systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.08912v1",
    "title": "The Leaky Pipeline in Physics Publishing",
    "year": 2020,
    "authors": [
      "Clara O Ross",
      "Aditya Gupta",
      "Ninareh Mehrabi",
      "Goran Muric",
      "Kristina Lerman"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in physics publishing, highlighting bias affecting women's representation in prestigious journals, which relates to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women physicists"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bibliographic data and infers gender from names",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.08850v2",
    "title": "Against Scale: Provocations and Resistances to Scale Thinking",
    "year": 2020,
    "authors": [
      "Alex Hanna",
      "Tina M. Park"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses social structures and systemic change, emphasizing resistance to social inequality through alternative collaborative systems.",
      "inequality_type": [
        "social",
        "inequality"
      ],
      "other_detail": "Focuses on systemic social inequality and resistance strategies",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes recent work and proposes evaluative questions",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.08146v1",
    "title": "Online Decision Trees with Fairness",
    "year": 2020,
    "authors": [
      "Wenbin Zhang",
      "Liang Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI decision-making, focusing on discrimination related to sensitive attributes like gender and ethnicity.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Fairness in AI decision systems",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Experiment"
      ],
      "methodology_detail": "Online decision tree algorithms for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.07343v3",
    "title": "Causal Multi-Level Fairness",
    "year": 2020,
    "authors": [
      "Vishwali Mhasawade",
      "Rumi Chunara"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in algorithmic systems affecting marginalized groups, considering macro-level social factors like discrimination and neighborhood effects.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Multi-level causal fairness in algorithms",
      "affected_populations": [
        "marginalized groups",
        "individuals affected by bias"
      ],
      "methodology": [
        "Causal Inference",
        "Algorithmic Fairness"
      ],
      "methodology_detail": "Formalizes multi-level fairness using causal tools",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.06820v1",
    "title": "Equitable Allocation of Healthcare Resources with Fair Cox Models",
    "year": 2020,
    "authors": [
      "Kamrun Naher Keya",
      "Rashidul Islam",
      "Shimei Pan",
      "Ian Stockwell",
      "James R. Foulds"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare resource allocation, focusing on equitable treatment across demographic groups, which relates to social inequalities such as health disparities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "racial",
        "disability"
      ],
      "other_detail": "Fairness in survival models for healthcare prioritization",
      "affected_populations": [
        "vulnerable populations",
        "patients in need"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Fair Cox models with fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.06529v5",
    "title": "On the Fairness of Causal Algorithmic Recourse",
    "year": 2020,
    "authors": [
      "Julius von Kügelgen",
      "Amir-Hossein Karimi",
      "Umang Bhatt",
      "Isabel Valera",
      "Adrian Weller",
      "Bernhard Schölkopf"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in algorithmic recourse, which impacts social groups and inequalities. It discusses how AI systems can produce biased outcomes affecting marginalized populations. The focus on fairness criteria relates to social discrimination issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focus on fairness in decision-making processes",
      "affected_populations": [
        "individuals facing unfavorable classifications"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Study",
        "Case Study"
      ],
      "methodology_detail": "Analyzes fairness criteria and tests on dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.06203v2",
    "title": "Mitigating Gender Bias in Machine Translation with Target Gender Annotations",
    "year": 2020,
    "authors": [
      "Artūrs Stafanovičs",
      "Toms Bergmanis",
      "Mārcis Pinnis"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social discrimination issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Addressing gender stereotypes in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Using annotated data to reduce stereotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.06113v1",
    "title": "FaiR-N: Fair and Robust Neural Networks for Structured Data",
    "year": 2020,
    "authors": [
      "Shubham Sharma",
      "Alan H. Gee",
      "David Paydarfar",
      "Joydeep Ghosh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on recourse disparities across groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and recourse in decision-making",
      "affected_populations": [
        "protected attribute groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Training neural networks with fairness-aware loss functions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.06018v1",
    "title": "Gender Coreference and Bias Evaluation at WMT 2020",
    "year": 2020,
    "authors": [
      "Tom Kocmi",
      "Tomasz Limisiewicz",
      "Gabriel Stanovsky"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, highlighting gender disparities and bias issues in AI systems, which relate directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Using WinoMT test suite for bias evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.05332v2",
    "title": "Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It",
    "year": 2020,
    "authors": [
      "Danielle Saunders",
      "Rosie Sallis",
      "Bill Byrne"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in neural machine translation, a social issue related to gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language translation",
      "affected_populations": [
        "gender minorities",
        "non-binary individuals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "evaluates gender inflection control in translation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.05137v2",
    "title": "An Open Review of OpenReview: A Critical Analysis of the Machine Learning Conference Review Process",
    "year": 2020,
    "authors": [
      "David Tran",
      "Alex Valtchanov",
      "Keshav Ganapathy",
      "Raymond Feng",
      "Eric Slud",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in conference review processes, highlighting gender and institutional disparities, which relate to social inequality issues.",
      "inequality_type": [
        "gender",
        "institutional",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias in academic review and acceptance processes",
      "affected_populations": [
        "female authors",
        "institutional authors"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes review scores and acceptance data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.04396v5",
    "title": "Dropping Standardized Testing for Admissions Trades Off Information and Access",
    "year": 2020,
    "authors": [
      "Nikhil Garg",
      "Hannah Li",
      "Faidra Monachou"
    ],
    "categories": [
      "cs.CY",
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how standardized testing impacts access and information for diverse social groups, highlighting disparities in educational opportunities and fairness concerns.",
      "inequality_type": [
        "educational",
        "informational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on access barriers and informational disparities",
      "affected_populations": [
        "non-traditional backgrounds",
        "applicants from disadvantaged groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes informational trade-offs and applicant pool effects",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.02542v5",
    "title": "Astraea: Grammar-based Fairness Testing",
    "year": 2020,
    "authors": [
      "Ezekiel Soremekun",
      "Sakshi Udeshi",
      "Sudipta Chattopadhyay"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness violations in AI systems, addressing social bias related to gender and race, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on fairness testing in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Using grammar-based testing to reveal biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.07023v1",
    "title": "Understanding bias in facial recognition technologies",
    "year": 2020,
    "authors": [
      "David Leslie"
    ],
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how bias in facial recognition systems perpetuates discrimination and injustices related to race and systemic marginalization.",
      "inequality_type": [
        "racial",
        "systemic",
        "discrimination"
      ],
      "other_detail": "Focuses on bias and discrimination in AI systems",
      "affected_populations": [
        "racial minorities",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzes historical patterns and ethical implications",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.01101v2",
    "title": "Commuting Network Spillovers and COVID-19 Deaths Across US Counties",
    "year": 2020,
    "authors": [
      "Christopher Seto",
      "Aria Khademi",
      "Corina Graif",
      "Vasant G. Honavar"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "cs.LG",
      "q-bio.PE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and ethnic disparities in COVID-19 outcomes and discusses inequality in health and community interconnectedness.",
      "inequality_type": [
        "racial",
        "ethnic",
        "health"
      ],
      "other_detail": "Focus on health disparities linked to social factors",
      "affected_populations": [
        "racial groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Mixed effects regression and causal estimation",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.01079v6",
    "title": "On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach",
    "year": 2020,
    "authors": [
      "Junpei Komiyama",
      "Shunya Noda"
    ],
    "categories": [
      "econ.TH",
      "cs.GT",
      "econ.EM",
      "stat.ML"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines statistical discrimination in hiring, highlighting racial and social biases. It discusses how data limitations perpetuate underestimation of minority workers, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on employment discrimination and social learning failures",
      "affected_populations": [
        "minority workers"
      ],
      "methodology": [
        "Multi-Armed Bandit Model",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling social learning and discrimination dynamics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.00133v1",
    "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    "year": 2020,
    "authors": [
      "Nikita Nangia",
      "Clara Vania",
      "Rasika Bhalerao",
      "Samuel R. Bowman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases related to race, stereotypes, and protected groups in AI models, which are linked to social inequalities.",
      "inequality_type": [
        "racial",
        "social",
        "educational"
      ],
      "other_detail": "Focuses on stereotypes affecting disadvantaged groups",
      "affected_populations": [
        "racial minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Benchmarking biases in language models",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.13825v2",
    "title": "Information theoretic network approach to socioeconomic correlations",
    "year": 2020,
    "authors": [
      "Alec Kirkley"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes socioeconomic spatial patterns, inequality, and disparities across regions, which are core aspects of social inequality research.",
      "inequality_type": [
        "socioeconomic",
        "income",
        "geographic"
      ],
      "other_detail": "Focuses on regional socioeconomic disparities and inequality patterns",
      "affected_populations": [
        "regional populations",
        "local communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Network Analysis"
      ],
      "methodology_detail": "Uses information theoretic network approach and divergence measures",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.13650v1",
    "title": "Towards a Measure of Individual Fairness for Deep Learning",
    "year": 2020,
    "authors": [
      "Krystal Maughan",
      "Joseph P. Near"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI predictions related to protected attributes, which are linked to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability"
      ],
      "other_detail": "Focuses on individual fairness in AI predictions",
      "affected_populations": [
        "minority groups",
        "protected communities"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using automatic differentiation to measure bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.13028v2",
    "title": "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning",
    "year": 2020,
    "authors": [
      "Haochen Liu",
      "Wentao Wang",
      "Yiqi Wang",
      "Hui Liu",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in dialogue systems, reflecting social gender inequality and bias in AI.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "users of dialogue systems"
      ],
      "methodology": [
        "Adversarial Learning",
        "Experiment"
      ],
      "methodology_detail": "Debiasing framework for dialogue models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.12675v3",
    "title": "A Primal-Dual Subgradient Approachfor Fair Meta Learning",
    "year": 2020,
    "authors": [
      "Chen Zhao",
      "Feng Chen",
      "Zhuoyi Wang",
      "Latifur Khan"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing bias and equitable outcomes, which are central to social inequality issues.",
      "inequality_type": [
        "social",
        "fairness"
      ],
      "other_detail": "Addresses bias control in AI systems",
      "affected_populations": [
        "unseen classes",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Optimization",
        "Fairness Constraints"
      ],
      "methodology_detail": "Joint primal-dual parameter optimization for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.12562v1",
    "title": "Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach",
    "year": 2020,
    "authors": [
      "Cuong Tran",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and non-discrimination in AI models, focusing on sensitive attributes like gender and ethnicity, which are directly related to social inequalities.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Fairness constraints in privacy-preserving neural networks",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fairness and privacy trade-offs in neural network design",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.12040v1",
    "title": "Fairness in Semi-supervised Learning: Unlabeled Data Help to Reduce Discrimination",
    "year": 2020,
    "authors": [
      "Tao Zhang",
      "Tianqing Zhu",
      "Jing Li",
      "Mengde Han",
      "Wanlei Zhou",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and discrimination in AI, related to social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on algorithmic fairness and discrimination reduction",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Fairness framework and bias analysis in semi-supervised learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2010.06986v2",
    "title": "On the Problem of Underranking in Group-Fair Ranking",
    "year": 2020,
    "authors": [
      "Sruthi Gorantla",
      "Amit Deshpande",
      "Anand Louis"
    ],
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in ranking systems that can worsen social inequalities and stereotypes, particularly affecting marginalized groups. It discusses fairness in algorithmic ranking, which relates directly to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness and bias in algorithmic ranking systems",
      "affected_populations": [
        "minority groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Experimental"
      ],
      "methodology_detail": "Designs fair ranking algorithms with theoretical guarantees",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.11491v1",
    "title": "Understanding Fairness of Gender Classification Algorithms Across Gender-Race Groups",
    "year": 2020,
    "authors": [
      "Anoop Krishnan",
      "Ali Almadan",
      "Ajita Rattani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and performance disparities of gender classification algorithms across gender and race groups, highlighting social biases and unequal impacts on marginalized populations.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on algorithmic fairness across social groups",
      "affected_populations": [
        "women",
        "Black people",
        "Latino",
        "Middle Eastern"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes performance differences using facial datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.11406v1",
    "title": "Unfairness Discovery and Prevention For Few-Shot Regression",
    "year": 2020,
    "authors": [
      "Chen Zhao",
      "Feng Chen"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and discrimination related to protected groups such as race and gender in AI models, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in AI predictions for minority groups",
      "affected_populations": [
        "minority groups",
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Analysis",
        "Fairness Algorithm"
      ],
      "methodology_detail": "Causal Bayesian knowledge graph and bias mitigation algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.10808v1",
    "title": "Using Machine Learning to Develop a Novel COVID-19 Vulnerability Index (C19VI)",
    "year": 2020,
    "authors": [
      "Anuj Tiwari",
      "Arya V. Dadhania",
      "Vijay Avin Balaji Ragunathrao",
      "Edson R. A. Oliveira"
    ],
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper develops a vulnerability index highlighting racial and economic disparities in COVID-19 outcomes, directly addressing social inequalities.",
      "inequality_type": [
        "racial",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial and economic disparities in health outcomes",
      "affected_populations": [
        "racial minorities",
        "economically poor communities"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "RF model and homogeneity assessment techniques",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.10576v3",
    "title": "Ethical Machine Learning in Health Care",
    "year": 2020,
    "authors": [
      "Irene Y. Chen",
      "Emma Pierson",
      "Sherri Rose",
      "Shalmali Joshi",
      "Kadija Ferryman",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses ethical considerations in ML for health care, emphasizing social justice and addressing health inequities. It highlights concerns about amplifying existing disparities and aims to promote equitable ML practices. The focus on social justice framing indicates engagement with social inequality issues.",
      "inequality_type": [
        "health",
        "social justice"
      ],
      "other_detail": "Focus on health inequities and social justice",
      "affected_populations": [
        "health disparities",
        "social groups"
      ],
      "methodology": [
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing ethical considerations and social justice frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.10050v2",
    "title": "Measuring justice in machine learning",
    "year": 2020,
    "authors": [
      "Alan Lundgard"
    ],
    "categories": [
      "cs.CY",
      "I.2.0; K.4.1; J.1.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses justice in machine learning, referencing biases and fairness, which relate to social inequalities such as disability and societal benefit distribution.",
      "inequality_type": [
        "disability",
        "social",
        "fairness"
      ],
      "other_detail": "Focuses on justice measures and biases in AI systems",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes philosophical theories and fair ML approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.08270v4",
    "title": "Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals",
    "year": 2020,
    "authors": [
      "Saloni Dash",
      "Vineeth N Balasubramanian",
      "Amit Sharma"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates bias and fairness in image classifiers, highlighting social biases related to skin and hair color, which are linked to racial and aesthetic inequalities.",
      "inequality_type": [
        "racial",
        "appearance"
      ],
      "other_detail": "Bias related to skin and hair color",
      "affected_populations": [
        "racial groups",
        "individuals with different skin/hair tones"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Counterfactual generation and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.07838v2",
    "title": "FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition",
    "year": 2020,
    "authors": [
      "Tomáš Sixta",
      "Julio C. S. Jacques Junior",
      "Pau Buch-Cardona",
      "Neil M. Robertson",
      "Eduard Vazquez",
      "Sergio Escalera"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes bias in face recognition related to gender and skin color, highlighting disparities affecting different social groups. It discusses fairness issues and unequal impacts of AI systems across demographic groups. The focus on bias metrics and social attributes indicates addressing social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in face recognition algorithms",
      "affected_populations": [
        "females",
        "dark-skinned individuals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Analysis"
      ],
      "methodology_detail": "Evaluation of bias and accuracy across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.06251v1",
    "title": "Active Fairness Instead of Unawareness",
    "year": 2020,
    "authors": [
      "Boris Ruf",
      "Marcin Detyniecki"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and discrimination in AI, which relate to social inequalities such as bias and discrimination. It addresses how AI systems can reproduce or mitigate social biases. The focus on fairness and bias inherently involves social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing bias and fairness in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.07025v1",
    "title": "FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment",
    "year": 2020,
    "authors": [
      "Alejandro Peña",
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and discrimination in AI recruitment systems, highlighting unfair impacts on social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focus on bias and fairness in automated decision-making",
      "affected_populations": [
        "job applicants",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Testing bias mitigation algorithms in recruitment context",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.05283v6",
    "title": "Fair and accurate age prediction using distribution aware data curation and augmentation",
    "year": 2020,
    "authors": [
      "Yushi Cao",
      "David Berend",
      "Palina Tolmach",
      "Guy Amit",
      "Moshe Levy",
      "Yang Liu",
      "Asaf Shabtai",
      "Yuval Elovici"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems related to age, ethnicity, and gender, highlighting social bias issues. It aims to improve equitable age prediction across diverse social groups. The focus on fairness and bias in facial recognition directly relates to social inequality concerns.",
      "inequality_type": [
        "age",
        "ethnic",
        "gender"
      ],
      "other_detail": "Fairness in AI-based age prediction",
      "affected_populations": [
        "ethnic groups",
        "age groups",
        "gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Data curation and augmentation for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.05021v1",
    "title": "Investigating Gender Bias in BERT",
    "year": 2020,
    "authors": [
      "Rishabh Bhardwaj",
      "Navonil Majumder",
      "Soujanya Poria"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in language models, addressing gender discrimination issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias reduction in BERT embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.09936v1",
    "title": "Prune Responsibly",
    "year": 2020,
    "authors": [
      "Michela Paganini"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how model pruning impacts fairness and performance across different groups, highlighting biases affecting underrepresented or outlier populations.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "disability"
      ],
      "other_detail": "Focus on AI fairness and bias in model performance",
      "affected_populations": [
        "underrepresented groups",
        "outliers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes performance disparities across categories",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.04661v1",
    "title": "A Framework for Fairer Machine Learning in Organizations",
    "year": 2020,
    "authors": [
      "Lily Morse",
      "Mike H. M. Teodorescu",
      "Yazeed Awwad",
      "Gerald Kane"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "68T05",
      "I.2.6"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in machine learning affecting socio-economic outcomes, addressing social bias and discrimination issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness criteria in organizational AI applications",
      "affected_populations": [
        "job applicants",
        "housing seekers",
        "loan applicants"
      ],
      "methodology": [
        "Literature Review",
        "Framework Development"
      ],
      "methodology_detail": "Develops a fairness implementation framework for organizations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.04640v2",
    "title": "On the Fairness of 'Fake' Data in Legal AI",
    "year": 2020,
    "authors": [
      "Lauren Boswell",
      "Arjun Prakash"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in legal AI affecting fairness, implicating social discrimination issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "legal"
      ],
      "other_detail": "Focuses on fairness in legal decision-making processes",
      "affected_populations": [
        "defendants",
        "litigants",
        "legal defendants"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Critiques pre-processing and classifier modification methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.01715v2",
    "title": "Exploring Artist Gender Bias in Music Recommendation",
    "year": 2020,
    "authors": [
      "Dougal Shakespeare",
      "Lorenzo Porcaro",
      "Emilia Gómez",
      "Carlos Castillo"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in music recommendation systems, addressing social discrimination and fairness issues related to gender. It analyzes how algorithms may reinforce or mitigate gender disparities among artists, impacting social equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "female artists",
        "music consumers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias disparity measurement on listening datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.01454v5",
    "title": "Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information",
    "year": 2020,
    "authors": [
      "Enyan Dai",
      "Suhang Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues related to protected attributes like race and gender in AI systems, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on algorithmic fairness in graph neural networks",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Algorithm Development"
      ],
      "methodology_detail": "Develops fair GNNs with limited sensitive attribute info",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.01334v1",
    "title": "Gender Stereotype Reinforcement: Measuring the Gender Bias Conveyed by Ranking Algorithms",
    "year": 2020,
    "authors": [
      "Alessandro Fabris",
      "Alberto Purpura",
      "Gianmaria Silvello",
      "Gian Antonio Susto"
    ],
    "categories": [
      "cs.CY",
      "H.3.3"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender stereotypes and biases in ranking algorithms, addressing gender-based social bias and discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in AI and search engines",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Measuring bias reinforcement in ranking algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.08955v1",
    "title": "Neural Fair Collaborative Filtering",
    "year": 2020,
    "authors": [
      "Rashidul Islam",
      "Kamrun Naher Keya",
      "Ziqian Zeng",
      "Shimei Pan",
      "James Foulds"
    ],
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI recommender systems, focusing on fairness and discrimination issues affecting gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in social media-based recommendations",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Fairness Techniques"
      ],
      "methodology_detail": "Bias correction in neural collaborative filtering",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2009.00146v2",
    "title": "Nash Social Distancing Games with Equity Constraints: How Inequality Aversion Affects the Spread of Epidemics",
    "year": 2020,
    "authors": [
      "Ioannis Kordonis",
      "Athanasios-Rafail Lagos",
      "George P. Papavassilopoulos"
    ],
    "categories": [
      "cs.GT",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models social distancing with inequality constraints, focusing on payoffs related to vulnerable and non-vulnerable groups, and analyzes how inequality aversion impacts epidemic spread.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on payoff variance and social inequality constraints",
      "affected_populations": [
        "vulnerable",
        "non-vulnerable"
      ],
      "methodology": [
        "Game Theory",
        "Mathematical Modeling",
        "Numerical Analysis"
      ],
      "methodology_detail": "Analyzes Nash equilibria with inequality constraints",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.13122v1",
    "title": "Adversarial Learning for Counterfactual Fairness",
    "year": 2020,
    "authors": [
      "Vincent Grari",
      "Sylvain Lamprier",
      "Marcin Detyniecki"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, specifically counterfactual fairness related to sensitive attributes like gender and race, addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Fairness in individual-level predictions",
      "affected_populations": [
        "individuals with sensitive attributes"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Adversarial neural learning approach for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.10880v1",
    "title": "Improving Fair Predictions Using Variational Inference In Causal Models",
    "year": 2020,
    "authors": [
      "Rik Helwegen",
      "Christos Louizos",
      "Patrick Forré"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in AI models, addressing social bias and discrimination issues. It aims to improve equitable predictions, which relate directly to social inequalities. The use of causal models to audit and ensure fairness indicates a concern with social disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses fairness in predictive models for social justice",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Causal Modeling",
        "Variational Inference",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Using causal inference to ensure fair AI predictions",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.09656v1",
    "title": "Auditing Digital Platforms for Discrimination in Economic Opportunity Advertising",
    "year": 2020,
    "authors": [
      "Sara Kingsley",
      "Clara Wang",
      "Alex Mikhalenko",
      "Proteeti Sinha",
      "Chinmay Kulkarni"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines digital platform biases affecting socioeconomic opportunities across demographic groups, highlighting social discrimination and inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "gender",
        "age"
      ],
      "other_detail": "Focus on digital platform discrimination and access",
      "affected_populations": [
        "demographic groups",
        "users by age and gender"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed ad distribution and content bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.06755v1",
    "title": "Tackling COVID-19 through Responsible AI Innovation: Five Steps in the Right Direction",
    "year": 2020,
    "authors": [
      "David Leslie"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.OT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses disparities in COVID-19 impacts, biased AI outcomes, and societal inequities, emphasizing responsible AI practices to address these issues.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "racial",
        "discrimination"
      ],
      "other_detail": "Focus on equitable and just AI/ML practices",
      "affected_populations": [
        "vulnerable social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review",
        "System Design"
      ],
      "methodology_detail": "Guidelines for responsible AI development and governance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.06460v2",
    "title": "Hate Speech Detection and Racial Bias Mitigation in Social Media based on BERT model",
    "year": 2020,
    "authors": [
      "Marzieh Mozafari",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ],
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in hate speech detection, highlighting social discrimination issues. It addresses biases affecting racial groups in AI systems. The focus on racial bias mitigation relates directly to social inequality concerns.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "Bias in AI systems affecting racial groups",
      "affected_populations": [
        "African-American English speakers",
        "White English speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Bias Mitigation"
      ],
      "methodology_detail": "Using transfer learning and re-weighting techniques",
      "geographic_focus": [
        "Twitter (US-based)"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.07433v1",
    "title": "LiFT: A Scalable Framework for Measuring Fairness in ML Applications",
    "year": 2020,
    "authors": [
      "Sriram Vasudevan",
      "Krishnaram Kenthapadi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems, focusing on societal biases and discrimination, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on algorithmic bias and discrimination mitigation",
      "affected_populations": [
        "disadvantaged groups",
        "minority communities"
      ],
      "methodology": [
        "System Design",
        "Fairness Measurement",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Design and deployment of fairness measurement system",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.05261v1",
    "title": "Vindication, Virtue and Vitriol: A study of online engagement and abuse toward British MPs during the COVID-19 Pandemic",
    "year": 2020,
    "authors": [
      "Tracie Farrell",
      "Genevieve Gorrell",
      "Kalina Bontcheva"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses topics like racism and inequality, highlighting social biases and discrimination in online abuse toward MPs, especially related to social issues addressed by politicians.",
      "inequality_type": [
        "racial",
        "inequality"
      ],
      "other_detail": "Focus on online abuse related to social issues",
      "affected_populations": [
        "British MPs",
        "social groups discussed"
      ],
      "methodology": [
        "Mixed Methods"
      ],
      "methodology_detail": "Combines quantitative and qualitative analysis",
      "geographic_focus": [
        "United Kingdom"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.07309v1",
    "title": "Bias and Discrimination in AI: a cross-disciplinary perspective",
    "year": 2020,
    "authors": [
      "Xavier Ferrer",
      "Tom van Nuenen",
      "Jose M. Such",
      "Mark Coté",
      "Natalia Criado"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "68T01"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias and discrimination in AI, which relate to social inequalities like race, gender, and social groups. It emphasizes social bias, fairness, and impacts across different social groups, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on social bias and discrimination in AI",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Interdisciplinary literature survey and ethical analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.03808v2",
    "title": "Diverse Group Formation Based on Multiple Demographic Features",
    "year": 2020,
    "authors": [
      "Mohammed Alqahtani",
      "Susan Gauch",
      "Omar Salman",
      "Mohammed Ibrahim",
      "Reem Al-Saffar"
    ],
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and diversity in team formation, addressing bias against minorities and demographic representation, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic fairness in algorithmic group formation",
      "affected_populations": [
        "minorities",
        "underrepresented groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Dataset Creation",
        "Evaluation"
      ],
      "methodology_detail": "Designing diversity algorithms and testing on real data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.02754v2",
    "title": "Discovering and Categorising Language Biases in Reddit",
    "year": 2020,
    "authors": [
      "Xavier Ferrer",
      "Tom van Nuenen",
      "Jose M. Such",
      "Natalia Criado"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "68T50, 68T09, 91D30"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates language biases related to gender, religion, and ethnicity on Reddit, which are social categories linked to inequality and discrimination.",
      "inequality_type": [
        "gender",
        "religious",
        "ethnic"
      ],
      "other_detail": "Focus on online community biases",
      "affected_populations": [
        "women",
        "religious groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Semantic Analysis",
        "Data-Driven Approach"
      ],
      "methodology_detail": "Using word embeddings to discover biases",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.02214v1",
    "title": "Machine Learning Fairness in Justice Systems: Base Rates, False Positives, and False Negatives",
    "year": 2020,
    "authors": [
      "Jesse Russell"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in justice systems, focusing on racial disparities in error rates, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "justice"
      ],
      "other_detail": "Focus on racial disparities in error rates",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Analyzing error tradeoffs in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.00371v1",
    "title": "Análisis jurídico de la discriminación algorítmica en los procesos de selección laboral",
    "year": 2020,
    "authors": [
      "Andrés Páez",
      "Natalia Ramírez-Bustamante"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial and gender biases in algorithmic job selection, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias detection and legal analysis of discrimination",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Literature Review",
        "Legal Analysis"
      ],
      "methodology_detail": "Analyzes legal frameworks and bias cases",
      "geographic_focus": [
        "United States",
        "Colombia"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.15863v1",
    "title": "Indian Political Twitter and Caste Discrimination -- How Representation Does Not Equal Inclusion in Lok Sabha Networks",
    "year": 2020,
    "authors": [
      "Palashi Vaghela",
      "Ramaravind Kommiya Mothilal",
      "Joyojeet Pal"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines caste-based social discrimination and exclusion within political networks, highlighting persistent social inequalities in India.",
      "inequality_type": [
        "caste",
        "social"
      ],
      "other_detail": "Focus on caste discrimination in political social networks",
      "affected_populations": [
        "lower caste",
        "upper caste"
      ],
      "methodology": [
        "Network Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes Twitter network centrality and connectivity",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.15270v2",
    "title": "Fairness-Aware Online Personalization",
    "year": 2020,
    "authors": [
      "G Roshan Lal",
      "Sahin Cem Geyik",
      "Krishnaram Kenthapadi"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in algorithmic decision-making, highlighting biases affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias mitigation in personalized ranking systems",
      "affected_populations": [
        "disadvantaged groups",
        "minority populations"
      ],
      "methodology": [
        "Machine Learning",
        "Regularization",
        "Simulation"
      ],
      "methodology_detail": "Bias quantification and mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.15182v2",
    "title": "Visual Analysis of Discrimination in Machine Learning",
    "year": 2020,
    "authors": [
      "Qianwen Wang",
      "Zhenhua Xu",
      "Zhutian Chen",
      "Yong Wang",
      "Shixia Liu",
      "Huamin Qu"
    ],
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic discrimination in critical social applications, focusing on fairness and bias analysis, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in automated decision-making systems",
      "affected_populations": [
        "students",
        "minority groups"
      ],
      "methodology": [
        "Visual Analytics",
        "Causal Modeling",
        "Classification Rules Mining"
      ],
      "methodology_detail": "Interactive visualization for discrimination analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.14775v2",
    "title": "Intersectional Affirmative Action Policies for Top-k Candidates Selection",
    "year": 2020,
    "authors": [
      "Giorgio Barnabo'",
      "Carlos Castillo",
      "Michael Mathioudakis",
      "Sergio Celis"
    ],
    "categories": [
      "cs.CY",
      "cs.DS"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in top-k candidate selection affecting disadvantaged intersectional groups, aiming to reduce social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on intersectional fairness in selection processes",
      "affected_populations": [
        "disadvantaged groups",
        "intersectional minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Design and evaluation of fairness algorithms",
      "geographic_focus": [
        "OECD country"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.07341v1",
    "title": "Data, Power and Bias in Artificial Intelligence",
    "year": 2020,
    "authors": [
      "Susan Leavy",
      "Barry O'Sullivan",
      "Eugenia Siapera"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses societal bias, discrimination, and fairness issues in AI, which relate directly to social inequalities such as race, gender, and social justice.",
      "inequality_type": [
        "racial",
        "gender",
        "social justice",
        "discrimination"
      ],
      "other_detail": "Focus on bias mitigation and data justice in AI systems",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Literature Review",
        "Technical Solutions",
        "Policy Analysis"
      ],
      "methodology_detail": "Explores interdisciplinary approaches to bias and fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of societal bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.01548v1",
    "title": "Defining and Evaluating Fair Natural Language Generation",
    "year": 2020,
    "authors": [
      "Catherine Yeo",
      "Alyssa Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender biases in language models, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in NLP",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Analyzes biases in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.13891v1",
    "title": "Work Practices and Perceptions from Women Core Developers in OSS Communities",
    "year": 2020,
    "authors": [
      "Edna Dias Canedo",
      "Rodrigo Bonifácio",
      "Márcio Vinícius Okimoto",
      "Alexander Serebrenik",
      "Gustavo Pinto",
      "Eduardo Monteiro"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender diversity, underrepresentation, and gender bias in open source communities, addressing social inequality related to gender discrimination and participation disparities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender segregation in open source communities",
      "affected_populations": [
        "women developers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Mining repositories and survey of women core developers",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.10306v3",
    "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction",
    "year": 2020,
    "authors": [
      "Stephen R. Pfohl",
      "Agata Foryciarz",
      "Nigam H. Shah"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in healthcare AI, addressing disparities across social groups.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on health disparities and algorithmic fairness",
      "affected_populations": [
        "patients",
        "healthcare groups"
      ],
      "methodology": [
        "Empirical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates fairness interventions across healthcare datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.10083v3",
    "title": "The Geometry of Information Cocoon: Analyzing the Cultural Space with Word Embedding Models",
    "year": 2020,
    "authors": [
      "Huimin Xu",
      "Zhicong Chen",
      "Ruiqi Li",
      "Cheng-Jun Wang"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how digital media use and information cocoons differ by social class, highlighting disparities in access to knowledge and content. It discusses the impact on vulnerable groups and social inequality. The focus on social class and knowledge acquisition indicates a direct link to social inequality issues.",
      "inequality_type": [
        "class",
        "educational",
        "informational"
      ],
      "other_detail": "Focus on social class and digital media use",
      "affected_populations": [
        "lower social class",
        "higher social class"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Word embedding models and large-scale datasets analysis",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.10075v3",
    "title": "Investigating Bias and Fairness in Facial Expression Recognition",
    "year": 2020,
    "authors": [
      "Tian Xu",
      "Jennifer White",
      "Sinan Kalkan",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates bias and fairness in facial expression recognition, addressing social bias issues related to demographic groups. It discusses fairness strategies and bias mitigation, which relate to social inequalities. The focus on demographic bias indicates relevance to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "demographic"
      ],
      "other_detail": "Bias mitigation in facial expression datasets",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Comparative analysis of approaches on datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2008.04068v1",
    "title": "Crowd, Lending, Machine, and Bias",
    "year": 2020,
    "authors": [
      "Runshan Fu",
      "Yan Huang",
      "Param Vir Singh"
    ],
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases related to gender and race in ML algorithms and their impact on fairness in lending, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in algorithmic decision-making processes",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection and mitigation in ML algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.09541v2",
    "title": "Same-Day Delivery with Fairness",
    "year": 2020,
    "authors": [
      "Xinwei Chen",
      "Tong Wang",
      "Barrett W. Thomas",
      "Marlin W. Ulmer"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in delivery services, highlighting spatial and temporal disparities affecting minority neighborhoods, which relates to social inequality and fairness issues.",
      "inequality_type": [
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on spatial and regional fairness in service delivery",
      "affected_populations": [
        "minority neighborhoods",
        "underserved regions"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Markov decision process",
        "Deep Q-learning",
        "Computational analysis"
      ],
      "methodology_detail": "Optimizing fairness in dynamic delivery routing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.09530v1",
    "title": "A Distributionally Robust Approach to Fair Classification",
    "year": 2020,
    "authors": [
      "Bahar Taskesen",
      "Viet Anh Nguyen",
      "Daniel Kuhn",
      "Jose Blanchet"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI classification, focusing on preventing discrimination based on sensitive attributes like gender and ethnicity, which are directly related to social inequalities.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social discrimination"
      ],
      "other_detail": "Focuses on algorithmic fairness and discrimination prevention",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Distributionally robust optimization and fairness measures",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.08666v1",
    "title": "Conservative AI and social inequality: Conceptualizing alternatives to bias through social theory",
    "year": 2020,
    "authors": [
      "Mike Zajko"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses societal bias, inequality reproduction, and social structures linked to bias, addressing social discrimination and unequal impacts across groups.",
      "inequality_type": [
        "class",
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Focus on systemic inequality and bias in AI",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Sociological perspective on AI bias and inequality",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.08406v1",
    "title": "The role of collider bias in understanding statistics on racially biased policing",
    "year": 2020,
    "authors": [
      "Norman Fenton",
      "Martin Neil",
      "Steven Frazier"
    ],
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in policing, a social inequality issue, using causal models to understand disparities.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in law enforcement data",
      "affected_populations": [
        "unarmed blacks",
        "unarmed whites"
      ],
      "methodology": [
        "Causal Bayesian Networks",
        "Statistical Analysis"
      ],
      "methodology_detail": "Modeling collider bias in police encounter data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.07384v1",
    "title": "A Pairwise Fair and Community-preserving Approach to k-Center Clustering",
    "year": 2020,
    "authors": [
      "Brian Brubach",
      "Darshan Chakrabarti",
      "John P. Dickerson",
      "Samir Khuller",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas"
    ],
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in clustering, focusing on preventing unfair separation of data points representing people, which relates to social fairness issues.",
      "inequality_type": [
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on fairness in algorithmic clustering",
      "affected_populations": [
        "people in clusters"
      ],
      "methodology": [
        "Algorithm Design",
        "Fairness Analysis",
        "Experimental Evaluation"
      ],
      "methodology_detail": "Extending k-center algorithms for fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.07092v2",
    "title": "A Normative approach to Attest Digital Discrimination",
    "year": 2020,
    "authors": [
      "Natalia Criado",
      "Xavier Ferrer",
      "Jose M. Such"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "68T27, 68T01"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses digital discrimination affecting marginalized groups, such as low-income neighborhoods and women, which relates to social inequalities like economic and gender disparities.",
      "inequality_type": [
        "economic",
        "gender"
      ],
      "other_detail": "Focuses on algorithmic bias and fairness issues",
      "affected_populations": [
        "low-income individuals",
        "women"
      ],
      "methodology": [
        "Formalization",
        "Algorithm Development"
      ],
      "methodology_detail": "Using norms to formalize discrimination detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.06908v1",
    "title": "Language, communication and society: a gender based linguistics analysis",
    "year": 2020,
    "authors": [
      "P. Cutugno",
      "D. Chiarella",
      "R. Lucentini",
      "L. Marconi",
      "G. Morgavi"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates gender stereotypes and their impact on societal roles and potential inequalities, directly addressing gender-based social disparities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes and societal roles",
      "affected_populations": [
        "men",
        "women"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Questionnaire analysis of perceptions and stereotypes",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.06570v1",
    "title": "Towards causal benchmarking of bias in face analysis algorithms",
    "year": 2020,
    "authors": [
      "Guha Balakrishnan",
      "Yuanjun Xiong",
      "Wei Xia",
      "Pietro Perona"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face analysis algorithms related to gender and skin tone, which are social identity factors. It focuses on measuring algorithmic bias that impacts different social groups, thus relating to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias measurement in social identity attributes",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Synthetic data manipulation and bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.06141v1",
    "title": "Gender Classification and Bias Mitigation in Facial Images",
    "year": 2020,
    "authors": [
      "Wenying Wu",
      "Pavlos Protopapas",
      "Zheng Yang",
      "Panagiotis Michalatos"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias and representation issues in facial recognition, highlighting social discrimination against gender minorities. It discusses algorithmic bias affecting marginalized groups, which relates to social inequality.",
      "inequality_type": [
        "gender",
        "social"
      ],
      "other_detail": "Focus on gender minorities and bias mitigation",
      "affected_populations": [
        "gender minorities",
        "LGBTQ",
        "non-binary"
      ],
      "methodology": [
        "Dataset Creation",
        "Model Development",
        "Experiment",
        "Survey"
      ],
      "methodology_detail": "Creating inclusive datasets and improving classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.05516v4",
    "title": "A Causal Linear Model to Quantify Edge Flow and Edge Unfairness for UnfairEdge Prioritization and Discrimination Removal",
    "year": 2020,
    "authors": [
      "Pavan Ravishankar",
      "Pranshu Malviya",
      "Balaraman Ravindran"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on unfairness, discrimination, and bias related to sensitive groups such as race, which are key social inequality issues.",
      "inequality_type": [
        "racial",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in AI decision-making processes",
      "affected_populations": [
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Causal Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses causal Bayesian networks and fairness quantification",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.05443v3",
    "title": "Algorithmic Fairness in Education",
    "year": 2020,
    "authors": [
      "René F. Kizilcec",
      "Hansol Lee"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and bias in educational AI systems, which relate to social inequalities in access and discrimination.",
      "inequality_type": [
        "educational",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in educational technology systems",
      "affected_populations": [
        "students",
        "educators",
        "administrators"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes fairness notions and bias sources in algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.04611v2",
    "title": "A deep learning approach to identify unhealthy advertisements in street view images",
    "year": 2020,
    "authors": [
      "Gregory Palmer",
      "Mark Green",
      "Emma Boyland",
      "Yales Stefano Rios Vasconcelos",
      "Rahul Savani",
      "Alex Singleton"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how unhealthy advertisements are distributed unequally across deprived areas, impacting vulnerable populations' health and social inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on health-related social disparities",
      "affected_populations": [
        "deprived communities",
        "students"
      ],
      "methodology": [
        "Deep Learning",
        "Dataset Creation",
        "Computer Vision"
      ],
      "methodology_detail": "Automated classification of street view images",
      "geographic_focus": [
        "Liverpool, UK"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.04361v1",
    "title": "Understanding the impact of the alphabetical ordering of names in user interfaces: a gender bias analysis",
    "year": 2020,
    "authors": [
      "Daniel Sullivan",
      "Carlos Caminha",
      "Victor Dantas",
      "Elizabeth Furtado",
      "Vasco Furtado",
      "Virgílio Almeida"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in name ordering, highlighting gender imbalance issues. It analyzes how interface design can induce gender bias, a social fairness concern. The focus on gender imbalance relates directly to social inequality in technology use.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Empirical Data Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of name datasets across countries",
      "geographic_focus": [
        "Brazil",
        "Spain",
        "Multiple countries"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.04068v1",
    "title": "Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence",
    "year": 2020,
    "authors": [
      "Shakir Mohamed",
      "Marie-Therese Png",
      "William Isaac"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses power, vulnerability, and ethical considerations related to marginalized groups, emphasizing decolonial perspectives to address inequalities in AI impacts.",
      "inequality_type": [
        "racial",
        "social",
        "educational",
        "disability"
      ],
      "other_detail": "Focus on coloniality and vulnerable populations",
      "affected_populations": [
        "vulnerable peoples",
        "marginalized groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Applying decolonial theory to AI ethics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.03775v1",
    "title": "README: REpresentation learning by fairness-Aware Disentangling MEthod",
    "year": 2020,
    "authors": [
      "Sungho Park",
      "Dohyung Kim",
      "Sunhee Hwang",
      "Hyeran Byun"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair representation learning to mitigate bias related to protected attributes like gender and age, addressing social fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Fairness in AI representations",
      "affected_populations": [
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Deep Learning",
        "Variational AutoEncoder",
        "Fairness-aware Disentangling"
      ],
      "methodology_detail": "Disentangles latent space for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.05461v1",
    "title": "Grading video interviews with fairness considerations",
    "year": 2020,
    "authors": [
      "Abhishek Singhania",
      "Abhishek Unnam",
      "Varun Aggarwal"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness, bias, and discrimination in AI-based video interview scoring across racial and gender groups, highlighting social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Focus on fairness in AI assessment of social skills",
      "affected_populations": [
        "racial minorities",
        "women",
        "interview candidates"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Includes bias and fairness evaluation across social groups",
      "geographic_focus": [
        "multiple countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2007.00088v1",
    "title": "Evaluation of Fairness Trade-offs in Predicting Student Success",
    "year": 2020,
    "authors": [
      "Hansol Lee",
      "René F. Kizilcec"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in predictive models affecting student support, addressing social discrimination related to gender and race.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Bias in educational support allocation",
      "affected_populations": [
        "students",
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates bias and fairness trade-offs in predictive models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.16742v2",
    "title": "FairRec: Fairness-aware News Recommendation with Decomposed Adversarial Learning",
    "year": 2020,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Xiting Wang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias related to sensitive attributes like gender, aiming to reduce unfairness in news recommendation, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "informational"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "male users",
        "female users"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Learning",
        "Regularization",
        "Decomposition"
      ],
      "methodology_detail": "Bias-aware and bias-free user embedding separation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.16402v1",
    "title": "Reading Between the Demographic Lines: Resolving Sources of Bias in Toxicity Classifiers",
    "year": 2020,
    "authors": [
      "Elizabeth Reichert",
      "Helen Qiu",
      "Jasmine Bayrooti"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI systems affecting marginalized groups, highlighting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in toxicity classification affecting marginalized identities",
      "affected_populations": [
        "women",
        "LGBTQ+",
        "ethnic minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation techniques in classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.16309v2",
    "title": "Adversarial Learning for Debiasing Knowledge Graph Embeddings",
    "year": 2020,
    "authors": [
      "Mario Arduini",
      "Lorenzo Noci",
      "Federico Pirovano",
      "Ce Zhang",
      "Yash Raj Shrestha",
      "Bibek Paudel"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender and social structures in knowledge graphs, which can impact social groups. It discusses social bias and fairness issues in AI systems. The focus on biases embedded in representations relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Bias mitigation in knowledge graph embeddings",
      "affected_populations": [
        "minority groups",
        "gender groups"
      ],
      "methodology": [
        "Adversarial Learning",
        "Algorithm Debiasing"
      ],
      "methodology_detail": "Filtering sensitive attributes from embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.13699v1",
    "title": "On Fair Selection in the Presence of Implicit Variance",
    "year": 2020,
    "authors": [
      "Vitalii Emelianov",
      "Nicolas Gast",
      "Krishna P. Gummadi",
      "Patrick Loiseau"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness mechanisms in selection processes, which relate to social discrimination and inequality based on demographic attributes. It discusses how these mechanisms impact utility and fairness, implying relevance to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in selection algorithms",
      "affected_populations": [
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling selection with group-dependent variance",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.13114v3",
    "title": "Fairness without Demographics through Adversarially Reweighted Learning",
    "year": 2020,
    "authors": [
      "Preethi Lahoti",
      "Alex Beutel",
      "Jilin Chen",
      "Kang Lee",
      "Flavien Prost",
      "Nithum Thain",
      "Xuezhi Wang",
      "Ed H. Chi"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on protected groups like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI without protected demographic data",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Reweighted Learning"
      ],
      "methodology_detail": "Reweighting approach to improve fairness without protected features",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.13025v2",
    "title": "Fair Active Learning",
    "year": 2020,
    "authors": [
      "Hadis Anahideh",
      "Abolfazl Asudeh",
      "Saravanan Thirumuruganathan"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on demographic parity, a fairness measure related to social groups. It aims to prevent discrimination in machine learning models, which impacts social inequality. The emphasis on fairness indicates concern with social bias and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and discrimination in AI models",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware active learning algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.11350v3",
    "title": "Achieving Fairness via Post-Processing in Web-Scale Recommender Systems",
    "year": 2020,
    "authors": [
      "Preetam Nandy",
      "Cyrus Diciccio",
      "Divya Venugopalan",
      "Heloise Logan",
      "Kinjal Basu",
      "Noureddine El Karoui"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME",
      "62P30, 62A01"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper extends fairness definitions to address bias related to protected attributes like gender and race in recommender systems, aiming to mitigate social discrimination. It focuses on fairness measures that ensure equal treatment of qualified candidates regardless of social groupings. The research directly tackles algorithmic bias impacting societal inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social"
      ],
      "other_detail": "Fairness in algorithmic decision-making",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness metrics and ranking adjustments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.16879v1",
    "title": "Combating Anti-Blackness in the AI Community",
    "year": 2020,
    "authors": [
      "Devin Guillory"
    ],
    "categories": [
      "cs.CY",
      "I.2.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses anti-Blackness and systemic discrimination within the AI community, focusing on racial inequality and social bias. It discusses how AI systems contribute to and can help dismantle racist systems, indicating a focus on social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on anti-Blackness and systemic discrimination",
      "affected_populations": [
        "Black communities",
        "AI community members"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing systemic issues and ethical considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.10667v1",
    "title": "Towards Threshold Invariant Fair Classification",
    "year": 2020,
    "authors": [
      "Mingliang Chen",
      "Min Wu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on equitable treatment across social groups based on sensitive attributes like race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Fairness in machine learning models",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Equalizing risk distributions across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.10194v5",
    "title": "Gender Inequality in Research Productivity During the COVID-19 Pandemic",
    "year": 2020,
    "authors": [
      "Ruomeng Cui",
      "Hao Ding",
      "Feng Zhu"
    ],
    "categories": [
      "cs.DL",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in research productivity during COVID-19, highlighting gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female academics",
        "male academics"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Difference-in-differences approach using research data",
      "geographic_focus": [
        "United States",
        "6 other countries"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.12621v4",
    "title": "Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning",
    "year": 2020,
    "authors": [
      "Vedant Nanda",
      "Samuel Dooley",
      "Sahil Singla",
      "Soheil Feizi",
      "John P. Dickerson"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to sensitive attributes like race and gender in AI systems, highlighting disparities in robustness across social groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on robustness bias in deep learning models",
      "affected_populations": [
        "race groups",
        "gender groups"
      ],
      "methodology": [
        "Empirical Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring robustness bias across datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.10085v2",
    "title": "Socially Fair k-Means Clustering",
    "year": 2020,
    "authors": [
      "Mehrdad Ghadiri",
      "Samira Samadi",
      "Santosh Vempala"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in clustering algorithms affecting different social groups, highlighting fairness issues relevant to social inequality.",
      "inequality_type": [
        "racial",
        "demographic",
        "social fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness in social contexts",
      "affected_populations": [
        "demographic groups",
        "subgroups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness-aware clustering algorithm design",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.08688v1",
    "title": "Causal intersectionality for fair ranking",
    "year": 2020,
    "authors": [
      "Ke Yang",
      "Joshua R. Loftus",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses intersectional fairness, which relates to social discrimination and inequality across multiple social groups, using causal modeling to improve fair rankings.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on intersectionality in fairness modeling",
      "affected_populations": [
        "social groups",
        "users of rankings"
      ],
      "methodology": [
        "Causal Modeling",
        "Experiment"
      ],
      "methodology_detail": "Applying causal inference to fairness in rankings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.08669v1",
    "title": "On Adversarial Bias and the Robustness of Fair Machine Learning",
    "year": 2020,
    "authors": [
      "Hongyan Chang",
      "Ta Duy Nguyen",
      "Sasi Kumar Murakonda",
      "Ehsan Kazemi",
      "Reza Shokri"
    ],
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness constraints and bias in machine learning, focusing on discrimination and group fairness, which relate to social inequalities such as race, gender, and group disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and bias in AI systems",
      "affected_populations": [
        "social groups",
        "discriminated groups"
      ],
      "methodology": [
        "Experiment",
        "Empirical Evaluation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Attacks on fair machine learning algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.08350v2",
    "title": "Societal biases reinforcement through machine learning: A credit scoring perspective",
    "year": 2020,
    "authors": [
      "Bertrand K. Hassani"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.CO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how social biases related to gender and ethnicity are transmitted into credit scoring models, reflecting societal discrimination. It analyzes the impact of social biases on algorithmic decision-making in banking. The focus on social groups and bias propagation indicates a direct link to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias transmission in financial decision-making",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predicting gender and ethnicity from application data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.08315v7",
    "title": "Mitigating Gender Bias in Captioning Systems",
    "year": 2020,
    "authors": [
      "Ruixiang Tang",
      "Mengnan Du",
      "Yuening Li",
      "Zirui Liu",
      "Na Zou",
      "Xia Hu"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in image captioning, highlighting social gender disparities and AI fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Reorganized dataset and proposed bias mitigation model",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.08267v4",
    "title": "Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility",
    "year": 2020,
    "authors": [
      "Sen Cui",
      "Weishen Pan",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in ranking algorithms, focusing on disparities across protected groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "health",
        "educational"
      ],
      "other_detail": "Fairness in algorithmic ranking across social groups",
      "affected_populations": [
        "protected groups",
        "patients",
        "students"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Post-processing fairness adjustment in ranking models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.07986v2",
    "title": "Fairness Under Feature Exemptions: Counterfactual and Observational Measures",
    "year": 2020,
    "authors": [
      "Sanghamitra Dutta",
      "Praveen Venkatesh",
      "Piotr Mardziel",
      "Anupam Datta",
      "Pulkit Grover"
    ],
    "categories": [
      "cs.IT",
      "cs.CR",
      "cs.CY",
      "cs.LG",
      "math.IT",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities related to protected attributes like gender and race, focusing on fairness in AI systems, which are central to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Fairness in algorithmic decision-making",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Information Theory",
        "Causal Analysis"
      ],
      "methodology_detail": "Decomposition of disparity measures using information theory",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.07906v2",
    "title": "Fair Influence Maximization: A Welfare Optimization Approach",
    "year": 2020,
    "authors": [
      "Aida Rahmattalabi",
      "Shahin Jabbari",
      "Himabindu Lakkaraju",
      "Phebe Vayanos",
      "Max Izenberg",
      "Ryan Brown",
      "Eric Rice",
      "Milind Tambe"
    ],
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in influence maximization, aiming to prevent exclusion of minority communities, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focuses on fairness in social interventions",
      "affected_populations": [
        "minority communities",
        "social groups"
      ],
      "methodology": [
        "Algorithm Design",
        "Social Welfare Theory",
        "Optimization"
      ],
      "methodology_detail": "Framework based on welfare functions and influence algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.07845v2",
    "title": "Towards Gender-Neutral Face Descriptors for Mitigating Bias in Face Recognition",
    "year": 2020,
    "authors": [
      "Prithviraj Dhar",
      "Joshua Gleason",
      "Hossein Souri",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias and privacy issues in face recognition, which relate to social discrimination and fairness. It aims to reduce gender bias in AI systems, impacting social equality. The focus on gender bias indicates a concern with social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias reduction in face descriptors",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.07647v1",
    "title": "Quota-based debiasing can decrease representation of already underrepresented groups",
    "year": 2020,
    "authors": [
      "Ivan Smirnov",
      "Florian Lemmerich",
      "Markus Strohmaier"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in selection processes affecting underrepresented social groups, highlighting issues related to representation and fairness. It examines how debiasing methods impact social disparities, indicating a focus on social inequality. The abstract explicitly mentions underrepresentation and fairness concerns in societal decision-making.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on representation and fairness in societal decisions",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes datasets from real-world domains",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.06865v1",
    "title": "Exploring Algorithmic Fairness in Robust Graph Covering Problems",
    "year": 2020,
    "authors": [
      "Aida Rahmattalabi",
      "Phebe Vayanos",
      "Anthony Fulginiti",
      "Eric Rice",
      "Bryan Wilder",
      "Amulya Yadav",
      "Milind Tambe"
    ],
    "categories": [
      "math.OC",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias and fairness constraints affecting marginalized social groups, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "group fairness"
      ],
      "other_detail": "Focus on social discrimination in algorithmic decision-making",
      "affected_populations": [
        "marginalized groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Fairness constraints and approximation schemes in algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.05754v1",
    "title": "Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus",
    "year": 2020,
    "authors": [
      "Luisa Bentivogli",
      "Beatrice Savoldi",
      "Matteo Negri",
      "Mattia Antonino Di Gangi",
      "Roldano Cattoni",
      "Marco Turchi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in speech translation, addressing gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI translation systems",
      "affected_populations": [
        "gendered language speakers",
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "System Design"
      ],
      "methodology_detail": "Evaluates translation technologies on gender bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.05255v1",
    "title": "DeepFair: Deep Learning for Improving Fairness in Recommender Systems",
    "year": 2020,
    "authors": [
      "Jesús Bobadilla",
      "Raúl Lara-Cabrera",
      "Ángel González-Prieto",
      "Fernando Ortega"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.5.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in recommender systems, which impact social groups and can perpetuate bias. It discusses bias management and fairness without demographic data, indicating concern with social discrimination. The focus on fairness relates to social inequality in algorithmic impacts.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Fairness in AI recommendations without demographic data",
      "affected_populations": [
        "minority groups",
        "social groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Collaborative Filtering algorithm evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.04599v6",
    "title": "Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms",
    "year": 2020,
    "authors": [
      "Akshat Pandey",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines AI bias impacting fare prices across neighborhoods with different demographic characteristics, highlighting racial, socioeconomic, age, and educational disparities.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "age",
        "educational"
      ],
      "other_detail": "Bias in AI-driven ridehailing fare discrimination",
      "affected_populations": [
        "non-white neighborhoods",
        "low-income residents",
        "younger residents",
        "higher-education areas"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Meta-analysis of city-level demographic and fare data",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.04279v3",
    "title": "Interplay between Upsampling and Regularization for Provider Fairness in Recommender Systems",
    "year": 2020,
    "authors": [
      "Ludovico Boratto",
      "Gianni Fenu",
      "Mirko Marras"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in recommendation exposure affecting minority provider groups, highlighting social fairness issues related to race and visibility.",
      "inequality_type": [
        "racial",
        "informational"
      ],
      "other_detail": "focus on provider fairness in recommender systems",
      "affected_populations": [
        "minority providers",
        "disadvantaged groups"
      ],
      "methodology": [
        "Experiment",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing disparities via simulated catalog representations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.04029v1",
    "title": "Ethics, Data Science, and Health and Human Services: Embedded Bias in Policy Approaches to Teen Pregnancy Prevention",
    "year": 2020,
    "authors": [
      "Davon Woodard",
      "Huthaifa I. Ashqar",
      "Taoran Ji"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines disparities in teen pregnancy prevention policies, funding models, and societal outcomes, highlighting ethical considerations related to social groups and resource distribution.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focus on policy impacts on at-risk youth in Chicago",
      "affected_populations": [
        "at-risk teens",
        "Chicago communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Ethics Analysis"
      ],
      "methodology_detail": "Evaluation of policy models and societal outcomes",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.03983v3",
    "title": "Advertising for Demographically Fair Outcomes",
    "year": 2020,
    "authors": [
      "Lodewijk Gelauff",
      "Ashish Goel",
      "Kamesh Munagala",
      "Sravya Yandamuri"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in online advertising to achieve demographic representativeness, focusing on equitable outcomes across social groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on demographic fairness in advertising outcomes",
      "affected_populations": [
        "minority groups",
        "general population"
      ],
      "methodology": [
        "Empirical Analysis",
        "Algorithm Design"
      ],
      "methodology_detail": "Real-world experiments and optimization algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.03977v1",
    "title": "IP Geolocation Underestimates Regressive Economic Patterns in MOOC Usage",
    "year": 2020,
    "authors": [
      "Daniela Ganelin",
      "Isaac Chuang"
    ],
    "categories": [
      "cs.CY",
      "cs.NI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines socioeconomic disparities in MOOC registration and geolocation bias, highlighting inequalities in access and measurement accuracy.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Bias in geolocation affecting economic pattern detection",
      "affected_populations": [
        "distressed areas",
        "low-income users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Using geolocation and mailing address data comparison",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.03955v5",
    "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
    "year": 2020,
    "authors": [
      "Wei Guo",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race and gender in language models, reflecting social discrimination. It analyzes intersectional biases affecting marginalized groups, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on intersectional biases in language models",
      "affected_populations": [
        "African American females",
        "Mexican American females"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and measurement in neural language models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.01938v1",
    "title": "Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings",
    "year": 2020,
    "authors": [
      "Vaibhav Kumar",
      "Tenzin Singhay Bhotia",
      "Vaibhav Kumar",
      "Tanmoy Chakraborty"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, which relates to social gender inequality and discrimination. It focuses on mitigating biases that can perpetuate stereotypes and unfair treatment in AI systems. The work directly engages with social bias issues in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI language models",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias reduction and evaluation in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.01770v2",
    "title": "What's Sex Got To Do With Fair Machine Learning?",
    "year": 2020,
    "authors": [
      "Lily Hu",
      "Issa Kohler-Hausmann"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social constructs of sex and their implications for discrimination, addressing gender and social meaning. It critically examines how causal models relate to social groups and discrimination, which are central to social inequality debates.",
      "inequality_type": [
        "gender",
        "social"
      ],
      "other_detail": "Focuses on social meaning of sex and discrimination",
      "affected_populations": [
        "women",
        "gender groups"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes causal and constitutive models of social groups",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.04944v1",
    "title": "A Machine Learning System for Retaining Patients in HIV Care",
    "year": 2020,
    "authors": [
      "Avishek Kumar",
      "Arthi Ramachandran",
      "Adolfo De Unanue",
      "Christina Sung",
      "Joe Walsh",
      "John Schneider",
      "Jessica Ridgway",
      "Stephanie Masiello Schuette",
      "Jeff Lauritsen",
      "Rayid Ghani"
    ],
    "categories": [
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in HIV care retention, focusing on fairness and equitable intervention. It aims to reduce health disparities among vulnerable populations. The emphasis on fairness indicates concern with social inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on health disparities in HIV care",
      "affected_populations": [
        "persons living with HIV"
      ],
      "methodology": [
        "Machine Learning"
      ],
      "methodology_detail": "Predictive modeling for care retention",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2006.03498v1",
    "title": "Commuting Variability by Wage Groups in Baton Rouge 1990-2010",
    "year": 2020,
    "authors": [
      "Yujie Hu",
      "Fahui Wang",
      "Chester Wilmot"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines commuting patterns and mobility disparities across income groups, highlighting socioeconomic inequalities. It analyzes how different wage groups experience varying commuting distances and stability over time, indicating social inequality in mobility. The focus on low-wage workers' mobility further emphasizes socioeconomic disparities.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Mobility disparities among income groups",
      "affected_populations": [
        "low-wage workers",
        "residents of Baton Rouge"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Monte Carlo simulation and distribution analysis",
      "geographic_focus": [
        "Baton Rouge"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.13041v1",
    "title": "Examining Racial Bias in an Online Abuse Corpus with Structural Topic Modeling",
    "year": 2020,
    "authors": [
      "Thomas Davidson",
      "Debasmita Bhattacharya"
    ],
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in hate speech data, addressing racial discrimination and bias in AI systems.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on racial bias in social media data",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Structural Topic Modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes content and bias in social media posts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.12246v1",
    "title": "Demoting Racial Bias in Hate Speech Detection",
    "year": 2020,
    "authors": [
      "Mengzhou Xia",
      "Anjalie Field",
      "Yulia Tsvetkov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in hate speech detection, highlighting social discrimination against African American English speakers.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias mitigation in AI for racial fairness",
      "affected_populations": [
        "African American English speakers"
      ],
      "methodology": [
        "Adversarial Training",
        "Experiment"
      ],
      "methodology_detail": "Mitigates linguistic bias in hate speech classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.12379v2",
    "title": "Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness",
    "year": 2020,
    "authors": [
      "Sumon Biswas",
      "Hridesh Rajan"
    ],
    "categories": [
      "cs.LG",
      "cs.SE",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in machine learning models, focusing on discrimination related to protected attributes like race, sex, and age, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "racial groups",
        "women",
        "elderly"
      ],
      "methodology": [
        "Empirical Analysis",
        "Fairness Metrics Evaluation",
        "Mitigation Techniques"
      ],
      "methodology_detail": "Benchmarking models and analyzing fairness impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.14621v1",
    "title": "Fair Classification via Unconstrained Optimization",
    "year": 2020,
    "authors": [
      "Ibrahim Alabdulmohsin"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML",
      "68T05",
      "I.2.6; I.5"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in AI classification, addressing bias and discrimination issues related to social groups. It discusses algorithms to improve fairness across demographic groups, which directly relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses algorithmic fairness and bias mitigation",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness constraint optimization in ML models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.12974v1",
    "title": "Opportunistic Multi-aspect Fairness through Personalized Re-ranking",
    "year": 2020,
    "authors": [
      "Nasim Sonboli",
      "Farzad Eskandanian",
      "Robin Burke",
      "Weiwen Liu",
      "Bamshad Mobasher"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems affecting social groups, focusing on multiple social dimensions such as race and gender, and aims to improve equitable outcomes in recommendation systems.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "educational"
      ],
      "other_detail": "Multiple fairness dimensions in recommendation re-ranking",
      "affected_populations": [
        "social groups",
        "recommendation users"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Fairness-aware re-ranking approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.10430v1",
    "title": "Gender Slopes: Counterfactual Fairness for Computer Vision Models by Attribute Manipulation",
    "year": 2020,
    "authors": [
      "Jungseock Joo",
      "Kimmo Kärkkäinen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in AI systems, highlighting social discrimination issues. It examines how AI models may perpetuate or reflect societal biases. The focus on fairness and representation indicates addressing social inequality.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI image classification",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Computer Vision",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Using attribute manipulation to test fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.10050v2",
    "title": "Risk of Training Diagnostic Algorithms on Data with Demographic Bias",
    "year": 2020,
    "authors": [
      "Samaneh Abbasi-Sureshjani",
      "Ralf Raumanns",
      "Britt E. J. Michels",
      "Gerard Schouten",
      "Veronika Cheplygina"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses demographic biases in medical datasets and their impact on diagnostic fairness, addressing racial, age, and gender disparities in AI healthcare applications.",
      "inequality_type": [
        "racial",
        "age",
        "gender",
        "health"
      ],
      "other_detail": "Focuses on demographic bias in medical AI",
      "affected_populations": [
        "patients by demographics"
      ],
      "methodology": [
        "Survey",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Analyzes dataset demographics and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.08033v1",
    "title": "Towards classification parity across cohorts",
    "year": 2020,
    "authors": [
      "Aarsh Patel",
      "Rahul Gupta",
      "Mukund Harakere",
      "Satyapriya Krishna",
      "Aman Alok",
      "Peng Liu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, aiming to reduce performance disparities across different social groups defined by explicit and implicit sensitive features, addressing social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Implicit cohort fairness across language-based groups",
      "affected_populations": [
        "ethnic groups",
        "gender groups",
        "language users"
      ],
      "methodology": [
        "Machine Learning",
        "Clustering",
        "Loss Function Modification"
      ],
      "methodology_detail": "Clustering embeddings, fairness optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.07734v1",
    "title": "Uncovering Gender Bias in Media Coverage of Politicians with Machine Learning",
    "year": 2020,
    "authors": [
      "Susan Leavy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in media coverage of politicians, addressing gender inequality and social bias in representation. It uses AI techniques to analyze societal disparities in political portrayal.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female politicians"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning"
      ],
      "methodology_detail": "Analyzing media content for bias detection",
      "geographic_focus": [
        "Ireland"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.07302v2",
    "title": "Investigating Bias in Deep Face Analysis: The KANFace Dataset and Empirical Study",
    "year": 2020,
    "authors": [
      "Markos Georgopoulos",
      "Yannis Panagakis",
      "Maja Pantic"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic bias in face analysis, addressing social discrimination related to race, gender, and age, and examines AI fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Focus on demographic bias in facial recognition systems",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Deep Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes bias using a large-scale, annotated facial dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.07293v1",
    "title": "Statistical Equity: A Fairness Classification Objective",
    "year": 2020,
    "authors": [
      "Ninareh Mehrabi",
      "Yuzhong Huang",
      "Fred Morstatter"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, considering biases and equitable decisions, which relate to social inequalities such as discrimination and bias. It discusses societal biases embedded in data and algorithms, impacting different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "social groups",
        "historically biased communities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluations of fairness definitions and impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.06618v2",
    "title": "Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective Learning",
    "year": 2020,
    "authors": [
      "Procheta Sen",
      "Debasis Ganguly"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender and ethnicity in AI predictions, which are social inequalities. It focuses on reducing stereotypes and prejudices in AI outputs, directly relating to social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias reduction in AI predictions",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Multi-Objective Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias-aware learning framework to reduce stereotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.06898v2",
    "title": "Mitigating Gender Bias in Machine Learning Data Sets",
    "year": 2020,
    "authors": [
      "Susan Leavy",
      "Gerardine Meaney",
      "Karen Wade",
      "Derek Greene"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI, a form of social inequality, by analyzing and mitigating gender-based disparities in machine learning data. It draws on gender theory and sociolinguistics to identify bias, highlighting societal gender inequalities embedded in language and AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in textual training data",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Theoretical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzing bias levels in language data and models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.05921v3",
    "title": "Intersectional Bias in Hate Speech and Abusive Language Datasets",
    "year": 2020,
    "authors": [
      "Jae Yeon Kim",
      "Carlos Ortiz",
      "Sarah Nam",
      "Sarah Santiago",
      "Vivek Datta"
    ],
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in hate speech datasets, highlighting social discrimination and bias issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in annotated social media data",
      "affected_populations": [
        "African Americans",
        "Twitter users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzed annotated Twitter dataset for bias patterns",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.05906v1",
    "title": "Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI",
    "year": 2020,
    "authors": [
      "Sandra Wachter",
      "Brent Mittelstadt",
      "Chris Russell"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination under EU law, focusing on social biases and fairness issues in AI systems affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "discrimination"
      ],
      "other_detail": "Focus on legal and social discrimination in AI",
      "affected_populations": [
        "discriminated groups"
      ],
      "methodology": [
        "Legal Analysis",
        "Theoretical Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes legal standards and proposes statistical measures",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.06625v2",
    "title": "Cyberbullying Detection with Fairness Constraints",
    "year": 2020,
    "authors": [
      "Oguzhan Gencoglu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias and fairness issues in cyberbullying detection, which relate to social discrimination and inequality in online interactions.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "digital",
        "informational"
      ],
      "other_detail": "Focuses on social bias mitigation in AI systems",
      "affected_populations": [
        "cyberbullying victims",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Constraints",
        "Experiment"
      ],
      "methodology_detail": "Incorporates fairness constraints into model training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.04364v1",
    "title": "It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations",
    "year": 2020,
    "authors": [
      "Samson Tan",
      "Shafiq Joty",
      "Min-Yen Kan",
      "Richard Socher"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.NE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses linguistic discrimination against non-standard language speakers, highlighting biases in NLP models that can reinforce social inequalities related to race and linguistic background.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias in language models affecting marginalized linguistic groups",
      "affected_populations": [
        "non-standard language speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Adversarial Training"
      ],
      "methodology_detail": "Perturbation and fine-tuning of NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.04176v3",
    "title": "In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction",
    "year": 2020,
    "authors": [
      "Caroline Wang",
      "Bin Han",
      "Bhrij Patel",
      "Cynthia Rudin"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in recidivism prediction models, which relate to social discrimination and inequality across racial and geographic groups.",
      "inequality_type": [
        "racial",
        "geographic",
        "fairness"
      ],
      "other_detail": "focus on fairness in criminal justice AI models",
      "affected_populations": [
        "racial minorities",
        "geographic communities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "evaluates fairness and performance across groups",
      "geographic_focus": [
        "Florida",
        "Kentucky"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.04074v2",
    "title": "Adversarial Graph Embeddings for Fair Influence Maximization over Social Networks",
    "year": 2020,
    "authors": [
      "Moein Khajehnejad",
      "Ahmad Asgharian Rezaei",
      "Mahmoudreza Babaei",
      "Jessica Hoffmann",
      "Mahdi Jalili",
      "Adrian Weller"
    ],
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair influence maximization targeting minorities, addressing social fairness issues related to sensitive attributes like race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Fairness in influence spread across social groups",
      "affected_populations": [
        "minorities",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Adversarial graph embeddings for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.03474v1",
    "title": "Ensuring Fairness under Prior Probability Shifts",
    "year": 2020,
    "authors": [
      "Arpita Biswas",
      "Suvam Mukherjee"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI predictions, focusing on subgroup disparities, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Fairness under distribution shifts in AI systems",
      "affected_populations": [
        "racial minorities",
        "low-income groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Prevalence estimation and ensemble classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.02439v3",
    "title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation",
    "year": 2020,
    "authors": [
      "Brendan Kennedy",
      "Xisen Jin",
      "Aida Mostafazadeh Davani",
      "Morteza Dehghani",
      "Xiang Ren"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in hate speech detection related to social identity groups, impacting fairness and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Bias mitigation in AI classifiers for social groups",
      "affected_populations": [
        "racial minorities",
        "LGBTQ+",
        "social groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Using post-hoc explanations and regularization techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.01980v1",
    "title": "Can gender inequality be created without inter-group discrimination?",
    "year": 2020,
    "authors": [
      "Sylvie Huet1",
      "Floriana Gargiulo",
      "Felicia Pratto"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.MA"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models gender-based status and self-esteem disparities without prejudice, addressing gender inequality as a social phenomenon.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender hierarchy formation without discrimination",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Agent-based modeling"
      ],
      "methodology_detail": "Simulates interactions to observe inequality emergence",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.00965v1",
    "title": "Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation",
    "year": 2020,
    "authors": [
      "Tianlu Wang",
      "Xi Victoria Lin",
      "Nazneen Fatema Rajani",
      "Bryan McCann",
      "Vicente Ordonez",
      "Caiming Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, a form of social bias affecting gender equality. It discusses mitigating gender bias in AI, which impacts social fairness. The focus on gender bias aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.00962v2",
    "title": "Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations",
    "year": 2020,
    "authors": [
      "Saif M. Mohammad"
    ],
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in authorship and citations in NLP, addressing gender inequality and bias in academic research.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female researchers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes authorship and citation data over time",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.00699v1",
    "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
    "year": 2020,
    "authors": [
      "Jieyu Zhao",
      "Subhabrata Mukherjee",
      "Saghar Hosseini",
      "Kai-Wei Chang",
      "Ahmed Hassan Awadallah"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in multilingual embeddings, highlighting social bias issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and transfer analysis in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.00614v1",
    "title": "Multi-Dimensional Gender Bias Classification",
    "year": 2020,
    "authors": [
      "Emily Dinan",
      "Angela Fan",
      "Ledell Wu",
      "Jason Weston",
      "Douwe Kiela",
      "Adina Williams"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in NLP models, a social discrimination issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Decomposing bias and annotating datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.00372v3",
    "title": "Do Neural Ranking Models Intensify Gender Bias?",
    "year": 2020,
    "authors": [
      "Navid Rekabsaz",
      "Markus Schedl"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI retrieval systems, highlighting societal gender inequalities. It analyzes how neural models reinforce or intensify gender bias, a key aspect of social discrimination. The focus on gender bias in AI directly relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement framework and retrieval experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2005.00268v2",
    "title": "Towards Controllable Biases in Language Generation",
    "year": 2020,
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Premkumar Natarajan",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on societal biases in language generation, analyzing and mitigating biases related to demographic groups, which are central to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias analysis and mitigation in AI language models",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Bias mitigation techniques"
      ],
      "methodology_detail": "Using adversarial triggers for bias control",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.13515v4",
    "title": "Addressing Artificial Intelligence Bias in Retinal Disease Diagnostics",
    "year": 2020,
    "authors": [
      "Philippe Burlina",
      "Neil Joshi",
      "William Paul",
      "Katia D. Pacheco",
      "Neil M. Bressler"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in AI diagnostic accuracy across racial groups, highlighting bias related to skin pigmentation and ethnicity, which are social constructs linked to racial inequality.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias mitigation in AI diagnostics for racial disparities",
      "affected_populations": [
        "darker-skin individuals",
        "lighter-skin individuals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Generative debiasing methods applied to medical imaging data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.13282v1",
    "title": "Genetic programming approaches to learning fair classifiers",
    "year": 2020,
    "authors": [
      "William La Cava",
      "Jason H. Moore"
    ],
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in algorithms, addressing social bias and discrimination issues related to protected groups. It discusses algorithmic fairness, which impacts social inequalities such as race, gender, and other protected categories. The emphasis on subgroup fairness indicates concern with social disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "protected groups",
        "social minorities"
      ],
      "methodology": [
        "Genetic Programming",
        "Multi-objective Optimization",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Optimizing fairness and accuracy trade-offs in classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.12332v1",
    "title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds",
    "year": 2020,
    "authors": [
      "Kawin Ethayarajh"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias measurement in AI systems, focusing on fairness related to protected attributes like gender, which are linked to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias measurement in AI fairness",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Using Bernstein bounds for bias estimation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.11531v2",
    "title": "Statistical Discrimination in Ratings-Guided Markets",
    "year": 2020,
    "authors": [
      "Yeon-Koo Che",
      "Kyungmin Kim",
      "Weijie Zhong"
    ],
    "categories": [
      "cs.GT",
      "econ.TH"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination based on social identities in markets, highlighting social bias and fairness issues in algorithmic social learning.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "informational"
      ],
      "other_detail": "Focus on social identity-based discrimination in ratings systems",
      "affected_populations": [
        "social groups",
        "market participants"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Modeling user attention and data sampling effects",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.10846v4",
    "title": "Reducing the Filtering Effect in Public School Admissions: A Bias-aware Analysis for Targeted Interventions",
    "year": 2020,
    "authors": [
      "Yuri Faenza",
      "Swati Gupta",
      "Aapeli Vuorinen",
      "Xuan Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses socioeconomic disparities affecting school admissions and bias reduction strategies.",
      "inequality_type": [
        "economic",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on disadvantaged students' access to top schools",
      "affected_populations": [
        "disadvantaged students",
        "low-income students"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Bias modeling and intervention impact assessment",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.11246v2",
    "title": "SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning",
    "year": 2020,
    "authors": [
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez",
      "Manuel Cebrian",
      "Nick Obradovich",
      "Iyad Rahwan"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic discrimination in face recognition, focusing on fairness across demographic groups such as gender and ethnicity.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in biometric AI systems",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Discrimination-aware training with Sensitive Loss",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.09456v1",
    "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
    "year": 2020,
    "authors": [
      "Moin Nadeem",
      "Anna Bethke",
      "Siva Reddy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring stereotypical biases in language models related to race, gender, religion, and profession, which are social groups affected by inequality and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "religion"
      ],
      "other_detail": "Bias measurement in AI models",
      "affected_populations": [
        "racial groups",
        "women",
        "religious groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops a bias dataset and evaluates models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.09293v4",
    "title": "A Social Network Analysis of Occupational Segregation",
    "year": 2020,
    "authors": [
      "I. Sebastian Buhai",
      "Marco J. van der Leij"
    ],
    "categories": [
      "econ.TH",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes occupational segregation driven by social homophily, leading to labor market disparities based on race or gender, directly addressing social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "labor market"
      ],
      "other_detail": "Focuses on racial and gender occupational disparities",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Calibration and Simulation"
      ],
      "methodology_detail": "Modeling social interactions and equilibrium outcomes",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.08945v1",
    "title": "Exploring Racial Bias within Face Recognition via per-subject Adversarially-Enabled Data Augmentation",
    "year": 2020,
    "authors": [
      "Seyma Yucer",
      "Samet Akçay",
      "Noura Al-Moubayed",
      "Toby P. Breckon"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition, aiming to reduce racial performance disparities, which relates directly to social racial inequality.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in AI systems",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Data Augmentation",
        "Experiment"
      ],
      "methodology_detail": "Using adversarial image transformation for dataset balancing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.08361v2",
    "title": "Unsupervised Discovery of Implicit Gender Bias",
    "year": 2020,
    "authors": [
      "Anjalie Field",
      "Yulia Tsvetkov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases related to gender, a key aspect of social inequality, and examines implicit gender bias in society and AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Implicit gender bias in social comments",
      "affected_populations": [
        "women",
        "female politicians"
      ],
      "methodology": [
        "Natural Language Processing",
        "Unsupervised Learning",
        "Adversarial Learning",
        "Propensity Matching"
      ],
      "methodology_detail": "Focus on bias detection without human labels",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.07173v1",
    "title": "Bias in Multimodal AI: Testbed for Fair Automatic Recruitment",
    "year": 2020,
    "authors": [
      "Alejandro Peña",
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in AI recruitment, highlighting discrimination issues. It addresses how AI systems can perpetuate or mitigate social biases, impacting marginalized groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in automated decision-making systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Synthetic profiles and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.06592v3",
    "title": "InsideBias: Measuring Bias in Deep Networks and Application to Face Gender Biometrics",
    "year": 2020,
    "authors": [
      "Ignacio Serna",
      "Alejandro Peña",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in face recognition models related to gender and ethnicity, highlighting social bias issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias detection in facial gender recognition models",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias measurement and detection methods in neural networks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.06524v1",
    "title": "Contrastive Examples for Addressing the Tyranny of the Majority",
    "year": 2020,
    "authors": [
      "Viktoriia Sharmanska",
      "Lisa Anne Hendricks",
      "Trevor Darrell",
      "Novi Quadrianto"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI systems related to group representation, exposing social biases and fairness issues, particularly in face recognition and tabular data. It discusses how AI models can perpetuate or obscure social disparities. The focus on group membership and bias measures indicates a concern with social inequality in AI outcomes.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias and fairness in AI systems",
      "affected_populations": [
        "minorities",
        "majorities"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Using generative adversarial networks for contrastive data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.05455v1",
    "title": "Evidence of economic segregation from mobility lockdown during COVID-19 epidemic",
    "year": 2020,
    "authors": [
      "Giovanni Bonaccorsi",
      "Francesco Pierri",
      "Matteo Cinelli",
      "Francesco Porcelli",
      "Alessandro Galeazzi",
      "Andrea Flori",
      "Ana Lucia Schmidt",
      "Carlo Michele Valensise",
      "Antonio Scala",
      "Walter Quattrociocchi",
      "Fabio Pammolli"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates how lockdown measures disproportionately affect municipalities with higher inequality and lower income, highlighting socioeconomic disparities during COVID-19.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on economic and geographic segregation effects",
      "affected_populations": [
        "low-income individuals",
        "municipalities with higher inequality"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of mobility data and fiscal capacity",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.04907v1",
    "title": "Socioeconomic correlations of urban patterns inferred from aerial images: interpreting activation maps of Convolutional Neural Networks",
    "year": 2020,
    "authors": [
      "Jacob Levy Abitbol",
      "Márton Karsai"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on socioeconomic status and urban patterns, addressing economic and social disparities related to urbanization. It analyzes how AI models interpret socioeconomic inequalities through aerial imagery, which relates to societal inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Focuses on urban socioeconomic disparities in France",
      "affected_populations": [
        "urban residents"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Using CNNs and activation map interpretation",
      "geographic_focus": [
        "France"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.03133v2",
    "title": "Neutralizing Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation",
    "year": 2020,
    "authors": [
      "Seungjae Shin",
      "Kyungwoo Song",
      "JoonHo Jang",
      "Hyemi Kim",
      "Weonyoung Joo",
      "Il-Chul Moon"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, a form of social discrimination. It focuses on mitigating gender bias in AI systems, which impacts social equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Latent disentanglement and counterfactual generation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.02028v1",
    "title": "Measuring Social Biases of Crowd Workers using Counterfactual Queries",
    "year": 2020,
    "authors": [
      "Bhavya Ghai",
      "Q. Vera Liao",
      "Yunfeng Zhang",
      "Klaus Mueller"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases related to gender and race in crowdsourcing, which are social inequalities. It addresses fairness issues in AI systems by measuring biases of crowd workers. The work aims to reduce biased data collection, impacting social equity.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias measurement in crowdsourcing",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using counterfactual fairness to quantify bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2004.01019v3",
    "title": "Face Quality Estimation and Its Correlation to Demographic and Non-Demographic Bias in Face Recognition",
    "year": 2020,
    "authors": [
      "Philipp Terhörst",
      "Jan Niklas Kolf",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in face recognition related to demographic groups such as ethnicity, age, and pose, highlighting issues of fairness and discrimination in AI systems.",
      "inequality_type": [
        "racial",
        "age",
        "demographic"
      ],
      "other_detail": "Bias transfer in face quality assessment and recognition systems",
      "affected_populations": [
        "ethnic groups",
        "age groups",
        "demographic subgroups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias analysis on face datasets and quality assessment algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.14263v2",
    "title": "A survey of bias in Machine Learning through the prism of Statistical Parity for the Adult Data Set",
    "year": 2020,
    "authors": [
      "Philippe Besse",
      "Eustasio del Barrio",
      "Paula Gordaliza",
      "Jean-Michel Loubes",
      "Laurent Risser"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and fairness in machine learning, focusing on disparities affecting social groups, particularly through socioeconomic data.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Bias measurement in socioeconomic dataset",
      "affected_populations": [
        "low-income individuals",
        "minority groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Machine Learning",
        "Disparate Impact index"
      ],
      "methodology_detail": "Bias quantification and bias reduction approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.13808v1",
    "title": "Fairness Evaluation in Presence of Biased Noisy Labels",
    "year": 2020,
    "authors": [
      "Riccardo Fogliato",
      "Max G'Sell",
      "Alexandra Chouldechova"
    ],
    "categories": [
      "stat.ME",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in criminal justice risk assessments, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "social",
        "fairness"
      ],
      "other_detail": "Focus on racial bias in predictive models",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Sensitivity Analysis",
        "Experimental Analysis"
      ],
      "methodology_detail": "Assessing bias impact under label noise assumptions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.12375v1",
    "title": "Democratic Value and Money for Decentralized Digital Society",
    "year": 2020,
    "authors": [
      "Bryan Ford"
    ],
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.DC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses economic inequality, fairness, and democratic participation, addressing social disparities related to wealth and opportunity.",
      "inequality_type": [
        "wealth",
        "economic",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on monetary and economic inequality",
      "affected_populations": [
        "vulnerable populations",
        "future generations"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Conceptual Framework"
      ],
      "methodology_detail": "Explores alternative monetary principles and democratic governance",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.11634v1",
    "title": "Unfair Exposure of Artists in Music Recommendation",
    "year": 2020,
    "authors": [
      "Himan Abdollahpouri",
      "Robin Burke",
      "Masoud Mansoury"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how recommendation algorithms systematically favor certain groups of artists, leading to unequal exposure. This reflects social bias and fairness issues affecting artists' opportunities, which are social inequality concerns.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Bias against less popular or marginalized artists",
      "affected_populations": [
        "less popular artists",
        "marginalized artists"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzing algorithm bias using music dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.11067v2",
    "title": "Punishing defectors and rewarding cooperators: Do people discriminate between genders?",
    "year": 2020,
    "authors": [
      "Hélène Barcelo",
      "Valerio Capraro"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.GT",
      "q-bio.PE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates gender-based discrimination in social decision-making, addressing gender inequality. It examines whether individuals discriminate between men and women in cooperative contexts, which relates to social gender bias.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "men",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Multiple studies with controlled experiments",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.10704v1",
    "title": "Towards Neural Machine Translation for Edoid Languages",
    "year": 2020,
    "authors": [
      "Iroro Orife"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses linguistic marginalization and access to information for indigenous Nigerian language speakers, highlighting inequalities in communication, socio-economic participation, and empowerment.",
      "inequality_type": [
        "linguistic",
        "informational",
        "socioeconomic"
      ],
      "other_detail": "Language preservation and access disparities",
      "affected_populations": [
        "Edoid language speakers",
        "indigenous Nigerian communities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Developing translation models for marginalized languages",
      "geographic_focus": [
        "Nigeria"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.10354v6",
    "title": "Fairway: A Way to Build Fair ML Software",
    "year": 2020,
    "authors": [
      "Joymallya Chakraborty",
      "Suvodeep Majumder",
      "Zhe Yu",
      "Tim Menzies"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic bias related to social groups such as race and gender, addressing fairness issues in AI systems that impact social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "ethical"
      ],
      "other_detail": "Focus on bias mitigation in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Bias Mitigation"
      ],
      "methodology_detail": "Combines pre-processing and in-processing bias removal techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.08835v1",
    "title": "Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media",
    "year": 2020,
    "authors": [
      "Massimo Stella"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes perceptions of gender disparities in science via social media, addressing gender bias and stereotypes, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender gap perception in science",
      "affected_populations": [
        "women in science"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Text-mining and network analysis of social media data",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.08132v1",
    "title": "Gender Representation in Open Source Speech Resources",
    "year": 2020,
    "authors": [
      "Mahault Garnerin",
      "Solange Rossato",
      "Laurent Besacier"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender representation in speech resources, highlighting gender balance issues and transparency, which relate to gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender representation in speech corpora",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Analysis"
      ],
      "methodology_detail": "Analyzing gender data in open speech corpora",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.11515v1",
    "title": "Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings",
    "year": 2020,
    "authors": [
      "Haoran Zhang",
      "Amy X. Lu",
      "Mohamed Abdalla",
      "Matthew McDermott",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in clinical embeddings related to gender, ethnicity, insurance, and language, indicating a focus on social disparities and fairness issues in AI systems affecting marginalized groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Biases in clinical AI affecting marginalized populations",
      "affected_populations": [
        "gender groups",
        "ethnic minorities",
        "socioeconomic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias quantification and performance evaluation across groups",
      "geographic_focus": [
        "MIMIC-III hospital dataset (US)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.05330v3",
    "title": "Fairness by Explicability and Adversarial SHAP Learning",
    "year": 2020,
    "authors": [
      "James M. Hickey",
      "Pietro G. Di Stefano",
      "Vlasios Vasileiou"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing bias affecting unprivileged groups, which relates to social inequalities such as socioeconomic and racial disparities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Fairness in AI predictions and bias mitigation",
      "affected_populations": [
        "unprivileged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Using SHAP values and regularizations for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.05048v1",
    "title": "Auditing ML Models for Individual Bias and Unfairness",
    "year": 2020,
    "authors": [
      "Songkai Xue",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on auditing ML models for individual bias and unfairness, specifically revealing gender and racial biases in a criminal recidivism prediction tool, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on bias detection in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Formalizes auditing as an optimization problem",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.04794v1",
    "title": "Addressing multiple metrics of group fairness in data-driven decision making",
    "year": 2020,
    "authors": [
      "Marius Miron",
      "Songül Tolan",
      "Emilia Gómez",
      "Carlos Castillo"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness metrics related to protected groups like gender and race, addressing social bias in AI systems.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on discrimination measurement in machine learning",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Principal Component Analysis"
      ],
      "methodology_detail": "Analyzes fairness metrics clustering and visualization",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.03699v2",
    "title": "Removing Disparate Impact of Differentially Private Stochastic Gradient Descent on Model Accuracy",
    "year": 2020,
    "authors": [
      "Depeng Xu",
      "Wei Du",
      "Xintao Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in utility loss caused by differential privacy, affecting underrepresented groups, which relates to social inequality issues.",
      "inequality_type": [
        "social",
        "disparate impact"
      ],
      "other_detail": "Focuses on algorithmic fairness in privacy-preserving ML",
      "affected_populations": [
        "underrepresented groups",
        "complex classes"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes impact of privacy mechanisms on group utility",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.03151v2",
    "title": "Demographic Bias in Presentation Attack Detection of Iris Recognition Systems",
    "year": 2020,
    "authors": [
      "Meiling Fang",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic bias in biometric AI systems, highlighting gender disparities in protection, which relates to social discrimination and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in biometric security",
      "affected_populations": [
        "female users",
        "male users"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes bias using baseline algorithms and a database",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.02488v2",
    "title": "Demographic Bias in Biometrics: A Survey on an Emerging Challenge",
    "year": 2020,
    "authors": [
      "P. Drozdowski",
      "C. Rathgeb",
      "A. Dantcheva",
      "N. Damer",
      "C. Busch"
    ],
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses systemic bias and social discrimination in biometric AI systems, highlighting racial bias and societal impacts.",
      "inequality_type": [
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic bias and fairness issues",
      "affected_populations": [
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Survey of existing bias estimation and mitigation studies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.01380v2",
    "title": "The Magic Word: A Coding Tutorial-Game to Engage Female Teenagers in App Design",
    "year": 2020,
    "authors": [
      "Bernadette Spieler",
      "Naomi Pfaff",
      "Stefania Makrygiannaki",
      "Wolfgang Slany"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender disparities in educational engagement through AI-based game design, focusing on female teenagers' participation and preferences, which relates to gender inequality.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on female teenagers' engagement in tech education",
      "affected_populations": [
        "female teenagers"
      ],
      "methodology": [
        "Survey",
        "Focus Group",
        "Prototype Testing"
      ],
      "methodology_detail": "Involves early-stage user involvement and feedback",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.00683v1",
    "title": "Detection and Mitigation of Bias in Ted Talk Ratings",
    "year": 2020,
    "authors": [
      "Rupam Acharyya",
      "Shouman Das",
      "Ankani Chattoraj",
      "Oishani Sengupta",
      "Md Iftekar Tanveer"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines implicit bias related to race and gender in social platform ratings, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in social perception and AI fairness",
      "affected_populations": [
        "racial groups",
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes viewer ratings for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.11836v1",
    "title": "No computation without representation: Avoiding data and algorithm biases through diversity",
    "year": 2020,
    "authors": [
      "Caitlin Kuhlman",
      "Latifa Jackson",
      "Rumi Chunara"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI related to underrepresented groups and advocates for diversity to address inequities, indicating a focus on social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on diversity in computing fields and datasets",
      "affected_populations": [
        "underrepresented groups",
        "minorities",
        "women"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analyzes existing literature and practices in AI ethics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.11651v2",
    "title": "Fair Learning with Private Demographic Data",
    "year": 2020,
    "authors": [
      "Hussein Mozannar",
      "Mesrob I. Ohannessian",
      "Nathan Srebro"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and non-discrimination in AI, focusing on protected attributes like race. It aims to enable fair predictions despite privacy restrictions, directly relating to social bias issues.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Fairness in AI with privacy constraints",
      "affected_populations": [
        "racial minorities",
        "students"
      ],
      "methodology": [
        "Machine Learning",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Adapting fair learning algorithms for privatized data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.11645v1",
    "title": "Facebook Ads as a Demographic Tool to Measure the Urban-Rural Divide",
    "year": 2020,
    "authors": [
      "Daniele Rama",
      "Yelena Mejova",
      "Michele Tizzoni",
      "Kyriaki Kalimeri",
      "Ingmar Weber"
    ],
    "categories": [
      "cs.CY",
      "cs.SI",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines urban-rural divides in income and education, highlighting socioeconomic inequalities. It discusses disparities in access to data and insights into rural wellbeing, addressing geographic and socioeconomic inequalities.",
      "inequality_type": [
        "income",
        "educational",
        "geographic",
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Focuses on rural-urban socioeconomic disparities in Italy",
      "affected_populations": [
        "rural residents",
        "urban residents"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Using Facebook data and official census comparison",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.11442v3",
    "title": "DeBayes: a Bayesian Method for Debiasing Network Embeddings",
    "year": 2020,
    "authors": [
      "Maarten Buyl",
      "Tijl De Bie"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in network embeddings, focusing on reducing discrimination in AI systems, which relates to social inequality issues such as discrimination based on race, gender, or other sensitive traits.",
      "inequality_type": [
        "racial",
        "gender",
        "other"
      ],
      "other_detail": "Bias in network-based decision-making systems",
      "affected_populations": [
        "social groups",
        "vulnerable users"
      ],
      "methodology": [
        "Machine Learning",
        "Bayesian Methods",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Debiasing network embeddings using Bayesian priors",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.12143v1",
    "title": "Fairness-Aware Learning with Prejudice Free Representations",
    "year": 2020,
    "authors": [
      "Ramanujam Madhavan",
      "Mohit Wadhwa"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on discrimination related to sensitive attributes like race and gender, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Detecting and removing latent discriminatory features",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.10774v2",
    "title": "Counterfactual fairness: removing direct effects through regularization",
    "year": 2020,
    "authors": [
      "Pietro G. Di Stefano",
      "James M. Hickey",
      "Vlasios Vasileiou"
    ],
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on reducing bias against unprivileged groups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "causal fairness in algorithmic decision-making",
      "affected_populations": [
        "unprivileged groups",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Analysis",
        "Regularization"
      ],
      "methodology_detail": "causal regularization to remove direct effects",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.10361v2",
    "title": "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition",
    "year": 2020,
    "authors": [
      "Xiaolei Huang",
      "Linzi Xing",
      "Franck Dernoncourt",
      "Michael J. Paul"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases in hate speech detection, focusing on race, gender, age, and nationality, which are social inequality factors. It analyzes fairness and bias in AI models related to social groups. The work addresses social discrimination issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "nationality"
      ],
      "other_detail": "Demographic bias in AI fairness evaluation",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups",
        "nationalities"
      ],
      "methodology": [
        "Dataset Creation",
        "Empirical Analysis",
        "Machine Learning",
        "Natural Language Processing"
      ],
      "methodology_detail": "Multilingual Twitter corpus and bias evaluation",
      "geographic_focus": [
        "English-speaking countries",
        "Italy",
        "Poland",
        "Portugal",
        "Spain"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.09471v1",
    "title": "Learning Fairness-aware Relational Structures",
    "year": 2020,
    "authors": [
      "Yue Zhang",
      "Arti Ramesh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing bias and discrimination, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses social bias in AI systems",
      "affected_populations": [
        "discriminated groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness-aware structure learning algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.09343v3",
    "title": "Robust Optimization for Fairness with Noisy Protected Groups",
    "year": 2020,
    "authors": [
      "Serena Wang",
      "Wenshuo Guo",
      "Harikrishna Narasimhan",
      "Andrew Cotter",
      "Maya Gupta",
      "Michael I. Jordan"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning related to protected groups such as race and gender, which are key social categories linked to inequality. It focuses on ensuring fairness despite noisy group labels, directly engaging with social bias issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in protected group classification",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Robust Optimization",
        "Theoretical Analysis",
        "Case Study"
      ],
      "methodology_detail": "Develops robust algorithms with theoretical guarantees",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.07789v1",
    "title": "Elitism in Mathematics and Inequality",
    "year": 2020,
    "authors": [
      "Ho-Chun Herbert Chang",
      "Feng Fu"
    ],
    "categories": [
      "math.HO",
      "cs.SI",
      "physics.soc-ph"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes disparities in elite mathematician representation across social groups, including ethnicity and nationality, highlighting issues of under-representation and access.",
      "inequality_type": [
        "racial",
        "ethnic",
        "nationality",
        "educational"
      ],
      "other_detail": "Focus on elite access and international academic mobility",
      "affected_populations": [
        "under-represented ethnic groups",
        "minority communities"
      ],
      "methodology": [
        "Network Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes mathematician networks and advisor relationships",
      "geographic_focus": [
        "Japan",
        "Arabic regions",
        "Africa",
        "East Asia"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.07786v1",
    "title": "Investigating Potential Factors Associated with Gender Discrimination in Collaborative Recommender Systems",
    "year": 2020,
    "authors": [
      "Masoud Mansoury",
      "Himan Abdollahpouri",
      "Jessie Smith",
      "Arman Dehpanah",
      "Mykola Pechenizkiy",
      "Bamshad Mobasher"
    ],
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender disparities in recommendation accuracy, highlighting fairness issues. It analyzes potential factors contributing to discriminatory performance across genders. This directly relates to social inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes recommendation performance factors across genders",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.07147v1",
    "title": "Fair Prediction with Endogenous Behavior",
    "year": 2020,
    "authors": [
      "Christopher Jung",
      "Sampath Kannan",
      "Changhwa Lee",
      "Mallesh M. Pai",
      "Aaron Roth",
      "Rakesh Vohra"
    ],
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "econ.EM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in criminal justice, involving demographic groups and societal decision-making, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in legal and social outcomes",
      "affected_populations": [
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Analyzes societal and agent behaviors in crime and incarceration",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.07676v3",
    "title": "A Possibility in Algorithmic Fairness: Can Calibration and Equal Error Rates Be Reconciled?",
    "year": 2020,
    "authors": [
      "Claire Lazar Reich",
      "Suhas Vijaykumar"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in algorithmic decision-making affecting social groups, specifically in criminal justice and credit lending, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "economic",
        "educational"
      ],
      "other_detail": "Focus on fairness in algorithmic risk assessments",
      "affected_populations": [
        "criminal defendants",
        "credit applicants"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness criteria reconciliation and algorithm development",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.06483v4",
    "title": "Face Recognition: Too Bias, or Not Too Bias?",
    "year": 2020,
    "authors": [
      "Joseph P Robinson",
      "Gennady Livitz",
      "Yann Henon",
      "Can Qin",
      "Yun Fu",
      "Samson Timoner"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in facial recognition systems related to gender and ethnicity, highlighting social fairness issues and human perception biases.",
      "inequality_type": [
        "gender",
        "ethnic",
        "racial"
      ],
      "other_detail": "Focus on social bias in AI systems",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Human Evaluation"
      ],
      "methodology_detail": "Balanced dataset and threshold analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2003.00827v2",
    "title": "CheXclusion: Fairness gaps in deep chest X-ray classifiers",
    "year": 2020,
    "authors": [
      "Laleh Seyyed-Kalantari",
      "Guanxiong Liu",
      "Matthew McDermott",
      "Irene Y. Chen",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI classifiers related to protected attributes like race, gender, and socioeconomic status, highlighting disparities across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on healthcare disparities in medical AI",
      "affected_populations": [
        "patients by race",
        "patients by gender",
        "patients by age",
        "patients by insurance"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Assessing fairness disparities in classifier performance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.11621v1",
    "title": "Algorithms for Fair Team Formation in Online Labour Marketplaces",
    "year": 2020,
    "authors": [
      "Giorgio Barnabò",
      "Adriano Fazzone",
      "Stefano Leonardi",
      "Chris Schwiegelshohn"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in algorithms related to gender and class in online labor markets, directly engaging with social discrimination issues.",
      "inequality_type": [
        "gender",
        "class"
      ],
      "other_detail": "Focuses on algorithmic fairness across social groups",
      "affected_populations": [
        "men",
        "women",
        "workers from different classes"
      ],
      "methodology": [
        "Algorithm Development",
        "Experimental"
      ],
      "methodology_detail": "Designs and tests fairness algorithms with real data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.03592v3",
    "title": "Post-Comparison Mitigation of Demographic Bias in Face Recognition Using Fair Score Normalization",
    "year": 2020,
    "authors": [
      "Philipp Terhörst",
      "Jan Niklas Kolf",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic bias in face recognition, focusing on fairness across social groups such as gender. It aims to reduce discriminatory effects of AI systems, directly relating to social bias and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in biometric AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias reduction and performance evaluation in face recognition",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.03508v1",
    "title": "Fair Correlation Clustering",
    "year": 2020,
    "authors": [
      "Saba Ahmadi",
      "Sainyam Galhotra",
      "Barna Saha",
      "Roy Schwartz"
    ],
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints in clustering, focusing on equitable representation of different groups (e.g., gender) within clusters, which relates to social fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Fairness in clustering for social groups",
      "affected_populations": [
        "gender",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Approximation algorithms and empirical validation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.03276v1",
    "title": "Asymmetric Rejection Loss for Fairer Face Recognition",
    "year": 2020,
    "authors": [
      "Haoyu Qin"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition, highlighting racial disparities and under-representation of non-Caucasian groups, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "non-Caucasian groups"
      ],
      "methodology": [
        "Deep Learning",
        "Semi-supervised Learning",
        "Experiment"
      ],
      "methodology_detail": "Proposes a novel loss function to reduce racial bias",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.03256v1",
    "title": "Diversity and Inclusion Metrics in Subset Selection",
    "year": 2020,
    "authors": [
      "Margaret Mitchell",
      "Dylan Baker",
      "Nyalleng Moorosi",
      "Emily Denton",
      "Ben Hutchinson",
      "Alex Hanna",
      "Timnit Gebru",
      "Jamie Morgenstern"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses diversity, inclusion, social power, and access differentials, indicating a focus on social inequalities. It emphasizes creating outputs that account for social disparities and biases in AI systems. The mention of social concepts suggests addressing social inequality issues.",
      "inequality_type": [
        "social",
        "racial",
        "gender",
        "educational",
        "access"
      ],
      "other_detail": "Focus on social power and access disparities",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Experiment",
        "Social choice methods",
        "Metrics development"
      ],
      "methodology_detail": "Human subject experiments and social choice techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.01559v1",
    "title": "Whose Side are Ethics Codes On? Power, Responsibility and the Social Good",
    "year": 2020,
    "authors": [
      "Anne L. Washington",
      "Rachel S. Kuo"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in digital resource access and social impacts on marginalized groups, highlighting inequalities related to social vulnerability and digital exposure.",
      "inequality_type": [
        "digital",
        "social",
        "geographic"
      ],
      "other_detail": "Focus on digital differential vulnerability and marginalized communities",
      "affected_populations": [
        "marginalized communities",
        "vulnerable populations"
      ],
      "methodology": [
        "Critical Discourse Analysis",
        "Interviews",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes ethics codes and community perspectives",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.00695v1",
    "title": "FAE: A Fairness-Aware Ensemble Framework",
    "year": 2020,
    "authors": [
      "Vasileios Iosifidis",
      "Besnik Fetahu",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on discrimination against protected groups based on gender, race, etc., which relates to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on algorithmic fairness and discrimination mitigation",
      "affected_populations": [
        "protected groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Interventions"
      ],
      "methodology_detail": "Pre- and post-processing fairness techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2002.00065v1",
    "title": "Analysis of Gender Inequality In Face Recognition Accuracy",
    "year": 2020,
    "authors": [
      "Vítor Albiero",
      "Krishnapriya K. S.",
      "Kushal Vangara",
      "Kai Zhang",
      "Michael C. King",
      "Kevin W. Bowyer"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in face recognition accuracy, highlighting biases affecting women, which relates to social gender inequality and algorithmic fairness issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on gender bias in facial recognition systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes distribution shifts across datasets and image subsets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.11585v1",
    "title": "Housing Search in the Age of Big Data: Smarter Cities or the Same Old Blind Spots?",
    "year": 2020,
    "authors": [
      "Geoff Boeing",
      "Max Besbris",
      "Ariela Schachter",
      "John Kuk"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how online housing platforms reflect or mitigate information inequalities linked to neighborhood sociodemographics, highlighting disparities in access and quality of housing search information.",
      "inequality_type": [
        "informational",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on neighborhood sociodemographics and platform biases",
      "affected_populations": [
        "low-income residents",
        "minority communities"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of Craigslist rental listings data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.10994v1",
    "title": "Credit Scoring for Good: Enhancing Financial Inclusion with Smartphone-Based Microlending",
    "year": 2020,
    "authors": [
      "María Óskarsdóttir",
      "Cristián Bravo",
      "Carlos Sarraute",
      "Bart Baesens",
      "Jan Vanthienen"
    ],
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on financial inclusion, which relates to socioeconomic inequality, and discusses fairness in credit scoring models, addressing social disparities.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on unbanked populations and financial access",
      "affected_populations": [
        "unbanked individuals",
        "poorest adults"
      ],
      "methodology": [
        "Machine Learning",
        "Feature Engineering",
        "Network Analysis",
        "Representation Learning"
      ],
      "methodology_detail": "Enhancing credit scoring accuracy ethically",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.09233v1",
    "title": "Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions",
    "year": 2020,
    "authors": [
      "Kit T. Rodolfa",
      "Erika Salomon",
      "Lauren Haynes",
      "Ivan Higuera Mendieta",
      "Jamie Larson",
      "Rayid Ghani"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.1; K.4.2; K.5.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing recidivism disparities through social service interventions, aiming to address inequalities in criminal justice outcomes affecting underserved communities.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Targets disparities in criminal justice system outcomes",
      "affected_populations": [
        "under-served communities",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Predictive Fairness",
        "Case Study"
      ],
      "methodology_detail": "Incorporates fairness metrics into ML models for justice",
      "geographic_focus": [
        "Los Angeles"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.08855v1",
    "title": "Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack",
    "year": 2020,
    "authors": [
      "Bo Zhang",
      "Ruotong Yu",
      "Haipei Sun",
      "Yanying Li",
      "Jun Xu",
      "Hui Wang"
    ],
    "categories": [
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in vulnerability to membership inference attacks across demographic groups, addressing fairness and discrimination issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness disparities in AI privacy attacks",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates vulnerability disparity on real datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.08767v1",
    "title": "Interventions for Ranking in the Presence of Implicit Bias",
    "year": 2020,
    "authors": [
      "L. Elisa Celis",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DS",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses implicit bias related to race and gender, which are social inequalities, and discusses interventions to mitigate these biases in ranking systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on algorithmic bias mitigation strategies",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Analysis"
      ],
      "methodology_detail": "Includes mathematical modeling and real-world data evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.06615v2",
    "title": "The Risk to Population Health Equity Posed by Automated Decision Systems: A Narrative Review",
    "year": 2020,
    "authors": [
      "Mitchell Burger"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses how AI and ADS may perpetuate and amplify existing social, economic, and health disparities, impacting population health equity.",
      "inequality_type": [
        "economic",
        "health",
        "social",
        "disparities"
      ],
      "other_detail": "Focus on health equity and social disparities",
      "affected_populations": [
        "vulnerable groups",
        "marginalized populations"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Hermeneutic approach to explore current and future uses",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.09753v1",
    "title": "Race, Gender and Beauty: The Effect of Information Provision on Online Hiring Biases",
    "year": 2020,
    "authors": [
      "Weiwen Leung",
      "Zheng Zhang",
      "Daviti Jibuti",
      "Jinhao Zhao",
      "Maximillian Klein",
      "Casey Pierce",
      "Lionel Robert",
      "Haiyi Zhu"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines hiring biases related to race and gender, highlighting discrimination against Black workers and attractiveness, and preferences for certain groups. It analyzes social biases in online hiring processes, which are key aspects of social inequality. The study also discusses how UI design can influence discriminatory outcomes.",
      "inequality_type": [
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Focus on online hiring biases and discrimination",
      "affected_populations": [
        "Black workers",
        "female workers",
        "Asian workers"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulated hiring decisions via Amazon MTurk",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.02271v2",
    "title": "Revealing Neural Network Bias to Non-Experts Through Interactive Counterfactual Examples",
    "year": 2020,
    "authors": [
      "Chelsea M. Myers",
      "Evan Freed",
      "Luis Fernando Laris Pardo",
      "Anushay Furqan",
      "Sebastian Risi",
      "Jichen Zhu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on revealing biases in AI algorithms, specifically social biases like gender bias, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on social bias detection in AI systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "System Design",
        "Experiment"
      ],
      "methodology_detail": "Design and initial testing of an interactive bias visualization tool",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.01796v5",
    "title": "Fair Active Learning",
    "year": 2020,
    "authors": [
      "Hadis Anahideh",
      "Abolfazl Asudeh",
      "Saravanan Thirumuruganathan"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing discrimination and bias, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Fairness in machine learning models",
      "affected_populations": [
        "societal groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Fairness Metrics"
      ],
      "methodology_detail": "Designing fair active learning algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.01002v2",
    "title": "The extent and drivers of gender imbalance in neuroscience reference lists",
    "year": 2020,
    "authors": [
      "Jordan D. Dworkin",
      "Kristin A. Linn",
      "Erin G. Teich",
      "Perry Zurn",
      "Russell T. Shinohara",
      "Danielle S. Bassett"
    ],
    "categories": [
      "cs.SI",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in neuroscience citation practices, addressing gender inequality in academic recognition and career advancement.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female neuroscientists",
        "male neuroscientists"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Citation data and social network analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  }
]