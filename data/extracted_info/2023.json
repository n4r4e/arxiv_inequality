[
  {
    "id": "http://arxiv.org/abs/2401.00093v1",
    "title": "Fairness-Enhancing Vehicle Rebalancing in the Ride-hailing System",
    "year": 2023,
    "authors": [
      "Xiaotong Guo",
      "Hanyong Xu",
      "Dingyi Zhuang",
      "Yunhan Zheng",
      "Jinhua Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses equitable access to ride-hailing services, focusing on fairness across regions, which relates to social inequalities such as geographic and socioeconomic disparities.",
      "inequality_type": [
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on equitable service distribution",
      "affected_populations": [
        "underserved communities",
        "riders",
        "drivers"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Simulation"
      ],
      "methodology_detail": "Demand prediction and vehicle rebalancing strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.17443v1",
    "title": "Break Out of a Pigeonhole: A Unified Framework for Examining Miscalibration, Bias, and Stereotype in Recommender Systems",
    "year": 2023,
    "authors": [
      "Yongsu Ahn",
      "Yu-Ru Lin"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases, stereotypes, and disparate impacts on social groups in recommender systems, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "age",
        "minority groups",
        "discrimination"
      ],
      "other_detail": "Focuses on social biases in AI recommendation algorithms",
      "affected_populations": [
        "women",
        "older users",
        "minority groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design",
        "Experiment"
      ],
      "methodology_detail": "Measuring framework and mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.17167v1",
    "title": "The Gatekeeper Effect: The Implications of Pre-Screening, Self-selection, and Bias for Hiring Processes",
    "year": 2023,
    "authors": [
      "Moran Koren"
    ],
    "categories": [
      "econ.TH",
      "cs.GT",
      "91B06, 91A10, 91A40, 91A80",
      "K.4.4; J.1; I.2.11; J.4; G.3"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in gatekeeping affecting marginalized groups and suggests measures like affirmative action, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Biases in decision-making processes affecting social groups",
      "affected_populations": [
        "applicants",
        "marginalized groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias effects and screening strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.15994v1",
    "title": "Practical Bias Mitigation through Proxy Sensitive Attribute Label Generation",
    "year": 2023,
    "authors": [
      "Bhushan Chaudhary",
      "Anubha Pandey",
      "Deepak Bhatt",
      "Darshika Tiwari"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI systems related to sensitive attributes, which are tied to social groups such as race and gender, aiming to improve fairness and reduce social discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in AI systems affecting social groups",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Clustering",
        "Unsupervised Learning"
      ],
      "methodology_detail": "Proxy label generation for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.15478v1",
    "title": "A Group Fairness Lens for Large Language Models",
    "year": 2023,
    "authors": [
      "Guanqun Bi",
      "Lei Shen",
      "Yuqiang Xie",
      "Yanan Cao",
      "Tiangang Zhu",
      "Xiaodong He"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases and fairness in large language models, addressing social discrimination and fairness issues affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focuses on social bias in AI systems",
      "affected_populations": [
        "social groups",
        "users of LLMs"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Constructs dataset and evaluates bias mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.15398v1",
    "title": "Fairness-Aware Structured Pruning in Transformers",
    "year": 2023,
    "authors": [
      "Abdelrahman Zayed",
      "Goncalo Mordido",
      "Samira Shabanian",
      "Ioana Baldini",
      "Sarath Chandar"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing social bias and discrimination issues related to gender and racial groups in language models.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on bias reduction in language models",
      "affected_populations": [
        "women",
        "Black people",
        "LGBTQ+",
        "Jewish communities"
      ],
      "methodology": [
        "Experiment",
        "Fairness Analysis",
        "Model Pruning"
      ],
      "methodology_detail": "Pruning attention heads to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.15307v1",
    "title": "Mitigating Algorithmic Bias on Facial Expression Recognition",
    "year": 2023,
    "authors": [
      "Glauco Amigo",
      "Pablo Rivas Perea",
      "Robert J. Marks"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in facial expression recognition, focusing on fairness across different groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI fairness",
      "affected_populations": [
        "minority groups",
        "diverse facial expression groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Debiasing variational autoencoder applied to facial data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.14804v2",
    "title": "Using large language models to promote health equity",
    "year": 2023,
    "authors": [
      "Emma Pierson",
      "Divya Shanmugam",
      "Rajiv Movva",
      "Jon Kleinberg",
      "Monica Agrawal",
      "Mark Dredze",
      "Kadija Ferryman",
      "Judy Wawira Gichoya",
      "Dan Jurafsky",
      "Pang Wei Koh",
      "Karen Levy",
      "Sendhil Mullainathan",
      "Ziad Obermeyer",
      "Harini Suresh",
      "Keyon Vafa"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses social biases, fairness, and equitable impacts of LLMs, focusing on promoting health equity and reducing societal discrimination.",
      "inequality_type": [
        "health",
        "educational",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on health equity and societal disparities",
      "affected_populations": [
        "underserved groups",
        "discriminated communities"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis",
        "System Design"
      ],
      "methodology_detail": "Proposing research directions and application strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.06162v1",
    "title": "A debiasing technique for place-based algorithmic patrol management",
    "year": 2023,
    "authors": [
      "Alexander Einarsson",
      "Simen Oestmo",
      "Lester Wollman",
      "Duncan Purves",
      "Ryan Jenkins"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in algorithmic patrol management, a social inequality issue.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias removal and accuracy assessment techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.14369v2",
    "title": "Quality-Diversity Generative Sampling for Learning with Synthetic Data",
    "year": 2023,
    "authors": [
      "Allen Chang",
      "Matthew C. Fontaine",
      "Serena Booth",
      "Maja J. MatariÄ",
      "Stefanos Nikolaidis"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on debiasing synthetic data to improve fairness in classifiers, addressing social bias and fairness issues related to race, skin tone, and age. It aims to reduce bias in AI systems impacting social groups.",
      "inequality_type": [
        "racial",
        "age",
        "health"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI datasets",
      "affected_populations": [
        "racial minorities",
        "age groups",
        "skin tone groups"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Synthetic data generation and classifier training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.14976v1",
    "title": "Gaussian Harmony: Attaining Fairness in Diffusion-based Face Generation Models",
    "year": 2023,
    "authors": [
      "Basudha Pal",
      "Arunkumar Kannan",
      "Ram Prabhakar Kathirvel",
      "Alice J. O'Toole",
      "Rama Chellappa"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face generation models related to sensitive attributes like race, gender, and age, which are social categories linked to inequality. It focuses on fairness in AI outputs, impacting social discrimination. The methodology aims to mitigate bias, a core aspect of social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Bias mitigation in AI-generated facial data",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using Gaussian mixture models for bias localization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.13173v1",
    "title": "Learning Fair Policies for Multi-stage Selection Problems from Observational Data",
    "year": 2023,
    "authors": [
      "Zhuangzhuang Jia",
      "Grani A. Hanasusanto",
      "Phebe Vayanos",
      "Weijun Xie"
    ],
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in selection policies affecting high-stakes decisions, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in AI decision-making processes",
      "affected_populations": [
        "job applicants",
        "loan applicants",
        "defendants"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Inference",
        "Optimization"
      ],
      "methodology_detail": "Linear rules, sample average approximation, mixed binary conic optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.12727v1",
    "title": "Black Content Creators' Responses and Resistance Strategies on TikTok",
    "year": 2023,
    "authors": [
      "Gianna Williams"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines systemic bias and discrimination against Black Content Creators on TikTok, addressing racial inequality and algorithmic bias.",
      "inequality_type": [
        "racial",
        "digital"
      ],
      "other_detail": "focus on algorithmic bias against Black creators",
      "affected_populations": [
        "Black Content Creators",
        "Brown Content Creators"
      ],
      "methodology": [
        "Content Analysis"
      ],
      "methodology_detail": "Assessing algorithmic interactions and harassment",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.12620v2",
    "title": "\"It Can Relate to Real Lives\": Attitudes and Expectations in Justice-Centered Data Structures & Algorithms for Non-Majors",
    "year": 2023,
    "authors": [
      "Anna Batra",
      "Iris Zhou",
      "Suh Young Choi",
      "Chongjiu Gao",
      "Yanbing Xiao",
      "Sonia Fereidooni",
      "Kevin Lin"
    ],
    "categories": [
      "cs.CY",
      "K.3.2"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines student experiences across gender and racial identities, highlighting disparities in confidence and belonging, and discusses social justice in computing education.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on student attitudes and identity in education",
      "affected_populations": [
        "women",
        "non-binary",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study",
        "Survey"
      ],
      "methodology_detail": "Analysis of student survey data over two quarters",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.12560v1",
    "title": "Comprehensive Validation on Reweighting Samples for Bias Mitigation via AIF360",
    "year": 2023,
    "authors": [
      "Christina Hastings Blow",
      "Lijun Qian",
      "Camille Gibson",
      "Pamela Obiomon",
      "Xishuang Dong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias mitigation related to sensitive attributes like gender and race, addressing fairness issues in AI systems that impact social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI fairness",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluation of fairness metrics on datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.11969v1",
    "title": "GroupMixNorm Layer for Learning Fair Models",
    "year": 2023,
    "authors": [
      "Anubha Pandey",
      "Aditi Rai",
      "Maneet Singh",
      "Deepak Bhatt",
      "Tanmoy Bhowmik"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in AI, focusing on protected attributes like gender and ethnicity, which relate to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on fairness in AI systems",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Metrics Evaluation",
        "Algorithm Development"
      ],
      "methodology_detail": "Proposes a new fairness layer for models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.11803v2",
    "title": "NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs",
    "year": 2023,
    "authors": [
      "Maria Antoniak",
      "Aakanksha Naik",
      "Carla S. Alvarado",
      "Lucy Lu Wang",
      "Irene Y. Chen"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses systemic health disparities, historical injustices, and social context in maternal healthcare, highlighting inequalities affecting marginalized groups.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on maternal health disparities and social context",
      "affected_populations": [
        "birthing people",
        "marginalized groups"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study",
        "Case Study",
        "Ethics Analysis"
      ],
      "methodology_detail": "Engaged healthcare workers and birthing people for insights",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.11699v1",
    "title": "Dissecting Bias of ChatGPT in College Major Recommendations",
    "year": 2023,
    "authors": [
      "Alex Zheng"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities and biases in AI recommendations related to race, gender, and socioeconomic status, indicating a focus on social inequalities and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Bias in AI recommendations across social groups",
      "affected_populations": [
        "students",
        "minority groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias metrics and prompt-based evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.10833v4",
    "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
    "year": 2023,
    "authors": [
      "Ehsan Latif",
      "Xiaoming Zhai",
      "Lei Liu"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases and disparities in AI scoring systems, addressing social gender inequality and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias and disparities in AI",
      "affected_populations": [
        "male students",
        "female students"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias analysis techniques applied to AI scoring outcomes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.10694v1",
    "title": "Discretionary Trees: Understanding Street-Level Bureaucracy via Machine Learning",
    "year": 2023,
    "authors": [
      "Gaurab Pokharel",
      "Sanmay Das",
      "Patrick J. Fowler"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discretionary decisions by street-level bureaucrats affecting vulnerable households, highlighting potential biases and inequities in resource allocation.",
      "inequality_type": [
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on resource allocation and decision-making biases",
      "affected_populations": [
        "homeless households",
        "vulnerable groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predicting and analyzing bureaucratic decision patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2401.05377v2",
    "title": "The impact of generative artificial intelligence on socioeconomic inequalities and policy making",
    "year": 2023,
    "authors": [
      "Valerio Capraro",
      "Austin Lentsch",
      "Daron Acemoglu",
      "Selin Akgun",
      "Aisel Akhmedova",
      "Ennio Bilancini",
      "Jean-FranĂ§ois Bonnefon",
      "Pablo BraĂąas-Garza",
      "Luigi Butera",
      "Karen M. Douglas",
      "Jim A. C. Everett",
      "Gerd Gigerenzer",
      "Christine Greenhow",
      "Daniel A. Hashimoto",
      "Julianne Holt-Lunstad",
      "Jolanda Jetten",
      "Simon Johnson",
      "Chiara Longoni",
      "Pete Lunn",
      "Simone Natale",
      "Iyad Rahwan",
      "Neil Selwyn",
      "Vivek Singh",
      "Siddharth Suri",
      "Jennifer Sutcliffe",
      "Joe Tomlinson",
      "Sander van der Linden",
      "Paul A. M. Van Lange",
      "Friederike Wall",
      "Jay J. Van Bavel",
      "Riccardo Viale"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses socioeconomic inequalities across domains like work, education, and healthcare, and examines how generative AI impacts these disparities.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "health",
        "digital"
      ],
      "other_detail": "Focuses on socioeconomic and informational inequalities",
      "affected_populations": [
        "workers",
        "students",
        "patients",
        "general public"
      ],
      "methodology": [
        "Literature Review",
        "Policy Analysis",
        "Interdisciplinary Overview"
      ],
      "methodology_detail": "Evaluates existing research and policy frameworks",
      "geographic_focus": [
        "European Union",
        "United States",
        "United Kingdom"
      ],
      "ai_relationship": "AI as amplifier and mitigator of inequalities",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.10181v2",
    "title": "Integrating Fairness and Model Pruning Through Bi-level Optimization",
    "year": 2023,
    "authors": [
      "Yucong Dai",
      "Gen Li",
      "Feng Luo",
      "Xiaolong Ma",
      "Yongkai Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on reducing algorithmic bias, which impacts social groups. It discusses social justice concerns related to unequal prediction outcomes. The emphasis on fairness criteria aligns with social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social justice"
      ],
      "other_detail": "Focus on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Bi-level Optimization",
        "Experiment"
      ],
      "methodology_detail": "Joint fairness-constrained model pruning framework",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.09626v1",
    "title": "Exploring Gender Disparities in Bumble's Match Recommendations",
    "year": 2023,
    "authors": [
      "Ritvik Aryan Kalra",
      "Pratham Gupta",
      "Ben Varghese",
      "Nimmi Rangaswamy"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic bias and discrimination related to gender in a dating platform, addressing social inequality issues. It discusses bias reproduction and moral obligations of platform creators, indicating a focus on social disparities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in dating algorithms",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "AI fairness analysis"
      ],
      "methodology_detail": "Testing bias in matching algorithms",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.07492v4",
    "title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models",
    "year": 2023,
    "authors": [
      "Manish Nagireddy",
      "Lamogha Chiazor",
      "Moninder Singh",
      "Ioana Baldini"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias and stigma amplification in AI, related to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on social bias and stigma in language models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Manual Evaluation"
      ],
      "methodology_detail": "Constructing bias benchmark and analyzing model outputs",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.07420v1",
    "title": "FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs",
    "year": 2023,
    "authors": [
      "Swanand Ravindra Kadhe",
      "Anisa Halimi",
      "Ambrish Rawat",
      "Nathalie Baracaldo"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on bias mitigation, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social"
      ],
      "other_detail": "Focus on fairness and bias in language models",
      "affected_populations": [
        "minority groups",
        "social groups"
      ],
      "methodology": [
        "Post-processing Bias Mitigation",
        "Empirical Evaluation",
        "Ensemble Methods"
      ],
      "methodology_detail": "Adapts fairness techniques for model ensembles",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.06979v1",
    "title": "On the notion of Hallucinations from the lens of Bias and Validity in Synthetic CXR Images",
    "year": 2023,
    "authors": [
      "Gauri Bhardwaj",
      "Yuvaraj Govindarajulu",
      "Sundaraparipurnan Narayanan",
      "Pavan Kulkarni",
      "Manojkumar Parmar"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and fairness issues in synthetic medical images, highlighting disparities among social groups, especially race and gender, affecting diagnostic accuracy and image quality.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on bias and fairness in medical AI",
      "affected_populations": [
        "Female Hispanic",
        "medical patients"
      ],
      "methodology": [
        "Experiment",
        "Bias analysis",
        "Classifier evaluation"
      ],
      "methodology_detail": "Assessing bias, validity, and hallucinations in synthetic images",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.06927v1",
    "title": "WE economy: Potential of mutual aid distribution based on moral responsibility and risk vulnerability",
    "year": 2023,
    "authors": [
      "Takeshi Kato"
    ],
    "categories": [
      "econ.TH",
      "cs.MA",
      "physics.soc-ph",
      "91B43, 91B70, 62P20",
      "J.4; K.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing wealth inequality and societal solidarity, addressing economic disparities.",
      "inequality_type": [
        "wealth",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on societal wealth distribution and moral responsibility",
      "affected_populations": [
        "general society"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": " econophysical modeling and simulation",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.10083v1",
    "title": "The Limits of Fair Medical Imaging AI In The Wild",
    "year": 2023,
    "authors": [
      "Yuzhe Yang",
      "Haoran Zhang",
      "Judy W Gichoya",
      "Dina Katabi",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and demographic biases in medical AI, addressing disparities across populations.",
      "inequality_type": [
        "health",
        "racial",
        "demographic"
      ],
      "other_detail": "Focus on fairness across different populations and sites",
      "affected_populations": [
        "patients",
        "medical subpopulations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes demographic encoding and fairness gaps",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.06306v1",
    "title": "Attribute Annotation and Bias Evaluation in Visual Datasets for Autonomous Driving",
    "year": 2023,
    "authors": [
      "David FernĂĄndez Llorca",
      "Pedro Frau",
      "Ignacio Parra",
      "RubĂŠn Izquierdo",
      "Emilia GĂłmez"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in datasets related to protected attributes like age, sex, and skin tone, highlighting underrepresentation of certain social groups, which relates to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "disability"
      ],
      "other_detail": "Bias and fairness in dataset representation",
      "affected_populations": [
        "children",
        "wheelchair users",
        "mobility vehicle users"
      ],
      "methodology": [
        "Attribute Annotation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Annotating protected attributes and analyzing their distribution",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.10080v2",
    "title": "No prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation",
    "year": 2023,
    "authors": [
      "Nimesh Agrawal",
      "Anuj Kumar Sirohi",
      "Jayadeva",
      "Sandeep Kumar"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in recommendation systems affecting demographic groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Fairness in AI systems across social groups",
      "affected_populations": [
        "demographic groups",
        "users by gender",
        "users by age"
      ],
      "methodology": [
        "Machine Learning",
        "Graph Neural Networks",
        "Differential Privacy",
        "Experimental Evaluation"
      ],
      "methodology_detail": "Combines GNN, fairness constraints, and privacy techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.05662v2",
    "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
    "year": 2023,
    "authors": [
      "Gustavo GonĂ§alves",
      "Emma Strubell"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases in language models, which relate to social discrimination and inequality issues such as race, gender, and social bias. It focuses on how AI systems reflect and potentially perpetuate societal inequalities. The study's concern with social bias and fairness indicates it addresses social inequality aspects.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on social bias in language models",
      "affected_populations": [
        "minority groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing bias measures before and after compression",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.05429v3",
    "title": "Mitigating Nonlinear Algorithmic Bias in Binary Classification",
    "year": 2023,
    "authors": [
      "Wendy Hui",
      "Wai Kwong Lau"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias related to age, a social group characteristic, and discusses fairness in AI, which relates to social inequality issues.",
      "inequality_type": [
        "age"
      ],
      "other_detail": "focus on age bias in classification",
      "affected_populations": [
        "young people"
      ],
      "methodology": [
        "Causal Modeling",
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "causal models for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.04832v2",
    "title": "Exposing Algorithmic Discrimination and Its Consequences in Modern Society: Insights from a Scoping Study",
    "year": 2023,
    "authors": [
      "Ramandeep Singh Dehal",
      "Mehak Sharma",
      "Ronnie de Souza Santos"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic discrimination based on attributes like race, gender, ethnicity, age, and disability, which are key social inequality factors. It focuses on social bias and fairness issues in AI systems, analyzing their impact on different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "age"
      ],
      "other_detail": "Focuses on social bias in AI systems",
      "affected_populations": [
        "ethnic minorities",
        "women",
        "disabled",
        "elderly"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Review of studies on algorithmic discrimination",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.04809v1",
    "title": "Navigating the Path of Women in Software Engineering: From Academia to Industry",
    "year": 2023,
    "authors": [
      "Tatalina Oliveira",
      "Ann Barcomb",
      "Ronnie de Souza Santos",
      "Helda Barros",
      "Maria Teresa Baldassarre",
      "CĂŠsar FranĂ§a"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in software engineering, highlighting systemic challenges faced by women, including bias, harassment, and work-life imbalance, which are social inequalities.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender disparities in tech careers",
      "affected_populations": [
        "women in software engineering"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Cross-sectional survey of women's experiences",
      "geographic_focus": [
        "Brazil"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.03843v1",
    "title": "Exposing Disparities in Flood Adaptation for Equitable Future Interventions",
    "year": 2023,
    "authors": [
      "Lidia Cano Pecharroman",
      "ChangHoon Hahn"
    ],
    "categories": [
      "econ.GN",
      "cs.LG",
      "q-fin.EC",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in flood adaptation benefits across socioeconomic and racial groups, highlighting inequities in savings and support.",
      "inequality_type": [
        "income",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial and economic disparities in flood policies",
      "affected_populations": [
        "low-income communities",
        "white communities",
        "non-white communities"
      ],
      "methodology": [
        "Causal Inference",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses deep generative models for causal effect estimation",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.03689v1",
    "title": "Evaluating and Mitigating Discrimination in Language Model Decisions",
    "year": 2023,
    "authors": [
      "Alex Tamkin",
      "Amanda Askell",
      "Liane Lovitt",
      "Esin Durmus",
      "Nicholas Joseph",
      "Shauna Kravec",
      "Karina Nguyen",
      "Jared Kaplan",
      "Deep Ganguli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates discrimination in language models across societal decision scenarios, addressing social bias and fairness issues affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability",
        "educational"
      ],
      "other_detail": "Focus on discrimination in societal decision-making",
      "affected_populations": [
        "minority groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Prompt engineering and systematic bias evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.10065v1",
    "title": "Exploring Social Bias in Downstream Applications of Text-to-Image Foundation Models",
    "year": 2023,
    "authors": [
      "Adhithya Prakash Saravanan",
      "Rafal Kocielnik",
      "Roy Jiang",
      "Pengrui Han",
      "Anima Anandkumar"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "F.2.2; I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases related to gender and race in AI models, which are forms of social inequality. It analyzes how these biases impact downstream applications, highlighting issues of discrimination and fairness. The focus on social bias in AI systems directly relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "focus on social bias and discrimination in AI",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Synthetic Image Generation",
        "Analysis"
      ],
      "methodology_detail": "Using synthetic images to probe biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.03027v2",
    "title": "Stable Diffusion Exposed: Gender Bias from Prompt to Image",
    "year": 2023,
    "authors": [
      "Yankun Wu",
      "Yuta Nakashima",
      "Noa Garcia"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI-generated images, highlighting social stereotypes and disparities related to gender representation.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in image generation",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Evaluation protocol for bias analysis in image generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.01509v1",
    "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
    "year": 2023,
    "authors": [
      "Vithya Yogarajan",
      "Gillian Dobbie",
      "Te Taka Keegan",
      "Rostam J. Neuwirth"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in language models affecting under-represented societies, highlighting social fairness issues.",
      "inequality_type": [
        "social",
        "racial",
        "geographic"
      ],
      "other_detail": "Focus on societal bias and under-represented populations",
      "affected_populations": [
        "indigenous groups",
        "under-represented societies"
      ],
      "methodology": [
        "Literature Review",
        "Survey"
      ],
      "methodology_detail": "Synthesizes current bias mitigation techniques",
      "geographic_focus": [
        "New Zealand"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.01261v2",
    "title": "TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models",
    "year": 2023,
    "authors": [
      "Aditya Chinchure",
      "Pushkar Shukla",
      "Gaurav Bhatt",
      "Kiri Salij",
      "Kartik Hosanagar",
      "Leonid Sigal",
      "Matthew Turk"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases in AI models related to societal attributes like gender and ethnicity, which are key aspects of social inequality. It examines how these biases affect image generation, highlighting issues of social discrimination and fairness. The analysis of biases and their intersectionality directly relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "ethnic",
        "racial"
      ],
      "other_detail": "Biases in AI models affecting social groups",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Semantic Explanation"
      ],
      "methodology_detail": "Bias measurement and semantic bias explanation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.00765v1",
    "title": "Explaining Knock-on Effects of Bias Mitigation",
    "year": 2023,
    "authors": [
      "Svetoslav Nizhnichenkov",
      "Rahul Nair",
      "Elizabeth Daly",
      "Brian Mac Namee"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias mitigation in AI systems, addressing fairness across social groups, and impacts on marginalized populations.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in machine learning outcomes",
      "affected_populations": [
        "privileged groups",
        "unprivileged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using a meta-classifier to identify impacted cohorts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.00610v3",
    "title": "Exploring Gender and Racial/Ethnic Bias Against Video Game Streamers: Comparing Perceived Gameplay Skill and Viewer Engagement",
    "year": 2023,
    "authors": [
      "David V. Nguyen",
      "Edward F. Melcer",
      "Deanne Adams"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates biases related to gender and race/ethnicity in perceptions of video game streamers, addressing social discrimination and bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias perception in digital entertainment contexts",
      "affected_populations": [
        "female streamers",
        "racial/ethnic minorities"
      ],
      "methodology": [
        "Experiment",
        "Survey",
        "Statistical Analysis"
      ],
      "methodology_detail": "Between-subjects survey experiment with rating comparisons",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.00554v1",
    "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
    "year": 2023,
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, race, religion, and nationality in legal AI outputs, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "religious",
        "nationality"
      ],
      "other_detail": "Biases in legal AI systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups",
        "nationalities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Bias Analysis"
      ],
      "methodology_detail": "Analyzing biases in AI-generated legal summaries",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.00434v1",
    "title": "PEFTDebias : Capturing debiasing information using PEFTs",
    "year": 2023,
    "authors": [
      "Sumit Agarwal",
      "Aditya Srikanth Veerubhotla",
      "Srijan Bansal"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating biases related to gender and race in foundation models, addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "PEFT for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.18812v1",
    "title": "What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations",
    "year": 2023,
    "authors": [
      "Raphael Tang",
      "Xinyu Zhang",
      "Jimmy Lin",
      "Ferhan Ture"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to ethnicity, nationality, religion, and politics in language models, which are social categories linked to inequality and discrimination.",
      "inequality_type": [
        "ethnic",
        "nationality",
        "religion",
        "political"
      ],
      "other_detail": "Biases in AI representations of social groups",
      "affected_populations": [
        "ethnic groups",
        "religious communities",
        "political groups",
        "nationalities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Probing contextual embeddings and bias transfer tests",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.00825v2",
    "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
    "year": 2023,
    "authors": [
      "Phillip Howard",
      "Avinash Madasu",
      "Tiep Le",
      "Gustavo Lujan Moreno",
      "Anahita Bhiwandiwalla",
      "Vasudev Lal"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper probes intersectional social biases related to race, gender, and physical traits in AI models, addressing social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "physical"
      ],
      "other_detail": "Focuses on intersectional social biases in AI models",
      "affected_populations": [
        "racial minorities",
        "women",
        "physically diverse individuals"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Using diffusion models to generate counterfactual image-text pairs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.18140v1",
    "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
    "year": 2023,
    "authors": [
      "David Esiobu",
      "Xiaoqing Tan",
      "Saghar Hosseini",
      "Megan Ung",
      "Yuchen Zhang",
      "Jude Fernandes",
      "Jane Dwivedi-Yu",
      "Eleonora Presani",
      "Adina Williams",
      "Eric Michael Smith"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring and mitigating social biases in LLMs across demographic groups, addressing fairness and discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on social bias and fairness in AI systems",
      "affected_populations": [
        "marginalized groups",
        "demographic minorities"
      ],
      "methodology": [
        "Benchmarking",
        "Dataset Creation",
        "Analysis"
      ],
      "methodology_detail": "Comparative bias metrics and mitigation techniques evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.17695v2",
    "title": "Fair Text-to-Image Diffusion via Fair Mapping",
    "year": 2023,
    "authors": [
      "Jia Li",
      "Lijie Hu",
      "Jingfeng Zhang",
      "Tianhang Zheng",
      "Hua Zhang",
      "Di Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI image generation related to demographic fairness, which impacts social groups based on race, gender, and other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focuses on demographic fairness in AI-generated images",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Debiasing diffusion models for fair image generation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.17627v1",
    "title": "Invisible Women in Digital Diplomacy: A Multidimensional Framework for Online Gender Bias Against Women Ambassadors Worldwide",
    "year": 2023,
    "authors": [
      "Yevgeniy Golovchenko",
      "Karolina StaĹczak",
      "Rebecca Adler-Nissen",
      "Patrice Wangen",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias and invisibility of women diplomats on social media, addressing gender-based social discrimination and inequality in digital diplomacy.",
      "inequality_type": [
        "gender",
        "digital"
      ],
      "other_detail": "Focus on online gender bias against women diplomats",
      "affected_populations": [
        "women diplomats"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Automated content and sentiment analysis of social media data",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.17373v1",
    "title": "The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation",
    "year": 2023,
    "authors": [
      "Yuchang Zhu",
      "Jintang Li",
      "Liang Chen",
      "Zibin Zheng"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI, focusing on demographic bias and discrimination, which are related to social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Fairness in AI systems without demographic data",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Knowledge Distillation"
      ],
      "methodology_detail": "Fairness-aware GNN training without demographic info",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.17252v1",
    "title": "Analyzing the Impact of Tax Credits on Households in Simulated Economic Systems with Learning Agents",
    "year": 2023,
    "authors": [
      "Jialin Dong",
      "Kshama Dwarakanath",
      "Svitlana Vyetrenko"
    ],
    "categories": [
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing household disparities through tax credits, addressing economic inequality.",
      "inequality_type": [
        "economic",
        "income",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focuses on household economic disparities and social welfare",
      "affected_populations": [
        "households"
      ],
      "methodology": [
        "Simulation",
        "Reinforcement Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Multi-agent simulation with learning agents",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.16362v1",
    "title": "Reducing Gender Bias in Machine Translation through Counterfactual Data Generation",
    "year": 2023,
    "authors": [
      "Ranjita Naik",
      "Spencer Rarrick",
      "Vishal Chowdhary"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a form of social bias affecting gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Addressing gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "Counterfactual data generation and domain adaptation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.15804v1",
    "title": "Voice Anonymization for All -- Bias Evaluation of the Voice Privacy Challenge Baseline System",
    "year": 2023,
    "authors": [
      "Anna Leschanowsky",
      "Ănal Ege Gaznepoglu",
      "Nils Peters"
    ],
    "categories": [
      "eess.AS",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and performance disparities across social subgroups based on sex and dialect, addressing fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "linguistic"
      ],
      "other_detail": "Bias in voice anonymization systems across social groups",
      "affected_populations": [
        "voice users",
        "subgroups by sex",
        "subgroups by dialect"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Assessing subgroup bias in anonymization performance",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.15108v2",
    "title": "Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision",
    "year": 2023,
    "authors": [
      "Nicholas Lui",
      "Bryan Chia",
      "William Berrios",
      "Candace Ross",
      "Douwe Kiela"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on racial biases in computer vision models and proposes methods to evaluate fairness across racial groups, directly addressing racial inequality and bias in AI systems.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Uses diffusion models to assess racial fairness in vision AI",
      "affected_populations": [
        "marginalized racial groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Generating and editing images to evaluate model bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.16180v1",
    "title": "Aiming to Minimize Alcohol-Impaired Road Fatalities: Utilizing Fairness-Aware and Domain Knowledge-Infused Artificial Intelligence",
    "year": 2023,
    "authors": [
      "Tejas Venkateswaran",
      "Sheikh Rabiul Islam",
      "Md Golam Moula Mehedi Hasan",
      "Mohiuddin Ahmed"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial profiling, demographic disparities, and equitable resource allocation in DUI enforcement, highlighting social biases and fairness issues.",
      "inequality_type": [
        "racial",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "racial minorities",
        "geographic regions",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness-Aware AI",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Incorporates domain knowledge and fairness constraints",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.14788v1",
    "title": "Evaluating Large Language Models through Gender and Racial Stereotypes",
    "year": 2023,
    "authors": [
      "Ananya Malik"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates biases related to gender and race in language models, which are social categories linked to inequality. It discusses biases in AI systems affecting social groups, indicating a focus on social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias evaluation in AI models",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias assessment framework and comparative analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.14126v1",
    "title": "Towards Auditing Large Language Models: Improving Text-based Stereotype Detection",
    "year": 2023,
    "authors": [
      "Wu Zekun",
      "Sahan Bulathwela",
      "Adriano Soares Koshiyama"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting and reducing stereotypes related to gender, race, religion, and profession, which are social biases contributing to inequality. It addresses societal biases in AI outputs, impacting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "religion",
        "social bias"
      ],
      "other_detail": "Focuses on stereotypes in language models",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "religious communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development",
        "AI auditing"
      ],
      "methodology_detail": "Develops dataset and classifier for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.13765v1",
    "title": "Learning Optimal and Fair Policies for Online Allocation of Scarce Societal Resources from Data Collected in Deployment",
    "year": 2023,
    "authors": [
      "Bill Tang",
      "ĂaÄÄąl KoĂ§yiÄit",
      "Eric Rice",
      "Phebe Vayanos"
    ],
    "categories": [
      "math.OC",
      "cs.LG",
      "physics.soc-ph"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on allocating scarce societal resources to marginalized groups, aiming to improve outcomes for populations experiencing homelessness, which relates to social inequality. It also discusses fairness constraints, indicating concern with equitable treatment across social groups.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "health"
      ],
      "other_detail": "Addresses resource allocation fairness and social disparities",
      "affected_populations": [
        "homeless individuals",
        "people experiencing homelessness"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Data-driven policy optimization and fairness constraints",
      "geographic_focus": [
        "Los Angeles"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.12684v1",
    "title": "Adversarial Reweighting Guided by Wasserstein Distance for Bias Mitigation",
    "year": 2023,
    "authors": [
      "Xuan Zhao",
      "Simone Fabbrizzi",
      "Paula Reyero Lobo",
      "Siamak Ghodsi",
      "Klaus Broelemann",
      "Steffen Staab",
      "Gjergji Kasneci"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI models caused by under-representation of minority groups, aiming to mitigate discrimination and unfair treatment, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on representation bias in data",
      "affected_populations": [
        "minority groups",
        "underrepresented populations"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Reweighting",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Bias mitigation via Wasserstein distance-based reweighting",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.14733v1",
    "title": "Thinking Outside the Box: Orthogonal Approach to Equalizing Protected Attributes",
    "year": 2023,
    "authors": [
      "Jiahui Liu",
      "Xiaohao Cai",
      "Mahesan Niranjan"
    ],
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to protected attributes like gender and ethnicity in clinical AI, aiming to mitigate disparities and improve fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in clinical decision-making",
      "affected_populations": [
        "patients",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Discriminant Dimensionality Reduction",
        "Orthogonalization"
      ],
      "methodology_detail": "Suppresses confounder effects to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.12435v4",
    "title": "Fair Enough? A map of the current limitations of the requirements to have fair algorithms",
    "year": 2023,
    "authors": [
      "Daniele Regoli",
      "Alessandro Castelnovo",
      "Nicole Inverardi",
      "Gabriele Nanino",
      "Ilaria Penco"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases and disparities in AI systems, highlighting societal demands for fairness and the social implications of algorithmic decisions.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disparities"
      ],
      "other_detail": "Focuses on fairness and bias in automated decision-making",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing societal and ethical aspects of fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social biases",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.10395v2",
    "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
    "year": 2023,
    "authors": [
      "Yi Yang",
      "Hanyu Duan",
      "Ahmed Abbasi",
      "John P. Lalor",
      "Kar Yan Tam"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in language models, which are social categories linked to inequality. It analyzes internal model biases that reflect societal stereotypes, addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social stereotypes in AI models",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes attention heads in transformer models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.09730v2",
    "title": "Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs",
    "year": 2023,
    "authors": [
      "Huaman Sun",
      "Jiaxin Pei",
      "Minje Choi",
      "David Jurgens"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases in LLMs related to race and gender, highlighting social bias issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in subjective judgment tasks",
      "affected_populations": [
        "White participants",
        "Asian participants",
        "Black participants",
        "Women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Evaluating LLMs on demographic-based subjective tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.09443v1",
    "title": "Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset",
    "year": 2023,
    "authors": [
      "Brooklyn Sheppard",
      "Anna Richter",
      "Allison Cohen",
      "Elizabeth Allyn Smith",
      "Tamara Kneese",
      "Carolyne Pelletier",
      "Ioana Baldini",
      "Yue Dong"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting and mitigating misogyny, a gender-based social bias, in NLP datasets, addressing gender inequality and discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Expert Annotation"
      ],
      "methodology_detail": "Collaborative annotation capturing subtle misogyny expressions",
      "geographic_focus": [
        "North America"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.09090v4",
    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
    "year": 2023,
    "authors": [
      "Marta Marchiori Manerba",
      "Karolina StaĹczak",
      "Riccardo Guidotti",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to race, religion, gender, and disabilities in language models, highlighting disparities faced by these groups.",
      "inequality_type": [
        "racial",
        "religion",
        "gender",
        "disability"
      ],
      "other_detail": "Biases mirror real-life social adversities faced by groups",
      "affected_populations": [
        "women",
        "people with disabilities",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Benchmarking",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias probing and benchmark creation for language models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.08836v2",
    "title": "Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English",
    "year": 2023,
    "authors": [
      "Spencer Rarrick",
      "Ranjita Naik",
      "Sundar Poudel",
      "Vishal Chowdhary"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social fairness issue affecting gender representation.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language translation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Creating benchmark dataset and evaluating translation methods",
      "geographic_focus": [
        "Turkey",
        "Hungary",
        "Finland",
        "Persia"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.08391v1",
    "title": "A Material Lens on Coloniality in NLP",
    "year": 2023,
    "authors": [
      "William Held",
      "Camille Harris",
      "Michael Best",
      "Diyi Yang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses coloniality's pervasive effects and inequality across societal and scientific domains, focusing on how NLP amplifies colonial harms and inequalities.",
      "inequality_type": [
        "racial",
        "geographic",
        "educational"
      ],
      "other_detail": "Focuses on colonial boundaries and global inequalities",
      "affected_populations": [
        "colonized regions",
        "marginalized groups"
      ],
      "methodology": [
        "Actor-Network Theory",
        "Quantitative Survey"
      ],
      "methodology_detail": "Network analysis of NLP research geography",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.07458v1",
    "title": "Trust in Queer Human-Robot Interaction",
    "year": 2023,
    "authors": [
      "Raj Korpan"
    ],
    "categories": [
      "cs.RO",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination and trust issues faced by queer individuals in HRI, highlighting social biases and inequality impacts related to gender and sexual identity.",
      "inequality_type": [
        "gender",
        "sexuality"
      ],
      "other_detail": "Focuses on social discrimination in AI and robotics",
      "affected_populations": [
        "queer people",
        "LGBTQIA+ community"
      ],
      "methodology": [
        "Qualitative Study",
        "Literature Review"
      ],
      "methodology_detail": "Proposes approaches for inclusive HRI research",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.07054v2",
    "title": "A Study of Implicit Ranking Unfairness in Large Language Models",
    "year": 2023,
    "authors": [
      "Chen Xu",
      "Wenjie Wang",
      "Yuxin Li",
      "Liang Pang",
      "Jun Xu",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates discriminatory ranking behaviors based on user attributes, including implicit biases related to non-sensitive profiles, highlighting fairness issues in AI systems affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability",
        "educational"
      ],
      "other_detail": "Focuses on fairness and discrimination in AI ranking systems",
      "affected_populations": [
        "users with biased profiles"
      ],
      "methodology": [
        "Evaluation Method",
        "Fair-aware Data Augmentation",
        "Experiment"
      ],
      "methodology_detail": "Assessing and mitigating implicit ranking unfairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2312.03710v1",
    "title": "Don't Overlook the Grammatical Gender: Bias Evaluation for Hindi-English Machine Translation",
    "year": 2023,
    "authors": [
      "Pushpdeep Singh"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in machine translation, addressing social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on grammatical gender cues in source language",
      "affected_populations": [
        "Hindi speakers",
        "English speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Creating gender-specific sentence sets for bias evaluation",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.06513v2",
    "title": "Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems",
    "year": 2023,
    "authors": [
      "Hsuan Su",
      "Rebecca Qian",
      "Chinnadhurai Sankar",
      "Shahin Shayandeh",
      "Shang-Tse Chen",
      "Hung-yi Lee",
      "Daniel M. Bikel"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias attribution in AI systems across demographic axes like gender, age, and race, addressing fairness issues related to social discrimination.",
      "inequality_type": [
        "gender",
        "age",
        "racial"
      ],
      "other_detail": "Bias attribution in AI systems",
      "affected_populations": [
        "women",
        "elderly",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias attribution analysis in dialogue systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.07604v2",
    "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
    "year": 2023,
    "authors": [
      "Xudong Shen",
      "Chao Du",
      "Tianyu Pang",
      "Min Lin",
      "Yongkang Wong",
      "Mohan Kankanhalli"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models related to gender, race, and age, which are social identity factors, and discusses fairness interventions to reduce social discrimination.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias mitigation in AI-generated images",
      "affected_populations": [
        "minority groups",
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Finetuning diffusion models for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.14703v1",
    "title": "ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management",
    "year": 2023,
    "authors": [
      "Angela Zhang",
      "Mert Yuksekgonul",
      "Joshua Guild",
      "James Zou",
      "Joseph C. Wu"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and racial biases in AI affecting clinical management, highlighting disparities impacting marginalized groups. It discusses how biases in LLMs influence healthcare outcomes for specific social groups, addressing social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Bias mitigation in AI clinical decision-making",
      "affected_populations": [
        "female patients",
        "African American patients",
        "Hispanic patients"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluating biases and mitigation strategies in ChatGPT responses",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.05746v1",
    "title": "Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models",
    "year": 2023,
    "authors": [
      "Joan Nwatu",
      "Oana Ignat",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates AI performance disparities across income groups, highlighting socioeconomic inequality impacts.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on income-related performance disparities in AI models",
      "affected_populations": [
        "low-income households",
        "wealthier households"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating model performance on geo-diverse income data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.05451v1",
    "title": "All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation",
    "year": 2023,
    "authors": [
      "Pragyan Banerjee",
      "Abhinav Java",
      "Surgan Jandial",
      "Simra Shahid",
      "Shaz Furniturewala",
      "Balaji Krishnamurthy",
      "Sumit Bhatia"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in language models, focusing on equitable treatment across demographics, which relates to social discrimination and bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "educational",
        "disability"
      ],
      "other_detail": "Focuses on fairness and bias in AI outputs",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Fairness framework for language model inference",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.05436v4",
    "title": "Fair Wasserstein Coresets",
    "year": 2023,
    "authors": [
      "Zikai Xiong",
      "Niccolò Dalmasso",
      "Shubham Sharma",
      "Freddy Lecue",
      "Daniele Magazzeni",
      "Vamsi K. Potluru",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing biases and demographic parity, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "social groups",
        "demographic subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness optimization via Wasserstein distance minimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.04892v2",
    "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
    "year": 2023,
    "authors": [
      "Shashank Gupta",
      "Vaishnavi Shrivastava",
      "Ameet Deshpande",
      "Ashwin Kalyan",
      "Peter Clark",
      "Ashish Sabharwal",
      "Tushar Khot"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to socio-demographic groups in LLMs, highlighting racial and social stereotypes affecting different populations. It examines how AI systems reflect and potentially reinforce social inequalities. The focus on biases against specific social groups indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI reflecting social group stereotypes",
      "affected_populations": [
        "racial groups",
        "socio-demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing biases across multiple datasets and personas",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.04559v1",
    "title": "Individual and gender inequality in computer science: A career study of cohorts from 1970 to 2000",
    "year": 2023,
    "authors": [
      "Haiko Lietz",
      "Mohsen Jadidi",
      "Daniel Kostic",
      "Milena Tsvetkova",
      "Claudia Wagner"
    ],
    "categories": [
      "cs.CY",
      "K.4.3; K.7.0; J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and individual inequalities in academic careers, focusing on disparities between women and men in productivity and impact, which are social inequality issues.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender disparities in computer science careers",
      "affected_populations": [
        "women",
        "male scholars"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Large-scale bibliographic data analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.03900v1",
    "title": "Why Fair Automated Hiring Systems Breach EU Non-Discrimination Law",
    "year": 2023,
    "authors": [
      "Robert Lee Poe"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic discrimination in hiring, affecting social groups based on protected attributes, thus addressing social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability"
      ],
      "other_detail": "Focus on discrimination in employment via AI systems",
      "affected_populations": [
        "job applicants",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Legal Analysis"
      ],
      "methodology_detail": "Legal compliance and algorithm fairness evaluation",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.03767v1",
    "title": "Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation",
    "year": 2023,
    "authors": [
      "Pushpdeep Singh"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social bias issue affecting gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Using gender-specific sentences for bias evaluation",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.03532v1",
    "title": "The Fairness Stitch: Unveiling the Potential of Model Stitching in Neural Network De-Biasing",
    "year": 2023,
    "authors": [
      "Modar Sulaiman",
      "Kallol Roy"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing biases affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Model stitching with fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.03425v1",
    "title": "An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets",
    "year": 2023,
    "authors": [
      "Faris F. Gulamali",
      "Ashwin S. Sawant",
      "Lora Liharska",
      "Carol R. Horowitz",
      "Lili Chan",
      "Patricia H. Kovatch",
      "Ira Hofer",
      "Karandeep Singh",
      "Lynne D. Richardson",
      "Emmanuel Mensah",
      "Alexander W Charney",
      "David L. Reich",
      "Jianying Hu",
      "Girish N. Nadkarni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in healthcare datasets, highlighting social disparities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial bias in medical AI datasets",
      "affected_populations": [
        "disadvantaged groups",
        "racial minorities"
      ],
      "methodology": [
        "Data Analysis",
        "Systematic Evaluation"
      ],
      "methodology_detail": "Using AEquity metric to assess bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.03186v1",
    "title": "Model-based Counterfactual Generator for Gender Bias Mitigation",
    "year": 2023,
    "authors": [
      "Ewoenam Kwaku Tokpo",
      "Toon Calders"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on mitigating gender bias in language models, addressing social discrimination related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Model Development",
        "Empirical Evaluation"
      ],
      "methodology_detail": "counterfactual data augmentation and model training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.02445v2",
    "title": "Pre-Construction of Opinion Dynamics Considering Structural Inequality: Interdisciplinary Analysis of Complex Social Stratification, Media Influence, and Functionalism",
    "year": 2023,
    "authors": [
      "Yasuko Kawahata"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses mechanisms maintaining inequality in social classes, focusing on social stratification, media influence, and societal structures, which relate to social inequality issues.",
      "inequality_type": [
        "class",
        "educational",
        "media",
        "social"
      ],
      "other_detail": "Focus on societal structures and media influence",
      "affected_populations": [
        "social classes",
        "educational groups",
        "media consumers"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes social theories and interdisciplinary perspectives",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.01838v1",
    "title": "When fairness is an abstraction: Equity and AI in Swedish compulsory education",
    "year": 2023,
    "authors": [
      "Marie Utterberg Modén",
      "Marisa Ponti",
      "Johan Lundin",
      "Martin Tallvid"
    ],
    "categories": [
      "cs.CY",
      "K.3; K.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in education, social groups, and AI impacts, addressing social inequalities.",
      "inequality_type": [
        "educational",
        "social",
        "inequality"
      ],
      "other_detail": "Focus on fairness and social justice in education",
      "affected_populations": [
        "students",
        "educators",
        "policy groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Content Analysis"
      ],
      "methodology_detail": "Analysis of policy documents and reports",
      "geographic_focus": [
        "Sweden"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.01591v3",
    "title": "Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs",
    "year": 2023,
    "authors": [
      "Debolina Halder Lina",
      "Arlei Silva"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models, focusing on protected attributes, which relate to social groups and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI decision-making processes",
      "affected_populations": [
        "disadvantaged communities",
        "minority groups"
      ],
      "methodology": [
        "Adversarial Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware imputation and adversarial training",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.01573v1",
    "title": "Improving Fairness using Vision-Language Driven Image Augmentation",
    "year": 2023,
    "authors": [
      "Moreno D'Incà",
      "Christos Tzelepis",
      "Ioannis Patras",
      "Nicu Sebe"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in facial recognition, focusing on biases related to protected characteristics like age and skin color, which are social attributes linked to inequality.",
      "inequality_type": [
        "racial",
        "age",
        "health"
      ],
      "other_detail": "Bias mitigation in facial attribute recognition",
      "affected_populations": [
        "racial minorities",
        "elderly",
        "skin color groups"
      ],
      "methodology": [
        "Deep Learning",
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Image augmentation and fairness metrics evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.01349v2",
    "title": "Post-hoc Orthogonalization for Mitigation of Protected Feature Bias in CXR Embeddings",
    "year": 2023,
    "authors": [
      "Tobias Weber",
      "Michael Ingrisch",
      "Bernd Bischl",
      "David Rügamer"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses protected features like race, age, and sex, and aims to mitigate biases related to these attributes in medical AI models, which are directly linked to social inequalities.",
      "inequality_type": [
        "racial",
        "age",
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in medical imaging AI",
      "affected_populations": [
        "patients",
        "racial groups",
        "age groups",
        "gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Orthogonalization technique to remove protected feature influence",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.00638v1",
    "title": "FAIRLABEL: Correcting Bias in Labels",
    "year": 2023,
    "authors": [
      "Srinivasan H Sengamedu",
      "Hien Pham"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.6"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in labels that reflect societal discrimination, aiming to reduce disparities across groups.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "gender"
      ],
      "other_detail": "Focuses on bias correction in societal datasets",
      "affected_populations": [
        "racial groups",
        "socioeconomic classes",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and correction algorithms evaluated",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.00306v1",
    "title": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation",
    "year": 2023,
    "authors": [
      "Xiangjue Dong",
      "Yibo Wang",
      "Philip S. Yu",
      "James Caverlee"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in large language models, addressing social discrimination and fairness issues related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on implicit and explicit gender bias in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Probing LLMs with generated inputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.20508v1",
    "title": "Parametric Fairness with Statistical Guarantees",
    "year": 2023,
    "authors": [
      "François HU",
      "Philipp Ratz",
      "Arthur Charpentier"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in algorithms, focusing on biases affecting social groups, particularly in wages, which relates to socioeconomic inequality.",
      "inequality_type": [
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on wage disparities and fairness metrics",
      "affected_populations": [
        "workers",
        "income groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Extends fairness metrics with distributional properties",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.20061v1",
    "title": "Evaluation Framework for Understanding Sensitive Attribute Association Bias in Latent Factor Recommendation Algorithms",
    "year": 2023,
    "authors": [
      "Lex Beattie",
      "Isabel Corpus",
      "Lucy H. Lin",
      "Praveen Ravichandran"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to sensitive attributes like gender, which can reinforce stereotypes and cause harm to social groups. It focuses on how recommendation systems entangle sensitive attributes, impacting stakeholder representation. This aligns with addressing social bias and inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in recommendation systems",
      "affected_populations": [
        "users",
        "stakeholders"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Bias evaluation methods in recommendation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.19986v1",
    "title": "Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models",
    "year": 2023,
    "authors": [
      "Preetam Prabhu Srikar Dammu",
      "Yunhe Feng",
      "Chirag Shah"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI models affecting vulnerable groups, specifically gender and racial minorities, aiming to reduce disparities and improve fairness.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in image classification models",
      "affected_populations": [
        "women",
        "people of color"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Using web search and generative models to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.10744v1",
    "title": "Advancing a Model of Students' Intentional Persistence in Machine Learning and Artificial Intelligence",
    "year": 2023,
    "authors": [
      "Sharon Ferguson",
      "Katherine Mao",
      "James Magarian",
      "Alison Olechowski"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities and social factors influencing persistence in ML/AI, highlighting gender and minority considerations, and discusses diversity promotion in the field.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on diversity and persistence in ML/AI education",
      "affected_populations": [
        "women",
        "minority students"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Survey of students in ML/AI courses",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.19691v1",
    "title": "Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness",
    "year": 2023,
    "authors": [
      "Jacy Reese Anthis",
      "Victor Veitch"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on social groups like race and gender, and discusses how algorithmic fairness relates to social discrimination and bias.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on fairness standards in AI systems",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Causal Modeling"
      ],
      "methodology_detail": "Using causal graphs to connect fairness concepts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.19130v2",
    "title": "Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender",
    "year": 2023,
    "authors": [
      "Ahmed Sabir",
      "Lluís Padró"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in image captioning systems, highlighting social gender bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI systems",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Measuring bias with a proposed gender score",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.18877v1",
    "title": "Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition",
    "year": 2023,
    "authors": [
      "Isaac Slaughter",
      "Craig Greenberg",
      "Reva Schwartz",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race, gender, age, disability, and nationality in speech models, which are forms of social inequality. It analyzes how these biases reflect and propagate human-like social disparities through AI systems. The study links model biases to real-world social group disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "age",
        "nationality"
      ],
      "other_detail": "Bias propagation in speech emotion recognition",
      "affected_populations": [
        "abled people",
        "disabled people",
        "European-Americans",
        "African-Americans",
        "females",
        "males",
        "U.S. accented speakers",
        "non-U.S. accented speakers",
        "younger people",
        "older people"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and correlation analysis in speech models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.18724v1",
    "title": "WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit Courts",
    "year": 2023,
    "authors": [
      "Elliott Ash",
      "Naman Goel",
      "Nianyun Li",
      "Claudia Marangon",
      "Peiyao Sun"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on criminal justice data related to race, gender, and socioeconomic factors, and discusses fairness in AI systems used in this context.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "gender"
      ],
      "other_detail": "Focus on systemic and algorithmic fairness issues",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "criminal defendants"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Curating and analyzing large criminal justice dataset",
      "geographic_focus": [
        "Wisconsin, USA"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.18458v2",
    "title": "Do Not Harm Protected Groups in Debiasing Language Representation Models",
    "year": 2023,
    "authors": [
      "Chloe Qinyu Zhu",
      "Rickard Stureborg",
      "Brandon Fain"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in language models affecting demographic groups, highlighting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on fairness in AI impacting protected social groups",
      "affected_populations": [
        "demographic groups",
        "protected groups"
      ],
      "methodology": [
        "Experiment",
        "Evaluation"
      ],
      "methodology_detail": "Assessing debiasing techniques on text classification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.18430v3",
    "title": "MCRAGE: Synthetic Healthcare Data for Fairness",
    "year": 2023,
    "authors": [
      "Keira Behal",
      "Jiayi Chen",
      "Caleb Fikes",
      "Sophia Xiao"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in healthcare outcomes due to imbalanced data across minority groups, focusing on racial, ethnic, and health inequalities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "health"
      ],
      "other_detail": "Addressing data imbalance in healthcare disparities",
      "affected_populations": [
        "minority groups",
        "patients of underrepresented classes"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Dataset Creation"
      ],
      "methodology_detail": "Synthetic data augmentation for imbalanced datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.17533v2",
    "title": "Decoding The Digital Fuku: Deciphering Colonial Legacies to Critically Assess ChatGPT in Dominican Education",
    "year": 2023,
    "authors": [
      "Anaelia Ovalle"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines educational disparities rooted in historical and colonial contexts, and discusses AI fairness and digital coloniality, which relate to social inequalities.",
      "inequality_type": [
        "educational",
        "digital",
        "colonial"
      ],
      "other_detail": "Focus on digital colonialism and educational disparities",
      "affected_populations": [
        "Dominican students",
        "Dominican educators"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Historical review and critical examination of AI impacts",
      "geographic_focus": [
        "Dominican Republic"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.17530v1",
    "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models",
    "year": 2023,
    "authors": [
      "Laura Cabello",
      "Emanuele Bugliarello",
      "Stephanie Brandl",
      "Desmond Elliott"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, addressing gender fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias quantification and fairness evaluation in models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.17489v1",
    "title": "Bias in Evaluation Processes: An Optimization-Based Model",
    "year": 2023,
    "authors": [
      "L. Elisa Celis",
      "Amit Kumar",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in evaluation processes affecting social groups, which relate to social inequality issues such as discrimination and fairness.",
      "inequality_type": [
        "educational",
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on evaluation bias and social group disparities",
      "affected_populations": [
        "students",
        "job applicants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Optimization-based modeling of evaluation biases",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.17428v1",
    "title": "''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text",
    "year": 2023,
    "authors": [
      "Rishav Hada",
      "Agrima Seth",
      "Harshita Diddee",
      "Kalika Bali"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, addressing societal gender inequalities and biases in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Creating and analyzing bias ratings in GPT-generated text",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.18368v2",
    "title": "Muslim-Violence Bias Persists in Debiased GPT Models",
    "year": 2023,
    "authors": [
      "Babak Hemmatian",
      "Razan Baltaji",
      "Lav R. Varshney"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models related to religion, revealing social discrimination and stereotypes, which impact social perceptions and fairness.",
      "inequality_type": [
        "religion",
        "social bias"
      ],
      "other_detail": "Focuses on religious bias and stereotypes in AI",
      "affected_populations": [
        "Muslims"
      ],
      "methodology": [
        "Experiment",
        "Content analysis"
      ],
      "methodology_detail": "Replication and bias analysis in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.16136v2",
    "title": "Analyzing Disparity and Temporal Progression of Internet Quality through Crowdsourced Measurements with Bias-Correction",
    "year": 2023,
    "authors": [
      "Hyeongseong Lee",
      "Udit Paul",
      "Arpit Gupta",
      "Elizabeth Belding",
      "Mengyang Gu"
    ],
    "categories": [
      "stat.AP",
      "cs.NI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes how internet performance varies across regions with different demographic profiles, highlighting socioeconomic and ethnic disparities. It examines sampling bias related to income, education, age, and ethnicity, indicating an intersection with social inequalities. The focus on demographic factors and regional disparities demonstrates a direct engagement with social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "ethnic",
        "geographic",
        "educational",
        "age"
      ],
      "other_detail": "Focus on regional and demographic disparities in internet quality",
      "affected_populations": [
        "low-income groups",
        "ethnic minorities",
        "younger populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias correction and regression analysis",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.16132v1",
    "title": "Diversity in Software Engineering Conferences and Journals",
    "year": 2023,
    "authors": [
      "Aditya Shankar Narayanan",
      "Dheeraj Vagavolu",
      "Nancy A Day",
      "Meiyappan Nagappan"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines ethnic, gender, and geographical diversity, highlighting biases and barriers faced by underrepresented groups in academia, which are key aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "geographic"
      ],
      "other_detail": "Focuses on representation disparities in academic publishing",
      "affected_populations": [
        "ethnic minorities",
        "women",
        "geographical minorities"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of publication and committee data over years",
      "geographic_focus": [
        "Africa",
        "South America",
        "Oceania"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.15847v1",
    "title": "A Novel Method for Analysing Racial Bias: Collection of Person Level References",
    "year": 2023,
    "authors": [
      "Muhammed Yusuf Kocyigit",
      "Anietie Andy",
      "Derry Wijaya"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes racial representation and bias in literature over time, highlighting disparities and toxic contexts for African Americans versus White Americans, directly addressing racial inequality.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial representation and implicit bias detection",
      "affected_populations": [
        "African Americans",
        "White Americans"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes historical text data and semantic spaces",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.18355v1",
    "title": "Health Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model",
    "year": 2023,
    "authors": [
      "Yohn Jairo Parra Bautista",
      "Vinicious Lima",
      "Carlos Theran",
      "Richard Alo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses health disparities affecting marginalized groups and examines AI's role in addressing or potentially perpetuating these inequalities.",
      "inequality_type": [
        "health",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on health disparities and AI's impact",
      "affected_populations": [
        "racial minorities",
        "low-income groups",
        "rural residents"
      ],
      "methodology": [
        "Natural Language Processing",
        "Cosine similarity analysis",
        "Comparative study"
      ],
      "methodology_detail": "Analyzed text queries using domain-specific and multi-purpose LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.15097v2",
    "title": "A Canonical Data Transformation for Achieving Inter- and Within-group Fairness",
    "year": 2023,
    "authors": [
      "Zachary McBride Lazri",
      "Ivan Brugere",
      "Xin Tian",
      "Dana Dachman-Soled",
      "Antigoni Polychroniadou",
      "Danial Dervovic",
      "Min Wu"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on demographic groups, which relates to social inequality issues such as racial and demographic fairness.",
      "inequality_type": [
        "racial",
        "demographic",
        "social"
      ],
      "other_detail": "Focuses on fairness across social groups in AI",
      "affected_populations": [
        "demographic groups",
        "individuals within groups"
      ],
      "methodology": [
        "Pre-processing framework",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Mapping feature vectors to a canonical domain",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.15055v1",
    "title": "Towards Conceptualization of \"Fair Explanation\": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators",
    "year": 2023,
    "authors": [
      "Tin Nguyen",
      "Jiannan Xu",
      "Aayushi Roy",
      "Hal Daumé III",
      "Marine Carpuat"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in the impact of AI explanations on content moderators, focusing on anti-Asian hate speech and differential psychological effects, addressing racial bias and fairness issues.",
      "inequality_type": [
        "racial",
        "fairness"
      ],
      "other_detail": "Focuses on racial impact in AI content moderation",
      "affected_populations": [
        "Asian moderators",
        "non-Asian moderators"
      ],
      "methodology": [
        "Experiment",
        "Psychological Impact Assessment"
      ],
      "methodology_detail": "Evaluates psychological effects across user groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.14607v2",
    "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
    "year": 2023,
    "authors": [
      "Yanchen Liu",
      "Srishti Gautam",
      "Jiaqi Ma",
      "Himabindu Lakkaraju"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases and fairness issues in LLMs, reflecting societal inequalities such as stereotypes and discrimination. It analyzes how biases inherited from training data impact different social groups in classification tasks. The focus on social biases and fairness directly relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "discrimination"
      ],
      "other_detail": "Bias inheritance from training data affects social groups",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.14329v1",
    "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
    "year": 2023,
    "authors": [
      "Mahdi Zakizadeh",
      "Kaveh Eskandari Miandoab",
      "Mohammad Taher Pilehvar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, a form of social discrimination, and evaluates how debiasing affects gender knowledge, directly relating to gender inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias and knowledge in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Develops a benchmark dataset and evaluates models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.13820v2",
    "title": "FERI: A Multitask-based Fairness Achieving Algorithm with Applications to Fair Organ Transplantation",
    "year": 2023,
    "authors": [
      "Can Li",
      "Dejian Lai",
      "Xiaoqian Jiang",
      "Kai Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare, focusing on gender and age disparities in liver transplantation outcomes, highlighting social discrimination issues.",
      "inequality_type": [
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Fairness in medical outcome prediction",
      "affected_populations": [
        "women",
        "elderly",
        "transplant patients"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Multitask learning with fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.12560v3",
    "title": "Fast Model Debias with Machine Unlearning",
    "year": 2023,
    "authors": [
      "Ruizhe Chen",
      "Jianfei Yang",
      "Huimin Xiong",
      "Jianhong Bai",
      "Tianxiang Hu",
      "Jin Hao",
      "Yang Feng",
      "Joey Tianyi Zhou",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models that perpetuate social inequalities such as gender and racial biases, impacting fairness and social justice.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias identification and removal techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.12421v2",
    "title": "Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling",
    "year": 2023,
    "authors": [
      "Wendy Hui",
      "Wai Kwong Lau"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in algorithmic decision-making, a social inequality issue. It focuses on detecting and mitigating gender bias in binary classification models. The use of causal modeling to reduce bias directly relates to social fairness concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focused on gender bias in AI models",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Causal Modeling",
        "Statistical Analysis",
        "Machine Learning"
      ],
      "methodology_detail": "Using causal models to mitigate bias",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.12350v3",
    "title": "Equipping Federated Graph Neural Networks with Structure-aware Group Fairness",
    "year": 2023,
    "authors": [
      "Nan Cui",
      "Xiuling Wang",
      "Wendy Hui Wang",
      "Violet Chen",
      "Yue Ning"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models, which relate to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in federated GNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.12127v2",
    "title": "A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation",
    "year": 2023,
    "authors": [
      "Giuseppe Attanasio",
      "Flor Miriam Plaza-del-Arco",
      "Debora Nozza",
      "Anne Lauscher"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in machine translation, addressing gender inequality and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Interpretability Methods",
        "Experiment"
      ],
      "methodology_detail": "Bias metrics and interpretability techniques applied",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.12076v2",
    "title": "Towards Exploring Fairness in Visual Transformer based Natural and GAN Image Detection Systems",
    "year": 2023,
    "authors": [
      "Manjary P. Gangan",
      "Anoop Kadan",
      "Lajish V L"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, racial, and intersectional domains in AI models, addressing social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "intersectional"
      ],
      "other_detail": "Bias in AI image forensics affecting social groups",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias evaluation across multiple social domains",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.11778v3",
    "title": "Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale",
    "year": 2023,
    "authors": [
      "Qichao Wang",
      "Tian Bian",
      "Yian Yin",
      "Tingyang Xu",
      "Hong Cheng",
      "Helen M. Meng",
      "Zibin Zheng",
      "Liang Chen",
      "Bingzhe Wu"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates stereotypes related to social groups such as gender, race, and religion in AI models, highlighting societal biases and ethical concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "religion"
      ],
      "other_detail": "Focuses on societal stereotypes in AI-generated content",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Detecting stereotypes in AI models using new architecture",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.11407v2",
    "title": "Group-blind optimal transport to group parity and its constrained variants",
    "year": 2023,
    "authors": [
      "Quan Zhou",
      "Jakub Marecek"
    ],
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning related to protected attributes like gender and race, aiming to reduce group disparities without individual attribute access.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on demographic group parity in AI fairness",
      "affected_populations": [
        "unprivileged groups",
        "privileged groups"
      ],
      "methodology": [
        "Fairness Algorithm",
        "Distribution Alignment"
      ],
      "methodology_detail": "Group-blind optimal transport approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.11079v1",
    "title": "Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models",
    "year": 2023,
    "authors": [
      "Hsuan Su",
      "Cheng-Chu Cheng",
      "Hua Farn",
      "Shachi H Kumar",
      "Saurav Sahay",
      "Shang-Tse Chen",
      "Hung-yi Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in large language models, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Test case generation and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.10571v1",
    "title": "Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems",
    "year": 2023,
    "authors": [
      "Sagi Shaier",
      "Kevin Bennett",
      "Lawrence Hunter",
      "Katharina von der Wense"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in biomedical QA systems related to patient demographics, highlighting potential unjustified disparities affecting health treatment.",
      "inequality_type": [
        "health",
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in biomedical AI systems affecting patient fairness",
      "affected_populations": [
        "patients",
        "medical groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing answer changes with demographic variations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.09373v1",
    "title": "Identifying and examining machine learning biases on Adult dataset",
    "year": 2023,
    "authors": [
      "Sahil Girhepuje"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in wage prediction, highlighting social gender disparities and fairness issues in AI models.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender bias in wage prediction models",
      "affected_populations": [
        "males",
        "females"
      ],
      "methodology": [
        "Machine Learning",
        "Ensemble Learning",
        "Bias Assessment"
      ],
      "methodology_detail": "Bias reduction and fairness evaluation in models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.09237v1",
    "title": "Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT's Perceptions of Indigenous Roles in Diverse Scenarios",
    "year": 2023,
    "authors": [
      "Cecilia Delgado Solorzano",
      "Carlos Toxtli Hernandez"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates societal biases related to indigeneity in AI perceptions, reflecting social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focuses on societal perceptions and biases about indigenous peoples",
      "affected_populations": [
        "Indigenous communities"
      ],
      "methodology": [
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Scenario generation and bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.09219v5",
    "title": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters",
    "year": 2023,
    "authors": [
      "Yixin Wan",
      "George Pu",
      "Jiao Sun",
      "Aparna Garimella",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI-generated recommendation letters, highlighting societal fairness concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in professional recommendation contexts",
      "affected_populations": [
        "female applicants"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias evaluation and model analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.08617v2",
    "title": "The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features",
    "year": 2023,
    "authors": [
      "Navita Goyal",
      "Connor Baumler",
      "Tin Nguyen",
      "Hal Daumé III"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI affecting fairness perceptions related to protected groups, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "other"
      ],
      "other_detail": "Biases in decision-making fairness",
      "affected_populations": [
        "protected groups"
      ],
      "methodology": [
        "Experiment",
        "Survey"
      ],
      "methodology_detail": "Studying perception and decision-making effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.08349v2",
    "title": "Performativity and Prospective Fairness",
    "year": 2023,
    "authors": [
      "Sebastian Zezulka",
      "Konstantin Genin"
    ],
    "categories": [
      "cs.CY",
      "K.4.1; K.4.2; J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how algorithmic deployment can affect social inequalities, specifically gender disparities in the labor market, by analyzing performative effects on structural inequalities.",
      "inequality_type": [
        "gender",
        "economic",
        "labor"
      ],
      "other_detail": "Focus on gender inequalities in employment outcomes",
      "affected_populations": [
        "women",
        "unemployed"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Estimating post-deployment inequality changes from pre-deployment data",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.06904v1",
    "title": "Mitigating stereotypical biases in text to image generative systems",
    "year": 2023,
    "authors": [
      "Piero Esposito",
      "Parmida Atighehchian",
      "Anastasis Germanidis",
      "Deepti Ghadiyaram"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to skin tone and gender in AI-generated images, which are social identity factors linked to inequality and discrimination.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation in AI-generated visual content",
      "affected_populations": [
        "people of darker skin",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Finetuning models with synthetic, diverse data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.06590v1",
    "title": "No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech Recognition through Pitch Manipulation",
    "year": 2023,
    "authors": [
      "Dennis Fucci",
      "Marco Gaido",
      "Matteo Negri",
      "Mauro Cettolo",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender disparities in speech recognition accuracy, highlighting under-representation and bias issues related to gender, a social group. It proposes a technical solution to reduce gender-based performance gaps, directly engaging with social fairness concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Addressing gender bias in AI systems",
      "affected_populations": [
        "female speakers"
      ],
      "methodology": [
        "Experiment",
        "Data Augmentation"
      ],
      "methodology_detail": "Pitch manipulation to simulate under-represented voices",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.06061v1",
    "title": "Auditing Gender Analyzers on Text Data",
    "year": 2023,
    "authors": [
      "Siddharth D Jaiswal",
      "Ankit Kumar Verma",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI systems against non-binary individuals, highlighting societal discrimination. It analyzes how AI tools propagate gender stereotypes, addressing social bias and fairness issues. The focus on gender bias in AI relates directly to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "non-binary individuals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Auditing and fine-tuning AI models on gender-inclusive datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.05294v1",
    "title": "Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus",
    "year": 2023,
    "authors": [
      "Andrea Piergentili",
      "Beatrice Savoldi",
      "Dennis Fucci",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social inequality issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Evaluation Methods"
      ],
      "methodology_detail": "Develops benchmark and evaluation approaches for gender-neutral translation",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.05135v1",
    "title": "Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT",
    "year": 2023,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Fabian Grob",
      "Shailja Thakur",
      "Hammond Pearce",
      "Benjamin Tan",
      "Ramesh Karri",
      "Siddharth Garg"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates bias in algorithmic hiring related to race, gender, and other protected attributes, directly addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social discrimination"
      ],
      "other_detail": "Focus on bias in AI hiring systems",
      "affected_populations": [
        "racial minorities",
        "women",
        "pregnant individuals"
      ],
      "methodology": [
        "Experiment",
        "Contrastive input decoding",
        "Replication of field experiments"
      ],
      "methodology_detail": "Testing bias in LLMs using resume matching tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.04585v3",
    "title": "Interventions Against Machine-Assisted Statistical Discrimination",
    "year": 2023,
    "authors": [
      "John Y. Zhu"
    ],
    "categories": [
      "econ.TH",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses statistical discrimination and interventions related to decision-making biases, which are central to social inequality issues like discrimination. It focuses on beliefs and biases in AI systems affecting social groups. The abstract emphasizes regulation and bias mitigation, relevant to social fairness.",
      "inequality_type": [
        "racial",
        "educational",
        "social"
      ],
      "other_detail": "Focus on bias regulation in machine learning",
      "affected_populations": [
        "social groups",
        "discriminated groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Designing belief-contingent interventions",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.04352v1",
    "title": "Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates",
    "year": 2023,
    "authors": [
      "Camille Olivia Little",
      "Debolina Halder Lina",
      "Genevera I. Allen"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias in AI models, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on fairness in machine learning models",
      "affected_populations": [
        "social groups",
        "individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops fair feature importance scores for interpretability",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.04348v1",
    "title": "AccEq-DRT: Planning Demand-Responsive Transit to reduce inequality of accessibility",
    "year": 2023,
    "authors": [
      "Duo Wang",
      "Andrea Araldo",
      "Mounim A. El Yacoubi"
    ],
    "categories": [
      "math.OC",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in accessibility to transportation, which impacts social groups based on location, affecting mobility and opportunity access.",
      "inequality_type": [
        "geographic",
        "urban-rural",
        "socioeconomic"
      ],
      "other_detail": "Focuses on spatial accessibility disparities in urban areas",
      "affected_populations": [
        "suburban residents",
        "low-density areas"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Combines graph models and optimization for planning",
      "geographic_focus": [
        "Montreal"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2311.10725v1",
    "title": "Should they? Mobile Biometrics and Technopolicy meet Queer Community Considerations",
    "year": 2023,
    "authors": [
      "Anaelia Ovalle",
      "Davi Liang",
      "Alicia Boyd"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4; K.5; I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses privacy, bias, and discrimination issues affecting marginalized groups, specifically the queer community, in the context of biometric data and AI systems.",
      "inequality_type": [
        "gender",
        "disability",
        "health"
      ],
      "other_detail": "Focus on marginalized social groups and privacy concerns",
      "affected_populations": [
        "queer community",
        "marginalized groups"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing ethical and social implications of biometric AI",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.04097v1",
    "title": "Impact of Gender on the Evaluation of Security Decisions",
    "year": 2023,
    "authors": [
      "Winnie Mbaka",
      "Katja Tuma"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates how gender influences security decision judgments, addressing gender bias and discrimination in decision-making processes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in security evaluations",
      "affected_populations": [
        "female analysts",
        "male analysts"
      ],
      "methodology": [
        "Experiment",
        "Empirical Study"
      ],
      "methodology_detail": "Analyzes gender impact through empirical data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.03647v2",
    "title": "Rethinking Algorithmic Fairness for Human-AI Collaboration",
    "year": 2023,
    "authors": [
      "Haosen Ge",
      "Hamsa Bastani",
      "Osbert Bastani"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in human-AI decision-making, focusing on equitable outcomes in contexts like criminal sentencing, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "criminal justice",
        "social"
      ],
      "other_detail": "Focus on fairness in criminal sentencing decisions",
      "affected_populations": [
        "racial minorities",
        "criminal defendants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Case Study"
      ],
      "methodology_detail": "Analysis of sentencing data before and after AI tool",
      "geographic_focus": [
        "Virginia"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.03557v1",
    "title": "Mobility Segregation Dynamics and Residual Isolation During Pandemic Interventions",
    "year": 2023,
    "authors": [
      "Rafiazka Millanida Hilman",
      "Manuel García-Herranz",
      "Vedran Sekara",
      "Márton Karsai"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.data-an",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines mobility patterns and segregation across socioeconomic groups during the pandemic, highlighting inequalities in mobility and social isolation.",
      "inequality_type": [
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Focus on mobility and socioeconomic segregation",
      "affected_populations": [
        "different socioeconomic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Analysis"
      ],
      "methodology_detail": "Mobility data coupled with socioeconomic info",
      "geographic_focus": [
        "Bogota",
        "Jakarta",
        "London",
        "New York"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.03258v2",
    "title": "Assessing Electricity Service Unfairness with Transfer Counterfactual Learning",
    "year": 2023,
    "authors": [
      "Song Wei",
      "Xiangrui Kong",
      "Alinson Santos Xavier",
      "Shixiang Zhu",
      "Yao Xie",
      "Feng Qiu"
    ],
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates systematic disparities in power outage durations based on demographic factors like income and age, highlighting issues of injustice in service provision to disadvantaged communities.",
      "inequality_type": [
        "income",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on power outage disparities in vulnerable groups",
      "affected_populations": [
        "low-income areas",
        "elderly populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Transfer Learning"
      ],
      "methodology_detail": "Estimating counterfactual unfairness with transfer learning",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.03146v5",
    "title": "Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data",
    "year": 2023,
    "authors": [
      "Son Nguyen",
      "Adam Wang",
      "Albert Montillo"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI models, addressing biases related to social groups such as race, gender, and age, and aims to improve equitable treatment across these groups.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Addresses fairness in algorithmic decision-making",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly",
        "patients"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Metrics",
        "Adversarial Training",
        "Bayesian Neural Networks"
      ],
      "methodology_detail": "Enhances fairness via architectural and loss function modifications",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.02988v1",
    "title": "Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples",
    "year": 2023,
    "authors": [
      "Phillip Howard",
      "Avinash Madasu",
      "Tiep Le",
      "Gustavo Lujan Moreno",
      "Vasudev Lal"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in vision-language models related to social attributes like race and gender, highlighting social discrimination issues. It focuses on intersectional biases, which are central to social inequality discussions. The study's aim to reveal biases in AI systems directly relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on intersectional social biases in AI",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Using diffusion models to generate counterfactual image-text pairs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.02752v1",
    "title": "Fair Feature Selection: A Comparison of Multi-Objective Genetic Algorithms",
    "year": 2023,
    "authors": [
      "James Brookhouse",
      "Alex Freitas"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI classifiers, addressing social bias related to race and gender, impacting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Fairness in AI decision-making processes",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Multi-Objective Optimization"
      ],
      "methodology_detail": "Genetic Algorithms for fair feature selection",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.02492v3",
    "title": "FairVision: Equitable Deep Learning for Eye Disease Screening via Fair Identity Scaling",
    "year": 2023,
    "authors": [
      "Yan Luo",
      "Muhammad Osama Khan",
      "Yu Tian",
      "Min Shi",
      "Zehao Dou",
      "Tobias Elze",
      "Yi Fang",
      "Mengyu Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness and biases in AI models across race, gender, and ethnicity, directly addressing social discrimination issues in healthcare AI.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "health"
      ],
      "other_detail": "Focuses on fairness in medical AI models",
      "affected_populations": [
        "patients with eye diseases",
        "minority groups"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Evaluation",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops a fairness method and releases a large dataset",
      "geographic_focus": [
        "Harvard (implying US)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.01679v1",
    "title": "Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features",
    "year": 2023,
    "authors": [
      "Hadi Elzayn",
      "Emily Black",
      "Patrick Vossler",
      "Nathanael Jo",
      "Jacob Goldin",
      "Daniel E. Ho"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness metrics related to protected attributes like race and gender, addressing social discrimination in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses fairness measurement with limited protected attribute data",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Estimating fairness bounds and training fair models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.00274v1",
    "title": "AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR",
    "year": 2023,
    "authors": [
      "Tobi Olatunji",
      "Tejumade Afonja",
      "Aditya Yadavalli",
      "Chris Chinenye Emezue",
      "Sahib Singh",
      "Bonaventure F. P. Dossou",
      "Joanne Osuchukwu",
      "Salomey Osei",
      "Atnafu Lambebo Tonja",
      "Naome Etori",
      "Clinton Mbataku"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias and accent performance disparities in speech recognition, highlighting inequalities faced by African accents and populations. It focuses on underrepresented groups and aims to improve AI fairness and inclusivity in healthcare and general domains.",
      "inequality_type": [
        "racial",
        "linguistic",
        "health"
      ],
      "other_detail": "Focus on African accents and healthcare disparities",
      "affected_populations": [
        "African speakers",
        "overworked clinicians"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Creating and benchmarking African accented speech datasets",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.17417v2",
    "title": "Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction",
    "year": 2023,
    "authors": [
      "Arjun Subramonian",
      "Levent Sagun",
      "Yizhou Sun"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how degree bias in GNN link prediction impacts social groups, affecting fairness and power dynamics, which relates to social inequality issues.",
      "inequality_type": [
        "social",
        "gender",
        "power",
        "inequality"
      ],
      "other_detail": "Focuses on within-group fairness and power disparities",
      "affected_populations": [
        "social groups",
        "network users"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias and fairness metrics in networks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.17197v1",
    "title": "An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features",
    "year": 2023,
    "authors": [
      "Mohamed Huti",
      "Tiarna Lee",
      "Elinor Sawyer",
      "Andrew P. King"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in AI models using race as a variable, highlighting social discrimination and bias issues. It examines how AI performance varies across racial groups, indicating social inequality concerns.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias in medical AI models based on race",
      "affected_populations": [
        "Black patients",
        "White patients"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes race bias in radiomics-based random forest models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.16887v1",
    "title": "Platformization of Inequality: Gender and Race in Digital Labor Platforms",
    "year": 2023,
    "authors": [
      "Isabel Munoz",
      "Pyeonghwa Kim",
      "Clea O'Neil",
      "Michael Dunn",
      "Steve Sawyer"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how digital labor platforms reinforce social identities like gender and race, leading to stereotypes, bias, and marginalization, which are core aspects of social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focus on online freelancing and platform effects",
      "affected_populations": [
        "female freelancers",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Data from 108 online freelancers analyzed",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.17347v1",
    "title": "Demographic Parity: Mitigating Biases in Real-World Data",
    "year": 2023,
    "authors": [
      "Orestis Loukas",
      "Ho-Ryun Chung"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in decision systems affecting social groups, aiming to mitigate discrimination related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI decision-making systems",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Derives bias-free datasets from real-world data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.01574v1",
    "title": "Potential Ways to Detect Unfairness in HRI and to Re-establish Positive Group Dynamics",
    "year": 2023,
    "authors": [
      "Astrid Rosenthal-von der Pütten",
      "Stefan Schiffer"
    ],
    "categories": [
      "cs.RO"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithm biases leading to social exclusion and discrimination, which relate to social inequalities such as discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on social bias detection in human-robot interactions",
      "affected_populations": [
        "human interactants",
        "social groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Detecting biases and developing fairness mechanisms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.01422v1",
    "title": "Service Pet Robot Design: Queer, Feminine and Sexuality Aspects",
    "year": 2023,
    "authors": [
      "Anna-Maria Velentza",
      "Antigoni Tsagkaropoulou"
    ],
    "categories": [
      "cs.RO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender and sexuality representation in robot design, highlighting issues of discrimination and bias affecting queer and feminine groups. It explores how design choices can promote inclusion, reflecting social inequality concerns. The focus on marginalized identities indicates a direct engagement with social inequality topics.",
      "inequality_type": [
        "gender",
        "sexuality"
      ],
      "other_detail": "Representation and inclusion in technology design",
      "affected_populations": [
        "queer individuals",
        "feminine figures"
      ],
      "methodology": [
        "Design Study",
        "Questionnaires",
        "Focus Groups"
      ],
      "methodology_detail": "Evaluating perceptions of robot and researcher",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.14921v1",
    "title": "A Democratic Platform for Engaging with Disabled Community in Generative AI Development",
    "year": 2023,
    "authors": [
      "Deepak Giri",
      "Erin Brady"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and discrimination faced by disabled individuals in AI systems, highlighting social inequalities related to disability and fairness in technology.",
      "inequality_type": [
        "disability",
        "fairness"
      ],
      "other_detail": "Focus on bias in AI affecting disabled communities",
      "affected_populations": [
        "disabled people"
      ],
      "methodology": [
        "System Design",
        "Qualitative Study"
      ],
      "methodology_detail": "Community engagement platform development and analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.14198v1",
    "title": "(Predictable) Performance Bias in Unsupervised Anomaly Detection",
    "year": 2023,
    "authors": [
      "Felix Meissen",
      "Svenja Breuer",
      "Moritz Knolle",
      "Alena Buyx",
      "Ruth Müller",
      "Georgios Kaissis",
      "Benedikt Wiestler",
      "Daniel Rückert"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in anomaly detection performance across demographic subgroups, highlighting fairness issues in AI systems affecting social groups.",
      "inequality_type": [
        "racial",
        "health",
        "demographic"
      ],
      "other_detail": "Focuses on fairness in medical imaging AI models",
      "affected_populations": [
        "demographic subgroups",
        "patients"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates model performance across subgroups using new fairness metric",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.14392v1",
    "title": "Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction",
    "year": 2023,
    "authors": [
      "Yuning Du",
      "Yuyang Xue",
      "Rohan Dharmakumar",
      "Sotirios A. Tsaftaris"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes fairness biases related to demographic groups in medical AI, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Focus on demographic fairness in medical imaging",
      "affected_populations": [
        "women",
        "older adults"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness analysis using model performance metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.13933v3",
    "title": "Fairness and Bias in Algorithmic Hiring: a Multidisciplinary Survey",
    "year": 2023,
    "authors": [
      "Alessandro Fabris",
      "Nina Baranowska",
      "Matthew J. Dennis",
      "David Graus",
      "Philipp Hacker",
      "Jorge Saldivar",
      "Frederik Zuiderveen Borgesius",
      "Asia J. Biega"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness in hiring, addressing biases affecting social groups and inequalities related to race, gender, and societal structures.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on employment and structural biases",
      "affected_populations": [
        "job applicants",
        "minority groups"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Multidisciplinary survey of systems and biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.14381v1",
    "title": "Survey of Social Bias in Vision-Language Models",
    "year": 2023,
    "authors": [
      "Nayeon Lee",
      "Yejin Bang",
      "Holy Lovenia",
      "Samuel Cahyawijaya",
      "Wenliang Dai",
      "Pascale Fung"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses social biases in AI models, which can reinforce social inequalities and unfairness across groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in AI models",
      "affected_populations": [
        "social groups",
        "minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Literature Review",
        "Survey"
      ],
      "methodology_detail": "Comparative analysis of bias studies in AI fields",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.13292v1",
    "title": "Beyond Fairness: Age-Harmless Parkinson's Detection via Voice",
    "year": 2023,
    "authors": [
      "Yicheng Wang",
      "Xiaotian Han",
      "Leisheng Yu",
      "Na Zou"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI detection of Parkinson's across age groups, highlighting disparities related to age, a social characteristic. It aims to improve detection accuracy for early-onset patients without sacrificing performance for the elderly, indicating concern for equitable healthcare outcomes.",
      "inequality_type": [
        "health",
        "age"
      ],
      "other_detail": "Fairness in medical AI across age groups",
      "affected_populations": [
        "young PD patients",
        "elderly PD patients"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Debiasing with GradCAM masking and ensemble models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.12491v2",
    "title": "Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation",
    "year": 2023,
    "authors": [
      "Bar Iluz",
      "Tomasz Limisiewicz",
      "Gabriel Stanovsky",
      "David Mareček"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, focusing on gender forms and representation, which relates directly to gender inequality and social bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language and AI",
      "affected_populations": [
        "female",
        "non-stereotypical gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes tokenization and bias measurement techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2310.03031v3",
    "title": "How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses",
    "year": 2023,
    "authors": [
      "Stefanie Urchs",
      "Veronika Thurner",
      "Matthias Aßenmacher",
      "Christian Heumann",
      "Stephanie Thiemichen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI responses, addressing gender inequality and bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Prompt analysis"
      ],
      "methodology_detail": "Analyzing response variations to gendered prompts",
      "geographic_focus": [
        "Germany",
        "English-speaking regions"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.11691v1",
    "title": "RAI4IoE: Responsible AI for Enabling the Internet of Energy",
    "year": 2023,
    "authors": [
      "Minhui Xue",
      "Surya Nepal",
      "Ling Liu",
      "Subbu Sethuvenkatraman",
      "Xingliang Yuan",
      "Carsten Rudolph",
      "Ruoxi Sun",
      "Greg Eisenhauer"
    ],
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses equitable access, participation, and data responsibility in energy systems, highlighting social fairness issues affecting disadvantaged groups.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on equitable participation in energy access and data use",
      "affected_populations": [
        "disadvantaged groups",
        "community members"
      ],
      "methodology": [
        "System Design",
        "Ethics Analysis"
      ],
      "methodology_detail": "Designing equitable AI frameworks and responsible data handling",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.12371v2",
    "title": "Fairness Hub Technical Briefs: AUC Gap",
    "year": 2023,
    "authors": [
      "Jinsook Lee",
      "Chris Brooks",
      "Renzhe Yu",
      "Rene Kizilcec"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses measuring bias in AI models across social subgroups, addressing fairness issues related to social groups such as gender, race, and socioeconomic status.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Focus on fairness in educational achievement models",
      "affected_populations": [
        "students in low-income schools",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using AUC Gap to assess model bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.11239v1",
    "title": "Data-Driven Analysis of Gender Fairness in the Software Engineering Academic Landscape",
    "year": 2023,
    "authors": [
      "Giordano d'Aloisio",
      "Andrea D'Angelo",
      "Francesca Marzi",
      "Diana Di Marco",
      "Giovanni Stilo",
      "Antinisca Di Marco"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in academic promotions, addressing gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female academics"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias metrics applied to promotion data",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.11062v2",
    "title": "The social stratification of internal migration and daily mobility during the COVID-19 pandemic",
    "year": 2023,
    "authors": [
      "Erick Elejalde",
      "Leo Ferres",
      "Víctor Navarro",
      "Loreto Bravo",
      "Emilio Zagheni"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines socioeconomic differentials in mobility behaviors during COVID-19, highlighting economic and income disparities across social groups.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on socioeconomic stratification in mobility patterns",
      "affected_populations": [
        "lower-income groups",
        "higher-income groups"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of mobile phone data",
      "geographic_focus": [
        "Santiago de Chile"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.10780v1",
    "title": "Towards affective computing that works for everyone",
    "year": 2023,
    "authors": [
      "Tessa Verhoef",
      "Eduard Fosch-Villaronga"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "K.4; K.5"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in affective computing datasets related to race, gender, age, and health, highlighting issues of inclusivity and fairness affecting different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focus on dataset diversity and societal impact",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "elderly",
        "health conditions"
      ],
      "methodology": [
        "Literature Review",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzing existing datasets and literature",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.10835v1",
    "title": "Analysing race and sex bias in brain age prediction",
    "year": 2023,
    "authors": [
      "Carolina Piçarra",
      "Ben Glocker"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in brain age prediction related to race and sex, highlighting disparities across social groups. It analyzes how demographic factors influence model performance and feature distributions, addressing social bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI models across social demographic groups",
      "affected_populations": [
        "Black",
        "White",
        "Asian",
        "male",
        "female"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Performance and feature distribution comparison across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.10641v2",
    "title": "KFC: Kinship Verification with Fair Contrastive Loss and Multi-Task Learning",
    "year": 2023,
    "authors": [
      "Jia Luo Peng",
      "Keng Wei Chang",
      "Shang-Hong Lai"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias and fairness in kinship verification, a social issue related to racial inequality. It explicitly discusses mitigating racial bias in AI systems. The focus on fairness and bias mitigation indicates engagement with social inequality concerns.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial bias in AI systems",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Fairness Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Combines dataset labeling, multi-task learning, and adversarial fairness methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.10215v1",
    "title": "In Consideration of Indigenous Data Sovereignty: Data Mining as a Colonial Practice",
    "year": 2023,
    "authors": [
      "Jennafer Shae Roberts",
      "Laura N Montoya"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial and cultural inequalities related to Indigenous data sovereignty and colonial practices in data mining.",
      "inequality_type": [
        "racial",
        "ethnic",
        "indigenous",
        "informational",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on Indigenous rights and data governance",
      "affected_populations": [
        "Indigenous communities"
      ],
      "methodology": [
        "Case Study",
        "Ethics Analysis"
      ],
      "methodology_detail": "Applying CARE Principles to data governance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.09825v3",
    "title": "Bias of AI-Generated Content: An Examination of News Produced by Large Language Models",
    "year": 2023,
    "authors": [
      "Xiao Fang",
      "Shangkun Che",
      "Minjia Mao",
      "Hongzhe Zhang",
      "Ming Zhao",
      "Xiaohang Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and racial biases in AI-generated news content, highlighting discrimination against females and Black individuals, thus addressing social inequalities related to race and gender.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI content generation",
      "affected_populations": [
        "females",
        "Black individuals"
      ],
      "methodology": [
        "Experiment",
        "Comparative Analysis",
        "Bias Evaluation"
      ],
      "methodology_detail": "Analyzing biases in AI-generated news content",
      "geographic_focus": [
        "United States",
        "Global"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.09697v3",
    "title": "Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels",
    "year": 2023,
    "authors": [
      "Panatchakorn Anantaprayoon",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in language models, addressing social discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Multilingual bias evaluation across English, Japanese, Chinese",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.09277v2",
    "title": "Fairness for All: Investigating Harms to Within-Group Individuals in Producer Fairness Re-ranking Optimization -- A Reproducibility Study",
    "year": 2023,
    "authors": [
      "Giovanni Pellegrini",
      "Vittorio Maria Faraco",
      "Yashar Deldjoo"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness issues in recommender systems, highlighting impacts on within-group individuals, which relates to social fairness and bias concerns.",
      "inequality_type": [
        "informational",
        "digital",
        "social"
      ],
      "other_detail": "Focuses on fairness within groups, not specific social identities",
      "affected_populations": [
        "cold items users",
        "producer groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Reproduction and modification of fairness algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.09120v1",
    "title": "Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie",
    "year": 2023,
    "authors": [
      "Kyrie Zhixuan Zhou",
      "Madelyn Rose Sanfilippo"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in large language models, highlighting social discrimination and bias issues related to gender. It discusses cultural influences on gender bias and proposes governance, indicating a focus on social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI responses",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Content Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzed social media discussions on gender bias",
      "geographic_focus": [
        "US",
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.09092v1",
    "title": "The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated",
    "year": 2023,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala",
      "Naoaki Okazaki"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social biases in language models related to gender, a key aspect of social inequality, and evaluates their impact on downstream tasks, addressing social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender-related social biases in AI",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Benchmark Analysis"
      ],
      "methodology_detail": "Evaluates debiasing effects across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08902v3",
    "title": "Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models",
    "year": 2023,
    "authors": [
      "Mahammed Kamruzzaman",
      "Md. Minul Islam Shovon",
      "Gene Louis Kim"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to age, beauty, nationality, and institutional factors in LLMs, which are linked to social discrimination and inequality. It examines how AI systems may perpetuate or reflect societal biases affecting different social groups. The focus on social group biases and their potential impact aligns with social inequality concerns.",
      "inequality_type": [
        "age",
        "beauty",
        "nationality",
        "institutional"
      ],
      "other_detail": "Biases along less-studied social dimensions",
      "affected_populations": [
        "older adults",
        "perceived attractive individuals",
        "national groups",
        "institutionalized groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Template-generated sentence completion tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08760v1",
    "title": "Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?",
    "year": 2023,
    "authors": [
      "Abhishek Mandal",
      "Susan Leavy",
      "Suzanne Little"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, addressing social gender inequality and bias amplification.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias amplification in AI architectures",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and model comparison",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08573v2",
    "title": "Indian-BhED: A Dataset for Measuring India-Centric Biases in Large Language Models",
    "year": 2023,
    "authors": [
      "Khyati Khandelwal",
      "Manuel Tonneau",
      "Andrew M. Bean",
      "Hannah Rose Kirk",
      "Scott A. Hale"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to caste and religion in India, highlighting societal stereotypes and their propagation by LLMs, which relate directly to social discrimination and inequality.",
      "inequality_type": [
        "caste",
        "religion"
      ],
      "other_detail": "Focus on societal stereotypes in Indian context",
      "affected_populations": [
        "caste groups",
        "religious communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Quantifying bias in language models using a new dataset",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08442v1",
    "title": "Toward responsible face datasets: modeling the distribution of a disentangled latent space for sampling face images from demographic groups",
    "year": 2023,
    "authors": [
      "Parsa Rahimi",
      "Christophe Ecabert",
      "Sebastien Marcel"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in facial recognition datasets related to demographic groups, impacting fairness and potentially leading to discrimination. It focuses on mitigating demographic biases in AI systems, which are linked to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in facial recognition datasets",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Synthetic data generation using StyleGAN",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08121v1",
    "title": "\"I'm Not Confident in Debiasing AI Systems Since I Know Too Little\": Teaching AI Creators About Gender Bias Through Hands-on Tutorials",
    "year": 2023,
    "authors": [
      "Kyrie Zhixuan Zhou",
      "Jiaxun Cao",
      "Xiaowen Yuan",
      "Daniel E. Weissglass",
      "Zachary Kilhoffer",
      "Madelyn Rose Sanfilippo",
      "Xin Tong"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI systems, a social inequality issue affecting women, and discusses educational efforts to mitigate this bias.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Experiment",
        "Tutorial Evaluation"
      ],
      "methodology_detail": "Assessing tutorial effectiveness with AI creators",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08047v3",
    "title": "Bias in News Summarization: Measures, Pitfalls and Corpora",
    "year": 2023,
    "authors": [
      "Julius Steen",
      "Katja Markert"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in AI-generated summaries, addressing social bias and fairness issues affecting different social groups.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social biases in AI summarization",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Controlled input generation for bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.07251v2",
    "title": "In-Contextual Gender Bias Suppression for Large Language Models",
    "year": 2023,
    "authors": [
      "Daisuke Oba",
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in large language models, a form of social bias. It discusses methods to reduce gender bias, which relates to social gender inequality and discrimination. The focus on gender bias suppression in AI systems directly pertains to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias suppression via textual prompts and templates",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.07110v1",
    "title": "Data Augmentation via Subgroup Mixup for Improving Fairness",
    "year": 2023,
    "authors": [
      "Madeline Navarro",
      "Camille Little",
      "Genevera I. Allen",
      "Santiago Segarra"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing biases across social groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "underrepresented groups",
        "societal minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Data Augmentation",
        "Experiment"
      ],
      "methodology_detail": "Pairwise mixup for data balancing and fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.06607v1",
    "title": "An Empirical Analysis of Racial Categories in the Algorithmic Fairness Literature",
    "year": 2023,
    "authors": [
      "Amina A. Abdu",
      "Irene V. Pasquetto",
      "Abigail Z. Jacobs"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial categories in algorithmic fairness, addressing racial discrimination and bias in AI systems.",
      "inequality_type": [
        "racial",
        "discrimination"
      ],
      "other_detail": "Focus on racial categorization and social implications",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Content Analysis"
      ],
      "methodology_detail": "Analyzes academic papers on algorithmic fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.07176v3",
    "title": "Optimal and Fair Encouragement Policy Evaluation and Learning",
    "year": 2023,
    "authors": [
      "Angela Zhou"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints and disparities in treatment take-up, which relate to social inequalities such as access and equity issues.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focus on treatment access and fairness constraints",
      "affected_populations": [
        "beneficiaries of social services",
        "impoverished groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Constrained Optimization",
        "Case Study"
      ],
      "methodology_detail": "Robust estimation under fairness and positivity constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.08624v1",
    "title": "Challenges in Annotating Datasets to Quantify Bias in Under-represented Society",
    "year": 2023,
    "authors": [
      "Vithya Yogarajan",
      "Gillian Dobbie",
      "Timothy Pistotti",
      "Joshua Bensemann",
      "Kobe Knowles"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias in AI systems affecting under-represented societies, highlighting social discrimination and fairness issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias quantification in under-represented societies",
      "affected_populations": [
        "New Zealand population"
      ],
      "methodology": [
        "Dataset Creation",
        "Manual Annotation",
        "Qualitative Study"
      ],
      "methodology_detail": "Annotation challenges and lessons learned",
      "geographic_focus": [
        "New Zealand"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.05148v2",
    "title": "Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color",
    "year": 2023,
    "authors": [
      "William Thong",
      "Przemyslaw Joniak",
      "Alice Xiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in computer vision related to skin color, which are linked to racial discrimination and fairness issues affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "fairness"
      ],
      "other_detail": "Focus on skin color bias in AI systems",
      "affected_populations": [
        "darker-skinned individuals",
        "racial minorities"
      ],
      "methodology": [
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring skin color biases in datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.05088v1",
    "title": "Towards Trustworthy Artificial Intelligence for Equitable Global Health",
    "year": 2023,
    "authors": [
      "Hong Qin",
      "Jude Kong",
      "Wandi Ding",
      "Ramneek Ahluwalia",
      "Christo El Morr",
      "Zeynep Engin",
      "Jake Okechukwu Effoduh",
      "Rebecca Hwa",
      "Serena Jingchuan Guo",
      "Laleh Seyyed-Kalantari",
      "Sylvia Kiwuwa Muyingo",
      "Candace Makeda Moore",
      "Ravi Parikh",
      "Reva Schwartz",
      "Dongxiao Zhu",
      "Xiaoqian Wang",
      "Yiye Zhang"
    ],
    "categories": [
      "cs.CY",
      "q-bio.OT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI bias, fairness, and equitable global health, addressing social inequalities related to health and social bias.",
      "inequality_type": [
        "health",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on bias mitigation and fairness in AI systems",
      "affected_populations": [
        "global health populations",
        "social groups"
      ],
      "methodology": [
        "Workshop",
        "Discussion",
        "Framework Analysis"
      ],
      "methodology_detail": "Expert convening and framework application",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.04997v1",
    "title": "Gender Bias in Multimodal Models: A Transnational Feminist Approach Considering Geographical Region and Culture",
    "year": 2023,
    "authors": [
      "Abhishek Mandal",
      "Suzanne Little",
      "Susan Leavy"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias and cultural stereotypes in AI models, highlighting social inequalities related to gender and regional disparities. It analyzes how AI perpetuates biases aligned with societal gender inequality indices. The focus on gender bias and cultural dimensions indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Focus on gender bias across regions",
      "affected_populations": [
        "women",
        "regional communities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement across global regions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.06415v4",
    "title": "Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large Language Models",
    "year": 2023,
    "authors": [
      "Arka Dutta",
      "Adel Khorramrouz",
      "Sujan Dutta",
      "Ashiqur R. KhudaBukhsh"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases related to race, gender, and identity groups in AI models, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on toxic content and social biases in language models",
      "affected_populations": [
        "minority groups",
        "women",
        "identity groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Bias Analysis"
      ],
      "methodology_detail": "Iterative bias elicitation and bias audit framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.04027v2",
    "title": "TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models",
    "year": 2023,
    "authors": [
      "Emmanuel Klu",
      "Sameer Sethi"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in text models, impacting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on text fairness and bias mitigation",
      "affected_populations": [
        "underrepresented groups",
        "minority communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Develops lexicon and annotation tools for fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.03876v1",
    "title": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs",
    "year": 2023,
    "authors": [
      "Patrick Haller",
      "Ansar Aynetdinov",
      "Alan Akbik"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly investigates biases related to social groups such as political, geographic, gender, and age, aiming to make biases in AI models transparent. It addresses social discrimination and bias in AI systems, which are key aspects of social inequality. The focus on demographic biases directly relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "geographic",
        "age",
        "political"
      ],
      "other_detail": "Explicitly models biases across social demographic groups",
      "affected_populations": [
        "gender groups",
        "geographic communities",
        "age groups",
        "political groups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Fine-tuning on demographic-specific texts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.03175v2",
    "title": "Gender-specific Machine Translation with Large Language Models",
    "year": 2023,
    "authors": [
      "Eduardo Sánchez",
      "Pierre Andrews",
      "Pontus Stenetorp",
      "Mikel Artetxe",
      "Marta R. Costa-jussà"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social bias issue affecting gender equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "evaluating gender-specific translation accuracy and bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.02160v1",
    "title": "Bias Propagation in Federated Learning",
    "year": 2023,
    "authors": [
      "Hongyan Chang",
      "Reza Shokri"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias propagation related to sensitive attributes like gender and race, affecting fairness in federated learning, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in AI models affecting social groups",
      "affected_populations": [
        "under-represented groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias propagation in real-world datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.02142v3",
    "title": "Characteristics of ChatGPT users from Germany: implications for the digital divide from web tracking data",
    "year": 2023,
    "authors": [
      "Celina Kacperski",
      "Denis Bonnay",
      "Juhi Kulshrestha",
      "Peter Selb",
      "Andreas Spitz",
      "Roberto Ulloa"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines digital disparities related to socio-demographic factors affecting ChatGPT usage, highlighting inequalities in access and engagement with AI technologies.",
      "inequality_type": [
        "digital",
        "educational",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on digital divide and social disparities in AI use",
      "affected_populations": [
        "German citizens",
        "underserved populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey",
        "Statistical Analysis"
      ],
      "methodology_detail": "Combines behavioral data and surveys for analysis",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.01964v1",
    "title": "Gender Inequalities: Women Researchers Require More Knowledge in Specific and Experimental Topics",
    "year": 2023,
    "authors": [
      "Shiqi Tang",
      "Dongyi Wang",
      "Jianhua Hou"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in scientific research, focusing on knowledge gaps and participation, which are aspects of social gender inequality.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focuses on gender disparities in scientific knowledge and participation",
      "affected_populations": [
        "women researchers",
        "female scientists"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Analyzes online Q&A data on researcher topics and knowledge",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.02467v1",
    "title": "Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)",
    "year": 2023,
    "authors": [
      "Yu Huang",
      "Jingchuan Guo",
      "William T Donahoo",
      "Zhengkang Fan",
      "Ying Lu",
      "Wei-Han Chen",
      "Huilin Tang",
      "Lori Bilello",
      "Elizabeth A Shenkman",
      "Jiang Bian"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social determinants of health and racial disparities in T2D hospitalization risk, addressing social inequalities. It develops a fairness-aware ML tool to identify social risk factors affecting vulnerable groups.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Addresses social disparities in health outcomes",
      "affected_populations": [
        "racial minorities",
        "socially disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Assessment"
      ],
      "methodology_detail": "Develops an explainable ML risk score with fairness optimization",
      "geographic_focus": [
        "Florida"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.01610v4",
    "title": "Fairness in Ranking under Disparate Uncertainty",
    "year": 2023,
    "authors": [
      "Richa Rastogi",
      "Thorsten Joachims"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in ranking algorithms, highlighting disparities affecting minority groups due to uncertainty, which relates to social fairness and inequality.",
      "inequality_type": [
        "racial",
        "educational",
        "informational"
      ],
      "other_detail": "Focuses on fairness disparities in AI ranking systems",
      "affected_populations": [
        "minority groups",
        "underrepresented groups"
      ],
      "methodology": [
        "Algorithm Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops a fairness criterion and efficient algorithm",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.01288v1",
    "title": "How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets",
    "year": 2023,
    "authors": [
      "Danula Hettiachchi",
      "Indigo Holcombe-James",
      "Stephanie Livingstone",
      "Anjalee de Silva",
      "Matthew Lease",
      "Flora D. Salim",
      "Mark Sanderson"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in annotating hate speech, focusing on gender and political attitudes, which relate to social discrimination and bias. It addresses how subjective perceptions influence content moderation, impacting social groups differently.",
      "inequality_type": [
        "gender",
        "political",
        "social bias"
      ],
      "other_detail": "Bias in content moderation and annotation processes",
      "affected_populations": [
        "women",
        "targeted groups"
      ],
      "methodology": [
        "Experiment",
        "Qualitative Study",
        "Crowdsourcing"
      ],
      "methodology_detail": "Studies on Mechanical Turk with interviews",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.14345v4",
    "title": "Bias Testing and Mitigation in LLM-based Code Generation",
    "year": 2023,
    "authors": [
      "Dong Huang",
      "Jie M. Zhang",
      "Qingwen Bu",
      "Xiaofei Xie",
      "Junjie Chen",
      "Heming Cui"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender, race, and age in AI-generated code, addressing social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Bias in AI code generation affecting social groups",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Experiment",
        "Empirical Study",
        "Bias Testing"
      ],
      "methodology_detail": "Testing bias prevalence and mitigation strategies in LLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2309.00035v1",
    "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
    "year": 2023,
    "authors": [
      "Laura Gustafson",
      "Chloe Rolland",
      "Nikhila Ravi",
      "Quentin Duval",
      "Aaron Adcock",
      "Cheng-Yang Fu",
      "Melissa Hall",
      "Candace Ross"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines performance disparities in computer vision models across demographic attributes such as gender and skin tone, highlighting issues of fairness and potential bias affecting different social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on fairness disparities in AI models",
      "affected_populations": [
        "women",
        "people of color"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Benchmarking"
      ],
      "methodology_detail": "Manual annotations and model performance analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.16681v3",
    "title": "One Model Many Scores: Using Multiverse Analysis to Prevent Fairness Hacking and Evaluate the Influence of Model Design Decisions",
    "year": 2023,
    "authors": [
      "Jan Simson",
      "Florian Pfisterer",
      "Christoph Kern"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in algorithmic decision-making, which are central to social inequalities such as health and social disparities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in AI systems affecting vulnerable groups",
      "affected_populations": [
        "vulnerable populations"
      ],
      "methodology": [
        "Multiverse Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes fairness metrics across decision combinations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.16549v2",
    "title": "Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection",
    "year": 2023,
    "authors": [
      "Fatma Elsafoury"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates bias in NLP models affecting hate speech detection, which relates to social discrimination and fairness issues impacting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on bias and fairness in AI systems",
      "affected_populations": [
        "minority groups",
        "socially marginalized"
      ],
      "methodology": [
        "Natural Language Processing",
        "Explainability",
        "Fairness Analysis"
      ],
      "methodology_detail": "Analyzes bias impact from multiple perspectives",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.15740v1",
    "title": "Beard Segmentation and Recognition Bias",
    "year": 2023,
    "authors": [
      "Kagan Ozturk",
      "Grace Bezold",
      "Aman Bhatta",
      "Haiyu Wu",
      "Kevin Bowyer"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in face recognition accuracy related to facial hair, highlighting bias across African-American and Caucasian groups, which relates to racial inequality and social bias in AI systems.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in facial recognition across racial groups",
      "affected_populations": [
        "African-American",
        "Caucasian"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Facial hair segmentation and bias evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.15668v1",
    "title": "Intersectional Inquiry, on the Ground and in the Algorithm",
    "year": 2023,
    "authors": [
      "Shanthi Robertson",
      "Liam Magee",
      "Karen Soldatić"
    ],
    "categories": [
      "cs.CY",
      "K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses intersectional social differences and biases in AI systems, including race, class, and disability, and examines their impacts through community engagement.",
      "inequality_type": [
        "race",
        "class",
        "disability"
      ],
      "other_detail": "Focus on intersectional bias in language models",
      "affected_populations": [
        "racial minorities",
        "disabled individuals",
        "social groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Analysis of Language Models"
      ],
      "methodology_detail": "Combines computational analysis with community workshops",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.14921v1",
    "title": "Gender bias and stereotypes in Large Language Models",
    "year": 2023,
    "authors": [
      "Hadas Kotek",
      "Rikker Dockum",
      "David Q. Sun"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in LLMs, highlighting societal gender stereotypes and their amplification, which directly relate to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in AI models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias testing and analysis of model responses",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.14415v1",
    "title": "Eleven Years of Gender Data Visualization: A Step Towards More Inclusive Gender Representation",
    "year": 2023,
    "authors": [
      "Florent Cabric",
      "Margrét Vilborg Bjarnadóttir",
      "Meng Ling",
      "Guðbjörg Linda Rafnsdóttir",
      "Petra Isenberg"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender representation and biases in data visualization, a social inequality dimension.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender visualization practices and biases",
      "affected_populations": [
        "gender minorities",
        "general public"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Literature Review",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzed visualizations across disciplines and sources",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.14049v1",
    "title": "Fairness and Privacy in Voice Biometrics:A Study of Gender Influences Using wav2vec 2.0",
    "year": 2023,
    "authors": [
      "Oubaida Chouchane",
      "Michele Panariello",
      "Chiara Galdi",
      "Massimiliano Todisco",
      "Nicholas Evans"
    ],
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender influences on fairness, privacy, and bias in voice biometric systems, addressing social bias and discrimination issues related to gender. It analyzes how AI systems may perpetuate or mitigate gender-based inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness and privacy in AI",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Adversarial Model Evaluation"
      ],
      "methodology_detail": "Fine-tuning wav2vec 2.0 for speaker verification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier and mitigator of bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.13591v1",
    "title": "Queering the ethics of AI",
    "year": 2023,
    "authors": [
      "Eduard Fosch-Villaronga",
      "Gianclaudio Malgieri"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "K.2; I.2.m"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The chapter discusses AI's role in perpetuating discrimination and inequalities, emphasizing intersectionality and vulnerable groups.",
      "inequality_type": [
        "gender",
        "racial",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focus on discrimination and intersectional inequalities",
      "affected_populations": [
        "discriminated groups",
        "vulnerable communities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Interdisciplinary approaches and critical analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.13415v1",
    "title": "An investigation into the impact of deep learning model choice on sex and race bias in cardiac MR segmentation",
    "year": 2023,
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Antón",
      "Bram Ruijsink",
      "Keana Aitcheson",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to sex and race in medical AI, highlighting disparities across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias in medical AI segmentation models",
      "affected_populations": [
        "patients by sex",
        "patients by race"
      ],
      "methodology": [
        "Experiment",
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates model bias severity and variation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.13242v1",
    "title": "Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness",
    "year": 2023,
    "authors": [
      "Sruthi Gorantla",
      "Eshaan Bhansali",
      "Amit Deshpande",
      "Anand Louis"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in ranking models, focusing on representation and ex-post fairness, which relate to social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI ranking systems",
      "affected_populations": [
        "underrepresented groups",
        "disadvantaged categories"
      ],
      "methodology": [
        "Algorithm Development",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Designing and testing a group-fair ranking model",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.13089v1",
    "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
    "year": 2023,
    "authors": [
      "Pranav Narayanan Venkit"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases in NLP models related to sociodemographic factors beyond race and gender, highlighting social bias and societal impacts. It emphasizes understanding bias's societal implications, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on sociodemographic biases in NLP models",
      "affected_populations": [
        "social groups",
        "minorities",
        "disadvantaged communities"
      ],
      "methodology": [
        "Literature Review",
        "Interdisciplinary Analysis"
      ],
      "methodology_detail": "Integrates multiple disciplinary perspectives on bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.12675v1",
    "title": "A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes",
    "year": 2023,
    "authors": [
      "Ario Sadafi",
      "Matthias Hehr",
      "Nassir Navab",
      "Carsten Marr"
    ],
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to age and sex in AI models, highlighting disparities affecting different social groups, which relates to social inequality and fairness issues.",
      "inequality_type": [
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias in medical AI affecting diverse patient groups",
      "affected_populations": [
        "female patients",
        "elderly patients"
      ],
      "methodology": [
        "Machine Learning",
        "Experimental",
        "Performance Evaluation"
      ],
      "methodology_detail": "Assessing bias impact on model performance across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.12539v3",
    "title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias",
    "year": 2023,
    "authors": [
      "Vipul Gupta",
      "Pranav Narayanan Venkit",
      "Hugo Laurençon",
      "Shomir Wilson",
      "Rebecca J. Passonneau"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring sociodemographic biases related to gender and race in language models, which are key aspects of social inequality. It discusses bias assessment across social groups, addressing fairness issues in AI systems. The work aims to quantify and compare biases that can impact different social populations.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on sociodemographic bias measurement",
      "affected_populations": [
        "women",
        "racial minorities",
        "social groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Develops diverse templates and prompts for bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.11840v1",
    "title": "Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition",
    "year": 2023,
    "authors": [
      "Pedro C. Neto",
      "Eduarda Caldeira",
      "Jaime S. Cardoso",
      "Ana F. Sequeira"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial biases in face recognition models, addressing social discrimination and fairness issues related to race. It analyzes how model compression impacts biases across ethnicity groups, highlighting social inequality concerns.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias reduction in AI face recognition systems",
      "affected_populations": [
        "ethnic minorities",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates model performance and bias across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.11732v1",
    "title": "(Un)fair Exposure in Deep Face Rankings at a Distance",
    "year": 2023,
    "authors": [
      "Andrea Atzori",
      "Gianni Fenu",
      "Mirko Marras"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in facial recognition affecting demographic groups, highlighting fairness issues related to race and potentially other social categories.",
      "inequality_type": [
        "racial",
        "demographic"
      ],
      "other_detail": "Bias in forensic face rankings",
      "affected_populations": [
        "racial minorities",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Evaluates bias across multiple face encoders and datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.10684v2",
    "title": "Systematic Offensive Stereotyping (SOS) Bias in Language Models",
    "year": 2023,
    "authors": [
      "Fatma Elsafoury"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to hate speech, which reflect societal discrimination against marginalized groups, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "disability"
      ],
      "other_detail": "Bias measurement and fairness in AI systems",
      "affected_populations": [
        "marginalized identities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Measuring and mitigating bias in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.09596v1",
    "title": "Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification",
    "year": 2023,
    "authors": [
      "Arpit Merchant",
      "Carlos Castillo"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in GNNs affecting protected demographic groups, focusing on fairness and disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI models for social groups",
      "affected_populations": [
        "protected demographic groups",
        "nodes in datasets"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness interventions and evaluation on datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.09726v1",
    "title": "Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health",
    "year": 2023,
    "authors": [
      "Jackson A. Killian",
      "Manish Jain",
      "Yugang Jia",
      "Jonathan Amar",
      "Erich Huang",
      "Milind Tambe"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and equity in decision-making algorithms, aiming to prevent disparities across groups, which directly relates to social inequalities such as health and resource access.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "disparity"
      ],
      "other_detail": "Focus on equitable outcomes in digital health systems",
      "affected_populations": [
        "patients",
        "underserved groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops algorithms and tests via simulations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.08710v1",
    "title": "A Framework for Designing Fair Ubiquitous Computing Systems",
    "year": 2023,
    "authors": [
      "Han Zhang",
      "Leijie Wang",
      "Yilun Sheng",
      "Xuhai Xu",
      "Jennifer Mankoff",
      "Anind K. Dey"
    ],
    "categories": [
      "cs.HC",
      "68U35",
      "H.5.2; I.2.m"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in ubiquitous systems impacting individuals, addressing bias and discrimination concerns, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "health",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic fairness and societal impact",
      "affected_populations": [
        "disadvantaged groups",
        "minorities",
        "vulnerable populations"
      ],
      "methodology": [
        "System Design",
        "Fairness-aware algorithms",
        "Evaluation criteria"
      ],
      "methodology_detail": "Integrates fairness considerations into system development",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.08482v1",
    "title": "Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features",
    "year": 2023,
    "authors": [
      "Yi Zhang",
      "Jitao Sang",
      "Junyang Wang",
      "Dongmei Jiang",
      "Yaowei Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models related to social attributes like gender and race, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "fairness"
      ],
      "other_detail": "Focuses on debiasing in visual recognition models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Intervention",
        "Experiment"
      ],
      "methodology_detail": "Debiasing via shortcut feature intervention",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.05441v1",
    "title": "Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation",
    "year": 2023,
    "authors": [
      "Hao Liang",
      "Pietro Perona",
      "Guha Balakrishnan"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in face recognition systems related to race and gender, which are social categories linked to inequality. It addresses social bias and fairness issues in AI systems affecting marginalized groups. The focus on racial and gender disparities indicates a direct connection to social inequality.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "Black",
        "East Asian"
      ],
      "methodology": [
        "Experiment",
        "Synthetic Data Creation",
        "Human Evaluation"
      ],
      "methodology_detail": "Manipulates attributes independently to assess bias causally",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.05224v1",
    "title": "Algorithmic Harms in Child Welfare: Uncertainties in Practice, Organization, and Street-level Decision-Making",
    "year": 2023,
    "authors": [
      "Devansh Saxena",
      "Shion Guha"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic impacts on decision-making in child welfare, highlighting fairness and systemic issues affecting vulnerable populations, which relate to social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on fairness and systemic impacts in child welfare",
      "affected_populations": [
        "children",
        "families",
        "social workers"
      ],
      "methodology": [
        "Ethnography",
        "Case Study"
      ],
      "methodology_detail": "Two-year ethnographic research at a child welfare agency",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.04674v1",
    "title": "Addressing Racial Bias in Facial Emotion Recognition",
    "year": 2023,
    "authors": [
      "Alex Fan",
      "Xingshuo Xiao",
      "Peter Washington"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in facial emotion recognition, highlighting disparities across racial groups, which relates directly to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial bias in AI systems",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes racial bias via dataset sampling and performance metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.04356v1",
    "title": "Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs",
    "year": 2023,
    "authors": [
      "Nickolas Littlefield",
      "Johannes F. Plate",
      "Kurt R. Weiss",
      "Ines Lohse",
      "Avani Chhabra",
      "Ismaeel A. Siddiqui",
      "Zoe Menezes",
      "George Mastorakos",
      "Sakshi Mehul Thakar",
      "Mehrnaz Abedian",
      "Matthew F. Gong",
      "Luke A. Carlson",
      "Hamidreza Moradi",
      "Soheyla Amirian",
      "Ahmad P. Tafti"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender and racial biases in medical AI, addressing social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Bias mitigation in medical imaging algorithms",
      "affected_populations": [
        "female patients",
        "racial minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Bias mitigation strategies"
      ],
      "methodology_detail": "Analyzes bias and proposes mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.04346v1",
    "title": "Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles",
    "year": 2023,
    "authors": [
      "Pranav Narayanan Venkit",
      "Sanjana Gautam",
      "Ruchi Panchanadikar",
      "Ting-Hao `Kenneth' Huang",
      "Shomir Wilson"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines nationality bias in NLP models, which influences societal perceptions and can perpetuate stereotypes, impacting social fairness and discrimination.",
      "inequality_type": [
        "nationality",
        "social bias"
      ],
      "other_detail": "Focuses on societal perception and bias in AI",
      "affected_populations": [
        "general public",
        "specific national groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Human evaluation and thematic analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02935v4",
    "title": "Bias Behind the Wheel: Fairness Testing of Autonomous Driving Systems",
    "year": 2023,
    "authors": [
      "Xinyue Li",
      "Zhenpeng Chen",
      "Jie M. Zhang",
      "Federica Sarro",
      "Ying Zhang",
      "Xuanzhe Liu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness issues related to demographic groups in autonomous driving, highlighting biases based on age and gender. It addresses social bias and fairness in AI systems affecting societal safety and equity. The focus on demographic disparities indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "age",
        "gender"
      ],
      "other_detail": "Demographic fairness in autonomous systems",
      "affected_populations": [
        "pedestrians",
        "children",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness testing across demographic groups using large datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02898v1",
    "title": "Elucidate Gender Fairness in Singing Voice Transcription",
    "year": 2023,
    "authors": [
      "Xiangming Gu",
      "Wei Zeng",
      "Ye Wang"
    ],
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in singing voice transcription, a social fairness issue affecting gender groups. It explores disparities in AI performance related to gender, aiming to improve fairness. This aligns with social inequality concerns in gender and algorithmic bias.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "male singers",
        "female singers"
      ],
      "methodology": [
        "Experiment",
        "Adversarial Training",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Bias reduction in AI system performance",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02887v2",
    "title": "The Impact of Group Membership Bias on the Quality and Fairness of Exposure in Ranking",
    "year": 2023,
    "authors": [
      "Ali Vardasbi",
      "Maarten de Rijke",
      "Fernando Diaz",
      "Mostafa Dehghani"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to sensitive attributes like gender affecting rankings, which relates to social discrimination and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias correction in ranking systems",
      "affected_populations": [
        "female candidates"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes bias impact and correction methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02129v2",
    "title": "Auditing Yelp's Business Ranking and Review Recommendation Through the Lens of Fairness",
    "year": 2023,
    "authors": [
      "Mohit Singhal",
      "Javier Pacheco",
      "Seyyed Mohammad Sadegh Moosavi Khorzooghi",
      "Tanushree Debi",
      "Abolfazl Asudeh",
      "Gautam Das",
      "Shirin Nilizadeh"
    ],
    "categories": [
      "cs.CY",
      "cs.DB"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in Yelp's algorithms related to user demographics and neighborhood characteristics, highlighting disparities affecting different social groups.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "demographic"
      ],
      "other_detail": "Bias against less-established users and neighborhood disparities",
      "affected_populations": [
        "small business owners",
        "less-established users",
        "residents in specific neighborhoods"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias assessment through data analysis of reviews and rankings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02053v2",
    "title": "The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations",
    "year": 2023,
    "authors": [
      "Abel Salinas",
      "Parth Vipul Shah",
      "Yuzhong Huang",
      "Robert McCormack",
      "Fred Morstatter"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases in LLMs related to gender and nationality, highlighting potential inequities in job recommendations affecting disadvantaged groups.",
      "inequality_type": [
        "gender",
        "nationality",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI affecting social and economic outcomes",
      "affected_populations": [
        "women",
        "Mexican workers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement via job recommendation analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.01948v1",
    "title": "A Multidimensional Analysis of Social Biases in Vision Transformers",
    "year": 2023,
    "authors": [
      "Jannik Brinkmann",
      "Paul Swoboda",
      "Christian Bartelt"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases such as racism and sexism in AI models, which are related to social discrimination and inequality. It analyzes how model design influences biases that impact different social groups. The focus on social biases in AI systems directly addresses issues of social inequality.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on social biases in AI representations",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring bias impacts in model embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.00755v2",
    "title": "The Bias Amplification Paradox in Text-to-Image Generation",
    "year": 2023,
    "authors": [
      "Preethi Seshadri",
      "Sameer Singh",
      "Yanai Elazar"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender in AI-generated images, highlighting social discrimination issues. It analyzes how biases in training data can reinforce stereotypes, impacting social groups. The focus on gender bias and its amplification relates directly to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias amplification in gender-occupation stereotypes",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Comparing gender ratios in data and generated images",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02680v1",
    "title": "Fair Models in Credit: Intersectional Discrimination and the Amplification of Inequity",
    "year": 2023,
    "authors": [
      "Savina Kim",
      "Stefan Lessmann",
      "Galina Andreeva",
      "Michael Rovatsos"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in credit access across social groups defined by gender, age, marital status, and children, highlighting intersectional discrimination. It discusses algorithmic bias affecting vulnerable populations. The focus on social categories and fairness issues confirms its relevance to social inequality.",
      "inequality_type": [
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Intersectional analysis of credit discrimination",
      "affected_populations": [
        "vulnerable groups",
        "credit seekers"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes credit data with ML models",
      "geographic_focus": [
        "Spain"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.00405v1",
    "title": "Who benefits from altmetrics? The effect of team gender composition on the link between online visibility and citation impact",
    "year": 2023,
    "authors": [
      "Orsolya Vásárhelyi",
      "Emőke-Ágnes Horvát"
    ],
    "categories": [
      "cs.CY",
      "J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in online visibility and citation impact, directly addressing gender-based inequality in academic recognition.",
      "inequality_type": [
        "gender",
        "informational"
      ],
      "other_detail": "Focuses on gender disparities in scientific dissemination",
      "affected_populations": [
        "women researchers"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses Coarsened Exact Matching for causal inference",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.16778v2",
    "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
    "year": 2023,
    "authors": [
      "Jiho Jin",
      "Jiseon Kim",
      "Nayeon Lee",
      "Haneul Yoo",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases in language models, reflecting societal stereotypes and discrimination, particularly in cultural contexts.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "educational",
        "social bias"
      ],
      "other_detail": "Cultural bias assessment in AI models",
      "affected_populations": [
        "Korean social groups",
        "cultural minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Survey",
        "Natural Language Processing"
      ],
      "methodology_detail": "Collecting and validating biases via survey and dataset construction",
      "geographic_focus": [
        "South Korea"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.02399v2",
    "title": "The Glamorisation of Unpaid Labour: AI and its Influencers",
    "year": 2023,
    "authors": [
      "Nana Mgbechikwere Nwachukwu",
      "Jennafer Shae Roberts",
      "Laura N Montoya"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how digital labor and data practices disproportionately impact workers in Africa, Latin America, and India, highlighting socioeconomic and geographic inequalities. It addresses exploitation and ethical concerns related to marginalized groups affected by AI-driven digital platforms. The focus on unequal impacts and societal harms indicates a clear engagement with social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on Global South workers and digital exploitation",
      "affected_populations": [
        "workers in Africa",
        "workers in Latin America",
        "workers in India"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing ethical practices and societal impacts",
      "geographic_focus": [
        "Africa",
        "Latin America",
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.15466v1",
    "title": "LUCID-GAN: Conditional Generative Models to Locate Unfairness",
    "year": 2023,
    "authors": [
      "Andres Algaba",
      "Carmen Mazijn",
      "Carina Prunkl",
      "Jan Danckaert",
      "Vincent Ginis"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting biases and unfairness in AI models, which relate to social discrimination issues such as fairness and bias. It discusses uncovering unethical biases that can impact different social groups. The emphasis on bias detection aligns with addressing social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias detection in AI models",
      "affected_populations": [
        "social groups",
        "discriminated groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Uses generative models to detect biases",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.13616v1",
    "title": "AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling",
    "year": 2023,
    "authors": [
      "Marguerite Sauce",
      "Antoine Chancel",
      "Antoine Ly"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination and fairness issues in risk modeling, related to social groups such as race and gender, within the context of insurance algorithms.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on indirect discrimination mitigation in AI models",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Mathematical Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Linear algebra techniques for fairness in risk models",
      "geographic_focus": [
        "Europe"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.13714v1",
    "title": "Diversity and Language Technology: How Techno-Linguistic Bias Can Cause Epistemic Injustice",
    "year": 2023,
    "authors": [
      "Paula Helm",
      "Gábor Bella",
      "Gertraud Koch",
      "Fausto Giunchiglia"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in language technology affecting marginalized communities and their worldviews, highlighting sociopolitical consequences and underrepresentation.",
      "inequality_type": [
        "linguistic",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Bias in technology design affecting marginalized language groups",
      "affected_populations": [
        "language communities",
        "marginalized groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing sociopolitical implications of techno-linguistic bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.13408v2",
    "title": "The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking",
    "year": 2023,
    "authors": [
      "Savina Dine Kim",
      "Galina Andreeva",
      "Michael Rovatsos"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines indirect discrimination and fairness issues affecting disadvantaged groups through financial vulnerability, highlighting social disparities related to socioeconomic status and potentially protected characteristics.",
      "inequality_type": [
        "socioeconomic",
        "financial",
        "informational"
      ],
      "other_detail": "Focus on fairness and discrimination in financial data",
      "affected_populations": [
        "disadvantaged groups",
        "at-risk individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Clustering",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using transaction data and classifiers to identify vulnerabilities",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.13405v1",
    "title": "Towards Bridging the Digital Language Divide",
    "year": 2023,
    "authors": [
      "Gábor Bella",
      "Paula Helm",
      "Gertraud Koch",
      "Fausto Giunchiglia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses linguistic bias in AI, highlighting unequal impacts on language communities, which relates to social inequality issues.",
      "inequality_type": [
        "linguistic",
        "cultural",
        "informational"
      ],
      "other_detail": "Focuses on language diversity and community needs",
      "affected_populations": [
        "language communities",
        "under-resourced speakers"
      ],
      "methodology": [
        "System Design",
        "Collaboration",
        "Literature Review"
      ],
      "methodology_detail": "Community-based approach to reduce bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.14366v2",
    "title": "Explainable Disparity Compensation for Efficient Fair Ranking",
    "year": 2023,
    "authors": [
      "Abraham Gale",
      "Amélie Marian"
    ],
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in ranking outcomes affecting underrepresented groups, focusing on fairness and transparency. It discusses bias mitigation in decision systems, which relates to social discrimination issues.",
      "inequality_type": [
        "educational",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in decision-making systems",
      "affected_populations": [
        "underrepresented groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Sampling-based algorithms for bonus point calculation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2308.00071v3",
    "title": "On The Role of Reasoning in the Identification of Subtle Stereotypes in Natural Language",
    "year": 2023,
    "authors": [
      "Jacob-Junqi Tian",
      "Omkar Dige",
      "D. B. Emerson",
      "Faiza Khan Khattak"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "68T50"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting stereotypes in language models, which relate to social biases and discrimination. It emphasizes addressing biases that reinforce harmful stereotypes, impacting social groups. The work aims to improve fairness and interpretability in AI systems concerning social biases.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focus on stereotypes and social biases in language models",
      "affected_populations": [
        "social groups",
        "minority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Qualitative Study",
        "Experiment"
      ],
      "methodology_detail": "Analyzing reasoning traces for stereotype detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.12065v1",
    "title": "Spectral Normalized-Cut Graph Partitioning with Fairness Constraints",
    "year": 2023,
    "authors": [
      "Jia Li",
      "Yanhao Wang",
      "Arpit Merchant"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints related to demographic groups in graph partitioning, focusing on equitable representation, which relates to social fairness issues.",
      "inequality_type": [
        "gender",
        "race",
        "ethnic",
        "social fairness"
      ],
      "other_detail": "Fairness in algorithmic graph partitioning",
      "affected_populations": [
        "demographic groups",
        "social groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Spectral Analysis",
        "Experimental Evaluation"
      ],
      "methodology_detail": "Fairness-aware spectral clustering algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.11298v1",
    "title": "A First Look at Fairness of Machine Learning Based Code Reviewer Recommendation",
    "year": 2023,
    "authors": [
      "Mohammad Mahdi Mohajer",
      "Alvine Boaye Belle",
      "Nima Shiri harzevili",
      "Junjie Wang",
      "Hadi Hemmati",
      "Song Wang",
      "Zhen Ming",
      "Jiang"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness issues related to gender bias in ML systems, highlighting discrimination against female reviewers.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI-based code reviewer recommendations",
      "affected_populations": [
        "female reviewers",
        "male reviewers"
      ],
      "methodology": [
        "Empirical Study",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes fairness and bias in ML recommendation systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10749v1",
    "title": "Mitigating Voter Attribute Bias for Fair Opinion Aggregation",
    "year": 2023,
    "authors": [
      "Ryosuke Ueda",
      "Koh Takeuchi",
      "Hisashi Kashima"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in opinion aggregation related to voter attributes like gender and race, which are social categories linked to inequality. It discusses fairness and bias mitigation in AI systems, impacting social groups. The focus on fairness in decision-making processes relates directly to social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Bias mitigation in opinion aggregation",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Soft label estimation and fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10522v1",
    "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
    "year": 2023,
    "authors": [
      "Somayeh Ghanbarzadeh",
      "Yan Huang",
      "Hamid Palangi",
      "Radames Cruz Moreno",
      "Hamed Khanpour"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social bias. It focuses on debiasing techniques to reduce societal gender stereotypes propagated by AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Fine-tuning with integrated MLM objectives",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10011v1",
    "title": "Towards Fair Face Verification: An In-depth Analysis of Demographic Biases",
    "year": 2023,
    "authors": [
      "Ioannis Sarridis",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Christos Diou"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in face recognition systems related to race, age, and gender, highlighting social disparities. It discusses demographic performance disparities and their societal implications. The focus on intersectionality and fairness issues indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "racial",
        "age",
        "gender"
      ],
      "other_detail": "Intersectional bias analysis in AI systems",
      "affected_populations": [
        "racial groups",
        "older adults",
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Metrics for bias and disparity measurement",
      "geographic_focus": [
        "Racial Faces in-the-Wild (RFW) benchmark"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.09209v1",
    "title": "Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models",
    "year": 2023,
    "authors": [
      "Pranav Narayanan Venkit",
      "Mukund Srinath",
      "Shomir Wilson"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines explicit disability bias in AI models, addressing social discrimination against PWD.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Explicit bias in AI models against disabled people",
      "affected_populations": [
        "people with disability"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and quantification in social media conversations",
      "geographic_focus": [
        "Twitter",
        "Reddit"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.09162v3",
    "title": "Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications",
    "year": 2023,
    "authors": [
      "Vishesh Thakur"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in AI language models, addressing social discrimination and bias issues related to gender, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Evaluates gender bias in generated text outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.08580v1",
    "title": "The Resume Paradox: Greater Language Differences, Smaller Pay Gaps",
    "year": 2023,
    "authors": [
      "Joshua R. Minot",
      "Marc Maier",
      "Bradford Demarest",
      "Nicholas Cheney",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds",
      "Morgan R. Frank"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in resumes and their relation to pay gaps, addressing gender-based social inequality. It analyzes self-representation and wage disparities, focusing on gender bias in employment. The study directly relates to social discrimination and inequality in the labor market.",
      "inequality_type": [
        "gender",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender pay gap and resume language",
      "affected_populations": [
        "women",
        "female workers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes resume language and wage data statistically",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.08368v1",
    "title": "Gender mobility in the labor market with skills-based matching models",
    "year": 2023,
    "authors": [
      "Ajaya Adhikari",
      "Steven Vethman",
      "Daan Vos",
      "Marc Lenz",
      "Ioana Cocu",
      "Ioannis Tolios",
      "Cor J. Veenman"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender segregation and bias in skills representation and matching models, directly addressing gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias propagation in AI-based labor market models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes language models and simulated data for bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.08722v1",
    "title": "Certifying the Fairness of KNN in the Presence of Dataset Bias",
    "year": 2023,
    "authors": [
      "Yannan Li",
      "Jingbo Wang",
      "Chao Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness certification of AI algorithms in biased datasets, addressing social bias and discrimination issues related to protected groups.",
      "inequality_type": [
        "racial",
        "ethnic",
        "disability",
        "gender"
      ],
      "other_detail": "Fairness in AI classification under dataset bias",
      "affected_populations": [
        "minority groups",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness certification and abstract interpretation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.08232v1",
    "title": "Learning for Counterfactual Fairness from Observational Data",
    "year": 2023,
    "authors": [
      "Jing Ma",
      "Ruocheng Guo",
      "Aidong Zhang",
      "Jundong Li"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on biases related to sensitive attributes like race and gender, which are central to social inequalities. It aims to mitigate biases that can lead to unfair treatment of social groups. The emphasis on counterfactual fairness relates directly to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI systems addressing social bias",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Representation Learning",
        "Causal Inference"
      ],
      "methodology_detail": "Counterfactual data augmentation and invariant penalty",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.08025v1",
    "title": "Analysing Gender Bias in Text-to-Image Models using Object Detection",
    "year": 2023,
    "authors": [
      "Harvey Mannering"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI-generated images, highlighting social gender stereotypes and discrimination. It analyzes how AI models reinforce gendered associations with objects, reflecting societal biases.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI-generated imagery",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Object detection and bias measurement in images",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10223v2",
    "title": "Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms",
    "year": 2023,
    "authors": [
      "Organizers of QueerInAI",
      "Nathan Dennler",
      "Anaelia Ovalle",
      "Ashwin Singh",
      "Luca Soldaini",
      "Arjun Subramonian",
      "Huy Tu",
      "William Agnew",
      "Avijit Ghosh",
      "Kyra Yee",
      "Irene Font Peradejordi",
      "Zeerak Talat",
      "Mayra Russo",
      "Jess de Jesus de Pinho Pinhal"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases and harms in AI affecting marginalized communities, specifically queer groups, highlighting social discrimination and power dynamics.",
      "inequality_type": [
        "gender",
        "social bias",
        "discrimination"
      ],
      "other_detail": "Focus on queer community perspectives in AI auditing",
      "affected_populations": [
        "queer communities"
      ],
      "methodology": [
        "Participatory Workshop",
        "Qualitative Study"
      ],
      "methodology_detail": "Community critique and redesign of bias bounties",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10213v1",
    "title": "Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts",
    "year": 2023,
    "authors": [
      "Shaina Raza",
      "Chen Ding",
      "Deval Pandya"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses hate speech, which involves social biases related to race, gender, and religion, and aims to reduce discriminatory language online.",
      "inequality_type": [
        "racial",
        "gender",
        "religious"
      ],
      "other_detail": "Focuses on social bias in online conversations",
      "affected_populations": [
        "targeted groups",
        "online users"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and prompt-based debiasing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10201v2",
    "title": "On the Mechanics of NFT Valuation: AI Ethics and Social Media",
    "year": 2023,
    "authors": [
      "Luyao Zhang",
      "Yutong Sun",
      "Yutong Quan",
      "Jiaxun Cao",
      "Xin Tong"
    ],
    "categories": [
      "cs.CY",
      "cs.DC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and skin tone biases affecting NFT valuation, highlighting social disparities.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in digital asset valuation",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Empirical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes social media, blockchain, and exchange data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.08624v1",
    "title": "National Origin Discrimination in Deep-learning-powered Automated Resume Screening",
    "year": 2023,
    "authors": [
      "Sihang Li",
      "Kuangzheng Li",
      "Haibing Lu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI systems used for resume screening, highlighting potential discrimination against demographic groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI systems affecting social groups",
      "affected_populations": [
        "racial minorities",
        "women",
        "job candidates"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Bias Mitigation"
      ],
      "methodology_detail": "Bias mitigation techniques validated on real resumes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.05862v2",
    "title": "Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes",
    "year": 2023,
    "authors": [
      "Connor Toups",
      "Rishi Bommasani",
      "Kathleen A. Creel",
      "Sarah H. Bana",
      "Dan Jurafsky",
      "Percy Liang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in machine learning outcomes across social groups, highlighting racial disparities and systemic failures affecting marginalized populations.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on societal impact of ML deployments",
      "affected_populations": [
        "racial minorities",
        "patients"
      ],
      "methodology": [
        "Ecosystem-level Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes model collections across datasets and modalities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.05842v4",
    "title": "The Butterfly Effect in Artificial Intelligence Systems: Implications for AI Bias and Fairness",
    "year": 2023,
    "authors": [
      "Emilio Ferrara"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses how small biases in AI can lead to unfair outcomes affecting underrepresented groups, highlighting issues of bias and fairness related to social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on bias amplification and fairness in AI systems",
      "affected_populations": [
        "underrepresented individuals",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Empirical Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Strategies to detect and mitigate bias effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.05029v1",
    "title": "FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms",
    "year": 2023,
    "authors": [
      "Normen Yu",
      "Gang Tan",
      "Saeid Tizpaz-Niari"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models affecting social decisions, highlighting social bias and unfairness impacting marginalized groups.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on algorithmic fairness and social impact",
      "affected_populations": [
        "minority groups",
        "general public"
      ],
      "methodology": [
        "Machine Learning",
        "System Design",
        "Experiment"
      ],
      "methodology_detail": "GUI development and fairness testing tools",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.04865v1",
    "title": "Social inequalities that matter for contact patterns, vaccination, and the spread of epidemics",
    "year": 2023,
    "authors": [
      "Adriana Manna",
      "Júlia Koltai",
      "Márton Karsai"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines how socio-demographic and economic factors influence contact and vaccination behaviors, highlighting disparities among social groups during the pandemic.",
      "inequality_type": [
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on social disparities in epidemic behaviors",
      "affected_populations": [
        "disadvantaged groups",
        "privileged groups"
      ],
      "methodology": [
        "Data Analysis",
        "Epidemiological Modeling"
      ],
      "methodology_detail": "Incorporates behavioral differences into epidemic models",
      "geographic_focus": [
        "Hungary"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.04247v1",
    "title": "VR Job Interview Using a Gender-Swapped Avatar",
    "year": 2023,
    "authors": [
      "Jieun Kim",
      "Hauke Sandhaus",
      "Susan R. Fussell"
    ],
    "categories": [
      "cs.HC",
      "H.5.m"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study addresses gender bias in job interviews and promotes diversity, inclusion, and reduction of bias, which are related to social inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in hiring processes",
      "affected_populations": [
        "job applicants",
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Semi-structured interview"
      ],
      "methodology_detail": "Mixed-method approach combining lab experiment and interviews",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.10200v1",
    "title": "Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings",
    "year": 2023,
    "authors": [
      "Sujan Dutta",
      "Parth Srivastava",
      "Vaishnavi Solunke",
      "Swaprava Nath",
      "Ashiqur R. KhudaBukhsh"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender inequality in divorce proceedings, highlighting societal disparities. It analyzes societal norms and biases reflected in legal cases. The focus on gender and societal norms confirms its relevance to social inequality.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender disparities in legal contexts",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing court proceedings for bias and inequality indicators",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.03360v1",
    "title": "Evaluating Biased Attitude Associations of Language Models in an Intersectional Context",
    "year": 2023,
    "authors": [
      "Shiva Omrani Sabbaghi",
      "Robert Wolfe",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, race, social class, and sexual orientation in language models, which reflect social inequalities. It analyzes how AI systems perpetuate and encode societal biases. The focus on intersectional biases and underrepresented groups further emphasizes social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "social class",
        "sexual orientation"
      ],
      "other_detail": "Focus on intersectional social biases in AI",
      "affected_populations": [
        "gender minorities",
        "racial groups",
        "social classes",
        "sexual minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement via embedding association tests",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.03306v2",
    "title": "When Fair Classification Meets Noisy Protected Attributes",
    "year": 2023,
    "authors": [
      "Avijit Ghosh",
      "Pablo Kvitca",
      "Christo Wilson"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in classification algorithms, addressing issues related to protected attributes such as race and gender, which are central to social inequality and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on fairness and protected attribute reliability",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Case Study",
        "Synthetic Perturbations",
        "Algorithm Evaluation"
      ],
      "methodology_detail": "Comparative analysis of fairness algorithms under noisy conditions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.03083v1",
    "title": "Predicting Opioid Use Outcomes in Minoritized Communities",
    "year": 2023,
    "authors": [
      "Abhay Goyal",
      "Nimay Parekh",
      "Lam Yin Cheung",
      "Koustuv Saha",
      "Frederick L Altice",
      "Robin O'hanlon",
      "Roger Ho Chun Man",
      "Christian Poellabauer",
      "Honoria Guarino",
      "Pedro Mateu Gelabert",
      "Navin Kumar"
    ],
    "categories": [
      "cs.CE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models predicting opioid use among minoritized communities, highlighting racial and social disparities. It discusses how data representation affects prediction accuracy for marginalized groups. The focus on systemic biases and social group impacts confirms its relevance to social inequality.",
      "inequality_type": [
        "racial",
        "ethnic",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI models for marginalized communities",
      "affected_populations": [
        "minoritized communities",
        "young adults"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias assessment and prediction accuracy analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.02891v2",
    "title": "BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables",
    "year": 2023,
    "authors": [
      "Ruta Binkyte",
      "Daniele Gorla",
      "Catuscia Palamidessi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making, focusing on bias related to sensitive attributes like race or gender, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias correction in AI decision systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Bayesian Inference",
        "Expectation-Maximization",
        "Statistical Analysis"
      ],
      "methodology_detail": "Estimating latent variables for fairness correction",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.02726v1",
    "title": "Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching",
    "year": 2023,
    "authors": [
      "Nima Shahbazi",
      "Nikola Danevski",
      "Fatemeh Nargesian",
      "Abolfazl Asudeh",
      "Divesh Srivastava"
    ],
    "categories": [
      "cs.DB",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in entity matching, highlighting biases related to demographic groups, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic group fairness in data matching",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Auditing fairness in social datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.01693v1",
    "title": "Racial Bias Trends in the Text of US Legal Opinions",
    "year": 2023,
    "authors": [
      "Rohan Jinturkar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in legal language, highlighting social discrimination. It analyzes implicit racial bias over time and regions, directly addressing racial inequality. The focus on racial bias in judicial opinions relates to social inequality issues.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "",
      "affected_populations": [
        "Black individuals",
        "White individuals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Word embeddings to measure implicit racial bias",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.01503v1",
    "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
    "year": 2023,
    "authors": [
      "Aniket Vashishtha",
      "Kabir Ahuja",
      "Sunayana Sitaram"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender biases in multilingual language models, highlighting social bias issues related to gender discrimination across languages.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Creating benchmarks and evaluating bias mitigation techniques",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.01384v1",
    "title": "Systematic Bias in Sample Inference and its Effect on Machine Learning",
    "year": 2023,
    "authors": [
      "Owen O'Neill",
      "Fintan Costello"
    ],
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses underprediction biases affecting minority groups, highlighting social disparities in AI predictions related to race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias due to small sample inference",
      "affected_populations": [
        "minority groups",
        "women",
        "men"
      ],
      "methodology": [
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias correlations in ML models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.01311v1",
    "title": "Autonomous Vehicles for All?",
    "year": 2023,
    "authors": [
      "Sakib Mahmud Khan",
      "M Sabbir Salek",
      "Vareva Harris",
      "Gurcan Comert",
      "Eric Morris",
      "Mashrur Chowdhury"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses fairness, equity, access disparities, bias, and societal impacts of AVs, indicating a focus on social inequalities.",
      "inequality_type": [
        "economic",
        "racial",
        "digital",
        "geographic",
        "social"
      ],
      "other_detail": "Focus on fairness, access, and bias in AV deployment",
      "affected_populations": [
        "lower-income groups",
        "racial minorities",
        "drivers",
        "general public"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing social and ethical implications of AVs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.05333v2",
    "title": "Wearable-based Fair and Accurate Pain Assessment Using Multi-Attribute Fairness Loss in Convolutional Neural Networks",
    "year": 2023,
    "authors": [
      "Yidong Zhu",
      "Shao-Hsien Liu",
      "Mohammad Arif Ul Alam"
    ],
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models predicting pain, focusing on disparities across protected groups such as gender and ethnicity, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Fairness in AI-based health assessments",
      "affected_populations": [
        "gender groups",
        "ethnic groups",
        "patients"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Fairness Loss Optimization"
      ],
      "methodology_detail": "Multi-attribute fairness loss in CNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2307.00101v1",
    "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models",
    "year": 2023,
    "authors": [
      "Harnoor Dhingra",
      "Preetiha Jayashanker",
      "Sayali Moghe",
      "Emma Strubell"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases against marginalized groups, specifically LGBTQIA+ community, in AI-generated text, addressing social discrimination and bias issues.",
      "inequality_type": [
        "gender",
        "sexual"
      ],
      "other_detail": "Focus on sexual identity stereotypes in AI",
      "affected_populations": [
        "queer people",
        "LGBTQIA+"
      ],
      "methodology": [
        "Natural Language Processing",
        "Bias Analysis",
        "Post-hoc Debiasing"
      ],
      "methodology_detail": "Using regard scores and SHAP analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.16635v3",
    "title": "Improving Fairness in Deepfake Detection",
    "year": 2023,
    "authors": [
      "Yan Ju",
      "Shu Hu",
      "Shan Jia",
      "George H. Chen",
      "Siwei Lyu"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in deepfake detection related to race and gender, impacting fairness across social groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on algorithmic fairness in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Develops loss functions to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.16552v1",
    "title": "Learning Fair Classifiers via Min-Max F-divergence Regularization",
    "year": 2023,
    "authors": [
      "Meiyu Zhong",
      "Ravi Tandon"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.IT",
      "math.IT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing social bias and discrimination issues related to sensitive attributes such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness regularization and empirical evaluation on real datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.15299v1",
    "title": "FAIRER: Fairness as Decision Rationale Alignment",
    "year": 2023,
    "authors": [
      "Tianlin Li",
      "Qing Guo",
      "Aishan Liu",
      "Mengnan Du",
      "Zhiming Li",
      "Yang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI, focusing on subgroup disparities such as gender, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Fairness in AI decision processes",
      "affected_populations": [
        "males",
        "females"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Neuron influence analysis and fairness regularization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.15298v1",
    "title": "Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task",
    "year": 2023,
    "authors": [
      "Sophie Jentzsch",
      "Cigdem Turan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, highlighting social gender inequalities and biases in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and model analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.15133v1",
    "title": "The Perspective of Software Professionals on Algorithmic Racism",
    "year": 2023,
    "authors": [
      "Ronnie de Souza Santos",
      "Luiz Fernando de Lima",
      "Cleyton Magalhaes"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic racism, which involves racial discrimination and social bias in technology affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on racial discrimination in software systems",
      "affected_populations": [
        "Black people"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study",
        "Descriptive Statistics"
      ],
      "methodology_detail": "Analyzed software professionals' perspectives on racial bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.15087v2",
    "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
    "year": 2023,
    "authors": [
      "Virginia K. Felkner",
      "Ho-Chun Herbert Chang",
      "Eugene Jang",
      "Jonathan May"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI systems that harm marginalized LGBTQ+ communities, reflecting social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Bias in AI models affecting marginalized groups",
      "affected_populations": [
        "LGBTQ+ community"
      ],
      "methodology": [
        "Community-sourced benchmark",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Community-driven bias measurement and mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.14858v2",
    "title": "Proportional Aggregation of Preferences for Sequential Decision Making",
    "year": 2023,
    "authors": [
      "Nikhil Chandak",
      "Shashwat Goel",
      "Dominik Peters"
    ],
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair decision-making and equitable preference aggregation, aiming to improve representation across social groups. It discusses demographic utility distribution and ethical dilemmas, indicating concern with social fairness. The methods address social bias and fairness in AI systems.",
      "inequality_type": [
        "social",
        "educational",
        "demographic"
      ],
      "other_detail": "Focus on equitable decision-making across social groups",
      "affected_populations": [
        "demographics",
        "users",
        "voters"
      ],
      "methodology": [
        "Algorithm Design",
        "Empirical Analysis",
        "Preference Modeling"
      ],
      "methodology_detail": "Includes synthetic data, election data, and moral dilemmas",
      "geographic_focus": [
        "US",
        "International"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.14518v2",
    "title": "Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis",
    "year": 2023,
    "authors": [
      "Ching-Hao Chiu",
      "Hao-Wei Chung",
      "Yu-Jen Chen",
      "Yiyu Shi",
      "Tsung-Yi Ho"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in medical AI, addressing bias affecting underprivileged populations.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Fairness in medical diagnosis AI systems",
      "affected_populations": [
        "underprivileged patients",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware multi-exit neural network framework",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.13141v2",
    "title": "On Hate Scaling Laws For Data-Swamps",
    "year": 2023,
    "authors": [
      "Abeba Birhane",
      "Vinay Prabhu",
      "Sang Han",
      "Vishnu Naresh Boddeti"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal biases, racial stereotypes, and hate content in datasets and models, highlighting disparities affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Focus on societal biases in AI models",
      "affected_populations": [
        "Black female",
        "Black male",
        "human faces"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Auditing datasets and model biases quantitatively",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.13064v1",
    "title": "Auditing Predictive Models for Intersectional Biases",
    "year": 2023,
    "authors": [
      "Kate S. Boxer",
      "Edward McFowland III",
      "Daniel B. Neill"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in predictive models affecting protected social groups, highlighting fairness issues related to race and potentially other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on intersectional biases in AI models",
      "affected_populations": [
        "racial minorities",
        "gender groups",
        "socially disadvantaged"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Detects biases in classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.12912v1",
    "title": "Mitigating Discrimination in Insurance with Wasserstein Barycenters",
    "year": 2023,
    "authors": [
      "Arthur Charpentier",
      "François Hu",
      "Philipp Ratz"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination in insurance related to sensitive social features like gender and race, and discusses bias mitigation in AI models, directly engaging with social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias mitigation in predictive models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using Wasserstein barycenters for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.12424v3",
    "title": "VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution",
    "year": 2023,
    "authors": [
      "Siobhan Mackenzie Hall",
      "Fernanda Gonçalves Abrantes",
      "Hanwen Zhu",
      "Grace Sodunke",
      "Aleksandar Shtedritski",
      "Hannah Rose Kirk"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, focusing on social discrimination related to gender roles and representation, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Benchmarking"
      ],
      "methodology_detail": "Evaluates bias in vision-language models using a new dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.13675v1",
    "title": "Intersectionality and Testimonial Injustice in Medical Records",
    "year": 2023,
    "authors": [
      "Kenya S. Andrews",
      "Bhuvani Shah",
      "Lu Cheng"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines testimonial injustice in medical records, focusing on intersectional demographic disparities, which relate to social discrimination and bias in healthcare.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on intersectionality in healthcare disparities",
      "affected_populations": [
        "patients with multiple identities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Fairness Metrics",
        "Empirical Study"
      ],
      "methodology_detail": "Analyzes real-world medical data for disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.12444v1",
    "title": "Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials",
    "year": 2023,
    "authors": [
      "Malikeh Ehghaghi",
      "Marija Stanojevic",
      "Ali Akram",
      "Jekaterina Novikova"
    ],
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in ASV performance across demographic groups, highlighting fairness concerns related to voice biometrics. It examines how factors like age, gender, native language, and disease severity impact system accuracy, indicating social bias considerations.",
      "inequality_type": [
        "gender",
        "age",
        "linguistic",
        "health"
      ],
      "other_detail": "Fairness implications in biometric AI systems",
      "affected_populations": [
        "older adults",
        "non-native speakers",
        "female speakers",
        "individuals with AD"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes ASV performance across demographic and audio quality factors",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.11181v1",
    "title": "Insufficiently Justified Disparate Impact: A New Criterion for Subgroup Fairness",
    "year": 2023,
    "authors": [
      "Neil Menghani",
      "Edward McFowland III",
      "Daniel B. Neill"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in algorithmic decision-making, focusing on disparities across social groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness disparities in AI recommendations",
      "affected_populations": [
        "racial minorities",
        "low-income groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Detects and mitigates disparities in AI predictions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.11132v1",
    "title": "Fairness-aware Message Passing for Graph Neural Networks",
    "year": 2023,
    "authors": [
      "Huaisheng Zhu",
      "Guoji Fu",
      "Zhimeng Guo",
      "Zhiwei Zhang",
      "Teng Xiao",
      "Suhang Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on societal biases related to sensitive attributes, which are linked to social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on algorithmic fairness in social contexts",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Framework Development",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Designs fairness-aware message passing framework for GNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.10807v1",
    "title": "The Myth of Meritocracy and the Matilda Effect in STEM: Paper Acceptance and Paper Citation",
    "year": 2023,
    "authors": [
      "Joana Fonseca"
    ],
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "Addresses gender bias in STEM academia, focusing on discrimination and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women in STEM"
      ],
      "methodology": [
        "Literature Review",
        "Proposed Reforms"
      ],
      "methodology_detail": "Suggests modifications to peer review and citation practices",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.10530v1",
    "title": "Gender Bias in Transformer Models: A comprehensive survey",
    "year": 2023,
    "authors": [
      "Praneeth Nemani",
      "Yericherla Deepak Joel",
      "Palla Vijay",
      "Farhana Ferdousi Liza"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, highlighting social discrimination issues related to gender. It discusses the impact of bias on downstream applications, emphasizing fairness and equity concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "review of bias measurement methods and metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.10123v2",
    "title": "Dual Node and Edge Fairness-Aware Graph Partition",
    "year": 2023,
    "authors": [
      "Tingwei Liu",
      "Peizhao Li",
      "Hongfu Liu"
    ],
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in social network graph partitioning, focusing on demographic groups and biases in edges, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on social fairness in network analysis",
      "affected_populations": [
        "demographic groups",
        "social network users"
      ],
      "methodology": [
        "Graph Partitioning",
        "Fairness Analysis",
        "Representation Learning"
      ],
      "methodology_detail": "Dual node and edge fairness-aware framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.09752v1",
    "title": "Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models",
    "year": 2023,
    "authors": [
      "Victor Steinborn",
      "Antonis Maronikolakis",
      "Hinrich Schütze"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, highlighting social discrimination and bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in language models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes prediction probabilities and creates attack datasets",
      "geographic_focus": [
        "Japan",
        "Korea"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.09711v1",
    "title": "A study on group fairness in healthcare outcomes for nursing home residents during the COVID-19 pandemic in the Basque Country",
    "year": 2023,
    "authors": [
      "Hristo Inouzhe",
      "Irantzu Barrio",
      "Paula Gordaliza",
      "María Xosé Rodríguez-Álvarez",
      "Itxaso Bengoechea",
      "José María Quintana"
    ],
    "categories": [
      "stat.ME",
      "cs.CY",
      "stat.AP",
      "62P10"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in healthcare outcomes based on nursing home status, a social group, during COVID-19, highlighting issues of fairness and potential social discrimination.",
      "inequality_type": [
        "health",
        "age",
        "geographic"
      ],
      "other_detail": "Focus on elderly in nursing homes during pandemic",
      "affected_populations": [
        "nursing home residents",
        "non-residents"
      ],
      "methodology": [
        "Statistical Analysis",
        "Causality",
        "Fair Learning"
      ],
      "methodology_detail": "Combines established and new causal/fairness techniques",
      "geographic_focus": [
        "Basque Country"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.09264v3",
    "title": "Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization",
    "year": 2023,
    "authors": [
      "Yan Luo",
      "Yu Tian",
      "Min Shi",
      "Louis R. Pasquale",
      "Lucy Q. Shen",
      "Nazlee Zebardast",
      "Tobias Elze",
      "Mengyu Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in glaucoma prevalence and fairness in medical imaging AI, highlighting social inequalities related to race and health.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial health disparities in glaucoma detection",
      "affected_populations": [
        "minority racial groups",
        "Black patients"
      ],
      "methodology": [
        "Dataset Creation",
        "Fairness Learning",
        "Machine Learning",
        "Deep Learning"
      ],
      "methodology_detail": "Develops a dataset and fairness normalization approach",
      "geographic_focus": [
        "Harvard (USA)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.08906v1",
    "title": "Participatory Research as a Path to Community-Informed, Gender-Fair Machine Translation",
    "year": 2023,
    "authors": [
      "Dagmar Gromann",
      "Manuel Lardelli",
      "Katta Spiel",
      "Sabrina Burtscher",
      "Lukas Daniel Klausner",
      "Arthur Mettinger",
      "Igor Miladinovic",
      "Sigrid Schefer-Wenzl",
      "Daniela Duh",
      "Katharina Bühn"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, focusing on non-binary and queer identities, highlighting social discrimination and bias issues related to gender in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness beyond binary categories",
      "affected_populations": [
        "non-binary people",
        "queer individuals",
        "translators"
      ],
      "methodology": [
        "Participatory Action Research",
        "Case Study"
      ],
      "methodology_detail": "Involves experiential experts in design process",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.08394v1",
    "title": "Compatibility of Fairness Metrics with EU Non-Discrimination Laws: Demographic Parity & Conditional Demographic Disparity",
    "year": 2023,
    "authors": [
      "Lisa Koutsoviti Koumeri",
      "Magali Legast",
      "Yasaman Yousefi",
      "Koen Vanhoof",
      "Axel Legay",
      "Christoph Schommer"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic fairness within EU non-discrimination laws, focusing on legal notions of discrimination and bias mitigation, which relate to social groups and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "other"
      ],
      "other_detail": "Legal fairness in algorithmic decision-making",
      "affected_populations": [
        "protected social groups"
      ],
      "methodology": [
        "Experiment",
        "Fairness Metrics Analysis"
      ],
      "methodology_detail": "Comparing classifiers with fairness constraints",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.08158v5",
    "title": "Sociodemographic Bias in Language Models: A Survey and Forward Path",
    "year": 2023,
    "authors": [
      "Vipul Gupta",
      "Pranav Narayanan Venkit",
      "Shomir Wilson",
      "Rebecca J. Passonneau"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses sociodemographic bias in language models, which relates to social discrimination and inequality across groups such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on bias in AI language models",
      "affected_populations": [
        "social groups",
        "minorities",
        "disadvantaged"
      ],
      "methodology": [
        "Literature Review",
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comprehensive review of past research",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.07643v1",
    "title": "Economical Accommodations for Neurodivergent Students in Software Engineering Education: Experiences from an Intervention in Four Undergraduate Courses",
    "year": 2023,
    "authors": [
      "Grischa Liebel",
      "Steinunn Gróa Sigurðardóttir"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities faced by neurodivergent students in education, highlighting issues of inclusion, accessibility, and support, which relate to social inequalities in health, education, and disability.",
      "inequality_type": [
        "health",
        "educational",
        "disability"
      ],
      "other_detail": "Focuses on neurodiversity and educational accommodations",
      "affected_populations": [
        "ND students",
        "NT students"
      ],
      "methodology": [
        "Case Study",
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Intervention evaluation in university courses",
      "geographic_focus": [
        "Iceland"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.07527v1",
    "title": "Discrimination through Image Selection by Job Advertisers on Facebook",
    "year": 2023,
    "authors": [
      "Varun Nagaraj Rao",
      "Aleksandra Korolova"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates discrimination in job advertising based on gender, race, and demographics, highlighting social biases and inequality issues. It examines how algorithmic amplification can perpetuate these disparities. The focus on protected attributes and representation aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "demographic"
      ],
      "other_detail": "Bias in ad imagery and delivery algorithms",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design",
        "Dataset Creation"
      ],
      "methodology_detail": "Analysis of ad campaigns and image representation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.07500v1",
    "title": "Adding guardrails to advanced chatbots",
    "year": 2023,
    "authors": [
      "Yanchen Wang",
      "Lisa Singh"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI systems that can cause harm or inequity among subpopulations, addressing fairness and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "informational"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "subpopulations",
        "users",
        "disadvantaged groups"
      ],
      "methodology": [
        "Experiment",
        "Literature Review",
        "System Design"
      ],
      "methodology_detail": "Testing responses and proposing safeguards",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.07427v1",
    "title": "Towards Fair and Explainable AI using a Human-Centered AI Approach",
    "year": 2023,
    "authors": [
      "Bhavya Ghai"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness, bias mitigation, and social biases in AI systems, addressing social discrimination and inequality issues related to gender, race, and social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Focuses on social biases in AI and datasets",
      "affected_populations": [
        "women",
        "racial minorities",
        "social groups"
      ],
      "methodology": [
        "Visual Analytics",
        "Human-in-the-loop",
        "Empirical Study",
        "Tool Development"
      ],
      "methodology_detail": "Using visual tools and human feedback for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.07415v1",
    "title": "Gender-Inclusive Grammatical Error Correction through Augmentation",
    "year": 2023,
    "authors": [
      "Gunnar Lund",
      "Kostiantyn Omelianchuk",
      "Igor Samokhin"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI systems, focusing on gender-related discrimination and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Augmentation techniques to reduce gender bias in GEC",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.06696v1",
    "title": "Toward Fair Facial Expression Recognition with Improved Distribution Alignment",
    "year": 2023,
    "authors": [
      "Mojtaba Kolahdouzi",
      "Ali Etemad"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues related to sensitive attributes like gender, age, race, and attractiveness in facial expression recognition, which are social categories linked to inequality and discrimination.",
      "inequality_type": [
        "gender",
        "age",
        "racial",
        "attractiveness"
      ],
      "other_detail": "Includes attractiveness as a sensitive attribute",
      "affected_populations": [
        "women",
        "elderly",
        "racial minorities",
        "attractive faces"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation and fairness evaluation in FER models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.05882v2",
    "title": "Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems",
    "year": 2023,
    "authors": [
      "Silvia Alma Piazzolla",
      "Beatrice Savoldi",
      "Luisa Bentivogli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine translation, highlighting social discrimination aspects.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzing translation outputs for gender bias",
      "geographic_focus": [
        "English/Spanish",
        "English/Italian",
        "English/French"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.05550v1",
    "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
    "year": 2023,
    "authors": [
      "Katelyn X. Mei",
      "Sonia Fereidooni",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "K.4; I.2.7; I.2.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases against stigmatized social groups, including health, disability, religion, and socioeconomic status, highlighting social discrimination in AI models.",
      "inequality_type": [
        "health",
        "disability",
        "socioeconomic",
        "religion"
      ],
      "other_detail": "Focus on social stigmas and bias in AI models",
      "affected_populations": [
        "stigmatized groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement via prompt-based evaluation and annotations",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.05307v1",
    "title": "Are fairness metric scores enough to assess discrimination biases in machine learning?",
    "year": 2023,
    "authors": [
      "Fanny Jourdan",
      "Laurent Risser",
      "Jean-Michel Loubes",
      "Nicholas Asher"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in machine learning models predicting occupation, highlighting biases related to gender discrimination. It discusses fairness metrics and their reliability in small datasets, directly addressing social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in NLP applications",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes bias metrics across varying sample sizes",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.05068v1",
    "title": "Shedding light on underrepresentation and Sampling Bias in machine learning",
    "year": 2023,
    "authors": [
      "Sami Zhioua",
      "Rūta Binkytė"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias and discrimination in machine learning, focusing on underrepresentation and sampling bias, which relate to social groups and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and bias measurement in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "underrepresented groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Decomposition of discrimination into variance, bias, noise",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.05066v1",
    "title": "Causal Fairness for Outcome Control",
    "year": 2023,
    "authors": [
      "Drago Plecko",
      "Elias Bareinboim"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and equitable decision-making involving sensitive attributes like gender and race, indicating a focus on social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social",
        "fairness"
      ],
      "other_detail": "Focus on algorithmic fairness and social justice implications",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Causal Analysis",
        "Algorithm Development",
        "Fairness Framework"
      ],
      "methodology_detail": "Causal tools and optimization procedures for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social fairness issues",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04735v2",
    "title": "Soft-prompt Tuning for Large Language Models to Evaluate Bias",
    "year": 2023,
    "authors": [
      "Jacob-Junqi Tian",
      "David Emerson",
      "Sevil Zanjani Miyandoab",
      "Deval Pandya",
      "Laleh Seyyed-Kalantari",
      "Faiza Khan Khattak"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases in large language models, which relate to social discrimination and fairness issues affecting different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Analyzes bias patterns in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic classes"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Bias Evaluation"
      ],
      "methodology_detail": "Using soft-prompt tuning to assess bias patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04597v1",
    "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
    "year": 2023,
    "authors": [
      "Himanshu Thakur",
      "Atishay Jain",
      "Praneetha Vaddamanu",
      "Paul Pu Liang",
      "Louis-Philippe Morency"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social bias affecting fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias mitigation in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Few-shot debiasing technique with minimal data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04573v1",
    "title": "Gender, names and other mysteries: Towards the ambiguous for gender-inclusive translation",
    "year": 2023,
    "authors": [
      "Danielle Saunders",
      "Katrina Olsen"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social aspect of inequality. It discusses gender ambiguity and inclusivity, directly relating to gender discrimination issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in translation",
      "affected_populations": [
        "gender minorities",
        "language users"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing gender bias in translation data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04212v2",
    "title": "Migrate Demographic Group For Fair GNNs",
    "year": 2023,
    "authors": [
      "YanMing Hu",
      "TianChi Liao",
      "JiaLong Chen",
      "Jing Bian",
      "ZiBin Zheng",
      "Chuan Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in GNNs related to demographic groups, focusing on bias and discrimination linked to sensitive attributes like race and age.",
      "inequality_type": [
        "racial",
        "age",
        "fairness"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "demographic groups",
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Dynamic demographic group migration framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04118v1",
    "title": "M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method",
    "year": 2023,
    "authors": [
      "Yinghao Zhu",
      "Jingkun An",
      "Enshen Zhou",
      "Lu An",
      "Junyi Gao",
      "Hao Li",
      "Haoran Feng",
      "Bo Hou",
      "Wen Tang",
      "Chengwei Pan",
      "Liantao Ma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues related to race, gender, and health disparities in healthcare AI, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focuses on healthcare disparities and bias mitigation",
      "affected_populations": [
        "minority groups",
        "patients"
      ],
      "methodology": [
        "Reweighting",
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in healthcare data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04107v2",
    "title": "BeMap: Balanced Message Passing for Fair Graph Neural Network",
    "year": 2023,
    "authors": [
      "Xiao Lin",
      "Jian Kang",
      "Weilin Cong",
      "Hanghang Tong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in graph neural networks, focusing on demographic group balance, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "demographic",
        "social fairness"
      ],
      "other_detail": "Focus on bias amplification among demographic groups",
      "affected_populations": [
        "demographic groups",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Theoretical Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Bias mitigation in graph neural networks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.04067v1",
    "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
    "year": 2023,
    "authors": [
      "Zhongbin Xie",
      "Thomas Lukasiewicz"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in language models related to gender, racial, and religious groups, which are social inequalities. It focuses on bias mitigation in AI systems, a key aspect of social fairness. The study's emphasis on social bias in AI aligns with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "religious"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias mitigation experiments on language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.06083v1",
    "title": "Improving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering",
    "year": 2023,
    "authors": [
      "Irina-Elena Veliche",
      "Pascale Fung"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in speech recognition, focusing on equitable performance across demographic groups, which relates to social bias and inequality.",
      "inequality_type": [
        "racial",
        "linguistic",
        "geographic",
        "educational"
      ],
      "other_detail": "Focus on accent and demographic fairness in ASR",
      "affected_populations": [
        "non-native speakers",
        "regional accents",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Unsupervised Clustering",
        "Experiment"
      ],
      "methodology_detail": "Using unsupervised acoustic clustering for fairness improvement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.03950v2",
    "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
    "year": 2023,
    "authors": [
      "Tamanna Hossain",
      "Sunipa Dev",
      "Sameer Singh"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, focusing on non-binary pronouns, which relates to social gender inequality and discrimination. It addresses biases in AI systems affecting marginalized gender groups. The study highlights the impact of AI on gender representation and inclusivity.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Non-binary gender representation in AI",
      "affected_populations": [
        "non-binary individuals",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Evaluating model performance on pronoun usage",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.03293v2",
    "title": "Towards Fairness in Personalized Ads Using Impression Variance Aware Reinforcement Learning",
    "year": 2023,
    "authors": [
      "Aditya Srinivas Timmaraju",
      "Mehdi Mashayekhi",
      "Mingliang Chen",
      "Qi Zeng",
      "Quintin Fettes",
      "Wesley Cheung",
      "Yihan Xiao",
      "Manojkumar Rangasamy Kannadasan",
      "Pushkar Tripathi",
      "Sean Gahagan",
      "Miranda Bogen",
      "Rob Roudani"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in ad impressions across demographic groups, focusing on racial and gender disparities, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in online advertising impacts social equity",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Quantitative Analysis",
        "Simulation",
        "A/B Testing"
      ],
      "methodology_detail": "Variance-aware reinforcement learning for fairness",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.03179v1",
    "title": "Fair Patient Model: Mitigating Bias in the Patient Representation Learned from the Electronic Health Records",
    "year": 2023,
    "authors": [
      "Sonish Sivarajkumar",
      "Yufei Huang",
      "Yanshan Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias mitigation in healthcare AI, addressing social discrimination related to health and demographic groups.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias reduction in patient data representations",
      "affected_populations": [
        "patients from diverse backgrounds"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Weighted loss function for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.02615v1",
    "title": "Path-Specific Counterfactual Fairness for Recommender Systems",
    "year": 2023,
    "authors": [
      "Yaochen Zhu",
      "Jing Ma",
      "Liang Wu",
      "Qi Guo",
      "Liangjie Hong",
      "Jundong Li"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in recommender systems, focusing on mitigating biases related to sensitive features like race or demographics, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic recommendations",
      "affected_populations": [
        "minority users",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Counterfactual inference and variational inference techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.02428v1",
    "title": "Taught by the Internet, Exploring Bias in OpenAIs GPT3",
    "year": 2023,
    "authors": [
      "Ali Ayaz",
      "Aditya Nawalgaria",
      "Ruilian Yin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in NLP models, focusing on gender bias, which relates to social discrimination and inequality. It analyzes how AI systems may perpetuate or mitigate social biases affecting different groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Model Development"
      ],
      "methodology_detail": "Testing bias mitigation techniques on GPT3",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.05500v1",
    "title": "Word-Level Explanations for Analyzing Bias in Text-to-Image Models",
    "year": 2023,
    "authors": [
      "Alexander Lin",
      "Lucas Monteiro Paes",
      "Sree Harsha Tanneru",
      "Suraj Srinivas",
      "Himabindu Lakkaraju"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in text-to-image models related to societal stereotypes of race and sex, addressing social discrimination and bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on societal stereotypes in AI-generated images",
      "affected_populations": [
        "minorities",
        "women",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Using masked language models to identify bias sources",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.01699v1",
    "title": "Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy",
    "year": 2023,
    "authors": [
      "Siamak Ghodsi",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses dataset bias related to protected attributes like race, aiming to reduce representation bias and discrimination in AI models.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on racial group distribution in datasets",
      "affected_populations": [
        "minority groups",
        "majority group"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Affinity clustering for data debiasing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.01417v1",
    "title": "The Flawed Foundations of Fair Machine Learning",
    "year": 2023,
    "authors": [
      "Robert Lee Poe",
      "Soumia Zohra El Mestari"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI, focusing on group disparities and equitable outcomes, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness trade-offs in algorithmic decision-making",
      "affected_populations": [
        "underprivileged groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes fairness trade-offs and introduces proof-of-concept evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.01333v1",
    "title": "Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations",
    "year": 2023,
    "authors": [
      "Vasantha Kumar Venugopal",
      "Abhishek Gupta",
      "Rohit Takhar",
      "Charlene Liew Jin Yee",
      "Catherine Jones",
      "Gilberto Szarf"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and bias in AI, focusing on disparities across demographic groups, which relates to social inequalities.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias in medical imaging AI systems",
      "affected_populations": [
        "demographic groups",
        "patients"
      ],
      "methodology": [
        "Bias auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Uses Aequitas toolkit for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.00905v1",
    "title": "T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation",
    "year": 2023,
    "authors": [
      "Jialu Wang",
      "Xinyue Gabby Liu",
      "Zonglin Di",
      "Yang Liu",
      "Xin Eric Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "I.2.6"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to social attributes like gender and demographics in AI-generated images, reflecting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "demographic"
      ],
      "other_detail": "Bias measurement in AI-generated imagery",
      "affected_populations": [
        "women",
        "racial minorities",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement using T2IAT framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.00639v2",
    "title": "Being Right for Whose Right Reasons?",
    "year": 2023,
    "authors": [
      "Terne Sasha Thorn Jakobsen",
      "Laura Cabello",
      "Anders Søgaard"
    ],
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models related to demographic groups, highlighting disparities in model alignment with different racial, age, and gender groups, which relates to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "age",
        "gender"
      ],
      "other_detail": "Focus on model bias and fairness across social groups",
      "affected_populations": [
        "racial minorities",
        "older adults",
        "younger individuals",
        "white annotators"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing model alignment with demographic-specific rationales",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2306.03773v1",
    "title": "Some voices are too common: Building fair speech recognition systems using the Common Voice dataset",
    "year": 2023,
    "authors": [
      "Lucas Maison",
      "Yannick Estève"
    ],
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in speech recognition related to demographic groups, addressing fairness and social bias issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "linguistic"
      ],
      "other_detail": "Biases toward accents and demographic groups in speech data",
      "affected_populations": [
        "non-native speakers",
        "people with accents",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing biases using the Common Voice dataset and fine-tuning models",
      "geographic_focus": [
        "France"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.19409v1",
    "title": "Examining risks of racial biases in NLP tools for child protective services",
    "year": 2023,
    "authors": [
      "Anjalie Field",
      "Amanda Coston",
      "Nupoor Gandhi",
      "Alexandra Chouldechova",
      "Emily Putnam-Hornstein",
      "David Steier",
      "Yulia Tsvetkov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial biases in NLP tools used in child protective services, highlighting racial disparities and fairness issues affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial bias in AI systems affecting children and families",
      "affected_populations": [
        "racial minorities",
        "families in CPS"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes word statistics and fairness in NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.19407v1",
    "title": "FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection",
    "year": 2023,
    "authors": [
      "Brandon Theodorou",
      "Lucas Glass",
      "Cao Xiao",
      "Jimeng Sun"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial and ethnic disparities in clinical trial site selection, aiming to improve diversity and representation among minority groups, which directly relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "health"
      ],
      "other_detail": "Focus on minority representation in clinical trials",
      "affected_populations": [
        "racial minorities",
        "ethnic minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Reinforcement Learning",
        "Experimental"
      ],
      "methodology_detail": "Handles missing data and optimizes fairness and enrollment",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.18160v4",
    "title": "Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation",
    "year": 2023,
    "authors": [
      "Yifei Wang",
      "Zhengyang Zhou",
      "Liqin Wang",
      "John Laurentiev",
      "Peter Hou",
      "Li Zhou",
      "Pengyu Hong"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "J.3"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on disparities affecting underprivileged groups, which relates to social inequality issues such as discrimination and bias.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses systematic fairness evaluation in AI decision-making",
      "affected_populations": [
        "underprivileged groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness index development and matching methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.17701v2",
    "title": "KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application",
    "year": 2023,
    "authors": [
      "Hwaran Lee",
      "Seokhee Hong",
      "Joonsuk Park",
      "Takyoung Kim",
      "Gunhee Kim",
      "Jung-Woo Ha"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in AI affecting demographic groups, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on social bias mitigation in language models",
      "affected_populations": [
        "demographic groups",
        "Korean society"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Developing bias dataset and evaluating bias reduction",
      "geographic_focus": [
        "South Korea"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.17177v1",
    "title": "Local Sharing and Sociality Effects on Wealth Inequality in a Simple Artificial Society",
    "year": 2023,
    "authors": [
      "John C. Stevenson"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CE",
      "cs.MA",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates wealth inequality and social behaviors affecting wealth distribution.",
      "inequality_type": [
        "wealth",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on resource sharing and sociality effects",
      "affected_populations": [
        "artificial society",
        "simulated agents"
      ],
      "methodology": [
        "Simulation",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Minimal model of a complex adaptive system",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.17072v1",
    "title": "Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models",
    "year": 2023,
    "authors": [
      "Eddie L. Ungless",
      "Björn Ross",
      "Anne Lauscher"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and misrepresentation of non-cisgender identities in AI-generated images, addressing gender-based social discrimination and stereotypes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender identity representation",
      "affected_populations": [
        "non-cisgender individuals"
      ],
      "methodology": [
        "Experiment",
        "Survey",
        "Interviews"
      ],
      "methodology_detail": "Analyzes model outputs and gathers community perspectives",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.16935v1",
    "title": "Gender Lost In Translation: How Bridging The Gap Between Languages Affects Gender Bias in Zero-Shot Multilingual Translation",
    "year": 2023,
    "authors": [
      "Lena Cabrera",
      "Jan Niehues"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in multilingual translation models, addressing gender fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI translation",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "evaluating gender bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.16641v1",
    "title": "Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales",
    "year": 2023,
    "authors": [
      "Paulina Toro Isaza",
      "Guangxuan Xu",
      "Akintoye Oloko",
      "Yufang Hou",
      "Nanyun Peng",
      "Dakuo Wang"
    ],
    "categories": [
      "cs.CL",
      "68T50"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in fairy tales, addressing gender inequality and social stereotypes using NLP methods.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in narrative structures",
      "affected_populations": [
        "female characters",
        "male characters"
      ],
      "methodology": [
        "Natural Language Processing",
        "Computational Pipeline",
        "Event Analysis"
      ],
      "methodology_detail": "Automated extraction and analysis of narrative event chains",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.16471v1",
    "title": "Bias, Consistency, and Partisanship in U.S. Asylum Cases: A Machine Learning Analysis of Extraneous Factors in Immigration Court Decisions",
    "year": 2023,
    "authors": [
      "Vyoma Raman",
      "Catherine Vera",
      "CJ Manna"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG",
      "J.4; I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines systemic biases in immigration court decisions, highlighting disparities based on extraneous factors like partisanship and judge variability, which relate to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "ethnic"
      ],
      "other_detail": "Bias in legal and immigration decision-making processes",
      "affected_populations": [
        "asylum seekers",
        "immigrants"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predictive modeling and time series analysis of court data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.16253v2",
    "title": "Uncovering and Categorizing Social Biases in Text-to-SQL",
    "year": 2023,
    "authors": [
      "Yan Liu",
      "Yan Gao",
      "Zhe Su",
      "Xiaokang Chen",
      "Elliott Ash",
      "Jian-Guang Lou"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in AI models, which relate to social discrimination and inequality issues such as stereotypes affecting different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on social biases in AI language models",
      "affected_populations": [
        "social groups",
        "demographics"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Building benchmarks to detect biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.16051v1",
    "title": "What about em? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns",
    "year": 2023,
    "authors": [
      "Anne Lauscher",
      "Debora Nozza",
      "Archie Crowley",
      "Ehm Miltersen",
      "Dirk Hovy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how machine translation systems misrepresent gendered and neopronouns, which can lead to discrimination against marginalized gender groups, highlighting issues of social bias and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender inclusivity in NLP",
      "affected_populations": [
        "non-binary individuals",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Error Analysis",
        "Survey"
      ],
      "methodology_detail": "Translation comparison and native speaker opinions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.15979v1",
    "title": "Monitoring Algorithmic Fairness",
    "year": 2023,
    "authors": [
      "Thomas A. Henzinger",
      "Mahyar Karimi",
      "Konstantin Kueffner",
      "Kaushik Mallik"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on monitoring algorithmic fairness, addressing bias against social groups, which relates directly to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in decision-making systems",
      "affected_populations": [
        "social groups",
        "applicants",
        "students"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Runtime verification of fairness properties",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.15389v1",
    "title": "Comparing Humans and Models on a Similar Scale: Towards Cognitive Gender Bias Evaluation in Coreference Resolution",
    "year": 2023,
    "authors": [
      "Gili Lior",
      "Gabriel Stanovsky"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in NLP models and compares it to human gender biases, addressing social discrimination related to gender. It analyzes how AI systems reflect or differ from human societal biases, thus engaging with social bias and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in coreference resolution",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Crowdsourcing",
        "Self-paced reading",
        "Question answering"
      ],
      "methodology_detail": "Comparative analysis of human and model biases",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.15377v1",
    "title": "Uncovering and Quantifying Social Biases in Code Generation",
    "year": 2023,
    "authors": [
      "Yan Liu",
      "Xiaokang Chen",
      "Yan Gao",
      "Zhe Su",
      "Fengji Zhang",
      "Daoguang Zan",
      "Jian-Guang Lou",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in code generation models, which relate to social discrimination and fairness issues affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focuses on social biases in AI-generated code",
      "affected_populations": [
        "social groups",
        "individuals in certain groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Develops dataset and metrics to evaluate social bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.15210v1",
    "title": "Detecting disparities in police deployments using dashcam data",
    "year": 2023,
    "authors": [
      "Matt Franchi",
      "J. D. Zamfirescu-Pereira",
      "Wendy Ju",
      "Emma Pierson"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in police deployment across neighborhoods, highlighting inequalities related to race and socioeconomic status. It discusses how police presence varies by neighborhood demographics, indicating social inequality concerns.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial and economic disparities in policing",
      "affected_populations": [
        "Black residents",
        "Hispanic residents",
        "neighborhoods"
      ],
      "methodology": [
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Detects police vehicles in dashcam images and analyzes deployment levels",
      "geographic_focus": [
        "United States",
        "New York City"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14711v3",
    "title": "Gender Biases in Automatic Evaluation Metrics for Image Captioning",
    "year": 2023,
    "authors": [
      "Haoyi Qiu",
      "Zi-Yi Dou",
      "Tianlu Wang",
      "Asli Celikyilmaz",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender biases in AI evaluation metrics, highlighting societal gender stereotypes and their impact on fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI evaluation metrics",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias in metrics and proposes mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14653v2",
    "title": "Hoarding without hoarders: unpacking the emergence of opportunity hoarding within schools",
    "year": 2023,
    "authors": [
      "João M. Souto-Maior"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.MA",
      "econ.TH"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and socioeconomic disparities in educational resource access, highlighting structural inequalities. It focuses on opportunity hoarding related to race and class within schools, addressing social inequality themes.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on structural racial and class inequalities in education",
      "affected_populations": [
        "Black students",
        "White students"
      ],
      "methodology": [
        "Agent-based modeling",
        "Qualitative analysis"
      ],
      "methodology_detail": "Simulations of resource competition based on social structures",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14597v1",
    "title": "Voices of Her: Analyzing Gender Differences in the AI Publication World",
    "year": 2023,
    "authors": [
      "Yiwen Ding",
      "Jiarui Liu",
      "Zhiheng Lyu",
      "Kun Zhang",
      "Bernhard Schoelkopf",
      "Zhijing Jin",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in AI research, addressing gender inequality and bias in academic communities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender disparities in AI publication and authorship",
      "affected_populations": [
        "female researchers",
        "male researchers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analyzes publication data and linguistic styles",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14574v1",
    "title": "Detecting and Mitigating Indirect Stereotypes in Word Embeddings",
    "year": 2023,
    "authors": [
      "Erin George",
      "Joyce Chew",
      "Deanna Needell"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses societal biases and stereotypes in language, which relate to social inequalities such as gender and race. It focuses on bias mitigation in AI, a key aspect of fairness and social equity. The work aims to reduce harmful stereotypes embedded in language models, impacting social perceptions.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on stereotypes in word embeddings",
      "affected_populations": [
        "women",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14468v1",
    "title": "Run Like a Girl! Sports-Related Gender Bias in Language and Vision",
    "year": 2023,
    "authors": [
      "Sophia Harrison",
      "Eleonora Gualdoni",
      "Gemma Boleda"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in datasets and models, highlighting underrepresentation and stereotyping of women, which relates directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language and vision AI",
      "affected_populations": [
        "women",
        "girls"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes dataset data and model bias reproduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14291v2",
    "title": "Evaluation of African American Language Bias in Natural Language Generation",
    "year": 2023,
    "authors": [
      "Nicholas Deas",
      "Jessi Grieser",
      "Shana Kleiner",
      "Desmond Patton",
      "Elsbeth Turcan",
      "Kathleen McKeown"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines language bias related to African American Language, highlighting racial bias in AI models, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Focuses on racial language bias in AI systems",
      "affected_populations": [
        "African Americans",
        "language communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Evaluation of language models on biased language datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14016v2",
    "title": "Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation",
    "year": 2023,
    "authors": [
      "Minwoo Lee",
      "Hyukhun Koh",
      "Kang-il Lee",
      "Dongdong Zhang",
      "Minsung Kim",
      "Kyomin Jung"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in multilingual machine translation, a social inequality issue related to gender discrimination in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias mitigation in AI",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Learning",
        "Fine-tuning"
      ],
      "methodology_detail": "Gender-aware contrastive learning for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13875v1",
    "title": "Fair Oversampling Technique using Heterogeneous Clusters",
    "year": 2023,
    "authors": [
      "Ryosuke Sonoda"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and group imbalance issues related to race, gender, and age, which are social inequalities. It focuses on algorithmic fairness to mitigate bias across social groups. The emphasis on fairness trade-offs indicates concern with social discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in machine learning classifiers",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Synthetic Data Generation"
      ],
      "methodology_detail": "Fair oversampling with heterogeneous clusters",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13862v2",
    "title": "A Trip Towards Fairness: Bias and De-Biasing in Large Language Models",
    "year": 2023,
    "authors": [
      "Leonardo Ranaldi",
      "Elena Sofia Ruzzetti",
      "Davide Venditti",
      "Dario Onorati",
      "Fabio Massimo Zanzotto"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender, race, religion, and profession in large language models, which are social categories linked to inequality and discrimination.",
      "inequality_type": [
        "gender",
        "race",
        "religion",
        "profession"
      ],
      "other_detail": "Bias in AI systems affecting social groups",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups",
        "professionals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and debiasing techniques evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.14396v1",
    "title": "FITNESS: A Causal De-correlation Approach for Mitigating Bias in Machine Learning Software",
    "year": 2023,
    "authors": [
      "Ying Xiao",
      "Shangwen Wang",
      "Sicen Liu",
      "Dingyuan Xue",
      "Xian Zhan",
      "Yepang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in AI systems affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in machine learning models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Analysis",
        "Multi-objective Optimization"
      ],
      "methodology_detail": "De-correlation of causal effects to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13485v1",
    "title": "Advancing Community Engaged Approaches to Identifying Structural Drivers of Racial Bias in Health Diagnostic Algorithms",
    "year": 2023,
    "authors": [
      "Jill A. Kuhlberg",
      "Irene Headen",
      "Ellis A. Ballard",
      "Donald Martin Jr."
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial bias in healthcare algorithms, societal impacts, and disparities affecting different racial groups.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial bias and health disparities in AI",
      "affected_populations": [
        "racial groups",
        "healthcare patients"
      ],
      "methodology": [
        "Qualitative Study",
        "Simulation Modeling"
      ],
      "methodology_detail": "Community-engaged modeling of societal and health data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13302v2",
    "title": "Language-Agnostic Bias Detection in Language Models with Bias Probing",
    "year": 2023,
    "authors": [
      "Abdullatif Köksal",
      "Omer Faruk Yalcin",
      "Ahmet Akbiyik",
      "M. Tahir Kilavuz",
      "Anna Korhonen",
      "Hinrich Schütze"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases in language models, which relate to social discrimination and inequality issues such as nationality bias. It examines how biases in AI systems reflect societal inequalities. The study links pretraining data to biases affecting social groups.",
      "inequality_type": [
        "nationality",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias detection in multilingual language models",
      "affected_populations": [
        "nationalities",
        "ethnic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias probing across multiple languages and templates",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.18569v2",
    "title": "Fairness of ChatGPT",
    "year": 2023,
    "authors": [
      "Yunqi Li",
      "Lanjing Zhang",
      "Yongfeng Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness and bias in LLMs across high-stakes fields, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on fairness in AI systems affecting social groups",
      "affected_populations": [
        "students",
        "criminals",
        "patients",
        "financial clients"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Assessing fairness metrics and output disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13198v1",
    "title": "Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale",
    "year": 2023,
    "authors": [
      "Marta R. Costa-jussà",
      "Pierre Andrews",
      "Eric Smith",
      "Prangthip Hansanti",
      "Christophe Ropers",
      "Elahe Kalbassi",
      "Cynthia Gao",
      "Daniel Licht",
      "Carleigh Wood"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic biases in language, focusing on gender and other social axes, and examines how AI models reflect these biases, thus relating to social discrimination and fairness issues.",
      "inequality_type": [
        "gender",
        "linguistic"
      ],
      "other_detail": "Focus on gender bias in multilingual NLP",
      "affected_populations": [
        "gender groups",
        "language speakers"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Creating multilingual bias dataset and analyzing translation biases",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13088v2",
    "title": "Should We Attend More or Less? Modulating Attention for Fairness",
    "year": 2023,
    "authors": [
      "Abdelrahman Zayed",
      "Goncalo Mordido",
      "Samira Shabanian",
      "Sarath Chandar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in NLP models, focusing on fairness and bias propagation, which relate directly to social discrimination issues such as gender stereotypes.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Attention modulation techniques for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12829v3",
    "title": "On Bias and Fairness in NLP: Investigating the Impact of Bias and Debiasing in Language Models on the Fairness of Toxicity Detection",
    "year": 2023,
    "authors": [
      "Fatma Elsafoury",
      "Stamos Katsigiannis"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in NLP affecting fairness, which relates to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focuses on fairness in toxicity detection across social groups",
      "affected_populations": [
        "identity groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Debiasing techniques"
      ],
      "methodology_detail": "Analyzes bias sources and debiasing impacts on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12709v1",
    "title": "Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis",
    "year": 2023,
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Björn Ross",
      "Adam Lopez"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI systems, which are social inequalities. It analyzes how cross-lingual transfer impacts bias, directly addressing social fairness issues in technology.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias transfer in multilingual sentiment analysis",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Counterfactual Evaluation",
        "Model Development"
      ],
      "methodology_detail": "Testing bias transfer across languages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12671v2",
    "title": "Transferring Fairness using Multi-Task Learning with Limited Demographic Information",
    "year": 2023,
    "authors": [
      "Carlos Aguirre",
      "Mark Dredze"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing social bias and discrimination across demographic groups, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Fairness transfer across demographic groups in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Loss Optimization",
        "Transfer Learning"
      ],
      "methodology_detail": "Using multi-task learning to improve fairness with limited demographic data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12622v2",
    "title": "Evaluating the Impact of Social Determinants on Health Prediction in the Intensive Care Unit",
    "year": 2023,
    "authors": [
      "Ming Ying Yang",
      "Gloria Hyunjung Kwak",
      "Tom Pollard",
      "Leo Anthony Celi",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social determinants of health, community-level disparities, and algorithmic biases affecting different populations, addressing social inequalities related to health and fairness.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "geographic",
        "disability"
      ],
      "other_detail": "Focus on health disparities and algorithmic fairness",
      "affected_populations": [
        "patients with limited data",
        "specific subpopulations"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Assessing impact of SDOH on prediction fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12495v2",
    "title": "Fair Without Leveling Down: A New Intersectional Fairness Definition",
    "year": 2023,
    "authors": [
      "Gaurav Maheshwari",
      "Aurélien Bellet",
      "Pascal Denis",
      "Mikaela Keller"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses intersectional fairness in AI, focusing on discrimination across sensitive social groups such as race, gender, and other identities, highlighting social bias issues in algorithms.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focuses on fairness in machine learning models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "disabled individuals"
      ],
      "methodology": [
        "Fairness Definition Development",
        "Benchmarking",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Proposes new fairness measure and evaluates existing approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12434v1",
    "title": "BiasAsker: Measuring the Bias in Conversational AI System",
    "year": 2023,
    "authors": [
      "Yuxuan Wan",
      "Wenxuan Wang",
      "Pinjia He",
      "Jiazhen Gu",
      "Haonan Bai",
      "Michael Lyu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases in conversational AI, which relate to social discrimination and stereotypes affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Addresses bias measurement in AI systems",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Constructs bias dataset and tests AI responses",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12376v1",
    "title": "Measuring Intersectional Biases in Historical Documents",
    "year": 2023,
    "authors": [
      "Nadav Borenstein",
      "Karolina Stańczak",
      "Thea Rolskov",
      "Natália da Silva Perez",
      "Natacha Klein Käfer",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race and gender in historical texts, aligning with social inequality topics. It examines how these biases intersect and evolve, reflecting societal inequalities. The focus on marginalized identities and bias analysis indicates relevance to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Intersectionality of biases in historical contexts",
      "affected_populations": [
        "marginalized groups",
        "historical communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using distributional semantics and word embeddings",
      "geographic_focus": [
        "Caribbean"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12178v2",
    "title": "Model Debiasing via Gradient-based Explanation on Representation",
    "year": 2023,
    "authors": [
      "Jindi Zhang",
      "Luning Wang",
      "Dan Su",
      "Yongxiang Huang",
      "Caleb Chen Cao",
      "Lei Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias related to sensitive attributes like gender, which directly relates to social discrimination and inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on bias mitigation in AI systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "gradient-based explanation for debiasing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.12090v2",
    "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
    "year": 2023,
    "authors": [
      "Wenyue Hua",
      "Yingqiang Ge",
      "Shuyuan Xu",
      "Jianchao Ji",
      "Yongfeng Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in recommendation systems related to sensitive features like gender and age, highlighting societal bias concerns.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Focus on fairness in AI recommendation systems",
      "affected_populations": [
        "users by gender",
        "users by age"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness evaluation and proposed mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11844v1",
    "title": "AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia",
    "year": 2023,
    "authors": [
      "Rida Qadri",
      "Renee Shelby",
      "Cynthia L. Bennett",
      "Remi Denton"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines cultural limitations of AI models in South Asia, highlighting how AI reproduces outsider perspectives shaped by global and regional power inequalities, thus addressing social inequalities related to race, culture, and marginalization.",
      "inequality_type": [
        "racial",
        "cultural",
        "geographic"
      ],
      "other_detail": "Focus on cultural and regional power disparities",
      "affected_populations": [
        "South Asian communities"
      ],
      "methodology": [
        "Qualitative Study",
        "Community-centered research"
      ],
      "methodology_detail": "Centering community perspectives on AI limitations",
      "geographic_focus": [
        "South Asia"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11807v1",
    "title": "On the Fairness Impacts of Private Ensembles Models",
    "year": 2023,
    "authors": [
      "Cuong Tran",
      "Ferdinando Fioretto"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and accuracy disparities among social groups affected by AI models, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic fairness impacts across groups",
      "affected_populations": [
        "social groups",
        "individuals in different groups"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes impacts of PATE on group disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11673v1",
    "title": "Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages",
    "year": 2023,
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Adam Lopez",
      "Roi Blanco",
      "Diego Marcheggiani"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in AI systems, which are social inequalities, and discusses bias mitigation, impacting social groups.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on bias evaluation in multilingual sentiment analysis",
      "affected_populations": [
        "gender groups",
        "racial/migrant groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Counterfactual Evaluation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Develops evaluation corpus for bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11602v1",
    "title": "Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing",
    "year": 2023,
    "authors": [
      "Yisong Xiao",
      "Aishan Liu",
      "Tianlin Li",
      "Xianglong Liu"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness testing in AI systems, addressing discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and discrimination in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Generative Adversarial Networks",
        "Experiment"
      ],
      "methodology_detail": "Generating natural discriminatory instances for fairness testing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11361v1",
    "title": "Group fairness without demographics using social networks",
    "year": 2023,
    "authors": [
      "David Liu",
      "Virginie Do",
      "Nicolas Usunier",
      "Maximilian Nickel"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems without sensitive attributes, focusing on social disparities related to protected groups. It aims to reduce inequality among protected classes, indicating a focus on social discrimination. The use of social networks to measure inequality relates directly to social group disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Uses social network homophily to measure fairness",
      "affected_populations": [
        "protected groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Network-based fairness measurement and algorithm design",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11348v2",
    "title": "In the Name of Fairness: Assessing the Bias in Clinical Record De-identification",
    "year": 2023,
    "authors": [
      "Yuxin Xiao",
      "Shulammite Lim",
      "Tom Joseph Pollard",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in de-identification systems across demographic groups, highlighting disparities related to race, gender, and other social categories, which directly relate to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "educational"
      ],
      "other_detail": "Bias in AI systems affecting marginalized groups",
      "affected_populations": [
        "racial minorities",
        "women",
        "patients"
      ],
      "methodology": [
        "Empirical Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Large-scale evaluation of de-identification performance across demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11262v1",
    "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
    "year": 2023,
    "authors": [
      "Jiaxu Zhao",
      "Meng Fang",
      "Zijing Shi",
      "Yitong Li",
      "Ling Chen",
      "Mykola Pechenizkiy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates social biases in Chinese conversational AI, focusing on biases related to age and appearance, which are social inequality issues. It discusses bias mitigation to reduce social discrimination in AI outputs.",
      "inequality_type": [
        "age",
        "appearance"
      ],
      "other_detail": "Focus on social biases in Chinese language models",
      "affected_populations": [
        "Chinese users",
        "social groups targeted by biases"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Bias Evaluation",
        "Bias Mitigation"
      ],
      "methodology_detail": "Develops dataset, evaluates and mitigates biases in models",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11242v1",
    "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages",
    "year": 2023,
    "authors": [
      "Sharon Levy",
      "Neha Anna John",
      "Ling Liu",
      "Yogarshi Vyas",
      "Jie Ma",
      "Yoshinari Fujinuma",
      "Miguel Ballesteros",
      "Vittorio Castelli",
      "Dan Roth"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race, religion, nationality, and gender across languages, highlighting social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "religious",
        "nationality",
        "gender"
      ],
      "other_detail": "Bias manifestation across cultural and linguistic groups",
      "affected_populations": [
        "minority groups",
        "majority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias template adaptation and cross-lingual analysis",
      "geographic_focus": [
        "Italy",
        "China",
        "Israel",
        "Spain"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.11140v1",
    "title": "Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model",
    "year": 2023,
    "authors": [
      "Chantal Amrhein",
      "Florian Schottmann",
      "Rico Sennrich",
      "Samuel Läubli"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing gender bias in language models, addressing gender fairness, a key aspect of social inequality. It discusses bias mitigation in AI outputs, which impacts social perceptions and treatment of gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Using translation models for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.10566v1",
    "title": "Smiling Women Pitching Down: Auditing Representational and Presentational Gender Biases in Image Generative AI",
    "year": 2023,
    "authors": [
      "Luhang Sun",
      "Mian Wei",
      "Yibing Sun",
      "Yoo Ji Suh",
      "Liwei Shen",
      "Sijia Yang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in AI-generated images, highlighting disparities in representation and presentation of women across occupations, which relates directly to gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI-generated imagery",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Auditing bias prevalence and comparing with census data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.10510v1",
    "title": "ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages",
    "year": 2023,
    "authors": [
      "Sourojit Ghosh",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI translation systems, highlighting gender stereotypes and disparities affecting women and men. It discusses how AI perpetuates gendered inequalities in language representation. The focus on gender bias in AI systems directly relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Testing translation accuracy and bias in AI systems",
      "geographic_focus": [
        "Bengali",
        "Farsi",
        "Malay",
        "Tagalog",
        "Thai",
        "Turkish"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.10407v1",
    "title": "BAD: BiAs Detection for Large Language Models in the context of candidate screening",
    "year": 2023,
    "authors": [
      "Nam Ho Koh",
      "Joseph Plata",
      "Joyce Chai"
    ],
    "categories": [
      "cs.CL",
      "I.2, I.2.7",
      "F.2.2, I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases in AI systems used in candidate screening, which can perpetuate social inequalities related to race, gender, and socioeconomic status. It explicitly aims to identify and quantify social biases in large language models affecting marginalized groups. The context of hiring and admissions directly relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI affecting social groups",
      "affected_populations": [
        "job applicants",
        "students",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "AI Bias Detection",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing bias instances in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.10201v4",
    "title": "Echoes of Biases: How Stigmatizing Language Affects AI Performance",
    "year": 2023,
    "authors": [
      "Yizhi Liu",
      "Weiguang Wang",
      "Guodong Gordon Gao",
      "Ritu Agarwal"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how clinician biases in EHR notes influence AI performance, highlighting racial disparities and bias mitigation strategies, directly addressing social inequalities related to race and health.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in healthcare AI",
      "affected_populations": [
        "black patients"
      ],
      "methodology": [
        "Deep Learning",
        "Explainable AI",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Transformer models and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.09941v4",
    "title": "\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
    "year": 2023,
    "authors": [
      "Anaelia Ovalle",
      "Palash Goyal",
      "Jwala Dhamala",
      "Zachary Jaggers",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Richard Zemel",
      "Rahul Gupta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "I.2; I.7; K.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender-based discrimination and bias in AI, centering TGNB voices, which relates to social gender inequality and bias in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender identity and societal norms",
      "affected_populations": [
        "TGNB individuals"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Qualitative Study"
      ],
      "methodology_detail": "Community-curated dataset and bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.09399v2",
    "title": "Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps",
    "year": 2023,
    "authors": [
      "Isel Grau",
      "Gonzalo Nápoles",
      "Fabian Hoitsma",
      "Lisa Koutsoviti Koumeri",
      "Koen Vanhoof"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses implicit bias and fairness in classification, relating to social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social bias"
      ],
      "other_detail": "Focus on implicit bias measurement in AI systems",
      "affected_populations": [
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fuzzy Cognitive Maps",
        "SHAP feature importance",
        "Case Study"
      ],
      "methodology_detail": "Integrates AI models with bias quantification techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.09330v1",
    "title": "Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation",
    "year": 2023,
    "authors": [
      "Bjørnar Vassøy",
      "Helge Langseth"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness issues in recommender systems, focusing on discrimination and bias affecting users, which relates to social inequality dimensions such as gender and potentially other social groups.",
      "inequality_type": [
        "gender",
        "discrimination"
      ],
      "other_detail": "Focus on consumer-side fairness and bias mitigation",
      "affected_populations": [
        "users",
        "gender groups"
      ],
      "methodology": [
        "Literature Review",
        "Taxonomy Development",
        "Evaluation Metric Analysis"
      ],
      "methodology_detail": "Categorization and systematic overview of research methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.09281v1",
    "title": "On the Origins of Bias in NLP through the Lens of the Jim Code",
    "year": 2023,
    "authors": [
      "Fatma Elsafoury",
      "Gavin Abercrombie"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper links biases in NLP to social issues like racism, sexism, and homophobia, emphasizing their social origins and impacts on marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "homophobia",
        "social discrimination"
      ],
      "other_detail": "Focuses on social biases in AI models from historical social issues",
      "affected_populations": [
        "racial minorities",
        "women",
        "LGBTQ+",
        "social groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Synthesizes social science literature on bias origins",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.09073v3",
    "title": "Consensus and Subjectivity of Skin Tone Annotation for ML Fairness",
    "year": 2023,
    "authors": [
      "Candice Schumann",
      "Gbolahan O. Olanubi",
      "Auriel Wright",
      "Ellis Monk Jr.",
      "Courtney Heldreth",
      "Susanna Ricco"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines subjective skin tone annotation, highlighting racial and social biases affecting fairness in AI systems.",
      "inequality_type": [
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on skin tone and annotator diversity",
      "affected_populations": [
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Survey"
      ],
      "methodology_detail": "Annotator studies and dataset release",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.09072v1",
    "title": "Skin Deep: Investigating Subjectivity in Skin Tone Annotations for Computer Vision Benchmark Datasets",
    "year": 2023,
    "authors": [
      "Teanna Barrett",
      "Quan Ze Chen",
      "Amy X. Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in skin tone annotations, highlighting social biases and their impact on AI fairness.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial disparities in computer vision datasets",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Literature Review",
        "Experiment"
      ],
      "methodology_detail": "Analyzes annotation procedures and conducts experiments on skin tone labeling",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.08157v4",
    "title": "Algorithmic Pluralism: A Structural Approach To Equal Opportunity",
    "year": 2023,
    "authors": [
      "Shomik Jain",
      "Vinith Suriyakumar",
      "Kathleen Creel",
      "Ashia Wilson"
    ],
    "categories": [
      "cs.CY",
      "68-06",
      "K.4.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses equal opportunity and systemic decision points affecting access, which relate to social inequalities such as socioeconomic and educational disparities.",
      "inequality_type": [
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on systemic opportunity access and decision bottlenecks",
      "affected_populations": [
        "diverse individuals"
      ],
      "methodology": [
        "Theoretical Analysis",
        "System Design"
      ],
      "methodology_detail": "Reframes systemic inequality through structural approach",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.07609v3",
    "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
    "year": 2023,
    "authors": [
      "Jizhi Zhang",
      "Keqin Bao",
      "Yang Zhang",
      "Wenjie Wang",
      "Fuli Feng",
      "Xiangnan He"
    ],
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness of AI recommendations across sensitive social attributes, addressing social bias and discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "educational",
        "disability"
      ],
      "other_detail": "Focuses on social fairness in AI recommendation systems",
      "affected_populations": [
        "users by gender",
        "racial groups",
        "disability groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Fairness Evaluation",
        "Experiment"
      ],
      "methodology_detail": "Develops benchmark and evaluates AI fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.07575v1",
    "title": "The Progression of Disparities within the Criminal Justice System: Differential Enforcement and Risk Assessment Instruments",
    "year": 2023,
    "authors": [
      "Miri Zilka",
      "Riccardo Fogliato",
      "Jiri Hron",
      "Bradley Butcher",
      "Carolyn Ashurst",
      "Adrian Weller"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in criminal justice risk assessments and their biases, directly addressing racial inequality and social bias issues.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial disparities in algorithmic decision-making",
      "affected_populations": [
        "Black individuals",
        "White individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias quantification and synthetic data augmentation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.06969v2",
    "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
    "year": 2023,
    "authors": [
      "Usman Gohar",
      "Lu Cheng"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI systems related to sensitive social attributes like race and gender, addressing social bias issues. It focuses on intersectional bias, a form of social discrimination affecting marginalized groups. The emphasis on fairness metrics and mitigation strategies relates directly to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on intersectional bias in AI fairness",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Survey of existing fairness notions and mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.07041v2",
    "title": "Fairness in Machine Learning meets with Equity in Healthcare",
    "year": 2023,
    "authors": [
      "Shaina Raza",
      "Parisa Osivand Pour",
      "Syed Raza Bashir"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in healthcare AI affecting demographic groups, highlighting fairness and equity issues.",
      "inequality_type": [
        "health",
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Focus on healthcare disparities and bias mitigation",
      "affected_populations": [
        "patients",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Case Study",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection and mitigation in healthcare data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.06415v1",
    "title": "WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?",
    "year": 2023,
    "authors": [
      "Ali Akbar Septiandri",
      "Marios Constantinides",
      "Mohammad Tahaei",
      "Daniele Quercia"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines the lack of diversity in AI research samples, highlighting issues related to Western-centric data collection and representation, which relate to social inequalities such as racial, geographic, and socioeconomic biases.",
      "inequality_type": [
        "racial",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on representation bias in AI research samples",
      "affected_populations": [
        "non-Western populations"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed conference papers for participant demographics",
      "geographic_focus": [
        "Western countries",
        "U.S."
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.05827v1",
    "title": "Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation",
    "year": 2023,
    "authors": [
      "Xiyang Hu",
      "Yan Huang",
      "Beibei Li",
      "Tian Lu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in loan screening affecting socioeconomic groups, promoting financial inclusion and fairness.",
      "inequality_type": [
        "socioeconomic",
        "financial",
        "inequality"
      ],
      "other_detail": "Bias mitigation in credit decision algorithms",
      "affected_populations": [
        "low socioeconomic borrowers"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Learning",
        "Domain Adaptation",
        "Experiment"
      ],
      "methodology_detail": "Transformer-based model with self-supervised learning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.06166v2",
    "title": "ChatGPT as a Text Simplification Tool to Remove Bias",
    "year": 2023,
    "authors": [
      "Charmaine Barker",
      "Dimitar Kazakov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in language models related to protected groups, which can lead to discriminatory decisions, thus engaging with social bias and fairness issues.",
      "inequality_type": [
        "racial",
        "linguistic",
        "social bias"
      ],
      "other_detail": "Focuses on language bias and discrimination mitigation",
      "affected_populations": [
        "sub-groups of people"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias reduction via text simplification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.04672v1",
    "title": "Augmented Datasheets for Speech Datasets and Ethical Decision-Making",
    "year": 2023,
    "authors": [
      "Orestis Papakyriakopoulos",
      "Anna Seo Gyeong Choi",
      "Jerone Andrews",
      "Rebecca Bourke",
      "William Thong",
      "Dora Zhao",
      "Alice Xiang",
      "Allison Koenecke"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper emphasizes diversity, ethics, and inclusivity in speech datasets, addressing social biases and fairness issues affecting marginalized groups.",
      "inequality_type": [
        "socioeconomic",
        "linguistic",
        "disability",
        "health"
      ],
      "other_detail": "Focus on equitable speech technology development",
      "affected_populations": [
        "speech-impaired",
        "linguistic minorities",
        "demographic groups"
      ],
      "methodology": [
        "Literature Review",
        "Dataset Creation",
        "Ethics Analysis"
      ],
      "methodology_detail": "Reviewing speech data ethics and diversity considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.03881v2",
    "title": "Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing",
    "year": 2023,
    "authors": [
      "Swagatika Dash"
    ],
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias and stereotypes in image search results, addressing social discrimination and fairness issues related to gender. It analyzes how AI systems can perpetuate occupational stereotypes, impacting perceptions and social equality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Occupational stereotypes in image retrieval",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement and re-ranking evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.03712v2",
    "title": "Statistical Inference for Fairness Auditing",
    "year": 2023,
    "authors": [
      "John J. Cherian",
      "Emmanuel J. Candès"
    ],
    "categories": [
      "stat.ME",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness auditing of AI models, addressing disparities across subpopulations, which relates to social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in predictive models",
      "affected_populations": [
        "demographic groups",
        "subpopulations"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Uses bootstrap for performance disparity bounds",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.13938v2",
    "title": "Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree",
    "year": 2023,
    "authors": [
      "Hilde Weerts",
      "Raphaële Xenidis",
      "Fabien Tarissan",
      "Henrik Palmer Olsen",
      "Mykola Pechenizkiy"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic unfairness and discrimination within EU law, focusing on fairness metrics and legal reasoning, which relate to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on legal and algorithmic fairness intersection",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Legal Analysis",
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Comparative legal and technical analysis",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.02629v1",
    "title": "Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A Case Study of Automated Video Interviews",
    "year": 2023,
    "authors": [
      "Brandon M Booth",
      "Louis Hickman",
      "Shree Krishna Subburaj",
      "Louis Tay",
      "Sang Eun Woo",
      "Sidney K. DMello"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias and fairness in affective computing, focusing on social discrimination and algorithmic impacts on social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focus on bias in AI systems affecting social groups",
      "affected_populations": [
        "job applicants",
        "video interviewees"
      ],
      "methodology": [
        "Case Study",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement in multimodal affective data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.02420v2",
    "title": "Beyond case studies: Teaching data science critique and ethics through sociotechnical surveillance studies",
    "year": 2023,
    "authors": [
      "Nicholas Rabb",
      "Desen Ozkan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social systems, oppression, and social theory in relation to surveillance, addressing issues of race, class, and social harms. It emphasizes analyzing social phenomena and inequalities embedded in sociotechnical systems.",
      "inequality_type": [
        "racial",
        "class",
        "social"
      ],
      "other_detail": "Focus on surveillance systems and social oppression",
      "affected_populations": [
        "racial minorities",
        "low-income groups",
        "social marginalized groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analysis of student reflections and critical social theories",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.01888v1",
    "title": "Fairness in AI Systems: Mitigating gender bias from language-vision models",
    "year": 2023,
    "authors": [
      "Lavisha Aggarwal",
      "Shruti Bhargava"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI systems, highlighting societal gender disparities and their amplification through machine learning models, which directly relates to social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Mitigation of gender bias in image captioning models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.01783v1",
    "title": "Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy",
    "year": 2023,
    "authors": [
      "Emily Aiken",
      "Esther Rolf",
      "Joshua Blumenstock"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in poverty representation across urban and rural areas, highlighting socioeconomic inequalities and biases in AI-based mapping that impact policy fairness.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on urban-rural disparities in poverty mapping",
      "affected_populations": [
        "urban poor",
        "rural poor"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of satellite and survey data across countries",
      "geographic_focus": [
        "ten countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.01141v1",
    "title": "Multidimensional Fairness in Paper Recommendation",
    "year": 2023,
    "authors": [
      "Reem Alsaffar",
      "Susan Gauch",
      "Hiba Al-Kawaz"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in academic paper recommendation, focusing on author diversity across protected social variables.",
      "inequality_type": [
        "gender",
        "ethnic",
        "geographic",
        "educational"
      ],
      "other_detail": "Multidimensional author diversity in AI recommendation systems",
      "affected_populations": [
        "female authors",
        "ethnic minorities",
        "early-career researchers"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates diversity algorithms using demographic profiles and recommendation outcomes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.00927v1",
    "title": "Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity",
    "year": 2023,
    "authors": [
      "Josh Gardner",
      "Renzhe Yu",
      "Quan Nguyen",
      "Christopher Brooks",
      "Rene Kizilcec"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and biases in educational models across institutions, highlighting intersectional disparities affecting student groups.",
      "inequality_type": [
        "educational",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in AI models",
      "affected_populations": [
        "students",
        "intersectional identity groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Transfer learning and fairness metrics applied to student data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.00817v1",
    "title": "Racial Bias within Face Recognition: A Survey",
    "year": 2023,
    "authors": [
      "Seyma Yucer",
      "Furkan Tektas",
      "Noura Al Moubayed",
      "Toby P. Breckon"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in face recognition, addressing racial disparities and societal implications of racial groupings.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in AI systems",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Comprehensive review of existing research",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.00147v2",
    "title": "Visualizing chest X-ray dataset biases using GANs",
    "year": 2023,
    "authors": [
      "Hao Liang",
      "Kevin Ni",
      "Guha Balakrishnan"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in chest X-ray datasets related to protected demographic attributes like race and gender, highlighting fairness issues in AI systems used for clinical predictions.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focuses on visual feature biases in medical imaging datasets",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Uses GANs to visualize feature differences",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.13933v1",
    "title": "Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy",
    "year": 2023,
    "authors": [
      "Louis Hickman",
      "Jason Kuruzovich",
      "Vincent Ng",
      "Kofi Arhin",
      "Danielle Wilson"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in ML training data and their impact on fairness, addressing racial inequality and bias in AI systems.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on minority groups in employment screening",
      "affected_populations": [
        "Black applicants",
        "Hispanic applicants"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Manipulation of training data sampling strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.13855v1",
    "title": "Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models",
    "year": 2023,
    "authors": [
      "Abhishek Mandal",
      "Susan Leavy",
      "Suzanne Little"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring gender bias in AI models, addressing social bias and fairness issues related to gender, which are aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement in multimodal models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.13419v2",
    "title": "Are Explainability Tools Gender Biased? A Case Study on Face Presentation Attack Detection",
    "year": 2023,
    "authors": [
      "Marco Huber",
      "Meiling Fang",
      "Fadi Boutros",
      "Naser Damer"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in explainability tools for face recognition, highlighting social discrimination aspects.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in AI explainability tools",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Case Study"
      ],
      "methodology_detail": "Analyzes bias in explainability tool outcomes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2305.02204v1",
    "title": "PopSim: An Individual-level Population Simulator for Equitable Allocation of City Resources",
    "year": 2023,
    "authors": [
      "Khanh Duy Nguyen",
      "Nima Shahbazi",
      "Abolfazl Asudeh"
    ],
    "categories": [
      "cs.CY",
      "E.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on racial segregation and equitable resource allocation in urban areas, addressing social disparities. It aims to audit and improve fairness in city resource distribution, which relates directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "geographic",
        "urban-rural"
      ],
      "other_detail": "Focus on racial segregation and resource fairness",
      "affected_populations": [
        "racial groups",
        "urban residents"
      ],
      "methodology": [
        "Dataset Creation",
        "Statistical Analysis",
        "Case Study"
      ],
      "methodology_detail": "Synthetic data generation and evaluation",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.12840v2",
    "title": "Spatiotemporal gender differences in urban vibrancy",
    "year": 2023,
    "authors": [
      "Thomas Collins",
      "Riccardo Di Clemente",
      "Mario Gutiérrez-Roig",
      "Federico Botta"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in urban vibrancy, highlighting potential gender segregation and inequality in city experiences, using high-frequency mobility data.",
      "inequality_type": [
        "gender",
        "urban-rural",
        "socioeconomic"
      ],
      "other_detail": "Focuses on gender disparities in urban activity patterns",
      "affected_populations": [
        "males",
        "females"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses Call Detail Record data for high-frequency spatial behavior",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.12573v1",
    "title": "Fairness and Bias in Truth Discovery Algorithms: An Experimental Analysis",
    "year": 2023,
    "authors": [
      "Simone Lazier",
      "Saravanan Thirumuruganathan",
      "Hadis Anahideh"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and fairness issues in AI algorithms, focusing on societal impacts related to sensitive attributes like gender and race.",
      "inequality_type": [
        "gender",
        "racial",
        "fairness"
      ],
      "other_detail": "Bias in crowd-labeled data affecting societal fairness",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias in crowd-labeled datasets and algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.11685v1",
    "title": "Child Face Recognition at Scale: Synthetic Data Generation and Performance Benchmark",
    "year": 2023,
    "authors": [
      "Magnus Falkenberg",
      "Anders Bensen Ottsen",
      "Mathias Ibsen",
      "Christian Rathgeb"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in facial recognition systems, highlighting disparities in performance across social groups, which relates to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Biases in AI systems affecting social groups",
      "affected_populations": [
        "children",
        "Asian",
        "Black",
        "female",
        "White",
        "Latino Hispanic"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Synthetic data generation and performance benchmarking",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.11530v5",
    "title": "A Conceptual Algorithm for Applying Ethical Principles of AI to Medical Practice",
    "year": 2023,
    "authors": [
      "Debesh Jha",
      "Gorkem Durak",
      "Vanshali Sharma",
      "Elif Keles",
      "Vedat Cicek",
      "Zheyuan Zhang",
      "Abhishek Srivastava",
      "Ashish Rauniyar",
      "Desta Haileselassie Hagos",
      "Nikhil Kumar Tomar",
      "Frank H. Miller",
      "Ahmet Topcu",
      "Anis Yazidi",
      "Jan Erik Håkegård",
      "Ulas Bagci"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI's potential to reduce disparities in healthcare access across demographic, racial, and socioeconomic groups, and addresses issues of bias and fairness in AI systems.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on healthcare disparities and algorithmic fairness",
      "affected_populations": [
        "racial minorities",
        "low-income groups",
        "underserved communities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis",
        "System Design"
      ],
      "methodology_detail": "Analyzes ethical frameworks and technical solutions",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.12810v1",
    "title": "Transcending the \"Male Code\": Implicit Masculine Biases in NLP Contexts",
    "year": 2023,
    "authors": [
      "Katie Seaborn",
      "Shruti Chandra",
      "Thibault Fabre"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines implicit masculine biases in language used by NLP systems, addressing gender bias and social discrimination. It focuses on how language coding perpetuates gender inequalities, especially masculinity norms. This relates directly to social inequality in gender representation and bias in AI.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "implicit masculine biases in language and NLP",
      "affected_populations": [
        "women",
        "femme-identifying",
        "genderqueer"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzed NLP datasets for gender bias patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.11223v1",
    "title": "A Group-Specific Approach to NLP for Hate Speech Detection",
    "year": 2023,
    "authors": [
      "Karina Halevy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting hate speech, which relates to social discrimination against protected groups, addressing issues of bias, stereotypes, and historical discrimination.",
      "inequality_type": [
        "racial",
        "ethnic",
        "discrimination"
      ],
      "other_detail": "Focus on antisemitism and hate speech detection",
      "affected_populations": [
        "Jewish community"
      ],
      "methodology": [
        "Natural Language Processing",
        "Case Study",
        "Knowledge Graph Infusion"
      ],
      "methodology_detail": "Incorporates historical and linguistic knowledge into NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.10190v1",
    "title": "A Large-scale Examination of \"Socioeconomic\" Fairness in Mobile Networks",
    "year": 2023,
    "authors": [
      "Souneil Park",
      "Pavol Mulinka",
      "Diego Perino"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines socioeconomic disparities in mobile network performance and deployment, linking infrastructure and service access to socioeconomic status, which relates to social inequality.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on socioeconomic status and urban geography",
      "affected_populations": [
        "mobile users",
        "urban residents"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Geo-socioeconomic perspective and infrastructure analysis",
      "geographic_focus": [
        "multiple cities"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.09362v2",
    "title": "Long-Term Fairness with Unknown Dynamics",
    "year": 2023,
    "authors": [
      "Tongxin Yin",
      "Reilly Raab",
      "Mingyan Liu",
      "Yang Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems affecting populations, addressing social fairness issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Long-term fairness in population dynamics",
      "affected_populations": [
        "demographic groups",
        "population"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Algorithm Development",
        "Statistical Analysis"
      ],
      "methodology_detail": "Adapts online learning for fairness over time",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.09867v1",
    "title": "Introducing Construct Theory as a Standard Methodology for Inclusive AI Models",
    "year": 2023,
    "authors": [
      "Susanna Raj",
      "Sudha Jamthe",
      "Yashaswini Viswanath",
      "Suresh Lokiah"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in AI facial recognition and proposes inclusive constructs to improve fairness, directly engaging with social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender"
      ],
      "other_detail": "Focuses on racial and gender inclusivity in AI models",
      "affected_populations": [
        "darker skin individuals",
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Testing AI APIs with curated datasets for inclusivity",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.08967v2",
    "title": "All a-board: sharing educational data science research with school districts",
    "year": 2023,
    "authors": [
      "Nabeel Gillani",
      "Doug Beeferman",
      "Cassandra Overney",
      "Christine Vega-Pourheydarian",
      "Deb Roy"
    ],
    "categories": [
      "cs.CY",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on racial and ethnic segregation in schools, a key social inequality issue, and explores how data-driven boundary changes can promote diversity and equity.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focus on school segregation and diversity",
      "affected_populations": [
        "racial minorities",
        "students in segregated districts"
      ],
      "methodology": [
        "Experiment",
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Email outreach and survey analysis",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.08882v2",
    "title": "Along the Margins: Marginalized Communities' Ethical Concerns about Social Platforms",
    "year": 2023,
    "authors": [
      "Lauren Olson",
      "Emitzá Guzmán",
      "Florian Kunneman"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines marginalized communities' ethical concerns, highlighting issues of discrimination and misrepresentation, which relate directly to social inequalities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "disability",
        "social discrimination"
      ],
      "other_detail": "Focuses on marginalized communities' concerns about social platforms",
      "affected_populations": [
        "marginalized communities"
      ],
      "methodology": [
        "Data Scraping",
        "Manual Annotation",
        "Natural Language Processing",
        "Trend Analysis"
      ],
      "methodology_detail": "Analyzed social media mentions and automated classification",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.08750v1",
    "title": "Not Only WEIRD but \"Uncanny\"? A Systematic Review of Diversity in Human-Robot Interaction Research",
    "year": 2023,
    "authors": [
      "Katie Seaborn",
      "Giulia Barbareschi",
      "Shruti Chandra"
    ],
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines representation and sampling biases across diverse social groups in HRI research, highlighting issues related to race, gender, age, disability, and other axes of human diversity, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focus on underrepresentation and misreporting in participant sampling",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly",
        "disabled",
        "diverse genders"
      ],
      "methodology": [
        "Systematic Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed 827 studies from major conferences",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.11094v1",
    "title": "Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis",
    "year": 2023,
    "authors": [
      "Vithya Yogarajan",
      "Gillian Dobbie",
      "Henry Gouk"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in language models affecting indigenous populations, highlighting social bias and inequality issues beyond US racial biases.",
      "inequality_type": [
        "racial",
        "ethnic",
        "geographic"
      ],
      "other_detail": "Focus on indigenous populations outside US context",
      "affected_populations": [
        "indigenous groups",
        "Māori"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Indigenous perspective analysis",
      "geographic_focus": [
        "New Zealand"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.08530v1",
    "title": "Popular Support for Balancing Equity and Efficiency in Resource Allocation: A Case Study in Online Advertising to Increase Welfare Program Awareness",
    "year": 2023,
    "authors": [
      "Allison Koenecke",
      "Eric Giannella",
      "Robb Willer",
      "Sharad Goel"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines resource allocation disparities affecting Spanish speakers, highlighting equity concerns and social preferences, thus addressing social inequality.",
      "inequality_type": [
        "linguistic",
        "socioeconomic"
      ],
      "other_detail": "Focus on language-based resource access disparities",
      "affected_populations": [
        "Spanish speakers",
        "immigrant communities"
      ],
      "methodology": [
        "Case Study",
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes ad costs and survey preferences",
      "geographic_focus": [
        "California"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.08041v1",
    "title": "Inclusive Child-centered AI: Employing design futuring for Inclusive design of inclusive AI by and with children in Finland and India",
    "year": 2023,
    "authors": [
      "Sumita Sharma",
      "Netta Iivari",
      "Leena Ventä-Olkkonen",
      "Heidi Hartikainen",
      "Marianne Kinnula"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses designing inclusive AI for children, addressing biases and fairness, which relate to social inequalities such as gender, race, and social inclusion.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social"
      ],
      "other_detail": "Focus on inclusive design and social fairness in AI",
      "affected_populations": [
        "children",
        "students in India",
        "students in Finland"
      ],
      "methodology": [
        "Case Study",
        "Qualitative Study",
        "Design Futuring"
      ],
      "methodology_detail": "Children's participatory future-oriented design sessions",
      "geographic_focus": [
        "India",
        "Finland"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.09826v2",
    "title": "Fairness in AI and Its Long-Term Implications on Society",
    "year": 2023,
    "authors": [
      "Ondrej Bohdal",
      "Timothy Hospedales",
      "Philip H. S. Torr",
      "Fazl Barez"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI systems that can deepen social disparities and impact marginalized groups, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on bias and societal impacts of AI fairness",
      "affected_populations": [
        "disadvantaged groups",
        "marginalized communities"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes current fairness strategies and societal implications",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.07683v2",
    "title": "Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, And Mitigation Strategies",
    "year": 2023,
    "authors": [
      "Emilio Ferrara"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI bias, fairness, and societal impacts, addressing social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on societal biases and inequalities in AI systems",
      "affected_populations": [
        "minorities",
        "patients",
        "workers",
        "content consumers"
      ],
      "methodology": [
        "Literature Review",
        "Systematic Analysis"
      ],
      "methodology_detail": "Review of interdisciplinary research on AI bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.06861v1",
    "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
    "year": 2023,
    "authors": [
      "Swapnil Sharma",
      "Nikita Anand",
      "Kranthi Kiran G. V.",
      "Alind Jain"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases in AI models, which reflect societal inequalities such as race and gender, and discusses their impact on social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on social biases in language models",
      "affected_populations": [
        "social groups",
        "targeted groups"
      ],
      "methodology": [
        "Benchmark Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias benchmarks comparison across models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.06596v1",
    "title": "Beyond Submodularity: A Unified Framework of Randomized Set Selection with Group Fairness Constraints",
    "year": 2023,
    "authors": [
      "Shaojie Tang",
      "Jing Yuan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in algorithmic decision-making, addressing bias and discrimination issues related to social groups such as gender. It aims to ensure equitable outcomes across different groups, which directly relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "discrimination",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness constraints",
      "affected_populations": [
        "gender groups",
        "social groups"
      ],
      "methodology": [
        "Algorithm Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Framework for randomized subset selection with fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.06205v2",
    "title": "Difficult Lessons on Social Prediction from Wisconsin Public Schools",
    "year": 2023,
    "authors": [
      "Juan C. Perdomo",
      "Tolani Britton",
      "Moritz Hardt",
      "Rediet Abebe"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "econ.GN",
      "q-fin.EC",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines dropout rates and their structural determinants, highlighting inequality in educational outcomes. It questions the value of individual risk predictions in contexts driven by inequality, implying a focus on social disparities.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on structural factors influencing dropout and intervention targeting",
      "affected_populations": [
        "students",
        "public schools"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Long-term data analysis of dropout risk and intervention effects",
      "geographic_focus": [
        "Wisconsin"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.06182v4",
    "title": "GNNUERS: Fairness Explanation in GNNs for Recommendation via Counterfactual Reasoning",
    "year": 2023,
    "authors": [
      "Giacomo Medda",
      "Francesco Fabbri",
      "Mirko Marras",
      "Ludovico Boratto",
      "Gianni Fenu"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and unfairness in recommendation systems, addressing social bias and discrimination issues related to protected groups.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness explanations in AI systems",
      "affected_populations": [
        "users",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Counterfactual reasoning in GNN-based recommendation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.05986v1",
    "title": "Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes",
    "year": 2023,
    "authors": [
      "Shaina Raza"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities in AI predictions based on sensitive attributes like gender, ethnicity, and insurance, highlighting social bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "health"
      ],
      "other_detail": "Focus on healthcare disparities and AI fairness",
      "affected_populations": [
        "patients by gender",
        "ethnicity groups",
        "insurance groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Fairness Analysis"
      ],
      "methodology_detail": "Fairness audit on model predictions across subgroups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.05783v3",
    "title": "Measuring Gender Bias in West Slavic Language Models",
    "year": 2023,
    "authors": [
      "Sandra Martinková",
      "Karolina Stańczak",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, highlighting social discrimination and bias issues related to gender. It assesses how AI systems encode and perpetuate harmful gender stereotypes, impacting social perceptions.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in Slavic languages",
      "affected_populations": [
        "women",
        "men",
        "non-binary individuals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Template-based bias measurement and toxicity analysis",
      "geographic_focus": [
        "Czech Republic",
        "Poland",
        "Slovakia"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.05603v2",
    "title": "Potential for allocative harm in an environmental justice data tool",
    "year": 2023,
    "authors": [
      "Benjamin Q. Huynh",
      "Elizabeth T. Chin",
      "Allison Koenecke",
      "Derek Ouyang",
      "Daniel E. Ho",
      "Mathew V. Kiang",
      "David H. Rehkopf"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates an environmental justice algorithm affecting resource allocation, highlighting potential disparities and ethical concerns related to social groups and funding impacts.",
      "inequality_type": [
        "environmental",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "focus on environmental justice and resource allocation",
      "affected_populations": [
        "neighborhoods",
        "communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "assessing model sensitivity and impact estimates",
      "geographic_focus": [
        "California"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.05481v1",
    "title": "Measuring Latency Reduction and the Digital Divide of Cloud Edge Datacenters",
    "year": 2023,
    "authors": [
      "Noah Martin",
      "Fahad Dogar"
    ],
    "categories": [
      "cs.NI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines digital divides based on geographic and socioeconomic factors, highlighting inequality in access to cloud edge computing and its impact on lower-income populations globally.",
      "inequality_type": [
        "income",
        "wealth",
        "socioeconomic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focuses on digital access disparities and socioeconomic impacts",
      "affected_populations": [
        "lower income US census tracts",
        "global low-income populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Latency measurements and satellite imagery analysis",
      "geographic_focus": [
        "United States",
        "Global"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.05391v1",
    "title": "Pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies",
    "year": 2023,
    "authors": [
      "Laura Gustafson",
      "Megan Richards",
      "Melissa Hall",
      "Caner Hazirbas",
      "Diane Bouchacourt",
      "Mark Ibrahim"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates performance disparities of AI systems across income and geographic groups, highlighting inequities. It analyzes how object recognition varies with socioeconomic and regional factors, addressing technological impacts on social groups.",
      "inequality_type": [
        "income",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on AI performance disparities across social groups",
      "affected_populations": [
        "low-income groups",
        "regional populations"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Annotating images and analyzing model vulnerabilities",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.05232v2",
    "title": "Lady and the Tramp Nextdoor: Online Manifestations of Economic Inequalities in the Nextdoor Social Network",
    "year": 2023,
    "authors": [
      "Waleed Iqbal",
      "Vahid Ghafouri",
      "Gareth Tyson",
      "Guillermo Suarez-Tangil",
      "Ignacio Castro"
    ],
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how online discourse varies by neighborhood income, reflecting socioeconomic inequalities. It analyzes disparities in sentiment and discussion topics, linking online behavior to economic status.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focuses on neighborhood-level income disparities",
      "affected_populations": [
        "neighborhood residents"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predicts income and inequality from online posts",
      "geographic_focus": [
        "United States",
        "United Kingdom"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.04874v2",
    "title": "ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment",
    "year": 2023,
    "authors": [
      "Eslam Mohamed Bakr",
      "Pengzhan Sun",
      "Li Erran Li",
      "Mohamed Elhoseiny"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias in image captioning related to protected attributes like gender, race, and emotions, which are social categories linked to inequality. It assesses bias amplification, a social fairness concern, in AI systems. The emphasis on bias measurement and social attributes indicates addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "emotional"
      ],
      "other_detail": "Bias amplification in AI image captioning systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "emotionally marginalized"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Developing and applying bias measurement metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.04849v4",
    "title": "The presence of White students and the emergence of Black-White within-school inequalities: two interaction-based mechanisms",
    "year": 2023,
    "authors": [
      "João M. Souto-Maior"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.MA",
      "econ.TH"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in educational resource access within schools, focusing on Black-White inequalities. It analyzes how the presence of White students influences these disparities, addressing racial and educational inequalities. The study aims to shed light on mechanisms affecting marginalized groups in racially integrated settings.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "",
      "affected_populations": [
        "Black students",
        "White students"
      ],
      "methodology": [
        "Agent-based modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Empirically calibrated and validated simulation model",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.04700v1",
    "title": "Achieving Long-term Fairness in Submodular Maximization through Randomization",
    "year": 2023,
    "authors": [
      "Shaojie Tang",
      "Jing Yuan",
      "Twumasi Mensah-Boateng"
    ],
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints related to group representation, which directly pertains to social fairness issues such as bias and inequality in algorithmic outcomes.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on group fairness in data selection algorithms",
      "affected_populations": [
        "social groups",
        "data subjects"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Designing approximation algorithms for fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.04199v1",
    "title": "Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks",
    "year": 2023,
    "authors": [
      "Verya Monjezi",
      "Ashutosh Trivedi",
      "Gang Tan",
      "Saeid Tizpaz-Niari"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness defects in AI systems affecting protected groups, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and bias in decision-making systems",
      "affected_populations": [
        "protected individuals",
        "disadvantaged groups"
      ],
      "methodology": [
        "Information-Theoretic Analysis",
        "Debugging Framework",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Quantifies fairness via information measures and causal debugging",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.06467v2",
    "title": "Towards Understanding the Benefits and Challenges of Demand Responsive Public Transit- A Case Study in the City of Charlotte, NC",
    "year": 2023,
    "authors": [
      "Sanaz Sadat Hosseini",
      "Mona Azarbayjani",
      "Jason Lawrence",
      "Hamed Tabkhi"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in public transit access affecting low-income communities, highlighting socioeconomic and geographic inequalities.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on transit access disparities in low-income areas",
      "affected_populations": [
        "low-income residents",
        "transit-dependent communities"
      ],
      "methodology": [
        "Survey",
        "Case Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing user acceptance and transit system issues",
      "geographic_focus": [
        "Charlotte, NC"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.04761v1",
    "title": "Connecting Fairness in Machine Learning with Public Health Equity",
    "year": 2023,
    "authors": [
      "Shaina Raza"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in ML affecting healthcare disparities, linking AI fairness to health equity, which relates to social inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focus on healthcare disparities and fairness",
      "affected_populations": [
        "protected groups",
        "healthcare populations"
      ],
      "methodology": [
        "Literature Review",
        "Framework Development",
        "Case Study"
      ],
      "methodology_detail": "Guidance on bias mitigation in ML pipeline",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.03801v1",
    "title": "Towards Inclusive Fairness Evaluation via Eliciting Disagreement Feedback from Non-Expert Stakeholders",
    "year": 2023,
    "authors": [
      "Mukund Telukunta",
      "Venkata Sriram Siddhardh Nadendla"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness notions in decision-making systems involving social groups, highlighting social discrimination and stakeholder perceptions, which relate to social inequality issues.",
      "inequality_type": [
        "racial",
        "health",
        "educational"
      ],
      "other_detail": "Focuses on social fairness perceptions in AI systems",
      "affected_populations": [
        "patients",
        "donors",
        "families"
      ],
      "methodology": [
        "Experiment",
        "Empirical Analysis"
      ],
      "methodology_detail": "Validation with real human feedback dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.03693v3",
    "title": "Model-Agnostic Gender Debiased Image Captioning",
    "year": 2023,
    "authors": [
      "Yusuke Hirota",
      "Yuta Nakashima",
      "Noa Garcia"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI, a form of social inequality, by proposing methods to reduce harmful societal gender stereotypes in image captioning models.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias mitigation in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation framework using synthetic data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.02828v1",
    "title": "Uncurated Image-Text Datasets: Shedding Light on Demographic Bias",
    "year": 2023,
    "authors": [
      "Noa Garcia",
      "Yusuke Hirota",
      "Yankun Wu",
      "Yuta Nakashima"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes societal bias and representation disparities in image-text datasets, highlighting how these biases affect different demographic groups and AI tasks, thus addressing social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on demographic bias in AI datasets and models",
      "affected_populations": [
        "demographic groups",
        "societal minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Annotated datasets and analyzed bias impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.02819v3",
    "title": "GPT detectors are biased against non-native English writers",
    "year": 2023,
    "authors": [
      "Weixin Liang",
      "Mert Yuksekgonul",
      "Yining Mao",
      "Eric Wu",
      "James Zou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI detectors against non-native English speakers, highlighting fairness issues affecting linguistic groups.",
      "inequality_type": [
        "linguistic",
        "educational"
      ],
      "other_detail": "Bias against non-native English writers in AI detection",
      "affected_populations": [
        "non-native English speakers"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Evaluating detector performance on different language backgrounds",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.02780v2",
    "title": "A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors",
    "year": 2023,
    "authors": [
      "Can Li",
      "Xiaoqian Jiang",
      "Kai Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI predictions across social groups, including gender, age, and race/ethnicity, aiming to reduce disparities in post-transplant risk assessments.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Focus on fairness in medical AI predictions",
      "affected_populations": [
        "transplant patients",
        "subpopulations by race/ethnicity",
        "gender",
        "age groups"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Algorithm",
        "Multi-task Learning"
      ],
      "methodology_detail": "Fairness-enhancing algorithms applied to predictive models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.02351v1",
    "title": "Constructing and deconstructing bias: modeling privilege and mentorship in agent-based simulations",
    "year": 2023,
    "authors": [
      "Andria L. Smith",
      "Simon Heuschkel",
      "Ksenia Keplinger",
      "Charley M. Wu"
    ],
    "categories": [
      "cs.MA"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models bias related to social features like race and gender, and discusses interventions to reduce bias, indicating a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias and privilege in social influence and leadership",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Agent-based modeling",
        "Simulation",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Modeling bias dynamics via simulations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.02284v1",
    "title": "Gradient Attention Balance Network: Mitigating Face Recognition Racial Bias via Gradient Attention",
    "year": 2023,
    "authors": [
      "Linzhi Huang",
      "Mei Wang",
      "Jiahao Liang",
      "Weihong Deng",
      "Hongzhi Shi",
      "Dongchao Wen",
      "Yingjie Zhang",
      "Jian Zhao"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition systems, highlighting disparities across racial groups.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial bias mitigation in AI systems",
      "affected_populations": [
        "darker-skinned people",
        "Caucasians"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Gradient attention and adversarial learning techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.02190v1",
    "title": "Globalizing Fairness Attributes in Machine Learning: A Case Study on Health in Africa",
    "year": 2023,
    "authors": [
      "Mercy Nyamewaa Asiedu",
      "Awa Dieng",
      "Abigail Oppong",
      "Maria Nagawa",
      "Sanmi Koyejo",
      "Katherine Heller"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in ML within the context of global health inequities in Africa, highlighting social disparities and ethical concerns related to power imbalances between regions.",
      "inequality_type": [
        "health",
        "geographic",
        "social",
        "inequity"
      ],
      "other_detail": "Focus on fairness attributes in healthcare ML applications",
      "affected_populations": [
        "African populations",
        "global health communities"
      ],
      "methodology": [
        "Case Study",
        "Fairness Attribute Proposal"
      ],
      "methodology_detail": "Developing fairness considerations for ML in health",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2304.06484v1",
    "title": "Exploring Gender and Race Biases in the NFT Market",
    "year": 2023,
    "authors": [
      "Howard Zhong",
      "Mark Hamilton"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "econ.GN",
      "q-fin.EC",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in NFT pricing, addressing social discrimination and fairness issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on social bias in digital asset valuation",
      "affected_populations": [
        "female",
        "darker-skinned individuals"
      ],
      "methodology": [
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes NFT price data and creates gender-labeled dataset",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.15592v2",
    "title": "Uncovering Bias in Personal Informatics",
    "year": 2023,
    "authors": [
      "Sofia Yfantidou",
      "Pavlos Sermpezis",
      "Athena Vakali",
      "Ricardo Baeza-Yates"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in personal health data affecting minority groups, including women and individuals with health issues, highlighting social disparities and fairness concerns in AI systems.",
      "inequality_type": [
        "health",
        "gender",
        "disability"
      ],
      "other_detail": "Focus on health-related social biases in AI systems",
      "affected_populations": [
        "women",
        "health minorities"
      ],
      "methodology": [
        "Empirical Study",
        "Analytical Framework"
      ],
      "methodology_detail": "Comprehensive bias analysis in data and models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.14128v1",
    "title": "The crime of being poor",
    "year": 2023,
    "authors": [
      "Georgina Curto",
      "Svetlana Kiritchenko",
      "Isar Nejadgholi",
      "Kathleen C. Fraser"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal biases linking poverty and crime, highlighting social discrimination and perceptions affecting vulnerable groups.",
      "inequality_type": [
        "socioeconomic",
        "class"
      ],
      "other_detail": "Focus on societal bias and perceptions",
      "affected_populations": [
        "the poor"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes Twitter data for bias measurement",
      "geographic_focus": [
        "eight English-speaking countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.13994v1",
    "title": "An Agent-Based Model for Poverty and Discrimination Policy-Making",
    "year": 2023,
    "authors": [
      "Nieves Montes",
      "Georgina Curto",
      "Nardine Osman",
      "Carles Sierra"
    ],
    "categories": [
      "cs.MA"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination against the poor (aporophobia) and its impact on poverty, addressing social inequality and discrimination. It also considers legal and institutional biases affecting marginalized groups.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on institutional discrimination against the poor",
      "affected_populations": [
        "poor",
        "disadvantaged"
      ],
      "methodology": [
        "Agent-Based Modeling",
        "Case Study",
        "Simulation"
      ],
      "methodology_detail": "Modeling institutional discrimination effects on poverty levels",
      "geographic_focus": [
        "Barcelona"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.13790v1",
    "title": "Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint",
    "year": 2023,
    "authors": [
      "Chia-Yuan Chang",
      "Jiayi Yuan",
      "Sirui Ding",
      "Qiaoyu Tan",
      "Kai Zhang",
      "Xiaoqian Jiang",
      "Xia Hu",
      "Na Zou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI systems related to underrepresented groups in clinical trials, highlighting social bias concerns.",
      "inequality_type": [
        "health",
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in patient-trial matching",
      "affected_populations": [
        "underrepresented patients",
        "minority groups"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Constraint",
        "Experiment"
      ],
      "methodology_detail": "Patient-criterion level fairness constraint implementation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.12913v1",
    "title": "What do Transgender Software Professionals say about a Career in the Software Industry?",
    "year": 2023,
    "authors": [
      "Ronnie de Souza Santos",
      "Brody Stuart-Verner",
      "Cleyton Magalhaes"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-based discrimination and workplace experiences of transgender professionals, addressing gender inequality in the software industry.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on transgender workplace experiences",
      "affected_populations": [
        "transgender professionals"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Interviews or surveys on transgender professionals' perspectives",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.12179v1",
    "title": "Mapping Language Literacy At Scale: A Case Study on Facebook",
    "year": 2023,
    "authors": [
      "Yu-Ru Lin",
      "Shaomei Wu",
      "Winter Mason"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in language literacy across gender, regions, and socioeconomic factors, highlighting inequalities in digital access and skills.",
      "inequality_type": [
        "gender",
        "socioeconomic",
        "geographic",
        "educational"
      ],
      "other_detail": "Focus on online literacy disparities and offline inequalities",
      "affected_populations": [
        "women",
        "men",
        "low-resource countries",
        "regional populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "Using aggregated Facebook posts for literacy estimation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.11529v1",
    "title": "Counterfactually Fair Regression with Double Machine Learning",
    "year": 2023,
    "authors": [
      "Patrick Rehill"
    ],
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI decisions related to discrimination, such as workplace hiring and GPA estimation, which are linked to social inequalities like gender and educational disparities.",
      "inequality_type": [
        "educational",
        "gender",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "job applicants",
        "law students"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses double machine learning for fairness adjustments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.11449v1",
    "title": "Bias mitigation techniques in image classification: fair machine learning in human heritage collections",
    "year": 2023,
    "authors": [
      "Dalia Ortiz Pablo",
      "Sushruth Badri",
      "Erik Norén",
      "Christoph Nötzli"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems affecting diverse populations, particularly in gender classification within cultural heritage collections, highlighting social discrimination concerns.",
      "inequality_type": [
        "gender",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "focus on fairness in image classification",
      "affected_populations": [
        "diverse ethnicity",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Bias mitigation techniques"
      ],
      "methodology_detail": "evaluates bias mitigation in neural networks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.11408v2",
    "title": "Stable Bias: Analyzing Societal Representations in Diffusion Models",
    "year": 2023,
    "authors": [
      "Alexandra Sasha Luccioni",
      "Christopher Akiki",
      "Margaret Mitchell",
      "Yacine Jernite"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases in AI-generated images related to gender, ethnicity, and representation, highlighting under-representation of marginalized identities, which directly relates to social inequalities.",
      "inequality_type": [
        "gender",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI representations of social groups",
      "affected_populations": [
        "marginalized identities",
        "social minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Bias measurement and comparison in image generation",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.10721v1",
    "title": "Right the docs: Characterising voice dataset documentation practices used in machine learning",
    "year": 2023,
    "authors": [
      "Kathy Reid",
      "Elizabeth T. Williams"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.SD",
      "eess.AS",
      "K.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias in voice AI systems across axes like age, gender, and accent, highlighting issues of unfair discrimination and bias reduction efforts, which relate to social inequalities.",
      "inequality_type": [
        "age",
        "gender",
        "accent"
      ],
      "other_detail": "Bias and fairness in voice datasets",
      "affected_populations": [
        "voice users",
        "marginalized groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Semi-structured Interviews",
        "Document Analysis"
      ],
      "methodology_detail": "Exploratory, combining interviews and dataset document analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.12808v1",
    "title": "PACO: Provocation Involving Action, Culture, and Oppression",
    "year": 2023,
    "authors": [
      "Vaibhav Garg",
      "Ganning Xu",
      "Munindar P. Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses religious provocation and discrimination in India, highlighting social tensions and potential violence, which are forms of social inequality.",
      "inequality_type": [
        "religion",
        "social discrimination"
      ],
      "other_detail": "Focus on religious tensions and provocation",
      "affected_populations": [
        "Muslims",
        "Hindus"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Using datasets and fine-tuned models for detection",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.10431v1",
    "title": "DeAR: Debiasing Vision-Language Models with Additive Residuals",
    "year": 2023,
    "authors": [
      "Ashish Seth",
      "Mayur Hemani",
      "Chirag Agarwal"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in vision-language models related to societal identity groups, impacting fairness and representation. It introduces a debiasing method and a dataset to evaluate fairness, directly engaging with social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "identity groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Debiasing technique and fairness benchmarking dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.10131v1",
    "title": "She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models",
    "year": 2023,
    "authors": [
      "Christoph Treude",
      "Hideaki Hata"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI systems related to software development tasks, highlighting gender-based disparities. It addresses social discrimination embedded in technology, specifically gender bias, which is a form of social inequality. The focus on gender bias in AI reflects broader societal inequalities related to gender roles.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Data Mining",
        "Systematic Translation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing pronoun associations across multiple translations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.09727v1",
    "title": "Towards Understanding the Open Source Interest in Gender-Related GitHub Projects",
    "year": 2023,
    "authors": [
      "Rita Garcia",
      "Christoph Treude",
      "Wendy La"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-related projects on GitHub, focusing on gender bias and inclusivity, which directly relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Data Mining",
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes project attributes and classifies gender bias",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.17555v2",
    "title": "Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness",
    "year": 2023,
    "authors": [
      "Anaelia Ovalle",
      "Arjun Subramonian",
      "Vagrant Gautam",
      "Gilbert Gee",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "K.4; I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines social inequalities such as race, gender, and social power within AI fairness, emphasizing intersectionality as a framework to address persistent social disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "social power"
      ],
      "other_detail": "Focuses on social inequalities in AI fairness research",
      "affected_populations": [
        "social minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Literature Review",
        "Critical Analysis"
      ],
      "methodology_detail": "Analyzes AI fairness literature and intersectionality discussions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social inequalities",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.12734v1",
    "title": "MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models",
    "year": 2023,
    "authors": [
      "Sepehr Janghorbani",
      "Gerard de Melo"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper assesses bias in AI models across multiple social groups beyond gender and race, addressing social discrimination and inequality in technology's impact on marginalized populations.",
      "inequality_type": [
        "racial",
        "religion",
        "nationality",
        "sexual orientation",
        "disability"
      ],
      "other_detail": "Expands bias assessment beyond traditional groups",
      "affected_populations": [
        "minorities",
        "religious groups",
        "disabilities",
        "sexual minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias benchmark and debiasing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.08403v1",
    "title": "DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision",
    "year": 2023,
    "authors": [
      "Sungwon Han",
      "Seungeon Lee",
      "Fangzhao Wu",
      "Sundong Kim",
      "Chuhan Wu",
      "Xiting Wang",
      "Xing Xie",
      "Meeyoung Cha"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing social discrimination related to gender and race, and aims to reduce bias in representations, directly engaging with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Fairness in AI representations",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Self-supervision",
        "Self-knowledge distillation"
      ],
      "methodology_detail": "Joint fairness optimization techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.08026v1",
    "title": "A Study on Bias and Fairness In Deep Speaker Recognition",
    "year": 2023,
    "authors": [
      "Amirhossein Hajavi",
      "Ali Etemad"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in speaker recognition systems across gender and nationality groups, addressing social bias issues in AI. It discusses how model choices impact fairness, which relates to social discrimination concerns.",
      "inequality_type": [
        "gender",
        "nationality"
      ],
      "other_detail": "Focuses on bias in AI systems affecting social groups",
      "affected_populations": [
        "gender groups",
        "nationality groups"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates fairness across different model architectures and loss functions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.07529v2",
    "title": "Thinking Upstream: Ethics and Policy Opportunities in AI Supply Chains",
    "year": 2023,
    "authors": [
      "David Gray Widder",
      "Richmond Wong"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses labor conditions and economic impacts in AI supply chains, highlighting issues related to workers' pay and working conditions, which are social inequalities. It emphasizes upstream ethical considerations affecting vulnerable populations involved in AI production.",
      "inequality_type": [
        "economic",
        "labor",
        "socioeconomic"
      ],
      "other_detail": "Focus on labor conditions in AI supply chains",
      "affected_populations": [
        "workers",
        "subcontracted laborers"
      ],
      "methodology": [
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing ethical and policy implications",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.07247v3",
    "title": "Are Models Trained on Indian Legal Data Fair?",
    "year": 2023,
    "authors": [
      "Sahil Girhepuje",
      "Anmol Goel",
      "Gokul S Krishnan",
      "Shreya Goyal",
      "Satyendra Pandey",
      "Ponnurangam Kumaraguru",
      "Balaraman Ravindran"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in AI models trained on Indian legal data, highlighting disparities related to social groups such as Hindus and Muslims, which reflects social inequality concerns.",
      "inequality_type": [
        "religion",
        "social discrimination"
      ],
      "other_detail": "Bias in legal AI models within Indian society",
      "affected_populations": [
        "Hindus",
        "Muslims"
      ],
      "methodology": [
        "Natural Language Processing",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Bias measurement using demographic parity",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.07202v1",
    "title": "Optimization of the location and design of urban green spaces",
    "year": 2023,
    "authors": [
      "Caroline Leboeuf",
      "Margarida Carvalho",
      "Yan Kestens",
      "Benoît Thierry"
    ],
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on equitable distribution and accessibility of urban green spaces, addressing disparities related to socioeconomic and geographic inequalities. It explicitly considers inequality attributes and environmental justice in urban planning. The methodology aims to reduce inequities in park access among different neighborhoods.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "environmental"
      ],
      "other_detail": "Focus on urban green space accessibility disparities",
      "affected_populations": [
        "urban residents",
        "neighborhoods"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Optimization and spatial interaction modeling",
      "geographic_focus": [
        "Montreal"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.06376v1",
    "title": "Assessing gender fairness in EEG-based machine learning detection of Parkinson's disease: A multi-center study",
    "year": 2023,
    "authors": [
      "Anna Kurbatskaya",
      "Alberto Jaramillo-Jimenez",
      "John Fredy Ochoa-Gomez",
      "Kolbjørn Brønnick",
      "Alvaro Fernandez-Quilez"
    ],
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in ML-based PD detection, highlighting fairness and bias issues related to gender, a social attribute.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender fairness in medical AI",
      "affected_populations": [
        "males",
        "females"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of detection accuracy and EEG features",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05995v1",
    "title": "Exploring Gender Bias in Remote Pair Programming among Software Engineering Students: The twincode Original Study and First External Replication",
    "year": 2023,
    "authors": [
      "Amador Durán",
      "Pablo Fernández",
      "Beatriz Bernárdez",
      "Nathaniel Weinman",
      "Aslıhan Akalın",
      "Armando Fox"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates gender bias in software engineering education, addressing gender inequality and social discrimination. It examines how perceived gender influences collaborative behavior, highlighting social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in educational and professional context",
      "affected_populations": [
        "women in tech",
        "male students"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Remote pair programming, behavioral and questionnaire data",
      "geographic_focus": [
        "Spain",
        "USA"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05950v2",
    "title": "Diversity in Software Engineering: A Survey about Scientists from Underrepresented Groups",
    "year": 2023,
    "authors": [
      "Ronnie de Souza Santos",
      "Brody Stuart-Verner",
      "Cleyton de Magalhaes"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses underrepresentation and discrimination of marginalized groups in software engineering, highlighting issues related to gender, race, and inclusion, which are social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on diversity and inclusion in tech education",
      "affected_populations": [
        "women",
        "LGBTQIA+",
        "non-white individuals"
      ],
      "methodology": [
        "Survey",
        "Qualitative Study"
      ],
      "methodology_detail": "Preliminary survey with students on awareness and representation",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05862v1",
    "title": "Monitoring Gender Gaps via LinkedIn Advertising Estimates: the case study of Italy",
    "year": 2023,
    "authors": [
      "Margherita Bertè",
      "Kyriaki Kalimeri",
      "Daniela Paolotti"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in the labor market, focusing on underrepresentation and inequalities across sociodemographic groups, using digital data sources.",
      "inequality_type": [
        "gender",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on gender gaps and regional disparities in Italy",
      "affected_populations": [
        "women",
        "Southern Italy"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Using LinkedIn advertising estimates for data collection",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05698v1",
    "title": "Fairness-enhancing deep learning for ride-hailing demand prediction",
    "year": 2023,
    "authors": [
      "Yunhan Zheng",
      "Qingyi Wang",
      "Dingyi Zhuang",
      "Shenhao Wang",
      "Jinhua Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in demand prediction, focusing on disparities between disadvantaged and privileged communities, highlighting social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Fairness in spatial-temporal demand forecasting",
      "affected_populations": [
        "disadvantaged neighborhoods",
        "privileged communities"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Develops a socially aware neural network and bias mitigation techniques",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05670v1",
    "title": "Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning",
    "year": 2023,
    "authors": [
      "Hongyin Luo",
      "James Glass"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social stereotypes and biases in AI models, which relate to social discrimination and inequality. It discusses how biases reflect societal stereotypes, impacting social groups. The focus on bias mitigation in language models aligns with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Bias reduction in AI language understanding",
      "affected_populations": [
        "social communities",
        "minority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Comparing textual entailment and similarity-based models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05655v1",
    "title": "Data, Data, Everywhere: Uncovering Everyday Data Experiences for People with Intellectual and Developmental Disabilities",
    "year": 2023,
    "authors": [
      "Keke Wu",
      "Michelle H Tran",
      "Emma Petersen",
      "Varsha Koushik",
      "Danielle Albers Szafir"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses data accessibility and exclusion of people with IDD, highlighting social disparities in data engagement and ethical concerns.",
      "inequality_type": [
        "disability",
        "informational"
      ],
      "other_detail": "Focus on data accessibility for disabled populations",
      "affected_populations": [
        "people with IDD",
        "caregivers"
      ],
      "methodology": [
        "Qualitative Study",
        "Interview"
      ],
      "methodology_detail": "Semi-structured interviews with participants",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.05429v2",
    "title": "Supporting the Careers of Developers with Disabilities: Lessons from Zup Innovation",
    "year": 2023,
    "authors": [
      "Isadora Cardoso-Pereira",
      "Geraldo Gomes",
      "Danilo Monteiro Ribeiro",
      "Alberto de Souza",
      "Danilo Lucena",
      "Gustavo Pinto"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination faced by people with disabilities, a social inequality issue, and discusses inclusion and accessibility in employment.",
      "inequality_type": [
        "disability",
        "educational",
        "employment"
      ],
      "other_detail": "Focus on employment inclusion for disabled developers",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews"
      ],
      "methodology_detail": "Analyzed participant interviews for challenges and limitations",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.04838v1",
    "title": "The Casual Conversations v2 Dataset",
    "year": 2023,
    "authors": [
      "Bilal Porgali",
      "Vítor Albiero",
      "Jordan Ryda",
      "Cristian Canton Ferrer",
      "Caner Hazirbas"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The dataset focuses on demographic attributes, skin tone, and voice characteristics, which relate to race, ethnicity, and physical attributes, and aims to evaluate fairness and bias in AI models. It explicitly involves diverse populations across multiple countries, highlighting social disparities. The emphasis on fairness assessment indicates a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "disability",
        "geographic",
        "age"
      ],
      "other_detail": "Focus on fairness and bias in AI models",
      "affected_populations": [
        "diverse demographic groups",
        "global populations"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing bias and fairness in AI models",
      "geographic_focus": [
        "Brazil",
        "India",
        "Indonesia",
        "Mexico",
        "Vietnam",
        "Philippines",
        "USA"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.03975v1",
    "title": "GATE: A Challenge Set for Gender-Ambiguous Translation Examples",
    "year": 2023,
    "authors": [
      "Spencer Rarrick",
      "Ranjita Naik",
      "Varun Mathur",
      "Sundar Poudel",
      "Vishal Chowdhary"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, highlighting stereotypical gender roles and bias perpetuation, which are social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI translation systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Creating and evaluating a gender-ambiguous translation corpus",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.03242v1",
    "title": "Evaluating the Fairness of Deep Learning Uncertainty Estimates in Medical Image Analysis",
    "year": 2023,
    "authors": [
      "Raghav Mehta",
      "Changjian Shui",
      "Tal Arbel"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in medical AI models across demographic groups, highlighting fairness issues related to race, sex, and age, which are social inequality factors.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias in medical AI models across social groups",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "elderly",
        "patients"
      ],
      "methodology": [
        "Experiment",
        "Machine Learning",
        "Deep Learning"
      ],
      "methodology_detail": "Fairness and uncertainty evaluation in medical AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.02580v2",
    "title": "Estimating Racial Disparities When Race is Not Observed",
    "year": 2023,
    "authors": [
      "Cory McCartan",
      "Robin Fisher",
      "Jacob Goldin",
      "Daniel E. Ho",
      "Kosuke Imai"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on estimating racial disparities without observed race data, addressing racial inequality. It develops methods to measure and correct biases in racial disparity estimates, directly engaging with social discrimination issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial disparities in policy and data analysis",
      "affected_populations": [
        "racial minorities",
        "homeowners",
        "voters"
      ],
      "methodology": [
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Bayesian models using surnames and location data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.01692v2",
    "title": "Travel Demand Forecasting: A Fair AI Approach",
    "year": 2023,
    "authors": [
      "Xiaojian Zhang",
      "Qian Ke",
      "Xilei Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI models related to protected attributes like race and income, highlighting social bias and inequality concerns in transportation decision-making.",
      "inequality_type": [
        "racial",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI models for multiple social attributes",
      "affected_populations": [
        "racial minorities",
        "low-income groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Regularization",
        "Case Study"
      ],
      "methodology_detail": "Introducing fairness regularization into travel demand models",
      "geographic_focus": [
        "Chicago, IL",
        "Austin, TX"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2303.01316v1",
    "title": "Interactive robots as inclusive tools to increase diversity in higher education",
    "year": 2023,
    "authors": [
      "Patrick Holthaus"
    ],
    "categories": [
      "cs.RO"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses underrepresentation of marginalized groups in STEM education, aiming to reduce inequalities related to gender, ethnicity, and access. It focuses on increasing diversity and inclusivity in higher education through interactive robots.",
      "inequality_type": [
        "gender",
        "ethnic",
        "educational"
      ],
      "other_detail": "Focus on diversity in STEM education",
      "affected_populations": [
        "underrepresented students",
        "minority groups"
      ],
      "methodology": [
        "Survey",
        "Case Study",
        "System Design"
      ],
      "methodology_detail": "Analyzing outreach events and proposing inclusive activities",
      "geographic_focus": [
        "United Kingdom"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.14027v1",
    "title": "Diversity matters: Robustness of bias measurements in Wikidata",
    "year": 2023,
    "authors": [
      "Paramita Das",
      "Sai Keerthana Karnam",
      "Anirban Panda",
      "Bhanu Prakash Reddy Guda",
      "Soumya Sarkar",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and demographics in knowledge graphs, highlighting societal biases and cultural differences, which are core aspects of social inequality.",
      "inequality_type": [
        "gender",
        "demographic",
        "cultural"
      ],
      "other_detail": "Bias measurement sensitivity across social groups",
      "affected_populations": [
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement and embedding algorithm comparison",
      "geographic_focus": [
        "global"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.13846v2",
    "title": "Domain Adaptive Decision Trees: Implications for Accuracy and Fairness",
    "year": 2023,
    "authors": [
      "Jose M. Alvarez",
      "Kristen M. Scott",
      "Salvatore Ruggieri",
      "Bettina Berendt"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and demographic impacts of decision models, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on fairness in AI decision-making",
      "affected_populations": [
        "demographic groups",
        "under-served groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Experiment"
      ],
      "methodology_detail": "Improving decision tree fairness and accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.14063v1",
    "title": "How optimal transport can tackle gender biases in multi-class neural-network classifiers for job recommendations?",
    "year": 2023,
    "authors": [
      "Fanny Jourdan",
      "Titon Tshiongo Kaninku",
      "Nicholas Asher",
      "Jean-Michel Loubes",
      "Laurent Risser"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender biases in AI recommendation systems, focusing on reducing algorithmic discrimination against women in job classification tasks.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female individuals",
        "male individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Optimal Transport",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation in neural networks using optimal transport",
      "geographic_focus": [
        "European Union"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.13136v1",
    "title": "Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling",
    "year": 2023,
    "authors": [
      "Rui Wang",
      "Pengyu Cheng",
      "Ricardo Henao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias and fairness issues in AI-generated text, aiming to reduce social bias against demographic groups, which relates to social inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on social bias mitigation in language models",
      "affected_populations": [
        "disadvantaged groups",
        "demographic minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Mutual Information Minimization",
        "Importance Sampling",
        "Model Distillation"
      ],
      "methodology_detail": "Techniques for debiasing language models and fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.13049v1",
    "title": "CASIA-Iris-Africa: A Large-scale African Iris Image Database",
    "year": 2023,
    "authors": [
      "Jawad Muhammad",
      "Yunlong Wang",
      "Junxing Hu",
      "Kunbo Zhang",
      "Zhenan Sun"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in iris recognition algorithms and highlights the lack of African representation in biometric databases, which impacts fairness and equity in biometric technology.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial bias in biometric systems",
      "affected_populations": [
        "Africans"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Performance Evaluation"
      ],
      "methodology_detail": "Evaluates algorithm performance on diverse racial data",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.12683v1",
    "title": "Intersectional Fairness: A Fractal Approach",
    "year": 2023,
    "authors": [
      "Giulio Filippi",
      "Sara Zannone",
      "Adriano Koshiyama"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on protected attributes like ethnicity and gender, which are central to social inequalities. It discusses intersectional fairness, a key concept in social discrimination analysis. The emphasis on bias and fairness relates directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on intersectional fairness in AI systems",
      "affected_populations": [
        "ethnic groups",
        "gender groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Mathematical Modeling"
      ],
      "methodology_detail": "Geometrical and fractal analysis of fairness levels",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.12326v1",
    "title": "Can Voice Assistants Be Microaggressors? Cross-Race Psychological Responses to Failures of Automatic Speech Recognition",
    "year": 2023,
    "authors": [
      "Kimi Wenzel",
      "Nitya Devireddy",
      "Cam Davidson",
      "Geoff Kaufman"
    ],
    "categories": [
      "cs.HC",
      "J.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines racial bias in speech recognition and its psychological impact on Black and white users, addressing racial disparities and social discrimination in AI systems.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in technology interactions",
      "affected_populations": [
        "Black users",
        "White users"
      ],
      "methodology": [
        "Experiment"
      ],
      "methodology_detail": "Controlled interaction with voice assistant",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.11944v3",
    "title": "Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference",
    "year": 2023,
    "authors": [
      "Jose M. Alvarez",
      "Salvatore Ruggieri"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination detection in classifiers, focusing on protected attributes like race or gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "disability"
      ],
      "other_detail": "Focuses on individual discrimination detection methods",
      "affected_populations": [
        "protected groups",
        "individuals subjected to discrimination"
      ],
      "methodology": [
        "Causal Data Mining",
        "Counterfactual Reasoning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses causal knowledge to generate counterfactuals",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.11562v1",
    "title": "Uncovering Bias in Face Generation Models",
    "year": 2023,
    "authors": [
      "Cristian Muñoz",
      "Sara Zannone",
      "Umar Mohammed",
      "Adriano Koshiyama"
    ],
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in face generation models related to social groups such as race and gender, highlighting issues of social bias and fairness. It analyzes how these biases affect different social groups, which directly relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI-generated face images",
      "affected_populations": [
        "minorities",
        "women",
        "racial groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias measurement and fairness evaluation in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.11479v1",
    "title": "Drop Edges and Adapt: a Fairness Enforcing Fine-tuning for Graph Neural Networks",
    "year": 2023,
    "authors": [
      "Indro Spinelli",
      "Riccardo Bianchini",
      "Simone Scardapane"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in link prediction, highlighting social impacts and demographic disparities in social networks.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on social network fairness and demographic bias",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness constraints and adjacency matrix fine-tuning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.10856v1",
    "title": "Overview of the TREC 2021 Fair Ranking Track",
    "year": 2023,
    "authors": [
      "Michael D. Ekstrand",
      "Graham McDonald",
      "Amifa Raj",
      "Isaac Johnson"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair ranking to address under-representation of protected characteristics in Wikipedia, aiming to reduce systemic biases affecting disadvantaged groups.",
      "inequality_type": [
        "racial",
        "educational",
        "informational"
      ],
      "other_detail": "Addresses bias in information access and representation",
      "affected_populations": [
        "disadvantaged groups",
        "protected societal groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Evaluates fairness in ranking algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.10575v1",
    "title": "Managing multi-facet bias in collaborative filtering recommender systems",
    "year": 2023,
    "authors": [
      "Samira Vaez Barenji",
      "Saeed Farzi"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to geographical origin and popularity, which are social attributes affecting fairness. It discusses bias mitigation in recommender systems, impacting groups based on location and item popularity. These aspects relate to social inequality and bias in AI.",
      "inequality_type": [
        "geographic",
        "informational"
      ],
      "other_detail": "Bias intersectionality in recommender systems",
      "affected_populations": [
        "item providers",
        "users"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias mitigation and performance evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.09991v1",
    "title": "Towards Measuring and Scoring Speaker Diarization Fairness",
    "year": 2023,
    "authors": [
      "Yannis Tevissen",
      "Jérôme Boudy",
      "Gérard Chollet",
      "Frédéric Petitpont"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in speaker diarization, focusing on biases related to gender, age, and accent, which are social identity factors. It addresses social bias and fairness issues in AI systems affecting different social groups.",
      "inequality_type": [
        "gender",
        "age",
        "linguistic"
      ],
      "other_detail": "Biases in speech processing systems",
      "affected_populations": [
        "women",
        "older adults",
        "non-native speakers"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates bias across demographic groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.09157v1",
    "title": "Designing Equitable Algorithms",
    "year": 2023,
    "authors": [
      "Alex Chohlas-Wood",
      "Madison Coots",
      "Sharad Goel",
      "Julian Nyarko"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithm fairness impacting marginalized social groups, highlighting disparities along racial, gender, and health lines.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on algorithmic fairness and social disparities",
      "affected_populations": [
        "marginalized groups",
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analyzes fairness constraints and real-world examples",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.08973v6",
    "title": "Measuring Equality in Machine Learning Security Defenses: A Case Study in Speech Recognition",
    "year": 2023,
    "authors": [
      "Luke E. Richards",
      "Edward Raff",
      "Cynthia Matuszek"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities across social subgroups in AI defenses, focusing on gender, accent, and age, which are social identity factors.",
      "inequality_type": [
        "gender",
        "age",
        "ethnic"
      ],
      "other_detail": "Fairness in adversarial robustness for social groups",
      "affected_populations": [
        "gender groups",
        "age groups",
        "social subgroups"
      ],
      "methodology": [
        "Experiment",
        "Case Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Empirical evaluation of defense fairness across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.08704v1",
    "title": "The Unbearable Weight of Massive Privilege: Revisiting Bias-Variance Trade-Offs in the Context of Fair Prediction",
    "year": 2023,
    "authors": [
      "Falaah Arif Khan",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in social data, bias, and demographic attributes.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic fairness in social datasets",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social classes"
      ],
      "methodology": [
        "Machine Learning",
        "Theoretical Analysis",
        "Empirical Testing"
      ],
      "methodology_detail": "Bias-variance analysis and group-specific classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.08204v2",
    "title": "Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting",
    "year": 2023,
    "authors": [
      "Giandomenico Cornacchia",
      "Vito Walter Anelli",
      "Fedelucio Narducci",
      "Azzurra Ragone",
      "Eugenio Di Sciascio"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and discrimination in AI, which relate to social inequalities like race and fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focuses on bias detection in algorithmic decision-making",
      "affected_populations": [
        "discriminated users",
        "protected groups"
      ],
      "methodology": [
        "Counterfactual Reasoning",
        "Algorithm Auditing",
        "Experimental Analysis"
      ],
      "methodology_detail": "Using counterfactuals to detect hidden bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.07372v2",
    "title": "Same Same, But Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection",
    "year": 2023,
    "authors": [
      "Soumyajit Gupta",
      "Sooyong Lee",
      "Maria De-Arteaga",
      "Matthew Lease"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in toxicity detection across demographic groups, highlighting disparities related to social groups such as race or gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on subgroup validity in AI models",
      "affected_populations": [
        "demographic groups",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Multi-Task Learning",
        "Experiment"
      ],
      "methodology_detail": "Conditional multi-task learning for subgroup-specific models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.07185v2",
    "title": "When mitigating bias is unfair: multiplicity and arbitrariness in algorithmic group fairness",
    "year": 2023,
    "authors": [
      "Natasa Krco",
      "Thibault Laugel",
      "Vincent Grari",
      "Jean-Michel Loubes",
      "Marcin Detyniecki"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias mitigation in AI, addressing social discrimination and unequal impacts across groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making processes",
      "affected_populations": [
        "social groups",
        "individuals impacted by bias"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluates bias mitigation impacts across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.07159v1",
    "title": "A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?",
    "year": 2023,
    "authors": [
      "Kathleen C. Fraser",
      "Svetlana Kiritchenko",
      "Isar Nejadgholi"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases and stereotypes in AI-generated images, relating to social attributes like threat and friendliness, which are linked to social stereotypes and biases.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on stereotypes and demographic biases in images",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes generated images and stereotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.06752v1",
    "title": "Provable Detection of Propagating Sampling Bias in Prediction Models",
    "year": 2023,
    "authors": [
      "Pavan Ravishankar",
      "Qingyu Mo",
      "Edward McFowland III",
      "Daniel B. Neill"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes bias propagation in prediction models using criminal justice datasets, which are linked to social inequalities such as racial and socioeconomic disparities.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on criminal justice bias and fairness",
      "affected_populations": [
        "racial minorities",
        "low-income groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Quantitative detection of bias propagation",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.10908v1",
    "title": "Human-Centric Multimodal Machine Learning: Recent Advances and Testbed on AI-based Recruitment",
    "year": 2023,
    "authors": [
      "Alejandro Peña",
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez",
      "Alfonso Ortega",
      "Ainhoa Herrarte",
      "Manuel Alcantara",
      "Javier Ortega-Garcia"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases related to gender and race in AI recruitment, highlighting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Biases in automated decision-making processes",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Synthetic profiles with biased scoring",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.06321v2",
    "title": "Parameter-efficient Modularised Bias Mitigation via AdapterFusion",
    "year": 2023,
    "authors": [
      "Deepak Kumar",
      "Oleg Lesota",
      "George Zerveas",
      "Daniel Cohen",
      "Carsten Eickhoff",
      "Markus Schedl",
      "Navid Rekabsaz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses societal biases related to gender, race, and age in AI models, focusing on fairness and bias mitigation techniques.",
      "inequality_type": [
        "gender",
        "race",
        "age"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.05995v1",
    "title": "Multi-dimensional discrimination in Law and Machine Learning -- A comparative overview",
    "year": 2023,
    "authors": [
      "Arjun Roy",
      "Jan Horstmann",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination based on protected characteristics like race and gender, and addresses fairness issues in AI systems, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "social discrimination"
      ],
      "other_detail": "Focus on multi-dimensional discrimination in legal and AI contexts",
      "affected_populations": [
        "social groups",
        "individuals with protected attributes"
      ],
      "methodology": [
        "Literature Review",
        "Legal Analysis",
        "Comparative Study"
      ],
      "methodology_detail": "Comparing legal and machine learning fairness definitions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of discrimination",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.05558v1",
    "title": "Overview of the TREC 2022 Fair Ranking Track",
    "year": 2023,
    "authors": [
      "Michael D. Ekstrand",
      "Graham McDonald",
      "Amifa Raj",
      "Isaac Johnson"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair exposure of protected characteristics in Wikipedia, addressing biases affecting disadvantaged social groups.",
      "inequality_type": [
        "racial",
        "educational",
        "informational"
      ],
      "other_detail": "Fair ranking to mitigate systemic biases in knowledge representation",
      "affected_populations": [
        "disadvantaged groups",
        "protected societal groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Evaluating fairness in ranking algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.04358v1",
    "title": "Mitigating Bias in Visual Transformers via Targeted Alignment",
    "year": 2023,
    "authors": [
      "Sruthi Sudhakar",
      "Viraj Prabhu",
      "Arvindkumar Krishnakumar",
      "Judy Hoffman"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness and bias in AI systems, specifically addressing social bias related to gender and attribute prediction, which are linked to social inequalities.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in computer vision models",
      "affected_populations": [
        "gender groups",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Evaluation",
        "Algorithm Debiasing"
      ],
      "methodology_detail": "Bias mitigation in transformer models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.04119v1",
    "title": "Local Law 144: A Critical Analysis of Regression Metrics",
    "year": 2023,
    "authors": [
      "Giulio Filippi",
      "Sara Zannone",
      "Airlie Hilliard",
      "Adriano Koshiyama"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias metrics in automated decision tools used in recruitment, focusing on racial and gender disparities. It critically analyzes how these metrics detect or fail to detect bias across social groups. The discussion directly relates to social discrimination and fairness in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias detection in employment decisions",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Comparison of bias metrics across examples and real data",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.03782v2",
    "title": "Does AI-Assisted Fact-Checking Disproportionately Benefit Majority Groups Online?",
    "year": 2023,
    "authors": [
      "Terrence Neumann",
      "Nicholas Wolczynski"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how benefits from AI-assisted fact-checking are distributed across different online communities, highlighting disparities between majority and minority groups, which relates to social inequality.",
      "inequality_type": [
        "informational",
        "digital",
        "social"
      ],
      "other_detail": "Focuses on online community disparities in misinformation exposure",
      "affected_populations": [
        "majority communities",
        "minority communities"
      ],
      "methodology": [
        "Simulation",
        "Algorithm Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses TACIT simulator and algorithmic interventions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.10893v3",
    "title": "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness",
    "year": 2023,
    "authors": [
      "Felix Friedrich",
      "Manuel Brack",
      "Lukas Struppek",
      "Dominik Hintersdorf",
      "Patrick Schramowski",
      "Sasha Luccioni",
      "Kristian Kersting"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in AI models related to human social groups and discusses fairness, which are directly linked to social inequalities such as gender and racial biases.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Bias control in generative models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.02463v3",
    "title": "Nationality Bias in Text Generation",
    "year": 2023,
    "authors": [
      "Pranav Narayanan Venkit",
      "Sanjana Gautam",
      "Ruchi Panchanadikar",
      "Ting-Hao 'Kenneth' Huang",
      "Shomir Wilson"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal biases related to nationality in language models, highlighting disparities based on internet usage and economic status, which relate to social inequality issues.",
      "inequality_type": [
        "economic",
        "digital",
        "nationality"
      ],
      "other_detail": "Focuses on bias amplification in language models",
      "affected_populations": [
        "countries with lower internet users",
        "societies with biased perceptions"
      ],
      "methodology": [
        "Natural Language Processing",
        "Sensitivity Analysis",
        "Adversarial Triggering"
      ],
      "methodology_detail": "Analyzes bias reduction techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.02404v3",
    "title": "The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default",
    "year": 2023,
    "authors": [
      "Brent Mittelstadt",
      "Sandra Wachter",
      "Chris Russell"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in machine learning, addressing social discrimination and bias, and examines impacts on social groups, linking AI fairness to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on fairness measures and substantive equality",
      "affected_populations": [
        "demographic groups",
        "social groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Legal Analysis"
      ],
      "methodology_detail": "Philosophical, legal, and fairness framework analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social inequality",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.02306v1",
    "title": "Fair Spatial Indexing: A paradigm for Group Spatial Fairness",
    "year": 2023,
    "authors": [
      "Sina Shaham",
      "Gabriel Ghinita",
      "Cyrus Shahabi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses spatial fairness in ML, focusing on geographic bias, which impacts social equity. It discusses bias mitigation related to location data, a social determinant affecting access and opportunities.",
      "inequality_type": [
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on spatial group fairness in ML",
      "affected_populations": [
        "underprivileged groups",
        "geographic minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Experiment"
      ],
      "methodology_detail": "Spatial indexing algorithm for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.01546v2",
    "title": "Group Fairness in Non-monotone Submodular Maximization",
    "year": 2023,
    "authors": [
      "Jing Yuan",
      "Shaojie Tang"
    ],
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints related to sensitive attributes like race and gender in algorithmic data selection, aiming to mitigate bias and representation issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness constraints in algorithmic data selection",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Develops approximation algorithms for fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.01448v1",
    "title": "Out of Context: Investigating the Bias and Fairness Concerns of \"Artificial Intelligence as a Service\"",
    "year": 2023,
    "authors": [
      "Kornel Lewicki",
      "Michelle Seng Ah Lee",
      "Jennifer Cobbe",
      "Jatinder Singh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases and fairness issues in AI systems, which relate to social discrimination and inequality. It examines how AIaaS can perpetuate societal biases affecting different social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on fairness and bias in AI services",
      "affected_populations": [
        "social groups",
        "end-users"
      ],
      "methodology": [
        "Literature Review",
        "Critical Analysis"
      ],
      "methodology_detail": "Systematic review and critical examination of AIaaS",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.01385v2",
    "title": "Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access",
    "year": 2023,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Ivan Brugere",
      "Sanghamitra Dutta",
      "Alan Mishler",
      "Siddharth Garg"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning related to sensitive attributes like race and gender, which are directly linked to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness without sensitive attribute access",
      "affected_populations": [
        "minority groups",
        "demographic subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Hyper-parameter tuning without sensitive attribute labels",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.00589v2",
    "title": "Diversity dilemmas: uncovering gender and nationality biases in graduate admissions across top North American computer science programs",
    "year": 2023,
    "authors": [
      "Ghazal Kalhor",
      "Tanin Zeraati",
      "Behnam Bahrak"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender and nationality biases in university admissions, addressing social discrimination and inequality related to gender and nationality.",
      "inequality_type": [
        "gender",
        "nationality"
      ],
      "other_detail": "Bias in academic admissions processes",
      "affected_populations": [
        "female students",
        "international students"
      ],
      "methodology": [
        "Statistical Analysis"
      ],
      "methodology_detail": "Hypothesis testing on admission data",
      "geographic_focus": [
        "North America"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.00419v1",
    "title": "For the Underrepresented in Gender Bias Research: Chinese Name Gender Prediction with Heterogeneous Graph Attention Network",
    "year": 2023,
    "authors": [
      "Zihao Pan",
      "Kai Peng",
      "Shuai Ling",
      "Haipeng Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in Chinese names, addressing gender inequality and bias in AI tools, which impacts underrepresented groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in Chinese name prediction",
      "affected_populations": [
        "female Chinese individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Graph Attention Network",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops a novel heterogenous graph model and dataset",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2302.00025v2",
    "title": "On the Within-Group Fairness of Screening Classifiers",
    "year": 2023,
    "authors": [
      "Nastaran Okati",
      "Stratis Tsirtsis",
      "Manuel Gomez Rodriguez"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues within demographic groups, which relate to social inequalities such as race, gender, and socioeconomic status, in the context of AI screening classifiers.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on within-group fairness in classification",
      "affected_populations": [
        "demographic groups",
        "qualified candidates"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Post-processing algorithm for classifier adjustment",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.13803v3",
    "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
    "year": 2023,
    "authors": [
      "Yao Qiang",
      "Chengyin Li",
      "Prashant Khanduri",
      "Dongxiao Zhu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias mitigation related to sensitive features, which pertains to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI systems addressing social biases",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fairness mitigation in vision transformer models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.12985v1",
    "title": "Integrating Earth Observation Data into Causal Inference: Challenges and Opportunities",
    "year": 2023,
    "authors": [
      "Connor T. Jerzak",
      "Fredrik Johansson",
      "Adel Daoud"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "62D20",
      "J.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on anti-poverty programs in Africa, addressing socioeconomic factors and inequality. It uses satellite imagery to infer confounders related to poverty, which are linked to social inequality. The study aims to improve causal inference in resource-limited settings, indirectly addressing inequality issues.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Using satellite data to address poverty-related confounding",
      "affected_populations": [
        "African communities",
        "poverty-affected groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Computer Vision"
      ],
      "methodology_detail": "Satellite image analysis for causal inference",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.12855v1",
    "title": "How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification",
    "year": 2023,
    "authors": [
      "Ewoenam Tokpo",
      "Pieter Delobelle",
      "Bettina Berendt",
      "Toon Calders"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social discrimination. It examines how AI systems perpetuate or mitigate gender bias, impacting fairness in downstream tasks.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Probing bias mitigation effects on text classification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.12364v1",
    "title": "Fair Decision-making Under Uncertainty",
    "year": 2023,
    "authors": [
      "Wenbin Zhang",
      "Jeremy C. Weiss"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI decision-making, focusing on social groups and discrimination under uncertainty, which relates to social inequality issues.",
      "inequality_type": [
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness constraints in algorithmic decisions",
      "affected_populations": [
        "social groups",
        "individuals"
      ],
      "methodology": [
        "Fairness Framework",
        "Empirical Evaluation",
        "Algorithm Design"
      ],
      "methodology_detail": "Fairness under censorship and uncertainty",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.12278v1",
    "title": "Pragmatic Fairness: Developing Policies with Outcome Disparity Control",
    "year": 2023,
    "authors": [
      "Limor Gultchin",
      "Siyuan Guo",
      "Alan Malek",
      "Silvia Chiappa",
      "Ricardo Silva"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints in policy design, aiming to reduce outcome disparities across sensitive groups, which relates to social inequalities such as racial or socioeconomic disparities.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on outcome disparity control in policies",
      "affected_populations": [
        "social groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Causal Framework",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Uses semi-synthetic models for illustration",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.12074v1",
    "title": "Comparing Intrinsic Gender Bias Evaluation Measures without using Human Annotated Examples",
    "year": 2023,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala",
      "Naoaki Okazaki"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases in language models, specifically gender bias, which relates to social inequality. It aims to evaluate and compare bias measurement methods without human annotation, addressing fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias evaluation measures",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias-controlled PLM creation and correlation analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.11994v1",
    "title": "Gender and Prestige Bias in Coronavirus News Reporting",
    "year": 2023,
    "authors": [
      "Rebecca Dorn",
      "Yiwen Ma",
      "Fred Morstatter",
      "Kristina Lerman"
    ],
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and academic prestige biases in news reporting, highlighting disparities in representation that relate to social inequalities.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Bias in expert representation in media coverage",
      "affected_populations": [
        "women",
        "less prestigious institutions"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes news articles and expert data quantitatively",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.11100v1",
    "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities",
    "year": 2023,
    "authors": [
      "Melissa Hall",
      "Laura Gustafson",
      "Aaron Adcock",
      "Ishan Misra",
      "Candace Ross"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in vision-language models, highlighting social disparities based on gender perceptions and biases, which relate directly to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias propagation in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates model performance across datasets and concepts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.10642v3",
    "title": "Group fairness in dynamic refugee assignment",
    "year": 2023,
    "authors": [
      "Daniel Freund",
      "Thodoris Lykouris",
      "Elisabeth Paulson",
      "Bradley Sturt",
      "Wentao Weng"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in refugee assignment, focusing on equitable treatment across social groups, which relates to social inequality and discrimination issues.",
      "inequality_type": [
        "geographic",
        "educational",
        "age",
        "country of origin"
      ],
      "other_detail": "Fairness in refugee allocation",
      "affected_populations": [
        "refugees",
        "asylum seekers",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designing algorithms with fairness guarantees",
      "geographic_focus": [
        "U.S.",
        "Netherlands"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.10075v3",
    "title": "Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges",
    "year": 2023,
    "authors": [
      "Andrea Piergentili",
      "Dennis Fucci",
      "Beatrice Savoldi",
      "Luisa Bentivogli",
      "Matteo Negri"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias and inclusivity in language technologies, focusing on gender-related issues and discrimination. It discusses technical challenges in mitigating gender bias in machine translation, which relates to social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focused on gender bias in language technology",
      "affected_populations": [
        "gender minorities",
        "language users"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis",
        "System Design"
      ],
      "methodology_detail": "review of guidelines and technical challenges",
      "geographic_focus": [
        "Italy"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.09902v3",
    "title": "Investigating Labeler Bias in Face Annotation for Machine Learning",
    "year": 2023,
    "authors": [
      "Luke Haliburton",
      "Sinksar Ghebremedhin",
      "Robin Welsch",
      "Albrecht Schmidt",
      "Sven Mayer"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how labeler bias affects datasets and AI fairness, highlighting racial and gender stereotypes impacting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on bias in AI training data",
      "affected_populations": [
        "ethnic minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Labeling task with diverse ethnicities and sexes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.09003v1",
    "title": "Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models",
    "year": 2023,
    "authors": [
      "Anoop Kadan",
      "Deepak P.",
      "Sahely Bhadra",
      "Manjary P. Gangan",
      "Lajish V. L"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates affective biases related to gender, race, and religion in AI models, highlighting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "religion"
      ],
      "other_detail": "Affective bias in language models",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias detection and evaluation in large corpora and models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.08412v1",
    "title": "Fair Credit Scorer through Bayesian Approach",
    "year": 2023,
    "authors": [
      "Zhuo Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in credit scoring, focusing on mitigating bias related to protected attributes like sex and age, which are linked to social inequalities.",
      "inequality_type": [
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Bias removal in credit scoring models",
      "affected_populations": [
        "women",
        "elderly",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Bayesian Approach",
        "Markov Chain Monte Carlo"
      ],
      "methodology_detail": "Using Bayesian methods to ensure fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.08375v1",
    "title": "Within-group fairness: A guidance for more sound between-group fairness",
    "year": 2023,
    "authors": [
      "Sara Kim",
      "Kyusang Yu",
      "Yongdai Kim"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI affecting sensitive social groups, highlighting potential unfair treatment within groups, which relates to social discrimination and bias.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on fairness within and between social groups",
      "affected_populations": [
        "sensitive groups",
        "individuals in groups"
      ],
      "methodology": [
        "Mathematical Definitions",
        "Learning Algorithms",
        "Numerical Studies"
      ],
      "methodology_detail": "Develops fairness definitions and algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social fairness issues",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.05798v2",
    "title": "Regulating For-Hire Autonomous Vehicles for An Equitable Multimodal Transportation Network",
    "year": 2023,
    "authors": [
      "Jing Gao",
      "Sen Li"
    ],
    "categories": [
      "math.OC",
      "cs.GT",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines spatial and social equity impacts of autonomous vehicles, focusing on how benefits are distributed among different income groups and locations, highlighting issues of social and spatial inequality.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on spatial and social equity in transportation",
      "affected_populations": [
        "low-income groups",
        "urban residents"
      ],
      "methodology": [
        "Game Theory",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Nash equilibrium and equity evaluation metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.05775v1",
    "title": "MLOps: A Primer for Policymakers on a New Frontier in Machine Learning",
    "year": 2023,
    "authors": [
      "Jazmia Henry"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses bias mitigation in models affecting marginalized groups and references works on algorithmic impacts on social groups, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on bias in deployed machine learning models",
      "affected_populations": [
        "marginalized groups",
        "minority communities"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Detection",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using tools during MLOps lifecycle to identify bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.05012v1",
    "title": "Fairly Private: Investigating The Fairness of Visual Privacy Preservation Algorithms",
    "year": 2023,
    "authors": [
      "Sophie Noiret",
      "Siddharth Ravi",
      "Martin Kampel",
      "Francisco Florez-Revuelta"
    ],
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in privacy preservation algorithms, revealing unequal protection across groups, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "disability"
      ],
      "other_detail": "Focuses on fairness disparities in AI privacy algorithms",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "disabled individuals"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Facial recognition performance on obfuscated images",
      "geographic_focus": [
        "PubFig dataset (likely global or unspecified)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.04780v2",
    "title": "Much Ado About Gender: Current Practices and Future Recommendations for Appropriate Gender-Aware Information Access",
    "year": 2023,
    "authors": [
      "Christine Pinney",
      "Amifa Raj",
      "Alex Hanna",
      "Michael D. Ekstrand"
    ],
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines gender representations in information access systems, addressing gender bias and fairness, which are social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender in AI and information retrieval",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Review of existing research and ethical considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.03784v2",
    "title": "Inside the Black Box: Detecting and Mitigating Algorithmic Bias across Racialized Groups in College Student-Success Prediction",
    "year": 2023,
    "authors": [
      "Denisa Gándara",
      "Hadis Anahideh",
      "Matthew P. Ison",
      "Lorenzo Picchiarini"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in algorithmic predictions for student success, highlighting bias and inequality issues related to race. It discusses how algorithms may perpetuate societal injustices, indicating a focus on social discrimination and bias.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focus on racial bias in educational algorithms",
      "affected_populations": [
        "racialized students"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses predictive modeling and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.03489v1",
    "title": "Unveiling and Mitigating Bias in Ride-Hailing Pricing for Equitable Policy Making",
    "year": 2023,
    "authors": [
      "Nripsuta Ani Saxena",
      "Wenbin Zhang",
      "Cyrus Shahabi"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in ride-hailing pricing affecting disadvantaged communities, highlighting socioeconomic and racial impacts.",
      "inequality_type": [
        "economic",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on disadvantaged neighborhoods and affordability",
      "affected_populations": [
        "low-income residents",
        "minority communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Real-world data validation of fairness measures",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.02989v1",
    "title": "Fair Multi-Exit Framework for Facial Attribute Classification",
    "year": 2023,
    "authors": [
      "Ching-Hao Chiu",
      "Hao-Wei Chung",
      "Yu-Jen Chen",
      "Yiyu Shi",
      "Tsung-Yi Ho"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in facial recognition, addressing bias mitigation affecting underprivileged groups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI systems for social equity",
      "affected_populations": [
        "underprivileged groups",
        "minority populations"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-oriented multi-exit neural network framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.01590v2",
    "title": "FATE in AI: Towards Algorithmic Inclusivity and Accessibility",
    "year": 2023,
    "authors": [
      "Isa Inuwa-Dutse"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social disparities related to AI bias, inclusivity, and fairness, focusing on marginalized communities in the global South.",
      "inequality_type": [
        "social",
        "geographic",
        "cultural"
      ],
      "other_detail": "Focus on underserved communities and cultural pluralism",
      "affected_populations": [
        "local communities",
        "underserved groups"
      ],
      "methodology": [
        "User Study",
        "Participatory Session"
      ],
      "methodology_detail": "Qualitative and participatory approaches",
      "geographic_focus": [
        "Global South"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.00792v1",
    "title": "The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings",
    "year": 2023,
    "authors": [
      "Francisco Valentini",
      "Germán Rosati",
      "Diego Fernandez Slezak",
      "Edgar Altszyler"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in word embeddings, a social bias issue. It analyzes how AI metrics reflect gender stereotypes, impacting societal perceptions. This directly relates to social inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias measurement issues",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes bias metrics and frequency effects in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.00440v1",
    "title": "Local Inequities in the Relative Production of and Exposure to Vehicular Air Pollution in Los Angeles",
    "year": 2023,
    "authors": [
      "Geoff Boeing",
      "Yougeng Lu",
      "Clemens Pilgram"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "econ.GN",
      "physics.app-ph",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and socioeconomic disparities in air pollution exposure and production, highlighting historical and structural inequalities. It analyzes how race, ethnicity, and socioeconomic status influence environmental burdens. The focus on racial injustice and geographic disparities confirms its relevance to social inequality.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on racial and economic disparities in pollution exposure",
      "affected_populations": [
        "racial minorities",
        "low-income communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analyzes spatial and demographic data in Los Angeles",
      "geographic_focus": [
        "Los Angeles"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 1.0
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.00395v1",
    "title": "CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation",
    "year": 2023,
    "authors": [
      "Ge Zhang",
      "Yizhi Li",
      "Yaoyao Wu",
      "Linyuan Zhang",
      "Chenghua Lin",
      "Jiayi Geng",
      "Shi Wang",
      "Jie Fu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in Chinese NLP, focusing on social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "Chinese women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Develops a Chinese corpus for bias probing and mitigation",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.00312v2",
    "title": "Collision of Environmental Injustice and Sea Level Rise: Assessment of Risk Inequality in Flood-induced Pollutant Dispersion from Toxic Sites in Texas",
    "year": 2023,
    "authors": [
      "Zhewei Liu",
      "Ali Mostafavi"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines disproportionate exposure of vulnerable communities to flood-induced pollution, highlighting environmental injustice related to socioeconomic and racial disparities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "environmental"
      ],
      "other_detail": "Focus on vulnerable populations and environmental risks",
      "affected_populations": [
        "low income",
        "minorities",
        "unemployed"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing risk disparities under flood scenarios",
      "geographic_focus": [
        "Texas"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  }
]