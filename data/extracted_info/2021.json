[
  {
    "id": "http://arxiv.org/abs/2112.14801v1",
    "title": "Modeling Prejudice and Its Effect on Societal Prosperity",
    "year": 2021,
    "authors": [
      "Deep Inder Mohan",
      "Arjun Verma",
      "Shrisha Rao"
    ],
    "categories": [
      "cs.MA",
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models prejudice, a key factor in social discrimination and inequality, and examines its impact on societal prosperity, addressing racial and social biases.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on prejudice's societal and group impacts",
      "affected_populations": [
        "racial groups",
        "social factions"
      ],
      "methodology": [
        "Agent-based modeling",
        "Simulation",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Multi-agent framework with social interaction modeling",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.14168v1",
    "title": "A Survey on Gender Bias in Natural Language Processing",
    "year": 2021,
    "authors": [
      "Karolina Stanczak",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gender bias in NLP, addressing social stereotypes and biases related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in language processing",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Survey"
      ],
      "methodology_detail": "Analyzes 304 papers on gender bias in NLP",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.13214v1",
    "title": "NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification",
    "year": 2021,
    "authors": [
      "Haibin Zheng",
      "Zhiqing Chen",
      "Tianyu Du",
      "Xuhong Zhang",
      "Yao Cheng",
      "Shouling Ji",
      "Jingyi Wang",
      "Yue Yu",
      "Jinyin Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness testing of AI systems, addressing bias and discrimination issues relevant to social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias detection",
      "affected_populations": [
        "social groups",
        "users of AI systems"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection and fairness testing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.12521v1",
    "title": "Biases in human mobility data impact epidemic modeling",
    "year": 2021,
    "authors": [
      "Frank Schlosser",
      "Vedran Sekara",
      "Dirk Brockmann",
      "Manuel Garcia-Herranz"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "q-bio.PE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in mobility data caused by socioeconomic disparities, specifically wealth inequality, affecting data representation and analysis.",
      "inequality_type": [
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focuses on wealth-based mobility disparities",
      "affected_populations": [
        "low-wealth individuals",
        "high-wealth individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Debiasing Framework"
      ],
      "methodology_detail": "Analyzes mobility data biases and proposes correction techniques",
      "geographic_focus": [
        "multiple countries"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.12014v2",
    "title": "Quantifying Gender Biases Towards Politicians on Reddit",
    "year": 2021,
    "authors": [
      "Sara Marjanovic",
      "Karolina Stańczak",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in online political discourse, addressing social discrimination against women in politics.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on online gender bias towards politicians",
      "affected_populations": [
        "female politicians"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes linguistic and discourse biases in social media",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.11279v3",
    "title": "Differential Parity: Relative Fairness Between Two Sets of Decisions",
    "year": 2021,
    "authors": [
      "Zhe Yu",
      "Xiaoyin Xi"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI decisions, which impacts social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on fairness standards in decision-making processes",
      "affected_populations": [
        "decision subjects",
        "social groups"
      ],
      "methodology": [
        "Fairness Analysis",
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Estimating fairness across decision sets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.09786v3",
    "title": "Distill and De-bias: Mitigating Bias in Face Verification using Knowledge Distillation",
    "year": 2021,
    "authors": [
      "Prithviraj Dhar",
      "Joshua Gleason",
      "Aniket Roy",
      "Carlos D. Castillo",
      "P. Jonathon Phillips",
      "Rama Chellappa"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face recognition related to gender and skintone, which are social attributes linked to inequality. It discusses social bias and fairness issues in AI systems affecting different social groups.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in face recognition systems",
      "affected_populations": [
        "gender minorities",
        "racial groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Knowledge distillation for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.09047v1",
    "title": "Citation inequity and gendered citation practices in contemporary physics",
    "year": 2021,
    "authors": [
      "Erin G. Teich",
      "Jason Z. Kim",
      "Christopher W. Lynn",
      "Samantha C. Simon",
      "Andrei A. Klishin",
      "Karol P. Szymula",
      "Pragya Srivastava",
      "Lee C. Bassett",
      "Perry Zurn",
      "Jordan D. Dworkin",
      "Dani S. Bassett"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-based citation disparities in physics, highlighting social bias and under-attribution of women's contributions.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women scientists"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes citation data and models expected citation rates",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.08910v3",
    "title": "Degendering Resumes for Fair Algorithmic Resume Screening",
    "year": 2021,
    "authors": [
      "Prasanna Parasurama",
      "João Sedoc"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in resume screening, a social discrimination issue. It explores how algorithmic processes can perpetuate gendered information, impacting fairness. This relates directly to social inequality based on gender.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "job applicants",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Gender classification and obfuscation experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.11193v2",
    "title": "There is an elephant in the room: Towards a critique on the use of fairness in biometrics",
    "year": 2021,
    "authors": [
      "Ana Valdivia",
      "Júlia Corbera-Serrajòrdia",
      "Aneta Swianiewicz"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critiques biometric fairness and its implications for marginalized groups, especially asylum seekers, highlighting how these systems reproduce political and social injustices.",
      "inequality_type": [
        "racial",
        "ethnic",
        "disability",
        "immigration"
      ],
      "other_detail": "Focus on border control and asylum seekers",
      "affected_populations": [
        "asylum seekers",
        "citizens"
      ],
      "methodology": [
        "Literature Review",
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Mathematical proofs and empirical reproductions",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.08637v3",
    "title": "Analyzing the Limits of Self-Supervision in Handling Bias in Language",
    "year": 2021,
    "authors": [
      "Lisa Bauer",
      "Karthik Gopalakrishnan",
      "Spandana Gella",
      "Yang Liu",
      "Mohit Bansal",
      "Dilek Hakkani-Tur"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates how language models capture biases related to gender and political affiliation, addressing social bias and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "political"
      ],
      "other_detail": "Focuses on bias detection and mitigation in language models",
      "affected_populations": [
        "gender groups",
        "political groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Prompting and decoding analysis of bias tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.09738v1",
    "title": "Improving Ethical Outcomes with Machine-in-the-Loop: Broadening Human Understanding of Data Annotations",
    "year": 2021,
    "authors": [
      "Ashis Kumer Biswas",
      "Geeta Verma",
      "Justin Otto Barber"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in AI systems affecting minoritized students, highlighting social disparities in educational outcomes.",
      "inequality_type": [
        "educational",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias mitigation in educational AI applications",
      "affected_populations": [
        "minoritized students"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation in NLP classifier",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.08256v1",
    "title": "Est-ce que vous compute? Code-switching, cultural identity, and AI",
    "year": 2021,
    "authors": [
      "Arianna Falbo",
      "Travis LaCroix"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how AI can reinforce social inequalities and marginalization through cultural smothering, impacting marginalized groups.",
      "inequality_type": [
        "racial",
        "cultural",
        "social",
        "educational"
      ],
      "other_detail": "Focus on epistemic oppression and marginalization",
      "affected_populations": [
        "marginalized social groups"
      ],
      "methodology": [
        "Ethics Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes ethical and epistemic issues in AI",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.08237v1",
    "title": "Exposure Inequality in People Recommender Systems: The Long-Term Effects",
    "year": 2021,
    "authors": [
      "Francesco Fabbri",
      "Maria Luisa Croci",
      "Francesco Bonchi",
      "Carlos Castillo"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how recommendation algorithms influence exposure disparities among social groups, potentially reinforcing inequalities. It discusses effects related to minority groups and exposure dynamics, which relate to social inequality issues.",
      "inequality_type": [
        "racial",
        "social",
        "informational"
      ],
      "other_detail": "Focuses on social exposure disparities in social networks",
      "affected_populations": [
        "minority groups",
        "social network users"
      ],
      "methodology": [
        "Model Development",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulates long-term effects of recommendation algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.06522v1",
    "title": "Anatomizing Bias in Facial Analysis",
    "year": 2021,
    "authors": [
      "Richa Singh",
      "Puspita Majumdar",
      "Surbhi Mittal",
      "Mayank Vatsa"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in facial analysis systems, focusing on discrimination related to gender, identity, and skin tone, which are social categories linked to inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI systems affecting social groups",
      "affected_populations": [
        "women",
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Systematic review and overview of bias algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.05675v2",
    "title": "Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support",
    "year": 2021,
    "authors": [
      "Michael Madaio",
      "Lisa Egede",
      "Hariharan Subramonyam",
      "Jennifer Wortman Vaughan",
      "Hanna Wallach"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness evaluations in AI, addressing social bias and disparities among demographic groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and disparities in AI evaluations",
      "affected_populations": [
        "demographic groups",
        "marginalized groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews",
        "Workshops"
      ],
      "methodology_detail": "Practitioner interviews and workshops",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.05630v1",
    "title": "On Fair Selection in the Presence of Implicit and Differential Variance",
    "year": 2021,
    "authors": [
      "Vitalii Emelianov",
      "Nicolas Gast",
      "Krishna P. Gummadi",
      "Patrick Loiseau"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in selection processes, addressing discrimination related to group variances, which impacts social groups. It discusses bias and fairness mechanisms affecting different demographic groups. The focus on group disparities indicates a concern with social inequality.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in selection affecting social groups",
      "affected_populations": [
        "disadvantaged groups",
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes decision models and fairness mechanisms mathematically",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.05194v1",
    "title": "Word Embeddings via Causal Inference: Gender Bias Reducing and Semantic Information Preserving",
    "year": 2021,
    "authors": [
      "Lei Ding",
      "Dengdeng Yu",
      "Jinhan Xie",
      "Wenxing Guo",
      "Shenggang Hu",
      "Meichen Liu",
      "Linglong Kong",
      "Hongsheng Dai",
      "Yanchun Bao",
      "Bei Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in NLP models, a form of social discrimination. It focuses on reducing gender bias in word embeddings, which impacts societal perceptions and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Causal Inference",
        "Experiment"
      ],
      "methodology_detail": "Using causal inference to reduce gender bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.04169v2",
    "title": "Equity Promotion in Online Resource Allocation",
    "year": 2021,
    "authors": [
      "Pan Xu",
      "Yifan Xu"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on equitable resource distribution among demographic groups, addressing social disparities related to race, gender, and age, especially in health contexts.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focus on equitable health resource allocation",
      "affected_populations": [
        "demographic groups",
        "requesters"
      ],
      "methodology": [
        "Linear Programming",
        "Quantitative Analysis"
      ],
      "methodology_detail": "LP-based algorithms and competitive ratio analysis",
      "geographic_focus": [
        "Minnesota"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.03858v1",
    "title": "Reducing Target Group Bias in Hate Speech Detectors",
    "year": 2021,
    "authors": [
      "Darsh J Shah",
      "Sinong Wang",
      "Han Fang",
      "Hao Ma",
      "Luke Zettlemoyer"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in hate speech detection affecting protected social groups, highlighting disparities related to race and gender, which are key social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation in AI systems for social groups",
      "affected_populations": [
        "Black Women",
        "Immigrants"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Token-level hate sense disambiguation and model evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.03240v1",
    "title": "Analyzing a Carceral Algorithm used by the Pennsylvania Department of Corrections",
    "year": 2021,
    "authors": [
      "Vanessa Massaro",
      "Swarup Dhar",
      "Darakhshan Mir",
      "Nathan C. Ryan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias and inequalities embedded in a carceral algorithm, highlighting issues of racial discrimination and systemic bias in incarceration practices.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial bias in incarceration algorithms",
      "affected_populations": [
        "incarcerated people",
        "racial minorities"
      ],
      "methodology": [
        "Qualitative Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Historical context and data analysis of 146,793 prisoners",
      "geographic_focus": [
        "Pennsylvania"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.02530v1",
    "title": "Exploring and Mitigating Gender Bias in Recommender Systems with Explicit Feedback",
    "year": 2021,
    "authors": [
      "Shrikant Saxena",
      "Shweta Jain"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in recommender systems, highlighting social discrimination and stereotypes. It discusses how biased recommendations reinforce gender stereotypes, impacting social equality. The focus on gender bias aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development",
        "Experiment"
      ],
      "methodology_detail": "Bias quantification and mitigation in recommendation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.02365v2",
    "title": "TransBoost: A Boosting-Tree Kernel Transfer Learning Algorithm for Improving Financial Inclusion",
    "year": 2021,
    "authors": [
      "Yiheng Sun",
      "Tian Lu",
      "Cong Wang",
      "Yuan Li",
      "Huaiyu Fu",
      "Jingran Dong",
      "Yunjie Xu"
    ],
    "categories": [
      "cs.LG",
      "q-fin.ST"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on improving financial inclusion, which addresses economic and social disparities by enabling broader access to financial services for underserved populations.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic"
      ],
      "other_detail": "Enhances access for excluded or underserved groups",
      "affected_populations": [
        "unbanked",
        "low-income users"
      ],
      "methodology": [
        "Machine Learning",
        "Transfer Learning",
        "Algorithm Development"
      ],
      "methodology_detail": "Combines tree-based models and kernel methods for risk evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.02170v1",
    "title": "Counterfactual Fairness in Mortgage Lending via Matching and Randomization",
    "year": 2021,
    "authors": [
      "Sama Ghoba",
      "Nathan Colaner"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in mortgage lending, highlighting unfairness and inequality among racial groups. It discusses algorithmic fairness and social discrimination in financial services. The focus on racial and ethnic impacts confirms its relevance to social inequality.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial disparities in mortgage approval",
      "affected_populations": [
        "African-American",
        "non-Hispanic White"
      ],
      "methodology": [
        "Matching",
        "Causal Inference",
        "Fair Machine Learning"
      ],
      "methodology_detail": "Matching-based approach for counterfactual fairness",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.02034v1",
    "title": "Could AI Democratise Education? Socio-Technical Imaginaries of an EdTech Revolution",
    "year": 2021,
    "authors": [
      "Sahan Bulathwela",
      "María Pérez-Ortiz",
      "Catherine Holloway",
      "John Shawe-Taylor"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "stat.ML",
      "K.3.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses educational inequality, access disparities, and social barriers in AI-driven education.",
      "inequality_type": [
        "educational",
        "geographic",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Focus on equitable access and social disparities in education",
      "affected_populations": [
        "disadvantaged students",
        "global learners",
        " marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Synthesizes potential impacts and socio-technical considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.00859v1",
    "title": "Are Investors Biased Against Women? Analyzing How Gender Affects Startup Funding in Europe",
    "year": 2021,
    "authors": [
      "Michael Färber",
      "Alexander Klein"
    ],
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in startup funding, addressing gender inequality.",
      "inequality_type": [
        "gender",
        "economic"
      ],
      "other_detail": "Focuses on gender bias in financial access",
      "affected_populations": [
        "female founders"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes funding data and founder characteristics",
      "geographic_focus": [
        "Europe"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.00301v1",
    "title": "Uncertainty in Criminal Justice Algorithms: simulation studies of the Pennsylvania Additive Classification Tool",
    "year": 2021,
    "authors": [
      "Swarup Dhar",
      "Vanessa Massaro",
      "Darakhshan Mir",
      "Nathan C. Ryan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities in carceral algorithms affecting incarcerated individuals, focusing on race, sex, and age. It analyzes how algorithmic variability impacts different social groups, highlighting issues of social bias and inequality in the justice system.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in carceral algorithms",
      "affected_populations": [
        "incarcerated individuals",
        "racial minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Simulation"
      ],
      "methodology_detail": "Analyzes outcome variability and propagation of uncertainty",
      "geographic_focus": [
        "Pennsylvania"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2112.00269v1",
    "title": "Unequal Opportunities in Multi-hop Referral Programs",
    "year": 2021,
    "authors": [
      "Yiguang Zhang",
      "Augustin Chaintreau"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines structural biases in social networks affecting disadvantaged groups, focusing on racial and social disparities in referral opportunities.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias amplification in multi-hop referral programs",
      "affected_populations": [
        "minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of real-world networks and network models",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.14581v2",
    "title": "Learning Fair Classifiers with Partially Annotated Group Labels",
    "year": 2021,
    "authors": [
      "Sangwon Jung",
      "Sanghyuk Chun",
      "Taesup Moon"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on group fairness and bias mitigation, which relates to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "social groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness-aware learning",
        "Pseudo-labeling"
      ],
      "methodology_detail": "Improves fairness metrics with pseudo group labels",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.14348v1",
    "title": "A Causal Approach for Unfair Edge Prioritization and Discrimination Removal",
    "year": 2021,
    "authors": [
      "Pavan Ravishankar",
      "Pranshu Malviya",
      "Balaraman Ravindran"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on unfairness related to sensitive attributes like race, addressing discrimination mitigation in decision-making processes.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial unfairness in decision systems",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Causal Analysis",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Causal modeling and fairness mitigation algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.13266v1",
    "title": "Examining Needs and Opportunities for Supporting Students Who Experience Discrimination",
    "year": 2021,
    "authors": [
      "Yasaman S. Sefidgar",
      "Paula S. Nurius",
      "Amanda Baughan",
      "Lisa A. Elkin",
      "Anind K. Dey",
      "Eve Riskin",
      "Jennifer Mankoff",
      "Margaret E. Morris"
    ],
    "categories": [
      "cs.HC",
      "J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses perceived discrimination among students, related to social bias and inequality in social and educational contexts.",
      "inequality_type": [
        "educational",
        "racial",
        "social"
      ],
      "other_detail": "Focus on discrimination experiences in college settings",
      "affected_populations": [
        "college students"
      ],
      "methodology": [
        "Qualitative Study",
        "Experience Sampling"
      ],
      "methodology_detail": "Interviews and experience sampling over ten weeks",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.13259v1",
    "title": "Identification of Bias Against People with Disabilities in Sentiment Analysis and Toxicity Detection Models",
    "year": 2021,
    "authors": [
      "Pranav Narayanan Venkit",
      "Shomir Wilson"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases against people with disabilities in AI models, addressing social discrimination and fairness issues related to disability.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Probing models with bias test corpus",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.12823v2",
    "title": "Fairness for AUC via Feature Augmentation",
    "year": 2021,
    "authors": [
      "Hortense Fong",
      "Vineet Kumar",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in classification, focusing on reducing performance disparities between protected groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness across social groups in AI models",
      "affected_populations": [
        "protected groups",
        "disadvantaged group"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Feature augmentation to improve group-specific AUC",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.12115v1",
    "title": "Algorithmic Fairness in Face Morphing Attack Detection",
    "year": 2021,
    "authors": [
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic fairness across ethnic groups, addressing racial and ethnic bias in AI systems.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on bias in face morphing attack detection algorithms",
      "affected_populations": [
        "ethnic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Performance benchmarking and fairness measurement across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.11665v2",
    "title": "RadFusion: Benchmarking Performance and Fairness for Multimodal Pulmonary Embolism Detection from CT and EHR",
    "year": 2021,
    "authors": [
      "Yuyin Zhou",
      "Shih-Cheng Huang",
      "Jason Alan Fries",
      "Alaa Youssef",
      "Timothy J. Amrhein",
      "Marcello Chang",
      "Imon Banerjee",
      "Daniel Rubin",
      "Lei Xing",
      "Nigam Shah",
      "Matthew P. Lungren"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in AI models across demographic groups, addressing social disparities in healthcare.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on fairness in medical AI systems",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Benchmarking"
      ],
      "methodology_detail": "Evaluates model fairness across protected subgroups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.11159v1",
    "title": "Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains",
    "year": 2021,
    "authors": [
      "Somya Khosla"
    ],
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language embeddings, addressing social discrimination related to gender. It analyzes how AI models may perpetuate gender stereotypes across languages and domains.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on linguistic gender bias in AI models",
      "affected_populations": [
        "Hindi speakers",
        "English speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring bias across corpora and embeddings",
      "geographic_focus": [
        "India",
        "Global"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.10745v1",
    "title": "COVID Induced Digital Inequality for Senior Citizens",
    "year": 2021,
    "authors": [
      "Nicky Qiu"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines digital inequality affecting senior citizens' access to health technology, highlighting disparities in technology use and social impacts.",
      "inequality_type": [
        "age",
        "digital",
        "health"
      ],
      "other_detail": "Focus on senior citizens' digital access in China",
      "affected_populations": [
        "senior citizens"
      ],
      "methodology": [
        "Qualitative Study",
        "Case Study"
      ],
      "methodology_detail": "Analysis of user interactions and social impacts",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.10390v1",
    "title": "The ComMA Dataset V0.2: Annotating Aggression and Bias in Multilingual Social Media Discourse",
    "year": 2021,
    "authors": [
      "Ritesh Kumar",
      "Enakshi Nandi",
      "Laishram Niranjana Devi",
      "Shyam Ratan",
      "Siddharth Singh",
      "Akash Bhagat",
      "Yogesh Dawer"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on annotating bias and aggression related to gender, religion, caste, and ethnicity in social media discourse, which are directly linked to social inequalities and discrimination.",
      "inequality_type": [
        "gender",
        "religious",
        "ethnic",
        "caste"
      ],
      "other_detail": "Bias annotation in multilingual social media comments",
      "affected_populations": [
        "gender groups",
        "religious communities",
        "ethnic groups",
        "caste groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Annotated multilingual social media comments for bias",
      "geographic_focus": [
        "India",
        "Bangladesh",
        "Multilingual regions"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.09983v1",
    "title": "Towards Measuring Fairness in Speech Recognition: Casual Conversations Dataset Transcriptions",
    "year": 2021,
    "authors": [
      "Chunxi Liu",
      "Michael Picheny",
      "Leda Sarı",
      "Pooja Chitkara",
      "Alex Xiao",
      "Xiaohui Zhang",
      "Mark Chou",
      "Andres Alvarado",
      "Caner Hazirbas",
      "Yatharth Saraf"
    ],
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in speech recognition related to gender and skin tone, which are social identity factors, and discusses fairness in AI systems.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in speech recognition accuracy across social groups",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating ASR models on diverse demographic data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.09507v1",
    "title": "Assessing Social Determinants-Related Performance Bias of Machine Learning Models: A case of Hyperchloremia Prediction in ICU Population",
    "year": 2021,
    "authors": [
      "Songzi Liu",
      "Yuan Luo"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines disparities in ML model performance across social groups defined by race, gender, and insurance status, highlighting social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on healthcare disparities in ICU patient outcomes",
      "affected_populations": [
        "ethnic minorities",
        "lower SES groups",
        "ICU patients"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Performance comparison across social subgroups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.09137v1",
    "title": "Two-Face: Adversarial Audit of Commercial Face Recognition Systems",
    "year": 2021,
    "authors": [
      "Siddharth D Jaiswal",
      "Karthikeya Duggirala",
      "Abhisek Dash",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in face recognition systems affecting minority groups, highlighting social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender"
      ],
      "other_detail": "Bias amplification under adversarial inputs",
      "affected_populations": [
        "minority groups",
        "racial minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Adversarial audit on datasets and systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.11203v1",
    "title": "A Data-Centric Behavioral Machine Learning Platform to Reduce Health Inequalities",
    "year": 2021,
    "authors": [
      "Dexian Tang",
      "Guillem Francès",
      "África Periáñez"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on reducing health inequalities in low- and middle-income countries through AI-driven health interventions, addressing disparities in health outcomes.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on health disparities in developing countries",
      "affected_populations": [
        "maternal",
        "newborns",
        "health workers"
      ],
      "methodology": [
        "Machine Learning",
        "Data Pipeline Design"
      ],
      "methodology_detail": "Developing a data-centric platform for health data analysis",
      "geographic_focus": [
        "low- and middle-income countries"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.08711v1",
    "title": "Two-step adversarial debiasing with partial learning -- medical image case-studies",
    "year": 2021,
    "authors": [
      "Ramon Correa",
      "Jiwoong Jason Jeong",
      "Bhavik Patel",
      "Hari Trivedi",
      "Judy W. Gichoya",
      "Imon Banerjee"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias and disparities in medical AI models, focusing on reducing racial bias in healthcare image classification, which relates directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias reduction in medical AI systems",
      "affected_populations": [
        "ethnic groups",
        "subpopulations"
      ],
      "methodology": [
        "Adversarial Machine Learning",
        "Experiment",
        "Case Study"
      ],
      "methodology_detail": "Two-step adversarial debiasing approach with partial learning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.08088v1",
    "title": "Assessing gender bias in medical and scientific masked language models with StereoSet",
    "year": 2021,
    "authors": [
      "Robert Robinson"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper assesses biases related to gender, race, religion, and profession in language models, highlighting social discrimination embedded in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "religion",
        "professional"
      ],
      "other_detail": "Bias assessment in AI language models",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups",
        "professionals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Bias Assessment Tool"
      ],
      "methodology_detail": "Using StereoSet to evaluate stereotypes in MLMs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.07997v2",
    "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    "year": 2021,
    "authors": [
      "Maarten Sap",
      "Swabha Swayamdipta",
      "Laura Vianna",
      "Xuhui Zhou",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how biases in toxicity annotations relate to social identities and beliefs, highlighting racial and political biases that impact marginalized groups. It discusses social discrimination and bias in AI systems affecting different social groups.",
      "inequality_type": [
        "racial",
        "political",
        "social bias"
      ],
      "other_detail": "Bias in toxicity detection systems",
      "affected_populations": [
        "Black communities",
        "AAVE speakers",
        "marginalized groups"
      ],
      "methodology": [
        "Experiment",
        "Survey",
        "Case Study"
      ],
      "methodology_detail": "Studies diverse annotators and analyzes toxicity ratings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.07889v1",
    "title": "An Outcome Test of Discrimination for Ranked Lists",
    "year": 2021,
    "authors": [
      "Jonathan Roth",
      "Guillaume Saint-Jacques",
      "YinYin Yu"
    ],
    "categories": [
      "econ.EM",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper extends an outcome test of discrimination, focusing on fairness in ranked lists, which relates to social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focus on discrimination in algorithmic and human decision-making",
      "affected_populations": [
        "group1",
        "group2"
      ],
      "methodology": [
        "Statistical Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Testing inequalities in ranking systems",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.06689v1",
    "title": "Strategic COVID-19 vaccine distribution can simultaneously elevate social utility and equity",
    "year": 2021,
    "authors": [
      "Lin Chen",
      "Fengli Xu",
      "Zhenyu Han",
      "Kun Tang",
      "Pan Hui",
      "James Evans",
      "Yong Li"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses balancing equity and utility in vaccine distribution, focusing on disparities among communities, including disadvantaged groups, and designing strategies to address social risks and harms.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "geographic"
      ],
      "other_detail": "Focuses on disadvantaged communities and societal risks",
      "affected_populations": [
        "disadvantaged communities",
        "societal groups"
      ],
      "methodology": [
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Epidemic modeling with demographic and mobility data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.05142v1",
    "title": "The Second-Level Smartphone Divide: A Typology of Smartphone Usage Based on Frequency of Use, Skills, and Types of Activities",
    "year": 2021,
    "authors": [
      "Alexander Wenz",
      "Florian Keusch"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines disparities in smartphone usage based on sociodemographic factors such as age, education, and device type, indicating an analysis of social inequalities. It highlights differences across social groups, reflecting inequalities in digital access and skills.",
      "inequality_type": [
        "digital",
        "educational",
        "socioeconomic",
        "age"
      ],
      "other_detail": "Focuses on second-level digital divide in smartphone use",
      "affected_populations": [
        "younger users",
        "higher education",
        "iPhone users"
      ],
      "methodology": [
        "Latent Class Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Classifies users based on usage, skills, and activities",
      "geographic_focus": [
        "Germany",
        "Austria"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.04673v2",
    "title": "Information-Theoretic Bias Assessment Of Learned Representations Of Pretrained Face Recognition",
    "year": 2021,
    "authors": [
      "Jiazhi Li",
      "Wael Abd-Almageed"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias in face recognition systems affecting protected demographic groups, addressing fairness and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Bias measurement in facial recognition representations",
      "affected_populations": [
        "minorities",
        "protected demographic groups"
      ],
      "methodology": [
        "Information-Theoretic Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Proposes a new bias assessment metric",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.04671v1",
    "title": "Equity and Privacy: More Than Just a Tradeoff",
    "year": 2021,
    "authors": [
      "David Pujol",
      "Ashwin Machanavajjhala"
    ],
    "categories": [
      "cs.CY",
      "cs.CR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses disparities in utility across different population groups when applying privacy technology, highlighting potential inequities affecting marginalized populations.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "geographic"
      ],
      "other_detail": "Focuses on utility disparities in privacy technology deployment",
      "affected_populations": [
        "marginalized groups",
        "population groups"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing utility disparities across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.03687v1",
    "title": "AI and Blackness: Towards moving beyond bias and representation",
    "year": 2021,
    "authors": [
      "Christopher L. Dancy",
      "P. Khalil Saucier"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; J.4.0; K.4.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial bias and antiblackness in AI, focusing on social discrimination and inequality related to race. It examines how AI systems impact marginalized racial groups and addresses issues of bias and representation.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on anti-Black racism in AI systems",
      "affected_populations": [
        "Black communities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Literature Review"
      ],
      "methodology_detail": "Auditing ConceptNet; analyzing sociocultural context",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.03638v1",
    "title": "Increasing Fairness in Predictions Using Bias Parity Score Based Loss Function Regularization",
    "year": 2021,
    "authors": [
      "Bhanu Jain",
      "Manfred Huber",
      "Ramez Elmasri"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI predictions, addressing bias reduction, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI decision-making processes",
      "affected_populations": [
        "recidivism risk groups",
        "income groups"
      ],
      "methodology": [
        "Machine Learning",
        "Regularization",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias Parity Score based regularization techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.06224v1",
    "title": "Occupational Income Inequality of Thailand: A Case Study of Exploratory Data Analysis beyond Gini Coefficient",
    "year": 2021,
    "authors": [
      "Wanetha Sudswong",
      "Anon Plangprasopchok",
      "Chainarong Amornbunchornvej"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC",
      "stat.AP",
      "62P25",
      "K.4.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes income inequality across occupations and regions, highlighting socioeconomic disparities. It discusses subpopulations within the economy, which relate to social inequality issues.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on occupational income disparities in Thailand",
      "affected_populations": [
        "workers",
        "occupations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Uses income data and network analysis techniques",
      "geographic_focus": [
        "Thailand"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.01201v2",
    "title": "Unintended Selection: Persistent Qualification Rate Disparities and Interventions",
    "year": 2021,
    "authors": [
      "Reilly Raab",
      "Yang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models group-level disparities in qualification rates, reflecting social inequalities. It examines how AI systems contribute to persistent disparities among subpopulations. The focus on group disparities and interventions indicates addressing social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "racial"
      ],
      "other_detail": "Focus on qualification disparities and fairness interventions",
      "affected_populations": [
        "subpopulations",
        "groups"
      ],
      "methodology": [
        "Mathematical Modeling",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Replicator equation and dynamical system analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.00340v1",
    "title": "Identifying and mitigating bias in algorithms used to manage patients in a pandemic",
    "year": 2021,
    "authors": [
      "Yifan Li",
      "Garrett Yoon",
      "Mustafa Nasir-Moin",
      "David Rosenberg",
      "Sean Neifert",
      "Douglas Kondziolka",
      "Eric Karl Oermann"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI systems affecting different social groups, specifically race, gender, and age, in healthcare decision-making during a pandemic.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias mitigation in healthcare AI models",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias detection and calibration techniques applied",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.00107v4",
    "title": "The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning",
    "year": 2021,
    "authors": [
      "Ahmed Izzidien",
      "David Stillwell"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in language, relating to social discrimination, and discusses how to measure and mitigate unfair biases in AI systems, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in language and AI systems",
      "affected_populations": [
        "discriminated groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Operationalising moral philosophy in computational systems",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2111.00052v3",
    "title": "Diagnosing Data from ICTs to Provide Focused Assistance in Agricultural Adoptions",
    "year": 2021,
    "authors": [
      "Ashwin Singh",
      "Mallika Subramanian",
      "Anmol Agarwal",
      "Pratyush Priyadarshi",
      "Shrey Gupta",
      "Kiran Garimella",
      "Sanjeev Kumar",
      "Ritesh Kumar",
      "Lokesh Garg",
      "Erica Arya",
      "Ponnurangam Kumaraguru"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in agricultural practice adoption related to gender and social factors, and discusses inequalities in access and benefits among farmers.",
      "inequality_type": [
        "gender",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on gender and regional disparities in adoption",
      "affected_populations": [
        "smallholder farmers",
        "women farmers",
        "rural communities"
      ],
      "methodology": [
        "Statistical Analysis",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses statistical tests and predictive modeling",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.15310v2",
    "title": "On the Fairness of Machine-Assisted Human Decisions",
    "year": 2021,
    "authors": [
      "Talia Gillis",
      "Bryce McLaughlin",
      "Jann Spiess"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "econ.GN",
      "q-fin.EC",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness, disparities, and biases in decision-making involving social groups such as gender. It examines how algorithmic predictions impact human decisions and social disparities.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on decision fairness and disparities",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Formal modeling",
        "Lab experiment"
      ],
      "methodology_detail": "Analyzes decision systems and tests gender-specific predictions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.14839v1",
    "title": "Hate Speech Classifiers Learn Human-Like Social Stereotypes",
    "year": 2021,
    "authors": [
      "Aida Mostafazadeh Davani",
      "Mohammad Atari",
      "Brendan Kennedy",
      "Morteza Dehghani"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how hate speech classifiers learn human-like biases, which can perpetuate social inequalities related to marginalized groups. It discusses biases in AI systems affecting social discrimination and fairness. The focus on stereotypes and their impact on social groups indicates a direct link to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Bias perpetuation in automated hate speech detection",
      "affected_populations": [
        "minority groups",
        "social minorities"
      ],
      "methodology": [
        "Social Psychological Analysis",
        "Computational Linguistics",
        "Experiment",
        "Data Analysis"
      ],
      "methodology_detail": "Combines social psychology and NLP methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.14419v3",
    "title": "Toward a Theory of Justice for Artificial Intelligence",
    "year": 2021,
    "authors": [
      "Iason Gabriel"
    ],
    "categories": [
      "cs.CY",
      "K.4.1; K.4.2; K.5.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses justice principles related to AI's impact on society, emphasizing fairness and support for the worst-off, which relates to social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on societal justice and fairness in AI deployment",
      "affected_populations": [
        "the worst-off members"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Applying Rawlsian political philosophy to AI",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.13054v2",
    "title": "Adaptive Data Debiasing through Bounded Exploration",
    "year": 2021,
    "authors": [
      "Yifan Yang",
      "Yang Liu",
      "Parinaz Naghizadeh"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in datasets that lead to unfair treatment of groups, discussing fairness and debiasing in algorithmic decision-making.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on data bias reduction for fair AI decisions",
      "affected_populations": [
        "social groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Adaptive debiasing algorithms with fairness considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.12838v1",
    "title": "Debiasing Credit Scoring using Evolutionary Algorithms",
    "year": 2021,
    "authors": [
      "Nigel Kingsman"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.NE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias and discrimination in credit scoring models, which relate to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias mitigation in AI models",
      "affected_populations": [
        "minority groups",
        "disadvantaged individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Empirical Study"
      ],
      "methodology_detail": "Analyzes bias trade-offs in AI training",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.12038v1",
    "title": "Characterizing Performance Inequity Across U.S. Ookla Speedtest Users",
    "year": 2021,
    "authors": [
      "Udit Paul",
      "Jiamo Liu",
      "Vivek Adarsh",
      "Mengyang Gu",
      "Arpit Gupta",
      "Elizabeth Belding"
    ],
    "categories": [
      "cs.NI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes disparities in internet performance across social groups, such as urban-rural and income levels, highlighting digital inequality. It examines how these disparities vary geographically and socioeconomically, addressing social inequality dimensions.",
      "inequality_type": [
        "digital",
        "geographic",
        "income",
        "urban-rural"
      ],
      "other_detail": "Focus on internet access and quality disparities",
      "affected_populations": [
        "urban users",
        "rural users",
        "high-income users",
        "low-income users"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of speedtest and demographic datasets",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.10200v1",
    "title": "fairadapt: Causal Reasoning for Fair Data Pre-processing",
    "year": 2021,
    "authors": [
      "Drago Plečko",
      "Nicolas Bennett",
      "Nicolai Meinshausen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML",
      "I.2.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias related to gender and race, aiming to mitigate discrimination. It discusses causal reasoning to promote fair decisions, directly engaging with social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Causal Inference",
        "Pre-processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses causal graphical models for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.09843v2",
    "title": "AequeVox: Automated Fairness Testing of Speech Recognition Systems",
    "year": 2021,
    "authors": [
      "Sai Sathiesh Rajan",
      "Sakshi Udeshi",
      "Sudipta Chattopadhyay"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in speech recognition across social groups, highlighting disparities related to race, gender, and language, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "linguistic"
      ],
      "other_detail": "Focus on fairness disparities in AI systems",
      "affected_populations": [
        "non-native English speakers",
        "female speakers",
        "Nigerian English speakers"
      ],
      "methodology": [
        "Experiment",
        "Fairness Testing",
        "User Study"
      ],
      "methodology_detail": "Simulations, error analysis, human comprehensibility assessment",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.09839v1",
    "title": "Measuring Hidden Bias within Face Recognition via Racial Phenotypes",
    "year": 2021,
    "authors": [
      "Seyma Yucer",
      "Furkan Tektas",
      "Noura Al Moubayed",
      "Toby P. Breckon"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in face recognition, addressing racial disparities and bias analysis, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "discrimination"
      ],
      "other_detail": "Focuses on racial phenotypes and bias detection",
      "affected_populations": [
        "racial groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using facial phenotypes for bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.09424v1",
    "title": "Don't Judge Me by My Face : An Indirect Adversarial Approach to Remove Sensitive Information From Multimodal Neural Representation in Asynchronous Job Video Interviews",
    "year": 2021,
    "authors": [
      "Léo Hemamou",
      "Arthur Guillon",
      "Jean-Claude Martin",
      "Chloé Clavel"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems analyzing job interviews, aiming to reduce bias related to sensitive attributes like gender and ethnicity, which are key social inequality factors.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Fairness in AI for employment contexts",
      "affected_populations": [
        "job candidates",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training"
      ],
      "methodology_detail": "Removing sensitive info without explicit labels",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.09295v3",
    "title": "Fair Tree Classifier using Strong Demographic Parity",
    "year": 2021,
    "authors": [
      "António Pereira Barata",
      "Frank W. Takes",
      "H. Jaap van den Herik",
      "Cor J. Veenman"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI models concerning sensitive attributes like gender and race, addressing social bias and discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic decision-making",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness-aware tree optimization techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08944v3",
    "title": "Developing a novel fair-loan-predictor through a multi-sensitive debiasing pipeline: DualFair",
    "year": 2021,
    "authors": [
      "Jashandeep Singh",
      "Arashdeep Singh",
      "Ariba Khan",
      "Amar Gupta"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI models affecting social groups based on race and socioeconomic status, specifically in loan prediction, highlighting fairness issues and impacts on marginalized populations.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fair lending and discrimination mitigation",
      "affected_populations": [
        "minority borrowers",
        "low-income groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metric Development",
        "Experiment"
      ],
      "methodology_detail": "Novel bias mitigation and fairness metric for multi-sensitive parameters",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08835v1",
    "title": "Towards More Accountable Search Engines: Online Evaluation of Representation Bias",
    "year": 2021,
    "authors": [
      "Aldo Lipani",
      "Florina Piroi",
      "Emine Yilmaz"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses representation bias in search engines related to societal features like gender, which impacts social perceptions and fairness.",
      "inequality_type": [
        "gender",
        "informational"
      ],
      "other_detail": "Focus on societal feature bias in AI systems",
      "affected_populations": [
        "women",
        "general users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluating gender bias in search engine results",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08583v1",
    "title": "ASR4REAL: An extended benchmark for speech models",
    "year": 2021,
    "authors": [
      "Morgane Riviere",
      "Jade Copet",
      "Gabriel Synnaeve"
    ],
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses performance disparities in speech recognition models based on accent and socio-economic status, indicating social bias and inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "accent",
        "gender"
      ],
      "other_detail": "Performance disparities across social and linguistic groups",
      "affected_populations": [
        "non-native speakers",
        "lower socio-economic groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Testing speech models on diverse real-life conditions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08527v3",
    "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
    "year": 2021,
    "authors": [
      "Nicholas Meade",
      "Elinor Poole-Dayan",
      "Siva Reddy"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases in language models, which relate to social discrimination and inequality issues such as gender and racial biases.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Empirical Survey",
        "Intrinsic Bias Benchmarking",
        "Performance Evaluation"
      ],
      "methodology_detail": "Comparative analysis of bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08396v2",
    "title": "Comparing Human and Machine Bias in Face Recognition",
    "year": 2021,
    "authors": [
      "Samuel Dooley",
      "Ryan Downing",
      "George Wei",
      "Nathan Shankar",
      "Bradon Thymes",
      "Gudrun Thorkelsdottir",
      "Tiye Kurtz-Miott",
      "Rachel Mattson",
      "Olufemi Obiwumi",
      "Valeriia Cherepanova",
      "Micah Goldblum",
      "John P Dickerson",
      "Tom Goldstein"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper compares human and machine bias in face recognition, highlighting disparities based on gender and skin tone, which are social identity factors. It discusses bias and fairness issues affecting different social groups. The focus on bias in facial analysis technologies relates directly to social discrimination concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in facial recognition technologies",
      "affected_populations": [
        "women",
        "dark-skinned individuals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Survey"
      ],
      "methodology_detail": "Comparing algorithms and human performance on face recognition tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.15733v1",
    "title": "Detecting Gender Bias in Transformer-based Models: A Case Study on BERT",
    "year": 2021,
    "authors": [
      "Bingbing Li",
      "Hongwu Peng",
      "Rajat Sainju",
      "Junhuan Yang",
      "Lei Yang",
      "Yueying Liang",
      "Weiwen Jiang",
      "Binghui Wang",
      "Hang Liu",
      "Caiwen Ding"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "I.2; I.7; H.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting gender bias in AI models, addressing gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Attention map analysis and bias detection techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08353v1",
    "title": "Revisiting Popularity and Demographic Biases in Recommender Evaluation and Effectiveness",
    "year": 2021,
    "authors": [
      "Nicola Neophytou",
      "Bhaskar Mitra",
      "Catherine Stinson"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases and disparities in recommender systems, highlighting differences in recommendation quality across gender, age, and country representation, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "age",
        "geographic",
        "demographic"
      ],
      "other_detail": "Focuses on social disparities in AI recommendation performance",
      "affected_populations": [
        "women",
        "older users",
        "underrepresented countries"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes recommendation utility across demographic groups",
      "geographic_focus": [
        "unspecified countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.08193v2",
    "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "year": 2021,
    "authors": [
      "Alicia Parrish",
      "Angelica Chen",
      "Nikita Nangia",
      "Vishakh Padmakumar",
      "Jason Phang",
      "Jana Thompson",
      "Phu Mon Htut",
      "Samuel R. Bowman"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to protected classes, highlighting stereotypes in AI outputs, which directly relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focuses on social biases in question answering models",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "protected classes"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Constructs bias benchmark and evaluates model responses",
      "geographic_focus": [
        "U.S."
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.07871v2",
    "title": "Socially Aware Bias Measurements for Hindi Language Representations",
    "year": 2021,
    "authors": [
      "Vijit Malik",
      "Sunipa Dev",
      "Akihiro Nishi",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates societal biases related to caste and religion in Hindi language representations, highlighting social discrimination and bias in AI systems.",
      "inequality_type": [
        "religion",
        "ethnic",
        "social discrimination"
      ],
      "other_detail": "Focus on cultural and societal biases in language models",
      "affected_populations": [
        "Hindi speakers",
        "caste groups",
        "religious communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and analysis in language models",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.06282v4",
    "title": "The Rich Get Richer: Disparate Impact of Semi-Supervised Learning",
    "year": 2021,
    "authors": [
      "Zhaowei Zhu",
      "Tianyi Luo",
      "Yang Liu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how semi-supervised learning impacts different sub-populations, highlighting disparities based on baseline accuracy, which relates to social fairness issues.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness across social groups",
      "affected_populations": [
        "high baseline group",
        "low baseline group"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Analyzes impact on sub-populations in classification tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.15728v1",
    "title": "Deep Learning for Bias Detection: From Inception to Deployment",
    "year": 2021,
    "authors": [
      "Md Abul Bashar",
      "Richi Nayak",
      "Anjor Kothare",
      "Vishal Sharma",
      "Kesavan Kandadai"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting unconscious bias related to gender, race, age, disability, and religion in enterprise content, which are social inequalities. It addresses social bias and fairness issues in AI systems used within workplaces. The goal is to promote inclusivity and reduce discrimination.",
      "inequality_type": [
        "gender",
        "race",
        "age",
        "disability",
        "religion"
      ],
      "other_detail": "Bias detection in workplace content",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly",
        "disabled",
        "religious groups"
      ],
      "methodology": [
        "Deep Learning",
        "Natural Language Processing",
        "Model Development"
      ],
      "methodology_detail": "Transfer learning on language models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.05367v3",
    "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
    "year": 2021,
    "authors": [
      "Zahra Fatemi",
      "Chen Xing",
      "Wenhao Liu",
      "Caiming Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Prompt-based fairness improvement technique",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.04384v1",
    "title": "Evaluation of Summarization Systems across Gender, Age, and Race",
    "year": 2021,
    "authors": [
      "Anna Jørgensen",
      "Anders Søgaard"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how demographic biases in evaluation affect AI summarization systems, highlighting social bias and fairness issues related to protected attributes like gender, age, and race.",
      "inequality_type": [
        "gender",
        "age",
        "racial"
      ],
      "other_detail": "Bias in AI evaluation processes",
      "affected_populations": [
        "end users",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "Evaluation sensitivity analysis across demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.04363v1",
    "title": "Certifying Robustness to Programmable Data Bias in Decision Trees",
    "year": 2021,
    "authors": [
      "Anna P. Meyer",
      "Aws Albarghouthi",
      "Loris D'Antoni"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "I.2.2; I.5.0; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses dataset biases related to societal inequities and fairness, aiming to certify robustness against biases affecting groups such as minorities. It focuses on bias models that can target specific social groups, indicating a concern with social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Bias certification in decision models for social fairness",
      "affected_populations": [
        "minorities",
        "underrepresented groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Symbolic techniques for robustness certification",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.03524v1",
    "title": "Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling",
    "year": 2021,
    "authors": [
      "Naveen Raman",
      "Sanket Shah",
      "John Dickerson"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses income inequality among ride-share drivers and fairness in algorithmic matching, which impacts socioeconomic disparities.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focuses on income distribution and fairness in AI systems",
      "affected_populations": [
        "ride-share drivers",
        "riders in underserved areas"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates fairness and income redistribution methods using NYC data",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.03026v1",
    "title": "Human Capabilities as Guiding Lights for the Field of AI-HRI: Insights from Engineering Education",
    "year": 2021,
    "authors": [
      "Tom Williams",
      "Ruchen Wen"
    ],
    "categories": [
      "cs.RO",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social justice frameworks and their application to AI-HRI, focusing on equitable societal impacts and community needs, indicating engagement with social inequalities.",
      "inequality_type": [
        "social",
        "educational",
        "inequity"
      ],
      "other_detail": "Focus on social justice and equitable technology development",
      "affected_populations": [
        "underserved communities",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis",
        "Speculative Exercise"
      ],
      "methodology_detail": "Analysis of AI-HRI papers and future scenarios",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.01962v1",
    "title": "Gender Bias in Remote Pair Programming among Software Engineering Students: The twincode Exploratory Study",
    "year": 2021,
    "authors": [
      "Amador Durán",
      "Pablo Fernández",
      "Beatriz Bernárdez",
      "Nathaniel Weinman",
      "Aslı Akalın",
      "Armando Fox"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study investigates gender bias in remote pair programming, directly addressing gender inequality and social bias in technology education.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women in tech",
        "students"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis",
        "Survey"
      ],
      "methodology_detail": "Behavior measurement and questionnaires during remote programming",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.01951v1",
    "title": "Multi-Objective Few-shot Learning for Fair Classification",
    "year": 2021,
    "authors": [
      "Ishani Mondal",
      "Procheta Sen",
      "Debasis Ganguly"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in classification related to secondary attributes like race and gender, aiming to mitigate biases and promote fairness in AI systems, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI classification",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Clustering",
        "Experiment"
      ],
      "methodology_detail": "Multi-objective learning with clustering heuristic",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.01109v3",
    "title": "FairMask: Better Fairness via Model-based Rebalancing of Protected Attributes",
    "year": 2021,
    "authors": [
      "Kewen Peng",
      "Joymallya Chakraborty",
      "Tim Menzies"
    ],
    "categories": [
      "cs.LG",
      "cs.SE",
      "D.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias affecting protected social groups, aiming to improve fairness and reduce discrimination based on gender and other social attributes.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on fairness in AI models",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation and explanation via model-based rebalancing",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.01094v1",
    "title": "Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models",
    "year": 2021,
    "authors": [
      "Wenqian Ye",
      "Fei Xu",
      "Yaojia Huang",
      "Cassie Huang",
      "Ji A"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, a social inequality issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Implicit gender bias at sentence-level",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Adversarial Examples Generation"
      ],
      "methodology_detail": "Generating and evaluating bias samples",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00857v3",
    "title": "FairFed: Enabling Group Fairness in Federated Learning",
    "year": 2021,
    "authors": [
      "Yahya H. Ezzeldin",
      "Shen Yan",
      "Chaoyang He",
      "Emilio Ferrara",
      "Salman Avestimehr"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness across demographic groups in AI models, addressing social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in federated learning across social groups",
      "affected_populations": [
        "demographic groups",
        "geographical populations"
      ],
      "methodology": [
        "Algorithm Development",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness-aware aggregation in federated learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00672v1",
    "title": "Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models",
    "year": 2021,
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in language models, highlighting disparities in representation and bias related to minority and female names, which reflect social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI representations of minority and female names",
      "affected_populations": [
        "minority groups",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes model representations and biases statistically",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00603v2",
    "title": "Algorithm Fairness in AI for Medicine and Healthcare",
    "year": 2021,
    "authors": [
      "Richard J. Chen",
      "Tiffany Y. Chen",
      "Jana Lipkova",
      "Judy J. Wang",
      "Drew F. K. Williamson",
      "Ming Y. Lu",
      "Sharifa Sahai",
      "Faisal Mahmood"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in healthcare diagnosis, treatment, and costs across racial and other social groups, highlighting algorithm biases affecting equitable care.",
      "inequality_type": [
        "racial",
        "health",
        "disparities"
      ],
      "other_detail": "Focuses on healthcare inequalities and algorithmic bias",
      "affected_populations": [
        "racial groups",
        "patients"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "System Design"
      ],
      "methodology_detail": "Review of fairness techniques and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00521v1",
    "title": "Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens",
    "year": 2021,
    "authors": [
      "Saad Hassan",
      "Matt Huenerfauth",
      "Cecilia Ovesdotter Alm"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines ableist bias in NLP systems, highlighting discrimination against disabled people and intersecting identities, addressing social inequalities.",
      "inequality_type": [
        "disability",
        "gender",
        "racial"
      ],
      "other_detail": "Intersections of multiple social identities analyzed",
      "affected_populations": [
        "people with disabilities",
        "gender minorities",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Word prediction analysis of BERT model",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00497v1",
    "title": "DiVRsify: Break the Cycle and Develop VR for Everyone",
    "year": 2021,
    "authors": [
      "Tabitha C. Peck",
      "Kyla McMullen",
      "John Quarles"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in VR technology affecting diverse social groups, highlighting inequalities related to gender, race, culture, age, disability, and other diversity dimensions, and calls for inclusive development.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability",
        "age",
        "cultural"
      ],
      "other_detail": "Focus on diversity in VR research and usability",
      "affected_populations": [
        "women",
        "racial minorities",
        "cultural groups",
        "disabled individuals",
        "older adults"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study",
        "Experiment"
      ],
      "methodology_detail": "Comparative analysis of VR usability across groups",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00116v2",
    "title": "#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics",
    "year": 2021,
    "authors": [
      "Jacqueline Comer",
      "Sam Work",
      "Kory W Mathewson",
      "Lana Cuthbertson",
      "Kasey Machin"
    ],
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender inequality in politics and online harassment, analyzing social bias in NLP systems affecting women in politics.",
      "inequality_type": [
        "gender",
        "digital",
        "social bias"
      ],
      "other_detail": "Focus on online toxicity and microaggressions against women",
      "affected_populations": [
        "women in politics",
        "female candidates"
      ],
      "methodology": [
        "Natural Language Processing",
        "Case Study",
        "Experiment"
      ],
      "methodology_detail": "Analyzes tweets and NLP intervention effectiveness",
      "geographic_focus": [
        "Canada",
        "United States",
        "New Zealand"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.00072v2",
    "title": "Inequality and Inequity in Network-based Ranking and Recommendation Algorithms",
    "year": 2021,
    "authors": [
      "Lisette Espín-Noboa",
      "Claudia Wagner",
      "Markus Strohmaier",
      "Fariba Karimi"
    ],
    "categories": [
      "cs.SI",
      "cs.DS",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how ranking algorithms influence inequality and inequity among social groups, focusing on minority representation and visibility, which relate to social disparities.",
      "inequality_type": [
        "racial",
        "minority",
        "social"
      ],
      "other_detail": "Focus on social group representation in rankings",
      "affected_populations": [
        "minorities",
        "majorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Network modeling and rank distribution analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.15294v1",
    "title": "Targeted Ads and/as Racial Discrimination: Exploring Trends in New York City Ads for College Scholarships",
    "year": 2021,
    "authors": [
      "Ho-Chun Herbert Chang",
      "Matt Bui",
      "Charlton McIlwain"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how targeted advertising perpetuates racial discrimination and systemic inequalities, focusing on impacts on marginalized groups and systemic bias in digital platforms.",
      "inequality_type": [
        "racial",
        "educational",
        "digital"
      ],
      "other_detail": "Focus on racial discrimination in online advertising",
      "affected_populations": [
        "racial minorities",
        "students",
        "NYC residents"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study",
        "Data Visualization"
      ],
      "methodology_detail": "Analyzes ad targeting patterns and census data",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.14047v1",
    "title": "Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution",
    "year": 2021,
    "authors": [
      "Hillary Dawkins"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI systems, a form of social inequality, by developing a test set to detect latent gender bias in coreference resolution, which impacts gender fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Creating and evaluating bias detection test set",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.13845v1",
    "title": "Not Color Blind: AI Predicts Racial Identity from Black and White Retinal Vessel Segmentations",
    "year": 2021,
    "authors": [
      "Aaron S. Coyner",
      "Praveer Singh",
      "James M. Brown",
      "Susan Ostmo",
      "R. V. Paul Chan",
      "Michael F. Chiang",
      "Jayashree Kalpathy-Cramer",
      "J. Peter Campbell"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial differences in retinal features and AI's ability to predict race, highlighting potential racial bias in medical AI systems, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Potential racial bias in medical AI systems",
      "affected_populations": [
        "Black infants",
        "White infants"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using CNNs to predict race from retinal images",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.13767v1",
    "title": "Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings",
    "year": 2021,
    "authors": [
      "Vaibhav Kumar",
      "Tenzin Singhay Bhotia",
      "Vaibhav Kumar",
      "Tanmoy Chakraborty"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, reflecting social gender inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement and debiasing in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.13283v3",
    "title": "CS Education for the Socially-Just Worlds We Need: The Case for Justice-Centered Approaches to CS in Higher Education",
    "year": 2021,
    "authors": [
      "Kevin Lin"
    ],
    "categories": [
      "cs.CY",
      "K.3.2"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social justice, equity, and resistance in CS education, emphasizing marginalized identities and social struggles, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social"
      ],
      "other_detail": "Focus on social justice in higher education",
      "affected_populations": [
        "underrepresented students",
        "minority groups"
      ],
      "methodology": [
        "Literature Review",
        "Case Study"
      ],
      "methodology_detail": "Analysis of pedagogical approaches and case implementation",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.13137v1",
    "title": "Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework",
    "year": 2021,
    "authors": [
      "Matan Halevy",
      "Camille Harris",
      "Amy Bruckman",
      "Diyi Yang",
      "Ayanna Howard"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial biases in AI systems, focusing on fairness and discrimination against African American English users.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias mitigation in language detection systems",
      "affected_populations": [
        "African American English users"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Ensemble Framework"
      ],
      "methodology_detail": "Bias reduction techniques in language classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.12422v1",
    "title": "Equality of opportunity in travel behavior prediction with deep neural networks and discrete choice models",
    "year": 2021,
    "authors": [
      "Yunhan Zheng",
      "Shenhao Wang",
      "Jinhua Zhao"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines prediction disparities across social groups such as ethnicity, income, and disability, highlighting biases that can exacerbate social inequities in transportation policy.",
      "inequality_type": [
        "racial",
        "income",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focus on prediction fairness in travel behavior modeling",
      "affected_populations": [
        "ethnic minorities",
        "low-income",
        "disabled",
        "rural populations"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation and fairness evaluation methods applied",
      "geographic_focus": [
        "Chicago",
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.10645v1",
    "title": "Contrastive Learning for Fair Representations",
    "year": 2021,
    "authors": [
      "Aili Shen",
      "Xudong Han",
      "Trevor Cohn",
      "Timothy Baldwin",
      "Lea Frermann"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in AI, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "fairness"
      ],
      "other_detail": "Focuses on bias mitigation in AI models",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Contrastive learning for fair representations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.10444v1",
    "title": "Fairness-aware Class Imbalanced Learning",
    "year": 2021,
    "authors": [
      "Shivashankar Subramanian",
      "Afshin Rahimi",
      "Timothy Baldwin",
      "Trevor Cohn",
      "Lea Frermann"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias and fairness in AI, addressing social discrimination issues. It evaluates methods to mitigate demographic biases in NLP tasks, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias mitigation in NLP applications",
      "affected_populations": [
        "minority groups",
        "majority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Methods"
      ],
      "methodology_detail": "Extends margin-loss approach with fairness enforcement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.10441v1",
    "title": "Evaluating Debiasing Techniques for Intersectional Biases",
    "year": 2021,
    "authors": [
      "Shivashankar Subramanian",
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn",
      "Lea Frermann"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to social attributes like gender and race, emphasizing intersectional fairness in NLP models, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "intersectional"
      ],
      "other_detail": "Focus on intersectional bias in NLP models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Evaluates debiasing techniques for multiple attributes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.09271v3",
    "title": "Rebuilding Trust: Queer in AI Approach to Artificial Intelligence Risk Management",
    "year": 2021,
    "authors": [
      "Ashwin",
      "William Agnew",
      "Umut Pajaro",
      "Hetvi Jethwani",
      "Arjun Subramonian"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses marginalized groups, trust, justice, and power dynamics in AI, focusing on gender, sexuality, and inclusion, which are central to social inequalities.",
      "inequality_type": [
        "gender",
        "sexuality",
        "social discrimination"
      ],
      "other_detail": "Focus on marginalized and minoritized groups in AI",
      "affected_populations": [
        "queer communities",
        "minoritized groups"
      ],
      "methodology": [
        "Participatory Design",
        "Theoretical Analysis",
        "Policy Frameworks"
      ],
      "methodology_detail": "Incorporates feminist, participatory, and epistemological approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier and subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.10431v2",
    "title": "Fairness without Imputation: A Decision Tree Approach for Fair Prediction with Missing Values",
    "year": 2021,
    "authors": [
      "Haewon Jeong",
      "Hao Wang",
      "Flavio P. Calmon"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on social bias related to group attributes like race and gender, and aims to reduce discrimination in predictive models.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI prediction with missing data",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Regularization"
      ],
      "methodology_detail": "Decision tree with fairness constraints and missing data handling",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.09946v1",
    "title": "Identifying biases in legal data: An algorithmic fairness perspective",
    "year": 2021,
    "authors": [
      "Jackson Sargent",
      "Melanie Weber"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "K.4; K.5"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in legal data, focusing on fairness across demographic groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias measurement in legal decision-making",
      "affected_populations": [
        "racial minorities",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Comparing decisions of typical and fair judges",
      "geographic_focus": [
        "Cook County, Illinois"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2110.09272v1",
    "title": "Multi-Objective Allocation of COVID-19 Testing Centers: Improving Coverage and Equity in Access",
    "year": 2021,
    "authors": [
      "Zhen Zhong",
      "Ribhu Sengupta",
      "Kamran Paynabar",
      "Lance A. Waller"
    ],
    "categories": [
      "cs.CY",
      "math.OC",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly addresses inequities in testing access related to sociodemographic patterns, racial, and income groups, aiming to improve fairness and reduce disparities.",
      "inequality_type": [
        "racial",
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on health access disparities during COVID-19",
      "affected_populations": [
        "racial groups",
        "low-income communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Comparative analysis of testing site allocations",
      "geographic_focus": [
        "Georgia, USA"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08934v2",
    "title": "Fairness Maximization among Offline Agents in Online-Matching Markets",
    "year": 2021,
    "authors": [
      "Will Ma",
      "Pan Xu",
      "Yifan Xu"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in match rates affecting marginalized social groups, such as low socioeconomic neighborhoods and racial/gender minorities, highlighting fairness issues in algorithmic decision-making.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness in algorithmic matching",
      "affected_populations": [
        "low socioeconomic status",
        "racial minorities",
        "gender minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "LP-based sampling algorithms for fairness optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08792v4",
    "title": "Learning to be Fair: A Consequentialist Approach to Equitable Decision-Making",
    "year": 2021,
    "authors": [
      "Alex Chohlas-Wood",
      "Madison Coots",
      "Henry Zhu",
      "Emma Brunskill",
      "Sharad Goel"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making affecting different social groups, highlighting downstream impacts on marginalized populations. It discusses equitable resource allocation and stakeholder preferences, which relate to social disparities.",
      "inequality_type": [
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focus on equitable decision-making and downstream consequences",
      "affected_populations": [
        "low-income people",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Contextual bandit algorithms optimizing utility functions",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08630v1",
    "title": "A Fairness Analysis on Private Aggregation of Teacher Ensembles",
    "year": 2021,
    "authors": [
      "Cuong Tran",
      "My H. Dinh",
      "Kyle Beiter",
      "Ferdinando Fioretto"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how a privacy-preserving AI framework impacts fairness and bias among social groups, addressing issues of unequal impacts and discrimination.",
      "inequality_type": [
        "racial",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic fairness and group disparities",
      "affected_populations": [
        "individuals",
        "social groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes algorithmic impacts on group fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08604v2",
    "title": "Enforcing fairness in private federated learning via the modified method of differential multipliers",
    "year": 2021,
    "authors": [
      "Borja Rodríguez-Gálvez",
      "Filip Granqvist",
      "Rogier van Dalen",
      "Matt Seigel"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in AI models, focusing on social groups and bias mitigation, which relates to social inequality concerns.",
      "inequality_type": [
        "social",
        "educational",
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in AI models across social groups",
      "affected_populations": [
        "under-represented groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Experiment",
        "Fairness Enforcement"
      ],
      "methodology_detail": "Extends fairness algorithms to federated learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08253v2",
    "title": "Balancing out Bias: Achieving Fairness Through Balanced Training",
    "year": 2021,
    "authors": [
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in NLP systems affecting demographic groups, aiming for fairness and equal opportunity, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on demographic fairness in AI systems",
      "affected_populations": [
        "minority groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation via balanced training and gated models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08240v2",
    "title": "Strategic Ranking",
    "year": 2021,
    "authors": [
      "Lydia T. Liu",
      "Nikhil Garg",
      "Christian Borgs"
    ],
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how ranking design impacts disparities in access and welfare, addressing inequities related to resource access and societal impact.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on resource access disparities and societal utility",
      "affected_populations": [
        "applicants",
        "students"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Game Theory"
      ],
      "methodology_detail": "Analyzes equilibria and utility trade-offs in ranking systems",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.08131v1",
    "title": "Studying Up Machine Learning Data: Why Talk About Bias When We Mean Power?",
    "year": 2021,
    "authors": [
      "Milagros Miceli",
      "Julian Posada",
      "Tianling Yang"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses power dynamics, historical inequities, and labor conditions in data, which relate to social inequalities. It emphasizes social contexts and systemic forces shaping datasets, extending beyond technical bias. The focus on societal and labor forces indicates engagement with social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "labor",
        "power"
      ],
      "other_detail": "Focus on data labor and systemic power structures",
      "affected_populations": [
        "data workers",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Critical Analysis"
      ],
      "methodology_detail": "Analyzes prior research and theoretical frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.07835v1",
    "title": "Incentives in Two-sided Matching Markets with Prediction-enhanced Preference-formation",
    "year": 2021,
    "authors": [
      "Stefania Ionescu",
      "Yuhao Du",
      "Kenneth Joseph",
      "Anikó Hannák"
    ],
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how strategic attacks in matching markets can increase inequality among student populations, indicating a focus on social disparities.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Impacts on student population inequality",
      "affected_populations": [
        "students",
        "schools"
      ],
      "methodology": [
        "Simulation",
        "Economic modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling and simulation of matching market dynamics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.06974v1",
    "title": "Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies",
    "year": 2021,
    "authors": [
      "Briana Vecchione",
      "Solon Barocas",
      "Karen Levy"
    ],
    "categories": [
      "cs.CY",
      "K.4.0; K.4.1; K.4.2"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social justice, racial equity, and social biases in auditing practices, linking algorithmic audits to social inequalities.",
      "inequality_type": [
        "racial",
        "social justice",
        "discrimination"
      ],
      "other_detail": "Focus on racial equity and social justice in auditing",
      "affected_populations": [
        "minority groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Literature Review",
        "Audit Studies",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Historical analysis of audit studies and social justice frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.06437v1",
    "title": "Uncovering Implicit Gender Bias in Narratives through Commonsense Inference",
    "year": 2021,
    "authors": [
      "Tenghao Huang",
      "Faeze Brahman",
      "Vered Shwartz",
      "Snigdha Chaturvedi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines implicit gender biases in narratives, highlighting gender inequality issues in AI-generated content.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Implicit gender bias in storytelling",
      "affected_populations": [
        "female characters",
        "male characters"
      ],
      "methodology": [
        "Natural Language Processing",
        "Commonsense Reasoning",
        "Analysis"
      ],
      "methodology_detail": "Using commonsense inference to detect biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.05931v1",
    "title": "FaiREO: User Group Fairness for Equality of Opportunity in Course Recommendation",
    "year": 2021,
    "authors": [
      "Agoritsa Polyzou",
      "Maria Kalantzi",
      "George Karypis"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems affecting student opportunities, which relates to social inequalities such as educational and socioeconomic disparities.",
      "inequality_type": [
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in educational opportunity",
      "affected_populations": [
        "students from protected groups"
      ],
      "methodology": [
        "Multi-objective Optimization",
        "Experimental Evaluation"
      ],
      "methodology_detail": "Balancing fairness and recommendation quality",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.07908v2",
    "title": "Auditing the Imputation Effect on Fairness of Predictive Analytics in Higher Education",
    "year": 2021,
    "authors": [
      "Hadis Anahideh",
      "Parian Haghighat",
      "Nazanin Nezami",
      "Denisa G`andara"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in predictive analytics affecting student success, highlighting societal disparities and imputation impacts that relate to social inequality.",
      "inequality_type": [
        "educational",
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focus on fairness in educational predictive modeling",
      "affected_populations": [
        "students",
        "underrepresented groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluates imputation effects on fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.05704v2",
    "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
    "year": 2021,
    "authors": [
      "Jaimeen Ahn",
      "Alice Oh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper studies ethnic bias in language models, addressing social bias and discrimination related to ethnicity, which are aspects of social inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on language-dependent ethnic bias in AI models",
      "affected_populations": [
        "ethnic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and mitigation in language models",
      "geographic_focus": [
        "English",
        "German",
        "Spanish",
        "Korean",
        "Turkish",
        "Chinese",
        "Arabic",
        "Greek"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.05433v1",
    "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search",
    "year": 2021,
    "authors": [
      "Jialu Wang",
      "Yang Liu",
      "Xin Eric Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in image search, a social inequality issue affecting gender fairness and representation in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Debiasing Techniques"
      ],
      "methodology_detail": "Debiasing approaches for image search models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.03858v2",
    "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation",
    "year": 2021,
    "authors": [
      "Shahar Levy",
      "Koren Lazar",
      "Gabriel Stanovsky"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI models, highlighting social discrimination issues related to gender roles and stereotypes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Creating and evaluating a large gender bias dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.03646v1",
    "title": "Sustainable Modular Debiasing of Language Models",
    "year": 2021,
    "authors": [
      "Anne Lauscher",
      "Tobias Lüken",
      "Goran Glavaš"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender and potentially other social groups in language models, which are linked to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in language models",
      "affected_populations": [
        "women",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Debiasing adapters via language modeling training",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.03300v1",
    "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
    "year": 2021,
    "authors": [
      "Eric Michael Smith",
      "Adina Williams"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender and race/ethnicity in AI dialogue models, addressing social discrimination and bias issues. It analyzes how AI systems reflect and potentially reinforce societal inequalities. The focus on demographic biases indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in conversational AI",
      "affected_populations": [
        "women",
        "racial/ethnic minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation techniques in dialogue models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.03229v4",
    "title": "Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets",
    "year": 2021,
    "authors": [
      "Matthew Gwilliam",
      "Srinidhi Hegde",
      "Lade Tinubu",
      "Alex Hanson"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition datasets, highlighting social discrimination issues related to race and fairness in AI systems.",
      "inequality_type": [
        "racial",
        "fairness"
      ],
      "other_detail": "Focuses on dataset bias affecting racial equity in AI",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes dataset composition and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.02691v1",
    "title": "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\"",
    "year": 2021,
    "authors": [
      "Zhixue Zhao",
      "Ziqi Zhang",
      "Frank Hopfgartner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in toxic comment classification related to identity terms, which reflect social discrimination based on race, ethnicity, or other social groups, impacting fairness and equality.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender"
      ],
      "other_detail": "Bias mitigation in social media content moderation",
      "affected_populations": [
        "racial groups",
        "ethnic groups",
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Leveraging subjectivity and identity terms in model design",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.02202v1",
    "title": "Fairness via AI: Bias Reduction in Medical Information",
    "year": 2021,
    "authors": [
      "Shiri Dori-Hacohen",
      "Roberto Montenegro",
      "Fabricio Murai",
      "Scott A. Hale",
      "Keen Sung",
      "Michela Blain",
      "Jennifer Edwards-Johnson"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses health information biases affecting minority groups and proposes AI-based bias reduction, directly engaging with social inequalities related to health and societal disparities.",
      "inequality_type": [
        "health",
        "racial",
        "informational"
      ],
      "other_detail": "Focus on health information bias and societal inequities",
      "affected_populations": [
        "minority groups",
        "patients"
      ],
      "methodology": [
        "AI as solution",
        "Bias detection",
        "Mitigation strategies"
      ],
      "methodology_detail": "Using AI to detect and reduce biased health info",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2109.00435v3",
    "title": "Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning",
    "year": 2021,
    "authors": [
      "Snehalkumar",
      "S. Gaikwad",
      "Shankar Iyer",
      "Dalton Lunga",
      "Yu-Ru Lin"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in humanitarian aid affecting vulnerable populations and discusses algorithmic harms related to fairness, bias, and ethics in data-driven methods.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "geographic",
        "disability"
      ],
      "other_detail": "Focus on vulnerable and marginalized communities",
      "affected_populations": [
        "vulnerable communities",
        "at-risk populations"
      ],
      "methodology": [
        "Data Science",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Developing novel data science methodologies for equitable policy",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.13477v2",
    "title": "Toward an Actionable Socioeconomic-Aware HCI",
    "year": 2021,
    "authors": [
      "Margaret Burnett",
      "Abrar Fallatah",
      "Catherine Hu",
      "Christopher Perdriau",
      "Christopher Mendez",
      "Caroline Gao",
      "Anita Sarma"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on socioeconomic disparities affecting user experiences with technology, addressing socioeconomic inequality directly.",
      "inequality_type": [
        "socioeconomic"
      ],
      "other_detail": "None",
      "affected_populations": [
        "socioeconomically diverse users"
      ],
      "methodology": [
        "Literature Review",
        "System Design"
      ],
      "methodology_detail": "Synthesizes research and develops a structured method",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.10379v1",
    "title": "Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models",
    "year": 2021,
    "authors": [
      "Chloe Ciora",
      "Nur Iren",
      "Malihe Alikhani"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in machine translation, addressing social discrimination related to gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on covert and overt gender bias in language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Case Study"
      ],
      "methodology_detail": "Analyzes gender bias in translation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.10343v1",
    "title": "Gender-based occupational segregation: a bit string approach",
    "year": 2021,
    "authors": [
      "Joana Passinhas",
      "Tanya Araújo"
    ],
    "categories": [
      "cs.SI",
      "cs.MA",
      "nlin.AO",
      "68U99",
      "I.6"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-based occupational segregation, a key aspect of social inequality, and its impact on the gender wage gap, addressing gender discrimination in labor markets.",
      "inequality_type": [
        "gender",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on gender discrimination in occupational choices",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Agent-based modeling",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Models discriminatory behavior in labor market dynamics",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.10265v1",
    "title": "Exploring Biases and Prejudice of Facial Synthesis via Semantic Latent Space",
    "year": 2021,
    "authors": [
      "Xuyang Shen",
      "Jo Plested",
      "Sabrina Caldwell",
      "Tom Gedeon"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in facial synthesis models related to gender, highlighting how dataset composition affects fairness, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI models",
      "affected_populations": [
        "female faces"
      ],
      "methodology": [
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Analyzing bias effects and model adjustments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.10096v1",
    "title": "The Ethical Implications of Digital Contact Tracing for LGBTQIA+ Communities",
    "year": 2021,
    "authors": [
      "Izak van Zyl",
      "Nyx McLean"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses ethical challenges faced by vulnerable communities, specifically LGBTQIA+ persons, in the context of digital contact tracing, highlighting issues of digital rights, freedoms, and social marginalization.",
      "inequality_type": [
        "gender",
        "health",
        "digital",
        "social"
      ],
      "other_detail": "Focus on marginalized LGBTQIA+ community and digital rights",
      "affected_populations": [
        "LGBTQIA+ persons"
      ],
      "methodology": [
        "Ethics Analysis"
      ],
      "methodology_detail": "Critical intersectional feminist approach",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.10089v1",
    "title": "Gender Data 4 Girls?: A Postcolonial Feminist Participatory Study in Bangladesh",
    "year": 2021,
    "authors": [
      "Isobel Talks"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines gender data collection and participatory approaches affecting women in Bangladesh, addressing gender inequality and postcolonial power dynamics.",
      "inequality_type": [
        "gender",
        "informational",
        "geographic"
      ],
      "other_detail": "Focus on gender data and postcolonial feminist critique",
      "affected_populations": [
        "women",
        "girls"
      ],
      "methodology": [
        "Participatory Action Research",
        "Qualitative Study"
      ],
      "methodology_detail": "Collaborative community-based research with young women",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.09959v1",
    "title": "Artificial Intelligence Ethics: An Inclusive Global Discourse?",
    "year": 2021,
    "authors": [
      "Cathy Roche",
      "Dave Lewis",
      "P. J. Wall"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines underrepresentation of Global South countries and women in AI ethics discourse, highlighting issues of marginalization and potential discrimination.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Focus on participation disparities in ethical frameworks",
      "affected_populations": [
        "Global South countries",
        "Women"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing AI ethics documentation for representation gaps",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.09958v1",
    "title": "Earth Observation and the New African Rural Datascapes: Defining an Agenda for Critical Research",
    "year": 2021,
    "authors": [
      "Rose Pritchard",
      "Wilhelm Kiwango",
      "Andy Challinor"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines how EO data impacts marginalized groups and local inequalities in African rural landscapes.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "rural-urban"
      ],
      "other_detail": "Focus on marginalized and rural populations in Africa",
      "affected_populations": [
        "poor rural communities",
        "marginalized peoples"
      ],
      "methodology": [
        "Critical analysis",
        "Theoretical Framework"
      ],
      "methodology_detail": "Drawing on critical data studies and political ecology",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.09947v1",
    "title": "For Better or for Worse? A Framework for Critical Analysis of ICT4D for Women",
    "year": 2021,
    "authors": [
      "Abhipsa Pal",
      "Rahul De'"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines gender-based digital divides and societal inequalities related to ICT diffusion, especially in developing countries.",
      "inequality_type": [
        "gender",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender and digital inequality in ICT4D",
      "affected_populations": [
        "women",
        "girls"
      ],
      "methodology": [
        "Critical Research Framework",
        "Literature Review"
      ],
      "methodology_detail": "Critical analysis of ICT4D initiatives and artifacts",
      "geographic_focus": [
        "developing countries"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.12255v1",
    "title": "Assessing Gender Bias in the Information Systems Field: An Analysis of the Impact on Citations",
    "year": 2021,
    "authors": [
      "Silvia Masiero",
      "Aleksi Aaltonen"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in academic citations within the IS field, addressing gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female IS academics",
        "male IS academics"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Panel regression on citation data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.09788v1",
    "title": "Gendering of Smartphone Ownership and Autonomy among Youth: Narratives from Rural India",
    "year": 2021,
    "authors": [
      "Renza Iqbal"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how gender influences smartphone ownership and autonomy, highlighting disparities rooted in social and cultural contexts, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "geographic",
        "digital"
      ],
      "other_detail": "Focus on youth in rural India",
      "affected_populations": [
        "rural youth",
        "girls",
        "boys"
      ],
      "methodology": [
        "Ethnographic Study",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing narratives and cultural influences",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.09783v1",
    "title": "From Digital Divide to Digital Justice in the Global South: Conceptualising Adverse Digital Incorporation",
    "year": 2021,
    "authors": [
      "Richard Heeks"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses digital inclusion and its impact on inequality in the global South, focusing on how digital systems can perpetuate or exacerbate social disparities, particularly through adverse digital incorporation.",
      "inequality_type": [
        "socioeconomic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on digital inclusion and inequality in the global South",
      "affected_populations": [
        "less-advantaged groups",
        "digital users in the global South"
      ],
      "methodology": [
        "Conceptual Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Develops a conceptual model of adverse digital incorporation",
      "geographic_focus": [
        "Global South"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.09712v1",
    "title": "Resilient ICT4D: Building and Sustaining our Community in Pandemic Times",
    "year": 2021,
    "authors": [
      "Silvia Masiero",
      "Petter Nielsen"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses COVID-19's disproportionate impact on vulnerable groups and deepening inequalities, highlighting socio-economic and marginalization issues within the pandemic context.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "geographic"
      ],
      "other_detail": "Focus on pandemic-related inequalities and marginalized narratives",
      "affected_populations": [
        "vulnerable people",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Analyzes existing research and narratives",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.08800v1",
    "title": "EqGNN: Equalized Node Opportunity in Graphs",
    "year": 2021,
    "authors": [
      "Uriel Singer",
      "Kira Radinsky"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in GNNs concerning sensitive attributes like race and gender, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Optimizing for equalized odds fairness criteria",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.08229v1",
    "title": "Tilted Platforms: Rental Housing Technology and the Rise of Urban Big Data Oligopolies",
    "year": 2021,
    "authors": [
      "Geoff Boeing",
      "Max Besbris",
      "David Wachsmuth",
      "Jake Wegmann"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how rental housing platforms reproduce biases and impact sociospatial inequality, addressing social disparities related to housing, information, and urban inequality.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "geographic",
        "informational"
      ],
      "other_detail": "Focus on urban housing and data biases",
      "affected_populations": [
        "urban residents",
        "housing seekers"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes existing scholarship and platform data biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.10132v2",
    "title": "TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis",
    "year": 2021,
    "authors": [
      "Esha Sarkar",
      "Michail Maniatakos"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting dataset bias related to sensitive groups such as ethnicity, which can lead to social discrimination in genomic analysis.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias detection in genomic datasets",
      "affected_populations": [
        "ethnic groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using backdoor techniques to identify bias",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.06581v1",
    "title": "Unravelling the Effect of Image Distortions for Biased Prediction of Pre-trained Face Recognition Models",
    "year": 2021,
    "authors": [
      "Puspita Majumdar",
      "Surbhi Mittal",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in face recognition models across gender and race groups, highlighting social disparities in AI performance.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on bias in AI systems affecting social groups",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Systematic Analysis"
      ],
      "methodology_detail": "Evaluates model performance under image distortions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.06487v1",
    "title": "Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study",
    "year": 2021,
    "authors": [
      "Ayush Kumar",
      "Pratik Kumar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in toxicity detection models related to identity groups such as religion and gender, highlighting social discrimination issues. It analyzes how AI systems perpetuate or mitigate social biases. The focus on bias against specific social groups indicates addressing social inequality concerns.",
      "inequality_type": [
        "gender",
        "religion"
      ],
      "other_detail": "Bias in AI models related to social identity groups",
      "affected_populations": [
        "gender groups",
        "religious groups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Evaluation of classifiers and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.05523v2",
    "title": "Fair Decision-Making for Food Inspections",
    "year": 2021,
    "authors": [
      "Shubham Singh",
      "Bhuvni Shah",
      "Chris Kanich",
      "Ian A. Kash"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities in food inspection scheduling, highlighting geographic and procedural inequalities affecting populations served.",
      "inequality_type": [
        "geographic",
        "health"
      ],
      "other_detail": "Focus on fairness in public health inspection processes",
      "affected_populations": [
        "restaurant patrons",
        "local communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes fairness and model impacts on populations",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.05412v1",
    "title": "Analyzing Race and Country of Citizenship Bias in Wikidata",
    "year": 2021,
    "authors": [
      "Zaina Shaik",
      "Filip Ilievski",
      "Fred Morstatter"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and national biases in Wikidata, highlighting underrepresentation of minorities and non-European/North American groups, which relates to social inequalities in representation.",
      "inequality_type": [
        "racial",
        "nationality"
      ],
      "other_detail": "Bias in digital knowledge representation",
      "affected_populations": [
        "minority racial groups",
        "non-European/North American citizens"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Comparing Wikidata to real-world datasets",
      "geographic_focus": [
        "Europe",
        "North America"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.05233v2",
    "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks",
    "year": 2021,
    "authors": [
      "Yushun Dong",
      "Ninghao Liu",
      "Brian Jalaian",
      "Jundong Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in AI systems, which relate to social discrimination and inequality, particularly in demographic groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Bias mitigation in attributed networks for fair GNNs",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias metrics and model-agnostic bias mitigation framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.04983v1",
    "title": "Learning Fair Face Representation With Progressive Cross Transformer",
    "year": 2021,
    "authors": [
      "Yong Li",
      "Yufei Sun",
      "Zhen Cui",
      "Shiguang Shan",
      "Jian Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition, a social discrimination issue. It discusses mitigating racial bias in AI systems, directly relating to social inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI face recognition systems",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Signal decomposition and transformer-based decoupling",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.04884v3",
    "title": "Retiring Adult: New Datasets for Fair Machine Learning",
    "year": 2021,
    "authors": [
      "Frances Ding",
      "Moritz Hardt",
      "John Miller",
      "Ludwig Schmidt"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on datasets related to income, employment, health, housing, and other socioeconomic factors, highlighting issues of fairness and bias in machine learning affecting social groups.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic",
        "health",
        "housing"
      ],
      "other_detail": "Focus on fairness in socioeconomic and demographic data",
      "affected_populations": [
        "low-income groups",
        "racial minorities",
        "urban residents"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Developing and analyzing datasets for fairness research",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.03764v1",
    "title": "PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition",
    "year": 2021,
    "authors": [
      "Prithviraj Dhar",
      "Joshua Gleason",
      "Aniket Roy",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in face recognition, focusing on gender and skin tone, which are social attributes linked to inequality. It aims to reduce encoding of sensitive attributes to mitigate bias, impacting social groups. This relates to social discrimination and fairness in AI systems.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in facial recognition systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training"
      ],
      "methodology_detail": "Descriptor-based de-biasing approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.02707v3",
    "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
    "year": 2021,
    "authors": [
      "Harrison Rosenberg",
      "Brian Tang",
      "Kassem Fawaz",
      "Somesh Jha"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities in face recognition obfuscation, highlighting fairness issues related to demographic groups.",
      "inequality_type": [
        "racial",
        "demographic",
        "fairness"
      ],
      "other_detail": "Focuses on demographic fairness in AI face recognition systems",
      "affected_populations": [
        "minority groups",
        "demographic groups"
      ],
      "methodology": [
        "Analytical Model",
        "Empirical Analysis"
      ],
      "methodology_detail": "Analyzes face embedding clustering and obfuscation performance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.02137v1",
    "title": "Under the Radar -- Auditing Fairness in ML for Humanitarian Mapping",
    "year": 2021,
    "authors": [
      "Lukas Kondmann",
      "Xiao Xiang Zhu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI predictions related to marginalized social groups, specifically scheduled castes and tribes, highlighting disparities in poverty and electrification estimates.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in humanitarian mapping affecting marginalized communities",
      "affected_populations": [
        "scheduled castes",
        "scheduled tribes"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Counterfactual fairness measurement using propensity score matching",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.04134v1",
    "title": "Fairness in Algorithmic Profiling: A German Case Study",
    "year": 2021,
    "authors": [
      "Christoph Kern",
      "Ruben L. Bach",
      "Hannah Mautner",
      "Frauke Kreuter"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and discrimination in algorithmic profiling affecting employment support, highlighting social bias concerns.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on employment and unemployment disparities",
      "affected_populations": [
        "job seekers",
        "long-term unemployed"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Auditing",
        "Case Study"
      ],
      "methodology_detail": "Evaluation of predictive models and fairness metrics",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.01764v1",
    "title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management",
    "year": 2021,
    "authors": [
      "Cécile Logé",
      "Emily Ross",
      "David Yaw Amoah Dadey",
      "Saahil Jain",
      "Adriel Saporta",
      "Andrew Y. Ng",
      "Pranav Rajpurkar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases in medical AI systems, highlighting disparities in treatment across race-gender groups, which relates to social inequality and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focuses on bias in medical decision-making",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Assessing bias using QA datasets and statistical tests",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.07699v1",
    "title": "Digital Divide: Mapping the geodemographics of internet accessibility across Great Britain",
    "year": 2021,
    "authors": [
      "Claire Powell",
      "Luke Burns"
    ],
    "categories": [
      "cs.CY",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines digital accessibility disparities linked to sociodemographic factors, impacting disadvantaged groups and highlighting social inequality.",
      "inequality_type": [
        "digital",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on digital access disparities across social groups",
      "affected_populations": [
        "disadvantaged",
        "minority groups",
        "urban poor"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Literature Review",
        "Case Study"
      ],
      "methodology_detail": "Using sociodemographic variables and spatial clustering",
      "geographic_focus": [
        "Great Britain"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.07354v1",
    "title": "Private Delivery Networks -- Extended Abstract",
    "year": 2021,
    "authors": [
      "Alex Berke",
      "Nicolas Lee",
      "Patrick Chwalek"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses privacy and wealth inequality concerns related to e-commerce delivery networks, highlighting impacts on marginalized groups and societal disparities.",
      "inequality_type": [
        "wealth",
        "digital",
        "geographic"
      ],
      "other_detail": "Focuses on privacy and economic disparities in delivery systems",
      "affected_populations": [
        "low-income communities",
        "digitally marginalized groups"
      ],
      "methodology": [
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designing privacy-preserving delivery network models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2108.00295v2",
    "title": "Fair Representation Learning using Interpolation Enabled Disentanglement",
    "year": 2021,
    "authors": [
      "Akshita Jha",
      "Bhanukiran Vinzamuri",
      "Chandan K. Reddy"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI representations, addressing bias and discrimination issues. It aims to improve fairness and accuracy in downstream tasks, including healthcare, which relates to social disparities. The emphasis on fairness and model auditing indicates concern with social inequality impacts.",
      "inequality_type": [
        "health",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "healthcare patients",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Fairness-utility trade-off analysis and adversarial learning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.12049v2",
    "title": "SVEva Fair: A Framework for Evaluating Fairness in Speaker Verification",
    "year": 2021,
    "authors": [
      "Wiebke Toussaint",
      "Aaron Yi Ding"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness in speaker verification, highlighting biases related to demographic attributes such as nationality and gender, which are social categories associated with inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "nationality"
      ],
      "other_detail": "Focus on demographic bias in AI systems",
      "affected_populations": [
        "female speakers",
        "nationalities"
      ],
      "methodology": [
        "Experiment",
        "Fairness Evaluation",
        "Framework Development"
      ],
      "methodology_detail": "Design of fairness evaluation framework and case study",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.11584v1",
    "title": "Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives",
    "year": 2021,
    "authors": [
      "Jonas-Dario Troles",
      "Ute Schmid"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in machine translation, reflecting social gender stereotypes and discrimination. It analyzes how AI systems perpetuate gendered language biases, impacting social perceptions.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in language translation",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Extended challenge set with biased adjectives and verbs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.13073v1",
    "title": "Loss of New Ideas: Potentially Long-lasting Effects of the Pandemic on Scientists",
    "year": 2021,
    "authors": [
      "Jian Gao",
      "Yian Yin",
      "Kyle R. Myers",
      "Karim R. Lakhani",
      "Dashun Wang"
    ],
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses differential impacts of the pandemic on scientists, highlighting disparities based on gender, parental status, and scientific field, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "age",
        "educational"
      ],
      "other_detail": "Disparities among demographic groups of scientists",
      "affected_populations": [
        "female scientists",
        "scientists with children"
      ],
      "methodology": [
        "Survey",
        "Large-scale publication data analysis"
      ],
      "methodology_detail": "Comparative surveys and publication data analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.09211v1",
    "title": "Understanding Gender and Racial Disparities in Image Recognition Models",
    "year": 2021,
    "authors": [
      "Rohan Mahadev",
      "Anindya Chakravarti"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates disparities in image recognition accuracy across demographic groups, focusing on gender and racial biases, thus addressing social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI prediction accuracy across social groups",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates fairness using datasets with demographic attributes",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.08362v1",
    "title": "Probabilistic Verification of Neural Networks Against Group Fairness",
    "year": 2021,
    "authors": [
      "Bing Sun",
      "Jun Sun",
      "Ting Dai",
      "Lijun Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in neural networks, addressing social bias and discrimination issues related to societal groups. It aims to verify and improve fairness, which directly relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focus on group fairness and social bias",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Formal Verification",
        "Markov Chain Learning",
        "Sensitivity Analysis"
      ],
      "methodology_detail": "Guarantees sound fairness verification and analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.07691v1",
    "title": "Intersectional Bias in Causal Language Models",
    "year": 2021,
    "authors": [
      "Liam Magee",
      "Lida Ghahremanlou",
      "Karen Soldatic",
      "Shanthi Robertson"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models related to social categories like gender, religion, and disability, highlighting social bias and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "religion",
        "disability"
      ],
      "other_detail": "Focus on intersectional social biases in AI-generated text",
      "affected_populations": [
        "women",
        "religious groups",
        "disabled individuals"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Bias detection in language model outputs",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.06243v2",
    "title": "Fairness-aware Summarization for Justified Decision-Making",
    "year": 2021,
    "authors": [
      "Moniba Keymanesh",
      "Tanya Berger-Wolf",
      "Micha Elsner",
      "Srinivasan Parthasarathy"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in decision-making models affecting protected groups such as race and gender, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Focus on fairness in AI explanations and justifications",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware summarization and attribution mechanisms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.05987v1",
    "title": "Generating Gender Augmented Data for NLP",
    "year": 2021,
    "authors": [
      "Nishtha Jain",
      "Maja Popovic",
      "Declan Groves",
      "Eva Vanmassenhove"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in NLP, a form of social inequality, by proposing methods to generate gender-balanced data, directly engaging with gender fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language processing",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning"
      ],
      "methodology_detail": "Neural machine translation for gender alternatives",
      "geographic_focus": [
        "Spanish-speaking regions"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.05704v1",
    "title": "How Could Equality and Data Protection Law Shape AI Fairness for People with Disabilities?",
    "year": 2021,
    "authors": [
      "Reuben Binns",
      "Reuben Kirkham"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI fairness for people with disabilities, addressing social discrimination and legal protections related to disability, a recognized social inequality dimension.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Focus on legal and fairness issues for disabled persons",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Legal Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes legal frameworks and conceptual approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.05175v6",
    "title": "Strategyproof Mechanisms For Group-Fair Facility Location Problems",
    "year": 2021,
    "authors": [
      "Houyu Zhou",
      "Minming Li",
      "Hau Chan"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in facility location based on social groupings such as ethnicity and age, aiming to fairly minimize group costs. It discusses intergroup and intragroup fairness, which relate to social disparities. The focus on social groupings indicates a concern with social inequality issues.",
      "inequality_type": [
        "ethnic",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Fairness in social group-based facility location",
      "affected_populations": [
        "ethnic groups",
        "age groups"
      ],
      "methodology": [
        "Algorithm Design",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Designing mechanisms with approximation guarantees",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.04642v10",
    "title": "Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness",
    "year": 2021,
    "authors": [
      "Ben Green"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness and justice, addressing social bias and inequality issues in AI systems, with implications for equitable policy reforms.",
      "inequality_type": [
        "social",
        "racial",
        "gender",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making processes",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes fairness theories and methodologies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.01624v1",
    "title": "Implicit Gender Bias in Computer Science -- A Qualitative Study",
    "year": 2021,
    "authors": [
      "Aurélie Breidenbach",
      "Caroline Mahlow",
      "Andreas Schreiber"
    ],
    "categories": [
      "cs.CY",
      "cs.SE",
      "K.4.2; K.7.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gender bias and barriers in computer science, addressing gender inequality and social discrimination. It focuses on social obstacles affecting women's participation in tech fields.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender bias in tech sector",
      "affected_populations": [
        "women",
        "female students"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Interviews or focus groups likely used",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.03900v1",
    "title": "The Price of Diversity",
    "year": 2021,
    "authors": [
      "Hari Bandi",
      "Dimitris Bertsimas"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses systemic bias related to gender, race, and ethnicity in decision-making processes, aiming to improve diversity and meritocracy. It discusses algorithmic fairness and societal impacts of biased data. The focus on societal groups and bias indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in decision-making processes",
      "affected_populations": [
        "women",
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Optimization",
        "Case Study",
        "Algorithm Development"
      ],
      "methodology_detail": "Label flipping and interpretable classification trees",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2107.00593v3",
    "title": "Disaggregated Interventions to Reduce Inequality",
    "year": 2021,
    "authors": [
      "Lucius E. J. Bynum",
      "Joshua R. Loftus",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on disparities in opportunity and access, aiming to reduce inequality through targeted interventions. It explicitly addresses pre-existing real-world disparities and social categories, indicating a focus on social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "racial"
      ],
      "other_detail": "Disaggregated approach to social disparities",
      "affected_populations": [
        "underserved groups",
        "social minorities"
      ],
      "methodology": [
        "Causal Modeling",
        "Constrained Optimization",
        "Algorithm Development"
      ],
      "methodology_detail": "Integrates social science insights with causal and optimization methods",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.15917v1",
    "title": "Explaining Caste-based Digital Divide in India",
    "year": 2021,
    "authors": [
      "R Vaidehi",
      "A Bheemeshwar Reddy",
      "Sudatta Banerjee"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes caste-based digital divides rooted in socioeconomic disparities, addressing social inequality in India.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "digital"
      ],
      "other_detail": "Focuses on caste-related socioeconomic deprivation",
      "affected_populations": [
        "disadvantaged caste groups",
        "others"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Survey",
        "Non-linear decomposition"
      ],
      "methodology_detail": "Uses survey data and decomposition methods",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.15767v1",
    "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes",
    "year": 2021,
    "authors": [
      "Xian Li"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making related to protected classes such as race and social status, which are key social inequality factors.",
      "inequality_type": [
        "racial",
        "social",
        "geographic"
      ],
      "other_detail": "Focus on protected classes and fairness in AI decisions",
      "affected_populations": [
        "racial groups",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Hierarchical random forest model evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.15103v1",
    "title": "Sexism in the Judiciary",
    "year": 2021,
    "authors": [
      "Noa Baker Gillis"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in the judiciary, a social inequality issue, using NLP methods. It addresses gender discrimination and bias detection in legal texts, impacting social fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops automated bias detection methods",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.14829v1",
    "title": "Dataset Bias Mitigation Through Analysis of CNN Training Scores",
    "year": 2021,
    "authors": [
      "Ekberjan Derman"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses dataset bias related to gender groups, impacting fairness in AI classification. It focuses on mitigating categorical bias, which is a social inequality issue. The methodology aims to improve fairness across social groups in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in dataset affecting gender classification",
      "affected_populations": [
        "male",
        "female"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias mitigation in CNN training datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.14365v1",
    "title": "Integrating topic modeling and word embedding to characterize violent deaths",
    "year": 2021,
    "authors": [
      "Alina Arseniev-Koehler",
      "Susan D. Cochran",
      "Vickie M. Mays",
      "Kai-Wei Chang",
      "Jacob Gates Foster"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in violent death narratives, highlighting gendered disparities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in violent death reporting",
      "affected_populations": [
        "female victims",
        "male victims"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Topic modeling and word embedding integration",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.14072v1",
    "title": "Detecting race and gender bias in visual representation of AI on web search engines",
    "year": 2021,
    "authors": [
      "Mykola Makhortykh",
      "Aleksandra Urman",
      "Roberto Ulloa"
    ],
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial and gender bias in AI representations, highlighting social discrimination issues. It analyzes how search engine outputs reflect societal biases, affecting perceptions of different social groups. This directly relates to social inequality and bias in technology.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI visual representation",
      "affected_populations": [
        "non-white groups",
        "women",
        "men"
      ],
      "methodology": [
        "Mixed-Method",
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes search results and visual bias patterns",
      "geographic_focus": [
        "Western",
        "Non-Western"
      ],
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.13456v2",
    "title": "Interpreting Criminal Charge Prediction and Its Algorithmic Bias via Quantum-Inspired Complex Valued Networks",
    "year": 2021,
    "authors": [
      "Abdul Rafae Khan",
      "Jia Xu",
      "Peter Varsanyi",
      "Rachit Pabreja"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in criminal justice decisions, addressing racial and age disparities, and analyzes the impact of social identifiers on predictive fairness.",
      "inequality_type": [
        "racial",
        "age",
        "social bias"
      ],
      "other_detail": "Focuses on bias in criminal justice AI systems",
      "affected_populations": [
        "racial minorities",
        "older individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Uses complex-valued networks inspired by quantum physics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.13455v2",
    "title": "Fairness Deconstructed: A Sociotechnical View of 'Fair' Algorithms in Criminal Justice",
    "year": 2021,
    "authors": [
      "Rajiv Movva"
    ],
    "categories": [
      "cs.CY",
      "K.4.2; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial biases in criminal justice algorithms and social context impacts, addressing social discrimination and inequality. It critiques fairness in AI systems and their societal implications.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on criminal justice and racial bias",
      "affected_populations": [
        "racial minorities",
        "criminal justice populations"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Sociotechnical perspective and critique of fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.13219v1",
    "title": "Towards Understanding and Mitigating Social Biases in Language Models",
    "year": 2021,
    "authors": [
      "Paul Pu Liang",
      "Chiyu Wu",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in language models, which relate to social discrimination and stereotypes affecting groups based on gender, race, and other social constructs.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on biases in AI systems affecting social groups",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Benchmark Development",
        "Human Evaluation"
      ],
      "methodology_detail": "Bias measurement and mitigation techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.12387v2",
    "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation",
    "year": 2021,
    "authors": [
      "Esther Puyol-Anton",
      "Bram Ruijsink",
      "Stefan K. Piechnik",
      "Stefan Neubauer",
      "Steffen E. Petersen",
      "Reza Razavi",
      "Andrew P. King"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias and fairness in AI-based cardiac MRI segmentation, addressing social discrimination related to race. It analyzes how data imbalance affects different racial groups, highlighting social inequality issues in healthcare AI.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "racial groups",
        "patients"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation strategies in AI segmentation models",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.12182v2",
    "title": "Fairness for Image Generation with Uncertain Sensitive Attributes",
    "year": 2021,
    "authors": [
      "Ajil Jalal",
      "Sushrut Karmalkar",
      "Jessica Hoffmann",
      "Alexandros G. Dimakis",
      "Eric Price"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI image generation, addressing issues related to group fairness, which are inherently tied to social categories like race and ethnicity.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focus on group fairness and identity ambiguity",
      "affected_populations": [
        "Asian groups",
        "racial minorities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes fairness definitions and validates with experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.12576v2",
    "title": "DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?",
    "year": 2021,
    "authors": [
      "Archit Uniyal",
      "Rakshit Naidu",
      "Sasikanth Kotti",
      "Sahib Singh",
      "Patrik Joslin Kenfack",
      "Fatemehsadat Mireshghallah",
      "Andrew Trask"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines differential impacts of privacy-preserving AI on sub-populations, highlighting disparities in model utility for under-represented groups, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in AI privacy mechanisms",
      "affected_populations": [
        "minorities",
        "under-represented groups"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Comparative analysis of privacy mechanisms' fairness impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.11410v2",
    "title": "A Survey of Race, Racism, and Anti-Racism in NLP",
    "year": 2021,
    "authors": [
      "Anjalie Field",
      "Su Lin Blodgett",
      "Zeerak Waseem",
      "Yulia Tsvetkov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines race-related bias and racial justice issues in NLP, highlighting social discrimination and bias concerns.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on race and racial bias in NLP",
      "affected_populations": [
        "marginalized racial groups"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "Survey of 79 papers on race in NLP",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.10407v2",
    "title": "When Efficiency meets Equity in Congestion Pricing and Revenue Refunding Schemes",
    "year": 2021,
    "authors": [
      "Devansh Jalota",
      "Kiril Solovey",
      "Karthik Gopalakrishnan",
      "Stephen Zoepf",
      "Hamsa Balakrishnan",
      "Marco Pavone"
    ],
    "categories": [
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses wealth inequality and socioeconomic disparities through congestion pricing and refunding schemes, aiming to reduce wealth inequality and improve equity.",
      "inequality_type": [
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focus on wealth distribution and fairness in transportation",
      "affected_populations": [
        "low-income users",
        "general commuters"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Design and characterization of schemes and equilibrium analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.08680v1",
    "title": "Evaluating Gender Bias in Hindi-English Machine Translation",
    "year": 2021,
    "authors": [
      "Gauri Gupta",
      "Krithika Ramesh",
      "Sanjay Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in machine translation, addressing social bias and fairness issues related to gender in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language technology",
      "affected_populations": [
        "Hindi speakers",
        "English speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and comparison across metrics",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.08503v2",
    "title": "Understanding and Evaluating Racial Biases in Image Captioning",
    "year": 2021,
    "authors": [
      "Dora Zhao",
      "Angelina Wang",
      "Olga Russakovsky"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in image captioning, focusing on disparities related to skin color and race, which are social inequalities. It analyzes how AI systems propagate these biases, impacting different social groups.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias propagation in AI systems",
      "affected_populations": [
        "people with darker skin",
        "racial minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Annotations and bias comparison in captions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.07112v2",
    "title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations",
    "year": 2021,
    "authors": [
      "Clarice Wang",
      "Kathryn Wang",
      "Andrew Bian",
      "Rashidul Islam",
      "Kamrun Naher Keya",
      "James Foulds",
      "Shimei Pan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI recommendations and its social acceptance, highlighting gender disparities and human perceptions, which directly relate to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias and social acceptance",
      "affected_populations": [
        "women",
        "career seekers"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Debiasing techniques and user study evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.06683v2",
    "title": "Assessing Multilingual Fairness in Pre-trained Multimodal Representations",
    "year": 2021,
    "authors": [
      "Jialu Wang",
      "Yang Liu",
      "Xin Eric Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases and fairness across languages, including race, gender, and age, indicating a focus on social discrimination and inequality issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Biases conditioned on demographic groups in images",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing fairness metrics across languages and demographics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.06007v1",
    "title": "Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG by Synthetic Augmentation",
    "year": 2021,
    "authors": [
      "Yunhao Ba",
      "Zhen Wang",
      "Kerim Doruk Karinca",
      "Oyku Deniz Bozkurt",
      "Achuta Kadambi"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in data representation for dark-skinned subjects in AI-based physiological measurement, aiming to reduce bias and improve fairness.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial bias in medical AI datasets",
      "affected_populations": [
        "dark-skinned individuals"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Synthetic Augmentation"
      ],
      "methodology_detail": "Synthetic skin tone translation to balance datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.03761v4",
    "title": "FairCal: Fairness Calibration for Face Verification",
    "year": 2021,
    "authors": [
      "Tiago Salvador",
      "Stephanie Cairns",
      "Vikram Voleti",
      "Noah Marshall",
      "Adam Oberman"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face recognition models related to ethnicity, impacting minority groups and law enforcement, which relates to racial and social fairness issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social"
      ],
      "other_detail": "Bias reduction in AI systems affecting social groups",
      "affected_populations": [
        "minority groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Bias measurement and fairness calibration techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.03521v1",
    "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
    "year": 2021,
    "authors": [
      "Soumya Barikeri",
      "Anne Lauscher",
      "Ivan Vulić",
      "Goran Glavaš"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender, race, religion, and queerness in conversational AI, reflecting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "religion",
        "queerness"
      ],
      "other_detail": "Bias mitigation in conversational AI systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "religious groups",
        "LGBTQ+"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias measurement and mitigation evaluation framework",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.02866v2",
    "title": "Conditional Contrastive Learning for Improving Fairness in Self-Supervised Learning",
    "year": 2021,
    "authors": [
      "Martin Q. Ma",
      "Yao-Hung Hubert Tsai",
      "Paul Pu Liang",
      "Han Zhao",
      "Kun Zhang",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI representations related to sensitive attributes like gender and race, which are central to social inequalities. It focuses on reducing bias and unfairness in AI models, directly engaging with social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "fairness"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Conditional contrastive learning approach for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.02702v2",
    "title": "Subgroup Fairness in Two-Sided Markets",
    "year": 2021,
    "authors": [
      "Quan Zhou",
      "Jakub Marecek",
      "Robert N. Shorten"
    ],
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues related to gender disparities in two-sided markets, exemplified by Uber driver earnings. It introduces subgroup fairness concepts aimed at reducing inequalities among social groups. The focus on pay disparities and fairness mechanisms directly relates to social inequality concerns.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in economic transactions within markets",
      "affected_populations": [
        "female workers",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Market mechanism design and optimization techniques",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.02674v2",
    "title": "Differentially Empirical Risk Minimization under the Fairness Lens",
    "year": 2021,
    "authors": [
      "Cuong Tran",
      "My H. Dinh",
      "Ferdinando Fioretto"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in AI systems affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness disparities in private machine learning",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes data and model properties causing disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.02553v2",
    "title": "Fair Exploration via Axiomatic Bargaining",
    "year": 2021,
    "authors": [
      "Jackie Baek",
      "Vivek F. Farias"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how the costs of exploration in AI are distributed across social groups, focusing on fairness related to race and age in healthcare settings, which directly pertains to social inequalities.",
      "inequality_type": [
        "racial",
        "age",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI decision-making across social groups",
      "affected_populations": [
        "patients",
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Algorithm Design",
        "Case Study"
      ],
      "methodology_detail": "Using axiomatic bargaining and contextual bandits",
      "geographic_focus": null,
      "ai_relationship": "AI as fairness mechanism",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.02216v1",
    "title": "Fairness-Aware Unsupervised Feature Selection",
    "year": 2021,
    "authors": [
      "Xiaoying Xing",
      "Hongfu Liu",
      "Chen Chen",
      "Jundong Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on discrimination related to protected attributes such as gender and race, which are central to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Fairness in AI feature selection",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fairness evaluation and feature selection experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.02183v1",
    "title": "Towards Equal Gender Representation in the Annotations of Toxic Language Detection",
    "year": 2021,
    "authors": [
      "Elizabeth Excell",
      "Noura Al Moubayed"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in toxic language annotations, highlighting how data and model biases reinforce gender disparities. It addresses gender-related fairness issues in AI systems. The focus on gender bias in annotations directly relates to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI annotation processes",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzing annotation data and model bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.04757v2",
    "title": "Fair Machine Learning under Limited Demographically Labeled Data",
    "year": 2021,
    "authors": [
      "Mustafa Safa Ozdayi",
      "Murat Kantarcioglu",
      "Rishabh Iyer"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on biases related to demographic attributes such as gender and race, which are directly linked to social inequalities. It discusses bias mitigation methods in machine learning models that impact social groups. The focus on demographic fairness indicates a concern with social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI models with limited demographic data",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Experimental Analysis"
      ],
      "methodology_detail": "Fair training algorithms with limited demographic labels",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.01601v1",
    "title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",
    "year": 2021,
    "authors": [
      "Jiao Sun",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in Wikipedia, highlighting social stereotypes and disparities between men and women in event representation.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in information representation",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Event detection and bias quantification",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.06054v5",
    "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline",
    "year": 2021,
    "authors": [
      "Sumon Biswas",
      "Hridesh Rajan"
    ],
    "categories": [
      "cs.LG",
      "D.2.0; I.2.5"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning, focusing on discrimination related to race, gender, and other social groups, and analyzes how data preprocessing impacts social bias.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness and discrimination in AI systems",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Causal Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness measurement and pipeline evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.00772v2",
    "title": "Information Theoretic Measures for Fairness-aware Feature Selection",
    "year": 2021,
    "authors": [
      "Sajad Khodadadian",
      "Mohamed Nafea",
      "AmirEmad Ghassami",
      "Negar Kiyavash"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IT",
      "math.IT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and discrimination in decision-making, related to social groups such as race and gender, through fairness-aware feature selection in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "discrimination"
      ],
      "other_detail": "Focuses on bias mitigation in AI decision processes",
      "affected_populations": [
        "unprivileged groups",
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Information Theoretic Measures",
        "Quantitative Analysis",
        "Game Theory"
      ],
      "methodology_detail": "Uses Shapley values and information theory for bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.00720v3",
    "title": "Fair-Net: A Network Architecture For Reducing Performance Disparity Between Identifiable Sub-Populations",
    "year": 2021,
    "authors": [
      "Arghya Datta",
      "S. Joshua Swamidass"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in classifier performance across gender and racial sub-populations, highlighting fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on algorithmic fairness and performance disparity reduction",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Neural network architecture for fairness improvement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.00181v1",
    "title": "Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives",
    "year": 2021,
    "authors": [
      "Meichun Jiao",
      "Ziyang Luo"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in Chinese word embeddings, addressing gender inequality and social bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on language-based gender bias in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing gender bias through word embedding analysis",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.00169v1",
    "title": "Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation",
    "year": 2021,
    "authors": [
      "Adithya Renduchintala",
      "Denise Diaz",
      "Kenneth Heafield",
      "Xian Li",
      "Mona Diab"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in neural machine translation, focusing on how speed optimizations affect gendered noun translation accuracy, directly addressing gender bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias amplification in AI translation models",
      "affected_populations": [
        "gendered noun speakers"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops a gender bias test set, SimpleGEN",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.14890v1",
    "title": "Rawlsian Fair Adaptation of Deep Learning Classifiers",
    "year": 2021,
    "authors": [
      "Kulin Shah",
      "Pooja Gupta",
      "Amit Deshpande",
      "Chiranjib Bhattacharyya"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in classification across sensitive groups, addressing social discrimination and bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness across social groups in AI",
      "affected_populations": [
        "race",
        "gender",
        "socioeconomic groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Mathematical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness adaptation without retraining models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.14874v2",
    "title": "BiasRV: Uncovering Biased Sentiment Predictions at Runtime",
    "year": 2021,
    "authors": [
      "Zhou Yang",
      "Muhammad Hilmi Asyrofi",
      "David Lo"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in sentiment analysis, focusing on gender bias, which relates to social discrimination and inequality. It aims to detect and mitigate biased predictions that can unfairly impact social groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias detection in AI systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and fairness evaluation in NLP systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.14172v2",
    "title": "A Stochastic Alternating Balance $k$-Means Algorithm for Fair Clustering",
    "year": 2021,
    "authors": [
      "Suyun Liu",
      "Luis Nunes Vicente"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in clustering, aiming to reduce discrimination across demographic groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness in human-centric decision systems",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designs a novel stochastic fair clustering algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.14146v1",
    "title": "Deep Fair Discriminative Clustering",
    "year": 2021,
    "authors": [
      "Hongjing Zhang",
      "Ian Davidson"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in clustering related to protected status variables, which are linked to social groups such as race, gender, and other social identities, aiming to reduce algorithmic bias and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on algorithmic fairness and social group discrimination",
      "affected_populations": [
        "social groups",
        "protected communities"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Development",
        "Fairness Optimization"
      ],
      "methodology_detail": "Incorporates fairness constraints into deep clustering algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.13782v1",
    "title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation",
    "year": 2021,
    "authors": [
      "Marco Gaido",
      "Beatrice Savoldi",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in speech translation, addressing social gender inequality and algorithmic fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI translation systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Comparison"
      ],
      "methodology_detail": "Analyzes segmentation strategies' impact on gender translation",
      "geographic_focus": [
        "English-Italian",
        "English-French"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.13774v1",
    "title": "Mapping urban socioeconomic inequalities in developing countries through Facebook advertising data",
    "year": 2021,
    "authors": [
      "Serena Giurgola",
      "Simone Piaggesi",
      "Márton Karsai",
      "Yelena Mejova",
      "André Panisson",
      "Michele Tizzoni"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on socioeconomic inequalities within urban areas using digital data, addressing economic and social disparities. It aims to measure and map socioeconomic conditions, which are core aspects of social inequality.",
      "inequality_type": [
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Uses digital traces to assess urban socioeconomic disparities",
      "affected_populations": [
        "urban residents"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predicts socioeconomic status from Facebook advertising data",
      "geographic_focus": [
        "United States",
        "Colombia",
        "Chile",
        "Morocco"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.13515v1",
    "title": "Vaccine Credential Technology Principles",
    "year": 2021,
    "authors": [
      "Divya Siddarth",
      "Vi Hart",
      "Bethan Cantrell",
      "Kristina Yasuda",
      "Josh Mandel",
      "Karen Easterbrook"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses concerns about inequities, discrimination, and exclusion related to vaccine credentialing, highlighting impacts on marginalized groups and the importance of ethical considerations.",
      "inequality_type": [
        "health",
        "educational",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Impacts on marginalized and vulnerable populations",
      "affected_populations": [
        "marginalized groups",
        "vaccine access disparities"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Evaluates ethical and technological considerations",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.12856v2",
    "title": "Beyond Algorithmic Bias: A Socio-Computational Interrogation of the Google Search by Image Algorithm",
    "year": 2021,
    "authors": [
      "Orestis Papakyriakopoulos",
      "Arwa Michelle Mboya"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in an AI system, linking results to societal structures and attitudes, thus addressing social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI reflecting societal power dynamics",
      "affected_populations": [
        "non-white individuals",
        "women"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analysis of image labels and socio-cultural context",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.12754v1",
    "title": "Computer Vision and Conflicting Values: Describing People with Automated Alt Text",
    "year": 2021,
    "authors": [
      "Margot Hanley",
      "Solon Barocas",
      "Karen Levy",
      "Shiri Azenkot",
      "Helen Nissenbaum"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines issues related to identity categories, bias, and fairness in automated descriptions, impacting marginalized groups such as those with disabilities, race, and gender. It discusses ethical dilemmas and normative concerns in AI deployment affecting social groups.",
      "inequality_type": [
        "disability",
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Focus on ethical and social implications of AI systems",
      "affected_populations": [
        "blind and low vision people",
        "racial minorities",
        "gender groups"
      ],
      "methodology": [
        "Case Study",
        "Ethics Analysis",
        "Policy Analysis"
      ],
      "methodology_detail": "Analyzes policies and practices in AI and museums",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.12195v3",
    "title": "Bias in Machine Learning Software: Why? How? What to do?",
    "year": 2021,
    "authors": [
      "Joymallya Chakraborty",
      "Suvodeep Majumder",
      "Tim Menzies"
    ],
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias in AI affecting social groups based on sex, race, age, and other attributes, indicating a focus on social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI systems affecting social groups",
      "affected_populations": [
        "racial minorities",
        "women",
        "elderly",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation techniques and fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.10990v1",
    "title": "Wisdom for the Crowd: Discoursive Power in Annotation Instructions for Computer Vision",
    "year": 2021,
    "authors": [
      "Milagros Miceli",
      "Julian Posada"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how annotation instructions reflect and reinforce social inequalities, particularly in terms of labor commodification, power asymmetries, and social biases embedded in datasets and AI systems.",
      "inequality_type": [
        "economic",
        "class",
        "geographic",
        "social"
      ],
      "other_detail": "Reinforces social and economic inequalities through dataset reproduction",
      "affected_populations": [
        "annotation workers",
        "global South populations"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes discourses in annotation instructions",
      "geographic_focus": [
        "Argentina",
        "Venezuela"
      ],
      "ai_relationship": "AI as perpetuator of social inequality",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.09059v1",
    "title": "The State of AI Ethics Report (January 2021)",
    "year": 2021,
    "authors": [
      "Abhishek Gupta",
      "Alexandrine Royer",
      "Connor Wright",
      "Falaah Arif Khan",
      "Victoria Heath",
      "Erick Galinkin",
      "Ryan Khurana",
      "Marianna Bergamaschi Ganapini",
      "Muriam Fancy",
      "Masa Sweidan",
      "Mo Akif",
      "Renjie Butalid"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The report discusses systemic silencing and erasure of Black women's contributions, highlighting racial and gender inequalities. It also addresses issues related to AI ethics, bias, and discrimination, which are linked to social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on Black women's contributions and systemic silencing",
      "affected_populations": [
        "Black women",
        "minority groups"
      ],
      "methodology": [
        "Literature Review",
        "Expert Commentary"
      ],
      "methodology_detail": "Synthesizes research and expert insights on AI ethics",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.08847v2",
    "title": "Beyond \"Fairness:\" Structural (In)justice Lenses on AI for Education",
    "year": 2021,
    "authors": [
      "Michael Madaio",
      "Su Lin Blodgett",
      "Elijah Mayfield",
      "Ezekiel Dixon-Román"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "K.3; K.4; I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines how educational AI reproduces systemic injustices and inequities, drawing on critical theory and Black feminist scholarship to address social disparities beyond mere performance fairness.",
      "inequality_type": [
        "racial",
        "educational",
        "social",
        "inequity"
      ],
      "other_detail": "Focus on systemic injustice in educational AI systems",
      "affected_populations": [
        "marginalized communities",
        "students of color"
      ],
      "methodology": [
        "Critical Theory",
        "Literature Review"
      ],
      "methodology_detail": "Theoretical critique informed by critical and feminist scholarship",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of systemic injustice",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.08667v2",
    "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency",
    "year": 2021,
    "authors": [
      "Kyra Yee",
      "Uthaipon Tantipongpipat",
      "Shubhanshu Mishra"
    ],
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in AI-driven image cropping related to skin tone and gender, highlighting issues of fairness and representation affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "representation"
      ],
      "other_detail": "Focuses on visual representation and social bias in AI",
      "affected_populations": [
        "dark-skinned individuals",
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Includes fairness metrics and human-centered design",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.08493v2",
    "title": "Identifying Undercompensated Groups Defined By Multiple Attributes in Risk Adjustment",
    "year": 2021,
    "authors": [
      "Anna Zink",
      "Sherri Rose"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in health care payments affecting vulnerable groups, highlighting issues of discrimination and fairness in risk adjustment formulas.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on health coverage and insurer incentives",
      "affected_populations": [
        "patients with multiple chronic conditions",
        "groups without chronic conditions"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops a machine learning method for group importance",
      "geographic_focus": [
        "U.S."
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.07513v2",
    "title": "Decision Making with Differential Privacy under a Fairness Lens",
    "year": 2021,
    "authors": [
      "Ferdinando Fioretto",
      "Cuong Tran",
      "Pascal Van Hentenryck"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how differential privacy impacts resource allocation across social groups, highlighting disproportionate effects on certain populations, which relates to social inequality and fairness issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on fairness in privacy-preserving data release",
      "affected_populations": [
        "social groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes impacts on decision tasks using census data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.07244v1",
    "title": "Fairly Private Through Group Tagging and Relation Impact",
    "year": 2021,
    "authors": [
      "Poushali Sengupta",
      "Subhankar Mishra"
    ],
    "categories": [
      "cs.CR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems, focusing on protected groups like gender. It aims to improve fairness in decision-making processes, which relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "fairness"
      ],
      "other_detail": "Focus on group fairness in classification systems",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Fairness in AI",
        "Case Study"
      ],
      "methodology_detail": "Gender equality case study in admission system",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.07168v1",
    "title": "Cohort Shapley value for algorithmic fairness",
    "year": 2021,
    "authors": [
      "Masayoshi Mase",
      "Art B. Owen",
      "Benjamin B. Seiler"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "econ.EM",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates algorithmic fairness related to protected attributes like race, addressing social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in predictive algorithms",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Game Theory",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses Cohort Shapley value for variable importance",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.06604v1",
    "title": "Towards Equity and Algorithmic Fairness in Student Grade Prediction",
    "year": 2021,
    "authors": [
      "Weijie Jiang",
      "Zachary A. Pardos"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly addresses racial fairness and educational equity, analyzing disparities in AI grade prediction across social groups.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focus on fairness in educational outcomes",
      "affected_populations": [
        "students",
        "underserved groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Strategies",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Strategies for balancing fairness and performance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.06442v1",
    "title": "An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings",
    "year": 2021,
    "authors": [
      "Hemank Lamba",
      "Kit T. Rodolfa",
      "Rayid Ghani"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness methods in high-stakes policy settings, addressing social bias and disparities in outcomes. It focuses on fairness in machine learning applications impacting societal groups.",
      "inequality_type": [
        "racial",
        "educational",
        "health",
        "social"
      ],
      "other_detail": "Focus on fairness in policy-related ML applications",
      "affected_populations": [
        "social groups",
        "policy beneficiaries"
      ],
      "methodology": [
        "Machine Learning",
        "Empirical Analysis"
      ],
      "methodology_detail": "Comparative evaluation of bias reduction methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.05541v1",
    "title": "Evaluating Gender Bias in Natural Language Inference",
    "year": 2021,
    "authors": [
      "Shanya Sharma",
      "Manan Dey",
      "Koustuv Sinha"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in NLP models, addressing gender inequality and social bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender stereotypes in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias detection via challenge task with gendered premises",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.04534v1",
    "title": "Improving Fairness of AI Systems with Lossless De-biasing",
    "year": 2021,
    "authors": [
      "Yan Zhou",
      "Murat Kantarcioglu",
      "Chris Clifton"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in AI affecting underrepresented groups, which relates to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "underrepresented groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Oversampling and fairness metrics evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.04054v3",
    "title": "Societal Biases in Language Generation: Progress and Challenges",
    "year": 2021,
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Premkumar Natarajan",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses societal biases in language generation, focusing on how data and techniques contribute to biases affecting marginalized groups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on societal biases and fairness in AI",
      "affected_populations": [
        "marginalized groups",
        "minorities"
      ],
      "methodology": [
        "Literature Review",
        "Experiment"
      ],
      "methodology_detail": "Quantifying bias effects in decoding techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.03632v2",
    "title": "CASIA-Face-Africa: A Large-scale African Face Image Database",
    "year": 2021,
    "authors": [
      "Jawad Muhammad",
      "Yunlong Wang",
      "Caiyong Wang",
      "Kunbo Zhang",
      "Zhenan Sun"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition systems and provides a large African face database to study this issue, directly engaging with racial inequality and bias in AI.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial bias in biometric AI systems",
      "affected_populations": [
        "African subjects"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Collecting and annotating face images for bias analysis",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.02778v1",
    "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification",
    "year": 2021,
    "authors": [
      "Haochen Liu",
      "Wei Jin",
      "Hamid Karimi",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI models related to demographic groups, impacting fairness and social equity.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in AI affecting social group fairness",
      "affected_populations": [
        "demographic groups",
        "authors' groups"
      ],
      "methodology": [
        "Deep Learning",
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation in text classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.02091v2",
    "title": "When Fair Ranking Meets Uncertain Inference",
    "year": 2021,
    "authors": [
      "Avijit Ghosh",
      "Ritam Dutt",
      "Christo Wilson"
    ],
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in ranking systems, focusing on demographic inference and its impact on fairness, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic ranking systems",
      "affected_populations": [
        "job applicants",
        "credit seekers"
      ],
      "methodology": [
        "Simulation",
        "Case Study",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using real datasets and simulations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.01774v2",
    "title": "Envisioning Communities: A Participatory Approach Towards AI for Social Good",
    "year": 2021,
    "authors": [
      "Elizabeth Bondi",
      "Lily Xu",
      "Diana Acosta-Navas",
      "Jackson A. Killian"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social justice, marginalized communities, and equitable welfare, addressing social inequalities related to race, class, and marginalized groups through participatory AI approaches.",
      "inequality_type": [
        "racial",
        "class",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on community-driven social good and justice",
      "affected_populations": [
        "marginalized communities",
        "affected groups"
      ],
      "methodology": [
        "Participatory Approach",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Using capabilities approach and community engagement framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier and tool for social good",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.01441v2",
    "title": "Distributive Justice and Fairness Metrics in Automated Decision-making: How Much Overlap Is There?",
    "year": 2021,
    "authors": [
      "Matthias Kuppler",
      "Christoph Kern",
      "Ruben L. Bach",
      "Frauke Kreuter"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination and fairness in resource allocation, addressing social groups and justice concerns.",
      "inequality_type": [
        "socioeconomic",
        "disability",
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on distributive justice in algorithmic decision-making",
      "affected_populations": [
        "vulnerable groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes fairness metrics and justice theories",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.01031v1",
    "title": "Algorithms are not neutral: Bias in collaborative filtering",
    "year": 2021,
    "authors": [
      "Catherine Stinson"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in algorithms that disproportionately affect marginalized groups, highlighting issues like marginalization and discrimination, which are central to social inequality.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Biases impact marginalized and vulnerable populations",
      "affected_populations": [
        "marginalized people",
        "users of algorithms"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes biases in collaborative filtering algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.00908v3",
    "title": "Impact of Gender Debiased Word Embeddings in Language Modeling",
    "year": 2021,
    "authors": [
      "Christine Basta",
      "Marta R. Costa-jussà"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender and data imbalance in language models, which are social fairness issues affecting marginalized groups.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on bias mitigation in AI models",
      "affected_populations": [
        "women",
        "social minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Analyzes bias in language models with debiased embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.00558v1",
    "title": "An Examination of Fairness of AI Models for Deepfake Detection",
    "year": 2021,
    "authors": [
      "Loc Trinh",
      "Yan Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models related to race and gender, highlighting disparities and discrimination across social groups. It discusses how datasets and model training can reinforce social biases. The focus on racial and gender disparities indicates a direct engagement with social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI detection of deepfakes",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Analyzing datasets and model performance disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2105.07844v1",
    "title": "Does \"AI\" stand for augmenting inequality in the era of covid-19 healthcare?",
    "year": 2021,
    "authors": [
      "David Leslie",
      "Anjali Mazumder",
      "Aidan Peppin",
      "Maria Wolters",
      "Alexa Hagerty"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how AI can reinforce social disparities related to race, socioeconomic status, and health, especially during COVID-19. It examines biases in AI systems that impact vulnerable groups, highlighting social discrimination issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on health disparities and social bias in AI",
      "affected_populations": [
        "marginalized groups",
        "minoritized ethnic groups",
        "older populations",
        "low socioeconomic status"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing biases and social impacts of AI systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.14536v1",
    "title": "State-level Racially Motivated Hate Crimes Contrast Public Opinion on the #StopAsianHate and #StopAAPIHate Movement",
    "year": 2021,
    "authors": [
      "Hanjia Lyu",
      "Yangxin Fan",
      "Ziyu Xiong",
      "Mayya Komisarchik",
      "Jiebo Luo"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines racial bias, hate crimes, and public opinion, addressing racial inequality and social discrimination.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial bias and hate crimes",
      "affected_populations": [
        "Asian Americans",
        "Pacific Islanders",
        "Black communities"
      ],
      "methodology": [
        "Social Media Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Twitter data analysis of public opinion",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.14492v1",
    "title": "Questioning causality on sex, gender and COVID-19, and identifying bias in large-scale data-driven analyses: the Bias Priority Recommendations and Bias Catalog for Pandemics",
    "year": 2021,
    "authors": [
      "Natalia Díaz-Rodríguez",
      "Rūta Binkytė-Sadauskienė",
      "Wafae Bakkali",
      "Sannidhi Bookseller",
      "Paola Tubaro",
      "Andrius Bacevicius",
      "Raja Chatila"
    ],
    "categories": [
      "cs.CY",
      "cs.IR",
      "K.4.1; K.4.2"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases, discrimination, and fairness issues related to gender and COVID-19, highlighting social implications of data analysis and policy impacts.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on bias and discrimination in pandemic data analysis",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis",
        "Bias Catalog Development"
      ],
      "methodology_detail": "Analyzes sex-disaggregated data and bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.14067v2",
    "title": "Improving Fairness in Speaker Recognition",
    "year": 2021,
    "authors": [
      "Gianni Fenu",
      "Giacomo Medda",
      "Mirko Marras",
      "Giacomo Meloni"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness disparities in speaker recognition systems across demographic groups, specifically gender, addressing social bias and fairness issues in AI.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on fairness in biometric AI systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "balancing demographic representation in training data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.13640v2",
    "title": "Societal Biases in Retrieved Contents: Measurement Framework and Adversarial Mitigation for BERT Rankers",
    "year": 2021,
    "authors": [
      "Navid Rekabsaz",
      "Simone Kopeinik",
      "Markus Schedl"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses societal biases in retrieval systems, focusing on fairness related to social groups such as gender. It discusses measuring and mitigating biases that reinforce stereotypes, which are core issues of social inequality.",
      "inequality_type": [
        "gender",
        "informational"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Experiment",
        "Fairness Measurement"
      ],
      "methodology_detail": "Develops fairness measurement and adversarial bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.13312v3",
    "title": "Multi-fairness under class-imbalance",
    "year": 2021,
    "authors": [
      "Arjun Roy",
      "Vasileios Iosifidis",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning related to protected groups, highlighting bias against minority classes, which relates to social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "class"
      ],
      "other_detail": "Focus on protected attributes and minority group bias",
      "affected_populations": [
        "women",
        "non-white groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness measure and boosting approach",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.13083v1",
    "title": "Using Radio Archives for Low-Resource Speech Recognition: Towards an Intelligent Virtual Assistant for Illiterate Users",
    "year": 2021,
    "authors": [
      "Moussa Doumbouya",
      "Lisa Einstein",
      "Chris Piech"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on improving speech recognition for illiterate and low-resource language speakers, addressing digital and educational inequalities affecting marginalized populations.",
      "inequality_type": [
        "educational",
        "linguistic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on illiterate and low-resource language communities",
      "affected_populations": [
        "illiterate users",
        "low-resource language speakers"
      ],
      "methodology": [
        "Unsupervised Learning",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Using radio archives for speech representation learning",
      "geographic_focus": [
        "West Africa"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.12920v1",
    "title": "Equity and Artificial Intelligence in Education: Will \"AIEd\" Amplify or Alleviate Inequities in Education?",
    "year": 2021,
    "authors": [
      "Kenneth Holstein",
      "Shayan Doroudi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses how AIEd systems may impact educational equity and exacerbate existing inequalities across social groups, addressing social disparities and bias issues.",
      "inequality_type": [
        "educational",
        "social",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on educational equity and systemic biases",
      "affected_populations": [
        "learners from different groups",
        "educational disparities"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Examines perspectives and frameworks for equity",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.12037v1",
    "title": "Precarity: Modeling the Long Term Effects of Compounded Decisions on Individual Instability",
    "year": 2021,
    "authors": [
      "Pegah Nokhiz",
      "Aravinda Kanchana Ruwanpathirana",
      "Neal Patwari",
      "Suresh Venkatasubramanian"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how decision-making impacts individual stability across income groups, highlighting heterogeneity and policy effects, which relates to socioeconomic inequality.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on long-term precarity and decision impacts",
      "affected_populations": [
        "income classes",
        "decision subjects"
      ],
      "methodology": [
        "Model Development",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulates compounded decision effects over time",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.08769v1",
    "title": "Fair Representation Learning for Heterogeneous Information Networks",
    "year": 2021,
    "authors": [
      "Ziqian Zeng",
      "Rashidul Islam",
      "Kamrun Naher Keya",
      "James Foulds",
      "Yangqiu Song",
      "Shimei Pan"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems affecting social groups, specifically gender bias in career counseling, highlighting social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in employment",
      "affected_populations": [
        "women",
        "job seekers"
      ],
      "methodology": [
        "Representation Learning",
        "Fairness Algorithms",
        "Evaluation"
      ],
      "methodology_detail": "Debiasing techniques for HINs in AI",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.08666v2",
    "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models",
    "year": 2021,
    "authors": [
      "Tejas Srinivasan",
      "Yonatan Bisk"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender biases in multimodal AI models, highlighting social stereotypes and discrimination. It addresses biases related to gender, a key aspect of social inequality. The focus on biases in AI systems affecting social groups indicates relevance to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Biases reinforce gender stereotypes in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Bias Analysis"
      ],
      "methodology_detail": "Analyzes biases in vision-and-language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.07780v2",
    "title": "Impact of gender on the formation and outcome of mentoring relationships in academic research",
    "year": 2021,
    "authors": [
      "Leah P. Schwartz",
      "Jean Liénard",
      "Stephen V. David"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in academic mentoring and career progression, highlighting social inequalities related to gender and resource access.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Resource disparities linked to mentor status and prestige",
      "affected_populations": [
        "women trainees",
        "women mentors"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of mentoring relationships and resource access data",
      "geographic_focus": [
        "unspecified, likely global or US-based"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.07505v2",
    "title": "Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models",
    "year": 2021,
    "authors": [
      "Karolina Stańczak",
      "Sagnik Ray Choudhury",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models related to politicians, addressing gender inequality and social bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "female politicians",
        "male politicians"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement via adjective and verb usage",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.07429v2",
    "title": "First the worst: Finding better gender translations during beam search",
    "year": 2021,
    "authors": [
      "Danielle Saunders",
      "Rosie Sallis",
      "Bill Byrne"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social fairness issue affecting gender representation.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "gender minorities",
        "women"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Adjusting inference procedures for gender bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.06973v1",
    "title": "[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation",
    "year": 2021,
    "authors": [
      "Haswanth Aekula",
      "Sugam Garg",
      "Animesh Gupta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, which relates to social gender inequality and discrimination in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.12553v3",
    "title": "Avoiding bias when inferring race using name-based approaches",
    "year": 2021,
    "authors": [
      "Diego Kozlowski",
      "Dakota S. Murray",
      "Alexis Bell",
      "Will Hulsey",
      "Vincent Larivière",
      "Thema Monroe-White",
      "Cassidy R. Sugimoto"
    ],
    "categories": [
      "cs.CY",
      "cs.IR",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in academia and addresses biases in racial inference algorithms, directly engaging with racial inequality and social bias issues.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on racial inference bias in academic authorship",
      "affected_populations": [
        "Black authors",
        "White authors"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Uses census and mortgage data for racial inference",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.06001v3",
    "title": "Gender Bias in Machine Translation",
    "year": 2021,
    "authors": [
      "Beatrice Savoldi",
      "Marco Gaido",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a social discrimination issue affecting societal perceptions and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "users",
        "society"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Critical review and conceptual analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.04502v1",
    "title": "Auditing for Discrimination in Algorithms Delivering Job Ads",
    "year": 2021,
    "authors": [
      "Basileal Imana",
      "Aleksandra Korolova",
      "John Heidemann"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in algorithmic ad delivery, highlighting social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "job seekers"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Black-box audit distinguishing qualification-based and bias-based skew",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.03007v1",
    "title": "Representative & Fair Synthetic Data",
    "year": 2021,
    "authors": [
      "Paul Tiwald",
      "Alexandra Ebert",
      "Daniel T. Soukup"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender and race in data, aiming to improve fairness in AI, which directly relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in training data for fair AI",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation"
      ],
      "methodology_detail": "Fair synthetic data generation with fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.02821v2",
    "title": "Towards Measuring Fairness in AI: the Casual Conversations Dataset",
    "year": 2021,
    "authors": [
      "Caner Hazirbas",
      "Joanna Bitton",
      "Brian Dolhansky",
      "Jacqueline Pan",
      "Albert Gordo",
      "Cristian Canton Ferrer"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates AI fairness across diverse social groups, highlighting disparities related to skin tone, age, and gender, which are social identity factors.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Focus on fairness across demographic attributes",
      "affected_populations": [
        "people with darker skin",
        "older adults",
        "women"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing model performance across demographic groups",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.02532v3",
    "title": "End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning",
    "year": 2021,
    "authors": [
      "Tal Feldman",
      "Ashley Peake"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in AI models, addressing social fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in deep learning models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Fairness Algorithms"
      ],
      "methodology_detail": "End-to-end bias mitigation framework combining multiple techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.01781v1",
    "title": "Reducing Racial Bias in Facial Age Prediction using Unsupervised Domain Adaptation in Regression",
    "year": 2021,
    "authors": [
      "Apoorva Gokhale",
      "Astuti Sharma",
      "Kaustav Datta",
      "Savyasachi"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in facial age prediction, a social discrimination issue, by proposing domain adaptation techniques to improve fairness across ethnic groups.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial bias in AI models",
      "affected_populations": [
        "ethnic minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Unsupervised domain adaptation for regression tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.00507v2",
    "title": "fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation",
    "year": 2021,
    "authors": [
      "Jakub Wiśniewski",
      "Przemysław Biecek"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.MS",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias detection and mitigation in AI systems, focusing on fairness and discrimination, which are central to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic bias and fairness validation",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metrics",
        "Visualization",
        "Bias Mitigation"
      ],
      "methodology_detail": "Tools for bias detection and mitigation in models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.00096v1",
    "title": "Taking Stock of the Present and Future of Smart Technologies for Older Adults and Caregivers",
    "year": 2021,
    "authors": [
      "Christina N. Harrington",
      "Ben Jelen",
      "Amanda Lazar",
      "Aqueasha Martin-Hammond",
      "Alisha Pradhan",
      "Blaine Reeder",
      "Katie Siek"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in access and design of smart technologies for older adults, emphasizing inequities in resource coordination, affordability, and inclusion of diverse populations.",
      "inequality_type": [
        "age",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Focus on under-resourced communities and diverse older adults",
      "affected_populations": [
        "older adults",
        "caregivers",
        "diverse communities"
      ],
      "methodology": [
        "Literature Review",
        "System Design"
      ],
      "methodology_detail": "Review of existing technologies and design recommendations",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.15237v2",
    "title": "Should College Dropout Prediction Models Include Protected Attributes?",
    "year": 2021,
    "authors": [
      "Renzhe Yu",
      "Hansol Lee",
      "René F. Kizilcec"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and discrimination related to protected social attributes in predictive models for student dropout, addressing social bias and inequality issues.",
      "inequality_type": [
        "educational",
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in educational predictive analytics",
      "affected_populations": [
        "underrepresented students",
        "first-generation students",
        "students with financial need"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Comparative analysis with/without protected attributes",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.15163v3",
    "title": "Countering Racial Bias in Computer Graphics Research",
    "year": 2021,
    "authors": [
      "Theodore Kim",
      "Holly Rushmeier",
      "Julie Dorsey",
      "Derek Nowrouzezahrai",
      "Raqi Syed",
      "Wojciech Jarosz",
      "A. M. Darke"
    ],
    "categories": [
      "cs.GR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial biases in computer graphics, focusing on visual features associated with racial groups, and proposes improvements to reduce racial bias, which relates directly to social discrimination and inequality.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on racial bias in visual representation",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Improving measures and practices in research",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.15122v2",
    "title": "Quantifying Bias in Automatic Speech Recognition",
    "year": 2021,
    "authors": [
      "Siyuan Feng",
      "Olya Kudina",
      "Bence Mark Halpern",
      "Odette Scharenborg"
    ],
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in ASR related to gender, age, and accents, which are social identity factors, and discusses bias mitigation strategies.",
      "inequality_type": [
        "gender",
        "age",
        "linguistic"
      ],
      "other_detail": "Focus on speech articulation bias in ASR systems",
      "affected_populations": [
        "gender groups",
        "age groups",
        "regional speakers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Word error rates and phoneme-level error analysis",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.13868v1",
    "title": "Equality before the Law: Legal Judgment Consistency Analysis for Fairness",
    "year": 2021,
    "authors": [
      "Yuzhong Wang",
      "Chaojun Xiao",
      "Shirong Ma",
      "Haoxi Zhong",
      "Cunchao Tu",
      "Tianyang Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities related to regional and gender inconsistencies in legal judgments, addressing social fairness issues.",
      "inequality_type": [
        "gender",
        "geographic"
      ],
      "other_detail": "Focus on legal judgment fairness disparities",
      "affected_populations": [
        "gender groups",
        "regional populations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Using LJP models and inconsistency measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2106.15298v1",
    "title": "US Fatal Police Shooting Analysis and Prediction",
    "year": 2021,
    "authors": [
      "Yuan Wang",
      "Yangxin Fan"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "cs.SI",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes racial disparities in police shootings and examines potential racial discrimination, addressing racial inequality. It also considers socioeconomic and demographic factors influencing police violence. The focus on race and social impacts indicates a genuine concern with social inequality.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial disparities in police violence",
      "affected_populations": [
        "racial minorities",
        "police victims"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Machine Learning"
      ],
      "methodology_detail": "Analyzes patterns, hotspots, and predictive models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.11852v1",
    "title": "Detecting Racial Bias in Jury Selection",
    "year": 2021,
    "authors": [
      "Jack Dunn",
      "Ying Daisy Zhuo"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in jury selection, directly addressing racial inequality and social discrimination. It analyzes disparities affecting specific racial groups within the justice system using AI methods.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "racial minorities",
        "jurors"
      ],
      "methodology": [
        "Statistical Analysis",
        "Machine Learning"
      ],
      "methodology_detail": "Optimal Feature Selection and Classification Trees",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.11806v1",
    "title": "Tackling Racial Bias in Automated Online Hate Detection: Towards Fair and Accurate Classification of Hateful Online Users Using Geometric Deep Learning",
    "year": 2021,
    "authors": [
      "Zo Ahmed",
      "Bertie Vidgen",
      "Scott A. Hale"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in online hate detection, focusing on fairness and social discrimination issues.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial fairness in AI systems",
      "affected_populations": [
        "African-American users"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Incorporates social network data for classification fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.11436v1",
    "title": "Responsible AI: Gender bias assessment in emotion recognition",
    "year": 2021,
    "authors": [
      "Artem Domnich",
      "Gholamreza Anbarjafari"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in facial expression recognition, highlighting social discrimination and fairness issues related to gender. It analyzes how AI systems impact different gender groups, addressing social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "men",
        "women"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing neural networks for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.13879v1",
    "title": "Examining mobility data justice during 2017 Hurricane Harvey",
    "year": 2021,
    "authors": [
      "Hengfang Deng",
      "Qi Wang"
    ],
    "categories": [
      "cs.SI",
      "physics.soc-ph",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in mobility data representativeness across socioeconomic neighborhoods, highlighting issues of justice and bias during a natural disaster.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on data bias affecting different socioeconomic neighborhoods",
      "affected_populations": [
        "urban residents",
        "socioeconomic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analyzes mobility data disparities during Hurricane Harvey",
      "geographic_focus": [
        "Houston, Texas"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.11023v1",
    "title": "Individually Fair Ranking",
    "year": 2021,
    "authors": [
      "Amanda Bower",
      "Hamid Eftekhari",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair ranking algorithms to address biases affecting minority groups, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fair ranking for minority and majority groups",
      "affected_populations": [
        "minority groups",
        "majority groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Optimal Transport",
        "Regularizer Design"
      ],
      "methodology_detail": "Fairness enforcement via optimal transport regularizer",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.10944v2",
    "title": "Emergence of Structural Inequalities in Scientific Citation Networks",
    "year": 2021,
    "authors": [
      "Buddhika Nettasinghe",
      "Nazanin Alipourfard",
      "Vikram Krishnamurthy",
      "Kristina Lerman"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.DL",
      "cs.SI",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in scientific recognition based on gender and institutional affiliation, reflecting social inequalities. It analyzes structural biases in citation networks, a form of social bias. The focus on gender and institutional status aligns with social inequality issues.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on academic recognition disparities",
      "affected_populations": [
        "female researchers",
        "top-ranked institution researchers"
      ],
      "methodology": [
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Citation network modeling and empirical data analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.10550v1",
    "title": "Gender and Racial Fairness in Depression Research using Social Media",
    "year": 2021,
    "authors": [
      "Carlos Aguirre",
      "Keith Harrigian",
      "Mark Dredze"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in depression classification across gender and racial groups, addressing social fairness and disparities in mental health research.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Focus on social fairness in AI models",
      "affected_populations": [
        "gender groups",
        "racial/ethnic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes model performance disparities across demographics",
      "geographic_focus": [
        "Twitter data (unspecified regions)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.09118v5",
    "title": "Balancing Biases and Preserving Privacy on Balanced Faces in the Wild",
    "year": 2021,
    "authors": [
      "Joseph P Robinson",
      "Can Qin",
      "Yann Henon",
      "Samson Timoner",
      "Yun Fu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic biases in facial recognition, focusing on race and gender disparities, and discusses fairness and privacy issues affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "privacy"
      ],
      "other_detail": "Bias mitigation and privacy preservation in facial recognition",
      "affected_populations": [
        "ethnic minorities",
        "women"
      ],
      "methodology": [
        "Dataset Creation",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias measurement and domain adaptation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.09068v1",
    "title": "Predicting Early Dropout: Calibration and Algorithmic Fairness Considerations",
    "year": 2021,
    "authors": [
      "Marzieh Karimi-Haghighi",
      "Carlos Castillo",
      "Davinia Hernandez-Leo",
      "Veronica Moreno Oliver"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic fairness in predicting student dropout, addressing biases affecting different social groups, particularly in educational contexts.",
      "inequality_type": [
        "educational",
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focus on fairness and bias mitigation in predictive models",
      "affected_populations": [
        "students at risk",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Calibration",
        "Fairness Analysis"
      ],
      "methodology_detail": "Includes calibration and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2104.03909v1",
    "title": "RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity",
    "year": 2021,
    "authors": [
      "David Liu",
      "Zohair Shafi",
      "William Fleisher",
      "Tina Eliassi-Rad",
      "Scott Alfeld"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on encoding the Rawlsian principle of fair equality of opportunity, which aims to address social inequalities related to background circumstances affecting social mobility and fairness. It discusses altering social structures and generating aspirational data to promote fairness in social outcomes, directly engaging with social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "social"
      ],
      "other_detail": "Focuses on fairness in social opportunities",
      "affected_populations": [
        "socially disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Altering Bayesian Networks to encode fairness principles",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.08763v1",
    "title": "Please Don't Go -- A Comprehensive Approach to Increase Women's Participation in Open Source Software",
    "year": 2021,
    "authors": [
      "Bianca Trinkenreich"
    ],
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender disparities in OSS participation, a social inequality issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender underrepresentation in tech",
      "affected_populations": [
        "women in tech"
      ],
      "methodology": [
        "Qualitative Study",
        "Empirical Investigation",
        "Collaboration with Linux Foundation"
      ],
      "methodology_detail": "Analyzing motivations, strategies, and impact assessments",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.09233v1",
    "title": "Towards Fair Affective Robotics: Continual Learning for Mitigating Bias in Facial Expression and Action Unit Recognition",
    "year": 2021,
    "authors": [
      "Ozgur Kara",
      "Nikhil Churamani",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness in affective AI systems, which relate to social discrimination and inequality issues across demographic groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability"
      ],
      "other_detail": "Bias mitigation in facial expression recognition systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "disabled individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Comparative analysis of bias mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.08637v1",
    "title": "Domain-Incremental Continual Learning for Mitigating Bias in Facial Expression and Action Unit Recognition",
    "year": 2021,
    "authors": [
      "Nikhil Churamani",
      "Ozgur Kara",
      "Hatice Gunes"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in facial recognition systems, which relate to social discrimination issues such as race, gender, and demographic fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health",
        "educational"
      ],
      "other_detail": "Focuses on algorithmic bias mitigation in facial analysis",
      "affected_populations": [
        "underrepresented demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Using continual learning for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.09055v1",
    "title": "OmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning",
    "year": 2021,
    "authors": [
      "Hantian Zhang",
      "Xu Chu",
      "Abolfazl Asudeh",
      "Shamkant B. Navathe"
    ],
    "categories": [
      "cs.CY",
      "cs.DB",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning, focusing on reducing bias against demographic groups such as race and gender, which are key aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness constraints in AI models",
      "affected_populations": [
        "minority groups",
        "females",
        "African Americans"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Fairness Constraint Specification"
      ],
      "methodology_detail": "Supports multiple fairness notions without model modifications",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.06828v2",
    "title": "Wasserstein Robust Classification with Fairness Constraints",
    "year": 2021,
    "authors": [
      "Yijie Wang",
      "Viet Anh Nguyen",
      "Grani A. Hanasusanto"
    ],
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in classification, specifically addressing equal opportunity, which relates to social fairness issues. It aims to improve fairness in AI systems, impacting social groups. The emphasis on fairness constraints indicates a concern with social discrimination and bias.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in classification algorithms",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness-constrained classification models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.06598v1",
    "title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces",
    "year": 2021,
    "authors": [
      "Niklas Friedrich",
      "Anne Lauscher",
      "Simone Paolo Ponzetto",
      "Goran Glavaš"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in word embeddings related to stereotypes like racism and sexism, which are social inequalities. It focuses on measuring and mitigating these biases to improve fairness in NLP systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation in language models",
      "affected_populations": [
        "minority groups",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "System Design"
      ],
      "methodology_detail": "Bias measurement and debiasing platform development",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.06413v1",
    "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
    "year": 2021,
    "authors": [
      "Pengyu Cheng",
      "Weituo Hao",
      "Siyang Yuan",
      "Shijing Si",
      "Lawrence Carin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social bias and fairness issues in NLP models, which relate to social discrimination and inequality. It focuses on reducing bias in pretrained text encoders, impacting social groups affected by such biases.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in AI models",
      "affected_populations": [
        "social groups",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Learning",
        "Natural Language Processing"
      ],
      "methodology_detail": "Debiasing sentence representations via neural network",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.09058v2",
    "title": "Understanding the Representation and Representativeness of Age in AI Data Sets",
    "year": 2021,
    "authors": [
      "Joon Sung Park",
      "Michael S. Bernstein",
      "Robin N. Brewer",
      "Ece Kamar",
      "Meredith Ringel Morris"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic representation, specifically age, in AI datasets, highlighting under-representation of older adults. It addresses issues of fairness and inclusivity in AI data collection, which relate to social inequality concerns.",
      "inequality_type": [
        "age"
      ],
      "other_detail": "Focus on age representation in datasets",
      "affected_populations": [
        "older adults"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of publicly available datasets and metadata",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.05841v1",
    "title": "Interpretable bias mitigation for textual data: Reducing gender bias in patient notes while maintaining classification performance",
    "year": 2021,
    "authors": [
      "Joshua R. Minot",
      "Nicholas Cheney",
      "Marc Maier",
      "Danne C. Elbers",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds"
    ],
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in medical language, impacting health equity.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on bias reduction in clinical language data",
      "affected_populations": [
        "patients",
        "healthcare providers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Data Augmentation",
        "Experiment"
      ],
      "methodology_detail": "Using BERT-based classifiers for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.04243v2",
    "title": "Estimating and Improving Fairness with Adversarial Learning",
    "year": 2021,
    "authors": [
      "Xiaoxiao Li",
      "Ziteng Cui",
      "Yifan Wu",
      "Lin Gu",
      "Tatsuya Harada"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias mitigation in AI, addressing social discrimination related to attributes like sex and skin tone, which are linked to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "health"
      ],
      "other_detail": "Bias mitigation in medical AI systems",
      "affected_populations": [
        "underprivileged groups",
        "minority patients"
      ],
      "methodology": [
        "Deep Learning",
        "Adversarial Training",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and fairness estimation in medical images",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.04048v1",
    "title": "Fairness in TabNet Model by Disentangled Representation for the Prediction of Hospital No-Show",
    "year": 2021,
    "authors": [
      "Sabri Boughorbel",
      "Fethi Jarray",
      "Abdou Kadri"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare AI, focusing on equitable access and bias mitigation related to sensitive attributes. It aims to improve fairness in no-show predictions, which can impact different social groups. The emphasis on fairness and representation learning indicates a concern with social disparities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Fairness in healthcare AI systems",
      "affected_populations": [
        "patients",
        "underserved groups"
      ],
      "methodology": [
        "Deep Learning",
        "Representation Learning",
        "Fairness Optimization"
      ],
      "methodology_detail": "Disentangled representations for fairness",
      "geographic_focus": [
        "unspecified hospital dataset"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.03598v2",
    "title": "WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings",
    "year": 2021,
    "authors": [
      "Bhavya Ghai",
      "Md Naimul Hoque",
      "Klaus Mueller"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on intersectional biases related to race, gender, and social groups in AI, addressing social discrimination and bias detection.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "ethnic"
      ],
      "other_detail": "Focuses on biases in word embeddings related to social groups",
      "affected_populations": [
        "Black females",
        "Muslims",
        "Poor women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Interactive Tool Development",
        "Qualitative Feedback"
      ],
      "methodology_detail": "Bias detection and visualization in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.03332v1",
    "title": "An Agent-based Model to Evaluate Interventions on Online Dating Platforms to Decrease Racial Homogamy",
    "year": 2021,
    "authors": [
      "Stefania Ionescu",
      "Aniko Hannak",
      "Kenneth Joseph"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial homogamy and interracial relationships, which relate directly to racial inequality and social segregation. It analyzes how online platform interventions impact societal racial disparities. The focus on societal inequalities in relationships indicates a clear connection to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on racial disparities in online dating",
      "affected_populations": [
        "racial groups",
        "online daters"
      ],
      "methodology": [
        "Agent-based Modeling",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling interventions' effects on societal racial dynamics",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.01335v1",
    "title": "How Fair is Fairness-aware Representative Ranking and Methods for Fair Ranking",
    "year": 2021,
    "authors": [
      "Akrati Saxena",
      "George Fletcher",
      "Mykola Pechenizkiy"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in ranking algorithms affecting minority groups, highlighting bias and fairness issues related to social attributes.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in algorithmic ranking systems",
      "affected_populations": [
        "minority candidates",
        "sub-active groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes fairness metrics and bias quantification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.01168v1",
    "title": "Narratives and Counternarratives on Data Sharing in Africa",
    "year": 2021,
    "authors": [
      "Rediet Abebe",
      "Kehinde Aruleba",
      "Abeba Birhane",
      "Sara Kingsley",
      "George Obaido",
      "Sekou L. Remy",
      "Swathi Sadagopan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses power imbalances, colonial legacies, and inequities in data sharing affecting African communities, highlighting social and racial inequalities.",
      "inequality_type": [
        "racial",
        "economic",
        "geographic",
        "social"
      ],
      "other_detail": "Focus on colonial legacies and power dynamics",
      "affected_populations": [
        "African communities",
        "data experts in Africa"
      ],
      "methodology": [
        "Qualitative Study",
        "Storytelling",
        "Interviews",
        "Research Analysis"
      ],
      "methodology_detail": "Using fictional personas and interview data",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2103.01093v3",
    "title": "Quantifying Indirect Gender Discrimination on Collaborative Platforms",
    "year": 2021,
    "authors": [
      "Orsolya Vasarhelyi",
      "Balazs Vedres"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender-based discrimination and disparities on digital platforms, addressing social inequality related to gender and social bias in AI systems.",
      "inequality_type": [
        "gender",
        "informational",
        "digital"
      ],
      "other_detail": "Focus on gender discrimination in online collaborative environments",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing platform data and behavioral patterns",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.12586v5",
    "title": "A Stochastic Optimization Framework for Fair Risk Minimization",
    "year": 2021,
    "authors": [
      "Andrew Lowy",
      "Sina Baharlouei",
      "Rakesh Pavan",
      "Meisam Razaviyayn",
      "Ahmad Beirami"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing social bias and discrimination issues related to sensitive attributes such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness constraints in machine learning algorithms",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Experimental Analysis"
      ],
      "methodology_detail": "Develops and tests a scalable fairness algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.11929v4",
    "title": "PolicySpace2: modeling markets and endogenous public policies",
    "year": 2021,
    "authors": [
      "Bernardo Alves Furtado"
    ],
    "categories": [
      "cs.MA",
      "econ.GN",
      "physics.soc-ph",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models public policies aimed at reducing inequality and poverty, focusing on socioeconomic impacts and distributional effects.",
      "inequality_type": [
        "income",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focus on poverty alleviation and inequality reduction",
      "affected_populations": [
        "low-income households",
        "urban residents"
      ],
      "methodology": [
        "Agent-based modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Simulation of policy impacts using spatial data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.11925v3",
    "title": "Chasm in Hegemony: Explaining and Reproducing Disparities in Homophilous Networks",
    "year": 2021,
    "authors": [
      "Yiguang Zhang",
      "Jessy Xinyi Han",
      "Ilica Mahajan",
      "Priyanjana Bengani",
      "Augustin Chaintreau"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in minority representation across social hierarchy levels, indicating a focus on social inequality. It discusses social disparities, minority-majority dynamics, and fairness applications. The analysis of social disparities in networks aligns with social inequality topics.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on minority representation in social networks",
      "affected_populations": [
        "minorities",
        "majorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Network growth modeling and empirical analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.10750v1",
    "title": "Coping with Mistreatment in Fair Algorithms",
    "year": 2021,
    "authors": [
      "Ankit Kulshrestha",
      "Ilya Safro"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic fairness and bias affecting societal groups, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness metrics and bias mitigation in algorithms",
      "affected_populations": [
        "sensitive groups",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes fairness metrics and bias mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.10349v1",
    "title": "Everything is Relative: Understanding Fairness with Optimal Transport",
    "year": 2021,
    "authors": [
      "Kweku Kwegyir-Aggrey",
      "Rebecca Santorella",
      "Sarah M. Brown"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in automated decision-making, which are related to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias detection",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Optimal Transport",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using optimal transport maps for fairness analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.09103v1",
    "title": "Gender Bias, Social Bias and Representation: 70 Years of B$^H$ollywood",
    "year": 2021,
    "authors": [
      "Kunal Khadilkar",
      "Ashiqur R. KhudaBukhsh",
      "Tom M. Mitchell"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes social and gender biases in Bollywood dialogues over 70 years, reflecting social norms and inequalities. It examines representation related to gender, geographic, and religious groups, indicating a focus on social disparities.",
      "inequality_type": [
        "gender",
        "geographic",
        "religion"
      ],
      "other_detail": "Focus on social and cultural biases in media",
      "affected_populations": [
        "women",
        "geographic groups",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing dialogue corpora over time",
      "geographic_focus": [
        "India",
        "Global comparison with Hollywood"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.07333v1",
    "title": "AI Ethics Needs Good Data",
    "year": 2021,
    "authors": [
      "Angela Daly",
      "S Kate Devitt",
      "Monique Mann"
    ],
    "categories": [
      "cs.AI",
      "K.4.1; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract emphasizes power, political economy, and societal wellbeing, especially for marginalized groups, indicating a focus on social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "disenfranchisement"
      ],
      "other_detail": "Focus on marginalized and disenfranchised populations",
      "affected_populations": [
        "marginalized groups",
        "disenfranchised communities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Conceptual framework development and critique",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.06761v1",
    "title": "MIMIC-IF: Interpretability and Fairness Evaluation of Deep Learning Models on MIMIC-IV Dataset",
    "year": 2021,
    "authors": [
      "Chuizheng Meng",
      "Loc Trinh",
      "Nan Xu",
      "Yan Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in healthcare treatment and prediction fairness across demographic groups, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Focus on healthcare disparities and algorithmic fairness",
      "affected_populations": [
        "ethnicity groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Analyzes dataset bias, interpretability, and fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.06619v3",
    "title": "Differences in the spatial landscape of urban mobility: gender and socioeconomic perspectives",
    "year": 2021,
    "authors": [
      "Mariana Macedo",
      "Laura Lotero",
      "Alessio Cardillo",
      "Ronaldo Menezes",
      "Hugo Barbosa"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender and socioeconomic disparities in urban mobility, highlighting inequalities in access to opportunities.",
      "inequality_type": [
        "socioeconomic",
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "men",
        "women",
        "socioeconomic groups"
      ],
      "methodology": [
        "Computational Analysis",
        "Statistical Analysis",
        "Information-Theoretical Approaches",
        "Survey"
      ],
      "methodology_detail": "Analyzes large-scale travel survey data",
      "geographic_focus": [
        "Medellín",
        "Bogotá",
        "São Paulo"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.04565v2",
    "title": "A Ranking Approach to Fair Classification",
    "year": 2021,
    "authors": [
      "Jakob Schoeffer",
      "Niklas Kuehl",
      "Isabel Valera"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making systems, aiming to reduce stereotypes and bias, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social bias"
      ],
      "other_detail": "Focus on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "disadvantaged groups",
        "minorities",
        "students",
        "job applicants"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness evaluation and ranking-based decision systems",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.04342v1",
    "title": "The Limits of Computation in Solving Equity Trade-Offs in Machine Learning and Justice System Risk Assessment",
    "year": 2021,
    "authors": [
      "Jesse Russell"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial equity issues in justice-related machine learning tools, highlighting disparities and fairness concerns affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "social",
        "fairness"
      ],
      "other_detail": "Focus on racial equity in justice system risk assessments",
      "affected_populations": [
        "racial minorities",
        "justice-involved individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Analyzes score distributions and model performance metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.04130v3",
    "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
    "year": 2021,
    "authors": [
      "Hannah Kirk",
      "Yennie Jun",
      "Haider Iqbal",
      "Elias Benussi",
      "Filippo Volpin",
      "Frederic A. Dreyer",
      "Aleksandar Shtedritski",
      "Yuki M. Asano"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases in language models related to gender, ethnicity, religion, and occupation, reflecting societal inequalities. It discusses how models inherit and sometimes reinforce existing disparities. The focus on occupational stereotypes and demographic distributions directly relates to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Occupational biases and societal distribution reflections",
      "affected_populations": [
        "women",
        "ethnic minorities",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes model outputs and fits logistic models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.03054v1",
    "title": "Removing biased data to improve fairness and accuracy",
    "year": 2021,
    "authors": [
      "Sahil Verma",
      "Michael Ernst",
      "Rene Just"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in data affecting fairness, which relates to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in AI systems",
      "affected_populations": [
        "social groups",
        "discriminated groups"
      ],
      "methodology": [
        "Machine Learning",
        "Debiasing",
        "Experiment"
      ],
      "methodology_detail": "Removing biased data to improve fairness and accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.02841v1",
    "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries",
    "year": 2021,
    "authors": [
      "Stephanie Hirmer",
      "Alycia Leonard",
      "Josephine Tumwesige",
      "Costanza Conforti"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses the under-representation of marginalized, illiterate communities in NLP data collection, highlighting issues related to social inequality in access and representation.",
      "inequality_type": [
        "educational",
        "geographic",
        "informational"
      ],
      "other_detail": "Focus on rural, illiterate populations in developing countries",
      "affected_populations": [
        "illiterate communities",
        "rural residents"
      ],
      "methodology": [
        "Dataset Creation",
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Addressing data collection challenges and ethical considerations",
      "geographic_focus": [
        "Low-Income Countries"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.02320v1",
    "title": "One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision",
    "year": 2021,
    "authors": [
      "Zaid Khan",
      "Yun Fu"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial categories in computer vision datasets, highlighting biases, stereotypes, and inconsistencies that impact racial representation and fairness. It discusses how racial classification systems encode stereotypes and affect marginalized groups. This directly relates to social inequality issues surrounding race and representation.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in AI datasets",
      "affected_populations": [
        "racial minorities",
        "face image subjects"
      ],
      "methodology": [
        "Empirical Analysis",
        "Dataset Comparison"
      ],
      "methodology_detail": "Analyzing racial category consistency across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.02279v1",
    "title": "Insiders and Outsiders in Research on Machine Learning and Society",
    "year": 2021,
    "authors": [
      "Yu Tao",
      "Kush R. Varshney"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities related to researcher backgrounds and perspectives in ML research, highlighting issues of inclusion and epistemic boundaries, which relate to social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on researcher diversity and epistemic boundaries",
      "affected_populations": [
        "researchers from underrepresented backgrounds"
      ],
      "methodology": [
        "Sociological Analysis",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analyzes arXiv authorship data and sociological perspectives",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.04256v1",
    "title": "Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits",
    "year": 2021,
    "authors": [
      "Jack Bandy"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The review highlights algorithm behaviors like discrimination and distortion, which relate to social bias and inequality issues. It discusses problematic algorithm behaviors impacting public-facing systems, often affecting marginalized groups. The focus on fairness, bias, and social impact indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on algorithmic bias and social impact",
      "affected_populations": [
        "minority groups",
        "women",
        "disabled individuals",
        "low-income users"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "Qualitative Study"
      ],
      "methodology_detail": "Systematic review following PRISMA guidelines",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.04257v3",
    "title": "Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities",
    "year": 2021,
    "authors": [
      "Nenad Tomasev",
      "Kevin R. McKee",
      "Jackie Kay",
      "Shakir Mohamed"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social discrimination, bias, and impacts on marginalized groups, specifically queer communities, in relation to AI systems.",
      "inequality_type": [
        "gender",
        "sexual orientation",
        "health",
        "privacy"
      ],
      "other_detail": "Focus on unobserved social characteristics in fairness",
      "affected_populations": [
        "queer communities",
        "LGBTQ+"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Explores fairness concepts and sociotechnical impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.01859v2",
    "title": "BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems",
    "year": 2021,
    "authors": [
      "Muhammad Hilmi Asyrofi",
      "Zhou Yang",
      "Imam Nur Bani Yusuf",
      "Hong Jin Kang",
      "Ferdian Thung",
      "David Lo"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on demographic biases related to gender and other characteristics in AI systems, which are social inequalities. It addresses bias detection and fairness issues in sentiment analysis, reflecting social discrimination concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias detection in AI systems",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Automated template generation and bias testing",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.01287v1",
    "title": "Detection of Racial Bias from Physiological Responses",
    "year": 2021,
    "authors": [
      "Fateme Nikseresht",
      "Runze Yan",
      "Rachel Lew",
      "Yingzheng Liu",
      "Rose M. Sebastian",
      "Afsaneh Doryab"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias detection through physiological responses, directly addressing racial discrimination and implicit bias, which are key aspects of social inequality.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on unconscious racial bias detection methods",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Physiological data analysis during bias assessment",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.01265v1",
    "title": "The Limits of Global Inclusion in AI Development",
    "year": 2021,
    "authors": [
      "Alan Chan",
      "Chinasa T. Okolo",
      "Zachary Terner",
      "Angelina Wang"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses economic power, global inequality, and redistribution of opportunities, highlighting social disparities in AI development.",
      "inequality_type": [
        "economic",
        "wealth",
        "power"
      ],
      "other_detail": "Focus on power redistribution over inclusion",
      "affected_populations": [
        "underrepresented groups",
        "global marginalized communities"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Conceptual argument on power and inequality",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.00417v1",
    "title": "Priority-based Post-Processing Bias Mitigation for Individual and Group Fairness",
    "year": 2021,
    "authors": [
      "Pranay Lohia"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems affecting societal outcomes, such as tariff allotment and criminal sentencing, which relate to social justice issues.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "class",
        "discrimination"
      ],
      "other_detail": "Focus on fairness and injustice in societal resource allocation",
      "affected_populations": [
        "general population",
        "individuals facing injustice"
      ],
      "methodology": [
        "Post-processing Bias Mitigation",
        "Case Study",
        "Algorithm Development"
      ],
      "methodology_detail": "Priority-based fairness adjustment in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.00128v2",
    "title": "The effect of differential victim crime reporting on predictive policing systems",
    "year": 2021,
    "authors": [
      "Nil-Jana Akpinar",
      "Maria De-Arteaga",
      "Alexandra Chouldechova"
    ],
    "categories": [
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in crime reporting and their impact on policing, highlighting potential unequal effects on different areas. It addresses fairness issues in predictive policing systems, which relate to social inequalities. The focus on differential reporting rates suggests an underlying concern with social disparities.",
      "inequality_type": [
        "geographic",
        "social"
      ],
      "other_detail": "focus on spatial disparities in crime reporting",
      "affected_populations": [
        "residents in high/low reporting areas"
      ],
      "methodology": [
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Based on crime reporting survey data and simulation",
      "geographic_focus": [
        "Bogotá, Colombia"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2102.00086v1",
    "title": "Challenges in Automated Debiasing for Toxic Language Detection",
    "year": 2021,
    "authors": [
      "Xuhui Zhou",
      "Maarten Sap",
      "Swabha Swayamdipta",
      "Noah A. Smith",
      "Yejin Choi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to race and dialects in toxic language detection, impacting fairness and social discrimination.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Focus on dialectal and racial bias in AI models",
      "affected_populations": [
        "African American English speakers",
        "minority groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Testing debiasing methods and proposing data correction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.12715v3",
    "title": "Disparate Impact Diminishes Consumer Trust Even for Advantaged Users",
    "year": 2021,
    "authors": [
      "Tim Draws",
      "Zoltán Szlávik",
      "Benjamin Timmermans",
      "Nava Tintarev",
      "Kush R. Varshney",
      "Michael Hind"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how algorithmic unfairness impacts consumer trust across social groups, specifically gender, highlighting issues of social bias and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender-related disparate impact",
      "affected_populations": [
        "advantaged users",
        "disadvantaged users"
      ],
      "methodology": [
        "Experiment"
      ],
      "methodology_detail": "User study on trust and algorithmic impact",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.12406v2",
    "title": "Fairness for Whom? Understanding the Reader's Perception of Fairness in Text Summarization",
    "year": 2021,
    "authors": [
      "Anurag Shandilya",
      "Abhisek Dash",
      "Abhijnan Chakraborty",
      "Kripabandhu Ghosh",
      "Saptarshi Ghosh"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness perceptions in AI-generated summaries across social groups, addressing social bias and fairness issues.",
      "inequality_type": [
        "racial",
        "social",
        "informational"
      ],
      "other_detail": "Focus on social fairness perceptions in text summarization",
      "affected_populations": [
        "social groups",
        "microblog users"
      ],
      "methodology": [
        "Experiment",
        "Human-in-the-loop",
        "Automated graph-based analysis"
      ],
      "methodology_detail": "Quantifying perceived bias in summaries",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.11974v1",
    "title": "Disembodied Machine Learning: On the Illusion of Objectivity in NLP",
    "year": 2021,
    "authors": [
      "Zeerak Waseem",
      "Smarika Lulz",
      "Joachim Bingel",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social bias, marginalization, and the subjective nature of data in ML, which relate to social inequalities such as marginalization and bias against social groups.",
      "inequality_type": [
        "social bias",
        "marginalization"
      ],
      "other_detail": "Focuses on bias and marginalization in AI development",
      "affected_populations": [
        "marginalized groups"
      ],
      "methodology": [
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes development process and bias construction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.11775v2",
    "title": "Moral and Social Ramifications of Autonomous Vehicles",
    "year": 2021,
    "authors": [
      "Veljko Dubljević",
      "Sean Douglas",
      "Jovan Milojevich",
      "Nirav Ajmeri",
      "William A. Bauer",
      "George F. List",
      "Munindar P. Singh"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines impacts on drivers' livelihoods and perceptions, highlighting socioeconomic concerns and disempowerment, which relate to social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "income",
        "employment"
      ],
      "other_detail": "Focus on driver livelihoods and social impact",
      "affected_populations": [
        "professional drivers",
        "semi-professional drivers"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Semi-structured interviews with drivers",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.11358v1",
    "title": "Detecting discriminatory risk through data annotation based on Bayesian inferences",
    "year": 2021,
    "authors": [
      "Elena Beretta",
      "Antonio Vetrò",
      "Bruno Lepri",
      "Juan Carlos De Martin"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "68P99 (Primary), 68T01 (Secondary)"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting discriminatory risks in data, specifically addressing racial discrimination, which relates to social inequality issues.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on racial discrimination detection",
      "affected_populations": [
        "minority groups"
      ],
      "methodology": [
        "Bayesian Statistical Analysis",
        "Data Annotation"
      ],
      "methodology_detail": "Bayesian inference for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.10367v1",
    "title": "Modeling Assumptions Clash with the Real World: Transparency, Equity, and Community Challenges for Student Assignment Algorithms",
    "year": 2021,
    "authors": [
      "Samantha Robertson",
      "Tonya Nguyen",
      "Niloufar Salehi"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses socioeconomic barriers affecting equitable school assignment, highlighting inequalities faced by families. It examines how algorithmic assumptions clash with real-world social disparities, especially socioeconomic inequalities.",
      "inequality_type": [
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on socioeconomic barriers in school assignment",
      "affected_populations": [
        "low-income families",
        "disadvantaged students"
      ],
      "methodology": [
        "Case Study",
        "Value Sensitive Design"
      ],
      "methodology_detail": "Analyzes school district algorithms and stakeholder engagement",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.09995v2",
    "title": "Re-imagining Algorithmic Fairness in India and Beyond",
    "year": 2021,
    "authors": [
      "Nithya Sambasivan",
      "Erin Arnesen",
      "Ben Hutchinson",
      "Tulsee Doshi",
      "Vinodkumar Prabhakaran"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI's impact on marginalized communities in India, challenging assumptions related to data reliability, standards, and aspirations, indicating a focus on social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "class",
        "geographic",
        "disability"
      ],
      "other_detail": "Focus on India-specific social and economic disparities",
      "affected_populations": [
        "oppressed communities",
        "marginalized groups"
      ],
      "methodology": [
        "Qualitative Study",
        "Discourse Analysis"
      ],
      "methodology_detail": "Interviews and discourse analysis of AI deployments",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.09869v2",
    "title": "Black Feminist Musings on Algorithmic Oppression",
    "year": 2021,
    "authors": [
      "Lelia Marie Hampton"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines algorithmic oppression affecting marginalized groups, emphasizing racial and gender injustices, and discusses systemic impacts on social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on algorithmic oppression and marginalized communities",
      "affected_populations": [
        "Black communities",
        "Women",
        "Marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis",
        "Ethics Analysis"
      ],
      "methodology_detail": "Critical feminist and philosophical critique of technology",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.09688v2",
    "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models",
    "year": 2021,
    "authors": [
      "Daniel de Vassimon Manela",
      "David Errington",
      "Thomas Fisher",
      "Boris van Breugel",
      "Pasquale Minervini"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in language models, addressing social gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Metrics for bias measurement and bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.09294v1",
    "title": "Censorship of Online Encyclopedias: Implications for NLP Models",
    "year": 2021,
    "authors": [
      "Eddie Yang",
      "Margaret E. Roberts"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how government censorship influences training data, affecting AI outputs related to concepts like democracy, freedom, and equality, which are tied to social inequalities. It highlights disparities in representations that can impact social perceptions and biases. The focus on societal concepts and their differential treatment indicates a concern with social inequality issues.",
      "inequality_type": [
        "political",
        "informational"
      ],
      "other_detail": "Censorship's impact on social concept representations",
      "affected_populations": [
        "Chinese citizens",
        "users of Wikipedia"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Analyzing word embeddings and downstream AI applications",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.07463v1",
    "title": "Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices",
    "year": 2021,
    "authors": [
      "Marc Cheong",
      "Kobi Leins",
      "Simon Coghlan"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender representation and inclusion within computer science communities, highlighting disparities and underrepresentation of women, which are social inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender representation in research communities",
      "affected_populations": [
        "women in CS"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of research publication data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.06811v1",
    "title": "Optimal Pre-Processing to Achieve Fairness and Its Relationship with Total Variation Barycenter",
    "year": 2021,
    "authors": [
      "Farhad Farokhi"
    ],
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.OC",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on disparities related to protected attributes like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic decision-making",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Bounds and optimization of distribution distances",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.05783v2",
    "title": "Persistent Anti-Muslim Bias in Large Language Models",
    "year": 2021,
    "authors": [
      "Abubakar Abid",
      "Maheen Farooqi",
      "James Zou"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal biases related to religion, highlighting discrimination and unfair stereotypes in AI systems, which relate to social inequality issues.",
      "inequality_type": [
        "religion"
      ],
      "other_detail": "religious bias in AI systems",
      "affected_populations": [
        "Muslims"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Probing GPT-3 with prompts and analogies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.04199v1",
    "title": "Where you live matters: a spatial analysis of COVID-19 mortality",
    "year": 2021,
    "authors": [
      "Behzad Javaheri"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes spatial disparities in COVID-19 mortality within Mexico, highlighting geographic and health-related inequalities. It discusses differences across regions and municipalities, indicating social and health disparities. No mention of AI or algorithmic bias, but it addresses social and health inequalities.",
      "inequality_type": [
        "geographic",
        "health"
      ],
      "other_detail": "Focus on regional and health disparities in Mexico",
      "affected_populations": [
        "residents of northern Mexico",
        "Mexico City residents"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Spatial mapping and demographic data analysis",
      "geographic_focus": [
        "Mexico"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.01673v5",
    "title": "Characterizing Intersectional Group Fairness with Worst-Case Comparisons",
    "year": 2021,
    "authors": [
      "Avijit Ghosh",
      "Lea Genuit",
      "Mary Reagan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in AI related to social categories like race and gender, emphasizing intersectionality and fairness metrics, which directly relate to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on intersectional fairness in algorithms",
      "affected_populations": [
        "minority groups",
        "underrepresented subgroups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Proposes a worst-case comparison method for fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.00786v4",
    "title": "Do Abstractions Have Politics? Toward a More Critical Algorithm Analysis",
    "year": 2021,
    "authors": [
      "Kevin Lin"
    ],
    "categories": [
      "cs.CY",
      "K.3.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how data structures and algorithms embody values with political consequences, affecting social identities and distribution of benefits and harms, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "social",
        "discrimination"
      ],
      "other_detail": "Impacts on social identities and power dynamics",
      "affected_populations": [
        "social minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Critical analysis",
        "Case Study"
      ],
      "methodology_detail": "Affordance analysis based on STS, philosophy, HCI",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2101.00352v3",
    "title": "Characterizing Fairness Over the Set of Good Models Under Selective Labels",
    "year": 2021,
    "authors": [
      "Amanda Coston",
      "Ashesh Rambachan",
      "Alexandra Chouldechova"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in predictive models, which impacts social groups, especially in high-stakes decisions like credit scoring and recidivism prediction.",
      "inequality_type": [
        "economic",
        "racial",
        "educational"
      ],
      "other_detail": "Focuses on fairness across social groups in AI models",
      "affected_populations": [
        "credit applicants",
        "criminal defendants"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes fairness properties of predictive models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  }
]