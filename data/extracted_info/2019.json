[
  {
    "id": "http://arxiv.org/abs/1912.12012v1",
    "title": "Graduate Employment Prediction with Bias",
    "year": 2019,
    "authors": [
      "Teng Guo",
      "Feng Xia",
      "Shihao Zhen",
      "Xiaomei Bai",
      "Dongyu Zhang",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses unconscious biases affecting employment outcomes, which relate to social discrimination and inequality in education and labor markets.",
      "inequality_type": [
        "educational",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focuses on biases impacting job prospects for students",
      "affected_populations": [
        "college students"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias-aware prediction framework with GAN and LSTM",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.09318v1",
    "title": "AI and Holistic Review: Informing Human Reading in College Admissions",
    "year": 2019,
    "authors": [
      "AJ Alvero",
      "Noah Arthurs",
      "anthony lising antonio",
      "Benjamin W. Domingue",
      "Ben Gebre-Medhin",
      "Sonia Giebel",
      "Mitchell L. Stevens"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how demographic characteristics like gender and income can be inferred from application essays, highlighting potential biases in holistic review processes, which relate to social inequalities.",
      "inequality_type": [
        "income",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias in college admissions evaluation",
      "affected_populations": [
        "applicants",
        "underrepresented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Predictive modeling of demographic attributes from essays",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.08189v4",
    "title": "Learning from Discriminatory Training Data",
    "year": 2019,
    "authors": [
      "Przemyslaw A. Grabowicz",
      "Nicholas Perello",
      "Kenta Takatsu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "physics.soc-ph",
      "I.2.6; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination in AI systems trained on biased data, focusing on fairness and protected groups, which relates directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "other"
      ],
      "other_detail": "Addresses protected groups intersectionality in discrimination mitigation",
      "affected_populations": [
        "protected groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Analysis",
        "Fairness Algorithms"
      ],
      "methodology_detail": "Uses probabilistic interventions and counterfactual formulations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.00089v3",
    "title": "Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics",
    "year": 2019,
    "authors": [
      "Debjani Saha",
      "Candice Schumann",
      "Duncan C. McElfresh",
      "John P. Dickerson",
      "Michelle L. Mazurek",
      "Michael Carl Tschantz"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines public understanding of fairness metrics in AI, which relate to social bias and discrimination issues. It addresses how algorithmic fairness impacts social groups and perceptions, linking AI fairness to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focuses on social bias in machine learning fairness definitions",
      "affected_populations": [
        "general public",
        "social groups"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring comprehension via online survey",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.07398v2",
    "title": "Accuracy comparison across face recognition algorithms: Where are we on measuring race bias?",
    "year": 2019,
    "authors": [
      "Jacqueline G. Cavazos",
      "P. Jonathon Phillips",
      "Carlos D. Castillo",
      "Alice J. O'Toole"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines race bias in face recognition algorithms, highlighting social disparities related to race. It discusses how algorithm performance varies across racial groups, indicating social inequality concerns.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial bias in AI systems",
      "affected_populations": [
        "East Asian",
        "Caucasian"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzed algorithm accuracy across racial groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.07376v1",
    "title": "Algorithmic Injustices: Towards a Relational Ethics",
    "year": 2019,
    "authors": [
      "Abeba Birhane",
      "Fred Cummins"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses algorithmic bias impacting vulnerable social groups and emphasizes centering these groups through relational ethics, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on vulnerable social groups and biases",
      "affected_populations": [
        "vulnerable groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Ethics Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing ethical frameworks and existing research",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.06171v1",
    "title": "Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination",
    "year": 2019,
    "authors": [
      "Miranda Bogen",
      "Aaron Rieke",
      "Shazeda Ahmed"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses access to sensitive attribute data related to race, gender, and disparities in domains like employment, credit, and healthcare, highlighting issues of social discrimination and fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "educational"
      ],
      "other_detail": "Focus on data access affecting social disparities",
      "affected_populations": [
        "racial minorities",
        "women",
        "health patients"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzes policies and practices across domains",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.05474v1",
    "title": "Female Librarians and Male Computer Programmers? Gender Bias in Occupational Images on Digital Media Platforms",
    "year": 2019,
    "authors": [
      "Vivek Singh",
      "Mary Chayko",
      "Raj Inamdar",
      "Diana Floegel"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender stereotypes in digital media, addressing gender bias and inequality in occupational representations.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on occupational gender stereotypes in media",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Content Analysis"
      ],
      "methodology_detail": "Analyzes platform content for stereotypes",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2001.11552v1",
    "title": "Unwanted Advances in Higher Education: Uncovering Sexual Harassment Experiences in Academia with Text Mining",
    "year": 2019,
    "authors": [
      "Amir Karami",
      "Cynthia Nicole White",
      "Kayla Ford",
      "Suzanne Swan",
      "Melek Yildiz Spinel"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CL",
      "cs.CY",
      "cs.SI",
      "stat.AP",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender-based sexual harassment, a form of social inequality, in academia. It explores power dynamics and discrimination faced by women and victims reporting harassment. The focus on gender harassment and power differential indicates a clear link to social inequality issues.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender harassment in higher education",
      "affected_populations": [
        "women",
        "students"
      ],
      "methodology": [
        "Text Mining",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes survey data for hidden topics",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.04883v4",
    "title": "Roles for Computing in Social Change",
    "year": 2019,
    "authors": [
      "Rediet Abebe",
      "Solon Barocas",
      "Jon Kleinberg",
      "Karen Levy",
      "Manish Raghavan",
      "David G. Robinson"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses social problems, justice, and inequality, emphasizing social change roles for computing.",
      "inequality_type": [
        "social",
        "racial",
        "gender",
        "educational",
        "inequality"
      ],
      "other_detail": "Addresses social injustice and inequality broadly",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Conceptual framework for computational roles in social change",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.03802v3",
    "title": "Group Fairness in Bandit Arm Selection",
    "year": 2019,
    "authors": [
      "Candice Schumann",
      "Zhi Lang",
      "Nicholas Mattei",
      "John P. Dickerson"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness across social groups with protected features, aiming to mitigate societal biases in decision-making processes.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "age"
      ],
      "other_detail": "Bias correction in algorithmic decision-making",
      "affected_populations": [
        "racial groups",
        "age groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware bandit algorithms with bias correction",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.03593v1",
    "title": "Towards a Critical Race Methodology in Algorithmic Fairness",
    "year": 2019,
    "authors": [
      "Alex Hanna",
      "Emily Denton",
      "Andrew Smart",
      "Jamila Smith-Loud"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines racial categories in algorithmic fairness, addressing racial inequality and social constructs. It emphasizes structural and relational aspects of race, linking to social inequality issues.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on race and social construction",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Grounded in critical race theory and sociological work",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.02761v2",
    "title": "Measuring Social Bias in Knowledge Graph Embeddings",
    "year": 2019,
    "authors": [
      "Joseph Fisher",
      "Dave Palfrey",
      "Christos Christodoulopoulos",
      "Arpit Mittal"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to gender, religion, ethnicity, and nationality in knowledge graph embeddings, which reflect social inequalities and discrimination. It discusses how these biases encode stereotypes about social groups, impacting fairness in AI systems.",
      "inequality_type": [
        "gender",
        "religion",
        "ethnic",
        "nationality"
      ],
      "other_detail": "Biases related to social group stereotypes in AI",
      "affected_populations": [
        "women",
        "men",
        "religious groups",
        "ethnic groups",
        "nationalities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring bias in knowledge graph embeddings",
      "geographic_focus": [
        "Wikidata",
        "Freebase"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.01842v1",
    "title": "Algorithmic Discrimination: Formulation and Exploration in Deep Learning-based Face Biometrics",
    "year": 2019,
    "authors": [
      "Ignacio Serna",
      "Aythami Morales",
      "Julian Fierrez",
      "Manuel Cebrian",
      "Nick Obradovich",
      "Iyad Rahwan"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities in face recognition performance, highlighting algorithmic discrimination across groups, which relates to social bias and inequality.",
      "inequality_type": [
        "racial",
        "demographic",
        "social bias"
      ],
      "other_detail": "Focus on demographic group disparities in AI systems",
      "affected_populations": [
        "racial groups",
        "demographic groups"
      ],
      "methodology": [
        "Experiment",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Performance comparison across demographic groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.01799v1",
    "title": "Addressing Marketing Bias in Product Recommendations",
    "year": 2019,
    "authors": [
      "Mengting Wan",
      "Jianmo Ni",
      "Rishabh Misra",
      "Julian McAuley"
    ],
    "categories": [
      "cs.IR",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in product recommendations related to marketing images, which can reinforce stereotypes and underrepresent certain social groups, such as gender or niche markets. It discusses how marketing strategies influence consumer interactions, potentially perpetuating social biases in AI systems.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in marketing affecting social group representation",
      "affected_populations": [
        "women",
        "niche market groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes real-world datasets and algorithm responses",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/1912.00578v1",
    "title": "Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models",
    "year": 2019,
    "authors": [
      "Shruti Bhargava",
      "David Forsyth"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in image captioning, a social fairness issue affecting gender representation in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in datasets and models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation techniques in AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.11025v1",
    "title": "Women, politics and Twitter: Using machine learning to change the discourse",
    "year": 2019,
    "authors": [
      "Lana Cuthbertson",
      "Alex Kearney",
      "Riley Dawson",
      "Ashia Zawaduk",
      "Eve Cuthbertson",
      "Ann Gordon-Tighe",
      "Kory W Mathewson"
    ],
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender inequality in politics and online abuse targeting women, highlighting social discrimination and bias. It also examines how AI interventions can influence social discourse and fairness.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on online abuse and political gender inequality",
      "affected_populations": [
        "women in politics"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Classification and response system validation on harassment datasets",
      "geographic_focus": [
        "Canada"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.11558v1",
    "title": "FairyTED: A Fair Rating Predictor for TED Talk Data",
    "year": 2019,
    "authors": [
      "Rupam Acharyya",
      "Shouman Das",
      "Ankani Chattoraj",
      "Md. Iftekhar Tanveer"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI predictions related to speaker attributes like gender and race, addressing social bias and discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "educational"
      ],
      "other_detail": "Fairness in public speech quality prediction",
      "affected_populations": [
        "female speakers",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Models",
        "Counterfactual Fairness",
        "Neural Language Models"
      ],
      "methodology_detail": "Combines causal inference with neural language models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.10787v1",
    "title": "A Causal Inference Method for Reducing Gender Bias in Word Embedding Relations",
    "year": 2019,
    "authors": [
      "Zekun Yang",
      "Juan Feng"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, a social bias issue affecting gender equality. It focuses on mitigating gender bias in AI systems, which impacts social fairness. The work relates to social discrimination and bias in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Causal Inference",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Using causal approach to reduce gender bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.10692v1",
    "title": "Mitigate Bias in Face Recognition using Skewness-Aware Reinforcement Learning",
    "year": 2019,
    "authors": [
      "Mei Wang",
      "Weihong Deng"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition, aiming to improve fairness across racial groups, which directly relates to social racial inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial fairness in AI systems",
      "affected_populations": [
        "non-Caucasians",
        "Caucasians"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Reinforcement Learning",
        "Dataset Creation"
      ],
      "methodology_detail": "Adaptive margin learning and reinforcement learning approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.10640v1",
    "title": "Algorithmic Bias in Recidivism Prediction: A Causal Perspective",
    "year": 2019,
    "authors": [
      "Aria Khademi",
      "Vasant Honavar"
    ],
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in recidivism prediction algorithms, addressing racial inequality and social discrimination in AI systems.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "focus on racial bias in criminal justice",
      "affected_populations": [
        "African American defendants"
      ],
      "methodology": [
        "Causal Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "causal inference using FACT and Neyman-Rubin framework",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.08080v4",
    "title": "Jointly De-biasing Face Recognition and Demographic Attribute Estimation",
    "year": 2019,
    "authors": [
      "Sixue Gong",
      "Xiaoming Liu",
      "Anil K. Jain"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face recognition related to demographic groups, highlighting social disparities in algorithm performance.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Bias reduction in AI systems for social fairness",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Deep Learning",
        "Adversarial Network",
        "Experiment"
      ],
      "methodology_detail": "Disentangled feature learning for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.08556v1",
    "title": "Towards Reducing Bias in Gender Classification",
    "year": 2019,
    "authors": [
      "Komal K. Teru",
      "Aishik Chakraborty"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in gender classification systems, highlighting social discrimination. It aims to reduce racial bias, a key aspect of social inequality. The focus on bias mitigation in AI systems relates directly to social fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Addressing racial bias in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Adversarially trained autoencoder for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.06837v1",
    "title": "Dynamic Modeling and Equilibria in Fair Decision Making",
    "year": 2019,
    "authors": [
      "Joshua Williams",
      "J. Zico Kolter"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness constraints and population dynamics in decision-making systems, highlighting impacts on disadvantaged groups and the effects of misestimation, which relate to social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "income",
        "wealth"
      ],
      "other_detail": "Focus on lending and credit scoring",
      "affected_populations": [
        "disadvantaged groups",
        "poor",
        "credit applicants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Population dynamics modeling with continuous state representation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.05395v1",
    "title": "Allowing for equal opportunities for artists in music recommendation",
    "year": 2019,
    "authors": [
      "Christine Bauer"
    ],
    "categories": [
      "cs.IR",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in music recommendation systems affecting artists' opportunities, highlighting issues related to diversity, fairness, and inequality among artists, which are social inequality concerns.",
      "inequality_type": [
        "gender",
        "ethnic",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focus on artist diversity and opportunity in AI systems",
      "affected_populations": [
        "less popular artists",
        "diverse artist groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzing bias patterns and diversity metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.04931v2",
    "title": "Efficient Fair Principal Component Analysis",
    "year": 2019,
    "authors": [
      "Mohammad Mahdi Kamani",
      "Farzin Haddadpour",
      "Rana Forsati",
      "Mehrdad Mahdavi"
    ],
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.OC",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on reducing bias related to sensitive groups such as race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in dimensionality reduction for sensitive groups",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Empirical Study"
      ],
      "methodology_detail": "Fairness-aware PCA with Pareto optimality",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.08292v1",
    "title": "Fairness through Equality of Effort",
    "year": 2019,
    "authors": [
      "Wen Huang",
      "Yongkai Wu",
      "Lu Zhang",
      "Xintao Wu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on disparities between protected and unprotected groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and effort in decision-making",
      "affected_populations": [
        "protected group",
        "unprotected group"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Optimization",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness algorithms and data correction methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.04322v1",
    "title": "Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness",
    "year": 2019,
    "authors": [
      "Zhu Li",
      "Adrian Perez-Suay",
      "Gustau Camps-Valls",
      "Dino Sejdinovic"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on biases related to sensitive attributes like gender and race, which are central to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Kernel Methods",
        "Statistical Analysis"
      ],
      "methodology_detail": "Kernel dependence regularization and Gaussian processes",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.03842v2",
    "title": "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation",
    "year": 2019,
    "authors": [
      "Emily Dinan",
      "Angela Fan",
      "Adina Williams",
      "Jack Urbanek",
      "Douwe Kiela",
      "Jason Weston"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in dialogue models, reflecting social gender inequalities. It analyzes how biases in AI systems perpetuate gender disparities and proposes mitigation techniques. This directly relates to social discrimination and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.03642v3",
    "title": "Towards Understanding Gender Bias in Relation Extraction",
    "year": 2019,
    "authors": [
      "Andrew Gaut",
      "Tony Sun",
      "Shirlyn Tang",
      "Yuxin Huang",
      "Jing Qian",
      "Mai ElSherief",
      "Jieyu Zhao",
      "Diba Mirza",
      "Elizabeth Belding",
      "Kai-Wei Chang",
      "William Yang Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in AI systems, addressing social discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in relation extraction",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "evaluates bias and mitigation techniques in NRE systems",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.05755v1",
    "title": "An Introduction to Artificial Intelligence and Solutions to the Problems of Algorithmic Discrimination",
    "year": 2019,
    "authors": [
      "Nicholas Schmidt",
      "Bryce Stephens"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "68T01",
      "K.5.2; K.4.1; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic bias affecting minorities, women, and protected classes, focusing on fairness and discrimination issues in AI systems, particularly in consumer credit, which relates to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in algorithmic decision-making",
      "affected_populations": [
        "minorities",
        "women",
        "protected classes"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Legal Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluating fairness and bias in algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.03562v1",
    "title": "The State of NLP Literature: A Diachronic Analysis of the ACL Anthology",
    "year": 2019,
    "authors": [
      "Saif M. Mohammad"
    ],
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender disparities in NLP publishing, citation rates, and participation, highlighting issues of gender inequality and inclusiveness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender disparities in academic publishing",
      "affected_populations": [
        "female researchers",
        "male researchers"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of authorship and citation data over time",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.03064v3",
    "title": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation",
    "year": 2019,
    "authors": [
      "Po-Sen Huang",
      "Huan Zhang",
      "Ray Jiang",
      "Robert Stanforth",
      "Johannes Welbl",
      "Jack Rae",
      "Vishal Maini",
      "Dani Yogatama",
      "Pushmeet Kohli"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to sensitive social attributes like gender and nationality in language models, which are linked to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "nationality"
      ],
      "other_detail": "Bias in sentiment reflecting social attribute disparities",
      "affected_populations": [
        "gender groups",
        "nationality groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Counterfactual evaluation and fairness metrics applied",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.02715v3",
    "title": "Fair Allocation through Selective Information Acquisition",
    "year": 2019,
    "authors": [
      "William Cai",
      "Johann Gaebler",
      "Nikhil Garg",
      "Sharad Goel"
    ],
    "categories": [
      "cs.CY",
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in resource allocation due to unequal information access, particularly affecting marginalized groups like those without credit histories, which relates to socioeconomic inequality.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focus on resource allocation fairness and information gaps",
      "affected_populations": [
        "unbanked individuals",
        "credit-invisible groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Linear programming for targeted screening decisions",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.01509v1",
    "title": "Understanding racial bias in health using the Medical Expenditure Panel Survey data",
    "year": 2019,
    "authors": [
      "Moninder Singh",
      "Karthikeyan Natesan Ramamurthy"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in health indicators and its impact on healthcare disparities, addressing racial inequality and social bias in AI systems.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial bias in health data and models",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias mitigation techniques in predictive models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.01485v1",
    "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
    "year": 2019,
    "authors": [
      "Yi Chern Tan",
      "L. Elisa Celis"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender, race, and intersectional identities in NLP models, which are social constructs linked to inequality and discrimination.",
      "inequality_type": [
        "gender",
        "racial",
        "intersectional"
      ],
      "other_detail": "Focus on bias in language models",
      "affected_populations": [
        "women",
        "racial minorities",
        "intersectional minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement at word and sentence levels",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.01468v2",
    "title": "Auditing and Achieving Intersectional Fairness in Classification Problems",
    "year": 2019,
    "authors": [
      "Giulio Morina",
      "Viktoriia Oliinyk",
      "Julian Waton",
      "Ines Marusic",
      "Konstantinos Georgatzis"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems concerning sensitive attributes like race and gender, addressing social bias and discrimination issues. It studies intersectional fairness, which relates directly to social inequalities affecting marginalized groups. The emphasis on fairness metrics and bias mitigation in classification models relates to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on intersectional fairness in machine learning",
      "affected_populations": [
        "racial minorities",
        "women",
        "underrepresented groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Extends fairness metrics and develops bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1911.00461v1",
    "title": "On the Unintended Social Bias of Training Language Generation Models with Data from Local Media",
    "year": 2019,
    "authors": [
      "Omar U. Florez"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Mitigates gender bias in AI-generated text",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Decoupled architecture for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.14120v1",
    "title": "What is Fair? Exploring Pareto-Efficiency for Fairness Constrained Classifiers",
    "year": 2019,
    "authors": [
      "Ananth Balashankar",
      "Alyssa Lees",
      "Chris Welty",
      "Lakshminarayanan Subramanian"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness constraints in AI models across social groups, aiming to mitigate bias related to race and gender, which are key social inequality factors.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates subgroup performance and fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.13913v4",
    "title": "Toward Gender-Inclusive Coreference Resolution",
    "year": 2019,
    "authors": [
      "Yang Trista Cao",
      "Hal Daum√© III"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender biases in AI systems, highlighting social discrimination and bias issues related to gender identities, including trans and cis stakeholders.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in NLP systems",
      "affected_populations": [
        "trans people",
        "cis people"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Analysis"
      ],
      "methodology_detail": "develops datasets and conducts bias analysis",
      "geographic_focus": [
        "English text"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.11452v1",
    "title": "Fairness Sample Complexity and the Case for Human Intervention",
    "year": 2019,
    "authors": [
      "Ananth Balashankar",
      "Alyssa Lees"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in machine learning, subgroup disparities, and the need for human intervention, which relate to social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focuses on fairness and subgroup sample complexity in AI",
      "affected_populations": [
        "subgroups by sensitive variables"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bounds on sample complexity for fair learning",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.10872v1",
    "title": "Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition",
    "year": 2019,
    "authors": [
      "Ninareh Mehrabi",
      "Thamme Gowda",
      "Fred Morstatter",
      "Nanyun Peng",
      "Aram Galstyan"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI systems, highlighting social discrimination against women.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in named entity recognition",
      "affected_populations": [
        "women",
        "female names"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and dataset analysis",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.12583v1",
    "title": "Solidarity should be a core ethical principle of Artificial Intelligence",
    "year": 2019,
    "authors": [
      "Miguel Luengo-Oroz"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses AI's impact on inequality, redistribution, and societal burdens, emphasizing solidarity to prevent increased disparities.",
      "inequality_type": [
        "economic",
        "social",
        "inequality"
      ],
      "other_detail": "Focuses on societal fairness and long-term social impacts",
      "affected_populations": [
        "disadvantaged groups",
        "marginalized communities"
      ],
      "methodology": [
        "Ethics Analysis"
      ],
      "methodology_detail": "Philosophical and normative evaluation of AI principles",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.10486v3",
    "title": "Does Gender Matter? Towards Fairness in Dialogue Systems",
    "year": 2019,
    "authors": [
      "Haochen Liu",
      "Jamell Dacon",
      "Wenqi Fan",
      "Hui Liu",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates fairness issues related to gender and race in dialogue systems, highlighting social biases and prejudice. It addresses how AI inherits and amplifies societal inequalities, focusing on social discrimination. The study aims to understand and mitigate bias, directly engaging with social inequality concerns.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI dialogue systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Constructs benchmark dataset and proposes bias mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.07162v3",
    "title": "Conditional Learning of Fair Representations",
    "year": 2019,
    "authors": [
      "Han Zhao",
      "Amanda Coston",
      "Tameem Adel",
      "Geoffrey J. Gordon"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair representations in classification, aiming to reduce disparities across demographic groups, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Addresses algorithmic fairness across social groups",
      "affected_populations": [
        "demographic subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Fair representation learning algorithms and theoretical proofs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.06155v1",
    "title": "GeoSES -- um √çndice Socioecon√¥mico para Estudos de Sa√∫de no Brasil",
    "year": 2019,
    "authors": [
      "Ligia Vizeu Barrozo",
      "Michel Fornaciali",
      "Carmen Diva Saldiva de Andr√©",
      "Guilherme Augusto Zimeo Morais",
      "Giselle Mansur",
      "William Cabral-Miranda",
      "Jo√£o Ricardo Sato",
      "Edson Amaro J√∫nior"
    ],
    "categories": [
      "cs.OH",
      "stat.AP"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper develops an index to measure socioeconomic disparities affecting health outcomes in Brazil, addressing social inequality dimensions.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "geographic"
      ],
      "other_detail": "Focus on health inequalities related to socioeconomic factors",
      "affected_populations": [
        "Brazilian population"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Principal Component Analysis"
      ],
      "methodology_detail": "Constructs index from census data",
      "geographic_focus": [
        "Brazil"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.03676v4",
    "title": "Representation Learning with Statistical Independence to Mitigate Bias",
    "year": 2019,
    "authors": [
      "Ehsan Adeli",
      "Qingyu Zhao",
      "Adolf Pfefferbaum",
      "Edith V. Sullivan",
      "Li Fei-Fei",
      "Juan Carlos Niebles",
      "Kilian M. Pohl"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in AI, related to social groups such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in machine learning models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "patients"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training",
        "Statistical Analysis"
      ],
      "methodology_detail": "Using adversarial loss to reduce bias dependence",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.02321v1",
    "title": "The Impact of Data Preparation on the Fairness of Software Systems",
    "year": 2019,
    "authors": [
      "In√™s Valentim",
      "Nuno Louren√ßo",
      "Nuno Antunes"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in machine learning models, focusing on biases related to sensitive attributes like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes fairness metrics across datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1910.06144v2",
    "title": "What does it mean to solve the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems",
    "year": 2019,
    "authors": [
      "Javier Sanchez-Monedero",
      "Lina Dencik",
      "Lilian Edwards"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4; J.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination and bias mitigation in hiring systems, focusing on social groups such as protected classes, and discusses legal and social implications related to inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "disability",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias and discrimination in employment",
      "affected_populations": [
        "protected groups",
        "job applicants"
      ],
      "methodology": [
        "Literature Review",
        "System Design",
        "Legal Analysis"
      ],
      "methodology_detail": "Analyzes system design, legal context, and bias mitigation strategies",
      "geographic_focus": [
        "UK"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.11414v1",
    "title": "Inequality is rising where social network segregation interacts with urban topology",
    "year": 2019,
    "authors": [
      "Gerg≈ë T√≥th",
      "Johannes Wachs",
      "Riccardo Di Clemente",
      "√Åkos Jakobi",
      "Bence S√°gv√°ri",
      "J√°nos Kert√©sz",
      "Bal√°zs Lengyel"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how urban spatial structures influence social network fragmentation and income inequality, addressing socioeconomic disparities linked to geographic and urban planning factors.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on urban spatial factors and social network segregation",
      "affected_populations": [
        "urban residents",
        "city inhabitants"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of online social network data and urban spatial features",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.09758v3",
    "title": "Empirical Analysis of Multi-Task Learning for Reducing Model Bias in Toxic Comment Detection",
    "year": 2019,
    "authors": [
      "Ameya Vaidya",
      "Feng Mai",
      "Yue Ning"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in AI models related to identity groups, which reflects social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Focus on reducing bias against identity groups in toxicity detection",
      "affected_populations": [
        "racial minorities",
        "gender minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Multi-task learning with attention layer for bias reduction",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.09138v1",
    "title": "Uncovering Sociological Effect Heterogeneity using Machine Learning",
    "year": 2019,
    "authors": [
      "Jennie E. Brand",
      "Jiahui Xu",
      "Bernard Koch",
      "Pablo Geraldo"
    ],
    "categories": [
      "stat.OT",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines treatment effects on wages, a key aspect of economic inequality, and discusses heterogeneity related to social groups. It addresses social disparities in educational and economic outcomes using sociological and causal inference methods.",
      "inequality_type": [
        "economic",
        "educational"
      ],
      "other_detail": "Focus on college effects on wages",
      "affected_populations": [
        "students",
        "workers"
      ],
      "methodology": [
        "Machine Learning",
        "Causal Inference",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using causal trees and matching techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.08518v3",
    "title": "Bias In, Bias Out? Evaluating the Folk Wisdom",
    "year": 2019,
    "authors": [
      "Ashesh Rambachan",
      "Jonathan Roth"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in decision-making processes affecting social groups, highlighting discriminatory selection and bias reversal, which relate directly to social inequalities such as racial or socioeconomic discrimination.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on bias in algorithmic decision-making processes",
      "affected_populations": [
        "racial minorities",
        "disadvantaged groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Simulation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias dynamics through models and NYC dataset",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.08081v1",
    "title": "A Distributed Fair Machine Learning Framework with Private Demographic Data Protection",
    "year": 2019,
    "authors": [
      "Hui Hu",
      "Yijun Liu",
      "Zhen Wang",
      "Chao Lan"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning, which relates to social discrimination and bias. It focuses on protecting demographic data to ensure equitable treatment across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "privacy-preserving fairness in AI",
      "affected_populations": [
        "social groups",
        "demographic minorities"
      ],
      "methodology": [
        "Fair Machine Learning",
        "Distributed Framework",
        "Theoretical Analysis"
      ],
      "methodology_detail": "privacy-aware fair learning methods",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.08982v1",
    "title": "AdaFair: Cumulative Fairness Adaptive Boosting",
    "year": 2019,
    "authors": [
      "Vasileios Iosifidis",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on discrimination related to protected attributes like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and discrimination in decision-making",
      "affected_populations": [
        "protected groups",
        "non-protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware boosting with class imbalance handling",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.06429v1",
    "title": "Recommendation or Discrimination?: Quantifying Distribution Parity in Information Retrieval Systems",
    "year": 2019,
    "authors": [
      "Rinat Khaziev",
      "Bryce Casavant",
      "Pearce Washabaugh",
      "Amy A. Winecoff",
      "Matthew Graham"
    ],
    "categories": [
      "stat.ML",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in information retrieval systems related to protected variables like race and skin tone, which are social categories linked to inequality.",
      "inequality_type": [
        "racial",
        "ethnic",
        "disability",
        "age"
      ],
      "other_detail": "Focuses on bias in AI recommendation systems",
      "affected_populations": [
        "racial minorities",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Testing distribution parity in recommendations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.05583v1",
    "title": "Minimizing Margin of Victory for Fair Political and Educational Districting",
    "year": 2019,
    "authors": [
      "Ana-Andreea Stoica",
      "Abhijnan Chakraborty",
      "Palash Dey",
      "Krishna P. Gummadi"
    ],
    "categories": [
      "cs.SI",
      "cs.GT",
      "cs.MA"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fair districting to reduce racial segregation and gerrymandering, directly related to social inequalities. It focuses on equitable partitioning of populations to mitigate bias and discrimination. The application in UK and US contexts highlights its relevance to social inequality issues.",
      "inequality_type": [
        "racial",
        "educational"
      ],
      "other_detail": "Focus on racial segregation and political fairness",
      "affected_populations": [
        "racial minorities",
        "students",
        "voters"
      ],
      "methodology": [
        "Algorithm Design",
        "Heuristic Algorithms",
        "Case Study"
      ],
      "methodology_detail": "Designing algorithms to promote fairness in districting",
      "geographic_focus": [
        "UK",
        "US"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.05282v1",
    "title": "Algorithmic and Economic Perspectives on Fairness",
    "year": 2019,
    "authors": [
      "David C. Parkes",
      "Rakesh V. Vohra",
      "other workshop participants"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses algorithmic fairness concerns related to discrimination and social equity issues, addressing biases against groups based on ethnicity, gender, and other social categories.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and discrimination in algorithmic decision-making",
      "affected_populations": [
        "ethnic minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Review of historical and ethical perspectives on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.04386v1",
    "title": "Attesting Biases and Discrimination using Language Semantics",
    "year": 2019,
    "authors": [
      "Xavier Ferrer Aran",
      "Jose M. Such",
      "Natalia Criado"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "68T50"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in language that can lead to unfair treatment by AI systems, which relates to social discrimination and fairness issues affecting different groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability",
        "educational"
      ],
      "other_detail": "Focus on language biases and fairness in AI systems",
      "affected_populations": [
        "minority groups",
        "disadvantaged individuals"
      ],
      "methodology": [
        "Natural Language Processing",
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing language biases and ethical implications",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.03567v1",
    "title": "What You See Is What You Get? The Impact of Representation Criteria on Human Bias in Hiring",
    "year": 2019,
    "authors": [
      "Andi Peng",
      "Besmira Nushi",
      "Emre Kiciman",
      "Kori Inkpen",
      "Siddharth Suri",
      "Ece Kamar"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in hiring decisions, a key aspect of social inequality. It examines how representation criteria influence gender bias, directly addressing gender discrimination issues. The focus on human bias in decision-making relates to social disparities based on gender.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Controlled experiments measuring human decision-making",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.03166v1",
    "title": "Equalizing Recourse across Groups",
    "year": 2019,
    "authors": [
      "Vivek Gupta",
      "Pegah Nokhiz",
      "Chitradeep Dutta Roy",
      "Suresh Venkatasubramanian"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in decision-making, recourse, and equitable treatment across demographic groups, addressing social discrimination concerns.",
      "inequality_type": [
        "economic",
        "educational",
        "demographic"
      ],
      "other_detail": "Focus on fairness and equitable recourse in AI decisions",
      "affected_populations": [
        "demographic groups",
        "disadvantaged individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Regularized Optimization",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Minimizing recourse disparity across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.03013v1",
    "title": "Approaching Machine Learning Fairness through Adversarial Network",
    "year": 2019,
    "authors": [
      "Xiaoqian Wang",
      "Heng Huang"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing discrimination related to sensitive features like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Network",
        "Experiment"
      ],
      "methodology_detail": "Fairness optimization via adversarial training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.02224v2",
    "title": "Examining Gender Bias in Languages with Grammatical Gender",
    "year": 2019,
    "authors": [
      "Pei Zhou",
      "Weijia Shi",
      "Jieyu Zhao",
      "Kuan-Hao Huang",
      "Muhao Chen",
      "Ryan Cotterell",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social bias affecting gender equality. It discusses bias mitigation in AI systems, which impacts social perceptions and fairness. The focus on gender bias aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and mitigation in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.02156v1",
    "title": "Bidding Strategies with Gender Nondiscrimination: Constraints for Online Ad Auctions",
    "year": 2019,
    "authors": [
      "Milad Nasr",
      "Michael Tschantz"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender-based discrimination in online ad auctions, a social inequality issue. It focuses on fairness and bias in algorithmic decision-making affecting gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "addressing gender nondiscrimination in advertising",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Mathematical Analysis",
        "Simulations"
      ],
      "methodology_detail": "Analyzes costs and strategies for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.01251v1",
    "title": "Avoiding Resentment Via Monotonic Fairness",
    "year": 2019,
    "authors": [
      "Guy W. Cole",
      "Sinead A. Williamson"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making, focusing on demographic balance and individual resentment, which relate to social discrimination and bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "demographic"
      ],
      "other_detail": "Fairness in AI decision boundaries",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using constrained models for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.00982v1",
    "title": "Quantifying Infra-Marginality and Its Trade-off with Group Fairness",
    "year": 2019,
    "authors": [
      "Arpita Biswas",
      "Siddharth Barman",
      "Amit Deshpande",
      "Amit Sharma"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in decision-making affecting social groups, such as race and gender, and addresses fairness in AI systems, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on individual and group fairness trade-offs in AI",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Theoretical Analysis",
        "Empirical Analysis"
      ],
      "methodology_detail": "Includes theoretical proofs and empirical validation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.00871v3",
    "title": "It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution",
    "year": 2019,
    "authors": [
      "Rowan Hall Maudslay",
      "Hila Gonen",
      "Ryan Cotterell",
      "Simone Teufel"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, a form of social bias, and proposes methods to mitigate it, directly engaging with gender inequality issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Empirical comparison of bias mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1909.00066v3",
    "title": "Counterfactual Risk Assessments, Evaluation, and Fairness",
    "year": 2019,
    "authors": [
      "Amanda Coston",
      "Alan Mishler",
      "Edward H. Kennedy",
      "Alexandra Chouldechova"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and evaluation of risk assessments in high-stakes decisions, which relate to social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on fairness in decision-making tools",
      "affected_populations": [
        "children",
        "criminal justice",
        "medical patients"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Counterfactual fairness metrics and doubly robust estimation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.11018v1",
    "title": "A cross-sectional study of social inequities in medical crowdfunding campaigns in the United States",
    "year": 2019,
    "authors": [
      "Nora Kenworthy",
      "Zhihang Dong",
      "Anne Montgomery",
      "Emily Fuller",
      "Lauren Berliner"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in medical crowdfunding related to race, gender, and age, highlighting systemic inequalities and differential outcomes among social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Focus on health-related social disparities in crowdfunding",
      "affected_populations": [
        "non-white users",
        "women organizers",
        "marginalized groups"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of campaign data and outcomes",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.08717v1",
    "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
    "year": 2019,
    "authors": [
      "Mahault Garnerin",
      "Solange Rossato",
      "Laurent Besacier"
    ],
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender imbalance in broadcast data and its impact on ASR performance, highlighting gender representation disparities. It discusses how data imbalance affects women, indicating a focus on gender inequality in AI systems. The analysis relates to social bias and fairness issues in technology.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on media representation and AI performance",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing data imbalance and performance metrics",
      "geographic_focus": [
        "France"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.08939v3",
    "title": "AI and Accessibility: A Discussion of Ethical Considerations",
    "year": 2019,
    "authors": [
      "Meredith Ringel Morris"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses accessibility barriers faced by people with disabilities, highlighting social and ethical issues related to societal inclusion and bias in AI systems.",
      "inequality_type": [
        "disability",
        "health",
        "social"
      ],
      "other_detail": "Focus on societal accessibility barriers and ethical considerations",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzing ethical challenges in AI accessibility",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.04913v1",
    "title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age",
    "year": 2019,
    "authors": [
      "Kimmo K√§rkk√§inen",
      "Jungseock Joo"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face datasets, impacting fairness and accuracy across racial groups.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on dataset bias and model fairness",
      "affected_populations": [
        "non-White racial groups",
        "women"
      ],
      "methodology": [
        "Dataset Creation",
        "Machine Learning",
        "Evaluation"
      ],
      "methodology_detail": "Constructs balanced face dataset and evaluates models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.06165v1",
    "title": "Oxford Handbook on AI Ethics Book Chapter on Race and Gender",
    "year": 2019,
    "authors": [
      "Timnit Gebru"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses biases in AI systems affecting marginalized groups and examines societal and political factors contributing to inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on bias and societal impact of AI systems",
      "affected_populations": [
        "dark skinned women",
        "African Americans",
        "lower socioeconomic classes"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Review of studies and societal implications",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.02810v1",
    "title": "Debiasing Embeddings for Reduced Gender Bias in Text Classification",
    "year": 2019,
    "authors": [
      "Flavien Prost",
      "Nithum Thain",
      "Tolga Bolukbasi"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in AI, a social inequality issue, and its impact on downstream classification tasks.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in text classification",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes debiasing techniques and their effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.02641v2",
    "title": "Paired-Consistency: An Example-Based Model-Agnostic Approach to Fairness Regularization in Machine Learning",
    "year": 2019,
    "authors": [
      "Yair Horesh",
      "Noa Haas",
      "Elhanan Mishraky",
      "Yehezkel S. Resheff",
      "Shir Meir Lador"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on non-discrimination related to protected attributes such as gender, age, and race, which are key social inequality factors.",
      "inequality_type": [
        "gender",
        "age",
        "race",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fairness regularization without explicit protected attribute access",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness regularization via paired consistency metric",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.01024v3",
    "title": "What is the Point of Fairness? Disability, AI and The Complexity of Justice",
    "year": 2019,
    "authors": [
      "Cynthia L. Bennett",
      "Os Keyes"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI's impact on disabled people, highlighting structural injustices and power dynamics, which relate to social inequality issues.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Focus on structural injustices in disability and AI",
      "affected_populations": [
        "disabled people"
      ],
      "methodology": [
        "Case Study",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes AI case studies and disability theory",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.00176v2",
    "title": "FairSight: Visual Analytics for Fairness in Decision Making",
    "year": 2019,
    "authors": [
      "Yongsu Ahn",
      "Yu-Ru Lin"
    ],
    "categories": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision making, which relates to social discrimination and bias in AI systems affecting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social bias"
      ],
      "other_detail": "Focus on fairness and bias in algorithmic decision making",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Visual Analytics",
        "Case Study",
        "User Study"
      ],
      "methodology_detail": "Design and evaluation of a fairness visualization system",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1908.00625v1",
    "title": "Learning about spatial inequalities: Capturing the heterogeneity in the urban environment",
    "year": 2019,
    "authors": [
      "J. Siqueira-Gay",
      "M. A. Giannotti",
      "M. Sester"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes spatial inequalities in urban services, highlighting socioeconomic disparities among low-income populations.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on urban spatial disparities and access to services",
      "affected_populations": [
        "low-income residents",
        "peripheral urban populations"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Multidimensional spatial analysis of urban data",
      "geographic_focus": [
        "large city of the global south"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.09013v1",
    "title": "Conscientious Classification: A Data Scientist's Guide to Discrimination-Aware Classification",
    "year": 2019,
    "authors": [
      "Brian d'Alessandro",
      "Cathy O'Neil",
      "Tom LaGatta"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination in AI systems, addressing social bias and fairness issues that impact marginalized groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on discrimination mitigation in data science practices",
      "affected_populations": [
        "minority groups",
        "socially marginalized"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing",
        "System Design"
      ],
      "methodology_detail": "Analyzes practices and proposes mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.08968v2",
    "title": "Infant Mortality Prediction using Birth Certificate Data",
    "year": 2019,
    "authors": [
      "Antonia Saravanou",
      "Clemens Noelke",
      "Nicholas Huntington",
      "Dolores Acevedo-Garcia",
      "Dimitrios Gunopulos"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial disparities in infant mortality and compares models across racial groups, addressing social inequalities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in infant mortality",
      "affected_populations": [
        "racial groups",
        "ethnic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Feature importance and model comparison across groups",
      "geographic_focus": [
        "USA"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.08922v1",
    "title": "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990",
    "year": 2019,
    "authors": [
      "Melvin Wevers"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in historical newspapers using word embeddings, addressing gender-related social bias and inequality. It analyzes how language reflects societal gender biases over time, linking to social inequality concepts.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "society"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using word embeddings to measure language bias over time",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.08646v1",
    "title": "Fair quantile regression",
    "year": 2019,
    "authors": [
      "Dana Yang",
      "John Lafferty",
      "David Pollard"
    ],
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in quantile regression related to protected attributes like race, aiming to reduce disparities in estimated outcomes across social groups.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial disparities in birth weights",
      "affected_populations": [
        "birth mothers",
        "newborns"
      ],
      "methodology": [
        "Statistical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Empirical process analysis and adjustment procedure",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.07892v1",
    "title": "Global AI Ethics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence",
    "year": 2019,
    "authors": [
      "Alexa Hagerty",
      "Igor Rubinov"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses AI's role in entrenching social divides and exacerbating inequalities among marginalized groups globally.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender",
        "educational",
        "health",
        "geographic"
      ],
      "other_detail": "Focus on social impacts and inequality patterns worldwide",
      "affected_populations": [
        "marginalized groups",
        "low-income countries",
        "diverse social groups"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study"
      ],
      "methodology_detail": "Review of social science scholarship across regions",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.07253v1",
    "title": "Fairness and Diversity in the Recommendation and Ranking of Participatory Media Content",
    "year": 2019,
    "authors": [
      "Muskaan",
      "Mehak Preet Dhaliwal",
      "Aaditeshwar Seth"
    ],
    "categories": [
      "cs.SI",
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair and diverse content recommendation for marginalized rural communities, addressing social disparities in access and representation.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "geographic"
      ],
      "other_detail": "Focus on rural, low-income, less-literate populations",
      "affected_populations": [
        "rural communities",
        "low-income users"
      ],
      "methodology": [
        "Model Development",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluation using call-logs and fairness metrics",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.07237v1",
    "title": "FAHT: An Adaptive Fairness-aware Decision Tree Classifier",
    "year": 2019,
    "authors": [
      "Wenbin Zhang",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making systems, focusing on discrimination related to sensitive attributes, which are linked to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic decision-making systems",
      "affected_populations": [
        "minority groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness-aware streaming decision tree algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.06260v1",
    "title": "Counterfactual Reasoning for Fair Clinical Risk Prediction",
    "year": 2019,
    "authors": [
      "Stephen Pfohl",
      "Tony Duan",
      "Daisy Yi Ding",
      "Nigam H. Shah"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in healthcare AI, focusing on disparities affecting underrepresented groups, which relates to social inequalities such as health and racial disparities.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in clinical risk prediction models",
      "affected_populations": [
        "underrepresented groups",
        "patients with disparities"
      ],
      "methodology": [
        "Machine Learning",
        "Counterfactual Analysis",
        "Variational Autoencoder"
      ],
      "methodology_detail": "Counterfactual fairness with generative models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of disparities",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.04103v1",
    "title": "Historical comparison of gender inequality in scientific careers across countries and disciplines",
    "year": 2019,
    "authors": [
      "Junming Huang",
      "Alexander J. Gates",
      "Roberta Sinatra",
      "Albert-Laszlo Barabasi"
    ],
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender disparities in academic careers, addressing gender inequality.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bibliometric longitudinal career analysis",
      "geographic_focus": [
        "83 countries"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.02227v2",
    "title": "Toward Fairness in AI for People with Disabilities: A Research Roadmap",
    "year": 2019,
    "authors": [
      "Anhong Guo",
      "Ece Kamar",
      "Jennifer Wortman Vaughan",
      "Hanna Wallach",
      "Meredith Ringel Morris"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI's impact on people with disabilities, addressing fairness and potential discrimination, which relates to social inequality issues.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Focus on fairness and inclusion for disabled populations",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Risk Assessment",
        "Literature Review"
      ],
      "methodology_detail": "Assessing AI impacts on disability groups",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.01671v1",
    "title": "Quantifying Algorithmic Biases over Time",
    "year": 2019,
    "authors": [
      "Vivek K. Singh",
      "Ishaan Singh"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to gender over time, highlighting social discrimination issues in algorithms.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias dynamics",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Bias variation measurement over time",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.01439v2",
    "title": "Operationalizing Individual Fairness with Pairwise Fair Representations",
    "year": 2019,
    "authors": [
      "Preethi Lahoti",
      "Krishna P. Gummadi",
      "Gerhard Weikum"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems, focusing on social discrimination and bias mitigation, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "social",
        "fairness"
      ],
      "other_detail": "Focuses on fairness in algorithmic decision-making",
      "affected_populations": [
        "social groups",
        "individuals subject to bias"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Modeling",
        "Experiment"
      ],
      "methodology_detail": "Learning fair representations without explicit human-defined metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.00510v1",
    "title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to Shine a Light on Workplace Sexism and Sexual Harassment",
    "year": 2019,
    "authors": [
      "Amir Karami",
      "Suzanne C. Swan",
      "Cynthia Nicole White",
      "Kayla Ford"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates workplace sexism and sexual harassment, which are forms of social gender inequality, using text mining on online experiences.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on workplace gender discrimination and harassment",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Text mining and thematic analysis of online reports",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.00020v2",
    "title": "Training individually fair ML models with Sensitive Subspace Robustness",
    "year": 2019,
    "authors": [
      "Mikhail Yurochkin",
      "Amanda Bower",
      "Yuekai Sun"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems related to gender and racial biases, which are social inequalities. It focuses on algorithmic fairness to prevent discriminatory impacts on marginalized groups. The work aims to reduce social bias in machine learning applications.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Distributionally Robust Optimization"
      ],
      "methodology_detail": "Enforcing fairness during model training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.03834v1",
    "title": "Differences in Online Course Usage and IP Geolocation Bias by Local Economic Profile",
    "year": 2019,
    "authors": [
      "Daniela Ganelin"
    ],
    "categories": [
      "cs.CY",
      "cs.NI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines socioeconomic disparities in MOOC registration and geolocation bias, highlighting inequalities related to economic status and geographic location.",
      "inequality_type": [
        "economic",
        "geographic",
        "digital"
      ],
      "other_detail": "Bias in geolocation accuracy affecting distressed areas",
      "affected_populations": [
        "students from distressed areas",
        "MOOC registrants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes registration data and geolocation accuracy",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.10490v1",
    "title": "Age and gender bias in pedestrian detection algorithms",
    "year": 2019,
    "authors": [
      "Martim Brandao"
    ],
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in pedestrian detection related to age and gender, which are social categories, and discusses ethical implications and fairness issues in AI systems.",
      "inequality_type": [
        "age",
        "gender"
      ],
      "other_detail": "Bias in AI performance across social groups",
      "affected_populations": [
        "children",
        "adults",
        "male",
        "female"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Analyzes performance disparities using extended dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1907.03827v1",
    "title": "FairST: Equitable Spatial and Temporal Demand Prediction for New Mobility Systems",
    "year": 2019,
    "authors": [
      "An Yan",
      "Bill Howe"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly focuses on fairness in demand prediction for mobility systems, aiming to reduce socioeconomic disparities and improve equity across demographic groups.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "urban-rural"
      ],
      "other_detail": "Focuses on fairness in urban mobility demand prediction",
      "affected_populations": [
        "urban residents",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metrics",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Incorporates fairness regularization and novel metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.09218v4",
    "title": "FlipTest: Fairness Testing via Optimal Transport",
    "year": 2019,
    "authors": [
      "Emily Black",
      "Samuel Yeom",
      "Matt Fredrikson"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination in classifiers, focusing on fairness across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focus on algorithmic discrimination and fairness testing",
      "affected_populations": [
        "social groups",
        "individuals in protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using optimal transport for fairness testing",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.09208v3",
    "title": "Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices",
    "year": 2019,
    "authors": [
      "Manish Raghavan",
      "Solon Barocas",
      "Jon Kleinberg",
      "Karen Levy"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias and fairness in algorithmic hiring, addressing social discrimination issues related to race, gender, and other social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "educational",
        "social bias"
      ],
      "other_detail": "Focus on bias mitigation in employment assessments",
      "affected_populations": [
        "job candidates",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Qualitative Study",
        "Legal Analysis"
      ],
      "methodology_detail": "Analyzes vendor practices and legal implications",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.08976v1",
    "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    "year": 2019,
    "authors": [
      "Tony Sun",
      "Andrew Gaut",
      "Shirlyn Tang",
      "Yuxin Huang",
      "Mai ElSherief",
      "Jieyu Zhao",
      "Diba Mirza",
      "Elizabeth Belding",
      "Kai-Wei Chang",
      "William Yang Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in NLP, a social inequality issue affecting gender fairness and representation.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review"
      ],
      "methodology_detail": "review of bias recognition and mitigation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.07337v1",
    "title": "Measuring Bias in Contextualized Word Representations",
    "year": 2019,
    "authors": [
      "Keita Kurita",
      "Nidhi Vyas",
      "Ayush Pareek",
      "Alan W Black",
      "Yulia Tsvetkov"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in AI, specifically gender bias, which relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and case study evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.05993v1",
    "title": "Conceptor Debiasing of Word Representations Evaluated on WEAT",
    "year": 2019,
    "authors": [
      "Saket Karve",
      "Lyle Ungar",
      "Jo√£o Sedoc"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in word embeddings related to race and gender, which are social inequalities, and evaluates fairness using WEAT, a social bias test.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on social bias mitigation in AI representations",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and debiasing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.03843v2",
    "title": "Learning Fair Naive Bayes Classifiers by Discovering and Eliminating Discrimination Patterns",
    "year": 2019,
    "authors": [
      "YooJung Choi",
      "Golnoosh Farnadi",
      "Behrouz Babaki",
      "Guy Van den Broeck"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on discrimination patterns related to sensitive attributes, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and discrimination in algorithmic decision-making",
      "affected_populations": [
        "social groups",
        "individuals with sensitive attributes"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Discovering and eliminating discrimination patterns in classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.03695v1",
    "title": "Gendered Pronoun Resolution using BERT and an extractive question answering formulation",
    "year": 2019,
    "authors": [
      "Rakesh Chada"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in coreference resolution, highlighting social gender bias issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in NLP systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Using BERT and QA formulations for pronoun resolution",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.02775v1",
    "title": "Fair Division Without Disparate Impact",
    "year": 2019,
    "authors": [
      "Alexander Peysakhovich",
      "Christian Kroer"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in resource allocation across protected classes, focusing on disparities related to social groups such as race and socioeconomic status.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "disparate impact"
      ],
      "other_detail": "Focuses on fairness constraints in algorithmic division mechanisms",
      "affected_populations": [
        "protected classes",
        "social groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Modifies and analyzes fair division algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.02164v2",
    "title": "Data preprocessing to mitigate bias: A maximum entropy based approach",
    "year": 2019,
    "authors": [
      "L. Elisa Celis",
      "Vijay Keswani",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias mitigation in data related to social attributes like gender and race, aiming to reduce social bias in AI applications.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on fairness and representation in data preprocessing",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Statistical Analysis"
      ],
      "methodology_detail": "Maximum entropy approach for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.01747v1",
    "title": "Balanced Ranking with Diversity Constraints",
    "year": 2019,
    "authors": [
      "Ke Yang",
      "Vasilis Gkatzelis",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and representation issues related to social groups, aiming to balance in-group fairness and diversity constraints, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness across multiple sensitive attributes",
      "affected_populations": [
        "disadvantaged groups",
        "minority populations"
      ],
      "methodology": [
        "Optimization",
        "Experimental Evaluation"
      ],
      "methodology_detail": "Formulates constraints as integer linear programs",
      "geographic_focus": null,
      "ai_relationship": "AI as fairness measurement and balancing tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.00742v1",
    "title": "Gender-preserving Debiasing for Pre-trained Word Embeddings",
    "year": 2019,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in word embeddings, a form of social discrimination. It aims to reduce stereotypical biases that perpetuate gender inequality in AI applications. This directly relates to social inequality issues concerning gender discrimination.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias reduction in AI language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.00591v1",
    "title": "Evaluating Gender Bias in Machine Translation",
    "year": 2019,
    "authors": [
      "Gabriel Stanovsky",
      "Noah A. Smith",
      "Luke Zettlemoyer"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in machine translation, addressing social discrimination related to gender roles and fairness in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Morphological Analysis"
      ],
      "methodology_detail": "Evaluation protocol for gender bias in translation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.00285v2",
    "title": "Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination",
    "year": 2019,
    "authors": [
      "Nathan Kallus",
      "Xiaojie Mao",
      "Angela Zhou"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in algorithmic decisions affecting protected groups, addressing social fairness issues.",
      "inequality_type": [
        "racial",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on protected class membership inference",
      "affected_populations": [
        "racial minorities",
        "patients",
        "borrowers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Assessing disparities with data combination and bounds",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.00128v1",
    "title": "Achieving Fairness in Determining Medicaid Eligibility through Fairgroup Construction",
    "year": 2019,
    "authors": [
      "Boli Fang",
      "Miao Jiang",
      "Jerry Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems affecting Medicaid eligibility, a social resource allocation issue, highlighting potential inequalities in decision-making processes.",
      "inequality_type": [
        "economic",
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in social resource distribution",
      "affected_populations": [
        "Medicaid applicants",
        "low-income individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Fairgroup Construction",
        "Experiment"
      ],
      "methodology_detail": "Improving fairness in regressive classifiers",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1906.11891v1",
    "title": "Characterizing Bias in Classifiers using Generative Models",
    "year": 2019,
    "authors": [
      "Daniel McDuff",
      "Shuang Ma",
      "Yale Song",
      "Ashish Kapoor"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases in classifiers related to race and gender, which are social inequalities. It aims to characterize and analyze these biases systematically. The abstract explicitly mentions racial and gender biases in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias characterization in AI classifiers",
      "affected_populations": [
        "minorities",
        "women",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using generative models and Bayesian Optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.12801v2",
    "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
    "year": 2019,
    "authors": [
      "Yusu Qian",
      "Urwa Muaz",
      "Ben Zhang",
      "Jae Won Hyun"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social discrimination. It aims to mitigate gender bias in AI outputs, directly relating to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focused on gender bias in AI language models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Loss Function Modification"
      ],
      "methodology_detail": "Bias mitigation via loss function adjustment",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.12744v2",
    "title": "Fair Decision Making using Privacy-Protected Data",
    "year": 2019,
    "authors": [
      "Satya Kuppam",
      "Ryan Mckenna",
      "David Pujol",
      "Michael Hay",
      "Ashwin Machanavajjhala",
      "Gerome Miklau"
    ],
    "categories": [
      "cs.DB"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how privacy-preserving mechanisms impact fairness across social groups in decision-making, highlighting potential disproportionate effects on marginalized populations.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on fairness in privacy-protected data use",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Empirical Analysis",
        "Fairness Measures",
        "Differential Privacy"
      ],
      "methodology_detail": "Analyzes tradeoffs in privacy and fairness impacts",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.12728v1",
    "title": "Fairness and Missing Values",
    "year": 2019,
    "authors": [
      "Fernando Mart√≠nez-Plumed",
      "C√®sar Ferri",
      "David Nieves",
      "Jos√© Hern√°ndez-Orallo"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias in decision making related to protected groups and data collection disparities, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on data bias affecting marginalized groups",
      "affected_populations": [
        "minority groups",
        "protected groups"
      ],
      "methodology": [
        "Analysis",
        "Empirical Study"
      ],
      "methodology_detail": "Analyzes sources and impacts of missing data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.12516v1",
    "title": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "year": 2019,
    "authors": [
      "Thomas Davidson",
      "Debasmita Bhattacharya",
      "Ingmar Weber"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in AI systems used for hate speech detection, highlighting racial disparities and potential discrimination against African-American users.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial bias in AI datasets and classifiers",
      "affected_populations": [
        "African-American users",
        "social media users"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Machine Learning"
      ],
      "methodology_detail": "Training classifiers and analyzing prediction disparities",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.11684v1",
    "title": "On Measuring Gender Bias in Translation of Gender-neutral Pronouns",
    "year": 2019,
    "authors": [
      "Won Ik Cho",
      "Ji Won Kim",
      "Seok Min Kim",
      "Nam Soo Kim"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in machine translation, addressing social discrimination based on gender, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in language translation",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Constructs test sets and evaluates bias indices",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.11985v6",
    "title": "Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals underreported bias types",
    "year": 2019,
    "authors": [
      "David Rozado"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes biases related to race, socioeconomic status, age, religion, and political orientation in word embeddings, highlighting social discrimination aspects.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "age",
        "religion",
        "political"
      ],
      "other_detail": "Multiple social dimensions beyond common bias types",
      "affected_populations": [
        "African-Americans",
        "middle class",
        "working class",
        "elderly",
        "religious groups",
        "political groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Large-scale sentiment association analysis in embedding models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.13134v2",
    "title": "FairSearch: A Tool For Fairness in Ranked Search Results",
    "year": 2019,
    "authors": [
      "Meike Zehlike",
      "Tom S√ºhr",
      "Carlos Castillo",
      "Ivan Kitanovski"
    ],
    "categories": [
      "cs.IR",
      "H.3.3"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in search rankings, focusing on societal impacts and potential marginalization of unprivileged groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Fairness in algorithmic search results",
      "affected_populations": [
        "unprivileged groups",
        " marginalized individuals"
      ],
      "methodology": [
        "Algorithm Development",
        "Fairness Algorithms"
      ],
      "methodology_detail": "Implementation of fair ranking algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.10870v2",
    "title": "Equal Opportunity and Affirmative Action via Counterfactual Predictions",
    "year": 2019,
    "authors": [
      "Yixin Wang",
      "Dhanya Sridhar",
      "David M. Blei"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making related to social attributes like race and opportunity, aiming to reduce discrimination. It discusses legal notions of fairness such as equal opportunity and affirmative action, which are central to social inequality issues. The focus on adjusting algorithms to prevent unfair treatment indicates a direct engagement with social disparities.",
      "inequality_type": [
        "racial",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in automated decision-making systems",
      "affected_populations": [
        "disadvantaged groups",
        "minorities",
        "low-income individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Theoretical Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Using causal models and counterfactual predictions",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.10546v3",
    "title": "Protecting the Protected Group: Circumventing Harmful Fairness",
    "year": 2019,
    "authors": [
      "Omer Ben-Porat",
      "Fedor Sandomirskiy",
      "Moshe Tennenholtz"
    ],
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness constraints in algorithms affecting protected groups, highlighting potential worsening of disadvantaged groups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness constraints impacting protected social groups",
      "affected_populations": [
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Theoretical Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Analyzes fairness constraints and optimal classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.09947v2",
    "title": "Affirmative Action Policies for Top-k Candidates Selection, With an Application to the Design of Policies for University Admissions",
    "year": 2019,
    "authors": [
      "Michael Mathioudakis",
      "Carlos Castillo",
      "Giorgio Barnabo",
      "Sergio Celis"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on designing affirmative action policies to increase disadvantaged groups' representation in university admissions, directly addressing social inequality issues related to socio-demographic disparities.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "racial"
      ],
      "other_detail": "Focuses on underrepresented socio-demographic groups in education",
      "affected_populations": [
        "disadvantaged groups",
        "minority students"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Uses causal models and empirical data analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.10171v1",
    "title": "Perceptions of Gender Diversity's impact on mood in software development teams",
    "year": 2019,
    "authors": [
      "Kelly Blincoe",
      "Olga Springer",
      "Michal R. Wrobel"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender diversity and discrimination in IT teams, addressing gender inequality and perceptions, which are social inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women in IT",
        "gender-diverse teams"
      ],
      "methodology": [
        "Qualitative Study",
        "Survey"
      ],
      "methodology_detail": "Interviews and questionnaires analyzed",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.09105v1",
    "title": "Demographic Differentials in Facebook Usage Around the World",
    "year": 2019,
    "authors": [
      "Sofia Gil-Clavel",
      "Emilio Zagheni"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities in Facebook usage, focusing on gender, age, and regional differences, which relate to social inequalities. It analyzes how digital technology adoption varies across social groups, highlighting disparities. The study addresses social inequality through demographic and regional lenses.",
      "inequality_type": [
        "gender",
        "age",
        "geographic"
      ],
      "other_detail": "Focus on digital and social disparities in technology use",
      "affected_populations": [
        "women",
        "older adults",
        "regional populations"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using Facebook advertisement data for demographic analysis",
      "geographic_focus": [
        "North America",
        "Northern Europe",
        "Asia"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.07360v4",
    "title": "Contrastive Fairness in Machine Learning",
    "year": 2019,
    "authors": [
      "Tapabrata Chakraborti",
      "Arijit Patra",
      "Alison Noble"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses fairness in decision-making related to protected attributes like race and sex, highlighting concerns about social discrimination and bias in algorithmic decisions.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on fairness and discrimination in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Causal Inference",
        "Mathematical Tools"
      ],
      "methodology_detail": "Using causal inference to address contrastive fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.07026v2",
    "title": "Fairness in Machine Learning with Tractable Models",
    "year": 2019,
    "authors": [
      "Michael Varley",
      "Vaishak Belle"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SC",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on reducing disparate treatment across demographic groups, specifically gender. It discusses bias mitigation techniques and fairness definitions, directly engaging with social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focused on gender fairness in credit scoring",
      "affected_populations": [
        "male",
        "female"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Using tractable probabilistic models and experiments",
      "geographic_focus": [
        "Germany"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.06618v4",
    "title": "On the Fairness of Time-Critical Influence Maximization in Social Networks",
    "year": 2019,
    "authors": [
      "Junaid Ali",
      "Mahmoudreza Babaei",
      "Abhijnan Chakraborty",
      "Baharan Mirzasoleiman",
      "Krishna P. Gummadi",
      "Adish Singla"
    ],
    "categories": [
      "cs.SI",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in influence dissemination across socially salient groups, highlighting potential exacerbation of inequality due to algorithmic influence maximization, especially under time-critical conditions.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "informational"
      ],
      "other_detail": "Focuses on fairness in influence spread across social groups",
      "affected_populations": [
        "minority groups",
        "social groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware influence maximization algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.05786v2",
    "title": "Software Engineering for Fairness: A Case Study with Hyperparameter Optimization",
    "year": 2019,
    "authors": [
      "Joymallya Chakraborty",
      "Tianpei Xia",
      "Fahmid M. Fahid",
      "Tim Menzies"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on reducing discrimination related to protected attributes like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social discrimination"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Hyperparameter Optimization",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Using hyperparameter tuning to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.06134v1",
    "title": "Recommending Dream Jobs in a Biased Real World",
    "year": 2019,
    "authors": [
      "Nadia Fawaz"
    ],
    "categories": [
      "cs.IR",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in recommender systems that affect job recommendations, highlighting societal biases such as gender gaps. It addresses fairness issues in AI systems impacting social groups. The focus on bias reduction relates directly to social inequality concerns.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in job recommendation systems",
      "affected_populations": [
        "women",
        "job seekers"
      ],
      "methodology": [
        "Machine Learning",
        "Bias Mitigation Techniques"
      ],
      "methodology_detail": "Techniques to reduce bias at system stages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.03403v1",
    "title": "Fairness across Network Positions in Cyberbullying Detection Algorithms",
    "year": 2019,
    "authors": [
      "Vivek Singh",
      "Connor Hofenbitzer"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic fairness and bias related to social network positions, impacting potential victims differently based on their network centrality, which relates to social disparities.",
      "inequality_type": [
        "social",
        "gender",
        "racial",
        "educational",
        "digital"
      ],
      "other_detail": "Focus on network position disparities in algorithmic detection",
      "affected_populations": [
        "cyberbullying victims",
        "social network users"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Audits detection performance disparities based on network centrality",
      "geographic_focus": [
        "Twitter data",
        "unspecified regions"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.01347v2",
    "title": "Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets",
    "year": 2019,
    "authors": [
      "Chris Dulhanty",
      "Alexander Wong"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases in image datasets, focusing on gender, age, and potential biases affecting social groups, which relates directly to social inequality issues.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Bias in demographic annotations of image datasets",
      "affected_populations": [
        "women",
        "elderly",
        "young men"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Automated demographic annotation and bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.00569v2",
    "title": "Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness",
    "year": 2019,
    "authors": [
      "Xueru Zhang",
      "Mohammad Mahdi Khalili",
      "Cem Tekin",
      "Mingyan Liu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how ML decisions impact group representation over time, highlighting fairness and disparities among demographic groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness and representation disparities in AI systems",
      "affected_populations": [
        "demographic groups",
        "users"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes user dynamics and fairness criteria effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/1905.01989v3",
    "title": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search",
    "year": 2019,
    "authors": [
      "Sahin Cem Geyik",
      "Stuart Ambler",
      "Krishnaram Kenthapadi"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias related to protected attributes like gender and age, impacting fairness in ranking systems. It discusses fairness measures and their application to hiring, a domain with social inequality implications.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "focus on employment and recruitment fairness",
      "affected_populations": [
        "women",
        "older individuals"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias measurement and fairness re-ranking algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.13341v1",
    "title": "Learning Fair Representations via an Adversarial Framework",
    "year": 2019,
    "authors": [
      "Rui Feng",
      "Yang Yang",
      "Yuehan Lyu",
      "Chenhao Tan",
      "Yizhou Sun",
      "Chunping Wang"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on protected attributes like race and gender, which are central to social inequalities. It aims to mitigate bias and discrimination in societal-critical decision-making domains. The emphasis on fairness guarantees and protected group indistinguishability indicates a focus on social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fair representation learning for equitable classification",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Framework",
        "Statistical Analysis"
      ],
      "methodology_detail": "Learning fair representations via adversarial training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.12919v2",
    "title": "Female citation impact superiority 1996-2018 in six out of seven English-speaking nations",
    "year": 2019,
    "authors": [
      "Mike Thelwall"
    ],
    "categories": [
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in academic citation impact, a social inequality issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female academics"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of citation data over time",
      "geographic_focus": [
        "Australia",
        "Canada",
        "Ireland",
        "Jamaica",
        "New Zealand",
        "UK",
        "USA"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.12631v1",
    "title": "Detecting inter-sectional accuracy differences in driver drowsiness detection algorithms",
    "year": 2019,
    "authors": [
      "Mkhuseli Ngxande",
      "Jule-Raymond Tapamo",
      "Michael Burke"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses racial bias in driver drowsiness detection models, highlighting disparities affecting dark-skinned drivers, especially in African contexts.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in facial recognition across skin tones",
      "affected_populations": [
        "dark-skinned drivers",
        "African drivers"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Visualization"
      ],
      "methodology_detail": "Evaluates CNN performance and visualizes bias using PCA",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.08783v1",
    "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings",
    "year": 2019,
    "authors": [
      "Christine Basta",
      "Marta R. Costa-juss√†",
      "Noe Casas"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in word embeddings, addressing gender inequality in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "measuring bias in embedding models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.05419v4",
    "title": "FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning",
    "year": 2019,
    "authors": [
      "√Ångel Alexander Cabrera",
      "Will Epperson",
      "Fred Hohman",
      "Minsuk Kahng",
      "Jamie Morgenstern",
      "Duen Horng Chau"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on discovering biases in machine learning models that impact demographic subgroups, addressing fairness and social bias issues.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focus on bias detection in predictive models",
      "affected_populations": [
        "income groups",
        "racial minorities",
        "gender groups"
      ],
      "methodology": [
        "Visual Analytics",
        "Subgroup Discovery",
        "Machine Learning"
      ],
      "methodology_detail": "Interactive visualization for bias detection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.05254v4",
    "title": "Attraction-Repulsion clustering with applications to fairness",
    "year": 2019,
    "authors": [
      "Eustasio del Barrio",
      "Hristo Inouzhe",
      "Jean-Michel Loubes"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "62H30, 68T10"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and diversity in clustering related to protected attributes like race and gender, addressing social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "educational"
      ],
      "other_detail": "Fairness in algorithmic clustering",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Clustering",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Perturbation-based clustering promoting diversity",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.05233v1",
    "title": "What's in a Name? Reducing Bias in Bios without Access to Protected Attributes",
    "year": 2019,
    "authors": [
      "Alexey Romanov",
      "Maria De-Arteaga",
      "Hanna Wallach",
      "Jennifer Chayes",
      "Christian Borgs",
      "Alexandra Chouldechova",
      "Sahin Geyik",
      "Krishnaram Kenthapadi",
      "Anna Rumshisky",
      "Adam Tauman Kalai"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias mitigation related to race and gender in AI systems, which are key social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias reduction without protected attribute access",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation using word embeddings and name analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.03352v2",
    "title": "Simple dynamic word embeddings for mapping perceptions in the public sphere",
    "year": 2019,
    "authors": [
      "Nabeel Gillani",
      "Roger Levy"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender, ethnicity, and refugees in language corpora, which are linked to social inequalities and discrimination.",
      "inequality_type": [
        "gender",
        "ethnic",
        "refugee"
      ],
      "other_detail": "Focus on linguistic biases reflecting social perceptions",
      "affected_populations": [
        "women",
        "ethnic groups",
        "refugees"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Dynamic embedding modeling of linguistic biases",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.03310v1",
    "title": "Gender Bias in Contextualized Word Embeddings",
    "year": 2019,
    "authors": [
      "Jieyu Zhao",
      "Tianlu Wang",
      "Mark Yatskar",
      "Ryan Cotterell",
      "Vicente Ordonez",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI embeddings, highlighting social discrimination aspects. It analyzes how AI systems encode and perpetuate gender biases, which relate directly to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes bias in embeddings and coreference systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.03035v1",
    "title": "Identifying and Reducing Gender Bias in Word-Level Language Models",
    "year": 2019,
    "authors": [
      "Shikha Bordia",
      "Samuel R. Bowman"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social bias affecting gender equality. It discusses measuring and reducing gender bias, which relates to social discrimination. The focus on gender bias in AI systems directly pertains to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and regularization techniques in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.04047v3",
    "title": "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
    "year": 2019,
    "authors": [
      "Thomas Manzini",
      "Yao Chong Lim",
      "Yulia Tsvetkov",
      "Alan W Black"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to race and ethnicity in language models, which are social categories linked to inequality and discrimination.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on bias in AI representations of social groups",
      "affected_populations": [
        "racial groups",
        "ethnic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Debiasing and evaluation of word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.02095v5",
    "title": "Discrimination through optimization: How Facebook's ad delivery can lead to skewed outcomes",
    "year": 2019,
    "authors": [
      "Muhammad Ali",
      "Piotr Sapiezynski",
      "Miranda Bogen",
      "Aleksandra Korolova",
      "Alan Mislove",
      "Aaron Rieke"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how ad delivery skew can lead to discriminatory outcomes along gender and racial lines, impacting marginalized groups. It highlights mechanisms that cause unequal exposure to opportunities despite neutral targeting, addressing social bias and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Discrimination in digital advertising delivery mechanisms",
      "affected_populations": [
        "racial minorities",
        "women",
        "disadvantaged groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes ad delivery data and optimization effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1904.01219v2",
    "title": "Deep Learning for Face Recognition: Pride or Prejudiced?",
    "year": 2019,
    "authors": [
      "Shruti Nagpal",
      "Maneet Singh",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race and age in face recognition, which are social categories linked to inequality. It examines in-group biases and subconscious encoding, addressing social discrimination issues in AI systems.",
      "inequality_type": [
        "racial",
        "age"
      ],
      "other_detail": "Bias in AI face recognition systems",
      "affected_populations": [
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Experiment",
        "Deep Learning",
        "Analysis"
      ],
      "methodology_detail": "36 experiments on multiple datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.11719v1",
    "title": "Fairness in Algorithmic Decision Making: An Excursion Through the Lens of Causality",
    "year": 2019,
    "authors": [
      "Aria Khademi",
      "Sanghack Lee",
      "David Foley",
      "Vasant Honavar"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination related to gender and race using causal models, focusing on fairness in algorithmic decision making, which directly relates to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and discrimination in AI systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Causal Modeling",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using causal models and potential outcomes framework",
      "geographic_focus": [
        "U.S.",
        "New York City"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.10598v1",
    "title": "Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making",
    "year": 2019,
    "authors": [
      "Sina Aghaei",
      "Mohammad Javad Azizi",
      "Phebe Vayanos"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making systems affecting social groups, highlighting issues like discrimination and disparate impact, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "educational",
        "discrimination"
      ],
      "other_detail": "Focus on fairness in socially sensitive decisions",
      "affected_populations": [
        "minority groups",
        "disadvantaged individuals"
      ],
      "methodology": [
        "Optimization",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Mixed-integer optimization for fair decision trees",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.10561v1",
    "title": "On Measuring Social Biases in Sentence Encoders",
    "year": 2019,
    "authors": [
      "Chandler May",
      "Alex Wang",
      "Shikha Bordia",
      "Samuel R. Bowman",
      "Rachel Rudinger"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in AI models related to gender, race, and social constructs, which are key aspects of social inequality. It focuses on measuring biases that reflect societal discrimination and inequality in language representations.",
      "inequality_type": [
        "gender",
        "racial",
        "social constructs"
      ],
      "other_detail": "Focuses on social biases in AI language models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement in sentence encoders",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.09209v1",
    "title": "A Simulation Based Dynamic Evaluation Framework for System-wide Algorithmic Fairness",
    "year": 2019,
    "authors": [
      "Efr√©n Cruz Cort√©s",
      "Debashis Ghosh"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic fairness and social discrimination, focusing on bias reinforcement and social dynamics affecting marginalized groups.",
      "inequality_type": [
        "racial",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on social discrimination via algorithmic decision-making",
      "affected_populations": [
        "racial minorities",
        "social groups"
      ],
      "methodology": [
        "Agent Based Models",
        "Reinforcement Learning",
        "Simulation",
        "Social Dynamics Analysis"
      ],
      "methodology_detail": "Modeling social interactions and decision processes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.07609v1",
    "title": "Multi-Differential Fairness Auditor for Black Box Classifiers",
    "year": 2019,
    "authors": [
      "Xavier Gitiaux",
      "Huzefa Rangwala"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination in AI impacting marginalized groups, specifically racial bias in recidivism risk assessment.",
      "inequality_type": [
        "racial",
        "social discrimination"
      ],
      "other_detail": "Focus on racial bias in criminal justice algorithms",
      "affected_populations": [
        "African-American individuals"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Identifies discrimination violations in black box classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.06469v3",
    "title": "Using Data Science to Understand the Film Industry's Gender Gap",
    "year": 2019,
    "authors": [
      "Dima Kagan",
      "Thomas Chesney",
      "Michael Fire"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "physics.data-an"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in movies, focusing on female representation, which relates to gender inequality. It analyzes social perceptions and portrayals affecting societal attitudes towards women. The study also discusses gender bias in media, a key social inequality issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender representation in film industry",
      "affected_populations": [
        "women",
        "female characters"
      ],
      "methodology": [
        "Data Science",
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzed movie social networks and dialogue data",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.03862v2",
    "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
    "year": 2019,
    "authors": [
      "Hila Gonen",
      "Yoav Goldberg"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in word embeddings, reflecting societal gender inequalities. It discusses how AI models encode and obscure gender biases, impacting social perceptions and fairness.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in NLP models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias reduction techniques in word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.01209v2",
    "title": "On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning",
    "year": 2019,
    "authors": [
      "Hoda Heidari",
      "Vedant Nanda",
      "Krishna P. Gummadi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines long-term impacts of decision policies on population groups, using social learning and segregation measures, which relate to social inequalities such as socioeconomic and educational disparities.",
      "inequality_type": [
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on long-term social and economic impacts",
      "affected_populations": [
        "social groups",
        "population segments"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Models social learning and population segregation effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1903.00967v2",
    "title": "Group-Fairness in Influence Maximization",
    "year": 2019,
    "authors": [
      "Alan Tsang",
      "Bryan Wilder",
      "Eric Rice",
      "Milind Tambe",
      "Yair Zick"
    ],
    "categories": [
      "cs.GT",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in influence maximization, focusing on equitable distribution across social groups, explicitly considering sensitive attributes like race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Fairness in social influence interventions",
      "affected_populations": [
        "homeless youth",
        "social groups"
      ],
      "methodology": [
        "Algorithm Design",
        "Fairness Constraints",
        "Experimental Analysis"
      ],
      "methodology_detail": "Formal fairness definitions and real data experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1902.11097v1",
    "title": "Predictive Inequity in Object Detection",
    "year": 2019,
    "authors": [
      "Benjamin Wilson",
      "Judy Hoffman",
      "Jamie Morgenstern"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines predictive disparities in object detection based on skin tone, a racial characteristic, highlighting potential bias and fairness issues in AI systems affecting different demographic groups.",
      "inequality_type": [
        "racial",
        "fairness"
      ],
      "other_detail": "Focuses on racial skin tone disparities in AI performance",
      "affected_populations": [
        "pedestrians with different skin tones"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes performance disparities across skin tone groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1902.05826v2",
    "title": "The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the xAUC Metric",
    "year": 2019,
    "authors": [
      "Nathan Kallus",
      "Angela Zhou"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in risk scores used in high-stakes decisions, addressing disparities across social groups such as race and socioeconomic status.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and disparate impact in AI systems",
      "affected_populations": [
        "racial minorities",
        "low-income groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Using xAUC disparity to measure fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1902.03731v1",
    "title": "Discrimination in the Age of Algorithms",
    "year": 2019,
    "authors": [
      "Jon Kleinberg",
      "Jens Ludwig",
      "Sendhil Mullainathan",
      "Cass R. Sunstein"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses discrimination, fairness, and transparency in decision-making, which are central to social inequalities related to race, gender, and social bias. It emphasizes how algorithms can impact discrimination detection and potentially promote equity.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on discrimination detection and transparency",
      "affected_populations": [
        "minority groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzing algorithm transparency and fairness implications",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1902.00496v1",
    "title": "Examining the Presence of Gender Bias in Customer Reviews Using Word Embedding",
    "year": 2019,
    "authors": [
      "A. Mishra",
      "H. Mishra",
      "S. Rathee"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in algorithms learned from human language, addressing gender inequality and social bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in reviews and algorithms",
      "affected_populations": [
        "female consumers",
        "male consumers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using word embedding (GloVe) on review data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1902.00375v2",
    "title": "Dynamic fairness - Breaking vicious cycles in automatic decision making",
    "year": 2019,
    "authors": [
      "Benjamin Paa√üen",
      "Astrid Bunge",
      "Carolin Hainke",
      "Leon Sindelar",
      "Matthias Vogelsang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses long-term inequalities and vicious cycles in decision making, which relate to social disparities. It emphasizes fairness in algorithms affecting different groups, indicating a focus on social bias and discrimination.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness cycles in automated decision processes",
      "affected_populations": [
        "social groups",
        "decision subjects"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Mathematical Modeling"
      ],
      "methodology_detail": "Models decision processes over time",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1902.00119v1",
    "title": "Race, Ethnicity and National Origin-based Discrimination in Social Media and Hate Crimes Across 100 U.S. Cities",
    "year": 2019,
    "authors": [
      "Kunal Relia",
      "Zhengyi Li",
      "Stephanie H. Cook",
      "Rumi Chunara"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines discrimination based on race, ethnicity, and national origin, and its relation to hate crimes, addressing racial and ethnic inequalities in social media and physical spaces.",
      "inequality_type": [
        "racial",
        "ethnic",
        "nationality"
      ],
      "other_detail": "Focus on discrimination and hate crimes",
      "affected_populations": [
        "racial groups",
        "ethnic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Develops spatially-diverse datasets and classification pipeline",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10837v4",
    "title": "Noise-tolerant fair classification",
    "year": 2019,
    "authors": [
      "Alexandre Louis Lamy",
      "Ziyuan Zhong",
      "Aditya Krishna Menon",
      "Nakul Verma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in classification related to sensitive features like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in AI classification under noisy sensitive data",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness-aware learning with noise-tolerance estimation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10501v2",
    "title": "Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions",
    "year": 2019,
    "authors": [
      "Hao Wang",
      "Berk Ustun",
      "Flavio P. Calmon"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in model performance across social groups defined by sensitive attributes, aiming to reduce social bias and fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on fairness in AI systems",
      "affected_populations": [
        "disadvantaged group",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Perturbation of input distributions to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10450v2",
    "title": "Toward Controlling Discrimination in Online Ad Auctions",
    "year": 2019,
    "authors": [
      "L. Elisa Celis",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discriminatory ad targeting related to gender and race, impacting social fairness. It focuses on fairness constraints in online ad auctions, which influence social group representation.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Fairness in online advertising targeting",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness-constrained auction mechanism design and optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10443v1",
    "title": "Improved Adversarial Learning for Fair Classification",
    "year": 2019,
    "authors": [
      "L. Elisa Celis",
      "Vijay Keswani"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI classification, addressing bias and discrimination issues, which relate to social inequalities such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI classification algorithms",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Adversarial learning and multi-objective optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10265v3",
    "title": "Implicit Diversity in Image Summarization",
    "year": 2019,
    "authors": [
      "L. Elisa Celis",
      "Vijay Keswani"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in image search results related to gender and skin tone, reflecting social inequalities. It aims to improve visible diversity without relying on explicit labels, thus tackling social bias in AI systems.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Implicitly addresses social bias in AI systems",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops and evaluates diversity algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10053v1",
    "title": "Towards Fair Deep Clustering With Multi-State Protected Variables",
    "year": 2019,
    "authors": [
      "Bokun Wang",
      "Ian Davidson"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair clustering to mitigate disparate impact across protected groups, addressing social fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in clustering across multiple protected groups",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Development",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Develops a fair clustering algorithm for multiple protected groups",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.10566v2",
    "title": "Fair Regression for Health Care Spending",
    "year": 2019,
    "authors": [
      "Anna Zink",
      "Sherri Rose"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "stat.ME",
      "stat.ML"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in health care payments affecting different groups, highlighting fairness issues in risk adjustment formulas, which relate to health and socioeconomic inequalities.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in health insurance risk adjustment",
      "affected_populations": [
        "undercompensated enrollees",
        "health insurers"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Develops fair regression methods with fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.09451v1",
    "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting",
    "year": 2019,
    "authors": [
      "Maria De-Arteaga",
      "Alexey Romanov",
      "Hanna Wallach",
      "Jennifer Chayes",
      "Christian Borgs",
      "Alexandra Chouldechova",
      "Sahin Geyik",
      "Krishnaram Kenthapadi",
      "Adam Tauman Kalai"
    ],
    "categories": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in occupation classification, highlighting social discrimination and unequal impacts on gender groups.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on gender bias in employment-related AI tasks",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes bias impact through semantic representation and classification rates",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.07656v1",
    "title": "Attenuating Bias in Word Vectors",
    "year": 2019,
    "authors": [
      "Sunipa Dev",
      "Jeff Phillips"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in word embeddings related to gender, race, ethnicity, and age, which are social categories linked to inequality and discrimination. It focuses on detecting and mitigating social biases in AI systems, impacting social fairness.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "age"
      ],
      "other_detail": "Bias mitigation in language models",
      "affected_populations": [
        "women",
        "racial groups",
        "ethnic minorities",
        "elderly"
      ],
      "methodology": [
        "Natural Language Processing",
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and removal techniques in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.05389v1",
    "title": "Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter",
    "year": 2019,
    "authors": [
      "Jacobo Levy Abitbol",
      "M√°rton Karsai",
      "Eric Fleury"
    ],
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.CY",
      "physics.data-an",
      "physics.soc-ph"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper infers socioeconomic status from online behavior, addressing economic and social class disparities.",
      "inequality_type": [
        "socioeconomic",
        "economic",
        "class"
      ],
      "other_detail": "Focuses on social stratification via online data",
      "affected_populations": [
        "Twitter users",
        "French population"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using open census, profiles, environmental data",
      "geographic_focus": [
        "France"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/1901.03116v2",
    "title": "Equalizing Gender Biases in Neural Machine Translation with Word Embeddings Techniques",
    "year": 2019,
    "authors": [
      "Joel Escud√© Font",
      "Marta R. Costa-juss√†"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in neural machine translation, a social fairness issue, by proposing methods to reduce gender stereotypes inherited from training data.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "occupational groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Debiasing techniques applied to word embeddings in translation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  }
]