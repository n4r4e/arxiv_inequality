[
  {
    "id": "http://arxiv.org/abs/2301.00719v2",
    "title": "Detection of Groups with Biased Representation in Ranking",
    "year": 2022,
    "authors": [
      "Jinyang Li",
      "Yuval Moskovitch",
      "H. V. Jagadish"
    ],
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in ranking algorithms, focusing on biased representation of groups, which relates to social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on algorithmic fairness and group representation",
      "affected_populations": [
        "social groups",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Detecting biased group representations in rankings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.14467v1",
    "title": "Cluster-level Group Representativity Fairness in $k$-means Clustering",
    "year": 2022,
    "authors": [
      "Stanley Simoes",
      "Deepak P",
      "Muiris MacCarthaigh"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in clustering related to sensitive attributes like race and gender, addressing social bias issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in group representation within clusters",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Optimizing fairness in clustering algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.13014v1",
    "title": "Bias Mitigation Framework for Intersectional Subgroups in Neural Networks",
    "year": 2022,
    "authors": [
      "Narine Kokhlikyan",
      "Bilal Alsallakh",
      "Fulton Wang",
      "Vivek Miglani",
      "Oliver Aobo Yang",
      "David Adkins"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models related to protected attributes, which are linked to social inequalities such as race and gender.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on intersectional subgroup bias mitigation",
      "affected_populations": [
        "social minorities",
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Framework"
      ],
      "methodology_detail": "Mutual information reduction for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.12799v2",
    "title": "A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models",
    "year": 2022,
    "authors": [
      "Xingmeng Zhao",
      "Ali Niazi",
      "Anthony Rios"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, highlighting disparities affecting gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in chemical NER models affecting gender groups",
      "affected_populations": [
        "female",
        "male"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Synthetic data and annotated Reddit corpus analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.13104v1",
    "title": "The URW-KG: a Resource for Tackling the Underrepresentation of non-Western Writers",
    "year": 2022,
    "authors": [
      "Marco Antonio Stranisci",
      "Giuseppe Spillo",
      "Cataldo Musto",
      "Viviana Patti",
      "Rossana Damiano"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses underrepresentation of non-Western writers, highlighting social discrimination and inequality in literary knowledge access.",
      "inequality_type": [
        "racial",
        "geographic",
        "informational"
      ],
      "other_detail": "Focus on representation disparity in digital literary archives",
      "affected_populations": [
        "non-Western writers",
        "readers",
        "scholars"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Integrating multiple data sources and evaluating embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.10678v3",
    "title": "Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias",
    "year": 2022,
    "authors": [
      "Yuen Chen",
      "Vethavikashini Chithrra Raghuram",
      "Justus Mattern",
      "Rada Mihalcea",
      "Zhijing Jin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, addressing social gender inequality. It measures and discusses occupational gender bias, a social discrimination issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on occupational gender bias in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Causal Analysis",
        "Benchmarking",
        "Experiment"
      ],
      "methodology_detail": "Develops causal bias measurement and tests models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2301.00646v1",
    "title": "Addressing the Selection Bias in Voice Assistance: Training Voice Assistance Model in Python with Equal Data Selection",
    "year": 2022,
    "authors": [
      "Kashav Piya",
      "Srijal Shrestha",
      "Cameran Frank",
      "Estephanos Jebessa",
      "Tauheed Khan Mohd"
    ],
    "categories": [
      "eess.AS",
      "cs.MA",
      "cs.RO",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in voice assistants, highlighting social discrimination and inequality in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Addressing gender bias in AI systems",
      "affected_populations": [
        "women",
        "underrepresented groups"
      ],
      "methodology": [
        "Deep Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Training voice recognition models with diverse data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.10563v2",
    "title": "BLIND: Bias Removal With No Demographics",
    "year": 2022,
    "authors": [
      "Hadas Orgad",
      "Yonatan Belinkov"
    ],
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases related to race and gender in AI models, which are key aspects of social inequality. It focuses on mitigating biases that impact marginalized groups without requiring demographic data. This directly relates to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation without demographic labels",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.10166v1",
    "title": "Protected Attributes Tell Us Who, Behavior Tells Us How: A Comparison of Demographic and Behavioral Oversampling for Fair Student Success Modeling",
    "year": 2022,
    "authors": [
      "Jade Maï Cock",
      "Muhammad Bilal",
      "Richard Davis",
      "Mirko Marras",
      "Tanja Käser"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias mitigation in educational algorithms, addressing social bias issues related to demographic attributes and behavior, which are linked to social inequalities.",
      "inequality_type": [
        "educational",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on bias mitigation in student success modeling",
      "affected_populations": [
        "students",
        "at-risk students"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation approaches evaluated on educational data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.08578v1",
    "title": "Provable Fairness for Neural Network Models using Formal Verification",
    "year": 2022,
    "authors": [
      "Giorgian Borca-Tasciuc",
      "Xingzhi Guo",
      "Stanley Bak",
      "Steven Skiena"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI models, addressing biases related to gender and race, which are social inequalities. It aims to verify and reduce unfairness in decision-making systems. The datasets used (COMPAS and ADULTS) are commonly associated with social bias studies.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI fairness verification",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Formal Verification",
        "Experiment",
        "Machine Learning"
      ],
      "methodology_detail": "Using formal methods to verify fairness properties",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.07877v2",
    "title": "Manifestations of Xenophobia in AI Systems",
    "year": 2022,
    "authors": [
      "Nenad Tomasev",
      "Jonathan Leader Maynard",
      "Iason Gabriel"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines xenophobia, a form of social discrimination affecting marginalized groups, and its manifestation in AI systems, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social",
        "discrimination"
      ],
      "other_detail": "Focus on xenophobia as social bias in AI",
      "affected_populations": [
        "marginalized groups",
        "social minorities"
      ],
      "methodology": [
        "Literature Review",
        "System Design",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing AI impacts across domains",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.07852v1",
    "title": "The effects of gender bias in word embeddings on depression prediction",
    "year": 2022,
    "authors": [
      "Gizem Sogancioglu",
      "Heysem Kaya"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in word embeddings and its impact on depression prediction, addressing gender bias and its societal implications in AI systems.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on bias transfer affecting mental health prediction",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Data Augmentation"
      ],
      "methodology_detail": "Bias analysis and mitigation in embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.06803v1",
    "title": "Fair Infinitesimal Jackknife: Mitigating the Influence of Biased Training Data Points Without Refitting",
    "year": 2022,
    "authors": [
      "Prasanna Sattigeri",
      "Soumya Ghosh",
      "Inkit Padhi",
      "Pierre Dognin",
      "Kush R. Varshney"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing biases related to sensitive attributes like race and gender, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias mitigation in AI fairness",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Influence-based data point selection for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.06750v1",
    "title": "FairRoad: Achieving Fairness for Recommender Systems with Optimized Antidote Data",
    "year": 2022,
    "authors": [
      "Minghong Fang",
      "Jia Liu",
      "Michinari Momma",
      "Yi Sun"
    ],
    "categories": [
      "cs.IR",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in recommender systems, focusing on social bias mitigation.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias reduction",
      "affected_populations": [
        "social groups",
        "users"
      ],
      "methodology": [
        "Optimization",
        "Experiment"
      ],
      "methodology_detail": "Constructs antidote data to improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.05435v1",
    "title": "Wireless earbuds for low-cost hearing screening",
    "year": 2022,
    "authors": [
      "Justin Chan",
      "Antonio Glenn",
      "Malek Itani",
      "Lisa R. Mancl",
      "Emily Gallagher",
      "Randall Bly",
      "Shwetak Patel",
      "Shyamnath Gollakota"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses global health disparities by developing low-cost hearing screening technology, aiming to improve access in low-income regions.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Access to affordable hearing screening in low-income countries",
      "affected_populations": [
        "low-income communities",
        "underserved populations"
      ],
      "methodology": [
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Clinical study with low-cost device testing",
      "geographic_focus": [
        "low- and middle-income countries"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.01725v1",
    "title": "Fairness in Contextual Resource Allocation Systems: Metrics and Incompatibility Results",
    "year": 2022,
    "authors": [
      "Nathanael Jo",
      "Bill Tang",
      "Kathryn Dullerud",
      "Sina Aghaei",
      "Eric Rice",
      "Phebe Vayanos"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in resource allocation affecting marginalized groups, highlighting systemic injustices related to race, gender, and other social factors.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in resource distribution systems",
      "affected_populations": [
        "disadvantaged communities",
        "marginalized groups"
      ],
      "methodology": [
        "Fairness Metrics",
        "Theoretical Analysis",
        "Incompatibility Results"
      ],
      "methodology_detail": "Analyzes fairness metrics and their interactions",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2212.00480v1",
    "title": "Identifying Different Layers of Online Misogyny",
    "year": 2022,
    "authors": [
      "Wienke Strathern",
      "Juergen Pfeffer"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses online misogyny, a form of gender-based social discrimination, and examines how AI tools detect such bias.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on online gender-based hostility",
      "affected_populations": [
        "women",
        "public figures"
      ],
      "methodology": [
        "Case Study",
        "Natural Language Processing",
        "Literature Review"
      ],
      "methodology_detail": "classification scheme and API evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.15937v1",
    "title": "Robustness Disparities in Face Detection",
    "year": 2022,
    "authors": [
      "Samuel Dooley",
      "George Z. Wei",
      "Tom Goldstein",
      "John P. Dickerson"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in face detection related to gender, age, skin tone, and lighting, which are social identity factors. It highlights disparities in system robustness impacting marginalized groups. These aspects relate directly to social discrimination and inequality in AI outcomes.",
      "inequality_type": [
        "gender",
        "age",
        "racial"
      ],
      "other_detail": "Bias in facial detection robustness",
      "affected_populations": [
        "women",
        "older adults",
        "darker skin individuals",
        "dim lighting individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzing robustness across datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.15265v1",
    "title": "Assessing Bias in Face Image Quality Assessment",
    "year": 2022,
    "authors": [
      "Žiga Babnik",
      "Vitomir Štruc"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases related to race and sex in face recognition systems, highlighting issues of social bias and fairness in AI. It analyzes how these biases affect different social groups, particularly racial groups. The focus on demographic bias aligns with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI face recognition systems",
      "affected_populations": [
        "racial minorities",
        "white individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias assessment across multiple face recognition models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.15181v1",
    "title": "MixFairFace: Towards Ultimate Fairness via MixFair Adapter in Face Recognition",
    "year": 2022,
    "authors": [
      "Fu-En Wang",
      "Chien-Yi Wang",
      "Min Sun",
      "Shang-Hong Lai"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses demographic bias and fairness in face recognition, focusing on reducing identity bias without sensitive attribute labels, which relates to social discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness across social groups in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness evaluation and bias reduction techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.14858v1",
    "title": "\"Explain it in the Same Way!\" -- Model-Agnostic Group Fairness of Counterfactual Explanations",
    "year": 2022,
    "authors": [
      "André Artelt",
      "Barbara Hammer"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in the complexity of counterfactual explanations across protected groups, highlighting fairness issues related to social groups.",
      "inequality_type": [
        "social",
        "fairness"
      ],
      "other_detail": "focus on algorithmic fairness across social groups",
      "affected_populations": [
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "evaluates explanation complexity disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.14531v1",
    "title": "Equity Promotion in Public Transportation",
    "year": 2022,
    "authors": [
      "Anik Pramanik",
      "Pan Xu",
      "Yifan Xu"
    ],
    "categories": [
      "cs.AI",
      "cs.DS",
      "cs.GT"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on promoting social equity in public transportation, addressing disparities among protected groups based on race, income, and other factors, thus directly engaging with social inequalities.",
      "inequality_type": [
        "economic",
        "income",
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focuses on transportation access disparities",
      "affected_populations": [
        "poverty-stricken households",
        "disadvantaged communities"
      ],
      "methodology": [
        "Optimization Model",
        "Linear Programming",
        "Experimental Testing"
      ],
      "methodology_detail": "Algorithm design and real data testing",
      "geographic_focus": [
        "Chicago"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.14498v1",
    "title": "The Impact of Racial Distribution in Training Data on Face Recognition Bias: A Closer Look",
    "year": 2022,
    "authors": [
      "Manideep Kolla",
      "Aravinth Savadamuthu"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial distribution in training data and its impact on face recognition bias, directly addressing racial fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in facial recognition algorithms",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Varying racial distributions and analyzing bias metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.14489v2",
    "title": "Mitigating Relational Bias on Knowledge Graphs",
    "year": 2022,
    "authors": [
      "Yu-Neng Chuang",
      "Kwei-Herng Lai",
      "Ruixiang Tang",
      "Mengnan Du",
      "Chia-Yuan Chang",
      "Na Zou",
      "Xia Hu"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias related to gender and nationality, which are social identity factors, and discusses fairness in AI systems.",
      "inequality_type": [
        "gender",
        "nationality"
      ],
      "other_detail": "Relational bias in knowledge graphs",
      "affected_populations": [
        "gender groups",
        "nationality groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Mitigation of bias in KGNN models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.14402v1",
    "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
    "year": 2022,
    "authors": [
      "Aristides Milios",
      "Parishad BehnamGhader"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases related to religion and ethnicity in language models, which are forms of social discrimination. It discusses biases in AI systems that can impact different social groups, aligning with social inequality issues.",
      "inequality_type": [
        "religious",
        "ethnic"
      ],
      "other_detail": "Focuses on cultural and linguistic biases in AI models",
      "affected_populations": [
        "religious groups",
        "ethnic minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement via sentence pseudo-likelihood",
      "geographic_focus": [
        "English-speaking",
        "Greek",
        "Persian"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.14358v1",
    "title": "A Moral- and Event- Centric Inspection of Gender Bias in Fairy Tales at A Large Scale",
    "year": 2022,
    "authors": [
      "Zhixuan Zhou",
      "Jiao Sun",
      "Jiaxin Pei",
      "Nanyun Peng",
      "Jinjun Xiong"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in fairy tales, highlighting gender disparities and stereotypes, which relate directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "children",
        "female characters",
        "male characters"
      ],
      "methodology": [
        "Computational Analysis",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes gender differences in moral foundations and events",
      "geographic_focus": [
        "7 cultures"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.13352v1",
    "title": "Improving dermatology classifiers across populations using images generated by large diffusion models",
    "year": 2022,
    "authors": [
      "Luke W. Sagers",
      "James A. Diao",
      "Matthew Groh",
      "Pranav Rajpurkar",
      "Adewole S. Adamson",
      "Arjun K. Manrai"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in dermatological diagnosis and improves representation across skin types, which relates to health and racial inequalities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focuses on skin type diversity in medical AI",
      "affected_populations": [
        "underrepresented skin types",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Synthetic data augmentation for classification improvement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11892v2",
    "title": "Equality of Effort via Algorithmic Recourse",
    "year": 2022,
    "authors": [
      "Francesca E. D. Raimondi",
      "Andrew R. Lawrence",
      "Hana Chockler"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and equality of effort in automated decision systems, which relate to social discrimination and bias. It considers the impact of algorithmic outcomes on protected groups, implying a focus on social inequality issues.",
      "inequality_type": [
        "socioeconomic"
      ],
      "other_detail": "focus on fairness and effort in automated systems",
      "affected_populations": [
        "protected individuals",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Recourse",
        "Quantitative Analysis"
      ],
      "methodology_detail": "measuring minimal intervention costs for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11512v1",
    "title": "Bursting the Burden Bubble? An Assessment of Sharma et al.'s Counterfactual-based Fairness Metric",
    "year": 2022,
    "authors": [
      "Yochem van Rosmalen",
      "Florian van der Steen",
      "Sebastiaan Jans",
      "Daan van der Weijden"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness metrics in machine learning, focusing on bias against unprivileged groups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in AI models affecting social groups",
      "affected_populations": [
        "women",
        "people of color"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Comparative analysis of fairness metrics on datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.12504v1",
    "title": "Identifying gender bias in blockbuster movies through the lens of machine learning",
    "year": 2022,
    "authors": [
      "Muhammad Junaid Haris",
      "Aanchal Upreti",
      "Melih Kurtaran",
      "Filip Ginter",
      "Sebastien Lafond",
      "Sepinoud Azimi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias in movies, highlighting societal stereotypes and disparities, which relate directly to social gender inequality. It also discusses biases in portrayals that influence societal perceptions. The focus on gender roles and stereotypes indicates a clear connection to social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Sentiment analysis, embeddings, pattern detection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11206v1",
    "title": "Cultural Re-contextualization of Fairness Research in Language Technologies in India",
    "year": 2022,
    "authors": [
      "Shaily Bhatt",
      "Sunipa Dev",
      "Partha Talukdar",
      "Shachi Dave",
      "Vinodkumar Prabhakaran"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in NLP models within the Indian societal context, addressing social disparities and fairness issues. It explicitly focuses on social biases along axes relevant to India, indicating a concern with social inequality.",
      "inequality_type": [
        "social",
        "gender",
        "educational",
        "geographic"
      ],
      "other_detail": "Focuses on Indian societal and cultural disparities",
      "affected_populations": [
        "Indian social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Empirical Study",
        "Natural Language Processing",
        "Data Analysis"
      ],
      "methodology_detail": "Analyzes biases in corpora and models",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11183v2",
    "title": "Causal Fairness Assessment of Treatment Allocation with Electronic Health Records",
    "year": 2022,
    "authors": [
      "Linying Zhang",
      "Lauren R. Richter",
      "Yixin Wang",
      "Anna Ostropolets",
      "Noemie Elhadad",
      "David M. Blei",
      "George Hripcsak"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in treatment allocation, considering social determinants and potential biases affecting different social groups, particularly in healthcare disparities.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "racial",
        "social determinants"
      ],
      "other_detail": "Focus on healthcare disparities and social determinants",
      "affected_populations": [
        "patients with coronary artery disease",
        "socially disadvantaged groups"
      ],
      "methodology": [
        "Causal Fairness Algorithm",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Assessing fairness conditioned on treatment benefit likelihood",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11109v2",
    "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
    "year": 2022,
    "authors": [
      "Abdelrahman Zayed",
      "Prasanna Parthasarathi",
      "Goncalo Mordido",
      "Hamid Palangi",
      "Samira Shabanian",
      "Sarath Chandar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models, focusing on gender bias mitigation, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on fairness in predictive models",
      "affected_populations": [
        "marginalized groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Data Augmentation"
      ],
      "methodology_detail": "Pruning examples to improve fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11100v2",
    "title": "Data-driven Tracking of the Bounce-back Path after Disasters: Critical Milestones of Population Activity Recovery and Their Spatial Inequality",
    "year": 2022,
    "authors": [
      "Yuqin Jiang",
      "Faxi Yuan",
      "Hamed Farahmand",
      "Kushal Acharya",
      "Jingdi Zhang",
      "Ali Mostafavi"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines spatial inequality in post-disaster population recovery, focusing on socioeconomic and demographic variations across regions.",
      "inequality_type": [
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on spatial inequality in recovery milestones",
      "affected_populations": [
        "disaster-affected communities",
        "spatial regions"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses statistical measures of recovery and inequality",
      "geographic_focus": [
        "Harris County, Texas"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.11087v3",
    "title": "Conceptor-Aided Debiasing of Large Language Models",
    "year": 2022,
    "authors": [
      "Li S. Yifei",
      "Lyle Ungar",
      "João Sedoc"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases in language models, which reflect societal inequalities related to race, gender, and other social groups, and discusses methods to mitigate these biases.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on social bias mitigation in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias subspace identification and removal techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.09942v1",
    "title": "Professional Presentation and Projected Power: A Case Study of Implicit Gender Information in English CVs",
    "year": 2022,
    "authors": [
      "Jinrui Yang",
      "Sheilla Njoto",
      "Marc Cheong",
      "Leah Ruppanner",
      "Lea Frermann"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gendered language in CVs, highlighting gender bias in hiring, a key aspect of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes gender signals in CV language patterns",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.09865v1",
    "title": "Gender Bias in Big Data Analysis",
    "year": 2022,
    "authors": [
      "Thomas J. Misa"
    ],
    "categories": [
      "cs.CY",
      "J.4; K.2; K.4; K.7"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in big data analysis, highlighting social discrimination related to gender. It assesses how gender prediction software reflects and potentially perpetuates gender bias, impacting social perceptions and professional fields.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in computing",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Comparative analysis of software and human data",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.09511v1",
    "title": "Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review",
    "year": 2022,
    "authors": [
      "Mikel K. Ngueajio",
      "Gloria Washington"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in ASR systems related to gender, race, and disability, highlighting social discrimination and inequality issues in AI technology.",
      "inequality_type": [
        "gender",
        "racial",
        "disability"
      ],
      "other_detail": "Bias mitigation in speech recognition systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "disabled users"
      ],
      "methodology": [
        "Literature Review",
        "System Design",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Survey of bias mitigation techniques and system approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.08742v1",
    "title": "Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN",
    "year": 2022,
    "authors": [
      "Anaelia Ovalle",
      "Sunipa Dev",
      "Jieyu Zhao",
      "Majid Sarrafzadeh",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on bias and fairness in healthcare AI, addressing disparities among patient groups, which relates to social inequalities such as health disparities and inequities.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on healthcare disparities and bias detection",
      "affected_populations": [
        "disadvantaged communities",
        "healthcare patients"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection in clinical prediction models",
      "geographic_focus": [
        "MIMIC-III dataset (US-based hospital data)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.08667v2",
    "title": "County-level Algorithmic Audit of Racial Bias in Twitter's Home Timeline",
    "year": 2022,
    "authors": [
      "Luca Belli",
      "Kyra Yee",
      "Uthaipon Tantipongpipat",
      "Aaron Gonzales",
      "Kristian Lum",
      "Moritz Hardt"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in Twitter's ranking system using county-level racial demographics, addressing racial inequality and social bias in online platforms.",
      "inequality_type": [
        "racial",
        "informational"
      ],
      "other_detail": "Focuses on racial bias in social media visibility",
      "affected_populations": [
        "Black or African American",
        "White"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "County-level analysis using demographic and platform data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.07520v1",
    "title": "Wikigender: A Machine Learning Model to Detect Gender Bias in Wikipedia",
    "year": 2022,
    "authors": [
      "Natalie Bolón Brun",
      "Sofia Kypraiou",
      "Natalia Gullón Altés",
      "Irene Petlacalco Barrios"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in Wikipedia, highlighting social discrimination based on gender. It analyzes how language and portrayal differ for women and men, reflecting societal biases.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in media representation",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes language bias in biographies",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.07350v2",
    "title": "Does Debiasing Inevitably Degrade the Model Performance",
    "year": 2022,
    "authors": [
      "Yiran Liu",
      "Xiao Liu",
      "Haotian Chen",
      "Yang Yu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a social inequality issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Theoretical framework and causality-detection fine-tuning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.06740v1",
    "title": "Lessons from Digital India for the Right to Internet Access",
    "year": 2022,
    "authors": [
      "Kaustubh D. Dhole"
    ],
    "categories": [
      "cs.NI",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses Internet access disparities across gender and geographic groups, highlighting social inequalities in digital access and rights.",
      "inequality_type": [
        "gender",
        "geographic",
        "digital",
        "socioeconomic"
      ],
      "other_detail": "Focus on digital divide and access rights",
      "affected_populations": [
        "women",
        "rural residents",
        "urban poor"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes global debate and Indian context",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.06321v2",
    "title": "Sociodemographic inequalities in student achievement: An intersectional multilevel analysis of individual heterogeneity and discriminatory accuracy (MAIHDA) with application to students in London, England",
    "year": 2022,
    "authors": [
      "Lucy Prior",
      "Clare Evans",
      "Juan Merlo",
      "George Leckie"
    ],
    "categories": [
      "stat.AP",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines sociodemographic disparities in student achievement, focusing on intersectional inequalities related to race, gender, socioeconomic status, and special needs, indicating a clear focus on social inequality.",
      "inequality_type": [
        "educational",
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Intersectional analysis of multiple social determinants",
      "affected_populations": [
        "students in London",
        "marginalized groups"
      ],
      "methodology": [
        "Multilevel Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using MAIHDA approach for intersectional stratification",
      "geographic_focus": [
        "London, England"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.05617v1",
    "title": "Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey",
    "year": 2022,
    "authors": [
      "Otávio Parraga",
      "Martin D. More",
      "Christian M. Oliveira",
      "Nathan S. Gavenski",
      "Lucas S. Kupssinskü",
      "Adilson Medronha",
      "Luis V. Moura",
      "Gabriel S. Simões",
      "Rodrigo C. Barros"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI, addressing biases related to social groups such as race and gender, which are core aspects of social inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "racial minorities",
        "women",
        "age groups"
      ],
      "methodology": [
        "Literature Review",
        "Survey"
      ],
      "methodology_detail": "Organizes existing debiasing methods in vision and language AI",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.05321v3",
    "title": "Fairness and bias correction in machine learning for depression prediction: results from four study populations",
    "year": 2022,
    "authors": [
      "Vien Ngoc Dang",
      "Anna Cascarano",
      "Rosa H. Mulder",
      "Charlotte Cecil",
      "Maria A. Zuluaga",
      "Jerónimo Hernández-González",
      "Karim Lekadir"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses structural biases in mental healthcare data and emphasizes fairness in ML models, addressing social disparities related to health and inequality.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focus on mental health disparities and bias mitigation",
      "affected_populations": [
        "under-served populations",
        "mental health patients"
      ],
      "methodology": [
        "Machine Learning",
        "Bias mitigation techniques",
        "Systematic study"
      ],
      "methodology_detail": "Includes bias analysis and mitigation in ML models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.04568v1",
    "title": "Towards Algorithmic Fairness in Space-Time: Filling in Black Holes",
    "year": 2022,
    "authors": [
      "Cheryl Flynn",
      "Aritra Guha",
      "Subhabrata Majumdar",
      "Divesh Srivastava",
      "Zhengyi Zhou"
    ],
    "categories": [
      "stat.AP",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses spatial and temporal biases affecting marginalized groups and societal disparities, highlighting issues like digital divides and environmental justice, with a focus on fairness in geospatial AI applications.",
      "inequality_type": [
        "digital",
        "geographic",
        "environmental"
      ],
      "other_detail": "Focus on spatial-temporal societal biases and fairness",
      "affected_populations": [
        "minority populations",
        "urban communities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Developing and adapting ML strategies for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.04442v2",
    "title": "Algorithmic Bias in Machine Learning Based Delirium Prediction",
    "year": 2022,
    "authors": [
      "Sandhya Tripathi",
      "Bradley A Fritz",
      "Michael S Avidan",
      "Yixin Chen",
      "Christopher R King"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how sociodemographic factors like race and sex influence ML model performance, highlighting social disparities in healthcare outcomes.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health",
        "age"
      ],
      "other_detail": "Focus on intersectionality in healthcare prediction models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Experimental Analysis"
      ],
      "methodology_detail": "Evaluates model performance across social subgroups",
      "geographic_focus": [
        "MIMIC-III (US)",
        "another academic hospital"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.03905v1",
    "title": "Dynamics of Gender Bias in Computing",
    "year": 2022,
    "authors": [
      "Thomas J Misa"
    ],
    "categories": [
      "cs.CY",
      "K.1; K.2; K.4.2"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in computing, a social inequality issue, using historical data and analyzing its dynamics over time.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in computing profession",
      "affected_populations": [
        "women in computing"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Archival data analysis over multiple decades",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.06398v1",
    "title": "Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach",
    "year": 2022,
    "authors": [
      "Jiayao Zhang",
      "Hongming Zhang",
      "Zhun Deng",
      "Dan Roth"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness disparities in peer review related to gender, geography, and institutional prestige, addressing social bias and discrimination in academic evaluation processes.",
      "inequality_type": [
        "gender",
        "geographic",
        "educational"
      ],
      "other_detail": "Focuses on bias in academic peer review processes",
      "affected_populations": [
        "female authors",
        "authors from developing countries",
        "less prestigious institutions"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using language models for bias detection and analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.03588v1",
    "title": "Are Face Detection Models Biased?",
    "year": 2022,
    "authors": [
      "Surbhi Mittal",
      "Kartik Thakral",
      "Puspita Majumdar",
      "Mayank Vatsa",
      "Richa Singh"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in face detection related to gender and skin-tone, highlighting disparities across social groups. It addresses fairness issues in AI systems affecting marginalized demographics. The focus on bias and unequal detection performance indicates a concern with social inequality.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in facial detection performance across demographics",
      "affected_populations": [
        "women",
        "people of color"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Evaluation"
      ],
      "methodology_detail": "Analyzing detection accuracy disparities using curated dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.02882v1",
    "title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models",
    "year": 2022,
    "authors": [
      "Yizhi Li",
      "Ge Zhang",
      "Bohao Yang",
      "Chenghua Lin",
      "Shi Wang",
      "Anton Ragni",
      "Jie Fu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes regional bias in language models, which relates to geographical inequality and social discrimination. It addresses biases affecting social groups based on location, a form of social inequality. The focus on bias measurement in AI systems impacts social fairness.",
      "inequality_type": [
        "geographic",
        "social"
      ],
      "other_detail": "Bias in regional and social group representation",
      "affected_populations": [
        "regional groups",
        "social groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and hierarchical evaluation method",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.02139v2",
    "title": "Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity",
    "year": 2022,
    "authors": [
      "Faisal Hamman",
      "Jiahao Chen",
      "Sanghamitra Dutta"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness, bias, and privacy concerns related to protected attributes such as race and gender, which are central to social inequalities. It discusses how querying for fairness metrics can leak sensitive social group information, impacting marginalized populations.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on privacy and fairness in algorithmic bias detection",
      "affected_populations": [
        "racial minorities",
        "gender groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Differential Privacy"
      ],
      "methodology_detail": "Bias query analysis and privacy-preserving techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.01847v1",
    "title": "Seeing the Unseen: Errors and Bias in Visual Datasets",
    "year": 2022,
    "authors": [
      "Hongrui Jin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in visual datasets that lead to racial misrecognition and representation issues, highlighting social bias and fairness concerns in AI systems.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Biases in datasets affecting social groups",
      "affected_populations": [
        "Black people",
        "ethnic minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Tracking dataset errors and impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.01355v1",
    "title": "MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation",
    "year": 2022,
    "authors": [
      "Anna Currey",
      "Maria Nădejde",
      "Raghavendra Pappagari",
      "Mia Mayer",
      "Stanislas Lauly",
      "Xing Niu",
      "Benjamin Hsu",
      "Georgiana Dinu"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender accuracy in machine translation, addressing gender bias and fairness issues, which are related to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Creating gender-balanced, counterfactual translation datasets",
      "geographic_focus": [
        "multiple languages"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.01253v1",
    "title": "Fair Visual Recognition via Intervention with Proxy Features",
    "year": 2022,
    "authors": [
      "Yi Zhang",
      "Jitao Sang",
      "Junyang Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI models related to social attributes like gender and race, which are key aspects of social inequality.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on fairness in societal AI applications",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Causal Intervention",
        "Experiment"
      ],
      "methodology_detail": "Debiasing via proxy features and causal intervention",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.01207v3",
    "title": "Bias-Aware Face Mask Detection Dataset",
    "year": 2022,
    "authors": [
      "Alperen Kantarcı",
      "Ferda Ofli",
      "Muhammad Imran",
      "Hazım Kemal Ekenel"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial and age biases in face mask detection datasets, highlighting social bias issues in AI systems.",
      "inequality_type": [
        "racial",
        "age"
      ],
      "other_detail": "Focus on dataset bias mitigation for fairness",
      "affected_populations": [
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Developing and evaluating bias-aware datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.16449v1",
    "title": "Gender Bias in Computing",
    "year": 2022,
    "authors": [
      "Thomas J. Misa"
    ],
    "categories": [
      "cs.CY",
      "K.2; K.4; K.7.1"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in computing workforce participation, addressing gender inequality. It analyzes historical and cultural factors influencing women's involvement, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender bias in computing workforce",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "New data estimation and gender analysis method",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2211.00073v1",
    "title": "Digital divide among the States of Mexico: a comparison 2010-2020",
    "year": 2022,
    "authors": [
      "Sergio R. Coria",
      "Luz M. Garcia-Garcia"
    ],
    "categories": [
      "cs.CY",
      "J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in ICT access across Mexican states, highlighting geographic and socioeconomic inequalities. It discusses how these disparities affect digital inclusion, which relates to social inequality. The focus on regional differences and access issues indicates a concern with social disparities.",
      "inequality_type": [
        "geographic",
        "socioeconomic",
        "digital"
      ],
      "other_detail": "Focus on regional ICT access disparities in Mexico",
      "affected_populations": [
        "residents of South and Southeast Mexico",
        "households with limited ICT access"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of census data from 2010 and 2020",
      "geographic_focus": [
        "Mexico"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.16024v1",
    "title": "Addressing Bias in Face Detectors using Decentralised Data collection with incentives",
    "year": 2022,
    "authors": [
      "M. R. Ahan",
      "Robin Lehmann",
      "Richard Blythman"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face detection models related to ethnicity, gender, and age, which are social categories. It discusses fairness and bias mitigation, indicating a focus on social disparities in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Bias in AI systems affecting social groups",
      "affected_populations": [
        "ethnic minorities",
        "women",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias evaluation and fairness enrichment in decentralized data collection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.15907v1",
    "title": "Credit-Based Congestion Pricing: Equilibrium Properties and Optimal Scheme Design",
    "year": 2022,
    "authors": [
      "Devansh Jalota",
      "Jessica Lazarus",
      "Alexandre Bayen",
      "Marco Pavone"
    ],
    "categories": [
      "cs.GT",
      "cs.MA",
      "math.OC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on congestion pricing to address social inequity concerns, particularly low-income users' access and benefits, aiming to mitigate socioeconomic disparities in transportation.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on low-income travel credits",
      "affected_populations": [
        "low-income travelers"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Optimization",
        "Case Study"
      ],
      "methodology_detail": "Modeling traffic equilibria and designing optimal schemes",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.15901v1",
    "title": "Mitigating Health Disparities in EHR via Deconfounder",
    "year": 2022,
    "authors": [
      "Zheng Liu",
      "Xiaohan Li",
      "Philip Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses health disparities, which are social inequalities related to health and demographic groups, and discusses fairness in AI healthcare models.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on healthcare disparities and fairness in EHR models",
      "affected_populations": [
        "demographic groups",
        "patients"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses CVAE and empirical experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.15230v1",
    "title": "How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?",
    "year": 2022,
    "authors": [
      "Hritik Bansal",
      "Da Yin",
      "Masoud Monajatipoor",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in AI models related to gender, skin color, and culture, which are social groups affected by inequality. It studies how ethical interventions can influence representation diversity, addressing social bias issues.",
      "inequality_type": [
        "gender",
        "racial",
        "cultural"
      ],
      "other_detail": "Focuses on social bias in AI-generated imagery",
      "affected_populations": [
        "women",
        "racial minorities",
        "cultural groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Evaluates image diversity with ethical prompts",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.14975v1",
    "title": "MABEL: Attenuating Gender Bias using Textual Entailment Data",
    "year": 2022,
    "authors": [
      "Jacqueline He",
      "Mengzhou Xia",
      "Christiane Fellbaum",
      "Danqi Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on mitigating gender bias in AI representations",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Contrastive Learning",
        "Experiment"
      ],
      "methodology_detail": "Using entailment data for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.14424v1",
    "title": "Geographic Citation Gaps in NLP Research",
    "year": 2022,
    "authors": [
      "Mukund Rungta",
      "Janvijay Singh",
      "Saif M. Mohammad",
      "Diyi Yang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in NLP research publication and citation based on geographic location, highlighting inequities among countries and regions.",
      "inequality_type": [
        "geographic",
        "educational",
        "informational"
      ],
      "other_detail": "Focuses on global research publication disparities",
      "affected_populations": [
        "researchers from Africa",
        "researchers from South America",
        "global academic community"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes publication and citation data across regions",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.13664v3",
    "title": "Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher Mixture Model",
    "year": 2022,
    "authors": [
      "Jean-Rémy Conti",
      "Nathan Noiry",
      "Vincent Despiegel",
      "Stéphane Gentric",
      "Stéphan Clémençon"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in face recognition, a social fairness issue.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation via a new loss function",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.11924v3",
    "title": "Men Also Do Laundry: Multi-Attribute Bias Amplification",
    "year": 2022,
    "authors": [
      "Dora Zhao",
      "Jerone T. A. Andrews",
      "Alice Xiang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases, specifically gender bias, in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias amplification in computer vision models",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation benchmarking",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.11762v1",
    "title": "Detecting Unintended Social Bias in Toxic Language Datasets",
    "year": 2022,
    "authors": [
      "Nihar Sahoo",
      "Himanshu Gupta",
      "Pushpak Bhattacharyya"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social biases related to race, gender, religion, political, and LGBTQ groups in toxic language datasets, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "religion",
        "political",
        "LGBTQ"
      ],
      "other_detail": "Bias detection in social groups within AI datasets",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "religious groups",
        "LGBTQ community",
        "political groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Transformer models and bias annotation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.11471v1",
    "title": "Choose Your Lenses: Flaws in Gender Bias Evaluation",
    "year": 2022,
    "authors": [
      "Hadas Orgad",
      "Yonatan Belinkov"
    ],
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates gender bias in AI systems, directly addressing social gender inequality and bias measurement issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias evaluation in AI models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes bias metrics and dataset impacts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.09943v3",
    "title": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition",
    "year": 2022,
    "authors": [
      "Samuel Dooley",
      "Rhea Sanjay Sukthanker",
      "John P. Dickerson",
      "Colin White",
      "Frank Hutter",
      "Micah Goldblum"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in face recognition systems across socio-demographic groups, highlighting fairness issues related to race and gender, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in AI systems affecting social groups",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Neural Architecture Search",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Optimizing models for fairness and accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.08859v1",
    "title": "Social Biases in Automatic Evaluation Metrics for NLG",
    "year": 2022,
    "authors": [
      "Mingqi Gao",
      "Xiaojun Wan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases, particularly gender bias, in AI evaluation metrics, highlighting social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI evaluation metrics",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias quantification and dataset construction",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.08781v2",
    "title": "Stochastic Differentially Private and Fair Learning",
    "year": 2022,
    "authors": [
      "Andrew Lowy",
      "Devansh Gupta",
      "Meisam Razaviyayn"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and discrimination issues in AI models affecting demographic groups such as race, gender, and age, which are central to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "social discrimination"
      ],
      "other_detail": "Focus on fairness and privacy in high-stakes decision systems",
      "affected_populations": [
        "demographic groups",
        "individuals with sensitive attributes"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Stochastic DP fair learning algorithms with convergence guarantees",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.07864v3",
    "title": "Gender Animus Can Still Exist Under Favorable Disparate Impact: a Cautionary Tale from Online P2P Lending",
    "year": 2022,
    "authors": [
      "Xudong Shen",
      "Tianhui Tan",
      "Tuan Q. Phan",
      "Jussi Keppo"
    ],
    "categories": [
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender discrimination in online lending, highlighting biases and disparities affecting female borrowers, thus addressing gender-based social inequality.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender discrimination in financial access",
      "affected_populations": [
        "female borrowers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Estimating discrimination using observational data",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.07626v1",
    "title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation",
    "year": 2022,
    "authors": [
      "Tianxiang Sun",
      "Junliang He",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social biases related to race, gender, religion, age, and socioeconomic status in AI metrics, highlighting fairness issues affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "religious",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI evaluation metrics",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "religious communities",
        "elderly",
        "socioeconomic classes"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias measurement and mitigation in language models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.15500v2",
    "title": "COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation",
    "year": 2022,
    "authors": [
      "Nan Wang",
      "Qifan Wang",
      "Yi-Chia Wang",
      "Maziar Sanjabi",
      "Jingzhou Liu",
      "Hamed Firooz",
      "Hongning Wang",
      "Shaoliang Nie"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI-generated explanations, impacting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in AI explanation generation",
      "affected_populations": [
        "users with protected attributes"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Counterfactual fairness framework and human evaluations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.07455v2",
    "title": "Controlling Bias Exposure for Fair Interpretable Predictions",
    "year": 2022,
    "authors": [
      "Zexue He",
      "Yu Wang",
      "Julian McAuley",
      "Bodhisattwa Prasad Majumder"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in NLP models related to gender and race, which are social categories linked to inequality. It discusses fairness and bias mitigation, key issues in social inequality debates.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation techniques in NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.07269v2",
    "title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models",
    "year": 2022,
    "authors": [
      "Haozhe An",
      "Zongxia Li",
      "Jieyu Zhao",
      "Rachel Rudinger"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases related to demographic groups in NLP models, which are linked to social inequalities such as race, gender, and other social identities.",
      "inequality_type": [
        "racial",
        "gender",
        "other"
      ],
      "other_detail": "demographic biases in social commonsense reasoning",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "generating and testing social bias detection methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.06680v1",
    "title": "Walk a Mile in Their Shoes: a New Fairness Criterion for Machine Learning",
    "year": 2022,
    "authors": [
      "Norman Matloff"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper proposes a fairness criterion based on group-level impacts, addressing bias and fairness issues in AI systems affecting social groups, particularly related to race. It discusses how algorithmic decisions can differentially impact groups, implying a focus on social inequality. The approach aims to mitigate issues like covariate correlation with sensitive attributes, relevant to social bias concerns.",
      "inequality_type": [
        "racial",
        "social",
        "fairness"
      ],
      "other_detail": "Focuses on group-level fairness in machine learning",
      "affected_populations": [
        "racial groups",
        "protected groups"
      ],
      "methodology": [
        "Machine Learning",
        "Empirical Study"
      ],
      "methodology_detail": "Group impact analysis and fairness criterion evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.06630v1",
    "title": "Fairness via Adversarial Attribute Neighbourhood Robust Learning",
    "year": 2022,
    "authors": [
      "Qi Qi",
      "Shervin Ardeshir",
      "Yi Xu",
      "Tianbao Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness across sensitive groups like race and gender, addressing social bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Fairness in AI representations across social groups",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Learning",
        "Optimization"
      ],
      "methodology_detail": "Robust adversarial neighborhood loss and optimization algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.05936v1",
    "title": "Equal Experience in Recommender Systems",
    "year": 2022,
    "authors": [
      "Jaewoong Cho",
      "Moonseok Choi",
      "Changho Suh"
    ],
    "categories": [
      "cs.LG",
      "cs.IR",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues in recommender systems related to biased data reflecting social stereotypes, impacting different social groups.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Fairness in AI systems addressing social stereotypes",
      "affected_populations": [
        "male students",
        "female students"
      ],
      "methodology": [
        "Optimization Framework",
        "Algorithm Development",
        "Experimental Evaluation"
      ],
      "methodology_detail": "Fairness regularization in recommender system algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.05831v1",
    "title": "Social-Group-Agnostic Word Embedding Debiasing via the Stereotype Content Model",
    "year": 2022,
    "authors": [
      "Ali Omrani",
      "Brendan Kennedy",
      "Mohammad Atari",
      "Morteza Dehghani"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias in word embeddings related to social groups such as gender, race, and age, which are connected to social inequalities. It focuses on mitigating stereotypes in AI systems, a key aspect of social fairness. The work aims to reduce social bias in AI, impacting social inequality issues.",
      "inequality_type": [
        "gender",
        "race",
        "age"
      ],
      "other_detail": "addresses social bias in AI systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Debiasing word embeddings using stereotype content model",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.05457v1",
    "title": "Are Pretrained Multilingual Models Equally Fair Across Languages?",
    "year": 2022,
    "authors": [
      "Laura Cabello Piqueras",
      "Anders Søgaard"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness across languages, addressing social bias and discrimination in AI systems, which relates to social inequality issues.",
      "inequality_type": [
        "linguistic",
        "digital",
        "informational"
      ],
      "other_detail": "Focuses on fairness disparities across language groups",
      "affected_populations": [
        "non-English speakers",
        "multilingual communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates fairness disparities across languages and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.05366v1",
    "title": "Race Bias Analysis of Bona Fide Errors in face anti-spoofing",
    "year": 2022,
    "authors": [
      "Latifah Abduh",
      "Ioannis Ivrissimtzis"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG",
      "I.5.4; I.5.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly examines race bias in face anti-spoofing, addressing racial fairness issues in AI systems, which relates directly to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on race bias in biometric AI systems",
      "affected_populations": [
        "racial groups",
        "ethnic minorities"
      ],
      "methodology": [
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias analysis across response distributions and latent space",
      "geographic_focus": [
        "Replay Attack database",
        "SiW database",
        "RFW database"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.05332v1",
    "title": "Gender Stereotyping Impact in Facial Expression Recognition",
    "year": 2022,
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in facial expression recognition, highlighting social stereotypes and their impact on different gender groups, which relates directly to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Altering gender proportions to measure bias effects",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.04369v1",
    "title": "A Differentiable Distance Approximation for Fairer Image Classification",
    "year": 2022,
    "authors": [
      "Nicholas Rosa",
      "Tom Drummond",
      "Mehrtash Harandi"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI models, addressing bias related to protected attributes like ethnicity and gender, which are social inequalities.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias measurement and fairness in AI models",
      "affected_populations": [
        "ethnic groups",
        "women",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and fairness optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.03901v2",
    "title": "A fairness assessment of mobility-based COVID-19 case prediction models",
    "year": 2022,
    "authors": [
      "Abdolmajid Erfani",
      "Vanessa Frias-Martinez"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in mobility data affecting different demographic groups, highlighting racial, socioeconomic, age, and urban-rural disparities in COVID-19 prediction accuracy.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "age",
        "geographic"
      ],
      "other_detail": "Bias in data representation across social groups",
      "affected_populations": [
        "older",
        "poorer",
        "non-white",
        "less educated"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Correlation Study"
      ],
      "methodology_detail": "Analyzed model performance against sociodemographic traits",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.06351v1",
    "title": "A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter",
    "year": 2022,
    "authors": [
      "Kyra Yee",
      "Alice Schoenauer Sebag",
      "Olivia Redfield",
      "Emily Sheng",
      "Matthias Eck",
      "Luca Belli"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in content moderation affecting marginalized groups, highlighting social discrimination and unequal impacts of AI systems.",
      "inequality_type": [
        "racial",
        "social",
        "digital"
      ],
      "other_detail": "Bias in AI moderation of marginalized communities",
      "affected_populations": [
        "marginalized groups",
        "Twitter users"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Detects and measures harm severity without demographic labels",
      "geographic_focus": [
        "Twitter (global platform)"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.02516v1",
    "title": "Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation",
    "year": 2022,
    "authors": [
      "I. Elizabeth Kumar",
      "Keegan E. Hines",
      "John P. Dickerson"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses credit access disparities affecting socioeconomic groups and addresses fairness issues in algorithms impacting marginalized populations.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "racial"
      ],
      "other_detail": "Focus on credit access and discrimination in lending",
      "affected_populations": [
        "demographic groups",
        "minorities",
        "low-income individuals"
      ],
      "methodology": [
        "Literature Review",
        "Regulatory Analysis",
        "Fair ML Research"
      ],
      "methodology_detail": "Analyzes legal, policy, and technical aspects of fairness",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.02015v2",
    "title": "Conformalized Fairness via Quantile Regression",
    "year": 2022,
    "authors": [
      "Meichen Liu",
      "Lei Ding",
      "Dengdeng Yu",
      "Wulong Liu",
      "Linglong Kong",
      "Bei Jiang"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in algorithmic predictions related to sensitive attributes like race and gender, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI predictions for social groups",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness adjustment and quantile regression techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.01933v1",
    "title": "PreprintMatch: a tool for preprint publication detection applied to analyze global inequities in scientific publishing",
    "year": 2022,
    "authors": [
      "Peter Eckmann",
      "Anita Bandrowski"
    ],
    "categories": [
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes disparities in scientific publishing related to income and country-level resources, highlighting inequities between low and high income countries.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focuses on global research publication disparities",
      "affected_populations": [
        "researchers from low income countries",
        "global scientific community"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Matching preprints to published papers, statistical comparison",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.00139v2",
    "title": "Code Reviews in Open Source Projects : How Do Gender Biases Affect Participation and Outcomes?",
    "year": 2022,
    "authors": [
      "Sayma Sultana",
      "Asif Kamal Turzo",
      "Amiangshu Bosu"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender biases in open source software development, highlighting disparities in participation and outcomes based on gender, which relates directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women developers",
        "men developers"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Regression models on project datasets",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.15605v8",
    "title": "Bias Mimicking: A Simple Sampling Approach for Bias Mitigation",
    "year": 2022,
    "authors": [
      "Maan Qraitem",
      "Kate Saenko",
      "Bryan A. Plummer"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses dataset bias related to underrepresented groups such as gender, which impacts fairness in visual recognition systems, reflecting social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias mitigation in AI datasets",
      "affected_populations": [
        "women",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Sampling Methods",
        "Algorithm Development"
      ],
      "methodology_detail": "Class-conditioned sampling for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.15550v1",
    "title": "The More Secure, The Less Equally Usable: Gender and Ethnicity (Un)fairness of Deep Face Recognition along Security Thresholds",
    "year": 2022,
    "authors": [
      "Andrea Atzori",
      "Gianni Fenu",
      "Mirko Marras"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in face recognition accuracy across demographic groups, highlighting fairness issues related to gender and ethnicity, which are social categories. It discusses how security thresholds impact usability disparities among these groups, indicating a focus on social bias in AI systems.",
      "inequality_type": [
        "gender",
        "ethnic"
      ],
      "other_detail": "Fairness disparities in biometric security systems",
      "affected_populations": [
        "gender groups",
        "ethnic groups"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing face recognition performance across demographics and security levels",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2210.08983v1",
    "title": "Temporal Analysis and Gender Bias in Computing",
    "year": 2022,
    "authors": [
      "Thomas J. Misa"
    ],
    "categories": [
      "cs.CY",
      "K.2; K.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias and gender shifts in computing, highlighting social disparities and biases related to gender over time.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in computing history",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of historical name-gender data over decades",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.13939v1",
    "title": "Racial Bias in the Beautyverse",
    "year": 2022,
    "authors": [
      "Piera Riccio",
      "Nuria Oliver"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.SI",
      "I.2.m; J.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial biases in beauty filters, highlighting social discrimination and bias in AI systems affecting racial groups.",
      "inequality_type": [
        "racial"
      ],
      "other_detail": "focus on racial bias in social media beauty filters",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Computer Vision",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing biases in beauty filter techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.13678v2",
    "title": "FAIR-FATE: Fair Federated Learning with Momentum",
    "year": 2022,
    "authors": [
      "Teresa Salazar",
      "Miguel Fernandes",
      "Helder Araujo",
      "Pedro Henriques Abreu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "68T07",
      "I.2.m"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing discrimination related to sensitive attributes like race and gender, which are key social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social discrimination"
      ],
      "other_detail": "Fairness in decentralized machine learning context",
      "affected_populations": [
        "unprivileged groups",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware federated learning algorithm evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.13627v2",
    "title": "How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI",
    "year": 2022,
    "authors": [
      "Kaiping Chen",
      "Anqi Shao",
      "Jirayu Burapacheep",
      "Yixuan Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how GPT-3 responses vary across social groups on issues like climate change and BLM, highlighting disparities in user experience and response tone, which relate to social inequalities such as race, education, and opinions.",
      "inequality_type": [
        "racial",
        "educational",
        "opinion-based"
      ],
      "other_detail": "Focuses on social disparities in AI-human interactions",
      "affected_populations": [
        "minority groups",
        "education minorities",
        "opinion minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Auditing GPT-3 responses across diverse groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.13177v7",
    "title": "Addressing Fairness Issues in Deep Learning-Based Medical Image Analysis: A Systematic Review",
    "year": 2022,
    "authors": [
      "Zikang Xu",
      "Jun Li",
      "Qingsong Yao",
      "Han Li",
      "Mingyue Zhao",
      "S. Kevin Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness issues in medical AI, focusing on disparities across subgroups like gender, which relates to social inequality. It addresses algorithmic bias affecting specific social groups, highlighting social discrimination concerns.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on fairness in medical AI systems",
      "affected_populations": [
        "elderly females",
        "medical patients"
      ],
      "methodology": [
        "Fairness Evaluation",
        "Unfairness Mitigation",
        "Literature Review"
      ],
      "methodology_detail": "Categorizes and reviews fairness approaches in MedIA",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.12226v5",
    "title": "Re-contextualizing Fairness in NLP: The Case of India",
    "year": 2022,
    "authors": [
      "Shaily Bhatt",
      "Sunipa Dev",
      "Partha Talukdar",
      "Shachi Dave",
      "Vinodkumar Prabhakaran"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases and stereotypes in NLP related to social disparities in India, focusing on fairness and societal axes such as region and religion.",
      "inequality_type": [
        "religion",
        "region",
        "social disparities"
      ],
      "other_detail": "Focus on cultural and societal biases in NLP models",
      "affected_populations": [
        "religious groups",
        "regional communities"
      ],
      "methodology": [
        "NLP",
        "Dataset Creation",
        "Bias Evaluation"
      ],
      "methodology_detail": "Developing fairness resources and bias analysis tools",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.11984v3",
    "title": "Gender Bias in Fake News: An Analysis",
    "year": 2022,
    "authors": [
      "Navya Sahadevan",
      "Deepak P"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias, a form of social inequality, in fake news, highlighting issues of gender discrimination and bias in media content.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Lexicon-based analysis over benchmark datasets",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.10589v1",
    "title": "Understanding and Developing Equitable and Fair Transportation Systems",
    "year": 2022,
    "authors": [
      "Weizi Li"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses infrastructure's role in facilitating inequality, connecting affluent and poor communities, and aims to promote equity in transportation systems.",
      "inequality_type": [
        "economic",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on infrastructure's impact on social disparities",
      "affected_populations": [
        "poor communities",
        "affluent communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "System Design"
      ],
      "methodology_detail": "Analyzing network and demographic data",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.10222v4",
    "title": "Fairness Reprogramming",
    "year": 2022,
    "authors": [
      "Guanhua Zhang",
      "Yihua Zhang",
      "Yang Zhang",
      "Wenqi Fan",
      "Qing Li",
      "Sijia Liu",
      "Shiyu Chang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, aiming to mitigate biases related to social groups, which directly relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on demographic bias mitigation in AI models",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Criterion Evaluation"
      ],
      "methodology_detail": "Uses fairness criteria and reprogramming techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.11762v1",
    "title": "Towards Auditing Unsupervised Learning Algorithms and Human Processes For Fairness",
    "year": 2022,
    "authors": [
      "Ian Davidson",
      "S. S. Ravi"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in algorithms, which relates to social bias and discrimination issues affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness auditing in AI systems",
      "affected_populations": [
        "social groups",
        "protected groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates fairness across multiple social groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.09592v1",
    "title": "Closing the Gender Wage Gap: Adversarial Fairness in Job Recommendation",
    "year": 2022,
    "authors": [
      "Clara Rus",
      "Jeffrey Luppes",
      "Harrie Oosterhuis",
      "Gido H. Schoenmacker"
    ],
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in job recommendations, aiming to reduce wage disparities, which directly relates to gender-based social inequality.",
      "inequality_type": [
        "gender",
        "economic",
        "income"
      ],
      "other_detail": "Focuses on gender wage gap mitigation via AI fairness techniques",
      "affected_populations": [
        "women",
        "job seekers"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Uses adversarial networks to debias word representations",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.09035v3",
    "title": "Fairness in Face Presentation Attack Detection",
    "year": 2022,
    "authors": [
      "Meiling Fang",
      "Wufei Yang",
      "Arjan Kuijper",
      "Vitomir Struc",
      "Naser Damer"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in face presentation attack detection, highlighting disparities across demographic groups such as gender and occlusion features, which relate to social inequalities like gender bias.",
      "inequality_type": [
        "gender",
        "disability"
      ],
      "other_detail": "Focuses on fairness disparities in AI face security systems",
      "affected_populations": [
        "female",
        "faces with occlusion"
      ],
      "methodology": [
        "Dataset Creation",
        "Fairness Analysis",
        "Experiment"
      ],
      "methodology_detail": "Includes dataset annotation and fairness evaluation methods",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.08321v1",
    "title": "Enhanced Fairness Testing via Generating Effective Initial Individual Discriminatory Instances",
    "year": 2022,
    "authors": [
      "Minghua Ma",
      "Zhao Tian",
      "Max Hort",
      "Federica Sarro",
      "Hongyu Zhang",
      "Qingwei Lin",
      "Dongmei Zhang"
    ],
    "categories": [
      "cs.SE",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness testing in AI, addressing discrimination related to protected attributes like race and age, which are social inequality issues.",
      "inequality_type": [
        "racial",
        "age",
        "fairness"
      ],
      "other_detail": "Focus on individual discrimination in AI decision-making",
      "affected_populations": [
        "racial minorities",
        "elderly",
        "discriminated individuals"
      ],
      "methodology": [
        "Experiment",
        "Fairness Testing"
      ],
      "methodology_detail": "Seed selection for generating discriminatory instances",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.07912v3",
    "title": "Algorithmic decision making methods for fair credit scoring",
    "year": 2022,
    "authors": [
      "Darie Moldovan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias mitigation in credit scoring, which impacts social groups and can perpetuate inequalities.",
      "inequality_type": [
        "economic",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on algorithmic fairness in financial decision-making",
      "affected_populations": [
        "loan applicants",
        "disadvantaged groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metrics",
        "Bias Mitigation Methods",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluation of bias mitigation techniques across fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.07375v1",
    "title": "Wealth Dynamics Over Generations: Analysis and Interventions",
    "year": 2022,
    "authors": [
      "Krishna Acharya",
      "Eshwar Ram Arunachaleswaran",
      "Sampath Kannan",
      "Aaron Roth",
      "Juba Ziani"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper models wealth dynamics across generations, focusing on inequality persistence and interventions, which directly relates to socioeconomic and wealth inequality.",
      "inequality_type": [
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Focuses on intergenerational wealth disparities",
      "affected_populations": [
        "less wealthy",
        "generations"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling wealth dynamics and intervention effects",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.07044v1",
    "title": "Fair Inference for Discrete Latent Variable Models",
    "year": 2022,
    "authors": [
      "Rashidul Islam",
      "Shimei Pan",
      "James R. Foulds"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI models, addressing societal biases and discrimination, particularly in criminal justice risk assessments, which relate to social inequalities such as race and justice.",
      "inequality_type": [
        "racial",
        "justice",
        "discrimination"
      ],
      "other_detail": "Focus on fairness in criminal justice AI systems",
      "affected_populations": [
        "racial minorities",
        "criminal defendants"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Fairness penalty in probabilistic graphical models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.14237v1",
    "title": "Accuracy, Fairness, and Interpretability of Machine Learning Criminal Recidivism Models",
    "year": 2022,
    "authors": [
      "Eric Ingram",
      "Furkan Gursoy",
      "Ioannis A. Kakadiaris"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in criminal recidivism models, which impact marginalized social groups. It discusses trade-offs between accuracy and fairness, indicating concern with social discrimination. The focus on fairness issues in AI systems relates directly to social inequality.",
      "inequality_type": [
        "racial",
        "social",
        "discrimination"
      ],
      "other_detail": "focus on fairness in criminal justice",
      "affected_populations": [
        "racial minorities",
        "parolees"
      ],
      "methodology": [
        "Machine Learning",
        "Comparative Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates models on accuracy, fairness, and interpretability",
      "geographic_focus": [
        "Georgia, United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.05602v1",
    "title": "It's Not Fairness, and It's Not Fair: The Failure of Distributional Equality and the Promise of Relational Equality in Complete-Information Hiring Games",
    "year": 2022,
    "authors": [
      "Benjamin Fish",
      "Luke Stark"
    ],
    "categories": [
      "cs.CY",
      "cs.GT",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses social relations and fairness in hiring, highlighting social injustice beyond resource distribution.",
      "inequality_type": [
        "social",
        "class",
        "gender",
        "racial"
      ],
      "other_detail": "Focus on relational social unfairness in employment contexts",
      "affected_populations": [
        "job applicants",
        "hiring candidates"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Analyzes equilibrium in hiring markets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.05274v4",
    "title": "Fairness in Forecasting of Observations of Linear Dynamical Systems",
    "year": 2022,
    "authors": [
      "Quan Zhou",
      "Jakub Marecek",
      "Robert N. Shorten"
    ],
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.DS",
      "math.ST",
      "stat.TH"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI forecasting, focusing on under-representation bias affecting social groups. It extends fairness notions to dynamical systems, with empirical tests on biased datasets related to insurance and criminal justice. These aspects relate directly to social discrimination and inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on fairness in time-series forecasting",
      "affected_populations": [
        "social groups",
        "underrepresented subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness-constrained learning and convexification techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.03661v1",
    "title": "Efficient Gender Debiasing of Pre-trained Indic Language Models",
    "year": 2022,
    "authors": [
      "Neeraja Kirtane",
      "V Manushree",
      "Aditya Kane"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, reflecting societal gender inequalities. It focuses on mitigating gender bias in AI systems, which impacts social fairness. The work directly relates to social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and mitigation in language models",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.03089v1",
    "title": "Decoding Demographic un-fairness from Indian Names",
    "year": 2022,
    "authors": [
      "Medidoddi Vahini",
      "Jalend Bantupalli",
      "Souvic Chakraborty",
      "Animesh Mukherjee"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.DL",
      "cs.LG",
      "cs.SI",
      "J.4; K.4.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on demographic classification related to gender and caste, which are social categories linked to inequality. It analyzes biases in AI models that impact social groups within India, highlighting issues of fairness and discrimination.",
      "inequality_type": [
        "gender",
        "caste",
        "socioeconomic"
      ],
      "other_detail": "Focus on social groups in Indian demographic context",
      "affected_populations": [
        "women",
        "caste groups"
      ],
      "methodology": [
        "Machine Learning",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Training classifiers on demographic data",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.02965v3",
    "title": "Risk of Bias in Chest Radiography Deep Learning Foundation Models",
    "year": 2022,
    "authors": [
      "Ben Glocker",
      "Charles Jones",
      "Melanie Roschewitz",
      "Stefan Winzeck"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "eess.IV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in AI models, highlighting disparities in performance across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias in medical AI affecting vulnerable groups",
      "affected_populations": [
        "female patients",
        "Black patients",
        "Asian patients"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Bias detection and performance disparity analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.02836v2",
    "title": "Studying Bias in GANs through the Lens of Race",
    "year": 2022,
    "authors": [
      "Vongani H. Maluleke",
      "Neerja Thakkar",
      "Tim Brooks",
      "Ethan Weber",
      "Trevor Darrell",
      "Alexei A. Efros",
      "Angjoo Kanazawa",
      "Devin Guillory"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "I.4"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in generative AI models, highlighting racial disparities in image quality and representation, which directly relate to social racial inequalities.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focuses on racial bias in AI-generated images",
      "affected_populations": [
        "Black people",
        "White people"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes racial distributions and image quality metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.02793v1",
    "title": "\"Es geht um Respekt, nicht um Technologie\": Erkenntnisse aus einem Interessensgruppen-übergreifenden Workshop zu genderfairer Sprache und Sprachtechnologie",
    "year": 2022,
    "authors": [
      "Sabrina Burtscher",
      "Katta Spiel",
      "Lukas Daniel Klausner",
      "Manuel Lardelli",
      "Dagmar Gromann"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender fairness in language technology, focusing on social gender identities and their representation, which relates to social discrimination and bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender fairness and social negotiation",
      "affected_populations": [
        "non-binary individuals",
        "transgender people",
        "language users"
      ],
      "methodology": [
        "Workshop",
        "Qualitative Study"
      ],
      "methodology_detail": "Participatory workshop with diverse interest groups",
      "geographic_focus": [
        "Austria"
      ],
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.01627v1",
    "title": "A systematic study of race and sex bias in CNN-based cardiac MR segmentation",
    "year": 2022,
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Anton",
      "Bram Ruijsink",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic biases related to race and sex in medical AI, highlighting disparities in health data representation and their impact on different groups.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on medical imaging bias and demographic representation",
      "affected_populations": [
        "racial groups",
        "sex groups"
      ],
      "methodology": [
        "Experiment",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Training CNN models with varied demographic data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.14175v1",
    "title": "RAGUEL: Recourse-Aware Group Unfairness Elimination",
    "year": 2022,
    "authors": [
      "Aparajita Haldar",
      "Teddy Cunningham",
      "Hakan Ferhatosmanoglu"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making systems, focusing on group-level disparities and recourse, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in algorithms",
      "affected_populations": [
        "sub-groups",
        "disadvantaged groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Re-ranking",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Fairness-aware re-ranking and intervention strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.13528v1",
    "title": "CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions",
    "year": 2022,
    "authors": [
      "Arezou Pakzad",
      "Kumar Abhishek",
      "Ghassan Hamarneh"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in skin lesion classification across skin types, highlighting social disparities related to race and health. It aims to reduce bias in AI systems affecting different demographic groups. The focus on skin type fairness relates to racial and health inequalities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias reduction in medical AI systems",
      "affected_populations": [
        "dark skin individuals",
        "light skin individuals"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware representation learning and evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2209.05440v1",
    "title": "Bias Impact Analysis of AI in Consumer Mobile Health Technologies: Legal, Technical, and Policy",
    "year": 2022,
    "authors": [
      "Kristine Gloria",
      "Nidhi Rastogi",
      "Stevie DeGroff"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines algorithmic bias in health technologies affecting marginalized groups, addressing social discrimination and unequal impacts across social groups.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on health disparities and marginalized populations",
      "affected_populations": [
        "patients",
        "mental health users"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Assessing bias mitigation mechanisms in AI systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.13235v1",
    "title": "How Segregation Patterns Affect the Availability of Fair District Plans",
    "year": 2022,
    "authors": [
      "William Hager",
      "Betseygail Rand"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial segregation and its impact on fair districting, addressing racial inequality and social fairness issues.",
      "inequality_type": [
        "racial",
        "geographic",
        "social fairness"
      ],
      "other_detail": "Focus on racial segregation and district fairness",
      "affected_populations": [
        "minority groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Simulation"
      ],
      "methodology_detail": "Synthetic city modeling and statistical validation",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.13061v3",
    "title": "On Biased Behavior of GANs for Face Verification",
    "year": 2022,
    "authors": [
      "Sasikanth Kotti",
      "Mayank Vatsa",
      "Richa Singh"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in face verification systems related to race and age, highlighting fairness issues and disparate impacts on social groups.",
      "inequality_type": [
        "racial",
        "age"
      ],
      "other_detail": "Bias in synthetic face data generation",
      "affected_populations": [
        "racial groups",
        "age groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Fairness Analysis"
      ],
      "methodology_detail": "Analyzes bias in GAN-generated data and face verification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.12786v1",
    "title": "LUCID: Exposing Algorithmic Bias through Inverse Design",
    "year": 2022,
    "authors": [
      "Carmen Mazijn",
      "Carina Prunkl",
      "Andres Algaba",
      "Jan Danckaert",
      "Vincent Ginis"
    ],
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias and fairness, which relate to social discrimination and inequality issues such as race, gender, and socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness and bias in AI decision-making",
      "affected_populations": [
        "social groups",
        "disadvantaged communities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Using canonical sets to reveal biases",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.12649v1",
    "title": "Targeted Influence with Community and Gender-Aware Seeding",
    "year": 2022,
    "authors": [
      "Maciej Styczen",
      "Bing-Jyue Chen",
      "Ya-Wen Teng",
      "Yvonne-Anne Pignolet",
      "Lydia Chen",
      "De-Nian Yang"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender imbalance in information dissemination, a social inequality issue, by proposing gender-aware seeding algorithms.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "females"
      ],
      "methodology": [
        "Algorithm Development",
        "Experiment"
      ],
      "methodology_detail": "Designing and validating a seeding algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.12212v2",
    "title": "Sustaining Fairness via Incremental Learning",
    "year": 2022,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Snigdha Chaturvedi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems, focusing on bias related to demographics such as gender and age, which are social categories associated with inequality.",
      "inequality_type": [
        "gender",
        "age",
        "demographic"
      ],
      "other_detail": "Fairness in decision-making systems",
      "affected_populations": [
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Representation Learning",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness-aware incremental learning system",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.11744v1",
    "title": "Enforcing Delayed-Impact Fairness Guarantees",
    "year": 2022,
    "authors": [
      "Aline Weber",
      "Blossom Metevier",
      "Yuriy Brun",
      "Philip S. Thomas",
      "Bruno Castro da Silva"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses long-term impacts of AI on social fairness, addressing inequality in decision-making affecting disadvantaged groups.",
      "inequality_type": [
        "social",
        "educational",
        "economic"
      ],
      "other_detail": "Focuses on long-term fairness impacts of AI systems",
      "affected_populations": [
        "disadvantaged individuals",
        "communities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Fairness guarantees in classification algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.11099v1",
    "title": "Explaining Bias in Deep Face Recognition via Image Characteristics",
    "year": 2022,
    "authors": [
      "Andrea Atzori",
      "Gianni Fenu",
      "Mirko Marras"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and performance disparities in face recognition models across social groups defined by gender and ethnicity, addressing social bias issues.",
      "inequality_type": [
        "gender",
        "ethnic",
        "racial"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "women",
        "ethnic minorities"
      ],
      "methodology": [
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes model performance across attribute groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.10625v1",
    "title": "Evaluation of group fairness measures in student performance prediction problems",
    "year": 2022,
    "authors": [
      "Tai Le Quy",
      "Thi Huyen Nguyen",
      "Gunnar Friege",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness measures in student performance prediction, addressing social fairness issues related to protected attributes like gender and race.",
      "inequality_type": [
        "educational",
        "gender",
        "racial"
      ],
      "other_detail": "Focuses on fairness in educational data mining",
      "affected_populations": [
        "students by gender",
        "students by race"
      ],
      "methodology": [
        "Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparative assessment of fairness measures",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.10451v2",
    "title": "Minimax AUC Fairness: Efficient Algorithm with Provable Convergence",
    "year": 2022,
    "authors": [
      "Zhenhuan Yang",
      "Yan Lok Ko",
      "Kush R. Varshney",
      "Yiming Ying"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning, focusing on mitigating disparities affecting marginalized groups based on race and gender, which are key social inequality dimensions.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on algorithmic fairness and societal impact",
      "affected_populations": [
        "marginalized groups",
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Includes fairness metric design and convergence proof",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.10095v1",
    "title": "Socially Fair Center-based and Linear Subspace Clustering",
    "year": 2022,
    "authors": [
      "Sruthi Gorantla",
      "Kishen N. Gowda",
      "Amit Deshpande",
      "Anand Louis"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in clustering, addressing disparities across sensitive groups, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic clustering across social groups",
      "affected_populations": [
        "demographic groups",
        "social groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Approximation algorithms for fair clustering",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.10013v1",
    "title": "FairDisCo: Fairer AI in Dermatology via Disentanglement Contrastive Learning",
    "year": 2022,
    "authors": [
      "Siyi Du",
      "Ben Hers",
      "Nourhan Bayasi",
      "Ghassan Hamarneh",
      "Rafeef Garbi"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial disparities in skin lesion diagnosis, highlighting bias and fairness issues in AI systems affecting darker skin types.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on skin-type bias in dermatology AI models",
      "affected_populations": [
        "darker skin types",
        "racial groups"
      ],
      "methodology": [
        "Deep Learning",
        "Fairness Evaluation",
        "Contrastive Learning"
      ],
      "methodology_detail": "Disentanglement and contrastive learning for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.08382v1",
    "title": "Deep Generative Views to Mitigate Gender Classification Bias Across Gender-Race Groups",
    "year": 2022,
    "authors": [
      "Sreeraj Ramachandran",
      "Ajita Rattani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender and racial bias in AI, highlighting social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on bias mitigation in facial gender classification",
      "affected_populations": [
        "women",
        "dark-skinned people"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Bias mitigation strategies and extensive validation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.08881v2",
    "title": "Modelling the long-term fairness dynamics of data-driven targeted help on job seekers",
    "year": 2022,
    "authors": [
      "Sebastian Scher",
      "Simone Kopeinik",
      "Andreas Trügler",
      "Dominik Kowald"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in targeted labor market interventions, focusing on group disparities such as gender. It discusses the impact of algorithmic decision-making on marginalized groups and fairness trade-offs over time.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on labor market and employment fairness",
      "affected_populations": [
        "job seekers",
        "minorities"
      ],
      "methodology": [
        "Statistical Analysis",
        "Model Development"
      ],
      "methodology_detail": "Combines dynamical modeling with data-driven approaches",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.07918v2",
    "title": "Ex-Ante Assessment of Discrimination in Dataset",
    "year": 2022,
    "authors": [
      "Jonathan Vasquez",
      "Xavier Gitiaux",
      "Huzefa Rangwala"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on identifying biases in datasets that may lead to discrimination against demographic groups, addressing social bias and fairness issues in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "religion"
      ],
      "other_detail": "Bias detection in sensitive demographic features",
      "affected_populations": [
        "minority groups",
        "underprivileged communities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Detects bias in datasets affecting social groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.08279v1",
    "title": "Error Parity Fairness: Testing for Group Fairness in Regression Tasks",
    "year": 2022,
    "authors": [
      "Furkan Gursoy",
      "Ioannis A. Kakadiaris"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI predictions across sensitive groups, specifically race, highlighting disparities in forecast errors, which relates to social inequalities such as racial disparities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on race-based differences in COVID-19 forecast errors",
      "affected_populations": [
        "racial groups",
        "county populations"
      ],
      "methodology": [
        "Statistical Analysis",
        "Permutation Test",
        "Case Study"
      ],
      "methodology_detail": "Testing fairness in regression predictions across groups",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.07613v1",
    "title": "Does lossy image compression affect racial bias within face recognition?",
    "year": 2022,
    "authors": [
      "Seyma Yucer",
      "Matt Poyser",
      "Noura Al Moubayed",
      "Toby P. Breckon"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in face recognition, highlighting disparities across racial groups, which relates directly to social inequality and discrimination.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial bias in AI systems",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement across racial phenotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.07472v1",
    "title": "Towards Inclusive HRI: Using Sim2Real to Address Underrepresentation in Emotion Expression Recognition",
    "year": 2022,
    "authors": [
      "Saba Akhyani",
      "Mehryar Abbasi Boroujeni",
      "Mo Chen",
      "Angelica Lim"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses underrepresentation of minority groups in emotion recognition, aiming to reduce bias and improve fairness in AI systems interacting with diverse populations.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on underrepresented social and demographic groups",
      "affected_populations": [
        "racial minorities",
        "gender minorities"
      ],
      "methodology": [
        "Simulation",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Using synthetic data to improve fairness in emotion recognition",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.06680v3",
    "title": "Locating disparities in machine learning",
    "year": 2022,
    "authors": [
      "Moritz von Zahn",
      "Oliver Hinz",
      "Stefan Feuerriegel"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on detecting disparities in machine learning outcomes across subgroups defined by sensitive attributes like age and gender, addressing issues of algorithmic fairness and discrimination.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Focus on algorithmic disparities and fairness",
      "affected_populations": [
        "women",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Data-driven framework for locating disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.06648v4",
    "title": "Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness",
    "year": 2022,
    "authors": [
      "Vincent Jeanselme",
      "Maria De-Arteaga",
      "Zhe Zhang",
      "Jessica Barrett",
      "Brian Tom"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in healthcare data influenced by societal and decision biases, impacting algorithmic fairness across social groups.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "racial",
        "gender"
      ],
      "other_detail": "Focus on healthcare data missingness and fairness implications",
      "affected_populations": [
        "patients",
        "healthcare groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Analyzes imputation strategies and fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.06613v1",
    "title": "A Study of Demographic Bias in CNN-based Brain MR Segmentation",
    "year": 2022,
    "authors": [
      "Stefanos Ioannou",
      "Hana Chockler",
      "Alexander Hammers",
      "Andrew P. King"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates demographic biases related to race and sex in AI models, highlighting potential impacts on health inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias in medical AI affecting marginalized groups",
      "affected_populations": [
        "Black individuals",
        "Women",
        "White individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias assessment in CNN segmentation performance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.05126v1",
    "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
    "year": 2022,
    "authors": [
      "Bhavya Ghai",
      "Klaus Mueller"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.HC",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias related to gender and race, impacting fairness and social equity. It focuses on mitigating social biases in AI systems affecting marginalized groups.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias mitigation in social datasets",
      "affected_populations": [
        "women",
        "black females"
      ],
      "methodology": [
        "Algorithm Auditing",
        "System Design",
        "Experiment"
      ],
      "methodology_detail": "Causal modeling and human-in-the-loop system",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.04440v2",
    "title": "Dispersion vs Disparity: Hiding Variability Can Encourage Stereotyping When Visualizing Social Outcomes",
    "year": 2022,
    "authors": [
      "Eli Holder",
      "Cindy Xiong"
    ],
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how visualization design influences perceptions of social disparities, specifically addressing social inequality related to race, gender, and other groups, and its potential to reinforce stereotypes.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on visual perception of social outcome disparities",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Visual perception experiments with crowdworkers",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.03621v1",
    "title": "Bias Reducing Multitask Learning on Mental Health Prediction",
    "year": 2022,
    "authors": [
      "Khadija Zanna",
      "Kusha Sridhar",
      "Han Yu",
      "Akane Sano"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes bias related to demographic groups in mental health AI models, addressing social disparities.",
      "inequality_type": [
        "racial",
        "ethnic",
        "age",
        "income",
        "health"
      ],
      "other_detail": "Bias mitigation in physiological-based mental health prediction",
      "affected_populations": [
        "demographic groups",
        "mental health patients"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Bias Mitigation"
      ],
      "methodology_detail": "Multi-task learning and epistemic uncertainty approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.03209v1",
    "title": "Bias and Fairness in Computer Vision Applications of the Criminal Justice System",
    "year": 2022,
    "authors": [
      "Sophie Noiret",
      "Jennifer Lumetzberger",
      "Martin Kampel"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI-driven practices in the criminal justice system that impact minority groups, highlighting issues of fairness and discrimination. It examines social bias and the risks of unfair impacts on marginalized populations.",
      "inequality_type": [
        "racial",
        "ethnic",
        "discrimination"
      ],
      "other_detail": "Focus on criminal justice and minority groups",
      "affected_populations": [
        "minority groups",
        "suspects",
        "communities"
      ],
      "methodology": [
        "Literature Review",
        "Experiment",
        "System Design"
      ],
      "methodology_detail": "Analyzing existing applications and risks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.02052v5",
    "title": "Large scale analysis of gender bias and sexism in song lyrics",
    "year": 2022,
    "authors": [
      "Lorenzo Betti",
      "Carlo Abrate",
      "Andreas Kaltenbrunner"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender bias and sexism in song lyrics, addressing social discrimination related to gender. It examines language biases and their evolution over time, highlighting societal inequalities reflected in popular culture.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on language bias and sexism in music",
      "affected_populations": [
        "women",
        "male artists"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes large-scale lyrics data and word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.01802v1",
    "title": "Mutual Information Scoring: Increasing Interpretability in Categorical Clustering Tasks with Applications to Child Welfare Data",
    "year": 2022,
    "authors": [
      "Pranav Sankhe",
      "Seventy F. Hall",
      "Melanie Sage",
      "Maria Y. Rodriquez",
      "Varun Chandola",
      "Kenneth Joseph"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases in data related to foster youth, highlighting systemic inequalities affecting marginalized groups.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "disability"
      ],
      "other_detail": "Analyzes systemic biases in administrative foster care data",
      "affected_populations": [
        "foster youth",
        "systemically marginalized groups"
      ],
      "methodology": [
        "Categorical Clustering",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Novel clustering to reveal data biases",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.01755v3",
    "title": "Debiasing Gender Bias in Information Retrieval Models",
    "year": 2022,
    "authors": [
      "Dhanasekar Sundararaman",
      "Vivek Subramanian"
    ],
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in information retrieval models, highlighting biases that impact gender representation and fairness, which are social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias mitigation and model evaluation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.01341v2",
    "title": "Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories",
    "year": 2022,
    "authors": [
      "Gizem Sogancioglu",
      "Fabian Mijsters",
      "Amar van Uden",
      "Jelle Peperzak"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in clinical embeddings, highlighting societal stereotypes and potential harm, which relates to gender inequality and social bias in AI systems.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focuses on gender bias in medical AI applications",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement in clinical word embeddings",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.01157v2",
    "title": "Global Performance Disparities Between English-Language Accents in Automatic Speech Recognition",
    "year": 2022,
    "authors": [
      "Alex DiChristofano",
      "Henry Shuster",
      "Shefali Chandra",
      "Neal Patwari"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in speech recognition related to geopolitical power, which reflects social and political inequalities affecting different populations.",
      "inequality_type": [
        "geographic",
        "political",
        "linguistic"
      ],
      "other_detail": "Bias related to geopolitical power and language use",
      "affected_populations": [
        "non-US speakers",
        "global English speakers"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Auditing speech recognition performance across countries",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.01127v1",
    "title": "Disparate Censorship & Undertesting: A Source of Label Bias in Clinical Machine Learning",
    "year": 2022,
    "authors": [
      "Trenton Chang",
      "Michael W. Sjoding",
      "Jenna Wiens"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in clinical testing that affect different patient groups, highlighting disparities related to testing rates and label bias, which are linked to social inequalities in healthcare access and resource allocation.",
      "inequality_type": [
        "health",
        "socioeconomic"
      ],
      "other_detail": "Focuses on healthcare access disparities affecting testing and labels",
      "affected_populations": [
        "patients with limited testing",
        "underserved groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Empirical Analysis"
      ],
      "methodology_detail": "Analyzes bias effects on model performance across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.14367v2",
    "title": "An Equity-Aware Recommender System for Curating Art Exhibits Based on Locally-Constrained Graph Matching",
    "year": 2022,
    "authors": [
      "Anna Haensch",
      "Dina Deitsch"
    ],
    "categories": [
      "cs.IR",
      "cs.LG",
      "90C05 (Primary) 91-10, 62P99 (Secondary)"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses equity in public art curation, focusing on representation and exposure of marginalized communities, which relates to social inequality. It discusses fairness and bias in algorithmic decision-making, aiming to de-prioritize in-group preferences. The approach seeks to mitigate unequal cultural representation, a form of social inequality.",
      "inequality_type": [
        "racial",
        "cultural",
        "social"
      ],
      "other_detail": "Focuses on cultural representation and community equity",
      "affected_populations": [
        "marginalized communities",
        "legacy communities"
      ],
      "methodology": [
        "Algorithm Design",
        "Optimization",
        "Fairness Metric Development"
      ],
      "methodology_detail": "Graph matching with equity constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.13665v3",
    "title": "Causal foundations of bias, disparity and fairness",
    "year": 2022,
    "authors": [
      "V. A. Traag",
      "L. Waltman"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "stat.AP"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper defines bias and disparity in causal terms related to social groups, focusing on gender and racial biases, and discusses their implications, indicating a focus on social inequality issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Focus on social biases in science and policing",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Case Study",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Applying causal definitions to real-world bias cases",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.00781v1",
    "title": "Debiasing Deep Chest X-Ray Classifiers using Intra- and Post-processing Methods",
    "year": 2022,
    "authors": [
      "Ričards Marcinkevičs",
      "Ece Ozkan",
      "Julia E. Vogt"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in medical AI systems related to sensitive attributes like race and gender, which are social categories linked to inequality. It discusses fairness and bias mitigation in healthcare AI, impacting social disparities.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias mitigation in medical imaging AI",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "patients"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Bias mitigation techniques applied to neural networks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.12631v2",
    "title": "A Learning and Control Perspective for Microfinance",
    "year": 2022,
    "authors": [
      "Christian Kurniawan",
      "Xiyu Deng",
      "Adhiraj Chakraborty",
      "Assane Gueye",
      "Niangjun Chen",
      "Yorie Nakahira"
    ],
    "categories": [
      "q-fin.GN",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses microfinance, which aims to reduce poverty and promote financial inclusion, inherently related to socioeconomic inequality. It discusses challenges faced by marginalized populations in accessing credit and financial services. The focus on fairness and social welfare indicates engagement with social inequality issues.",
      "inequality_type": [
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on financial inclusion and poverty alleviation",
      "affected_populations": [
        "poor communities",
        "rural populations"
      ],
      "methodology": [
        "Control Theory",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Control-theoretic modeling and decision algorithms",
      "geographic_focus": [
        "rural Africa"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.11569v1",
    "title": "Robots Enact Malignant Stereotypes",
    "year": 2022,
    "authors": [
      "Andrew Hundt",
      "William Agnew",
      "Vicky Zeng",
      "Severin Kacianka",
      "Matthew Gombolay"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how AI systems perpetuate stereotypes related to race and gender, leading to social discrimination. It discusses impacts on marginalized groups and calls for policy and safety frameworks addressing these inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on stereotypes and discrimination in robotics",
      "affected_populations": [
        "Women",
        "People of Color"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Interdisciplinary Analysis"
      ],
      "methodology_detail": "Auditing ML bias in robotic systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.11345v1",
    "title": "Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities",
    "year": 2022,
    "authors": [
      "Pranav Dheram",
      "Murugesan Ramakrishnan",
      "Anirudh Raju",
      "I-Fan Chen",
      "Brian King",
      "Katherine Powell",
      "Melissa Saboowala",
      "Karan Shetty",
      "Andreas Stolcke"
    ],
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses performance disparities in speech recognition across different user cohorts, which relate to social groups defined by geographic and demographic factors, indicating a focus on fairness and bias in AI systems.",
      "inequality_type": [
        "geographic",
        "demographic"
      ],
      "other_detail": "Focus on fairness in AI system performance disparities",
      "affected_populations": [
        "speech recognition users",
        "cohorts with lower performance"
      ],
      "methodology": [
        "Experiment",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using cohort discovery and mitigation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.10894v3",
    "title": "What are Your Pronouns? Examining Gender Pronoun Usage on Twitter",
    "year": 2022,
    "authors": [
      "Julie Jiang",
      "Emily Chen",
      "Luca Luceri",
      "Goran Murić",
      "Francesco Pierri",
      "Ho-Chun Herbert Chang",
      "Emilio Ferrara"
    ],
    "categories": [
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender pronoun usage, addressing gender identity and inclusion, which are related to social gender inequalities and discrimination. It analyzes social biases and fairness issues in online gender expression. The focus on gender identity and online social dynamics indicates a direct engagement with social inequality topics.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender identity and social inclusion",
      "affected_populations": [
        "gender minorities",
        "non-binary individuals"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes large-scale Twitter data and user behavior",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.10888v1",
    "title": "FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification",
    "year": 2022,
    "authors": [
      "Xiaofeng Lin",
      "Seungbae Kim",
      "Jungseock Joo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on mitigating biases affecting different social groups during model pruning.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in face attribute classification",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware pruning for face attribute classification",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.10246v1",
    "title": "GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection",
    "year": 2022,
    "authors": [
      "Aakash Varma Nadimpalli",
      "Ajita Rattani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and demographic disparities in deepfake detection, focusing on gender bias, which relates directly to social discrimination and inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender fairness in AI systems",
      "affected_populations": [
        "females",
        "males"
      ],
      "methodology": [
        "Dataset Creation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Annotated datasets and performance comparison across genders",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.10245v1",
    "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
    "year": 2022,
    "authors": [
      "Oskar van der Wal",
      "Jaap Jumelet",
      "Katrin Schulz",
      "Willem Zuidema"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, a form of social discrimination. It analyzes how biases related to gender develop and can be mitigated, directly addressing social bias issues in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language models",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Experiment",
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Training dynamics and bias representation analysis",
      "geographic_focus": [
        "English-speaking regions"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.10020v1",
    "title": "MANI-Rank: Multiple Attribute and Intersectional Group Fairness for Consensus Ranking",
    "year": 2022,
    "authors": [
      "Kathleen Cachel",
      "Elke Rundensteiner",
      "Lane Harrison"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in rankings across protected attributes like gender, race, and intersectionality, addressing social bias and discrimination in algorithmic decision-making.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Intersectional fairness in ranking algorithms",
      "affected_populations": [
        "women",
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Fairness Criteria Design",
        "Experimental Study"
      ],
      "methodology_detail": "Develops algorithms ensuring intersectional fairness in rankings",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.08336v2",
    "title": "When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes",
    "year": 2022,
    "authors": [
      "Canyu Chen",
      "Yueqing Liang",
      "Xiongxiao Xu",
      "Shangyu Xie",
      "Ashish Kundu",
      "Ali Payani",
      "Yuan Hong",
      "Kai Shu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias mitigation across demographic groups, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI predictions under privacy constraints",
      "affected_populations": [
        "demographic groups",
        "users with sensitive attributes"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Modeling",
        "Theoretical Analysis"
      ],
      "methodology_detail": "FairSP framework for bias correction and fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.08169v1",
    "title": "Ethnic Representation Analysis of Commercial Movie Posters",
    "year": 2022,
    "authors": [
      "Dima Kagan",
      "Mor Levy",
      "Michael Fire",
      "Galit Fuhrmann Alpert"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines ethnic bias in film posters, addressing racial representation and bias, which are aspects of social inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on ethnic bias in media representation",
      "affected_populations": [
        "ethnic minorities",
        "film industry minorities"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing poster data with AI models",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.08104v1",
    "title": "A Multibias-mitigated and Sentiment Knowledge Enriched Transformer for Debiasing in Multimodal Conversational Emotion Recognition",
    "year": 2022,
    "authors": [
      "Jinglin Wang",
      "Fang Ma",
      "Yazhou Zhang",
      "Dawei Song"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender, age, race, religion, and LGBTQ+ in AI systems, which are social inequalities. It focuses on mitigating social biases in multimodal emotion recognition, a social impact area.",
      "inequality_type": [
        "gender",
        "race",
        "age",
        "religion",
        "LGBTQ+"
      ],
      "other_detail": "Multiple social bias types in AI models",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly",
        "religious groups",
        "LGBTQ+"
      ],
      "methodology": [
        "Natural Language Processing",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in multimodal transformer models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.08037v2",
    "title": "Data Representativeness in Accessibility Datasets: A Meta-Analysis",
    "year": 2022,
    "authors": [
      "Rie Kamikubo",
      "Lining Wang",
      "Crystal Marte",
      "Amnah Mahmood",
      "Hernisa Kacorri"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic representation gaps and biases in datasets related to marginalized groups, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "disability",
        "age"
      ],
      "other_detail": "Focus on underrepresented marginalized communities in datasets",
      "affected_populations": [
        "people with disabilities",
        "older adults"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Review of 190 datasets' demographic representation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.07776v1",
    "title": "Adversarial Reweighting for Speaker Verification Fairness",
    "year": 2022,
    "authors": [
      "Minho Jin",
      "Chelsea J. -T. Ju",
      "Zeya Chen",
      "Yi-Chieh Liu",
      "Jasha Droppo",
      "Andreas Stolcke"
    ],
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in speaker verification, reducing performance disparities across gender and nationality groups, addressing social bias issues in AI systems.",
      "inequality_type": [
        "gender",
        "nationality"
      ],
      "other_detail": "Fairness in biometric AI systems",
      "affected_populations": [
        "male speakers",
        "female speakers",
        "US speakers",
        "UK speakers",
        "other nationalities"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Reweighting samples to improve fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.07765v2",
    "title": "FairFuse: Interactive Visual Support for Fair Consensus Ranking",
    "year": 2022,
    "authors": [
      "Hilson Shrestha",
      "Kathleen Cachel",
      "Mallak Alkhathlan",
      "Elke Rundensteiner",
      "Lane Harrison"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in ranking systems, addressing group fairness and protected attributes, which are central to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "fairness"
      ],
      "other_detail": "Focus on fairness in algorithmic ranking systems",
      "affected_populations": [
        "protected groups",
        "grouped by race or gender"
      ],
      "methodology": [
        "System Design",
        "Algorithm Auditing",
        "Visualization"
      ],
      "methodology_detail": "Design of visualization system for fair rankings",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.04712v1",
    "title": "Areas of Strategic Visibility: Disability Bias in Biometrics",
    "year": 2022,
    "authors": [
      "Jennifer Mankoff",
      "Devva Kasnitz",
      "Disability Studies",
      "L Jean Camp",
      "Jonathan Lazar",
      "Harry Hochheiser"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses how biometric systems impact disabled individuals, highlighting issues of accessibility and exclusion, which relate to social inequality and discrimination based on disability.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Focus on accessibility and bias in biometric systems",
      "affected_populations": [
        "disabled people"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing accessibility and bias issues in biometrics",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.06591v3",
    "title": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America",
    "year": 2022,
    "authors": [
      "Laura Alonso Alemany",
      "Luciana Benotti",
      "Hernán Maina",
      "Lucía González",
      "Mariela Rajngewerc",
      "Lautaro Martínez",
      "Jorge Sánchez",
      "Mauro Schilman",
      "Guido Ivetta",
      "Alexia Halvorsen",
      "Amanda Mata Rojo",
      "Matías Bordone",
      "Beatriz Busaniche"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on biases and stereotypes in language models, which relate to social discrimination and inequality, particularly in Latin America.",
      "inequality_type": [
        "racial",
        "gender",
        "social",
        "educational"
      ],
      "other_detail": "Focus on social biases in NLP models",
      "affected_populations": [
        "Latin American communities"
      ],
      "methodology": [
        "Qualitative Study",
        "Metric-based Approach",
        "System Design"
      ],
      "methodology_detail": "Collaborative exploration of biases and stereotypes",
      "geographic_focus": [
        "Latin America"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.06421v1",
    "title": "Deep Learning Discovery of Demographic Biomarkers in Echocardiography",
    "year": 2022,
    "authors": [
      "Grant Duffy",
      "Shoa L. Clarke",
      "Matthew Christensen",
      "Bryan He",
      "Neal Yuan",
      "Susan Cheng",
      "David Ouyang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic attributes like race, sex, and age, and discusses potential biases and disparities in AI predictions, highlighting issues related to social discrimination and fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "age"
      ],
      "other_detail": "Focus on demographic bias in medical AI",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "elderly"
      ],
      "methodology": [
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Predicting demographics from medical images",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.06273v1",
    "title": "Understanding Unfairness in Fraud Detection through Model and Data Bias Interactions",
    "year": 2022,
    "authors": [
      "José Pombal",
      "André F. Cruz",
      "João Bravo",
      "Pedro Saleiro",
      "Mário A. T. Figueiredo",
      "Pedro Bizarro"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "q-fin.ST"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic unfairness affecting protected social groups, such as race and gender, in high-stakes decision-making, highlighting social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on fairness in algorithmic decision-making",
      "affected_populations": [
        "racial minorities",
        "women",
        "financially disadvantaged"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Analyzes bias interactions and fairness trade-offs",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.06084v1",
    "title": "Towards A Holistic View of Bias in Machine Learning: Bridging Algorithmic Fairness and Imbalanced Learning",
    "year": 2022,
    "authors": [
      "Damien Dablain",
      "Bartosz Krawczyk",
      "Nitesh Chawla"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in machine learning related to protected groups such as gender and race, addressing social bias and inequality in algorithmic decisions.",
      "inequality_type": [
        "gender",
        "racial",
        "social"
      ],
      "other_detail": "Focuses on fairness and representation in AI models",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Development",
        "Fairness Metrics"
      ],
      "methodology_detail": "Proposes a novel oversampling algorithm for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.07915v1",
    "title": "On Curating Responsible and Representative Healthcare Video Recommendations for Patient Education and Health Literacy: An Augmented Intelligence Approach",
    "year": 2022,
    "authors": [
      "Krishna Pothugunta",
      "Xiao Liu",
      "Anjana Susarla",
      "Rema Padman"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in health information access and algorithmic biases affecting disadvantaged groups, minorities, and low health literacy users.",
      "inequality_type": [
        "health",
        "educational",
        "informational",
        "digital",
        "racial"
      ],
      "other_detail": "Bias in health-related video recommendations on social media",
      "affected_populations": [
        "disadvantaged",
        "minorities",
        "low health literacy users"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing video content and metadata for bias and representation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.05797v1",
    "title": "Examining Data Imbalance in Crowdsourced Reports for Improving Flash Flood Situational Awareness",
    "year": 2022,
    "authors": [
      "Miguel Esparza",
      "Hamed Farahmand",
      "Samuel Brody",
      "Ali Mostafavi"
    ],
    "categories": [
      "cs.CY",
      "physics.data-an"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines biases in crowdsourced disaster reports, highlighting disparities in data collection related to minority populations and geographic areas, indicating social inequality considerations.",
      "inequality_type": [
        "racial",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Biases in disaster data collection affecting minorities",
      "affected_populations": [
        "minority populations",
        "geographic communities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Includes spatial bias tests and regression analysis",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.04376v2",
    "title": "On Graph Neural Network Fairness in the Presence of Heterophilous Neighborhoods",
    "year": 2022,
    "authors": [
      "Donald Loveland",
      "Jiong Zhu",
      "Mark Heimann",
      "Ben Fish",
      "Michael T. Schaub",
      "Danai Koutra"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI models related to sensitive attributes, indicating a focus on social bias and discrimination issues.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Fairness in AI predictions related to social attributes",
      "affected_populations": [
        "social groups",
        "communities"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes fairness and homophily effects in GNNs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.02912v2",
    "title": "Towards Substantive Conceptions of Algorithmic Fairness: Normative Guidance from Equal Opportunity Doctrines",
    "year": 2022,
    "authors": [
      "Falaah Arif Khan",
      "Eleni Manis",
      "Julia Stoyanovich"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper uses political philosophy doctrines related to fair life chances, addressing social fairness concepts. It discusses normative judgments about fairness, which relate to social inequalities. The focus on equal opportunity doctrines inherently involves social inequality considerations.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "disability"
      ],
      "other_detail": "Normative fairness in algorithmic decision-making",
      "affected_populations": [
        "social groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Philosophical Framework"
      ],
      "methodology_detail": "Applying political philosophy to algorithmic fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.02463v1",
    "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
    "year": 2022,
    "authors": [
      "Przemyslaw Joniak",
      "Akiko Aizawa"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in language models, addressing social gender inequality and bias in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias detection and model pruning techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2208.06308v2",
    "title": "Developing a Philosophical Framework for Fair Machine Learning: Lessons From The Case of Algorithmic Collusion",
    "year": 2022,
    "authors": [
      "James Michelson"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in machine learning, focusing on injustices like algorithmic collusion affecting consumers broadly, which relates to social inequality issues beyond individual discrimination.",
      "inequality_type": [
        "economic",
        "market",
        "consumer",
        "social"
      ],
      "other_detail": "Focuses on market fairness and consumer impacts",
      "affected_populations": [
        "consumers",
        "market participants"
      ],
      "methodology": [
        "Case Study",
        "Ethics Analysis"
      ],
      "methodology_detail": "Analyzes algorithmic collusion case and normative principles",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.01056v2",
    "title": "Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models",
    "year": 2022,
    "authors": [
      "Yi Zhang",
      "Junyang Wang",
      "Jitao Sang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring and mitigating social biases, specifically gender bias, in AI models, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in vision-language models",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Dataset Creation",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Counterfactual bias measurement and debiasing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.01017v1",
    "title": "\"Y'all are just too sensitive\": A computational ethics approach to understanding how prejudice against marginalized communities becomes epistemic belief",
    "year": 2022,
    "authors": [
      "Johannah Sprinz"
    ],
    "categories": [
      "cs.MA",
      "cs.CY",
      "I.6.3"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines microaggressions and prejudice against marginalized groups, addressing social discrimination and bias. It explores how microaggressions influence beliefs within a simulated society, relating to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic"
      ],
      "other_detail": "Focuses on prejudice and microaggressions in society",
      "affected_populations": [
        "marginalized communities"
      ],
      "methodology": [
        "Simulation",
        "Computational Modeling"
      ],
      "methodology_detail": "NetLogo prototype simulating social interactions",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.00691v1",
    "title": "American == White in Multimodal Language-and-Image AI",
    "year": 2022,
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial biases in AI models, revealing biases associating American identity with being White, which reflect and propagate social racial inequalities.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias propagation in AI models",
      "affected_populations": [
        "Asian individuals",
        "Black individuals",
        "Latina/o individuals",
        "White individuals"
      ],
      "methodology": [
        "Experiment",
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Embedding association tests and downstream bias assessments",
      "geographic_focus": [
        "United States",
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.00108v1",
    "title": "Discrimination in machine learning algorithms",
    "year": 2022,
    "authors": [
      "Roberta Pappadà",
      "Francesco Pauli"
    ],
    "categories": [
      "stat.ML",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses discrimination in algorithms affecting individuals, which relates to social bias and fairness issues tied to race, gender, and potentially other social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic discrimination and bias detection",
      "affected_populations": [
        "individuals affected by decisions"
      ],
      "methodology": [
        "Statistical Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Detecting and eliminating biases in algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.14853v1",
    "title": "Fairness via In-Processing in the Over-parameterized Regime: A Cautionary Tale",
    "year": 2022,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Ivan Brugere",
      "Jiahao Chen",
      "Sanghamitra Dutta",
      "Alan Mishler",
      "Siddharth Garg"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI, addressing bias against minority groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness constraints in AI models",
      "affected_populations": [
        "minority groups",
        "underrepresented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Evaluation",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes fairness methods in over-parameterized neural networks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.14397v3",
    "title": "Fair Machine Learning in Healthcare: A Review",
    "year": 2022,
    "authors": [
      "Qizhang Feng",
      "Mengnan Du",
      "Na Zou",
      "Xia Hu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness concerns in healthcare ML, addressing disparities among demographic groups, which relates to social inequalities.",
      "inequality_type": [
        "health",
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and disparities in healthcare AI",
      "affected_populations": [
        "demographic groups",
        "healthcare patients"
      ],
      "methodology": [
        "Literature Review",
        "Fairness Metrics Analysis",
        "Bias Mitigation Strategies"
      ],
      "methodology_detail": "Analyzes fairness metrics and bias mitigation across ML stages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.12922v1",
    "title": "Visualizing Non-Fungible Token Ethics: A Case Study On CryptoPunks",
    "year": 2022,
    "authors": [
      "Yufan Zhang",
      "Zichao Chen",
      "Luyao Zhang",
      "Xin Tong"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial and gender fairness issues in NFT design and trading, highlighting disparities in creation and valuation across social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "economic"
      ],
      "other_detail": "Focuses on racial and gender fairness in digital art",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Data Scraping",
        "Sentiment Analysis",
        "Visualization"
      ],
      "methodology_detail": "Analyzed Twitter and blockchain data for fairness patterns",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.12333v1",
    "title": "Achievement and Fragility of Long-term Equitability",
    "year": 2022,
    "authors": [
      "Andrea Simonetto",
      "Ivano Notarnicola"
    ],
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on equitable resource allocation affecting communities, highlighting social outcomes and disparities. It discusses long-term social fairness and the fragility of social equity, indicating a direct concern with social inequality issues.",
      "inequality_type": [
        "socioeconomic",
        "educational",
        "health"
      ],
      "other_detail": "Focus on community-level resource distribution",
      "affected_populations": [
        "communities",
        "sub-Saharan populations"
      ],
      "methodology": [
        "Data-driven feedback online optimization",
        "Mathematical modeling"
      ],
      "methodology_detail": "Dynamic policies for equitable long-term allocation",
      "geographic_focus": [
        "Sub-Saharan Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.11993v1",
    "title": "A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages",
    "year": 2022,
    "authors": [
      "Akhter Al Amin",
      "Kazi Sinthia Kabir"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in language models through a disability lens, addressing social bias and fairness issues related to disability, a social inequality dimension.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Focus on bias related to disability in AI-generated text",
      "affected_populations": [
        "people with disabilities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Measuring bias in GPT-3 generated language",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.01518v1",
    "title": "Gender inequality and self-publication patterns among scientific editors",
    "year": 2022,
    "authors": [
      "Fengyuan Liu",
      "Petter Holme",
      "Matteo Chiesa",
      "Bedoor AlShebli",
      "Talal Rahwan"
    ],
    "categories": [
      "cs.CY",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in academic editing and publishing roles, highlighting underrepresentation and behavioral differences between men and women, which directly relate to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "female scientists",
        "female editors"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Large-scale dataset analysis of publication and editorial patterns",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.11484v2",
    "title": "Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models",
    "year": 2022,
    "authors": [
      "Virginia K. Felkner",
      "Ho-Chun Herbert Chang",
      "Eugene Jang",
      "Jonathan May"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases against queer and trans people in AI models, relating to social discrimination and inequality based on gender and sexual orientation.",
      "inequality_type": [
        "gender",
        "sexual orientation"
      ],
      "other_detail": "Focus on anti-queer bias in language models",
      "affected_populations": [
        "queer people",
        "trans people"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Developing benchmark dataset and bias mitigation via finetuning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.09987v1",
    "title": "Measuring Gender Bias in Educational Videos: A Case Study on YouTube",
    "year": 2022,
    "authors": [
      "Gizem Gezici",
      "Yucel Saygin"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in online educational videos, highlighting social discrimination related to gender. It analyzes perceived gender bias in search results, which relates to social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "none",
      "affected_populations": [
        "female students",
        "male students"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Bias Measurement"
      ],
      "methodology_detail": "Analyzed search bias and rank influence on gender representation",
      "geographic_focus": null,
      "ai_relationship": "Measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.09875v1",
    "title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models",
    "year": 2022,
    "authors": [
      "Emily Black",
      "Hadi Elzayn",
      "Alexandra Chouldechova",
      "Jacob Goldin",
      "Daniel E. Ho"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how algorithmic fairness impacts income distribution and tax burdens across income groups, addressing socioeconomic inequality.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focus on tax fairness and income disparities",
      "affected_populations": [
        "middle-income taxpayers",
        "high-income taxpayers"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes audit data using ML models",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.09860v1",
    "title": "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",
    "year": 2022,
    "authors": [
      "Yarden Tal",
      "Inbal Magar",
      "Roy Schwartz"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in AI models, addressing social gender inequality and bias issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on occupational gender bias",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and model evaluation techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.09346v1",
    "title": "Fairness-aware Model-agnostic Positive and Unlabeled Learning",
    "year": 2022,
    "authors": [
      "Ziwei Wu",
      "Jingrui He"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on social groups and bias mitigation, which relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focuses on fairness across social groups in AI models",
      "affected_populations": [
        "social groups",
        "individuals from different populations"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Post-processing Framework"
      ],
      "methodology_detail": "Fairness-aware PUL with statistical consistency",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.08697v2",
    "title": "Reviewer Preferences and Gender Disparities in Aesthetic Judgments",
    "year": 2022,
    "authors": [
      "Ida Marie Schytt Lassen",
      "Yuri Bizzoni",
      "Telma Peura",
      "Mads Rosendahl Thomsen",
      "Kristoffer Laigaard Nielbo"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in aesthetic judgments, highlighting gender disparities and cultural antagonism, which relate to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on cultural gender bias in aesthetic preferences",
      "affected_populations": [
        "male writers",
        "female reviewers"
      ],
      "methodology": [
        "Literature Review",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes literary reviews for bias patterns",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.08287v3",
    "title": "Definition drives design: Disability models and mechanisms of bias in AI technologies",
    "year": 2022,
    "authors": [
      "Denis Newman-Griffis",
      "Jessica Sage Rauchberg",
      "Rahaf Alharbi",
      "Louise Hickman",
      "Harry Hochheiser"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias in AI affecting disabled people, a social group, and discusses how design decisions influence fairness and equity. It emphasizes participation and transparency, key issues in social inequality. The focus on disability-related bias aligns with social inequality concerns.",
      "inequality_type": [
        "disability",
        "health"
      ],
      "other_detail": "Focus on disability-related bias in AI",
      "affected_populations": [
        "disabled people"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Case Study"
      ],
      "methodology_detail": "Using disability models to analyze AI bias",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.07113v1",
    "title": "Minorities in networks and algorithms",
    "year": 2022,
    "authors": [
      "Fariba Karimi",
      "Marcos Oliveira",
      "Markus Strohmaier"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.CY",
      "cs.SI",
      "nlin.AO"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses societal inequalities, marginalization, and biases in networks and algorithms affecting minorities.",
      "inequality_type": [
        "racial",
        "social",
        "health",
        "educational"
      ],
      "other_detail": "Focus on network-based inequalities and algorithmic visibility",
      "affected_populations": [
        "minorities",
        "social groups"
      ],
      "methodology": [
        "Data-driven models",
        "Theoretical analysis"
      ],
      "methodology_detail": "Complex network modeling and analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.06960v1",
    "title": "ABCinML: Anticipatory Bias Correction in Machine Learning Applications",
    "year": 2022,
    "authors": [
      "Abdulaziz A. Almuzaini",
      "Chidansh A. Bhatt",
      "David M. Pennock",
      "Vivek K. Singh"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias correction in AI systems, focusing on fairness across social groups such as gender and population subgroups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "focus on fairness and bias mitigation in algorithms",
      "affected_populations": [
        "gender groups",
        "population subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "anticipatory bias correction approach",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.01488v1",
    "title": "Award rate inequities in biomedical research",
    "year": 2022,
    "authors": [
      "Alessandra Zimmermann",
      "Richard Klavans",
      "Heather Offhaus",
      "Teri A. Grieb",
      "Caleb Smith"
    ],
    "categories": [
      "cs.CY",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines racial disparities in research proposal success rates and submission strategies, directly addressing racial inequality in academic funding.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Focuses on racial/ethnic disparities in biomedical research funding",
      "affected_populations": [
        "Black/African American",
        "Asian researchers",
        "White researchers"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of proposal submission and award data",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.06685v1",
    "title": "Causal Discovery for Fairness",
    "year": 2022,
    "authors": [
      "Rūta Binkytė-Sadauskienė",
      "Karima Makhlouf",
      "Carlos Pinzón",
      "Sami Zhioua",
      "Catuscia Palamidessi"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ME"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and discrimination related to sensitive attributes like gender and race, analyzing how causal discovery impacts social fairness in AI decisions.",
      "inequality_type": [
        "gender",
        "racial",
        "discrimination"
      ],
      "other_detail": "Focuses on fairness in AI decision-making processes",
      "affected_populations": [
        "minorities",
        "individuals",
        "social groups"
      ],
      "methodology": [
        "Causal Discovery",
        "Empirical Analysis",
        "Algorithm Review"
      ],
      "methodology_detail": "Analyzes causal discovery algorithms' impact on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.06410v3",
    "title": "Estimating Causal Effects Under Image Confounding Bias with an Application to Poverty in Africa",
    "year": 2022,
    "authors": [
      "Connor T. Jerzak",
      "Fredrik Johansson",
      "Adel Daoud"
    ],
    "categories": [
      "cs.LG",
      "stat.ME",
      "62D20",
      "I.4.0; I.2.0"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper estimates effects of policy interventions on poverty in African communities using satellite imagery, directly addressing socioeconomic inequality. It focuses on poverty, a key aspect of social inequality, and its impact on vulnerable populations.",
      "inequality_type": [
        "economic",
        "socioeconomic"
      ],
      "other_detail": "Focus on poverty in African communities",
      "affected_populations": [
        "African communities"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using image-based causal inference methods",
      "geographic_focus": [
        "Africa"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.06279v2",
    "title": "A Machine Learning Model for Predicting, Diagnosing, and Mitigating Health Disparities in Hospital Readmission",
    "year": 2022,
    "authors": [
      "Shaina Raza"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in healthcare data related to race, gender, and social determinants, aiming to improve fairness in predictions, which directly relates to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "health",
        "social determinants"
      ],
      "other_detail": "Bias mitigation in healthcare AI systems",
      "affected_populations": [
        "diabetic patients",
        "hospitalized patients"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Bias detection and mitigation pipeline evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.06149v1",
    "title": "Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis",
    "year": 2022,
    "authors": [
      "Maarten Buyl",
      "Christina Cociancig",
      "Cristina Frattone",
      "Nele Roekens"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination against persons with disabilities in AI hiring, a social inequality issue.",
      "inequality_type": [
        "disability"
      ],
      "other_detail": "Focus on algorithmic discrimination against PWDs",
      "affected_populations": [
        "persons with disabilities"
      ],
      "methodology": [
        "Ethics Analysis",
        "Legal Analysis",
        "Technical Analysis"
      ],
      "methodology_detail": "Addressing ethical, legal, and technical challenges",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.05330v4",
    "title": "The Gender Gap in Scholarly Self-Promotion on Social Media",
    "year": 2022,
    "authors": [
      "Hao Peng",
      "Misha Teplitskiy",
      "Daniel M. Romero",
      "Emőke-Ágnes Horvát"
    ],
    "categories": [
      "cs.DL",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender disparities in self-promotion among scholars, highlighting gender-based biases and differential impacts, which are core aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender disparities in academic visibility",
      "affected_populations": [
        "women scholars",
        "male scholars"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of Twitter data over six years",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.04789v1",
    "title": "Comprehensive Fair Meta-learned Recommender System",
    "year": 2022,
    "authors": [
      "Tianxin Wei",
      "Jingrui He"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in recommendation systems, addressing bias and discrimination issues, which relate to social inequality concerns.",
      "inequality_type": [
        "social",
        "fairness"
      ],
      "other_detail": "Addresses algorithmic fairness and bias mitigation",
      "affected_populations": [
        "users",
        "social groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Learning",
        "Empirical Evaluation"
      ],
      "methodology_detail": "Fairness-aware meta-learning framework tested on real data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.05050v1",
    "title": "Improved Approximation for Fair Correlation Clustering",
    "year": 2022,
    "authors": [
      "Sara Ahmadian",
      "Maryam Negahbani"
    ],
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in clustering, addressing representation across protected groups, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI clustering algorithms",
      "affected_populations": [
        "protected groups",
        "data points"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Improving fairness guarantees in clustering algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2207.01485v1",
    "title": "Machine Learning for Deferral of Care Prediction",
    "year": 2022,
    "authors": [
      "Muhammad Aurangzeb Ahmad",
      "Raafia Ahmed",
      "Steve Overman",
      "Patrick Campbell",
      "Corinne Stroum",
      "Bipin Karunakaran"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in care deferral affecting vulnerable populations and evaluates fairness across demographic and socioeconomic groups.",
      "inequality_type": [
        "socioeconomic",
        "health",
        "racial"
      ],
      "other_detail": "Focus on vulnerable and minority populations",
      "affected_populations": [
        "minority groups",
        "socioeconomically disadvantaged"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis"
      ],
      "methodology_detail": "Assessing model fairness across social groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.04179v1",
    "title": "Automating Ambiguity: Challenges and Pitfalls of Artificial Intelligence",
    "year": 2022,
    "authors": [
      "Abeba Birhane"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses how AI systems impact marginalized groups and embed societal injustices, highlighting issues of structural oppression and unequal impacts.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "structural oppression"
      ],
      "other_detail": "Focus on societal and structural injustices in AI",
      "affected_populations": [
        "marginalized groups",
        "societal minorities"
      ],
      "methodology": [
        "Literature Review",
        "Critical Analysis",
        "Dataset Auditing",
        "Framework Development"
      ],
      "methodology_detail": "Examines datasets, values, and ethical frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03883v2",
    "title": "Efficient Resource Allocation with Fairness Constraints in Restless Multi-Armed Bandits",
    "year": 2022,
    "authors": [
      "Dexun Li",
      "Pradeep Varakantham"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness constraints in decision-making processes affecting different groups, particularly in public health, which relates to social inequalities. It aims to ensure equitable representation and treatment across communities, addressing fairness issues in AI systems.",
      "inequality_type": [
        "health",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Fairness in public health interventions",
      "affected_populations": [
        "communities",
        "patients",
        "public health groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Designing fair RMAB algorithms and analyzing their properties",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03426v2",
    "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage",
    "year": 2022,
    "authors": [
      "Yu Wang",
      "Yuying Zhao",
      "Yushun Dong",
      "Huiyuan Chen",
      "Jundong Li",
      "Tyler Derr"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI, focusing on discrimination mitigation.",
      "inequality_type": [
        "racial",
        "gender",
        "discrimination"
      ],
      "other_detail": "Bias mitigation in graph neural networks",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Fairness-aware GNN design and evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03390v1",
    "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",
    "year": 2022,
    "authors": [
      "Aylin Caliskan",
      "Pimparkar Parth Ajay",
      "Tessa Charlesworth",
      "Robert Wolfe",
      "Mahzarin R. Banaji"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender biases in language, highlighting social gender stereotypes and disparities in representation, which relate directly to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias in language representations",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Uses statistical tests and clustering analyses",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03200v2",
    "title": "FairVFL: A Fair Vertical Federated Learning Framework with Contrastive Adversarial Learning",
    "year": 2022,
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Lingjuan Lyu",
      "Tong Xu",
      "Zhongliang Yang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness issues related to gender bias in AI models, which directly relates to social discrimination and inequality. It focuses on fairness-sensitive features and bias removal, impacting social groups. The emphasis on fairness in AI systems aligns with social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Fairness in AI models",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Learning",
        "Contrastive Learning"
      ],
      "methodology_detail": "Bias removal and fairness enhancement techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03150v3",
    "title": "Group Meritocratic Fairness in Linear Contextual Bandits",
    "year": 2022,
    "authors": [
      "Riccardo Grazzi",
      "Arya Akhavan",
      "John Isak Texas Falk",
      "Leonardo Cella",
      "Massimiliano Pontil"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems related to social groups, specifically focusing on racial and social biases in candidate selection, which directly relates to social inequality issues.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in algorithmic decision-making processes",
      "affected_populations": [
        "ethnic groups",
        "social groups"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designs a fair ranking algorithm with regret bounds",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.01691v1",
    "title": "Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals",
    "year": 2022,
    "authors": [
      "Shiva Omrani Sabbaghi",
      "Aylin Caliskan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in language models, which relates to social gender inequality and bias measurement.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on linguistic gender bias and social bias measurement",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and disentanglement techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03275v1",
    "title": "The Algorithmic Imprint",
    "year": 2022,
    "authors": [
      "Upol Ehsan",
      "Ranjit Singh",
      "Jacob Metcalf",
      "Mark O. Riedl"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how algorithmic impacts affect different social groups, especially in the Global South, highlighting issues of fairness, infrastructure, and social consequences.",
      "inequality_type": [
        "educational",
        "geographic",
        "socioeconomic"
      ],
      "other_detail": "Focus on global South and educational inequalities",
      "affected_populations": [
        "students",
        "teachers",
        "parents"
      ],
      "methodology": [
        "Case Study",
        "Qualitative Study",
        "Interviews"
      ],
      "methodology_detail": "Community engagement and timeline analysis",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.08978v1",
    "title": "Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study of African-American English",
    "year": 2022,
    "authors": [
      "Jamell Dacon"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial language discrimination and inclusivity in NLP, highlighting societal impacts on African-American English speakers.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Focus on language-based social discrimination",
      "affected_populations": [
        "African-American English speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Case Study"
      ],
      "methodology_detail": "Analyzing dialectal language features in NLP models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.00945v1",
    "title": "Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy",
    "year": 2022,
    "authors": [
      "Atoosa Kasirzadeh"
    ],
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses algorithmic biases affecting vulnerable social groups and structural injustices, linking AI fairness to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "social",
        "distributive justice"
      ],
      "other_detail": "Focus on structural injustices in algorithmic fairness",
      "affected_populations": [
        "vulnerable groups",
        "social minorities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes feminist philosophy and algorithmic fairness frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.00553v2",
    "title": "FETA: Fairness Enforced Verifying, Training, and Predicting Algorithms for Neural Networks",
    "year": 2022,
    "authors": [
      "Kiarash Mohammadi",
      "Aishwarya Sivaraman",
      "Golnoosh Farnadi"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in neural networks, addressing social bias and discrimination issues. It aims to ensure individual fairness, which relates to social inequalities like race, gender, and other protected groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on fairness and bias in AI systems",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Fairness verification and training techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.00480v1",
    "title": "Sex and Gender in the Computer Graphics Research Literature",
    "year": 2022,
    "authors": [
      "Ana Dodik",
      "Silvia Sellán",
      "Theodore Kim",
      "Amanda Phillips"
    ],
    "categories": [
      "cs.GR",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and sex treatment in computer graphics, highlighting biases and harmful effects, which relate to social discrimination and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on algorithmic bias in gender representation",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Literature Review",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Survey of existing research practices",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.00234v1",
    "title": "Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback",
    "year": 2022,
    "authors": [
      "Emmy Liu",
      "Michael Henry Tessler",
      "Nicole Dubosh",
      "Katherine Mosher Hiller",
      "Roger Levy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in medical evaluations, addressing gender-based social inequality. It examines differential treatment and perceptions of male and female students, highlighting gender disparities in professional contexts.",
      "inequality_type": [
        "gender",
        "educational"
      ],
      "other_detail": "Focus on gender bias in medical training evaluations",
      "affected_populations": [
        "female medical students",
        "male medical students"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using BERT model and LIWC comparison",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.00137v4",
    "title": "Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria",
    "year": 2022,
    "authors": [
      "Yiqiao Liao",
      "Parinaz Naghizadeh"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness criteria in AI, addressing biases affecting social groups.",
      "inequality_type": [
        "racial",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focus on dataset biases impacting social fairness",
      "affected_populations": [
        "disadvantaged individuals",
        "minority groups"
      ],
      "methodology": [
        "Analytical Study",
        "Numerical Experiments"
      ],
      "methodology_detail": "Analyzes fairness criteria robustness under biased data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.15951v2",
    "title": "Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues",
    "year": 2022,
    "authors": [
      "Sandhya Singh",
      "Prapti Roy",
      "Nihar Sahoo",
      "Niteesh Mallela",
      "Himanshu Gupta",
      "Pushpak Bhattacharyya",
      "Milind Savagaonkar",
      "Nidhi",
      "Roshni Ramnani",
      "Anutosh Maitra",
      "Shubhashis Sengupta"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to race, gender, and other social identities in movie scripts, which reflect and influence social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "age",
        "religion"
      ],
      "other_detail": "Bias detection in media content",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "ethnic communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Machine Learning"
      ],
      "methodology_detail": "Annotated dataset for bias detection in dialogues",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.15394v1",
    "title": "Diverse Representation via Computational Participatory Elections -- Lessons from a Case Study",
    "year": 2022,
    "authors": [
      "Florian Evéquoz",
      "Johan Rochel",
      "Vijay Keswani",
      "L. Elisa Celis"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fair representation in elections, addressing marginalized groups such as gender and ethnicity, which are social inequalities.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social"
      ],
      "other_detail": "Focus on political and social group representation",
      "affected_populations": [
        "marginalized groups",
        "voters",
        "candidate groups"
      ],
      "methodology": [
        "Case Study",
        "System Design",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Implementation and evaluation of electoral process",
      "geographic_focus": [
        "Switzerland"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.15171v5",
    "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
    "year": 2022,
    "authors": [
      "Lukas Hauzenberger",
      "Shahed Masoudian",
      "Deepak Kumar",
      "Markus Schedl",
      "Navid Rekabsaz"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to gender, race, and age in AI models, which are social inequality issues.",
      "inequality_type": [
        "gender",
        "race",
        "age"
      ],
      "other_detail": "Focus on social bias mitigation in language models",
      "affected_populations": [
        "women",
        "racial minorities",
        "elderly"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Modular bias mitigation approach with subnetworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.14136v1",
    "title": "Gender Bias in Password Managers",
    "year": 2022,
    "authors": [
      "Jeff Yan",
      "Dearbhla McCabe"
    ],
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in password manager preferences and usage, highlighting gender bias and disparities. It analyzes how social gender roles influence technology choices, indicating social inequality. The focus on gender bias in technology use aligns with social inequality issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "gender bias in technology use",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Qualitative Study",
        "Survey"
      ],
      "methodology_detail": "Interviews and questionnaire-based survey",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.14725v2",
    "title": "What are People Talking about in #BlackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerging in Online Social Movements through the Latent Dirichlet Allocation Model",
    "year": 2022,
    "authors": [
      "Xin Tong",
      "Yixuan Li",
      "Jiayi Li",
      "Rongqi Bei",
      "Luyao Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "cs.SI",
      "J.4; I.2.7; K.4.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes social movements addressing racial injustice and social awareness, focusing on minority groups' experiences and issues related to racism.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial justice movements on social media",
      "affected_populations": [
        "Black communities",
        "Asian communities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "LDA model and open-coding analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.13972v3",
    "title": "Counterfactual Fairness with Partially Known Causal Graph",
    "year": 2022,
    "authors": [
      "Aoqi Zuo",
      "Susan Wei",
      "Tongliang Liu",
      "Bo Han",
      "Kun Zhang",
      "Mingming Gong"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI related to sensitive attributes like gender and race, aiming to prevent discrimination. It discusses causal inference methods to ensure counterfactual fairness, directly linked to social bias mitigation.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on fairness in machine learning algorithms",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Causal Inference",
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Identifies causal relations with partial causal graphs",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.13641v1",
    "title": "Gender differences in research grant allocation -- a mixed picture",
    "year": 2022,
    "authors": [
      "Peter van den Besselaar",
      "Charlie Mom"
    ],
    "categories": [
      "stat.AP",
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in research grant allocation, addressing gender inequality in scientific funding processes.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in academic funding",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Includes merit variables and panel-level analysis",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.12676v3",
    "title": "Evaluating the Diversity, Equity and Inclusion of NLP Technology: A Case Study for Indian Languages",
    "year": 2022,
    "authors": [
      "Simran Khanuja",
      "Sebastian Ruder",
      "Partha Talukdar"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness, bias, and equitable access in NLP for diverse language speakers, highlighting inequalities in technology deployment.",
      "inequality_type": [
        "linguistic",
        "digital",
        "educational"
      ],
      "other_detail": "Focuses on language resource disparities in Indian languages",
      "affected_populations": [
        "Indian language speakers",
        "low-resource users"
      ],
      "methodology": [
        "Evaluation Paradigm",
        "Quantitative Analysis",
        "Model Development"
      ],
      "methodology_detail": "Uses Gini coefficient for measuring inequality",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.12554v4",
    "title": "Are Akpans Trick or Treat: Unveiling Helpful Biases in Assistant Systems",
    "year": 2022,
    "authors": [
      "Jiao Sun",
      "Yu Hou",
      "Jiin Kim",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness concerns in AI systems, highlighting disparities based on country development levels, which relate to socioeconomic and geographic inequalities.",
      "inequality_type": [
        "socioeconomic",
        "geographic",
        "informational"
      ],
      "other_detail": "Fairness disparities across countries in AI helpfulness",
      "affected_populations": [
        "users from less-developed countries",
        "users from highly-developed countries"
      ],
      "methodology": [
        "Experiment",
        "Human Annotation",
        "Model Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates helpfulness and fairness in dialogue systems",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.12391v1",
    "title": "Toward Understanding Bias Correlations for Mitigation in NLP",
    "year": 2022,
    "authors": [
      "Lu Cheng",
      "Suyu Ge",
      "Huan Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases related to race, gender, and religion in NLP, which are social identity aspects linked to social inequality. It discusses social bias, discrimination, and fairness issues in AI systems, aligning with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "religion"
      ],
      "other_detail": "Bias correlations across social identity groups",
      "affected_populations": [
        "ethnic minorities",
        "women",
        "religious groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzing bias correlations and mitigation strategies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.12204v1",
    "title": "Fairness in Selection Problems with Strategic Candidates",
    "year": 2022,
    "authors": [
      "Vitalii Emelianov",
      "Nicolas Gast",
      "Patrick Loiseau"
    ],
    "categories": [
      "cs.GT",
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and discrimination in selection processes, which relate to social inequalities such as race, gender, and socioeconomic status, especially through the lens of algorithmic fairness and bias mitigation.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational"
      ],
      "other_detail": "Focuses on fairness in AI-driven selection systems",
      "affected_populations": [
        "demographic groups",
        "candidates"
      ],
      "methodology": [
        "Game Theory",
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling strategic behavior and fairness constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.12093v1",
    "title": "Bias Discovery in Machine Learning Models for Mental Health",
    "year": 2022,
    "authors": [
      "Pablo Mosteiro",
      "Jesse Kuiper",
      "Judith Masthoff",
      "Floortje Scheepers",
      "Marco Spruit"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias related to gender in clinical psychiatry AI models, addressing social discrimination and fairness issues affecting gender groups.",
      "inequality_type": [
        "gender",
        "health"
      ],
      "other_detail": "Focus on gender bias in mental health AI",
      "affected_populations": [
        "patients",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Metrics",
        "Bias Mitigation Strategies"
      ],
      "methodology_detail": "Applied fairness metrics and mitigation in clinical data",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.11655v1",
    "title": "Attitudes, willingness, and resources to cover Article Publishing Charges (APC): the influence of age, position, income level country, discipline and open access habits",
    "year": 2022,
    "authors": [
      "Francisco Segado-Boj",
      "Juan-Jose Prieto-Gutiérrez",
      "Juan Martín-Quevedo"
    ],
    "categories": [
      "cs.DL"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities related to income, country, discipline, and access to funding, highlighting economic and geographic inequalities affecting scholarly publishing.",
      "inequality_type": [
        "economic",
        "income",
        "geographic",
        "educational"
      ],
      "other_detail": "Focus on global disparities in research funding and access",
      "affected_populations": [
        "researchers in low-income countries",
        "early-career scholars"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed survey data on attitudes towards APCs",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.11605v1",
    "title": "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
    "year": 2022,
    "authors": [
      "Afra Feyza Akyürek",
      "Sejin Paik",
      "Muhammed Yusuf Kocyigit",
      "Seda Akbiyik",
      "Şerife Leman Runyun",
      "Derry Wijaya"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in language models, which relate to social discrimination and inequality issues such as bias and fairness in AI outputs.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focuses on social biases in AI outputs",
      "affected_populations": [
        "social groups",
        "minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Natural Language Processing"
      ],
      "methodology_detail": "Bias benchmarks and prompt formulation analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.11485v2",
    "title": "Conditional Supervised Contrastive Learning for Fair Text Classification",
    "year": 2022,
    "authors": [
      "Jianfeng Chi",
      "William Shand",
      "Yaodong Yu",
      "Kai-Wei Chang",
      "Han Zhao",
      "Yuan Tian"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias mitigation in text classification, which relates to social fairness issues such as discrimination against underrepresented groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI systems for social equity",
      "affected_populations": [
        "underrepresented groups",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Contrastive Learning",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness constrained contrastive representation learning",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of bias and fairness mitigation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.10842v2",
    "title": "Addressing Strategic Manipulation Disparities in Fair Classification",
    "year": 2022,
    "authors": [
      "Vijay Keswani",
      "L. Elisa Celis"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses disparities in strategic manipulation costs across demographic groups, which relate to social fairness issues. It focuses on fairness in classification affecting minority groups, implying social inequality considerations.",
      "inequality_type": [
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness disparities in AI classification systems",
      "affected_populations": [
        "minority groups",
        "demographic groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes strategic costs and fairness metrics empirically",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.10828v4",
    "title": "What Do Compressed Multilingual Machine Translation Models Forget?",
    "year": 2022,
    "authors": [
      "Alireza Mohammadshahi",
      "Vassilina Nikoulina",
      "Alexandre Berard",
      "Caroline Brun",
      "James Henderson",
      "Laurent Besacier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in multilingual AI models, including gender and semantic biases, which relate to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "semantic"
      ],
      "other_detail": "Bias amplification in multilingual models",
      "affected_populations": [
        "under-represented languages",
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analysis of model biases across benchmarks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.10762v2",
    "title": "How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts",
    "year": 2022,
    "authors": [
      "Shanya Sharma",
      "Manan Dey",
      "Koustuv Sinha"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in translation systems, reflecting social gender inequality issues. It discusses mitigating stereotypical gender biases, which are social biases affecting gender groups. The focus on gender bias in AI systems relates directly to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI translation systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing"
      ],
      "methodology_detail": "Using contextual guidance to reduce gender bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.11264v2",
    "title": "Adaptive Fairness-Aware Online Meta-Learning for Changing Environments",
    "year": 2022,
    "authors": [
      "Chen Zhao",
      "Feng Mi",
      "Xintao Wu",
      "Kai Jiang",
      "Latifur Khan",
      "Feng Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing social bias related to protected groups such as race and gender, and aims to ensure statistical parity across these groups.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Fairness in AI systems across social groups",
      "affected_populations": [
        "race",
        "gender"
      ],
      "methodology": [
        "Machine Learning",
        "Online Learning",
        "Meta-Learning",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Fairness-aware online meta-learning in changing environments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.10200v2",
    "title": "The Fairness of Credit Scoring Models",
    "year": 2022,
    "authors": [
      "Christophe Hurlin",
      "Christophe Pérignon",
      "Sébastien Saurin"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.RM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in credit scoring, focusing on discrimination related to protected attributes like gender, age, and racial origin, which are social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "age"
      ],
      "other_detail": "Fairness in algorithmic decision-making",
      "affected_populations": [
        "protected groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Testing and optimizing fairness-performance trade-offs",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.10049v1",
    "title": "Assessing Demographic Bias Transfer from Dataset to Model: A Case Study in Facial Expression Recognition",
    "year": 2022,
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias and gender stereotypes in facial expression datasets, highlighting social disparities. It analyzes how these biases transfer to AI models, addressing fairness issues. The focus on demographic bias transfer relates directly to social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in facial expression recognition datasets",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias measurement and dataset analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.09977v1",
    "title": "FairNorm: Fair and Fast Graph Neural Network Training",
    "year": 2022,
    "authors": [
      "O. Deniz Kose",
      "Yanning Shen"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in GNNs, addressing bias and discrimination issues related to social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI systems",
      "affected_populations": [
        "social groups",
        "sensitive groups"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness-aware normalization",
        "Experiment"
      ],
      "methodology_detail": "Normalization framework to reduce bias and improve fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of bias and fairness solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.09867v3",
    "title": "Gender Bias in Meta-Embeddings",
    "year": 2022,
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala",
      "Naoaki Okazaki"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in AI embeddings, addressing gender fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias mitigation in AI representations",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Machine Learning"
      ],
      "methodology_detail": "Bias analysis and debiasing techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.09830v1",
    "title": "Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation",
    "year": 2022,
    "authors": [
      "Samhita Honnavalli",
      "Aesha Parekh",
      "Lily Ou",
      "Sophie Groenwold",
      "Sharon Levy",
      "Vicente Ordonez",
      "William Yang Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias and seniority perceptions in NLP models, highlighting societal gender inequalities. It examines how AI systems may reinforce gender-based professional stereotypes, addressing social discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Dataset Creation",
        "Experiment"
      ],
      "methodology_detail": "Probing bias in pretrained neural language models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.09240v1",
    "title": "Debiasing Neural Retrieval via In-batch Balancing Regularization",
    "year": 2022,
    "authors": [
      "Yuantong Li",
      "Xiaokai Wei",
      "Zijian Wang",
      "Shen Wang",
      "Parminder Bhatia",
      "Xiaofei Ma",
      "Andrew Arnold"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and bias mitigation in information retrieval systems, addressing social bias issues related to demographic subgroups such as gender. It aims to reduce ranking disparities among these groups, directly engaging with social fairness concerns.",
      "inequality_type": [
        "gender",
        "informational"
      ],
      "other_detail": "Bias mitigation in AI systems",
      "affected_populations": [
        "gender groups",
        "demographic subgroups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Regularization based on fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.09209v2",
    "title": "\"I'm sorry to hear that\": Finding New Biases in Language Models with a Holistic Descriptor Dataset",
    "year": 2022,
    "authors": [
      "Eric Michael Smith",
      "Melissa Hall",
      "Melanie Kambadur",
      "Eleonora Presani",
      "Adina Williams"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on measuring biases related to demographic identities, addressing social discrimination and fairness issues in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Includes multiple demographic axes and community participation",
      "affected_populations": [
        "minority groups",
        " marginalized communities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Develops bias measurement dataset and tests models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.09003v1",
    "title": "Diffusion and synchronization dynamics reveal the multi-scale patterns of spatial segregation",
    "year": 2022,
    "authors": [
      "Aleix Bassolas",
      "Sergio Gómez",
      "Alex Arenas"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes spatial segregation related to socioeconomic features like income, revealing patterns of deprivation and affluence in urban areas, which directly relate to social inequality.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "urban-rural"
      ],
      "other_detail": "Focuses on urban socioeconomic spatial patterns",
      "affected_populations": [
        "deprived citizens",
        "affluent citizens"
      ],
      "methodology": [
        "Diffusion analysis",
        "Synchronization dynamics",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using diffusion and synchronization times as proxies",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.08813v1",
    "title": "Regex in a Time of Deep Learning: The Role of an Old Technology in Age Discrimination Detection in Job Advertisements",
    "year": 2022,
    "authors": [
      "Anna Pillar",
      "Kyrill Poelmans",
      "Martha Larson"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses age discrimination in job ads, a social inequality issue.",
      "inequality_type": [
        "age"
      ],
      "other_detail": "focus on age discrimination detection",
      "affected_populations": [
        "job seekers over 40"
      ],
      "methodology": [
        "Qualitative Study",
        "Regex analysis",
        "Neural embeddings investigation"
      ],
      "methodology_detail": "Comparing regex and neural approaches for discrimination detection",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03262v3",
    "title": "Using sensitive data to prevent discrimination by artificial intelligence: Does the GDPR need a new exception?",
    "year": 2022,
    "authors": [
      "Marvin van Bekkum",
      "Frederik Zuiderveen Borgesius"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses AI discrimination related to ethnicity, a social inequality issue, and examines how legal frameworks impact addressing such inequalities.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on discrimination prevention and legal regulation",
      "affected_populations": [
        "ethnic minorities",
        "job applicants"
      ],
      "methodology": [
        "Legal Analysis",
        "Policy Analysis"
      ],
      "methodology_detail": "Analyzing GDPR and AI regulation implications",
      "geographic_focus": [
        "Europe"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.08148v3",
    "title": "Gender and Racial Bias in Visual Question Answering Datasets",
    "year": 2022,
    "authors": [
      "Yusuke Hirota",
      "Yuta Nakashima",
      "Noa Garcia"
    ],
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender and racial biases in VQA datasets, highlighting societal stereotypes and underrepresentation, directly addressing social discrimination issues.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias in AI datasets affecting social groups",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes dataset distributions and stereotypes",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.08112v1",
    "title": "The Fairness of Machine Learning in Insurance: New Rags for an Old Man?",
    "year": 2022,
    "authors": [
      "Laurence Barry",
      "Arthur Charpentier"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness and discrimination in insurance algorithms, relating to social biases and impacts on different social groups.",
      "inequality_type": [
        "economic",
        "social",
        "discrimination"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias in insurance",
      "affected_populations": [
        "insurance customers",
        "social groups"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes historical and current debates on discrimination",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of existing biases",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.07722v2",
    "title": "How Different Groups Prioritize Ethical Values for Responsible AI",
    "year": 2022,
    "authors": [
      "Maurice Jakesch",
      "Zana Buçinca",
      "Saleema Amershi",
      "Alexandra Olteanu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how different social groups prioritize ethical values in AI, highlighting disparities based on gender, race, and political orientation, which relate to social inequalities.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on social group differences in AI value prioritization",
      "affected_populations": [
        "women",
        "black individuals",
        "general public",
        "AI practitioners"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparative survey across groups",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as subject of regulation",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.07277v2",
    "title": "Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations",
    "year": 2022,
    "authors": [
      "Jessica Dai",
      "Sohini Upadhyay",
      "Ulrich Aivodji",
      "Stephen H. Bach",
      "Himabindu Lakkaraju"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in explanation quality across social groups, highlighting fairness issues in AI explanations related to gender and potentially other social categories.",
      "inequality_type": [
        "gender",
        "social bias",
        "fairness"
      ],
      "other_detail": "Focuses on explanation fairness in AI systems",
      "affected_populations": [
        "minority groups",
        "gender subgroups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates explanation disparities across groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2206.03226v1",
    "title": "Fairness and Explainability in Automatic Decision-Making Systems. A challenge for computer science and law",
    "year": 2022,
    "authors": [
      "Thierry Kirat",
      "Olivia Tambou",
      "Virginie Do",
      "Alexis Tsoukiàs"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness issues, group discrimination, and impacts on social groups, linking algorithmic decisions to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "social"
      ],
      "other_detail": "Focus on legal and social fairness implications",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "social groups"
      ],
      "methodology": [
        "Legal Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Cross-referencing legal concepts with technical fairness",
      "geographic_focus": [
        "United States",
        "Europe"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.06621v2",
    "title": "Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes",
    "year": 2022,
    "authors": [
      "Antonis Maronikolakis",
      "Philip Baader",
      "Hinrich Schütze"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases in hate speech related to race and gender, which are social inequalities, and examines how AI models propagate these biases, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focus on bias and discrimination in social groups",
      "affected_populations": [
        "African Americans",
        "masculine groups"
      ],
      "methodology": [
        "Data Analysis",
        "Natural Language Processing",
        "Model Evaluation"
      ],
      "methodology_detail": "Bias detection and mitigation in datasets and models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.06548v1",
    "title": "Meta Balanced Network for Fair Face Recognition",
    "year": 2022,
    "authors": [
      "Mei Wang",
      "Yaobin Zhang",
      "Weihong Deng"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in face recognition, a social discrimination issue.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in AI systems affecting different skin tones",
      "affected_populations": [
        "people with different skin tones"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Algorithm Development"
      ],
      "methodology_detail": "Bias quantification and mitigation in face recognition",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.08383v1",
    "title": "Bias and Fairness on Multimodal Emotion Detection Algorithms",
    "year": 2022,
    "authors": [
      "Matheus Schmitz",
      "Rehan Ahmed",
      "Jimi Cao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in emotion recognition systems related to modalities, highlighting issues of fairness and discrimination in AI, which are connected to social bias concerns.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias in emotion detection",
      "affected_populations": [
        "social groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Analyzes bias across multimodal emotion recognition models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.05661v1",
    "title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India",
    "year": 2022,
    "authors": [
      "Divya Ramesh",
      "Vaishnav Kameswaran",
      "Ding Wang",
      "Nithya Sambasivan"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines socioeconomic vulnerabilities and power dynamics affecting marginalized users in India, highlighting inequalities in platform-user relations and their impact on accountability.",
      "inequality_type": [
        "socioeconomic",
        "digital",
        "geographic"
      ],
      "other_detail": "Focus on financial stress and platform dependence in India",
      "affected_populations": [
        "financially stressed users",
        "vulnerable populations"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Interviews with users of loan platforms in India",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.05093v1",
    "title": "Richer Countries and Richer Representations",
    "year": 2022,
    "authors": [
      "Kaitlyn Zhou",
      "Kawin Ethayarajh",
      "Dan Jurafsky"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper links representation disparities in AI to country wealth and GDP, highlighting how frequency biases perpetuate economic inequalities.",
      "inequality_type": [
        "economic",
        "income",
        "wealth",
        "socioeconomic"
      ],
      "other_detail": "Bias in AI reflecting global economic disparities",
      "affected_populations": [
        "countries",
        "economically less wealthy nations"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Analyzes frequency and embedding performance correlations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04790v3",
    "title": "Don't Throw it Away! The Utility of Unlabeled Data in Fair Decision Making",
    "year": 2022,
    "authors": [
      "Miriam Rateike",
      "Ayan Majumdar",
      "Olga Mineeva",
      "Krishna P. Gummadi",
      "Isabel Valera"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in decision-making algorithms, which relates to social bias and discrimination issues affecting marginalized groups. It discusses biases in data and aims to improve fairness, impacting social inequalities.",
      "inequality_type": [
        "social",
        "racial",
        "gender",
        "educational",
        "health"
      ],
      "other_detail": "Focuses on algorithmic fairness and bias mitigation",
      "affected_populations": [
        "social groups",
        "marginalized communities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Uses variational autoencoder for data representation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04610v1",
    "title": "Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation",
    "year": 2022,
    "authors": [
      "Angelina Wang",
      "Vikram V. Ramaswamy",
      "Olga Russakovsky"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in machine learning, focusing on demographic attributes and intersectionality, which relate to social discrimination and inequality.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on demographic intersectionality in ML fairness",
      "affected_populations": [
        "minority groups",
        "underrepresented demographics"
      ],
      "methodology": [
        "Empirical Evaluation",
        "Machine Learning",
        "Data Analysis"
      ],
      "methodology_detail": "Empirical evaluation on census-derived datasets",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04505v1",
    "title": "Behind the Mask: Demographic bias in name detection for PII masking",
    "year": 2022,
    "authors": [
      "Courtney Mansfield",
      "Amandalynne Paullada",
      "Kristen Howell"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities in AI performance, highlighting racial bias in name detection, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Bias in AI systems affecting marginalized groups",
      "affected_populations": [
        "Black individuals",
        "Asian/Pacific Islanders"
      ],
      "methodology": [
        "Experiment",
        "Machine Learning",
        "Dataset Creation"
      ],
      "methodology_detail": "Evaluates PII masking systems on demographic data",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04321v2",
    "title": "Evaluating the Fairness Impact of Differentially Private Synthetic Data",
    "year": 2022,
    "authors": [
      "Blake Bullwinkel",
      "Kristen Grabarz",
      "Lily Ke",
      "Scarlett Gong",
      "Chris Tanner",
      "Joshua Allen"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI synthetic data, focusing on minority group representation, which relates to social fairness issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fairness impact on minority groups in synthetic data",
      "affected_populations": [
        "minority groups"
      ],
      "methodology": [
        "Empirical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Evaluates synthetic data models and fairness outcomes",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04221v1",
    "title": "The Forgotten Margins of AI Ethics",
    "year": 2022,
    "authors": [
      "Abeba Birhane",
      "Elayne Ruane",
      "Thomas Laurent",
      "Matthew S. Brown",
      "Johnathan Flowers",
      "Anthony Ventresque",
      "Christopher L. Dancy"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses marginalized groups, social impacts, and power asymmetries in AI ethics, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on structural and historical power asymmetries",
      "affected_populations": [
        "marginalized groups",
        "social minorities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Annotated recent AI ethics literature for social impact",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.03962v2",
    "title": "Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation",
    "year": 2022,
    "authors": [
      "Haiwen Feng",
      "Timo Bolkart",
      "Joachim Tesch",
      "Michael J. Black",
      "Victoria Abrevaya"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in skin tone estimation, impacting fairness in AI systems, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial fairness in AI-based skin tone estimation",
      "affected_populations": [
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Algorithm Development",
        "Experiment"
      ],
      "methodology_detail": "Includes new dataset and fairness-aware algorithm",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.03931v2",
    "title": "Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations",
    "year": 2022,
    "authors": [
      "Hammaad Adam",
      "Ming Ying Yang",
      "Kenrick Cato",
      "Ioana Baldini",
      "Charles Senteio",
      "Leo Anthony Celi",
      "Jiaming Zeng",
      "Moninder Singh",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial biases in clinical notes and their impact on healthcare disparities, highlighting implicit racial information and its effects on treatment decisions.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial bias in medical documentation",
      "affected_populations": [
        "racial minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Simulation"
      ],
      "methodology_detail": "Detects race from clinical notes and models' bias effects",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.03295v2",
    "title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations",
    "year": 2022,
    "authors": [
      "Aparna Balagopalan",
      "Haoran Zhang",
      "Kimia Hamidieh",
      "Thomas Hartvigsen",
      "Frank Rudzicz",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in explanations of AI models across social subgroups, highlighting disparities in explanation quality that relate to protected groups such as race, healthcare, and justice systems.",
      "inequality_type": [
        "racial",
        "health",
        "justice system",
        "educational"
      ],
      "other_detail": "Focus on fairness of AI explanations across social groups",
      "affected_populations": [
        "racial minorities",
        "patients",
        "justice system users",
        "students"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Auditing explanation fidelity across subgroups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04460v1",
    "title": "Rethinking Fairness: An Interdisciplinary Survey of Critiques of Hegemonic ML Fairness Approaches",
    "year": 2022,
    "authors": [
      "Lindsay Weinberg"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critiques ML fairness approaches related to marginalized groups, racial classification, and societal power dynamics, indicating a focus on social inequalities.",
      "inequality_type": [
        "racial",
        "social",
        "ethnic"
      ],
      "other_detail": "Interdisciplinary critique of AI fairness and social justice",
      "affected_populations": [
        "marginalized groups",
        "racial minorities",
        "social minorities"
      ],
      "methodology": [
        "Literature Review",
        "Ethics Analysis"
      ],
      "methodology_detail": "Interdisciplinary perspectives from social sciences and philosophy",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social inequalities",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.04556v1",
    "title": "Can transit investments in low-income neighbourhoods increase transit use? Exploring the nexus of income, car-ownership, and transit accessibility in Toronto",
    "year": 2022,
    "authors": [
      "Elnaz Yousefzadeh Barri",
      "Steven Farber",
      "Anna Kramer",
      "Hadi Jahanshahi",
      "Jeff Allen",
      "Eda Beyazit"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.NA",
      "math.NA"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how transit investments impact low-income households, addressing socioeconomic disparities in transit access and mobility. It explores how income and car ownership influence transit use, highlighting social equity considerations.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on low-income neighborhoods and mobility disparities",
      "affected_populations": [
        "low-income households",
        "transit users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Statistical Analysis"
      ],
      "methodology_detail": "Regression models analyzing transit use and accessibility",
      "geographic_focus": [
        "Toronto",
        "Hamilton",
        "GTHA"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.02673v1",
    "title": "On Disentangled and Locally Fair Representations",
    "year": 2022,
    "authors": [
      "Yaron Gurovich",
      "Sagie Benaim",
      "Lior Wolf"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI decisions related to sensitive groups such as race and gender, aiming to reduce social bias and discrimination. It focuses on creating fair representations to improve decision-making fairness. The emphasis on racial balance and sensitive attributes indicates a direct engagement with social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fair representations in AI systems",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Disentangled Representation Learning",
        "Fairness Objective"
      ],
      "methodology_detail": "Learning fair, disentangled embeddings for classification",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.02526v1",
    "title": "Theories of \"Gender\" in NLP Bias Research",
    "year": 2022,
    "authors": [
      "Hannah Devinney",
      "Jenny Björklund",
      "Henrik Björklund"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper critically examines gender bias in NLP, highlighting social biases and inclusivity issues related to gender identities, which are central to social inequality discussions.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender bias and inclusivity in NLP research",
      "affected_populations": [
        "trans people",
        "nonbinary people",
        "intersex people"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes existing research and conceptual frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.02389v1",
    "title": "Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility",
    "year": 2022,
    "authors": [
      "Johannes Himmelreich",
      "Désirée Lim"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses structural injustice, bias, and responsibility in AI, focusing on social discrimination and inequality, especially racial bias, and relates these to social justice frameworks.",
      "inequality_type": [
        "racial",
        "social",
        "inequality"
      ],
      "other_detail": "Focus on structural injustice and societal biases",
      "affected_populations": [
        "racial groups",
        "social groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Analyzes social theories and AI bias frameworks",
      "geographic_focus": null,
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.02052v1",
    "title": "Exploring Rawlsian Fairness for K-Means Clustering",
    "year": 2022,
    "authors": [
      "Stanley Simoes",
      "Deepak P",
      "Muiris MacCarthaigh"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explores fairness in clustering, addressing social bias and discrimination issues in AI systems.",
      "inequality_type": [
        "socioeconomic",
        "fairness"
      ],
      "other_detail": "Focuses on fairness principles in unsupervised learning",
      "affected_populations": [
        "social groups",
        "users of AI systems"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Development"
      ],
      "methodology_detail": "Postprocessing fairness adjustment in clustering",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.01166v1",
    "title": "The Theory of Artificial Immutability: Protecting Algorithmic Groups Under Anti-Discrimination Law",
    "year": 2022,
    "authors": [
      "Sandra Wachter"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines AI-driven profiling that impacts social groups and autonomy, addressing issues related to discrimination and inequality beyond traditional protected classes.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "disability"
      ],
      "other_detail": "Focus on algorithmic groups as de facto immutable traits",
      "affected_populations": [
        "social groups",
        "individuals impacted by profiling"
      ],
      "methodology": [
        "Legal Analysis",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes legal doctrines and proposes new harm theory",
      "geographic_focus": [
        "North America",
        "Europe"
      ],
      "ai_relationship": "AI as cause/amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.00819v1",
    "title": "A Novel Approach to Fairness in Automated Decision-Making using Affective Normalization",
    "year": 2022,
    "authors": [
      "Jesse Hoey",
      "Gabrielle Chan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses social biases related to race and gender in decision-making, proposing a fairness method to mitigate affective bias, which directly impacts social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias measurement and normalization in AI decisions",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Measuring affective bias and normalizing outcomes",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.00551v3",
    "title": "Gender Bias in Masked Language Models for Multiple Languages",
    "year": 2022,
    "authors": [
      "Masahiro Kaneko",
      "Aizhan Imankulova",
      "Danushka Bollegala",
      "Naoaki Okazaki"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in multilingual language models, addressing social discrimination related to gender. It evaluates how AI systems encode gender biases across languages, highlighting social fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Bias evaluation across multiple languages using parallel corpora",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.01512v1",
    "title": "Fair Feature Subset Selection using Multiobjective Genetic Algorithm",
    "year": 2022,
    "authors": [
      "Ayaz Ur Rehman",
      "Anas Nadeem",
      "Muhammad Zubair Malik"
    ],
    "categories": [
      "cs.NE",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper explicitly addresses fairness in AI, focusing on bias related to protected attributes such as race, gender, and age, and aims to improve fairness metrics alongside accuracy.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "disability"
      ],
      "other_detail": "Focus on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Machine Learning",
        "Evolutionary Algorithm",
        "Statistical Analysis"
      ],
      "methodology_detail": "Uses NSGA-II for Pareto optimization of fairness and accuracy",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.00313v1",
    "title": "FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings",
    "year": 2022,
    "authors": [
      "Cheng-Te Li",
      "Cheng Hsu",
      "Yang Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.LG",
      "cs.SI",
      "H.3.3"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in recommendation systems, addressing algorithmic bias affecting user groups with different protected attributes, which relates to social fairness issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in AI recommendation systems",
      "affected_populations": [
        "users with protected attributes"
      ],
      "methodology": [
        "Deep Learning",
        "Multi-Task Learning",
        "Graph Embeddings",
        "Experiment"
      ],
      "methodology_detail": "Integrates preference graph embeddings with sequential recommendation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.00048v1",
    "title": "Joint Multisided Exposure Fairness for Recommendation",
    "year": 2022,
    "authors": [
      "Haolun Wu",
      "Bhaskar Mitra",
      "Chen Ma",
      "Fernando Diaz",
      "Xue Liu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses systemic disparities in information exposure that can cause social harms, including marginalization and stereotypes, indicating a focus on social inequalities related to race, gender, and societal biases.",
      "inequality_type": [
        "racial",
        "gender",
        "informational"
      ],
      "other_detail": "Systemic biases in recommendation systems",
      "affected_populations": [
        "marginalized groups",
        "general users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Fairness metrics and stochastic ranking optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.13842v1",
    "title": "A Bottom-Up End-User Intelligent Assistant Approach to Empower Gig Workers against AI Inequality",
    "year": 2022,
    "authors": [
      "Toby Jia-Jun Li",
      "Yuwen Lu",
      "Jaylexia Clark",
      "Meng Chen",
      "Victor Cox",
      "Meng Jiang",
      "Yang Yang",
      "Tamara Kay",
      "Danielle Wood",
      "Jay Brockman"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses AI inequality affecting gig workers' access, data ownership, and working conditions, which relate to socioeconomic and digital inequalities impacting marginalized groups.",
      "inequality_type": [
        "socioeconomic",
        "digital",
        "informational"
      ],
      "other_detail": "Focuses on AI access and data ownership disparities",
      "affected_populations": [
        "gig workers",
        "unprivileged workers"
      ],
      "methodology": [
        "System Design",
        "Community Engagement"
      ],
      "methodology_detail": "Proposes end-user programmable AI assistants",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.13805v3",
    "title": "Investigating writing style as a contributor to gender gaps in science and technology",
    "year": 2022,
    "authors": [
      "Kara Kedrick",
      "Ekaterina Levitskaya",
      "Russell J. Funk"
    ],
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender differences in scientific writing styles and their impact on gender gaps, addressing gender inequality in science.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focuses on gender disparities in academic evaluation",
      "affected_populations": [
        "women in science",
        "female authors"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Natural Language Processing"
      ],
      "methodology_detail": "Analyzes linguistic features and citation patterns",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.13757v1",
    "title": "Network Creation with Homophilic Agents",
    "year": 2022,
    "authors": [
      "Martin Bullinger",
      "Pascal Lenzner",
      "Anna Melnichenko"
    ],
    "categories": [
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines social segregation driven by homophily, which relates to racial and social inequalities. It analyzes how agent preferences lead to network structures that reflect social biases. The focus on segregation and social structure indicates addressing social inequality issues.",
      "inequality_type": [
        "racial",
        "ethnic",
        "social"
      ],
      "other_detail": "focus on social segregation and bias",
      "affected_populations": [
        "ethnic groups",
        "social communities"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experimental Analysis"
      ],
      "methodology_detail": "modeling and simulation of network formation",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.12649v1",
    "title": "Study on the Fairness of Speaker Verification Systems on Underrepresented Accents in English",
    "year": 2022,
    "authors": [
      "Mariel Estevez",
      "Luciana Ferrer"
    ],
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and bias in AI systems across accent groups, addressing social bias and inequality in technology's impact on different linguistic communities.",
      "inequality_type": [
        "linguistic",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on accent-based fairness in speech AI",
      "affected_populations": [
        "non-native English speakers",
        "accented speakers"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Statistical Analysis"
      ],
      "methodology_detail": "Evaluates system performance across accent groups",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.12421v1",
    "title": "Disambiguation of morpho-syntactic features of African American English -- the case of habitual be",
    "year": 2022,
    "authors": [
      "Harrison Santiago",
      "Joshua Martin",
      "Sarah Moeller",
      "Kevin Tang"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial bias in NLP systems affecting African American English, highlighting social discrimination and bias in technology impacting a marginalized group.",
      "inequality_type": [
        "racial",
        "linguistic"
      ],
      "other_detail": "Bias in AI systems affecting racial linguistic groups",
      "affected_populations": [
        "African American speakers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Machine Learning",
        "Data Augmentation"
      ],
      "methodology_detail": "Balancing linguistic features to reduce bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.11247v1",
    "title": "\"It Feels Like Being Locked in A Cage\": Understanding Blind or Low Vision Streamers' Perceptions of Content Curation Algorithms",
    "year": 2022,
    "authors": [
      "Ethan Z. Rong",
      "Mo Morgana Zhou",
      "Zhicong Lu",
      "Mingming Fan"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines how content curation algorithms impact marginalized BLV streamers, highlighting issues of algorithmic bias and perceived disadvantages, which relate to social inequality and marginalization.",
      "inequality_type": [
        "disability",
        "informational",
        "digital"
      ],
      "other_detail": "Focus on marginalized visually impaired content creators",
      "affected_populations": [
        "BLV streamers",
        "visually impaired users"
      ],
      "methodology": [
        "Qualitative Study",
        "Interviews"
      ],
      "methodology_detail": "In-depth interviews with BLV streamers",
      "geographic_focus": [
        "China"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.10940v1",
    "title": "Balancing Fairness and Accuracy in Sentiment Detection using Multiple Black Box Models",
    "year": 2022,
    "authors": [
      "Abdulaziz A. Almuzaini",
      "Vivek K. Singh"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper audits gender bias in sentiment detection APIs and proposes fairness methods, addressing social bias and discrimination issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI systems",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Machine Learning",
        "Fairness Approach"
      ],
      "methodology_detail": "joint learning from multiple models for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.10262v1",
    "title": "An Examination of Bias of Facial Analysis based BMI Prediction Models",
    "year": 2022,
    "authors": [
      "Hera Siddiqui",
      "Ajita Rattani",
      "Karl Ricanek",
      "Twyla Hill"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial and gender biases in facial analysis-based BMI prediction, highlighting disparities across social groups, which relates directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias in AI systems across social groups",
      "affected_populations": [
        "Black Males",
        "White Females"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analysis of bias across demographic groups using facial datasets",
      "geographic_focus": [
        "Caucasian",
        "African-American"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.01072v1",
    "title": "The Equity Framework: Fairness Beyond Equalized Predictive Outcomes",
    "year": 2022,
    "authors": [
      "Keziah Naggita",
      "J. Ceasar Aguma"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness, access, and outcomes in decision-making models affecting social groups, highlighting structural inequities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational",
        "health"
      ],
      "other_detail": "Focus on fairness and equity in AI decision processes",
      "affected_populations": [
        "disadvantaged communities",
        "minority groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Algorithm Development"
      ],
      "methodology_detail": "Develops an equity scoring algorithm and decision guidance",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2205.01038v2",
    "title": "Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness",
    "year": 2022,
    "authors": [
      "McKane Andrus",
      "Sarah Villeneuve"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses demographic data collection, systemic oppression, and risks related to social groups, addressing issues of discrimination and fairness.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "discrimination"
      ],
      "other_detail": "Focus on systemic oppression and data governance",
      "affected_populations": [
        "individuals",
        "communities"
      ],
      "methodology": [
        "Ethics Analysis",
        "Qualitative Study"
      ],
      "methodology_detail": "Analyzing social risks and governance considerations",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.07900v1",
    "title": "Using HCI to Tackle Race and Gender Bias in ADHD Diagnosis",
    "year": 2022,
    "authors": [
      "Naba Rizvi",
      "Khalil Mrini"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses racial and gender biases in ADHD diagnosis, highlighting social disparities. It discusses how diagnostic technology may perpetuate stereotypes affecting marginalized groups. The focus on bias and fairness in AI systems relates directly to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias in diagnostic technology affecting marginalized groups",
      "affected_populations": [
        "children",
        "adults"
      ],
      "methodology": [
        "HCI",
        "Literature Review",
        "Future Study Suggestions"
      ],
      "methodology_detail": "Focus on human-computer interaction and bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.07897v1",
    "title": "Making Hidden Bias Visible: Designing a Feedback Ecosystem for Primary Care Providers",
    "year": 2022,
    "authors": [
      "Naba Rizvi",
      "Harshini Ramaswamy",
      "Reggie Casanova-Perez",
      "Andrea Hartzler",
      "Nadir Weibel"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses implicit bias in healthcare, impacting marginalized groups, which relates to social inequalities such as race, health, and potentially gender. It focuses on disparities in patient care and provider communication. The design aims to make biases visible, directly engaging with social discrimination issues.",
      "inequality_type": [
        "racial",
        "health",
        "gender"
      ],
      "other_detail": "Implicit bias in healthcare communication",
      "affected_populations": [
        "BIPOC",
        "LGBTQ"
      ],
      "methodology": [
        "System Design",
        "Qualitative Study"
      ],
      "methodology_detail": "Design of communication feedback ecosystem",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.06501v1",
    "title": "Clinical trial site matching with improved diversity using fair policy learning",
    "year": 2022,
    "authors": [
      "Rakshith S Srinivasa",
      "Cheng Qian",
      "Brandon Theodorou",
      "Jeffrey Spaeder",
      "Cao Xiao",
      "Lucas Glass",
      "Jimeng Sun"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "J.3; I.2.1"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses diversity and inclusion in clinical trials, focusing on equitable access for different social groups, which relates to social inequalities such as race, ethnicity, age, and socioeconomic factors.",
      "inequality_type": [
        "racial",
        "ethnic",
        "age",
        "socioeconomic",
        "health"
      ],
      "other_detail": "Focus on equitable trial site selection and patient access",
      "affected_populations": [
        "racial groups",
        "ethnic groups",
        "elderly",
        "socioeconomic classes"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness in AI",
        "Ranking Algorithms"
      ],
      "methodology_detail": "Fair policy learning with demographic parity constraints",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.05872v1",
    "title": "Robust Quantification of Gender Disparity in Pre-Modern English Literature using Natural Language Processing",
    "year": 2022,
    "authors": [
      "Akarsh Nagaraj",
      "Mayank Kejriwal"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper analyzes gender disparity in literature, highlighting social gender inequality. It uses NLP to quantify differences in representation, directly addressing gender-based social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female characters",
        "male characters"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Measuring gender disparity in literary texts",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.05459v1",
    "title": "Easy Adaptation to Mitigate Gender Bias in Multilingual Text Classification",
    "year": 2022,
    "authors": [
      "Xiaolei Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.SI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in multilingual text classification, focusing on mitigating gender-related biases in AI systems, which relates to social gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "female",
        "male"
      ],
      "methodology": [
        "Domain Adaptation",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Treats gender as domain for bias mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.05157v1",
    "title": "SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles",
    "year": 2022,
    "authors": [
      "Cuong Tran",
      "Keyu Zhu",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness and non-discrimination in AI models, addressing social bias issues related to demographic groups.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Fairness in AI models without access to sensitive data",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Analysis",
        "Experiment"
      ],
      "methodology_detail": "Scalable fair and private model training",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.04440v2",
    "title": "Are Two Heads the Same as One? Identifying Disparate Treatment in Fair Neural Networks",
    "year": 2022,
    "authors": [
      "Michael Lohaus",
      "Matthäus Kleindessner",
      "Krishnaram Kenthapadi",
      "Francesco Locatello",
      "Chris Russell"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in AI systems related to protected attributes like race and gender, highlighting issues of disparate treatment and potential legal implications under US law.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on fairness and discrimination in AI decision-making",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Algorithm Auditing",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Analyzes fairness approaches and their legal implications",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social bias",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.03558v1",
    "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
    "year": 2022,
    "authors": [
      "António Câmara",
      "Nina Taneja",
      "Tamjeed Azad",
      "Emily Allaway",
      "Richard Zemel"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper studies social biases related to gender, race, and ethnicity in multilingual NLP systems, addressing fairness issues impacting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on social biases in AI systems",
      "affected_populations": [
        "women",
        "racial minorities",
        "ethnic groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Statistical Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Multilingual bias measurement frameworks and statistical tools",
      "geographic_focus": [
        "English-speaking",
        "Spanish-speaking",
        "Arabic-speaking regions"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.03100v1",
    "title": "Data Justice Stories: A Repository of Case Studies",
    "year": 2022,
    "authors": [
      "David Leslie",
      "Morgan Briggs",
      "Antonella Perini",
      "Smera Jayadeva",
      "Cami Rincón",
      "Noopur Raval",
      "Abeba Birhane",
      "Rosamund Powell",
      "Michael Katell",
      "Mhairi Aitken"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses social justice, power dynamics, and societal transformation, indicating a focus on social inequalities across various contexts.",
      "inequality_type": [
        "social",
        "power",
        "inequity"
      ],
      "other_detail": "Focus on data justice and societal fairness",
      "affected_populations": [
        "marginalized groups",
        "digital users"
      ],
      "methodology": [
        "Case Study",
        "Literature Review"
      ],
      "methodology_detail": "Compilation of global data justice initiatives",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.03090v1",
    "title": "Advancing Data Justice Research and Practice: An Integrated Literature Review",
    "year": 2022,
    "authors": [
      "David Leslie",
      "Michael Katell",
      "Mhairi Aitken",
      "Jatinder Singh",
      "Morgan Briggs",
      "Rosamund Powell",
      "Cami Rincón",
      "Thompson Chengeta",
      "Abeba Birhane",
      "Antonella Perini",
      "Smera Jayadeva",
      "Anjali Mazumder"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.GL",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses data justice, social structures, and power dynamics affecting marginalized groups, emphasizing social justice in data ecosystems.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic",
        "educational",
        "disability"
      ],
      "other_detail": "Focus on justice and equity in data governance",
      "affected_populations": [
        "impacted communities",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Critical Empirical Analysis"
      ],
      "methodology_detail": "Systematic analysis of key texts and social structures",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.02947v3",
    "title": "Marrying Fairness and Explainability in Supervised Learning",
    "year": 2022,
    "authors": [
      "Przemyslaw Grabowicz",
      "Nicholas Perello",
      "Aarshee Mishra"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination and fairness issues related to protected social groups, such as race and gender, in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on algorithmic discrimination and fairness measures",
      "affected_populations": [
        "protected groups",
        "discriminated individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Measuring effects and developing post-processing fairness methods",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.00541v1",
    "title": "FairRank: Fairness-aware Single-tower Ranking Framework for News Recommendation",
    "year": 2022,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI systems affecting users with different sensitive attributes, which relates to social discrimination and inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in news recommendation systems",
      "affected_populations": [
        "users with sensitive attributes"
      ],
      "methodology": [
        "Adversarial Learning",
        "KL Loss",
        "Machine Learning"
      ],
      "methodology_detail": "Bias reduction techniques in ranking models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.00536v1",
    "title": "Semi-FairVAE: Semi-supervised Fair Representation Learning with Adversarial Variational Autoencoder",
    "year": 2022,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI representations related to sensitive attributes, addressing bias and discrimination issues. It aims to improve fairness when sensitive attribute labels are scarce, which relates to social bias mitigation. The work directly engages with fairness concerns impacting social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic",
        "socioeconomic"
      ],
      "other_detail": "Fair representation learning in AI systems",
      "affected_populations": [
        "social groups",
        "discriminated minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Adversarial variational autoencoder with semi-supervised learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.16663v1",
    "title": "Robust Reputation Independence in Ranking Systems for Multiple Sensitive Attributes",
    "year": 2022,
    "authors": [
      "Guilherme Ramos",
      "Ludovico Boratto",
      "Mirko Marras"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and discrimination in ranking systems related to sensitive attributes like gender and age, which are social categories linked to inequality.",
      "inequality_type": [
        "gender",
        "age"
      ],
      "other_detail": "Multiple sensitive attributes considered simultaneously",
      "affected_populations": [
        "gender groups",
        "age groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation and ranking quality assessment",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.16209v1",
    "title": "Fair Contrastive Learning for Facial Attribute Classification",
    "year": 2022,
    "authors": [
      "Sungho Park",
      "Jewook Lee",
      "Pilhyeon Lee",
      "Sunhee Hwang",
      "Dohyung Kim",
      "Hyeran Byun"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and bias in AI representations related to demographic groups, focusing on racial and gender fairness in facial attribute classification.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias mitigation in facial attribute AI systems",
      "affected_populations": [
        "demographic groups",
        "racial minorities",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Experiment",
        "Fairness Analysis"
      ],
      "methodology_detail": "Proposes a fairness-aware contrastive loss function",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.16024v2",
    "title": "Longitudinal Fairness with Censorship",
    "year": 2022,
    "authors": [
      "Wenbin Zhang",
      "Jeremy C. Weiss"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on social bias and discrimination in sensitive tasks like healthcare and recidivism prediction.",
      "inequality_type": [
        "health",
        "educational",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fairness in censored longitudinal data environments",
      "affected_populations": [
        "patients",
        "criminal defendants",
        "general public"
      ],
      "methodology": [
        "Fairness Measures",
        "Algorithm Development",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Addressing censorship in fairness algorithms",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.15395v1",
    "title": "Quantifying Societal Bias Amplification in Image Captioning",
    "year": 2022,
    "authors": [
      "Yusuke Hirota",
      "Yuta Nakashima",
      "Noa Garcia"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines societal biases related to gender and race in AI-generated image captions, addressing social discrimination and bias measurement.",
      "inequality_type": [
        "gender",
        "racial"
      ],
      "other_detail": "Bias amplification in AI image captioning",
      "affected_populations": [
        "women",
        "racial minorities"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Evaluates bias metrics and model behavior",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.13784v1",
    "title": "Disadvantaged Communities Have Lower Access to Urban Infrastructure",
    "year": 2022,
    "authors": [
      "Leonardo Nicoletti",
      "Mikhail Sirenko",
      "Trivik Verma"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.SI",
      "J.4; K.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in access to urban infrastructure linked to socioeconomic and racial groups, highlighting inequalities among urban communities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "geographic",
        "educational"
      ],
      "other_detail": "Focuses on spatial and social segregation in cities",
      "affected_populations": [
        "minorities",
        "low-income groups",
        "less educated"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Data-driven framework"
      ],
      "methodology_detail": "Analyzes accessibility data across multiple cities",
      "geographic_focus": [
        "54 cities"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.13369v2",
    "title": "Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings",
    "year": 2022,
    "authors": [
      "Sean Matthews",
      "John Hudzina",
      "Dawn Sepehr"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to gender and race in legal NLP systems, highlighting social discrimination issues. It discusses how these biases may perpetuate inequalities in legal contexts. The focus on racial and gender stereotypes indicates a direct engagement with social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Bias detection in legal language models",
      "affected_populations": [
        "racial minorities",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection in word embeddings trained on legal texts",
      "geographic_focus": [
        "U.S."
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.12748v1",
    "title": "Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning",
    "year": 2022,
    "authors": [
      "Natalie Dullerud",
      "Karsten Roth",
      "Kimia Hamidieh",
      "Nicolas Papernot",
      "Marzyeh Ghassemi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper evaluates fairness and bias in deep metric learning, focusing on subgroup performance disparities, which relate to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias propagation in AI representations affecting social groups",
      "affected_populations": [
        "minority groups",
        "subgroups"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates bias and fairness in AI representations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.12574v1",
    "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
    "year": 2022,
    "authors": [
      "Umang Gupta",
      "Jwala Dhamala",
      "Varun Kumar",
      "Apurv Verma",
      "Yada Pruksachatkun",
      "Satyapriya Krishna",
      "Rahul Gupta",
      "Kai-Wei Chang",
      "Greg Ver Steeg",
      "Aram Galstyan"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in language models, a form of social discrimination. It focuses on mitigating gender disparity in AI-generated text, directly related to gender inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Bias in AI language models",
      "affected_populations": [
        "women",
        "gender-neutral professions"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in language model training",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.11636v2",
    "title": "A Method for Estimating Individual Socioeconomic Status of Twitter Users",
    "year": 2022,
    "authors": [
      "Yuanmo He",
      "Milena Tsvetkova"
    ],
    "categories": [
      "cs.SI",
      "cs.CY",
      "K.6; J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on estimating socioeconomic status, a key aspect of social inequality, using social media data. It discusses socioeconomic and cultural capital, aligning with class and economic inequalities. No mention of racial, gender, health, or other social disparities is present.",
      "inequality_type": [
        "economic",
        "socioeconomic",
        "class"
      ],
      "other_detail": "Focuses on socioeconomic status estimation via social media",
      "affected_populations": [
        "Twitter users"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Correspondence Analysis"
      ],
      "methodology_detail": "Adapts political science method for SES estimation",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.11401v1",
    "title": "Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective",
    "year": 2022,
    "authors": [
      "Osama Khalid",
      "Jonathan Rusert",
      "Padmini Srinivasan"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "I.2.7"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases in taboo language detection across racial and community groups, highlighting racial biases and their impact on minority communities.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on racial biases in AI systems",
      "affected_populations": [
        "African Americans",
        "South Asians"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Community-specific classifiers for bias analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.11383v2",
    "title": "DIANES: A DEI Audit Toolkit for News Sources",
    "year": 2022,
    "authors": [
      "Xiaoxiao Shang",
      "Zhiyuan Peng",
      "Qiming Yuan",
      "Sabiq Khan",
      "Lauren Xie",
      "Yi Fang",
      "Subramaniam Vincent"
    ],
    "categories": [
      "cs.IR",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in news sourcing related to demographic groups, aligning with social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "ethnic"
      ],
      "other_detail": "Focus on diversity, equity, and inclusion in journalism",
      "affected_populations": [
        "minority groups",
        "underrepresented demographics"
      ],
      "methodology": [
        "Natural Language Processing",
        "System Design"
      ],
      "methodology_detail": "Real-time quote extraction and monitoring tools",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.10675v1",
    "title": "Mitigating Gender Bias in Machine Translation through Adversarial Learning",
    "year": 2022,
    "authors": [
      "Eve Fleisig",
      "Christiane Fellbaum"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender bias in machine translation, a form of social inequality, and discusses mitigation strategies to reduce harmful stereotypes affecting gender groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Adversarial Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in machine translation models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.10555v1",
    "title": "An Empirical Investigation on the Challenges Faced by Women in the Software Industry: A Case Study",
    "year": 2022,
    "authors": [
      "Bianca Trinkenreich",
      "Ricardo Britto",
      "Marco Aurelio Gerosa",
      "Igor Steinmacher"
    ],
    "categories": [
      "cs.SE",
      "cs.HC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender-based challenges and under-representation in the software industry, addressing social discrimination and inequality related to gender.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women in tech"
      ],
      "methodology": [
        "Case Study",
        "Survey",
        "Qualitative Study",
        "Mixed-Methods"
      ],
      "methodology_detail": "Exploratory case study with surveys and analysis",
      "geographic_focus": [
        "global"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.10382v1",
    "title": "Investigating End-Users' Values in Agriculture Mobile Applications Development: An Empirical Study on Bangladeshi Female Farmers",
    "year": 2022,
    "authors": [
      "Rifat Ara Shams",
      "Mojtaba Shahin",
      "Gillian Oliver",
      "Harsha Perera",
      "Jon Whittle",
      "Arif Nurwidyantoro",
      "Waqar Hussain"
    ],
    "categories": [
      "cs.SE"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study focuses on marginalized female farmers, exploring gender and socioeconomic values, highlighting social inequalities. It examines how demographics influence their values, indicating a concern with social disparities.",
      "inequality_type": [
        "gender",
        "socioeconomic",
        "rural-urban"
      ],
      "other_detail": "Focus on marginalized female farmers in Bangladesh",
      "affected_populations": [
        "female farmers",
        "rural communities"
      ],
      "methodology": [
        "Survey",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzed survey data from 193 farmers",
      "geographic_focus": [
        "Bangladesh"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.10264v1",
    "title": "Assessing Gender Bias in Predictive Algorithms using eXplainable AI",
    "year": 2022,
    "authors": [
      "Cristina Manresa-Yee",
      "Silvia Ramis"
    ],
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses gender bias in predictive algorithms, highlighting social discrimination issues. It explores how biases in data can lead to unfair outcomes affecting gender groups. The focus on bias and fairness in AI relates directly to social inequality concerns.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in AI",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Manipulation of facial expression dataset to analyze bias",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.10170v2",
    "title": "Equitable Ability Estimation in Neurodivergent Student Populations with Zero-Inflated Learner Models",
    "year": 2022,
    "authors": [
      "Niall Twomey",
      "Sarah McMullan",
      "Anat Elhalal",
      "Rafael Poyiadzi",
      "Luis Vaquero"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on equitable ability estimation for neurodivergent students, addressing disparities in educational assessment and learning opportunities, which relates to social inequality and fairness issues.",
      "inequality_type": [
        "educational",
        "disability"
      ],
      "other_detail": "Focuses on neurodivergent student populations",
      "affected_populations": [
        "ND students",
        "educational groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Zero-inflated learner models for behavioral simulation",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.09866v1",
    "title": "Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation",
    "year": 2022,
    "authors": [
      "Beatrice Savoldi",
      "Marco Gaido",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in speech translation, addressing gender inequality in AI systems.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender bias in language technologies",
      "affected_populations": [
        "gender groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes gender bias across languages and models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.09378v1",
    "title": "Revealing the determinants of gender inequality in urban cycling with large-scale data",
    "year": 2022,
    "authors": [
      "Alice Battiston",
      "Ludovico Napoli",
      "Paolo Bajardi",
      "André Panisson",
      "Alan Perotti",
      "Michael Szell",
      "Rossano Schifanella"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender disparities in urban cycling, linking it to urban safety and infrastructure, thus addressing gender inequality. It analyzes how urban environment factors influence women's participation, reflecting social inequality aspects.",
      "inequality_type": [
        "gender",
        "urban-rural",
        "health"
      ],
      "other_detail": "Focus on gender gap in cycling participation",
      "affected_populations": [
        "women",
        "urban residents"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Large-scale data analysis from cycling app",
      "geographic_focus": [
        "United States",
        "United Kingdom",
        "Italy",
        "Benelux"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.09192v1",
    "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists",
    "year": 2022,
    "authors": [
      "Giuseppe Attanasio",
      "Debora Nozza",
      "Dirk Hovy",
      "Elena Baralis"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and fairness issues in NLP models, which relate to social discrimination and inequality, particularly concerning gender and potentially other social groups.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focuses on bias mitigation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Experiment",
        "Model Development"
      ],
      "methodology_detail": "Fine-tuning BERT with entropy-based regularization",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.08933v1",
    "title": "The Digital Divide in Canada and the Role of LEO Satellites in Bridging the Gap",
    "year": 2022,
    "authors": [
      "Tuheen Ahmmed",
      "Afsoon Alidadi",
      "Zichao Zhang",
      "Aizaz U. Chaudhry",
      "Halim Yanikomeroglu"
    ],
    "categories": [
      "eess.SP",
      "cs.SY",
      "econ.GN",
      "eess.SY",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses geographic and infrastructural disparities affecting marginalized communities, notably Indigenous populations, in accessing broadband Internet, which impacts social and educational equity.",
      "inequality_type": [
        "geographic",
        "digital",
        "educational"
      ],
      "other_detail": "Focus on rural, remote, and Indigenous communities",
      "affected_populations": [
        "rural Canadians",
        "Indigenous communities"
      ],
      "methodology": [
        "Literature Review",
        "System Design"
      ],
      "methodology_detail": "Analyzing infrastructure solutions for inequality",
      "geographic_focus": [
        "Canada"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.08849v3",
    "title": "FairFoody: Bringing in Fairness in Food Delivery",
    "year": 2022,
    "authors": [
      "Anjali Gupta",
      "Rahul Yadav",
      "Ashish Nair",
      "Abhijnan Chakraborty",
      "Sayan Ranu",
      "Amitabha Bagchi"
    ],
    "categories": [
      "cs.SI",
      "cs.MA"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses income inequality among gig workers, highlighting socioeconomic disparities. It discusses fairness in income distribution, a social equity concern, within the context of food delivery platforms.",
      "inequality_type": [
        "income",
        "socioeconomic"
      ],
      "other_detail": "Focuses on income fairness among delivery agents",
      "affected_populations": [
        "gig workers",
        "delivery agents"
      ],
      "methodology": [
        "Algorithm Development",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Designs a novel matching algorithm for fair income distribution",
      "geographic_focus": [
        "India"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.08235v1",
    "title": "A Deep Dive into Dataset Imbalance and Bias in Face Identification",
    "year": 2022,
    "authors": [
      "Valeriia Cherepanova",
      "Steven Reich",
      "Samuel Dooley",
      "Hossein Souri",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses bias in face recognition systems affecting demographic groups, highlighting social fairness issues related to race and gender.",
      "inequality_type": [
        "racial",
        "gender"
      ],
      "other_detail": "Focuses on bias and fairness in AI systems",
      "affected_populations": [
        "non-white people",
        "women"
      ],
      "methodology": [
        "Experimental",
        "Dataset Analysis"
      ],
      "methodology_detail": "Analyzes bias effects in face identification datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.08199v1",
    "title": "(Re)Politicizing Digital Well-Being: Beyond User Engagements",
    "year": 2022,
    "authors": [
      "Niall Docherty",
      "Asia J. Biega"
    ],
    "categories": [
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses social, cultural, and political conditions affecting well-being, framing digital ill as linked to social inequalities and power relations.",
      "inequality_type": [
        "social",
        "political",
        "cultural"
      ],
      "other_detail": "Focuses on social and political inequalities beyond individual use",
      "affected_populations": [
        "social groups",
        "cultural communities",
        "political classes"
      ],
      "methodology": [
        "Interdisciplinary Approach",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Combines empirical, ideological, and political analysis",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.05174v2",
    "title": "Assessing Phenotype Definitions for Algorithmic Fairness",
    "year": 2022,
    "authors": [
      "Tony Y. Sun",
      "Shreyas Bhave",
      "Jaan Altosaar",
      "Noémie Elhadad"
    ],
    "categories": [
      "q-bio.OT",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in phenotype definitions across demographic groups, addressing social bias and inequality in health research.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Focus on health disparities in disease identification",
      "affected_populations": [
        "patients with Crohn's disease",
        "diabetes patients"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Fairness Metrics",
        "Empirical Study"
      ],
      "methodology_detail": "Assessing fairness across subgroups using established metrics",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.05051v1",
    "title": "Evaluating Proposed Fairness Models for Face Recognition Algorithms",
    "year": 2022,
    "authors": [
      "John J. Howard",
      "Eli J. Laird",
      "Yevgeniy B. Sirotin",
      "Rebecca E. Rubin",
      "Jerry L. Tipton",
      "Arun R. Vemury"
    ],
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness and demographic disparities in face recognition algorithms, highlighting social bias issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on algorithmic fairness and demographic error differentials",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "demographic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Algorithm Auditing",
        "Dataset Creation"
      ],
      "methodology_detail": "Disaggregated error rate analysis and fairness measure development",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.04913v2",
    "title": "Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers",
    "year": 2022,
    "authors": [
      "Dominik Zietlow",
      "Michael Lohaus",
      "Guha Balakrishnan",
      "Matthäus Kleindessner",
      "Francesco Locatello",
      "Bernhard Schölkopf",
      "Chris Russell"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in AI, highlighting impacts on disadvantaged groups and fairness trade-offs, which relate to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness in computer vision classifiers",
      "affected_populations": [
        "disadvantaged groups",
        "best performing groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Bias-variance decomposition extension and empirical validation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.04462v1",
    "title": "Downstream Fairness Caveats with Synthetic Healthcare Data",
    "year": 2022,
    "authors": [
      "Karan Bhanot",
      "Ioana Baldini",
      "Dennis Wei",
      "Jiaming Zeng",
      "Kristin P. Bennett"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines biases related to race and gender in healthcare data, addressing fairness issues affecting social groups.",
      "inequality_type": [
        "racial",
        "gender",
        "health"
      ],
      "other_detail": "Bias in synthetic healthcare data",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Fairness Evaluation"
      ],
      "methodology_detail": "Synthetic data generation and fairness comparison",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.06038v1",
    "title": "The Long Arc of Fairness: Formalisations and Ethical Discourse",
    "year": 2022,
    "authors": [
      "Pola Schwöbel",
      "Peter Remmers"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness in algorithmic decision making, focusing on social fairness issues such as gender representation on boards, which relates to social inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender equality in corporate governance",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Framework Development"
      ],
      "methodology_detail": "Develops a fairness framework integrating ethical and technical aspects",
      "geographic_focus": [
        "Europe"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2204.09591v1",
    "title": "A Survey on Bias and Fairness in Natural Language Processing",
    "year": 2022,
    "authors": [
      "Rajas Bansal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in NLP models related to gender, race, and culture, which are social inequalities. It examines social bias and fairness issues in AI systems affecting different social groups. The focus is on social discrimination and the societal impact of biases in technology.",
      "inequality_type": [
        "gender",
        "racial",
        "cultural"
      ],
      "other_detail": "Bias and fairness in language models",
      "affected_populations": [
        "gender groups",
        "racial groups",
        "cultural communities"
      ],
      "methodology": [
        "Literature Review",
        "Survey",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Analyzes bias origins, mitigation strategies, and future directions",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.02745v1",
    "title": "The Impact of Differential Privacy on Group Disparity Mitigation",
    "year": 2022,
    "authors": [
      "Victor Petrén Bach Hansen",
      "Atula Tejaswi Neerkaje",
      "Ramit Sawhney",
      "Lucie Flek",
      "Anders Søgaard"
    ],
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness and disparities across social groups affected by differential privacy, addressing social bias and inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and group performance disparities",
      "affected_populations": [
        "minority groups",
        "social groups"
      ],
      "methodology": [
        "Experiment",
        "Empirical Risk Minimization",
        "Group Distributionally Robust Training"
      ],
      "methodology_detail": "Evaluates privacy and fairness interactions across tasks",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.02110v1",
    "title": "FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis",
    "year": 2022,
    "authors": [
      "Yawen Wu",
      "Dewen Zeng",
      "Xiaowei Xu",
      "Yiyu Shi",
      "Jingtong Hu"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in medical AI, focusing on bias related to demographic attributes such as race, gender, and age, which are social categories linked to inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "age",
        "health"
      ],
      "other_detail": "Bias mitigation in medical diagnosis models",
      "affected_populations": [
        "patients with skin lesions",
        "demographic groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Pruning based on parameter importance differences",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.01854v1",
    "title": "A study on the distribution of social biases in self-supervised learning visual models",
    "year": 2022,
    "authors": [
      "Kirill Sirotkin",
      "Pablo Carballeira",
      "Marcos Escudero-Viñolo"
    ],
    "categories": [
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates social biases in AI models, which relate to social discrimination and inequality issues such as bias and fairness. It discusses how biases in training data can lead to discriminatory outcomes, directly addressing social inequality concerns.",
      "inequality_type": [
        "racial",
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Focuses on social biases in AI models",
      "affected_populations": [
        "social groups",
        "discriminated communities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Psychological assessment"
      ],
      "methodology_detail": "Using psychological tools to measure biases in models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier of social biases",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.01731v1",
    "title": "Do Perceived Gender Biases in Retrieval Results Affect Relevance Judgements?",
    "year": 2022,
    "authors": [
      "Klara Krieg",
      "Emilia Parada-Cabaleiro",
      "Markus Schedl",
      "Navid Rekabsaz"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in retrieval results and their impact on relevance judgments, addressing gender stereotypes and perceptions, which are social inequalities.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "Focus on gender stereotypes in information retrieval",
      "affected_populations": [
        "users",
        "annotators"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Relevance judgment experiments with gendered content",
      "geographic_focus": null,
      "ai_relationship": "Unclear",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.01584v1",
    "title": "Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models",
    "year": 2022,
    "authors": [
      "Zhibo Wang",
      "Xiaowei Dong",
      "Henry Xue",
      "Zhifei Zhang",
      "Weifeng Chiu",
      "Tao Wei",
      "Kui Ren"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI systems related to demographic attributes like gender and ethnicity, addressing social bias and discrimination issues.",
      "inequality_type": [
        "gender",
        "ethnic",
        "social bias"
      ],
      "other_detail": "Fairness in AI deployment without model retraining",
      "affected_populations": [
        "demographic groups",
        "societal minorities"
      ],
      "methodology": [
        "Adversarial Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Perturbation and discrimination detection techniques",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2203.00317v2",
    "title": "Explainability for identification of vulnerable groups in machine learning models",
    "year": 2022,
    "authors": [
      "Inga Strümke",
      "Marija Slavkovik"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses vulnerable groups and fairness in AI, addressing social bias and inequality issues.",
      "inequality_type": [
        "social",
        "health",
        "educational"
      ],
      "other_detail": "Focuses on vulnerability and fairness in machine learning models",
      "affected_populations": [
        "vulnerable individuals",
        "social groups"
      ],
      "methodology": [
        "Explainable Artificial Intelligence",
        "Analysis"
      ],
      "methodology_detail": "Analyzing model explanations for vulnerability detection",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.13883v1",
    "title": "EdgeMixup: Improving Fairness for Skin Disease Classification and Segmentation",
    "year": 2022,
    "authors": [
      "Haolin Yuan",
      "Armin Hadzic",
      "William Paul",
      "Daniella Villegas de Flores",
      "Philip Mathew",
      "John Aucott",
      "Yinzhi Cao",
      "Philippe Burlina"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in skin disease diagnosis related to skin tone, highlighting racial disparities in AI performance.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Bias reduction in medical AI systems",
      "affected_populations": [
        "darker skin tones",
        "lighter skin tones"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in skin disease classification",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.12962v1",
    "title": "How Do Mothers and Fathers Talk About Parenting to Different Audiences?: Stereotypes and Audience Effects: An Analysis of r/Daddit, r/Mommit, and r/Parenting Using Topic Modelling",
    "year": 2022,
    "authors": [
      "Melody Sepahpour-Fard",
      "Michael Quayle"
    ],
    "categories": [
      "cs.CY",
      "cs.SI",
      "68U15",
      "J.4"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The study examines gender stereotypes and domestic labor division, reflecting gender inequality in parenting roles.",
      "inequality_type": [
        "gender",
        "social"
      ],
      "other_detail": "Focuses on gender stereotypes in parenting discussions",
      "affected_populations": [
        "mothers",
        "fathers"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Topic modeling of Reddit comments",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.12603v1",
    "title": "Language technology practitioners as language managers: arbitrating data bias and predictive bias in ASR",
    "year": 2022,
    "authors": [
      "Nina Markl",
      "Stephen Joseph McNulty"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses biases in speech recognition affecting marginalized language varieties, highlighting social bias and inequality in technology impacts.",
      "inequality_type": [
        "linguistic",
        "social bias"
      ],
      "other_detail": "Focus on language varieties and marginalized speech communities",
      "affected_populations": [
        "speech communities",
        "marginalized language speakers"
      ],
      "methodology": [
        "Analysis",
        "Literature Review"
      ],
      "methodology_detail": "Using language policy perspective to analyze bias origins",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.12334v1",
    "title": "Trade-offs between Group Fairness Metrics in Societal Resource Allocation",
    "year": 2022,
    "authors": [
      "Tasfia Mashiat",
      "Xavier Gitiaux",
      "Huzefa Rangwala",
      "Patrick J. Fowler",
      "Sanmay Das"
    ],
    "categories": [
      "cs.CY",
      "cs.GT"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines fairness in resource allocation across sociodemographic groups, highlighting heterogeneity and trade-offs that impact marginalized populations.",
      "inequality_type": [
        "socioeconomic",
        "gender",
        "racial"
      ],
      "other_detail": "Focus on resource fairness and intersectional identities",
      "affected_populations": [
        "homeless households",
        "refugees",
        "sociodemographic groups"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Case Study"
      ],
      "methodology_detail": "Analyzes utility distributions and fairness metrics",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.11923v1",
    "title": "Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender",
    "year": 2022,
    "authors": [
      "Anne Lauscher",
      "Archie Crowley",
      "Dirk Hovy"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses discrimination against marginalized groups, especially non-binary individuals, through NLP modeling of pronouns, highlighting social bias and fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender identity and pronoun inclusivity",
      "affected_populations": [
        "non-binary individuals",
        "marginalized groups"
      ],
      "methodology": [
        "Literature Review",
        "Qualitative Study",
        "Model Development"
      ],
      "methodology_detail": "evaluates pronoun modeling approaches qualitatively and quantitatively",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.11499v1",
    "title": "Fairness-Aware Naive Bayes Classifier for Data with Multiple Sensitive Features",
    "year": 2022,
    "authors": [
      "Stelios Boulitsakis-Logothetis"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing discrimination based on sensitive attributes like race and gender, which are central to social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and discrimination mitigation in AI",
      "affected_populations": [
        "racial groups",
        "gender groups",
        "socioeconomic groups"
      ],
      "methodology": [
        "Machine Learning",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness constraints and empirical evaluation on datasets",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.11323v1",
    "title": "Improving fairness in speaker verification via Group-adapted Fusion Network",
    "year": 2022,
    "authors": [
      "Hua Shen",
      "Yuguang Yang",
      "Guoli Sun",
      "Ryan Langman",
      "Eunjung Han",
      "Jasha Droppo",
      "Andreas Stolcke"
    ],
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness disparities in speaker verification, focusing on demographic groups such as gender, highlighting social bias issues in AI systems.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias in voice recognition across demographic groups",
      "affected_populations": [
        "underrepresented gender groups"
      ],
      "methodology": [
        "Deep Learning",
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates fairness improvements in speaker verification models",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.09662v6",
    "title": "Reward Modeling for Mitigating Toxicity in Transformer-based Language Models",
    "year": 2022,
    "authors": [
      "Farshid Faal",
      "Ketra Schmitt",
      "Jia Yuan Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses toxicity and social bias in language models, which relate to social identities such as gender, race, and religion, impacting social fairness and discrimination.",
      "inequality_type": [
        "gender",
        "racial",
        "religion"
      ],
      "other_detail": "Bias mitigation in AI-generated content",
      "affected_populations": [
        "social identity groups"
      ],
      "methodology": [
        "Reinforcement Learning",
        "Natural Language Processing",
        "Experiment"
      ],
      "methodology_detail": "Reward modeling for toxicity detection and mitigation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.09527v1",
    "title": "Ethics and Efficacy of Unsolicited Anti-Trafficking SMS Outreach",
    "year": 2022,
    "authors": [
      "Rasika Bhalerao",
      "Nora McDonald",
      "Hanna Barakat",
      "Vaughn Hamilton",
      "Damon McCoy",
      "Elissa M. Redmiles"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines marginalized sex workers' experiences and risks related to outreach technology, highlighting social stigmatization and harm, which relate to social inequality issues.",
      "inequality_type": [
        "gender",
        "health",
        "disability",
        "social"
      ],
      "other_detail": "focus on marginalized and stigmatized populations",
      "affected_populations": [
        "sex workers",
        "trafficking victims"
      ],
      "methodology": [
        "Qualitative Study"
      ],
      "methodology_detail": "Interviews with stakeholders and workers",
      "geographic_focus": [
        "North America"
      ],
      "ai_relationship": "Unclear",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.09099v2",
    "title": "AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification",
    "year": 2022,
    "authors": [
      "Da Li",
      "Ming Yi",
      "Yukai He"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender discrimination and misogyny in social media, which are forms of social inequality. It focuses on detecting harmful content that perpetuates gender stereotypes and inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "N/A",
      "affected_populations": [
        "women",
        "social media users"
      ],
      "methodology": [
        "Multimodal Deep Learning",
        "Natural Language Processing",
        "Computer Vision"
      ],
      "methodology_detail": "Combines text and image modeling for misogyny detection",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.08011v2",
    "title": "Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks",
    "year": 2022,
    "authors": [
      "Jingyan Zhou",
      "Jiawen Deng",
      "Fei Mi",
      "Yitong Li",
      "Yasheng Wang",
      "Minlie Huang",
      "Xin Jiang",
      "Qun Liu",
      "Helen Meng"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on social bias detection in dialog systems, addressing implicit biases affecting marginalized groups, which relates to social discrimination and inequality.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on social bias in AI dialog systems",
      "affected_populations": [
        "marginalized groups"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Benchmarking",
        "Analysis"
      ],
      "methodology_detail": "Develops datasets and benchmarks for bias detection",
      "geographic_focus": [
        "Chinese"
      ],
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.07603v1",
    "title": "Fairness Indicators for Systematic Assessments of Visual Feature Extractors",
    "year": 2022,
    "authors": [
      "Priya Goyal",
      "Adriana Romero Soriano",
      "Caner Hazirbas",
      "Levent Sagun",
      "Nicolas Usunier"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness, biases, and disparities in computer vision systems affecting different social groups, addressing social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "geographic",
        "social"
      ],
      "other_detail": "Focuses on biases in visual AI systems",
      "affected_populations": [
        "demographic groups",
        "geographic regions"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis",
        "Experiment"
      ],
      "methodology_detail": "Fairness indicators applied to visual models",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.07349v1",
    "title": "IF-City: Intelligible Fair City Planning to Measure, Explain and Mitigate Inequality",
    "year": 2022,
    "authors": [
      "Yan Lyu",
      "Hangxin Lu",
      "Min Kyung Lee",
      "Gerhard Schmitt",
      "Brian Y. Lim"
    ],
    "categories": [
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in urban planning, aiming to address inequality in access to amenities for diverse groups, which relates to social disparities.",
      "inequality_type": [
        "geographic",
        "urban-rural",
        "socioeconomic"
      ],
      "other_detail": "Focuses on urban inequality and access",
      "affected_populations": [
        "urban residents",
        "diverse neighborhoods"
      ],
      "methodology": [
        "System Design",
        "Case Study",
        "Interactive Tool Development"
      ],
      "methodology_detail": "Design and evaluation of visual planning tool",
      "geographic_focus": [
        "New York City"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.07300v2",
    "title": "Choosing an algorithmic fairness metric for an online marketplace: Detecting and quantifying algorithmic bias on LinkedIn",
    "year": 2022,
    "authors": [
      "YinYin Yu",
      "Guillaume Saint-Jacques"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "q-fin.EC"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses algorithmic bias related to gender, a social inequality issue, and discusses fairness in AI systems affecting social groups.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on gender bias in online marketplace algorithms",
      "affected_populations": [
        "job seekers",
        "employers"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias detection and measurement framework",
      "geographic_focus": [
        "LinkedIn platform (global)"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.06196v1",
    "title": "Fairness-aware Configuration of Machine Learning Libraries",
    "year": 2022,
    "authors": [
      "Saeid Tizpaz-Niari",
      "Ashish Kumar",
      "Gang Tan",
      "Ashutosh Trivedi"
    ],
    "categories": [
      "cs.SE",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in machine learning, addressing biases affecting social groups. It investigates how hyperparameters influence algorithmic fairness, which relates to social discrimination issues.",
      "inequality_type": [
        "racial",
        "gender",
        "social fairness"
      ],
      "other_detail": "Focus on algorithmic fairness and social bias mitigation",
      "affected_populations": [
        "social groups",
        "users of ML systems"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Statistical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Testing hyperparameter effects on fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.05682v2",
    "title": "GenderedNews: Une approche computationnelle des écarts de représentation des genres dans la presse française",
    "year": 2022,
    "authors": [
      "Ange Richard",
      "Gilles Bastin",
      "François Portet"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender representation in French media using computational methods, addressing gender inequality. It focuses on gender imbalance, a key aspect of social inequality, through media analysis.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "None",
      "affected_populations": [
        "men",
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Metrics on gender mentions and quotes",
      "geographic_focus": [
        "France"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.04504v1",
    "title": "Prediction Sensitivity: Continual Audit of Counterfactual Fairness in Deployed Classifiers",
    "year": 2022,
    "authors": [
      "Krystal Maughan",
      "Ivoline C. Ngong",
      "Joseph P. Near"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems, focusing on discrimination and bias related to protected groups, which are central to social inequality issues.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and discrimination in AI systems",
      "affected_populations": [
        "individuals of different demographics"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Evaluates fairness metrics in deployed classifiers",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.03914v1",
    "title": "Impact of Network Centrality and Income on Slowing Infection Spread after Outbreaks",
    "year": 2022,
    "authors": [
      "Shiv G. Yücel",
      "Rafael H. M. Pereira",
      "Pedro S. Peixoto",
      "Chico Q. Camargo"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.CY",
      "cs.SI"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The abstract discusses socioeconomic inequalities affecting infection spread and lockdown effectiveness.",
      "inequality_type": [
        "income",
        "socioeconomic",
        "geographic"
      ],
      "other_detail": "Focus on socioeconomic disparities in mobility and isolation capacity",
      "affected_populations": [
        "low-income regions",
        "central network areas"
      ],
      "methodology": [
        "Quantitative Analysis"
      ],
      "methodology_detail": "Infection Delay Model integrating mobility and socioeconomic data",
      "geographic_focus": [
        "Sao Paulo metropolitan region"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.04073v1",
    "title": "The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms",
    "year": 2022,
    "authors": [
      "Jiwoong J. Jeong",
      "Brianna L. Vey",
      "Ananth Reddy",
      "Thomas Kim",
      "Thiago Santos",
      "Ramon Correa",
      "Raman Dutt",
      "Marina Mosunjac",
      "Gabriela Oprea-Ilies",
      "Geoffrey Smith",
      "Minjae Woo",
      "Christopher R. McAdams",
      "Mary S. Newell",
      "Imon Banerjee",
      "Judy Gichoya",
      "Hari Trivedi"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The dataset emphasizes racial diversity and aims to reduce bias in breast AI models, addressing racial disparities in healthcare.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focus on racial diversity in medical imaging data",
      "affected_populations": [
        "White patients",
        "African American patients"
      ],
      "methodology": [
        "Dataset Creation",
        "Model Development"
      ],
      "methodology_detail": "Creating diverse datasets for fair AI models",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.02943v4",
    "title": "Learning fair representation with a parametric integral probability metric",
    "year": 2022,
    "authors": [
      "Dongha Kim",
      "Kunwoong Kim",
      "Insung Kong",
      "Ilsang Ohn",
      "Yongdai Kim"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing bias related to sensitive variables like gender and race, which are central to social inequality issues.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Fair representation in AI models",
      "affected_populations": [
        "gender groups",
        "racial groups"
      ],
      "methodology": [
        "Machine Learning",
        "Adversarial Training",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Using parametric IPM for fair representation learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.02832v4",
    "title": "Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification",
    "year": 2022,
    "authors": [
      "Peter J. Bevan",
      "Amir Atapour-Abarghouei"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses skin tone bias in melanoma detection, highlighting disparities across skin tones, which relates to racial and health inequalities.",
      "inequality_type": [
        "racial",
        "health"
      ],
      "other_detail": "Focuses on racial bias in medical AI systems",
      "affected_populations": [
        "darker skin individuals",
        "lighter skin individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Algorithm Auditing",
        "Experiment"
      ],
      "methodology_detail": "Bias mitigation in skin lesion classification",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.01661v2",
    "title": "Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints",
    "year": 2022,
    "authors": [
      "Anay Mehrotra",
      "Bary S. R. Pradelski",
      "Nisheeth K. Vishnoi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "econ.TH",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses implicit bias related to race, gender, and intersectionality in selection processes, which are key social inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "intersectional"
      ],
      "other_detail": "Focus on implicit bias and intersectionality in social selection",
      "affected_populations": [
        "racial minorities",
        "women",
        "intersectional groups"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Modeling utility and constraints in selection processes",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.01615v1",
    "title": "Measuring Disparate Outcomes of Content Recommendation Algorithms with Distributional Inequality Metrics",
    "year": 2022,
    "authors": [
      "Tomo Lazovich",
      "Luca Belli",
      "Aaron Gonzales",
      "Amanda Bower",
      "Uthaipon Tantipongpipat",
      "Kristian Lum",
      "Ferenc Huszar",
      "Rumman Chowdhury"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities in content exposure on social media, which can reflect social inequalities. It discusses measuring outcomes that may impact different social groups, addressing bias and fairness issues in algorithms.",
      "inequality_type": [
        "informational",
        "digital",
        "social"
      ],
      "other_detail": "Focuses on online social network disparities",
      "affected_populations": [
        "social media users",
        "content consumers"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Distributional Inequality Metrics"
      ],
      "methodology_detail": "Uses economics-inspired metrics to evaluate disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.01013v1",
    "title": "Fairness of Machine Learning Algorithms in Demography",
    "year": 2022,
    "authors": [
      "Ibe Chukwuemeka Emmanuel",
      "Ekaterina Mitrofanova"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "68T05",
      "I.2.6; J.4; H.2.8"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in demographic predictions, addressing social bias related to gender and other sensitive features, which are linked to social inequalities.",
      "inequality_type": [
        "gender",
        "educational",
        "socioeconomic"
      ],
      "other_detail": "Fairness in demographic AI applications",
      "affected_populations": [
        "women",
        "students",
        "married individuals"
      ],
      "methodology": [
        "Machine Learning",
        "Fairness Evaluation",
        "Ensemble Modeling"
      ],
      "methodology_detail": "Using feature dropout and LIME explanations",
      "geographic_focus": [
        "Russia"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.03907v1",
    "title": "Context-Aware Discrimination Detection in Job Vacancies using Computational Language Models",
    "year": 2022,
    "authors": [
      "S. Vethman",
      "A. Adhikari",
      "M. H. T. de Boer",
      "J. A. G. M. van Genabeek",
      "C. J. Veenman"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses gender discrimination in job vacancies, a social inequality issue, using AI methods.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on explicit discrimination detection in employment",
      "affected_populations": [
        "women",
        "job applicants"
      ],
      "methodology": [
        "Machine Learning",
        "Natural Language Processing",
        "Dataset Creation"
      ],
      "methodology_detail": "detects discriminatory context in job descriptions",
      "geographic_focus": [
        "Netherlands"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.13367v1",
    "title": "Don't let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution to the Hiring Paradox",
    "year": 2022,
    "authors": [
      "Jad Salem",
      "Deven R. Desai",
      "Swati Gupta"
    ],
    "categories": [
      "cs.CY",
      "K.4.3; J.0"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses employment inequality, bias, and fairness issues related to race and individual treatment, linking legal and algorithmic solutions.",
      "inequality_type": [
        "racial",
        "educational",
        "disability"
      ],
      "other_detail": "Focus on employment and legal fairness",
      "affected_populations": [
        "job applicants",
        "minority groups"
      ],
      "methodology": [
        "Algorithm Auditing",
        "Legal Analysis",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Using partially ordered sets for evaluation data",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.11596v2",
    "title": "FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification",
    "year": 2022,
    "authors": [
      "Sean Current",
      "Yuntian He",
      "Saket Gurukar",
      "Srinivasan Parthasarathy"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI, focusing on bias mitigation affecting underserved communities, which relates to social inequality issues.",
      "inequality_type": [
        "social",
        "racial",
        "demographic"
      ],
      "other_detail": "focus on fairness in graph-based recommendations",
      "affected_populations": [
        "underserved communities",
        "underrepresented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Graph Neural Networks",
        "Optimization"
      ],
      "methodology_detail": "Graph modification techniques for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.11358v2",
    "title": "Fairness Implications of Encoding Protected Categorical Attributes",
    "year": 2022,
    "authors": [
      "Carlos Mougan",
      "Jose M. Alvarez",
      "Salvatore Ruggieri",
      "Steffen Staab"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.DS",
      "stat.ML"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness, bias, and discrimination related to protected categorical attributes, which are linked to social groups such as ethnicity and potentially race or gender.",
      "inequality_type": [
        "racial",
        "ethnic",
        "gender",
        "social"
      ],
      "other_detail": "Focuses on fairness and bias in machine learning models",
      "affected_populations": [
        "social groups",
        "underrepresented groups"
      ],
      "methodology": [
        "Machine Learning",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Comparative analysis of encoding methods and bias types",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.10913v1",
    "title": "Constructing games on networks for controlling the inequalities in the capital distribution",
    "year": 2022,
    "authors": [
      "Jarosław Adam Miszczak"
    ],
    "categories": [
      "physics.soc-ph",
      "cond-mat.stat-mech",
      "cs.GT",
      "cs.MA",
      "05C57, 62C86"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on capital/resource distribution inequality, a socioeconomic issue.",
      "inequality_type": [
        "wealth",
        "economic",
        "socioeconomic"
      ],
      "other_detail": "None",
      "affected_populations": [
        "society",
        "population"
      ],
      "methodology": [
        "Game Theory",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Using network-based game models to analyze inequality",
      "geographic_focus": null,
      "ai_relationship": "Not AI-related",
      "confidence": 0.85
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.10853v1",
    "title": "Feminist Perspective on Robot Learning Processes",
    "year": 2022,
    "authors": [
      "Juana Valeria Hurtado",
      "Valentina Mejia"
    ],
    "categories": [
      "cs.RO"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses biases in robot learning models linked to societal discrimination, emphasizing feminist perspectives to address social inequalities such as gender and social bias.",
      "inequality_type": [
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on societal discrimination and bias in AI",
      "affected_populations": [
        "less represented identities"
      ],
      "methodology": [
        "Literature Review",
        "Theoretical Analysis"
      ],
      "methodology_detail": "Feminist and social inequality frameworks applied",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.10683v2",
    "title": "Promises and Challenges of Causality for Ethical Machine Learning",
    "year": 2022,
    "authors": [
      "Aida Rahmattalabi",
      "Alice Xiang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses fairness, social categories, and causal inference related to social attributes like race and gender, addressing social bias in AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Focus on causal fairness and social category interventions",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Causal Inference",
        "Theoretical Analysis",
        "Experiment"
      ],
      "methodology_detail": "Applying causal frameworks to fairness evaluation",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.10053v2",
    "title": "Learning Resource Allocation Policies from Observational Data with an Application to Homeless Services Delivery",
    "year": 2022,
    "authors": [
      "Aida Rahmattalabi",
      "Phebe Vayanos",
      "Kathryn Dullerud",
      "Eric Rice"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on improving fairness and outcomes for underserved groups, specifically addressing racial and age disparities in homelessness services.",
      "inequality_type": [
        "racial",
        "age",
        "socioeconomic"
      ],
      "other_detail": "Targets homelessness and vulnerable populations",
      "affected_populations": [
        "Black individuals",
        "youth under 17"
      ],
      "methodology": [
        "Causal Inference",
        "Mixed-Integer Optimization",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Combines causal inference with optimization for policy learning",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.10047v2",
    "title": "Are Commercial Face Detection Models as Biased as Academic Models?",
    "year": 2022,
    "authors": [
      "Samuel Dooley",
      "George Z. Wei",
      "Tom Goldstein",
      "John P. Dickerson"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines demographic disparities in face detection performance, highlighting biases related to age and gender presentation, which are social identity factors. It compares biases between academic and commercial AI models, addressing fairness issues in facial recognition systems. These aspects relate directly to social discrimination and inequality in technology impacts.",
      "inequality_type": [
        "gender",
        "age",
        "racial"
      ],
      "other_detail": "Bias in facial recognition systems",
      "affected_populations": [
        "older individuals",
        "masculine gender presentation"
      ],
      "methodology": [
        "Experiment",
        "Algorithm Auditing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Performance comparison and bias measurement",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.09486v2",
    "title": "Bias in Automated Speaker Recognition",
    "year": 2022,
    "authors": [
      "Wiebke Toussaint Hutiri",
      "Aaron Ding"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines bias affecting gender and nationality groups in AI systems, highlighting social disparities. It discusses performance degradation for women and non-US nationals, indicating social inequality concerns.",
      "inequality_type": [
        "gender",
        "nationality"
      ],
      "other_detail": "Focus on voice recognition disparities across social groups",
      "affected_populations": [
        "female speakers",
        "non-US nationals"
      ],
      "methodology": [
        "Empirical Analysis",
        "Analytical Study"
      ],
      "methodology_detail": "Bias analysis across development stages",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.09425v1",
    "title": "Post-processing of Differentially Private Data: A Fairness Perspective",
    "year": 2022,
    "authors": [
      "Keyu Zhu",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines disparities caused by post-processing in data releases used for societal decisions, impacting groups differently. It analyzes fairness issues in data dissemination and downstream allocations, which relate to social inequalities.",
      "inequality_type": [
        "socioeconomic",
        "racial",
        "educational"
      ],
      "other_detail": "Focus on fairness in data release and decision-making",
      "affected_populations": [
        "social groups",
        "data subjects"
      ],
      "methodology": [
        "Theoretical Analysis",
        "Numerical Simulations"
      ],
      "methodology_detail": "Analyzes fairness bounds and proposes mechanisms",
      "geographic_focus": [
        "US"
      ],
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.8
    }
  },
  {
    "id": "http://arxiv.org/abs/2202.00471v3",
    "title": "Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study",
    "year": 2022,
    "authors": [
      "Kinshuk Sengupta",
      "Praveen Ranjan Srivastava"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates racial bias in AI data and its societal impact, addressing racial discrimination and fairness issues.",
      "inequality_type": [
        "racial",
        "social"
      ],
      "other_detail": "Focus on racial bias in language datasets",
      "affected_populations": [
        "racial groups"
      ],
      "methodology": [
        "Experiment",
        "Mixed methods"
      ],
      "methodology_detail": "Counterfactual analysis and user experience studies",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.08675v2",
    "title": "Gender Bias in Text: Labeled Datasets and Lexicons",
    "year": 2022,
    "authors": [
      "Jad Doughman",
      "Wael Khreich"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on gender bias in language, a form of social discrimination. It addresses societal implications of gender stereotypes and bias detection. The work aims to promote gender equality through NLP tools.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "gender minorities"
      ],
      "methodology": [
        "Dataset Creation",
        "Natural Language Processing",
        "Lexicon Development"
      ],
      "methodology_detail": "Collecting, annotating, and augmenting gender bias datasets",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.08643v1",
    "title": "Text Style Transfer for Bias Mitigation using Masked Language Modeling",
    "year": 2022,
    "authors": [
      "Ewoenam Kwaku Tokpo",
      "Toon Calders"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses bias and stereotypes in textual data, which impact social groups and fairness in AI systems.",
      "inequality_type": [
        "gender",
        "racial",
        "educational",
        "social bias"
      ],
      "other_detail": "Focuses on bias mitigation in language models",
      "affected_populations": [
        "women",
        "minority groups"
      ],
      "methodology": [
        "Natural Language Processing",
        "Model Development"
      ],
      "methodology_detail": "Style transfer using masked language modeling",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.08451v1",
    "title": "Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency",
    "year": 2022,
    "authors": [
      "Austin van Loon",
      "Salvatore Giorgi",
      "Robb Willer",
      "Johannes Eichstaedt"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines racial bias in language models and its relation to racial animus, highlighting issues of racial bias and representation in social data.",
      "inequality_type": [
        "racial",
        "ethnic"
      ],
      "other_detail": "Focuses on racial bias in social media language data",
      "affected_populations": [
        "Black Americans",
        "White Americans"
      ],
      "methodology": [
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Uses WEAT and correlation analysis",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.07856v2",
    "title": "Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms",
    "year": 2022,
    "authors": [
      "Mélanie Bernhardt",
      "Charles Jones",
      "Ben Glocker"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper discusses disparities in medical diagnosis affecting underserved groups, highlighting health inequality and algorithmic bias.",
      "inequality_type": [
        "health",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focuses on health disparities in AI diagnostics",
      "affected_populations": [
        "underserved groups",
        "health disparities"
      ],
      "methodology": [
        "Experiment",
        "Dataset Creation",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzes dataset bias and model performance disparities",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.07754v3",
    "title": "Grep-BiasIR: A Dataset for Investigating Gender Representation-Bias in Information Retrieval Results",
    "year": 2022,
    "authors": [
      "Klara Krieg",
      "Emilia Parada-Cabaleiro",
      "Gertraud Medicus",
      "Oleg Lesota",
      "Markus Schedl",
      "Navid Rekabsaz"
    ],
    "categories": [
      "cs.IR"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates gender bias in information retrieval, addressing social stereotypes and representation issues related to gender, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "",
      "affected_populations": [
        "women",
        "men"
      ],
      "methodology": [
        "Dataset Creation",
        "Experiment",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Creating and analyzing bias-sensitive search query dataset",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.07677v4",
    "title": "Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows",
    "year": 2022,
    "authors": [
      "Wiebke Toussaint",
      "Aaron Yi Ding",
      "Fahim Kawsar",
      "Akhil Mathur"
    ],
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.SE",
      "eess.AS"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates bias propagation in AI systems affecting gender groups, highlighting fairness issues.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on predictive performance disparity",
      "affected_populations": [
        "male",
        "female"
      ],
      "methodology": [
        "Experiment",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Bias measurement and empirical experiments",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.06336v1",
    "title": "Fair Group-Shared Representations with Normalizing Flows",
    "year": 2022,
    "authors": [
      "Mattia Cerrato",
      "Marius Köppel",
      "Alexander Segner",
      "Stefan Kramer"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning, focusing on removing biases related to groups, which are often tied to social inequalities such as race, gender, or socioeconomic status.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focuses on fairness and bias mitigation in AI systems",
      "affected_populations": [
        "underprivileged groups",
        "disadvantaged populations"
      ],
      "methodology": [
        "Machine Learning",
        "Deep Learning",
        "Algorithm Auditing"
      ],
      "methodology_detail": "Uses normalizing flows for fair representation learning",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.06224v2",
    "title": "Unintended Bias in Language Model-driven Conversational Recommendation",
    "year": 2022,
    "authors": [
      "Tianshu Shen",
      "Jiaru Li",
      "Mohamed Reda Bouadjenek",
      "Zheda Mai",
      "Scott Sanner"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper investigates biases related to race, gender, and socioeconomic indicators in AI recommendations, highlighting how these biases reinforce stereotypes and inequalities.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Bias reinforcement in AI recommendations",
      "affected_populations": [
        "racial minorities",
        "women",
        "low-income groups"
      ],
      "methodology": [
        "Experiment",
        "Natural Language Processing",
        "Statistical Analysis"
      ],
      "methodology_detail": "Analyzing bias manifestations in language model recommendations",
      "geographic_focus": null,
      "ai_relationship": "AI as amplifier",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.04461v1",
    "title": "Blackbox Post-Processing for Multiclass Fairness",
    "year": 2022,
    "authors": [
      "Preston Putzel",
      "Scott Lee"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in machine learning, focusing on reducing societal impacts of biased algorithms across demographic groups.",
      "inequality_type": [
        "racial",
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Fairness in multiclass classification settings",
      "affected_populations": [
        "demographic groups",
        "social minorities"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment"
      ],
      "methodology_detail": "Post-processing fairness adjustments",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.03681v3",
    "title": "FairEdit: Preserving Fairness in Graph Neural Networks through Greedy Graph Editing",
    "year": 2022,
    "authors": [
      "Donald Loveland",
      "Jiayi Pan",
      "Aaresh Farrokh Bhathena",
      "Yiyang Lu"
    ],
    "categories": [
      "cs.LG"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper focuses on fairness in AI, addressing social bias and discrimination issues related to equitable treatment in graph neural networks.",
      "inequality_type": [
        "social",
        "fairness"
      ],
      "other_detail": "Addresses algorithmic fairness in human-centered applications",
      "affected_populations": [
        "users of GNNs",
        "social groups impacted by bias"
      ],
      "methodology": [
        "Algorithm Development",
        "Fairness Optimization"
      ],
      "methodology_detail": "Edge editing algorithms for fairness improvement",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.03662v1",
    "title": "Learning Fair Node Representations with Graph Counterfactual Fairness",
    "year": 2022,
    "authors": [
      "Jing Ma",
      "Ruocheng Guo",
      "Mengting Wan",
      "Longqi Yang",
      "Aidong Zhang",
      "Jundong Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI related to sensitive attributes like race and gender, which are central to social inequalities. It focuses on mitigating biases affecting subpopulations, indicating a concern with social discrimination. The causal fairness framework aims to reduce social bias impacts in graph-based AI systems.",
      "inequality_type": [
        "racial",
        "gender",
        "social bias"
      ],
      "other_detail": "Fairness in graph neural networks",
      "affected_populations": [
        "racial groups",
        "gender groups"
      ],
      "methodology": [
        "Machine Learning",
        "Counterfactual Analysis",
        "Fairness Framework"
      ],
      "methodology_detail": "Counterfactual data augmentation for fairness",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.03173v1",
    "title": "Quantifying Gender Bias in Consumer Culture",
    "year": 2022,
    "authors": [
      "Reihane Boghrati",
      "Jonah Berger"
    ],
    "categories": [
      "cs.CL"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender bias in song lyrics, highlighting societal stereotypes and discrimination against women, which are key aspects of social inequality.",
      "inequality_type": [
        "gender"
      ],
      "other_detail": "focus on cultural stereotypes and discrimination",
      "affected_populations": [
        "women"
      ],
      "methodology": [
        "Natural Language Processing",
        "Quantitative Analysis"
      ],
      "methodology_detail": "Analyzes 250,000 songs over 50 years",
      "geographic_focus": null,
      "ai_relationship": "AI as measurement tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.03092v1",
    "title": "Uncovering the Source of Machine Bias",
    "year": 2022,
    "authors": [
      "Xiyang Hu",
      "Yan Huang",
      "Beibei Li",
      "Tian Lu"
    ],
    "categories": [
      "cs.LG",
      "econ.GN",
      "q-fin.EC",
      "stat.ML",
      "I.2"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender biases in human decision-making and AI, affecting gender equity.",
      "inequality_type": [
        "gender",
        "socioeconomic"
      ],
      "other_detail": "Focus on gender bias in financial decision-making",
      "affected_populations": [
        "female applicants",
        "male applicants"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Model Development",
        "Machine Learning"
      ],
      "methodology_detail": "Econometric modeling and algorithm comparison",
      "geographic_focus": null,
      "ai_relationship": "AI as mitigation tool",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.01148v1",
    "title": "Parity-based Cumulative Fairness-aware Boosting",
    "year": 2022,
    "authors": [
      "Vasileios Iosifidis",
      "Arjun Roy",
      "Eirini Ntoutsi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "is_ai_related_original": true,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper addresses fairness in AI systems related to protected attributes like gender and race, aiming to mitigate discrimination and social bias. It discusses disparities affecting underrepresented groups and fairness notions such as parity and equal opportunity. These focus areas directly relate to social inequalities and discrimination issues.",
      "inequality_type": [
        "gender",
        "racial",
        "social bias"
      ],
      "other_detail": "Focus on fairness in AI systems addressing discrimination",
      "affected_populations": [
        "females",
        "minority groups"
      ],
      "methodology": [
        "Machine Learning",
        "Experiment",
        "Fairness Optimization"
      ],
      "methodology_detail": "Fairness-aware boosting and ensemble optimization",
      "geographic_focus": null,
      "ai_relationship": "AI as solution",
      "confidence": 0.9
    }
  },
  {
    "id": "http://arxiv.org/abs/2201.00254v2",
    "title": "Subfield prestige and gender inequality in computing",
    "year": 2022,
    "authors": [
      "Nicholas LaBerge",
      "K. Hunter Wapman",
      "Allison C. Morgan",
      "Sam Zhang",
      "Daniel B. Larremore",
      "Aaron Clauset"
    ],
    "categories": [
      "cs.CY"
    ],
    "is_ai_related_original": false,
    "analysis": {
      "is_social_inequality": true,
      "reason": "The paper examines gender and racial disparities among computing faculty, focusing on underrepresentation and inequality trends.",
      "inequality_type": [
        "gender",
        "racial",
        "socioeconomic"
      ],
      "other_detail": "Focus on academic and professional disparities in computing",
      "affected_populations": [
        "women",
        "people of color"
      ],
      "methodology": [
        "Quantitative Analysis",
        "Dataset Creation"
      ],
      "methodology_detail": "Analyzes faculty and publication data across subfields",
      "geographic_focus": [
        "United States"
      ],
      "ai_relationship": "Not AI-related",
      "confidence": 0.9
    }
  }
]